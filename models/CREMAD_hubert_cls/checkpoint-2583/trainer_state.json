{
  "best_metric": 0.24708313558012496,
  "best_model_checkpoint": "models\\CREMAD_hubert_cls\\checkpoint-2583",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2583,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00038714672861014324,
      "grad_norm": 1.8722529411315918,
      "learning_rate": 3.871467286101433e-09,
      "loss": 1.9665,
      "step": 1
    },
    {
      "epoch": 0.0007742934572202865,
      "grad_norm": 1.3827552795410156,
      "learning_rate": 7.742934572202866e-09,
      "loss": 1.9174,
      "step": 2
    },
    {
      "epoch": 0.0011614401858304297,
      "grad_norm": 1.2626818418502808,
      "learning_rate": 1.1614401858304298e-08,
      "loss": 1.9406,
      "step": 3
    },
    {
      "epoch": 0.001548586914440573,
      "grad_norm": 1.2638156414031982,
      "learning_rate": 1.5485869144405732e-08,
      "loss": 1.9347,
      "step": 4
    },
    {
      "epoch": 0.0019357336430507162,
      "grad_norm": 1.4256352186203003,
      "learning_rate": 1.9357336430507163e-08,
      "loss": 1.941,
      "step": 5
    },
    {
      "epoch": 0.0023228803716608595,
      "grad_norm": 1.1354379653930664,
      "learning_rate": 2.3228803716608597e-08,
      "loss": 1.9343,
      "step": 6
    },
    {
      "epoch": 0.0027100271002710027,
      "grad_norm": 1.5203808546066284,
      "learning_rate": 2.710027100271003e-08,
      "loss": 1.9437,
      "step": 7
    },
    {
      "epoch": 0.003097173828881146,
      "grad_norm": 1.2510936260223389,
      "learning_rate": 3.0971738288811464e-08,
      "loss": 1.9503,
      "step": 8
    },
    {
      "epoch": 0.003484320557491289,
      "grad_norm": 1.2681835889816284,
      "learning_rate": 3.484320557491289e-08,
      "loss": 1.9602,
      "step": 9
    },
    {
      "epoch": 0.0038714672861014324,
      "grad_norm": 1.2337955236434937,
      "learning_rate": 3.8714672861014325e-08,
      "loss": 1.9297,
      "step": 10
    },
    {
      "epoch": 0.004258614014711576,
      "grad_norm": 1.2799679040908813,
      "learning_rate": 4.258614014711576e-08,
      "loss": 1.9389,
      "step": 11
    },
    {
      "epoch": 0.004645760743321719,
      "grad_norm": 1.1558623313903809,
      "learning_rate": 4.645760743321719e-08,
      "loss": 1.9505,
      "step": 12
    },
    {
      "epoch": 0.005032907471931862,
      "grad_norm": 1.2249882221221924,
      "learning_rate": 5.032907471931863e-08,
      "loss": 1.9634,
      "step": 13
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 1.39509117603302,
      "learning_rate": 5.420054200542006e-08,
      "loss": 1.943,
      "step": 14
    },
    {
      "epoch": 0.005807200929152149,
      "grad_norm": 2.282097816467285,
      "learning_rate": 5.807200929152149e-08,
      "loss": 1.9743,
      "step": 15
    },
    {
      "epoch": 0.006194347657762292,
      "grad_norm": 1.5924534797668457,
      "learning_rate": 6.194347657762293e-08,
      "loss": 1.9034,
      "step": 16
    },
    {
      "epoch": 0.006581494386372435,
      "grad_norm": 1.3129477500915527,
      "learning_rate": 6.581494386372436e-08,
      "loss": 1.9399,
      "step": 17
    },
    {
      "epoch": 0.006968641114982578,
      "grad_norm": 1.948675274848938,
      "learning_rate": 6.968641114982578e-08,
      "loss": 1.921,
      "step": 18
    },
    {
      "epoch": 0.007355787843592722,
      "grad_norm": 1.161292552947998,
      "learning_rate": 7.355787843592722e-08,
      "loss": 1.9295,
      "step": 19
    },
    {
      "epoch": 0.007742934572202865,
      "grad_norm": 1.602184772491455,
      "learning_rate": 7.742934572202865e-08,
      "loss": 1.9164,
      "step": 20
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 1.4578857421875,
      "learning_rate": 8.130081300813009e-08,
      "loss": 1.9829,
      "step": 21
    },
    {
      "epoch": 0.008517228029423151,
      "grad_norm": 1.7535805702209473,
      "learning_rate": 8.517228029423152e-08,
      "loss": 2.0017,
      "step": 22
    },
    {
      "epoch": 0.008904374758033295,
      "grad_norm": 1.3390156030654907,
      "learning_rate": 8.904374758033296e-08,
      "loss": 1.9736,
      "step": 23
    },
    {
      "epoch": 0.009291521486643438,
      "grad_norm": 1.3785299062728882,
      "learning_rate": 9.291521486643439e-08,
      "loss": 1.9085,
      "step": 24
    },
    {
      "epoch": 0.009678668215253582,
      "grad_norm": 1.6784433126449585,
      "learning_rate": 9.678668215253583e-08,
      "loss": 1.9873,
      "step": 25
    },
    {
      "epoch": 0.010065814943863724,
      "grad_norm": 1.1994949579238892,
      "learning_rate": 1.0065814943863725e-07,
      "loss": 1.9368,
      "step": 26
    },
    {
      "epoch": 0.010452961672473868,
      "grad_norm": 2.8482437133789062,
      "learning_rate": 1.045296167247387e-07,
      "loss": 2.0242,
      "step": 27
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 1.3148632049560547,
      "learning_rate": 1.0840108401084012e-07,
      "loss": 1.9134,
      "step": 28
    },
    {
      "epoch": 0.011227255129694155,
      "grad_norm": 1.808053731918335,
      "learning_rate": 1.1227255129694156e-07,
      "loss": 1.9751,
      "step": 29
    },
    {
      "epoch": 0.011614401858304297,
      "grad_norm": 1.7089858055114746,
      "learning_rate": 1.1614401858304298e-07,
      "loss": 1.9722,
      "step": 30
    },
    {
      "epoch": 0.012001548586914441,
      "grad_norm": 1.437395453453064,
      "learning_rate": 1.2001548586914442e-07,
      "loss": 1.9501,
      "step": 31
    },
    {
      "epoch": 0.012388695315524584,
      "grad_norm": 1.1888842582702637,
      "learning_rate": 1.2388695315524586e-07,
      "loss": 1.9494,
      "step": 32
    },
    {
      "epoch": 0.012775842044134728,
      "grad_norm": 1.641129732131958,
      "learning_rate": 1.277584204413473e-07,
      "loss": 1.9476,
      "step": 33
    },
    {
      "epoch": 0.01316298877274487,
      "grad_norm": 1.4256542921066284,
      "learning_rate": 1.316298877274487e-07,
      "loss": 1.9882,
      "step": 34
    },
    {
      "epoch": 0.013550135501355014,
      "grad_norm": 1.4232455492019653,
      "learning_rate": 1.3550135501355015e-07,
      "loss": 1.9382,
      "step": 35
    },
    {
      "epoch": 0.013937282229965157,
      "grad_norm": 1.3072998523712158,
      "learning_rate": 1.3937282229965157e-07,
      "loss": 1.9046,
      "step": 36
    },
    {
      "epoch": 0.014324428958575301,
      "grad_norm": 1.658265471458435,
      "learning_rate": 1.4324428958575303e-07,
      "loss": 1.9523,
      "step": 37
    },
    {
      "epoch": 0.014711575687185443,
      "grad_norm": 2.552384853363037,
      "learning_rate": 1.4711575687185445e-07,
      "loss": 1.9937,
      "step": 38
    },
    {
      "epoch": 0.015098722415795587,
      "grad_norm": 1.3056846857070923,
      "learning_rate": 1.509872241579559e-07,
      "loss": 1.9564,
      "step": 39
    },
    {
      "epoch": 0.01548586914440573,
      "grad_norm": 1.5663657188415527,
      "learning_rate": 1.548586914440573e-07,
      "loss": 1.9578,
      "step": 40
    },
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 1.4655976295471191,
      "learning_rate": 1.5873015873015874e-07,
      "loss": 1.9609,
      "step": 41
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 1.363230586051941,
      "learning_rate": 1.6260162601626018e-07,
      "loss": 1.9456,
      "step": 42
    },
    {
      "epoch": 0.01664730933023616,
      "grad_norm": 1.4018268585205078,
      "learning_rate": 1.6647309330236162e-07,
      "loss": 1.974,
      "step": 43
    },
    {
      "epoch": 0.017034456058846303,
      "grad_norm": 1.313948631286621,
      "learning_rate": 1.7034456058846304e-07,
      "loss": 1.9217,
      "step": 44
    },
    {
      "epoch": 0.017421602787456445,
      "grad_norm": 1.3514381647109985,
      "learning_rate": 1.7421602787456448e-07,
      "loss": 1.9363,
      "step": 45
    },
    {
      "epoch": 0.01780874951606659,
      "grad_norm": 1.3341065645217896,
      "learning_rate": 1.7808749516066592e-07,
      "loss": 1.947,
      "step": 46
    },
    {
      "epoch": 0.018195896244676733,
      "grad_norm": 1.6700693368911743,
      "learning_rate": 1.8195896244676736e-07,
      "loss": 1.9013,
      "step": 47
    },
    {
      "epoch": 0.018583042973286876,
      "grad_norm": 1.316781997680664,
      "learning_rate": 1.8583042973286877e-07,
      "loss": 1.9705,
      "step": 48
    },
    {
      "epoch": 0.018970189701897018,
      "grad_norm": 1.6118602752685547,
      "learning_rate": 1.897018970189702e-07,
      "loss": 1.945,
      "step": 49
    },
    {
      "epoch": 0.019357336430507164,
      "grad_norm": 1.5270217657089233,
      "learning_rate": 1.9357336430507165e-07,
      "loss": 1.9544,
      "step": 50
    },
    {
      "epoch": 0.019744483159117306,
      "grad_norm": 1.4832324981689453,
      "learning_rate": 1.9744483159117307e-07,
      "loss": 1.9545,
      "step": 51
    },
    {
      "epoch": 0.02013162988772745,
      "grad_norm": 1.3447526693344116,
      "learning_rate": 2.013162988772745e-07,
      "loss": 1.9503,
      "step": 52
    },
    {
      "epoch": 0.02051877661633759,
      "grad_norm": 1.3612908124923706,
      "learning_rate": 2.0518776616337592e-07,
      "loss": 1.9433,
      "step": 53
    },
    {
      "epoch": 0.020905923344947737,
      "grad_norm": 1.390224814414978,
      "learning_rate": 2.090592334494774e-07,
      "loss": 1.94,
      "step": 54
    },
    {
      "epoch": 0.02129307007355788,
      "grad_norm": 1.323393702507019,
      "learning_rate": 2.129307007355788e-07,
      "loss": 1.9492,
      "step": 55
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 1.2235162258148193,
      "learning_rate": 2.1680216802168024e-07,
      "loss": 1.9142,
      "step": 56
    },
    {
      "epoch": 0.022067363530778164,
      "grad_norm": 1.4587013721466064,
      "learning_rate": 2.2067363530778166e-07,
      "loss": 1.9667,
      "step": 57
    },
    {
      "epoch": 0.02245451025938831,
      "grad_norm": 1.5683465003967285,
      "learning_rate": 2.2454510259388312e-07,
      "loss": 1.9902,
      "step": 58
    },
    {
      "epoch": 0.022841656987998452,
      "grad_norm": 1.1734521389007568,
      "learning_rate": 2.2841656987998454e-07,
      "loss": 1.9399,
      "step": 59
    },
    {
      "epoch": 0.023228803716608595,
      "grad_norm": 1.223766803741455,
      "learning_rate": 2.3228803716608595e-07,
      "loss": 1.9228,
      "step": 60
    },
    {
      "epoch": 0.023615950445218737,
      "grad_norm": 1.4083489179611206,
      "learning_rate": 2.361595044521874e-07,
      "loss": 1.9386,
      "step": 61
    },
    {
      "epoch": 0.024003097173828883,
      "grad_norm": 1.222882628440857,
      "learning_rate": 2.4003097173828883e-07,
      "loss": 1.9436,
      "step": 62
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 1.1779512166976929,
      "learning_rate": 2.439024390243903e-07,
      "loss": 1.9587,
      "step": 63
    },
    {
      "epoch": 0.024777390631049168,
      "grad_norm": 1.3427867889404297,
      "learning_rate": 2.477739063104917e-07,
      "loss": 1.9423,
      "step": 64
    },
    {
      "epoch": 0.02516453735965931,
      "grad_norm": 1.3166260719299316,
      "learning_rate": 2.516453735965931e-07,
      "loss": 1.9297,
      "step": 65
    },
    {
      "epoch": 0.025551684088269456,
      "grad_norm": 1.241493582725525,
      "learning_rate": 2.555168408826946e-07,
      "loss": 1.9211,
      "step": 66
    },
    {
      "epoch": 0.025938830816879598,
      "grad_norm": 1.08889901638031,
      "learning_rate": 2.59388308168796e-07,
      "loss": 1.9327,
      "step": 67
    },
    {
      "epoch": 0.02632597754548974,
      "grad_norm": 1.323351502418518,
      "learning_rate": 2.632597754548974e-07,
      "loss": 1.9433,
      "step": 68
    },
    {
      "epoch": 0.026713124274099883,
      "grad_norm": 1.6102895736694336,
      "learning_rate": 2.6713124274099886e-07,
      "loss": 1.9331,
      "step": 69
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 1.2116812467575073,
      "learning_rate": 2.710027100271003e-07,
      "loss": 1.9339,
      "step": 70
    },
    {
      "epoch": 0.02748741773132017,
      "grad_norm": 1.355094075202942,
      "learning_rate": 2.7487417731320175e-07,
      "loss": 1.9562,
      "step": 71
    },
    {
      "epoch": 0.027874564459930314,
      "grad_norm": 1.508041262626648,
      "learning_rate": 2.7874564459930313e-07,
      "loss": 1.9267,
      "step": 72
    },
    {
      "epoch": 0.028261711188540456,
      "grad_norm": 1.884343147277832,
      "learning_rate": 2.8261711188540457e-07,
      "loss": 1.9387,
      "step": 73
    },
    {
      "epoch": 0.028648857917150602,
      "grad_norm": 1.177363395690918,
      "learning_rate": 2.8648857917150607e-07,
      "loss": 1.9102,
      "step": 74
    },
    {
      "epoch": 0.029036004645760744,
      "grad_norm": 1.6059622764587402,
      "learning_rate": 2.9036004645760745e-07,
      "loss": 1.966,
      "step": 75
    },
    {
      "epoch": 0.029423151374370887,
      "grad_norm": 1.9946407079696655,
      "learning_rate": 2.942315137437089e-07,
      "loss": 1.9223,
      "step": 76
    },
    {
      "epoch": 0.02981029810298103,
      "grad_norm": 1.584816575050354,
      "learning_rate": 2.9810298102981034e-07,
      "loss": 1.9353,
      "step": 77
    },
    {
      "epoch": 0.030197444831591175,
      "grad_norm": 1.761682152748108,
      "learning_rate": 3.019744483159118e-07,
      "loss": 1.9959,
      "step": 78
    },
    {
      "epoch": 0.030584591560201317,
      "grad_norm": 1.2681838274002075,
      "learning_rate": 3.058459156020132e-07,
      "loss": 1.9638,
      "step": 79
    },
    {
      "epoch": 0.03097173828881146,
      "grad_norm": 1.2599982023239136,
      "learning_rate": 3.097173828881146e-07,
      "loss": 1.9294,
      "step": 80
    },
    {
      "epoch": 0.0313588850174216,
      "grad_norm": 1.2641021013259888,
      "learning_rate": 3.1358885017421604e-07,
      "loss": 1.9599,
      "step": 81
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 1.806406855583191,
      "learning_rate": 3.174603174603175e-07,
      "loss": 1.9721,
      "step": 82
    },
    {
      "epoch": 0.03213317847464189,
      "grad_norm": 1.8139973878860474,
      "learning_rate": 3.2133178474641887e-07,
      "loss": 1.9249,
      "step": 83
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 1.4414734840393066,
      "learning_rate": 3.2520325203252037e-07,
      "loss": 1.9418,
      "step": 84
    },
    {
      "epoch": 0.03290747193186218,
      "grad_norm": 1.3878464698791504,
      "learning_rate": 3.290747193186218e-07,
      "loss": 1.9529,
      "step": 85
    },
    {
      "epoch": 0.03329461866047232,
      "grad_norm": 1.3376766443252563,
      "learning_rate": 3.3294618660472325e-07,
      "loss": 1.9536,
      "step": 86
    },
    {
      "epoch": 0.03368176538908246,
      "grad_norm": 1.1098401546478271,
      "learning_rate": 3.3681765389082463e-07,
      "loss": 1.9283,
      "step": 87
    },
    {
      "epoch": 0.034068912117692605,
      "grad_norm": 1.6624932289123535,
      "learning_rate": 3.406891211769261e-07,
      "loss": 1.9864,
      "step": 88
    },
    {
      "epoch": 0.03445605884630275,
      "grad_norm": 1.6171804666519165,
      "learning_rate": 3.445605884630275e-07,
      "loss": 1.9468,
      "step": 89
    },
    {
      "epoch": 0.03484320557491289,
      "grad_norm": 1.2598140239715576,
      "learning_rate": 3.4843205574912896e-07,
      "loss": 1.9234,
      "step": 90
    },
    {
      "epoch": 0.03523035230352303,
      "grad_norm": 1.2662670612335205,
      "learning_rate": 3.5230352303523034e-07,
      "loss": 1.9764,
      "step": 91
    },
    {
      "epoch": 0.03561749903213318,
      "grad_norm": 1.4466278553009033,
      "learning_rate": 3.5617499032133184e-07,
      "loss": 1.9249,
      "step": 92
    },
    {
      "epoch": 0.036004645760743324,
      "grad_norm": 1.3765774965286255,
      "learning_rate": 3.600464576074333e-07,
      "loss": 1.9697,
      "step": 93
    },
    {
      "epoch": 0.03639179248935347,
      "grad_norm": 1.7024292945861816,
      "learning_rate": 3.639179248935347e-07,
      "loss": 1.9322,
      "step": 94
    },
    {
      "epoch": 0.03677893921796361,
      "grad_norm": 1.2493489980697632,
      "learning_rate": 3.677893921796361e-07,
      "loss": 1.9433,
      "step": 95
    },
    {
      "epoch": 0.03716608594657375,
      "grad_norm": 1.2947214841842651,
      "learning_rate": 3.7166085946573755e-07,
      "loss": 1.9419,
      "step": 96
    },
    {
      "epoch": 0.037553232675183894,
      "grad_norm": 1.1667033433914185,
      "learning_rate": 3.75532326751839e-07,
      "loss": 1.9536,
      "step": 97
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 1.2410893440246582,
      "learning_rate": 3.794037940379404e-07,
      "loss": 1.9438,
      "step": 98
    },
    {
      "epoch": 0.03832752613240418,
      "grad_norm": 1.519470453262329,
      "learning_rate": 3.832752613240418e-07,
      "loss": 1.9765,
      "step": 99
    },
    {
      "epoch": 0.03871467286101433,
      "grad_norm": 1.4211254119873047,
      "learning_rate": 3.871467286101433e-07,
      "loss": 1.9347,
      "step": 100
    },
    {
      "epoch": 0.03910181958962447,
      "grad_norm": 1.6876616477966309,
      "learning_rate": 3.9101819589624475e-07,
      "loss": 1.9288,
      "step": 101
    },
    {
      "epoch": 0.03948896631823461,
      "grad_norm": 1.8199079036712646,
      "learning_rate": 3.9488966318234614e-07,
      "loss": 1.9188,
      "step": 102
    },
    {
      "epoch": 0.039876113046844755,
      "grad_norm": 1.3045974969863892,
      "learning_rate": 3.987611304684476e-07,
      "loss": 1.9071,
      "step": 103
    },
    {
      "epoch": 0.0402632597754549,
      "grad_norm": 1.272830605506897,
      "learning_rate": 4.02632597754549e-07,
      "loss": 1.9353,
      "step": 104
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 1.3459570407867432,
      "learning_rate": 4.0650406504065046e-07,
      "loss": 1.9307,
      "step": 105
    },
    {
      "epoch": 0.04103755323267518,
      "grad_norm": 1.138325572013855,
      "learning_rate": 4.1037553232675184e-07,
      "loss": 1.9277,
      "step": 106
    },
    {
      "epoch": 0.041424699961285325,
      "grad_norm": 1.2090156078338623,
      "learning_rate": 4.142469996128533e-07,
      "loss": 1.9336,
      "step": 107
    },
    {
      "epoch": 0.041811846689895474,
      "grad_norm": 1.7125427722930908,
      "learning_rate": 4.181184668989548e-07,
      "loss": 1.9698,
      "step": 108
    },
    {
      "epoch": 0.042198993418505616,
      "grad_norm": 1.2674199342727661,
      "learning_rate": 4.219899341850562e-07,
      "loss": 1.9546,
      "step": 109
    },
    {
      "epoch": 0.04258614014711576,
      "grad_norm": 1.2682015895843506,
      "learning_rate": 4.258614014711576e-07,
      "loss": 1.9724,
      "step": 110
    },
    {
      "epoch": 0.0429732868757259,
      "grad_norm": 1.6381535530090332,
      "learning_rate": 4.2973286875725905e-07,
      "loss": 1.9327,
      "step": 111
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 1.373987078666687,
      "learning_rate": 4.336043360433605e-07,
      "loss": 1.9685,
      "step": 112
    },
    {
      "epoch": 0.043747580332946186,
      "grad_norm": 1.551061987876892,
      "learning_rate": 4.374758033294619e-07,
      "loss": 1.9154,
      "step": 113
    },
    {
      "epoch": 0.04413472706155633,
      "grad_norm": 1.1360738277435303,
      "learning_rate": 4.413472706155633e-07,
      "loss": 1.9266,
      "step": 114
    },
    {
      "epoch": 0.04452187379016647,
      "grad_norm": 1.3859291076660156,
      "learning_rate": 4.4521873790166476e-07,
      "loss": 1.9895,
      "step": 115
    },
    {
      "epoch": 0.04490902051877662,
      "grad_norm": 1.1683683395385742,
      "learning_rate": 4.4909020518776625e-07,
      "loss": 1.9103,
      "step": 116
    },
    {
      "epoch": 0.04529616724738676,
      "grad_norm": 1.4732296466827393,
      "learning_rate": 4.5296167247386764e-07,
      "loss": 1.9578,
      "step": 117
    },
    {
      "epoch": 0.045683313975996905,
      "grad_norm": 1.3103270530700684,
      "learning_rate": 4.568331397599691e-07,
      "loss": 1.9643,
      "step": 118
    },
    {
      "epoch": 0.04607046070460705,
      "grad_norm": 1.541742205619812,
      "learning_rate": 4.607046070460705e-07,
      "loss": 1.9323,
      "step": 119
    },
    {
      "epoch": 0.04645760743321719,
      "grad_norm": 1.4031747579574585,
      "learning_rate": 4.645760743321719e-07,
      "loss": 1.9424,
      "step": 120
    },
    {
      "epoch": 0.04684475416182733,
      "grad_norm": 1.2545462846755981,
      "learning_rate": 4.6844754161827335e-07,
      "loss": 1.9171,
      "step": 121
    },
    {
      "epoch": 0.047231900890437474,
      "grad_norm": 1.7230219841003418,
      "learning_rate": 4.723190089043748e-07,
      "loss": 1.9284,
      "step": 122
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.366240382194519,
      "learning_rate": 4.7619047619047623e-07,
      "loss": 1.9595,
      "step": 123
    },
    {
      "epoch": 0.048006194347657766,
      "grad_norm": 1.2956231832504272,
      "learning_rate": 4.800619434765777e-07,
      "loss": 1.9364,
      "step": 124
    },
    {
      "epoch": 0.04839334107626791,
      "grad_norm": 1.6009840965270996,
      "learning_rate": 4.839334107626792e-07,
      "loss": 1.9576,
      "step": 125
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 1.8668384552001953,
      "learning_rate": 4.878048780487805e-07,
      "loss": 1.9437,
      "step": 126
    },
    {
      "epoch": 0.04916763453348819,
      "grad_norm": 1.2493070363998413,
      "learning_rate": 4.916763453348819e-07,
      "loss": 1.9069,
      "step": 127
    },
    {
      "epoch": 0.049554781262098335,
      "grad_norm": 1.4693695306777954,
      "learning_rate": 4.955478126209834e-07,
      "loss": 1.9378,
      "step": 128
    },
    {
      "epoch": 0.04994192799070848,
      "grad_norm": 1.3185383081436157,
      "learning_rate": 4.994192799070848e-07,
      "loss": 1.9629,
      "step": 129
    },
    {
      "epoch": 0.05032907471931862,
      "grad_norm": 1.330763339996338,
      "learning_rate": 5.032907471931862e-07,
      "loss": 1.9106,
      "step": 130
    },
    {
      "epoch": 0.05071622144792876,
      "grad_norm": 1.402917742729187,
      "learning_rate": 5.071622144792877e-07,
      "loss": 1.9535,
      "step": 131
    },
    {
      "epoch": 0.05110336817653891,
      "grad_norm": 1.5598068237304688,
      "learning_rate": 5.110336817653892e-07,
      "loss": 1.9177,
      "step": 132
    },
    {
      "epoch": 0.051490514905149054,
      "grad_norm": 1.5118436813354492,
      "learning_rate": 5.149051490514906e-07,
      "loss": 1.9613,
      "step": 133
    },
    {
      "epoch": 0.051877661633759196,
      "grad_norm": 1.3305963277816772,
      "learning_rate": 5.18776616337592e-07,
      "loss": 1.9311,
      "step": 134
    },
    {
      "epoch": 0.05226480836236934,
      "grad_norm": 1.5102858543395996,
      "learning_rate": 5.226480836236935e-07,
      "loss": 1.9032,
      "step": 135
    },
    {
      "epoch": 0.05265195509097948,
      "grad_norm": 1.3898465633392334,
      "learning_rate": 5.265195509097948e-07,
      "loss": 1.9022,
      "step": 136
    },
    {
      "epoch": 0.053039101819589624,
      "grad_norm": 1.5764615535736084,
      "learning_rate": 5.303910181958962e-07,
      "loss": 1.9854,
      "step": 137
    },
    {
      "epoch": 0.053426248548199766,
      "grad_norm": 1.2301346063613892,
      "learning_rate": 5.342624854819977e-07,
      "loss": 1.9294,
      "step": 138
    },
    {
      "epoch": 0.05381339527680991,
      "grad_norm": 1.7448948621749878,
      "learning_rate": 5.381339527680991e-07,
      "loss": 1.9606,
      "step": 139
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 1.4438631534576416,
      "learning_rate": 5.420054200542006e-07,
      "loss": 1.9549,
      "step": 140
    },
    {
      "epoch": 0.0545876887340302,
      "grad_norm": 1.266018033027649,
      "learning_rate": 5.45876887340302e-07,
      "loss": 1.9444,
      "step": 141
    },
    {
      "epoch": 0.05497483546264034,
      "grad_norm": 1.7858085632324219,
      "learning_rate": 5.497483546264035e-07,
      "loss": 1.965,
      "step": 142
    },
    {
      "epoch": 0.055361982191250485,
      "grad_norm": 1.4917521476745605,
      "learning_rate": 5.536198219125049e-07,
      "loss": 1.9479,
      "step": 143
    },
    {
      "epoch": 0.05574912891986063,
      "grad_norm": 1.7303102016448975,
      "learning_rate": 5.574912891986063e-07,
      "loss": 1.9638,
      "step": 144
    },
    {
      "epoch": 0.05613627564847077,
      "grad_norm": 1.435689091682434,
      "learning_rate": 5.613627564847078e-07,
      "loss": 1.914,
      "step": 145
    },
    {
      "epoch": 0.05652342237708091,
      "grad_norm": 1.178758978843689,
      "learning_rate": 5.652342237708091e-07,
      "loss": 1.9394,
      "step": 146
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 1.3719794750213623,
      "learning_rate": 5.691056910569106e-07,
      "loss": 1.9192,
      "step": 147
    },
    {
      "epoch": 0.057297715834301204,
      "grad_norm": 1.689508318901062,
      "learning_rate": 5.729771583430121e-07,
      "loss": 1.9621,
      "step": 148
    },
    {
      "epoch": 0.057684862562911346,
      "grad_norm": 1.4846851825714111,
      "learning_rate": 5.768486256291135e-07,
      "loss": 1.9338,
      "step": 149
    },
    {
      "epoch": 0.05807200929152149,
      "grad_norm": 2.1773600578308105,
      "learning_rate": 5.807200929152149e-07,
      "loss": 1.954,
      "step": 150
    },
    {
      "epoch": 0.05845915602013163,
      "grad_norm": 1.3258416652679443,
      "learning_rate": 5.845915602013164e-07,
      "loss": 1.9391,
      "step": 151
    },
    {
      "epoch": 0.05884630274874177,
      "grad_norm": 1.0761454105377197,
      "learning_rate": 5.884630274874178e-07,
      "loss": 1.9304,
      "step": 152
    },
    {
      "epoch": 0.059233449477351915,
      "grad_norm": 1.9608542919158936,
      "learning_rate": 5.923344947735192e-07,
      "loss": 1.9314,
      "step": 153
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 1.4622232913970947,
      "learning_rate": 5.962059620596207e-07,
      "loss": 1.9449,
      "step": 154
    },
    {
      "epoch": 0.0600077429345722,
      "grad_norm": 1.386207103729248,
      "learning_rate": 6.000774293457221e-07,
      "loss": 1.9475,
      "step": 155
    },
    {
      "epoch": 0.06039488966318235,
      "grad_norm": 1.374375343322754,
      "learning_rate": 6.039488966318236e-07,
      "loss": 1.9563,
      "step": 156
    },
    {
      "epoch": 0.06078203639179249,
      "grad_norm": 1.9986577033996582,
      "learning_rate": 6.078203639179249e-07,
      "loss": 1.8984,
      "step": 157
    },
    {
      "epoch": 0.061169183120402634,
      "grad_norm": 1.337410807609558,
      "learning_rate": 6.116918312040264e-07,
      "loss": 1.9413,
      "step": 158
    },
    {
      "epoch": 0.06155632984901278,
      "grad_norm": 1.569469928741455,
      "learning_rate": 6.155632984901278e-07,
      "loss": 1.9382,
      "step": 159
    },
    {
      "epoch": 0.06194347657762292,
      "grad_norm": 1.4656800031661987,
      "learning_rate": 6.194347657762292e-07,
      "loss": 1.9587,
      "step": 160
    },
    {
      "epoch": 0.06233062330623306,
      "grad_norm": 1.5173181295394897,
      "learning_rate": 6.233062330623307e-07,
      "loss": 1.8969,
      "step": 161
    },
    {
      "epoch": 0.0627177700348432,
      "grad_norm": 1.3920223712921143,
      "learning_rate": 6.271777003484321e-07,
      "loss": 1.9535,
      "step": 162
    },
    {
      "epoch": 0.06310491676345335,
      "grad_norm": 1.5692684650421143,
      "learning_rate": 6.310491676345336e-07,
      "loss": 1.964,
      "step": 163
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 1.865328073501587,
      "learning_rate": 6.34920634920635e-07,
      "loss": 1.9307,
      "step": 164
    },
    {
      "epoch": 0.06387921022067364,
      "grad_norm": 1.199167013168335,
      "learning_rate": 6.387921022067365e-07,
      "loss": 1.9431,
      "step": 165
    },
    {
      "epoch": 0.06426635694928377,
      "grad_norm": 1.1244466304779053,
      "learning_rate": 6.426635694928377e-07,
      "loss": 1.961,
      "step": 166
    },
    {
      "epoch": 0.06465350367789392,
      "grad_norm": 1.2851150035858154,
      "learning_rate": 6.465350367789392e-07,
      "loss": 1.9451,
      "step": 167
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 1.5376777648925781,
      "learning_rate": 6.504065040650407e-07,
      "loss": 1.9772,
      "step": 168
    },
    {
      "epoch": 0.06542779713511421,
      "grad_norm": 1.279203176498413,
      "learning_rate": 6.542779713511421e-07,
      "loss": 1.9595,
      "step": 169
    },
    {
      "epoch": 0.06581494386372436,
      "grad_norm": 1.3015878200531006,
      "learning_rate": 6.581494386372436e-07,
      "loss": 1.9246,
      "step": 170
    },
    {
      "epoch": 0.06620209059233449,
      "grad_norm": 1.3563429117202759,
      "learning_rate": 6.62020905923345e-07,
      "loss": 1.9266,
      "step": 171
    },
    {
      "epoch": 0.06658923732094464,
      "grad_norm": 1.6971479654312134,
      "learning_rate": 6.658923732094465e-07,
      "loss": 1.9665,
      "step": 172
    },
    {
      "epoch": 0.06697638404955478,
      "grad_norm": 1.6323630809783936,
      "learning_rate": 6.697638404955478e-07,
      "loss": 1.9093,
      "step": 173
    },
    {
      "epoch": 0.06736353077816493,
      "grad_norm": 2.0553784370422363,
      "learning_rate": 6.736353077816493e-07,
      "loss": 1.9728,
      "step": 174
    },
    {
      "epoch": 0.06775067750677506,
      "grad_norm": 1.456234335899353,
      "learning_rate": 6.775067750677507e-07,
      "loss": 1.9113,
      "step": 175
    },
    {
      "epoch": 0.06813782423538521,
      "grad_norm": 1.291272759437561,
      "learning_rate": 6.813782423538521e-07,
      "loss": 1.9161,
      "step": 176
    },
    {
      "epoch": 0.06852497096399536,
      "grad_norm": 2.527912139892578,
      "learning_rate": 6.852497096399536e-07,
      "loss": 2.01,
      "step": 177
    },
    {
      "epoch": 0.0689121176926055,
      "grad_norm": 1.593970537185669,
      "learning_rate": 6.89121176926055e-07,
      "loss": 1.9579,
      "step": 178
    },
    {
      "epoch": 0.06929926442121565,
      "grad_norm": 1.4862563610076904,
      "learning_rate": 6.929926442121565e-07,
      "loss": 1.9594,
      "step": 179
    },
    {
      "epoch": 0.06968641114982578,
      "grad_norm": 1.3331727981567383,
      "learning_rate": 6.968641114982579e-07,
      "loss": 1.9385,
      "step": 180
    },
    {
      "epoch": 0.07007355787843593,
      "grad_norm": 1.440492868423462,
      "learning_rate": 7.007355787843594e-07,
      "loss": 1.9491,
      "step": 181
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 1.2666289806365967,
      "learning_rate": 7.046070460704607e-07,
      "loss": 1.9182,
      "step": 182
    },
    {
      "epoch": 0.07084785133565621,
      "grad_norm": 2.116395950317383,
      "learning_rate": 7.084785133565622e-07,
      "loss": 1.8837,
      "step": 183
    },
    {
      "epoch": 0.07123499806426636,
      "grad_norm": 1.2691996097564697,
      "learning_rate": 7.123499806426637e-07,
      "loss": 1.9147,
      "step": 184
    },
    {
      "epoch": 0.0716221447928765,
      "grad_norm": 1.5789631605148315,
      "learning_rate": 7.162214479287651e-07,
      "loss": 1.9514,
      "step": 185
    },
    {
      "epoch": 0.07200929152148665,
      "grad_norm": 1.7871671915054321,
      "learning_rate": 7.200929152148666e-07,
      "loss": 1.9124,
      "step": 186
    },
    {
      "epoch": 0.07239643825009678,
      "grad_norm": 1.3387380838394165,
      "learning_rate": 7.239643825009679e-07,
      "loss": 1.9585,
      "step": 187
    },
    {
      "epoch": 0.07278358497870693,
      "grad_norm": 1.8699023723602295,
      "learning_rate": 7.278358497870694e-07,
      "loss": 2.0036,
      "step": 188
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 1.2714886665344238,
      "learning_rate": 7.317073170731707e-07,
      "loss": 1.9415,
      "step": 189
    },
    {
      "epoch": 0.07355787843592722,
      "grad_norm": 1.475311517715454,
      "learning_rate": 7.355787843592722e-07,
      "loss": 1.9552,
      "step": 190
    },
    {
      "epoch": 0.07394502516453735,
      "grad_norm": 2.807035446166992,
      "learning_rate": 7.394502516453736e-07,
      "loss": 2.012,
      "step": 191
    },
    {
      "epoch": 0.0743321718931475,
      "grad_norm": 1.132474422454834,
      "learning_rate": 7.433217189314751e-07,
      "loss": 1.9424,
      "step": 192
    },
    {
      "epoch": 0.07471931862175765,
      "grad_norm": 1.295541524887085,
      "learning_rate": 7.471931862175766e-07,
      "loss": 1.923,
      "step": 193
    },
    {
      "epoch": 0.07510646535036779,
      "grad_norm": 1.4100650548934937,
      "learning_rate": 7.51064653503678e-07,
      "loss": 1.9477,
      "step": 194
    },
    {
      "epoch": 0.07549361207897794,
      "grad_norm": 2.203207492828369,
      "learning_rate": 7.549361207897795e-07,
      "loss": 1.9149,
      "step": 195
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 1.3338545560836792,
      "learning_rate": 7.588075880758807e-07,
      "loss": 1.9155,
      "step": 196
    },
    {
      "epoch": 0.07626790553619822,
      "grad_norm": 1.1864159107208252,
      "learning_rate": 7.626790553619822e-07,
      "loss": 1.9412,
      "step": 197
    },
    {
      "epoch": 0.07665505226480836,
      "grad_norm": 1.3234816789627075,
      "learning_rate": 7.665505226480836e-07,
      "loss": 1.9523,
      "step": 198
    },
    {
      "epoch": 0.0770421989934185,
      "grad_norm": 1.321771502494812,
      "learning_rate": 7.704219899341851e-07,
      "loss": 1.9225,
      "step": 199
    },
    {
      "epoch": 0.07742934572202866,
      "grad_norm": 1.8656835556030273,
      "learning_rate": 7.742934572202866e-07,
      "loss": 1.9862,
      "step": 200
    },
    {
      "epoch": 0.07781649245063879,
      "grad_norm": 1.1836506128311157,
      "learning_rate": 7.78164924506388e-07,
      "loss": 1.9598,
      "step": 201
    },
    {
      "epoch": 0.07820363917924894,
      "grad_norm": 1.8203198909759521,
      "learning_rate": 7.820363917924895e-07,
      "loss": 1.9778,
      "step": 202
    },
    {
      "epoch": 0.07859078590785908,
      "grad_norm": 1.5249191522598267,
      "learning_rate": 7.859078590785908e-07,
      "loss": 1.9573,
      "step": 203
    },
    {
      "epoch": 0.07897793263646923,
      "grad_norm": 3.275322437286377,
      "learning_rate": 7.897793263646923e-07,
      "loss": 2.0267,
      "step": 204
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 1.334331750869751,
      "learning_rate": 7.936507936507937e-07,
      "loss": 1.9509,
      "step": 205
    },
    {
      "epoch": 0.07975222609368951,
      "grad_norm": 1.411104440689087,
      "learning_rate": 7.975222609368952e-07,
      "loss": 1.9031,
      "step": 206
    },
    {
      "epoch": 0.08013937282229965,
      "grad_norm": 1.5724178552627563,
      "learning_rate": 8.013937282229965e-07,
      "loss": 1.9363,
      "step": 207
    },
    {
      "epoch": 0.0805265195509098,
      "grad_norm": 1.317803144454956,
      "learning_rate": 8.05265195509098e-07,
      "loss": 1.8998,
      "step": 208
    },
    {
      "epoch": 0.08091366627951994,
      "grad_norm": 1.4644187688827515,
      "learning_rate": 8.091366627951995e-07,
      "loss": 1.9177,
      "step": 209
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 1.3745898008346558,
      "learning_rate": 8.130081300813009e-07,
      "loss": 1.9465,
      "step": 210
    },
    {
      "epoch": 0.08168795973674023,
      "grad_norm": 1.160555124282837,
      "learning_rate": 8.168795973674024e-07,
      "loss": 1.9458,
      "step": 211
    },
    {
      "epoch": 0.08207510646535036,
      "grad_norm": 1.3809980154037476,
      "learning_rate": 8.207510646535037e-07,
      "loss": 1.9491,
      "step": 212
    },
    {
      "epoch": 0.08246225319396051,
      "grad_norm": 1.1839203834533691,
      "learning_rate": 8.246225319396052e-07,
      "loss": 1.9471,
      "step": 213
    },
    {
      "epoch": 0.08284939992257065,
      "grad_norm": 1.617287516593933,
      "learning_rate": 8.284939992257066e-07,
      "loss": 1.9789,
      "step": 214
    },
    {
      "epoch": 0.0832365466511808,
      "grad_norm": 1.344551920890808,
      "learning_rate": 8.323654665118081e-07,
      "loss": 1.9604,
      "step": 215
    },
    {
      "epoch": 0.08362369337979095,
      "grad_norm": 1.2424730062484741,
      "learning_rate": 8.362369337979096e-07,
      "loss": 1.9451,
      "step": 216
    },
    {
      "epoch": 0.08401084010840108,
      "grad_norm": 2.60916805267334,
      "learning_rate": 8.401084010840109e-07,
      "loss": 2.007,
      "step": 217
    },
    {
      "epoch": 0.08439798683701123,
      "grad_norm": 1.6512129306793213,
      "learning_rate": 8.439798683701124e-07,
      "loss": 1.9516,
      "step": 218
    },
    {
      "epoch": 0.08478513356562137,
      "grad_norm": 1.2789881229400635,
      "learning_rate": 8.478513356562137e-07,
      "loss": 1.9559,
      "step": 219
    },
    {
      "epoch": 0.08517228029423152,
      "grad_norm": 1.200788140296936,
      "learning_rate": 8.517228029423152e-07,
      "loss": 1.9446,
      "step": 220
    },
    {
      "epoch": 0.08555942702284165,
      "grad_norm": 2.3394548892974854,
      "learning_rate": 8.555942702284166e-07,
      "loss": 1.9619,
      "step": 221
    },
    {
      "epoch": 0.0859465737514518,
      "grad_norm": 1.5540934801101685,
      "learning_rate": 8.594657375145181e-07,
      "loss": 1.9772,
      "step": 222
    },
    {
      "epoch": 0.08633372048006194,
      "grad_norm": 1.2303707599639893,
      "learning_rate": 8.633372048006195e-07,
      "loss": 1.9447,
      "step": 223
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 2.3388192653656006,
      "learning_rate": 8.67208672086721e-07,
      "loss": 1.9759,
      "step": 224
    },
    {
      "epoch": 0.08710801393728224,
      "grad_norm": 1.3377221822738647,
      "learning_rate": 8.710801393728225e-07,
      "loss": 1.938,
      "step": 225
    },
    {
      "epoch": 0.08749516066589237,
      "grad_norm": 1.5286308526992798,
      "learning_rate": 8.749516066589237e-07,
      "loss": 1.967,
      "step": 226
    },
    {
      "epoch": 0.08788230739450252,
      "grad_norm": 1.6421114206314087,
      "learning_rate": 8.788230739450252e-07,
      "loss": 1.9575,
      "step": 227
    },
    {
      "epoch": 0.08826945412311266,
      "grad_norm": 1.849982500076294,
      "learning_rate": 8.826945412311266e-07,
      "loss": 1.8846,
      "step": 228
    },
    {
      "epoch": 0.0886566008517228,
      "grad_norm": 1.2569493055343628,
      "learning_rate": 8.865660085172281e-07,
      "loss": 1.9301,
      "step": 229
    },
    {
      "epoch": 0.08904374758033294,
      "grad_norm": 1.2114485502243042,
      "learning_rate": 8.904374758033295e-07,
      "loss": 1.9306,
      "step": 230
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 1.7603652477264404,
      "learning_rate": 8.94308943089431e-07,
      "loss": 1.9647,
      "step": 231
    },
    {
      "epoch": 0.08981804103755324,
      "grad_norm": 2.010676145553589,
      "learning_rate": 8.981804103755325e-07,
      "loss": 1.8844,
      "step": 232
    },
    {
      "epoch": 0.09020518776616337,
      "grad_norm": 2.038668394088745,
      "learning_rate": 9.020518776616338e-07,
      "loss": 1.9139,
      "step": 233
    },
    {
      "epoch": 0.09059233449477352,
      "grad_norm": 1.405318260192871,
      "learning_rate": 9.059233449477353e-07,
      "loss": 1.9412,
      "step": 234
    },
    {
      "epoch": 0.09097948122338366,
      "grad_norm": 1.33861243724823,
      "learning_rate": 9.097948122338367e-07,
      "loss": 1.9402,
      "step": 235
    },
    {
      "epoch": 0.09136662795199381,
      "grad_norm": 1.2831127643585205,
      "learning_rate": 9.136662795199382e-07,
      "loss": 1.9616,
      "step": 236
    },
    {
      "epoch": 0.09175377468060394,
      "grad_norm": 1.9888291358947754,
      "learning_rate": 9.175377468060395e-07,
      "loss": 1.955,
      "step": 237
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 1.2296912670135498,
      "learning_rate": 9.21409214092141e-07,
      "loss": 1.9453,
      "step": 238
    },
    {
      "epoch": 0.09252806813782423,
      "grad_norm": 1.3826355934143066,
      "learning_rate": 9.252806813782423e-07,
      "loss": 1.914,
      "step": 239
    },
    {
      "epoch": 0.09291521486643438,
      "grad_norm": 1.311548113822937,
      "learning_rate": 9.291521486643438e-07,
      "loss": 1.9442,
      "step": 240
    },
    {
      "epoch": 0.09330236159504453,
      "grad_norm": 1.4872076511383057,
      "learning_rate": 9.330236159504453e-07,
      "loss": 1.9136,
      "step": 241
    },
    {
      "epoch": 0.09368950832365466,
      "grad_norm": 1.64584481716156,
      "learning_rate": 9.368950832365467e-07,
      "loss": 1.8918,
      "step": 242
    },
    {
      "epoch": 0.09407665505226481,
      "grad_norm": 1.434674859046936,
      "learning_rate": 9.407665505226482e-07,
      "loss": 1.9021,
      "step": 243
    },
    {
      "epoch": 0.09446380178087495,
      "grad_norm": 1.2531490325927734,
      "learning_rate": 9.446380178087496e-07,
      "loss": 1.9471,
      "step": 244
    },
    {
      "epoch": 0.0948509485094851,
      "grad_norm": 1.3403780460357666,
      "learning_rate": 9.485094850948511e-07,
      "loss": 1.957,
      "step": 245
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 1.2302957773208618,
      "learning_rate": 9.523809523809525e-07,
      "loss": 1.9588,
      "step": 246
    },
    {
      "epoch": 0.09562524196670538,
      "grad_norm": 2.386007308959961,
      "learning_rate": 9.562524196670538e-07,
      "loss": 1.9031,
      "step": 247
    },
    {
      "epoch": 0.09601238869531553,
      "grad_norm": 2.1216280460357666,
      "learning_rate": 9.601238869531553e-07,
      "loss": 1.8761,
      "step": 248
    },
    {
      "epoch": 0.09639953542392567,
      "grad_norm": 1.5414080619812012,
      "learning_rate": 9.639953542392568e-07,
      "loss": 1.9508,
      "step": 249
    },
    {
      "epoch": 0.09678668215253582,
      "grad_norm": 1.1540870666503906,
      "learning_rate": 9.678668215253583e-07,
      "loss": 1.9154,
      "step": 250
    },
    {
      "epoch": 0.09717382888114595,
      "grad_norm": 1.7975988388061523,
      "learning_rate": 9.717382888114596e-07,
      "loss": 1.8962,
      "step": 251
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.2319954633712769,
      "learning_rate": 9.75609756097561e-07,
      "loss": 1.9131,
      "step": 252
    },
    {
      "epoch": 0.09794812233836624,
      "grad_norm": 1.3950631618499756,
      "learning_rate": 9.794812233836624e-07,
      "loss": 1.9454,
      "step": 253
    },
    {
      "epoch": 0.09833526906697639,
      "grad_norm": 1.2658807039260864,
      "learning_rate": 9.833526906697639e-07,
      "loss": 1.9204,
      "step": 254
    },
    {
      "epoch": 0.09872241579558652,
      "grad_norm": 1.4267122745513916,
      "learning_rate": 9.872241579558654e-07,
      "loss": 1.9249,
      "step": 255
    },
    {
      "epoch": 0.09910956252419667,
      "grad_norm": 1.1770508289337158,
      "learning_rate": 9.910956252419669e-07,
      "loss": 1.9455,
      "step": 256
    },
    {
      "epoch": 0.09949670925280682,
      "grad_norm": 1.8326971530914307,
      "learning_rate": 9.949670925280684e-07,
      "loss": 1.9653,
      "step": 257
    },
    {
      "epoch": 0.09988385598141696,
      "grad_norm": 1.2412647008895874,
      "learning_rate": 9.988385598141696e-07,
      "loss": 1.9064,
      "step": 258
    },
    {
      "epoch": 0.1002710027100271,
      "grad_norm": 1.4522029161453247,
      "learning_rate": 1.0027100271002711e-06,
      "loss": 1.9688,
      "step": 259
    },
    {
      "epoch": 0.10065814943863724,
      "grad_norm": 1.323971152305603,
      "learning_rate": 1.0065814943863724e-06,
      "loss": 1.9651,
      "step": 260
    },
    {
      "epoch": 0.10104529616724739,
      "grad_norm": 1.4504698514938354,
      "learning_rate": 1.010452961672474e-06,
      "loss": 1.968,
      "step": 261
    },
    {
      "epoch": 0.10143244289585752,
      "grad_norm": 1.3116633892059326,
      "learning_rate": 1.0143244289585754e-06,
      "loss": 1.9339,
      "step": 262
    },
    {
      "epoch": 0.10181958962446767,
      "grad_norm": 1.3598530292510986,
      "learning_rate": 1.0181958962446769e-06,
      "loss": 1.9549,
      "step": 263
    },
    {
      "epoch": 0.10220673635307782,
      "grad_norm": 1.6924139261245728,
      "learning_rate": 1.0220673635307784e-06,
      "loss": 1.9765,
      "step": 264
    },
    {
      "epoch": 0.10259388308168796,
      "grad_norm": 1.5049668550491333,
      "learning_rate": 1.0259388308168797e-06,
      "loss": 1.9353,
      "step": 265
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 1.8106191158294678,
      "learning_rate": 1.0298102981029812e-06,
      "loss": 1.9957,
      "step": 266
    },
    {
      "epoch": 0.10336817653890824,
      "grad_norm": 1.6184200048446655,
      "learning_rate": 1.0336817653890824e-06,
      "loss": 1.9578,
      "step": 267
    },
    {
      "epoch": 0.10375532326751839,
      "grad_norm": 1.5791020393371582,
      "learning_rate": 1.037553232675184e-06,
      "loss": 1.9382,
      "step": 268
    },
    {
      "epoch": 0.10414246999612853,
      "grad_norm": 2.790430784225464,
      "learning_rate": 1.0414246999612854e-06,
      "loss": 2.014,
      "step": 269
    },
    {
      "epoch": 0.10452961672473868,
      "grad_norm": 1.7857729196548462,
      "learning_rate": 1.045296167247387e-06,
      "loss": 1.9737,
      "step": 270
    },
    {
      "epoch": 0.10491676345334881,
      "grad_norm": 1.3903334140777588,
      "learning_rate": 1.0491676345334882e-06,
      "loss": 1.9677,
      "step": 271
    },
    {
      "epoch": 0.10530391018195896,
      "grad_norm": 1.6178395748138428,
      "learning_rate": 1.0530391018195897e-06,
      "loss": 1.9698,
      "step": 272
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 1.5775846242904663,
      "learning_rate": 1.0569105691056912e-06,
      "loss": 1.9566,
      "step": 273
    },
    {
      "epoch": 0.10607820363917925,
      "grad_norm": 1.633928894996643,
      "learning_rate": 1.0607820363917925e-06,
      "loss": 1.9469,
      "step": 274
    },
    {
      "epoch": 0.1064653503677894,
      "grad_norm": 1.3794456720352173,
      "learning_rate": 1.064653503677894e-06,
      "loss": 1.9286,
      "step": 275
    },
    {
      "epoch": 0.10685249709639953,
      "grad_norm": 1.7416026592254639,
      "learning_rate": 1.0685249709639955e-06,
      "loss": 1.9669,
      "step": 276
    },
    {
      "epoch": 0.10723964382500968,
      "grad_norm": 1.34681236743927,
      "learning_rate": 1.072396438250097e-06,
      "loss": 1.9763,
      "step": 277
    },
    {
      "epoch": 0.10762679055361982,
      "grad_norm": 1.1881003379821777,
      "learning_rate": 1.0762679055361982e-06,
      "loss": 1.9483,
      "step": 278
    },
    {
      "epoch": 0.10801393728222997,
      "grad_norm": 1.3797640800476074,
      "learning_rate": 1.0801393728222997e-06,
      "loss": 1.9368,
      "step": 279
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 1.714133620262146,
      "learning_rate": 1.0840108401084012e-06,
      "loss": 1.959,
      "step": 280
    },
    {
      "epoch": 0.10878823073945025,
      "grad_norm": 1.6516321897506714,
      "learning_rate": 1.0878823073945025e-06,
      "loss": 1.9496,
      "step": 281
    },
    {
      "epoch": 0.1091753774680604,
      "grad_norm": 1.2354130744934082,
      "learning_rate": 1.091753774680604e-06,
      "loss": 1.911,
      "step": 282
    },
    {
      "epoch": 0.10956252419667054,
      "grad_norm": 1.9756921529769897,
      "learning_rate": 1.0956252419667055e-06,
      "loss": 1.9926,
      "step": 283
    },
    {
      "epoch": 0.10994967092528068,
      "grad_norm": 1.754900574684143,
      "learning_rate": 1.099496709252807e-06,
      "loss": 1.9661,
      "step": 284
    },
    {
      "epoch": 0.11033681765389082,
      "grad_norm": 1.5594189167022705,
      "learning_rate": 1.1033681765389083e-06,
      "loss": 1.9244,
      "step": 285
    },
    {
      "epoch": 0.11072396438250097,
      "grad_norm": 1.324192762374878,
      "learning_rate": 1.1072396438250098e-06,
      "loss": 1.9389,
      "step": 286
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.5862911939620972,
      "learning_rate": 1.111111111111111e-06,
      "loss": 1.9498,
      "step": 287
    },
    {
      "epoch": 0.11149825783972125,
      "grad_norm": 1.4328017234802246,
      "learning_rate": 1.1149825783972125e-06,
      "loss": 1.9242,
      "step": 288
    },
    {
      "epoch": 0.1118854045683314,
      "grad_norm": 1.3288697004318237,
      "learning_rate": 1.118854045683314e-06,
      "loss": 1.9258,
      "step": 289
    },
    {
      "epoch": 0.11227255129694154,
      "grad_norm": 1.9579726457595825,
      "learning_rate": 1.1227255129694155e-06,
      "loss": 1.9294,
      "step": 290
    },
    {
      "epoch": 0.11265969802555169,
      "grad_norm": 1.5679636001586914,
      "learning_rate": 1.126596980255517e-06,
      "loss": 1.9553,
      "step": 291
    },
    {
      "epoch": 0.11304684475416182,
      "grad_norm": 1.6586045026779175,
      "learning_rate": 1.1304684475416183e-06,
      "loss": 1.9197,
      "step": 292
    },
    {
      "epoch": 0.11343399148277197,
      "grad_norm": 1.574398398399353,
      "learning_rate": 1.1343399148277198e-06,
      "loss": 1.9067,
      "step": 293
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 1.2593274116516113,
      "learning_rate": 1.1382113821138213e-06,
      "loss": 1.9165,
      "step": 294
    },
    {
      "epoch": 0.11420828493999226,
      "grad_norm": 1.6787760257720947,
      "learning_rate": 1.1420828493999228e-06,
      "loss": 1.9367,
      "step": 295
    },
    {
      "epoch": 0.11459543166860241,
      "grad_norm": 1.301345705986023,
      "learning_rate": 1.1459543166860243e-06,
      "loss": 1.931,
      "step": 296
    },
    {
      "epoch": 0.11498257839721254,
      "grad_norm": 1.7856166362762451,
      "learning_rate": 1.1498257839721255e-06,
      "loss": 1.9596,
      "step": 297
    },
    {
      "epoch": 0.11536972512582269,
      "grad_norm": 2.10019588470459,
      "learning_rate": 1.153697251258227e-06,
      "loss": 1.9118,
      "step": 298
    },
    {
      "epoch": 0.11575687185443283,
      "grad_norm": 1.2561169862747192,
      "learning_rate": 1.1575687185443283e-06,
      "loss": 1.9156,
      "step": 299
    },
    {
      "epoch": 0.11614401858304298,
      "grad_norm": 1.9176281690597534,
      "learning_rate": 1.1614401858304298e-06,
      "loss": 2.0002,
      "step": 300
    },
    {
      "epoch": 0.11653116531165311,
      "grad_norm": 1.355483889579773,
      "learning_rate": 1.1653116531165313e-06,
      "loss": 1.9141,
      "step": 301
    },
    {
      "epoch": 0.11691831204026326,
      "grad_norm": 1.5007543563842773,
      "learning_rate": 1.1691831204026328e-06,
      "loss": 1.9266,
      "step": 302
    },
    {
      "epoch": 0.1173054587688734,
      "grad_norm": 1.4830043315887451,
      "learning_rate": 1.173054587688734e-06,
      "loss": 1.9098,
      "step": 303
    },
    {
      "epoch": 0.11769260549748355,
      "grad_norm": 1.8125677108764648,
      "learning_rate": 1.1769260549748356e-06,
      "loss": 1.9666,
      "step": 304
    },
    {
      "epoch": 0.1180797522260937,
      "grad_norm": 1.7718329429626465,
      "learning_rate": 1.180797522260937e-06,
      "loss": 1.9011,
      "step": 305
    },
    {
      "epoch": 0.11846689895470383,
      "grad_norm": 1.7778793573379517,
      "learning_rate": 1.1846689895470384e-06,
      "loss": 1.9549,
      "step": 306
    },
    {
      "epoch": 0.11885404568331398,
      "grad_norm": 1.3464010953903198,
      "learning_rate": 1.1885404568331398e-06,
      "loss": 1.9262,
      "step": 307
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 1.5550071001052856,
      "learning_rate": 1.1924119241192413e-06,
      "loss": 1.9432,
      "step": 308
    },
    {
      "epoch": 0.11962833914053426,
      "grad_norm": 1.3861435651779175,
      "learning_rate": 1.1962833914053428e-06,
      "loss": 1.9498,
      "step": 309
    },
    {
      "epoch": 0.1200154858691444,
      "grad_norm": 1.558093547821045,
      "learning_rate": 1.2001548586914441e-06,
      "loss": 1.9309,
      "step": 310
    },
    {
      "epoch": 0.12040263259775455,
      "grad_norm": 1.7558469772338867,
      "learning_rate": 1.2040263259775456e-06,
      "loss": 1.9673,
      "step": 311
    },
    {
      "epoch": 0.1207897793263647,
      "grad_norm": 1.3375784158706665,
      "learning_rate": 1.207897793263647e-06,
      "loss": 1.9214,
      "step": 312
    },
    {
      "epoch": 0.12117692605497483,
      "grad_norm": 1.4449307918548584,
      "learning_rate": 1.2117692605497484e-06,
      "loss": 1.9161,
      "step": 313
    },
    {
      "epoch": 0.12156407278358498,
      "grad_norm": 2.441638708114624,
      "learning_rate": 1.2156407278358499e-06,
      "loss": 1.9059,
      "step": 314
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 1.353731632232666,
      "learning_rate": 1.2195121951219514e-06,
      "loss": 1.9085,
      "step": 315
    },
    {
      "epoch": 0.12233836624080527,
      "grad_norm": 3.030303716659546,
      "learning_rate": 1.2233836624080529e-06,
      "loss": 1.994,
      "step": 316
    },
    {
      "epoch": 0.1227255129694154,
      "grad_norm": 1.4710289239883423,
      "learning_rate": 1.2272551296941541e-06,
      "loss": 1.9479,
      "step": 317
    },
    {
      "epoch": 0.12311265969802555,
      "grad_norm": 2.2766623497009277,
      "learning_rate": 1.2311265969802556e-06,
      "loss": 1.9691,
      "step": 318
    },
    {
      "epoch": 0.12349980642663569,
      "grad_norm": 1.7645599842071533,
      "learning_rate": 1.234998064266357e-06,
      "loss": 1.9268,
      "step": 319
    },
    {
      "epoch": 0.12388695315524584,
      "grad_norm": 1.4728820323944092,
      "learning_rate": 1.2388695315524584e-06,
      "loss": 1.9227,
      "step": 320
    },
    {
      "epoch": 0.12427409988385599,
      "grad_norm": 1.4844386577606201,
      "learning_rate": 1.24274099883856e-06,
      "loss": 1.9095,
      "step": 321
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 1.8163331747055054,
      "learning_rate": 1.2466124661246614e-06,
      "loss": 1.9141,
      "step": 322
    },
    {
      "epoch": 0.12504839334107626,
      "grad_norm": 1.5501534938812256,
      "learning_rate": 1.2504839334107627e-06,
      "loss": 1.9188,
      "step": 323
    },
    {
      "epoch": 0.1254355400696864,
      "grad_norm": 1.6929230690002441,
      "learning_rate": 1.2543554006968642e-06,
      "loss": 1.9111,
      "step": 324
    },
    {
      "epoch": 0.12582268679829656,
      "grad_norm": 1.8505001068115234,
      "learning_rate": 1.2582268679829657e-06,
      "loss": 1.9889,
      "step": 325
    },
    {
      "epoch": 0.1262098335269067,
      "grad_norm": 1.5359790325164795,
      "learning_rate": 1.2620983352690672e-06,
      "loss": 1.9704,
      "step": 326
    },
    {
      "epoch": 0.12659698025551683,
      "grad_norm": 2.2799971103668213,
      "learning_rate": 1.2659698025551684e-06,
      "loss": 1.9299,
      "step": 327
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 1.4610644578933716,
      "learning_rate": 1.26984126984127e-06,
      "loss": 1.9008,
      "step": 328
    },
    {
      "epoch": 0.12737127371273713,
      "grad_norm": 1.7973026037216187,
      "learning_rate": 1.2737127371273714e-06,
      "loss": 1.9837,
      "step": 329
    },
    {
      "epoch": 0.12775842044134728,
      "grad_norm": 1.6464828252792358,
      "learning_rate": 1.277584204413473e-06,
      "loss": 1.9364,
      "step": 330
    },
    {
      "epoch": 0.12814556716995743,
      "grad_norm": 1.63655686378479,
      "learning_rate": 1.2814556716995744e-06,
      "loss": 1.9066,
      "step": 331
    },
    {
      "epoch": 0.12853271389856755,
      "grad_norm": 1.439244270324707,
      "learning_rate": 1.2853271389856755e-06,
      "loss": 1.9623,
      "step": 332
    },
    {
      "epoch": 0.1289198606271777,
      "grad_norm": 1.2896493673324585,
      "learning_rate": 1.289198606271777e-06,
      "loss": 1.9274,
      "step": 333
    },
    {
      "epoch": 0.12930700735578785,
      "grad_norm": 1.627769947052002,
      "learning_rate": 1.2930700735578785e-06,
      "loss": 1.9523,
      "step": 334
    },
    {
      "epoch": 0.129694154084398,
      "grad_norm": 1.3762844800949097,
      "learning_rate": 1.29694154084398e-06,
      "loss": 1.9705,
      "step": 335
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 2.214961051940918,
      "learning_rate": 1.3008130081300815e-06,
      "loss": 1.9149,
      "step": 336
    },
    {
      "epoch": 0.13046844754161827,
      "grad_norm": 1.923017144203186,
      "learning_rate": 1.3046844754161827e-06,
      "loss": 1.9748,
      "step": 337
    },
    {
      "epoch": 0.13085559427022841,
      "grad_norm": 1.5595788955688477,
      "learning_rate": 1.3085559427022842e-06,
      "loss": 1.9196,
      "step": 338
    },
    {
      "epoch": 0.13124274099883856,
      "grad_norm": 1.6769872903823853,
      "learning_rate": 1.3124274099883857e-06,
      "loss": 1.9044,
      "step": 339
    },
    {
      "epoch": 0.1316298877274487,
      "grad_norm": 1.5886876583099365,
      "learning_rate": 1.3162988772744872e-06,
      "loss": 1.9429,
      "step": 340
    },
    {
      "epoch": 0.13201703445605883,
      "grad_norm": 1.8425449132919312,
      "learning_rate": 1.3201703445605885e-06,
      "loss": 1.9779,
      "step": 341
    },
    {
      "epoch": 0.13240418118466898,
      "grad_norm": 2.574467897415161,
      "learning_rate": 1.32404181184669e-06,
      "loss": 1.9092,
      "step": 342
    },
    {
      "epoch": 0.13279132791327913,
      "grad_norm": 1.966910719871521,
      "learning_rate": 1.3279132791327915e-06,
      "loss": 1.8938,
      "step": 343
    },
    {
      "epoch": 0.13317847464188928,
      "grad_norm": 1.4091365337371826,
      "learning_rate": 1.331784746418893e-06,
      "loss": 1.9338,
      "step": 344
    },
    {
      "epoch": 0.13356562137049943,
      "grad_norm": 1.697117805480957,
      "learning_rate": 1.3356562137049945e-06,
      "loss": 1.9069,
      "step": 345
    },
    {
      "epoch": 0.13395276809910955,
      "grad_norm": 1.5179247856140137,
      "learning_rate": 1.3395276809910955e-06,
      "loss": 1.9147,
      "step": 346
    },
    {
      "epoch": 0.1343399148277197,
      "grad_norm": 2.0698840618133545,
      "learning_rate": 1.343399148277197e-06,
      "loss": 1.9749,
      "step": 347
    },
    {
      "epoch": 0.13472706155632985,
      "grad_norm": 1.9285082817077637,
      "learning_rate": 1.3472706155632985e-06,
      "loss": 1.8833,
      "step": 348
    },
    {
      "epoch": 0.13511420828494,
      "grad_norm": 1.9182261228561401,
      "learning_rate": 1.3511420828494e-06,
      "loss": 1.8858,
      "step": 349
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 1.635923981666565,
      "learning_rate": 1.3550135501355013e-06,
      "loss": 1.9989,
      "step": 350
    },
    {
      "epoch": 0.13588850174216027,
      "grad_norm": 2.0719873905181885,
      "learning_rate": 1.3588850174216028e-06,
      "loss": 1.9787,
      "step": 351
    },
    {
      "epoch": 0.13627564847077042,
      "grad_norm": 1.3286869525909424,
      "learning_rate": 1.3627564847077043e-06,
      "loss": 1.9266,
      "step": 352
    },
    {
      "epoch": 0.13666279519938057,
      "grad_norm": 1.9464876651763916,
      "learning_rate": 1.3666279519938058e-06,
      "loss": 1.9644,
      "step": 353
    },
    {
      "epoch": 0.13704994192799072,
      "grad_norm": 1.4783718585968018,
      "learning_rate": 1.3704994192799073e-06,
      "loss": 1.9031,
      "step": 354
    },
    {
      "epoch": 0.13743708865660084,
      "grad_norm": 1.8775720596313477,
      "learning_rate": 1.3743708865660086e-06,
      "loss": 1.9766,
      "step": 355
    },
    {
      "epoch": 0.137824235385211,
      "grad_norm": 1.3642946481704712,
      "learning_rate": 1.37824235385211e-06,
      "loss": 1.9622,
      "step": 356
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 1.9857633113861084,
      "learning_rate": 1.3821138211382116e-06,
      "loss": 1.9673,
      "step": 357
    },
    {
      "epoch": 0.1385985288424313,
      "grad_norm": 2.044436454772949,
      "learning_rate": 1.385985288424313e-06,
      "loss": 1.9704,
      "step": 358
    },
    {
      "epoch": 0.1389856755710414,
      "grad_norm": 1.88657546043396,
      "learning_rate": 1.3898567557104143e-06,
      "loss": 1.9593,
      "step": 359
    },
    {
      "epoch": 0.13937282229965156,
      "grad_norm": 1.629560112953186,
      "learning_rate": 1.3937282229965158e-06,
      "loss": 1.9291,
      "step": 360
    },
    {
      "epoch": 0.1397599690282617,
      "grad_norm": 2.096426010131836,
      "learning_rate": 1.3975996902826173e-06,
      "loss": 1.8883,
      "step": 361
    },
    {
      "epoch": 0.14014711575687186,
      "grad_norm": 1.6367053985595703,
      "learning_rate": 1.4014711575687188e-06,
      "loss": 1.9506,
      "step": 362
    },
    {
      "epoch": 0.140534262485482,
      "grad_norm": 1.7556641101837158,
      "learning_rate": 1.4053426248548203e-06,
      "loss": 1.882,
      "step": 363
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 2.1869797706604004,
      "learning_rate": 1.4092140921409214e-06,
      "loss": 1.9229,
      "step": 364
    },
    {
      "epoch": 0.14130855594270228,
      "grad_norm": 1.9259355068206787,
      "learning_rate": 1.4130855594270229e-06,
      "loss": 1.8881,
      "step": 365
    },
    {
      "epoch": 0.14169570267131243,
      "grad_norm": 1.702422857284546,
      "learning_rate": 1.4169570267131244e-06,
      "loss": 1.8975,
      "step": 366
    },
    {
      "epoch": 0.14208284939992258,
      "grad_norm": 1.7902804613113403,
      "learning_rate": 1.4208284939992259e-06,
      "loss": 1.9469,
      "step": 367
    },
    {
      "epoch": 0.14246999612853273,
      "grad_norm": 1.5810487270355225,
      "learning_rate": 1.4246999612853273e-06,
      "loss": 1.9212,
      "step": 368
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.1189210414886475,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 1.9797,
      "step": 369
    },
    {
      "epoch": 0.143244289585753,
      "grad_norm": 1.8121310472488403,
      "learning_rate": 1.4324428958575301e-06,
      "loss": 1.9687,
      "step": 370
    },
    {
      "epoch": 0.14363143631436315,
      "grad_norm": 1.6179672479629517,
      "learning_rate": 1.4363143631436316e-06,
      "loss": 1.9583,
      "step": 371
    },
    {
      "epoch": 0.1440185830429733,
      "grad_norm": 1.5939662456512451,
      "learning_rate": 1.4401858304297331e-06,
      "loss": 1.8932,
      "step": 372
    },
    {
      "epoch": 0.14440572977158342,
      "grad_norm": 1.6596579551696777,
      "learning_rate": 1.4440572977158344e-06,
      "loss": 1.9112,
      "step": 373
    },
    {
      "epoch": 0.14479287650019357,
      "grad_norm": 2.0239057540893555,
      "learning_rate": 1.4479287650019359e-06,
      "loss": 1.9901,
      "step": 374
    },
    {
      "epoch": 0.14518002322880372,
      "grad_norm": 1.7644284963607788,
      "learning_rate": 1.4518002322880374e-06,
      "loss": 1.9437,
      "step": 375
    },
    {
      "epoch": 0.14556716995741387,
      "grad_norm": 1.8518065214157104,
      "learning_rate": 1.4556716995741389e-06,
      "loss": 1.9919,
      "step": 376
    },
    {
      "epoch": 0.14595431668602402,
      "grad_norm": 1.8030755519866943,
      "learning_rate": 1.4595431668602404e-06,
      "loss": 1.9337,
      "step": 377
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 1.7847497463226318,
      "learning_rate": 1.4634146341463414e-06,
      "loss": 1.958,
      "step": 378
    },
    {
      "epoch": 0.1467286101432443,
      "grad_norm": 1.6623131036758423,
      "learning_rate": 1.467286101432443e-06,
      "loss": 1.946,
      "step": 379
    },
    {
      "epoch": 0.14711575687185444,
      "grad_norm": 1.6354752779006958,
      "learning_rate": 1.4711575687185444e-06,
      "loss": 1.8925,
      "step": 380
    },
    {
      "epoch": 0.14750290360046459,
      "grad_norm": 2.0448474884033203,
      "learning_rate": 1.475029036004646e-06,
      "loss": 1.9933,
      "step": 381
    },
    {
      "epoch": 0.1478900503290747,
      "grad_norm": 1.1962838172912598,
      "learning_rate": 1.4789005032907472e-06,
      "loss": 1.9323,
      "step": 382
    },
    {
      "epoch": 0.14827719705768486,
      "grad_norm": 1.7342759370803833,
      "learning_rate": 1.4827719705768487e-06,
      "loss": 1.9459,
      "step": 383
    },
    {
      "epoch": 0.148664343786295,
      "grad_norm": 2.4976603984832764,
      "learning_rate": 1.4866434378629502e-06,
      "loss": 1.9026,
      "step": 384
    },
    {
      "epoch": 0.14905149051490515,
      "grad_norm": 2.0613508224487305,
      "learning_rate": 1.4905149051490517e-06,
      "loss": 1.9633,
      "step": 385
    },
    {
      "epoch": 0.1494386372435153,
      "grad_norm": 1.9321515560150146,
      "learning_rate": 1.4943863724351532e-06,
      "loss": 1.9258,
      "step": 386
    },
    {
      "epoch": 0.14982578397212543,
      "grad_norm": 1.7624300718307495,
      "learning_rate": 1.4982578397212545e-06,
      "loss": 1.9071,
      "step": 387
    },
    {
      "epoch": 0.15021293070073558,
      "grad_norm": 1.7151979207992554,
      "learning_rate": 1.502129307007356e-06,
      "loss": 1.9735,
      "step": 388
    },
    {
      "epoch": 0.15060007742934572,
      "grad_norm": 1.619200587272644,
      "learning_rate": 1.5060007742934574e-06,
      "loss": 1.8988,
      "step": 389
    },
    {
      "epoch": 0.15098722415795587,
      "grad_norm": 1.7529816627502441,
      "learning_rate": 1.509872241579559e-06,
      "loss": 1.8931,
      "step": 390
    },
    {
      "epoch": 0.151374370886566,
      "grad_norm": 1.6528289318084717,
      "learning_rate": 1.51374370886566e-06,
      "loss": 1.9409,
      "step": 391
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 1.6155864000320435,
      "learning_rate": 1.5176151761517615e-06,
      "loss": 1.9482,
      "step": 392
    },
    {
      "epoch": 0.1521486643437863,
      "grad_norm": 2.1943047046661377,
      "learning_rate": 1.521486643437863e-06,
      "loss": 1.9432,
      "step": 393
    },
    {
      "epoch": 0.15253581107239644,
      "grad_norm": 2.2073628902435303,
      "learning_rate": 1.5253581107239645e-06,
      "loss": 1.9736,
      "step": 394
    },
    {
      "epoch": 0.1529229578010066,
      "grad_norm": 2.9721994400024414,
      "learning_rate": 1.529229578010066e-06,
      "loss": 1.8918,
      "step": 395
    },
    {
      "epoch": 0.15331010452961671,
      "grad_norm": 2.8253636360168457,
      "learning_rate": 1.5331010452961673e-06,
      "loss": 1.9,
      "step": 396
    },
    {
      "epoch": 0.15369725125822686,
      "grad_norm": 1.879274606704712,
      "learning_rate": 1.5369725125822687e-06,
      "loss": 1.8827,
      "step": 397
    },
    {
      "epoch": 0.154084397986837,
      "grad_norm": 1.8416578769683838,
      "learning_rate": 1.5408439798683702e-06,
      "loss": 1.9564,
      "step": 398
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 2.139995813369751,
      "learning_rate": 1.5447154471544717e-06,
      "loss": 1.948,
      "step": 399
    },
    {
      "epoch": 0.1548586914440573,
      "grad_norm": 1.5404483079910278,
      "learning_rate": 1.5485869144405732e-06,
      "loss": 1.8985,
      "step": 400
    },
    {
      "epoch": 0.15524583817266743,
      "grad_norm": 1.5433573722839355,
      "learning_rate": 1.5524583817266745e-06,
      "loss": 1.887,
      "step": 401
    },
    {
      "epoch": 0.15563298490127758,
      "grad_norm": 1.6600062847137451,
      "learning_rate": 1.556329849012776e-06,
      "loss": 1.8934,
      "step": 402
    },
    {
      "epoch": 0.15602013162988773,
      "grad_norm": 1.8858287334442139,
      "learning_rate": 1.5602013162988775e-06,
      "loss": 1.9519,
      "step": 403
    },
    {
      "epoch": 0.15640727835849788,
      "grad_norm": 3.4226059913635254,
      "learning_rate": 1.564072783584979e-06,
      "loss": 2.0058,
      "step": 404
    },
    {
      "epoch": 0.156794425087108,
      "grad_norm": 1.5807850360870361,
      "learning_rate": 1.56794425087108e-06,
      "loss": 1.9268,
      "step": 405
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 1.7989859580993652,
      "learning_rate": 1.5718157181571816e-06,
      "loss": 1.9184,
      "step": 406
    },
    {
      "epoch": 0.1575687185443283,
      "grad_norm": 1.710532546043396,
      "learning_rate": 1.575687185443283e-06,
      "loss": 1.9577,
      "step": 407
    },
    {
      "epoch": 0.15795586527293845,
      "grad_norm": 2.220198154449463,
      "learning_rate": 1.5795586527293845e-06,
      "loss": 1.9777,
      "step": 408
    },
    {
      "epoch": 0.1583430120015486,
      "grad_norm": 1.4623024463653564,
      "learning_rate": 1.583430120015486e-06,
      "loss": 1.9544,
      "step": 409
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 1.648970365524292,
      "learning_rate": 1.5873015873015873e-06,
      "loss": 1.8919,
      "step": 410
    },
    {
      "epoch": 0.15911730545876887,
      "grad_norm": 3.689943552017212,
      "learning_rate": 1.5911730545876888e-06,
      "loss": 2.0226,
      "step": 411
    },
    {
      "epoch": 0.15950445218737902,
      "grad_norm": 1.7296627759933472,
      "learning_rate": 1.5950445218737903e-06,
      "loss": 1.9595,
      "step": 412
    },
    {
      "epoch": 0.15989159891598917,
      "grad_norm": 1.6155670881271362,
      "learning_rate": 1.5989159891598918e-06,
      "loss": 1.9146,
      "step": 413
    },
    {
      "epoch": 0.1602787456445993,
      "grad_norm": 1.8420119285583496,
      "learning_rate": 1.602787456445993e-06,
      "loss": 1.8759,
      "step": 414
    },
    {
      "epoch": 0.16066589237320944,
      "grad_norm": 2.0066492557525635,
      "learning_rate": 1.6066589237320946e-06,
      "loss": 1.9496,
      "step": 415
    },
    {
      "epoch": 0.1610530391018196,
      "grad_norm": 1.871625304222107,
      "learning_rate": 1.610530391018196e-06,
      "loss": 1.9185,
      "step": 416
    },
    {
      "epoch": 0.16144018583042974,
      "grad_norm": 1.4670833349227905,
      "learning_rate": 1.6144018583042976e-06,
      "loss": 1.9508,
      "step": 417
    },
    {
      "epoch": 0.1618273325590399,
      "grad_norm": 3.789684772491455,
      "learning_rate": 1.618273325590399e-06,
      "loss": 2.0372,
      "step": 418
    },
    {
      "epoch": 0.16221447928765,
      "grad_norm": 2.0267770290374756,
      "learning_rate": 1.6221447928765003e-06,
      "loss": 1.9542,
      "step": 419
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 2.1309025287628174,
      "learning_rate": 1.6260162601626018e-06,
      "loss": 1.93,
      "step": 420
    },
    {
      "epoch": 0.1629887727448703,
      "grad_norm": 1.7066789865493774,
      "learning_rate": 1.6298877274487033e-06,
      "loss": 1.9887,
      "step": 421
    },
    {
      "epoch": 0.16337591947348046,
      "grad_norm": 1.6124634742736816,
      "learning_rate": 1.6337591947348048e-06,
      "loss": 1.9384,
      "step": 422
    },
    {
      "epoch": 0.16376306620209058,
      "grad_norm": 2.008396863937378,
      "learning_rate": 1.6376306620209059e-06,
      "loss": 1.9237,
      "step": 423
    },
    {
      "epoch": 0.16415021293070073,
      "grad_norm": 3.0317580699920654,
      "learning_rate": 1.6415021293070074e-06,
      "loss": 1.9999,
      "step": 424
    },
    {
      "epoch": 0.16453735965931088,
      "grad_norm": 1.720217227935791,
      "learning_rate": 1.6453735965931089e-06,
      "loss": 1.8905,
      "step": 425
    },
    {
      "epoch": 0.16492450638792103,
      "grad_norm": 1.7010760307312012,
      "learning_rate": 1.6492450638792104e-06,
      "loss": 1.8893,
      "step": 426
    },
    {
      "epoch": 0.16531165311653118,
      "grad_norm": 1.8502633571624756,
      "learning_rate": 1.6531165311653119e-06,
      "loss": 1.8761,
      "step": 427
    },
    {
      "epoch": 0.1656987998451413,
      "grad_norm": 1.9518791437149048,
      "learning_rate": 1.6569879984514131e-06,
      "loss": 1.9272,
      "step": 428
    },
    {
      "epoch": 0.16608594657375145,
      "grad_norm": 1.5597469806671143,
      "learning_rate": 1.6608594657375146e-06,
      "loss": 1.9212,
      "step": 429
    },
    {
      "epoch": 0.1664730933023616,
      "grad_norm": 1.6594756841659546,
      "learning_rate": 1.6647309330236161e-06,
      "loss": 1.9082,
      "step": 430
    },
    {
      "epoch": 0.16686024003097175,
      "grad_norm": 2.568570137023926,
      "learning_rate": 1.6686024003097176e-06,
      "loss": 1.9329,
      "step": 431
    },
    {
      "epoch": 0.1672473867595819,
      "grad_norm": 1.8203858137130737,
      "learning_rate": 1.6724738675958191e-06,
      "loss": 1.977,
      "step": 432
    },
    {
      "epoch": 0.16763453348819202,
      "grad_norm": 1.8662112951278687,
      "learning_rate": 1.6763453348819204e-06,
      "loss": 1.9073,
      "step": 433
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 2.7789127826690674,
      "learning_rate": 1.6802168021680219e-06,
      "loss": 1.9687,
      "step": 434
    },
    {
      "epoch": 0.16840882694541232,
      "grad_norm": 1.8848450183868408,
      "learning_rate": 1.6840882694541234e-06,
      "loss": 1.9468,
      "step": 435
    },
    {
      "epoch": 0.16879597367402246,
      "grad_norm": 1.9737122058868408,
      "learning_rate": 1.6879597367402249e-06,
      "loss": 1.9407,
      "step": 436
    },
    {
      "epoch": 0.1691831204026326,
      "grad_norm": 1.8369290828704834,
      "learning_rate": 1.691831204026326e-06,
      "loss": 1.8792,
      "step": 437
    },
    {
      "epoch": 0.16957026713124274,
      "grad_norm": 2.665700912475586,
      "learning_rate": 1.6957026713124274e-06,
      "loss": 1.898,
      "step": 438
    },
    {
      "epoch": 0.16995741385985288,
      "grad_norm": 2.0381131172180176,
      "learning_rate": 1.699574138598529e-06,
      "loss": 1.9852,
      "step": 439
    },
    {
      "epoch": 0.17034456058846303,
      "grad_norm": 2.0582683086395264,
      "learning_rate": 1.7034456058846304e-06,
      "loss": 1.8832,
      "step": 440
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 2.215968370437622,
      "learning_rate": 1.707317073170732e-06,
      "loss": 1.9686,
      "step": 441
    },
    {
      "epoch": 0.1711188540456833,
      "grad_norm": 2.136892318725586,
      "learning_rate": 1.7111885404568332e-06,
      "loss": 1.9417,
      "step": 442
    },
    {
      "epoch": 0.17150600077429345,
      "grad_norm": 1.8776793479919434,
      "learning_rate": 1.7150600077429347e-06,
      "loss": 1.8849,
      "step": 443
    },
    {
      "epoch": 0.1718931475029036,
      "grad_norm": 2.1167948246002197,
      "learning_rate": 1.7189314750290362e-06,
      "loss": 1.9597,
      "step": 444
    },
    {
      "epoch": 0.17228029423151375,
      "grad_norm": 1.548703908920288,
      "learning_rate": 1.7228029423151377e-06,
      "loss": 1.9166,
      "step": 445
    },
    {
      "epoch": 0.17266744096012387,
      "grad_norm": 1.735309362411499,
      "learning_rate": 1.726674409601239e-06,
      "loss": 1.9208,
      "step": 446
    },
    {
      "epoch": 0.17305458768873402,
      "grad_norm": 2.2364118099212646,
      "learning_rate": 1.7305458768873405e-06,
      "loss": 1.9475,
      "step": 447
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 1.9087903499603271,
      "learning_rate": 1.734417344173442e-06,
      "loss": 1.9559,
      "step": 448
    },
    {
      "epoch": 0.17382888114595432,
      "grad_norm": 1.683915376663208,
      "learning_rate": 1.7382888114595434e-06,
      "loss": 1.8875,
      "step": 449
    },
    {
      "epoch": 0.17421602787456447,
      "grad_norm": 1.8590646982192993,
      "learning_rate": 1.742160278745645e-06,
      "loss": 1.9165,
      "step": 450
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 1.775416374206543,
      "learning_rate": 1.746031746031746e-06,
      "loss": 1.9376,
      "step": 451
    },
    {
      "epoch": 0.17499032133178474,
      "grad_norm": 1.5585452318191528,
      "learning_rate": 1.7499032133178475e-06,
      "loss": 1.899,
      "step": 452
    },
    {
      "epoch": 0.1753774680603949,
      "grad_norm": 1.9003162384033203,
      "learning_rate": 1.753774680603949e-06,
      "loss": 1.9676,
      "step": 453
    },
    {
      "epoch": 0.17576461478900504,
      "grad_norm": 2.2428629398345947,
      "learning_rate": 1.7576461478900505e-06,
      "loss": 1.9091,
      "step": 454
    },
    {
      "epoch": 0.17615176151761516,
      "grad_norm": 2.1124215126037598,
      "learning_rate": 1.7615176151761518e-06,
      "loss": 1.9249,
      "step": 455
    },
    {
      "epoch": 0.1765389082462253,
      "grad_norm": 2.023526430130005,
      "learning_rate": 1.7653890824622533e-06,
      "loss": 1.9182,
      "step": 456
    },
    {
      "epoch": 0.17692605497483546,
      "grad_norm": 2.263871192932129,
      "learning_rate": 1.7692605497483548e-06,
      "loss": 1.8959,
      "step": 457
    },
    {
      "epoch": 0.1773132017034456,
      "grad_norm": 1.727168083190918,
      "learning_rate": 1.7731320170344562e-06,
      "loss": 1.9249,
      "step": 458
    },
    {
      "epoch": 0.17770034843205576,
      "grad_norm": 2.3711769580841064,
      "learning_rate": 1.7770034843205577e-06,
      "loss": 1.89,
      "step": 459
    },
    {
      "epoch": 0.17808749516066588,
      "grad_norm": 1.6670210361480713,
      "learning_rate": 1.780874951606659e-06,
      "loss": 1.915,
      "step": 460
    },
    {
      "epoch": 0.17847464188927603,
      "grad_norm": 3.0449299812316895,
      "learning_rate": 1.7847464188927605e-06,
      "loss": 1.8581,
      "step": 461
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 2.191378116607666,
      "learning_rate": 1.788617886178862e-06,
      "loss": 1.8564,
      "step": 462
    },
    {
      "epoch": 0.17924893534649633,
      "grad_norm": 1.9549640417099,
      "learning_rate": 1.7924893534649635e-06,
      "loss": 1.9251,
      "step": 463
    },
    {
      "epoch": 0.17963608207510648,
      "grad_norm": 2.2771153450012207,
      "learning_rate": 1.796360820751065e-06,
      "loss": 1.9291,
      "step": 464
    },
    {
      "epoch": 0.1800232288037166,
      "grad_norm": 1.8234041929244995,
      "learning_rate": 1.800232288037166e-06,
      "loss": 1.8824,
      "step": 465
    },
    {
      "epoch": 0.18041037553232675,
      "grad_norm": 2.7391409873962402,
      "learning_rate": 1.8041037553232676e-06,
      "loss": 1.9647,
      "step": 466
    },
    {
      "epoch": 0.1807975222609369,
      "grad_norm": 2.333094358444214,
      "learning_rate": 1.807975222609369e-06,
      "loss": 1.9167,
      "step": 467
    },
    {
      "epoch": 0.18118466898954705,
      "grad_norm": 1.7818106412887573,
      "learning_rate": 1.8118466898954705e-06,
      "loss": 1.9099,
      "step": 468
    },
    {
      "epoch": 0.18157181571815717,
      "grad_norm": 2.2323849201202393,
      "learning_rate": 1.8157181571815718e-06,
      "loss": 1.9488,
      "step": 469
    },
    {
      "epoch": 0.18195896244676732,
      "grad_norm": 2.204543352127075,
      "learning_rate": 1.8195896244676733e-06,
      "loss": 1.9422,
      "step": 470
    },
    {
      "epoch": 0.18234610917537747,
      "grad_norm": 3.3988256454467773,
      "learning_rate": 1.8234610917537748e-06,
      "loss": 1.8778,
      "step": 471
    },
    {
      "epoch": 0.18273325590398762,
      "grad_norm": 1.9499329328536987,
      "learning_rate": 1.8273325590398763e-06,
      "loss": 1.8762,
      "step": 472
    },
    {
      "epoch": 0.18312040263259777,
      "grad_norm": 2.0696651935577393,
      "learning_rate": 1.8312040263259778e-06,
      "loss": 1.8734,
      "step": 473
    },
    {
      "epoch": 0.1835075493612079,
      "grad_norm": 1.943961501121521,
      "learning_rate": 1.835075493612079e-06,
      "loss": 1.8755,
      "step": 474
    },
    {
      "epoch": 0.18389469608981804,
      "grad_norm": 2.222642660140991,
      "learning_rate": 1.8389469608981806e-06,
      "loss": 1.8614,
      "step": 475
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 1.7302738428115845,
      "learning_rate": 1.842818428184282e-06,
      "loss": 1.95,
      "step": 476
    },
    {
      "epoch": 0.18466898954703834,
      "grad_norm": 2.9227631092071533,
      "learning_rate": 1.8466898954703836e-06,
      "loss": 1.8966,
      "step": 477
    },
    {
      "epoch": 0.18505613627564846,
      "grad_norm": 2.488569498062134,
      "learning_rate": 1.8505613627564846e-06,
      "loss": 1.8509,
      "step": 478
    },
    {
      "epoch": 0.1854432830042586,
      "grad_norm": 2.4713664054870605,
      "learning_rate": 1.8544328300425861e-06,
      "loss": 1.8599,
      "step": 479
    },
    {
      "epoch": 0.18583042973286876,
      "grad_norm": 2.2355926036834717,
      "learning_rate": 1.8583042973286876e-06,
      "loss": 1.8724,
      "step": 480
    },
    {
      "epoch": 0.1862175764614789,
      "grad_norm": 1.877783179283142,
      "learning_rate": 1.8621757646147891e-06,
      "loss": 1.9127,
      "step": 481
    },
    {
      "epoch": 0.18660472319008906,
      "grad_norm": 1.7124749422073364,
      "learning_rate": 1.8660472319008906e-06,
      "loss": 1.9015,
      "step": 482
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 2.6785898208618164,
      "learning_rate": 1.8699186991869919e-06,
      "loss": 1.9304,
      "step": 483
    },
    {
      "epoch": 0.18737901664730933,
      "grad_norm": 2.280778169631958,
      "learning_rate": 1.8737901664730934e-06,
      "loss": 1.9159,
      "step": 484
    },
    {
      "epoch": 0.18776616337591948,
      "grad_norm": 1.917722463607788,
      "learning_rate": 1.8776616337591949e-06,
      "loss": 1.9146,
      "step": 485
    },
    {
      "epoch": 0.18815331010452963,
      "grad_norm": 1.9566518068313599,
      "learning_rate": 1.8815331010452964e-06,
      "loss": 1.9506,
      "step": 486
    },
    {
      "epoch": 0.18854045683313975,
      "grad_norm": 2.3477766513824463,
      "learning_rate": 1.8854045683313977e-06,
      "loss": 1.9187,
      "step": 487
    },
    {
      "epoch": 0.1889276035617499,
      "grad_norm": 2.4909584522247314,
      "learning_rate": 1.8892760356174991e-06,
      "loss": 1.9382,
      "step": 488
    },
    {
      "epoch": 0.18931475029036005,
      "grad_norm": 2.0873820781707764,
      "learning_rate": 1.8931475029036006e-06,
      "loss": 1.9359,
      "step": 489
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 2.571516990661621,
      "learning_rate": 1.8970189701897021e-06,
      "loss": 1.9586,
      "step": 490
    },
    {
      "epoch": 0.19008904374758034,
      "grad_norm": 2.38154935836792,
      "learning_rate": 1.9008904374758036e-06,
      "loss": 1.947,
      "step": 491
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 2.5822691917419434,
      "learning_rate": 1.904761904761905e-06,
      "loss": 1.8536,
      "step": 492
    },
    {
      "epoch": 0.19086333720480061,
      "grad_norm": 1.8056399822235107,
      "learning_rate": 1.908633372048006e-06,
      "loss": 1.9063,
      "step": 493
    },
    {
      "epoch": 0.19125048393341076,
      "grad_norm": 3.5948235988616943,
      "learning_rate": 1.9125048393341077e-06,
      "loss": 1.967,
      "step": 494
    },
    {
      "epoch": 0.1916376306620209,
      "grad_norm": 2.4224820137023926,
      "learning_rate": 1.916376306620209e-06,
      "loss": 1.9639,
      "step": 495
    },
    {
      "epoch": 0.19202477739063106,
      "grad_norm": 2.342782974243164,
      "learning_rate": 1.9202477739063107e-06,
      "loss": 1.9245,
      "step": 496
    },
    {
      "epoch": 0.19241192411924118,
      "grad_norm": 3.6940810680389404,
      "learning_rate": 1.924119241192412e-06,
      "loss": 2.0253,
      "step": 497
    },
    {
      "epoch": 0.19279907084785133,
      "grad_norm": 3.257767915725708,
      "learning_rate": 1.9279907084785137e-06,
      "loss": 1.8736,
      "step": 498
    },
    {
      "epoch": 0.19318621757646148,
      "grad_norm": 3.9524102210998535,
      "learning_rate": 1.931862175764615e-06,
      "loss": 1.9619,
      "step": 499
    },
    {
      "epoch": 0.19357336430507163,
      "grad_norm": 3.4401321411132812,
      "learning_rate": 1.9357336430507166e-06,
      "loss": 2.0274,
      "step": 500
    },
    {
      "epoch": 0.19396051103368175,
      "grad_norm": 2.4816038608551025,
      "learning_rate": 1.9396051103368177e-06,
      "loss": 1.8443,
      "step": 501
    },
    {
      "epoch": 0.1943476577622919,
      "grad_norm": 3.461838960647583,
      "learning_rate": 1.943476577622919e-06,
      "loss": 1.8317,
      "step": 502
    },
    {
      "epoch": 0.19473480449090205,
      "grad_norm": 2.773894786834717,
      "learning_rate": 1.9473480449090207e-06,
      "loss": 1.9651,
      "step": 503
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 2.1346523761749268,
      "learning_rate": 1.951219512195122e-06,
      "loss": 1.948,
      "step": 504
    },
    {
      "epoch": 0.19550909794812235,
      "grad_norm": 2.639450788497925,
      "learning_rate": 1.9550909794812237e-06,
      "loss": 1.955,
      "step": 505
    },
    {
      "epoch": 0.19589624467673247,
      "grad_norm": 2.8137636184692383,
      "learning_rate": 1.9589624467673248e-06,
      "loss": 1.9477,
      "step": 506
    },
    {
      "epoch": 0.19628339140534262,
      "grad_norm": 2.0274181365966797,
      "learning_rate": 1.9628339140534263e-06,
      "loss": 1.8955,
      "step": 507
    },
    {
      "epoch": 0.19667053813395277,
      "grad_norm": 2.113105058670044,
      "learning_rate": 1.9667053813395277e-06,
      "loss": 1.8924,
      "step": 508
    },
    {
      "epoch": 0.19705768486256292,
      "grad_norm": 2.6175010204315186,
      "learning_rate": 1.9705768486256292e-06,
      "loss": 1.9068,
      "step": 509
    },
    {
      "epoch": 0.19744483159117304,
      "grad_norm": 2.2982449531555176,
      "learning_rate": 1.9744483159117307e-06,
      "loss": 1.9035,
      "step": 510
    },
    {
      "epoch": 0.1978319783197832,
      "grad_norm": 2.0805115699768066,
      "learning_rate": 1.9783197831978322e-06,
      "loss": 1.9404,
      "step": 511
    },
    {
      "epoch": 0.19821912504839334,
      "grad_norm": 2.186197519302368,
      "learning_rate": 1.9821912504839337e-06,
      "loss": 1.8626,
      "step": 512
    },
    {
      "epoch": 0.1986062717770035,
      "grad_norm": 2.702214002609253,
      "learning_rate": 1.986062717770035e-06,
      "loss": 1.8421,
      "step": 513
    },
    {
      "epoch": 0.19899341850561364,
      "grad_norm": 2.377042293548584,
      "learning_rate": 1.9899341850561367e-06,
      "loss": 1.9021,
      "step": 514
    },
    {
      "epoch": 0.19938056523422376,
      "grad_norm": 2.3623898029327393,
      "learning_rate": 1.9938056523422378e-06,
      "loss": 1.8568,
      "step": 515
    },
    {
      "epoch": 0.1997677119628339,
      "grad_norm": 2.3873507976531982,
      "learning_rate": 1.9976771196283393e-06,
      "loss": 1.9689,
      "step": 516
    },
    {
      "epoch": 0.20015485869144406,
      "grad_norm": 2.4235477447509766,
      "learning_rate": 2.0015485869144408e-06,
      "loss": 1.9833,
      "step": 517
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 2.501016139984131,
      "learning_rate": 2.0054200542005423e-06,
      "loss": 1.9063,
      "step": 518
    },
    {
      "epoch": 0.20092915214866433,
      "grad_norm": 2.4915363788604736,
      "learning_rate": 2.0092915214866433e-06,
      "loss": 1.8524,
      "step": 519
    },
    {
      "epoch": 0.20131629887727448,
      "grad_norm": 2.2108585834503174,
      "learning_rate": 2.013162988772745e-06,
      "loss": 1.9087,
      "step": 520
    },
    {
      "epoch": 0.20170344560588463,
      "grad_norm": 2.323448896408081,
      "learning_rate": 2.0170344560588463e-06,
      "loss": 1.9437,
      "step": 521
    },
    {
      "epoch": 0.20209059233449478,
      "grad_norm": 2.4101457595825195,
      "learning_rate": 2.020905923344948e-06,
      "loss": 1.8875,
      "step": 522
    },
    {
      "epoch": 0.20247773906310493,
      "grad_norm": 2.8812882900238037,
      "learning_rate": 2.0247773906310493e-06,
      "loss": 2.0077,
      "step": 523
    },
    {
      "epoch": 0.20286488579171505,
      "grad_norm": 2.8530945777893066,
      "learning_rate": 2.028648857917151e-06,
      "loss": 1.882,
      "step": 524
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 3.3358843326568604,
      "learning_rate": 2.0325203252032523e-06,
      "loss": 1.8882,
      "step": 525
    },
    {
      "epoch": 0.20363917924893535,
      "grad_norm": 2.7170238494873047,
      "learning_rate": 2.0363917924893538e-06,
      "loss": 1.8855,
      "step": 526
    },
    {
      "epoch": 0.2040263259775455,
      "grad_norm": 3.7087786197662354,
      "learning_rate": 2.0402632597754553e-06,
      "loss": 1.882,
      "step": 527
    },
    {
      "epoch": 0.20441347270615565,
      "grad_norm": 2.993332862854004,
      "learning_rate": 2.0441347270615568e-06,
      "loss": 1.9938,
      "step": 528
    },
    {
      "epoch": 0.20480061943476577,
      "grad_norm": 2.101775884628296,
      "learning_rate": 2.048006194347658e-06,
      "loss": 1.921,
      "step": 529
    },
    {
      "epoch": 0.20518776616337592,
      "grad_norm": 2.6562321186065674,
      "learning_rate": 2.0518776616337593e-06,
      "loss": 1.8606,
      "step": 530
    },
    {
      "epoch": 0.20557491289198607,
      "grad_norm": 2.7955009937286377,
      "learning_rate": 2.055749128919861e-06,
      "loss": 1.8412,
      "step": 531
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 3.4236927032470703,
      "learning_rate": 2.0596205962059623e-06,
      "loss": 1.83,
      "step": 532
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 3.0495593547821045,
      "learning_rate": 2.0634920634920634e-06,
      "loss": 2.0067,
      "step": 533
    },
    {
      "epoch": 0.2067363530778165,
      "grad_norm": 2.8688130378723145,
      "learning_rate": 2.067363530778165e-06,
      "loss": 1.9912,
      "step": 534
    },
    {
      "epoch": 0.20712349980642664,
      "grad_norm": 2.4124221801757812,
      "learning_rate": 2.0712349980642664e-06,
      "loss": 1.8411,
      "step": 535
    },
    {
      "epoch": 0.20751064653503679,
      "grad_norm": 3.1382622718811035,
      "learning_rate": 2.075106465350368e-06,
      "loss": 1.9271,
      "step": 536
    },
    {
      "epoch": 0.20789779326364694,
      "grad_norm": 2.927093505859375,
      "learning_rate": 2.0789779326364694e-06,
      "loss": 1.9559,
      "step": 537
    },
    {
      "epoch": 0.20828493999225706,
      "grad_norm": 2.526332139968872,
      "learning_rate": 2.082849399922571e-06,
      "loss": 1.902,
      "step": 538
    },
    {
      "epoch": 0.2086720867208672,
      "grad_norm": 2.562246084213257,
      "learning_rate": 2.0867208672086723e-06,
      "loss": 1.9005,
      "step": 539
    },
    {
      "epoch": 0.20905923344947736,
      "grad_norm": 4.358397483825684,
      "learning_rate": 2.090592334494774e-06,
      "loss": 1.9562,
      "step": 540
    },
    {
      "epoch": 0.2094463801780875,
      "grad_norm": 3.723266839981079,
      "learning_rate": 2.0944638017808753e-06,
      "loss": 1.8856,
      "step": 541
    },
    {
      "epoch": 0.20983352690669763,
      "grad_norm": 3.3886048793792725,
      "learning_rate": 2.0983352690669764e-06,
      "loss": 2.002,
      "step": 542
    },
    {
      "epoch": 0.21022067363530778,
      "grad_norm": 2.269824743270874,
      "learning_rate": 2.102206736353078e-06,
      "loss": 1.9524,
      "step": 543
    },
    {
      "epoch": 0.21060782036391792,
      "grad_norm": 2.7479076385498047,
      "learning_rate": 2.1060782036391794e-06,
      "loss": 1.8612,
      "step": 544
    },
    {
      "epoch": 0.21099496709252807,
      "grad_norm": 2.758011817932129,
      "learning_rate": 2.109949670925281e-06,
      "loss": 1.8876,
      "step": 545
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 2.6552813053131104,
      "learning_rate": 2.1138211382113824e-06,
      "loss": 1.974,
      "step": 546
    },
    {
      "epoch": 0.21176926054974834,
      "grad_norm": 3.232708692550659,
      "learning_rate": 2.1176926054974834e-06,
      "loss": 1.9336,
      "step": 547
    },
    {
      "epoch": 0.2121564072783585,
      "grad_norm": 2.7354931831359863,
      "learning_rate": 2.121564072783585e-06,
      "loss": 1.9484,
      "step": 548
    },
    {
      "epoch": 0.21254355400696864,
      "grad_norm": 2.8682477474212646,
      "learning_rate": 2.1254355400696864e-06,
      "loss": 1.9574,
      "step": 549
    },
    {
      "epoch": 0.2129307007355788,
      "grad_norm": 2.6743104457855225,
      "learning_rate": 2.129307007355788e-06,
      "loss": 1.9425,
      "step": 550
    },
    {
      "epoch": 0.21331784746418891,
      "grad_norm": 2.7308857440948486,
      "learning_rate": 2.1331784746418894e-06,
      "loss": 1.9423,
      "step": 551
    },
    {
      "epoch": 0.21370499419279906,
      "grad_norm": 4.331692218780518,
      "learning_rate": 2.137049941927991e-06,
      "loss": 1.8594,
      "step": 552
    },
    {
      "epoch": 0.2140921409214092,
      "grad_norm": 2.5042083263397217,
      "learning_rate": 2.1409214092140924e-06,
      "loss": 1.9768,
      "step": 553
    },
    {
      "epoch": 0.21447928765001936,
      "grad_norm": 2.335200071334839,
      "learning_rate": 2.144792876500194e-06,
      "loss": 1.8498,
      "step": 554
    },
    {
      "epoch": 0.2148664343786295,
      "grad_norm": 2.799895763397217,
      "learning_rate": 2.1486643437862954e-06,
      "loss": 1.8374,
      "step": 555
    },
    {
      "epoch": 0.21525358110723963,
      "grad_norm": 2.5952205657958984,
      "learning_rate": 2.1525358110723965e-06,
      "loss": 1.8815,
      "step": 556
    },
    {
      "epoch": 0.21564072783584978,
      "grad_norm": 3.546342134475708,
      "learning_rate": 2.156407278358498e-06,
      "loss": 1.9167,
      "step": 557
    },
    {
      "epoch": 0.21602787456445993,
      "grad_norm": 2.8184549808502197,
      "learning_rate": 2.1602787456445995e-06,
      "loss": 1.8496,
      "step": 558
    },
    {
      "epoch": 0.21641502129307008,
      "grad_norm": 2.996811866760254,
      "learning_rate": 2.164150212930701e-06,
      "loss": 1.8741,
      "step": 559
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 2.5355756282806396,
      "learning_rate": 2.1680216802168024e-06,
      "loss": 1.8463,
      "step": 560
    },
    {
      "epoch": 0.21718931475029035,
      "grad_norm": 2.502490282058716,
      "learning_rate": 2.1718931475029035e-06,
      "loss": 1.9196,
      "step": 561
    },
    {
      "epoch": 0.2175764614789005,
      "grad_norm": 2.6291375160217285,
      "learning_rate": 2.175764614789005e-06,
      "loss": 1.8721,
      "step": 562
    },
    {
      "epoch": 0.21796360820751065,
      "grad_norm": 1.9296364784240723,
      "learning_rate": 2.1796360820751065e-06,
      "loss": 1.9146,
      "step": 563
    },
    {
      "epoch": 0.2183507549361208,
      "grad_norm": 2.586912155151367,
      "learning_rate": 2.183507549361208e-06,
      "loss": 1.8551,
      "step": 564
    },
    {
      "epoch": 0.21873790166473092,
      "grad_norm": 3.1770145893096924,
      "learning_rate": 2.1873790166473095e-06,
      "loss": 1.9061,
      "step": 565
    },
    {
      "epoch": 0.21912504839334107,
      "grad_norm": 2.5562517642974854,
      "learning_rate": 2.191250483933411e-06,
      "loss": 1.9023,
      "step": 566
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 3.6946306228637695,
      "learning_rate": 2.1951219512195125e-06,
      "loss": 1.8845,
      "step": 567
    },
    {
      "epoch": 0.21989934185056137,
      "grad_norm": 2.6644959449768066,
      "learning_rate": 2.198993418505614e-06,
      "loss": 1.8888,
      "step": 568
    },
    {
      "epoch": 0.22028648857917152,
      "grad_norm": 3.0481698513031006,
      "learning_rate": 2.2028648857917155e-06,
      "loss": 1.9299,
      "step": 569
    },
    {
      "epoch": 0.22067363530778164,
      "grad_norm": 3.1044862270355225,
      "learning_rate": 2.2067363530778165e-06,
      "loss": 1.9954,
      "step": 570
    },
    {
      "epoch": 0.2210607820363918,
      "grad_norm": 2.7377381324768066,
      "learning_rate": 2.210607820363918e-06,
      "loss": 1.9922,
      "step": 571
    },
    {
      "epoch": 0.22144792876500194,
      "grad_norm": 3.2715463638305664,
      "learning_rate": 2.2144792876500195e-06,
      "loss": 1.8587,
      "step": 572
    },
    {
      "epoch": 0.2218350754936121,
      "grad_norm": 2.9851653575897217,
      "learning_rate": 2.218350754936121e-06,
      "loss": 1.873,
      "step": 573
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 3.0437581539154053,
      "learning_rate": 2.222222222222222e-06,
      "loss": 1.9325,
      "step": 574
    },
    {
      "epoch": 0.22260936895083236,
      "grad_norm": 3.1831600666046143,
      "learning_rate": 2.2260936895083236e-06,
      "loss": 1.8842,
      "step": 575
    },
    {
      "epoch": 0.2229965156794425,
      "grad_norm": 2.4917285442352295,
      "learning_rate": 2.229965156794425e-06,
      "loss": 1.8764,
      "step": 576
    },
    {
      "epoch": 0.22338366240805266,
      "grad_norm": 3.124706745147705,
      "learning_rate": 2.2338366240805266e-06,
      "loss": 1.8457,
      "step": 577
    },
    {
      "epoch": 0.2237708091366628,
      "grad_norm": 2.5710504055023193,
      "learning_rate": 2.237708091366628e-06,
      "loss": 1.867,
      "step": 578
    },
    {
      "epoch": 0.22415795586527293,
      "grad_norm": 3.237506628036499,
      "learning_rate": 2.2415795586527295e-06,
      "loss": 2.0212,
      "step": 579
    },
    {
      "epoch": 0.22454510259388308,
      "grad_norm": 3.0819568634033203,
      "learning_rate": 2.245451025938831e-06,
      "loss": 1.9654,
      "step": 580
    },
    {
      "epoch": 0.22493224932249323,
      "grad_norm": 3.028522253036499,
      "learning_rate": 2.2493224932249325e-06,
      "loss": 1.9505,
      "step": 581
    },
    {
      "epoch": 0.22531939605110338,
      "grad_norm": 3.6128528118133545,
      "learning_rate": 2.253193960511034e-06,
      "loss": 1.8228,
      "step": 582
    },
    {
      "epoch": 0.2257065427797135,
      "grad_norm": 3.3251895904541016,
      "learning_rate": 2.257065427797135e-06,
      "loss": 1.8851,
      "step": 583
    },
    {
      "epoch": 0.22609368950832365,
      "grad_norm": 2.2347934246063232,
      "learning_rate": 2.2609368950832366e-06,
      "loss": 1.9214,
      "step": 584
    },
    {
      "epoch": 0.2264808362369338,
      "grad_norm": 4.391674518585205,
      "learning_rate": 2.264808362369338e-06,
      "loss": 1.9317,
      "step": 585
    },
    {
      "epoch": 0.22686798296554395,
      "grad_norm": 3.3873775005340576,
      "learning_rate": 2.2686798296554396e-06,
      "loss": 1.8221,
      "step": 586
    },
    {
      "epoch": 0.2272551296941541,
      "grad_norm": 3.1601979732513428,
      "learning_rate": 2.272551296941541e-06,
      "loss": 1.8219,
      "step": 587
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 3.4796934127807617,
      "learning_rate": 2.2764227642276426e-06,
      "loss": 1.87,
      "step": 588
    },
    {
      "epoch": 0.22802942315137437,
      "grad_norm": 2.8448221683502197,
      "learning_rate": 2.280294231513744e-06,
      "loss": 1.969,
      "step": 589
    },
    {
      "epoch": 0.22841656987998452,
      "grad_norm": 3.1195778846740723,
      "learning_rate": 2.2841656987998455e-06,
      "loss": 1.9873,
      "step": 590
    },
    {
      "epoch": 0.22880371660859466,
      "grad_norm": 5.096436023712158,
      "learning_rate": 2.288037166085947e-06,
      "loss": 1.9543,
      "step": 591
    },
    {
      "epoch": 0.22919086333720481,
      "grad_norm": 2.8748507499694824,
      "learning_rate": 2.2919086333720485e-06,
      "loss": 1.9509,
      "step": 592
    },
    {
      "epoch": 0.22957801006581494,
      "grad_norm": 2.957690477371216,
      "learning_rate": 2.2957801006581496e-06,
      "loss": 1.8318,
      "step": 593
    },
    {
      "epoch": 0.22996515679442509,
      "grad_norm": 3.318359613418579,
      "learning_rate": 2.299651567944251e-06,
      "loss": 1.906,
      "step": 594
    },
    {
      "epoch": 0.23035230352303523,
      "grad_norm": 3.4155304431915283,
      "learning_rate": 2.3035230352303526e-06,
      "loss": 1.9993,
      "step": 595
    },
    {
      "epoch": 0.23073945025164538,
      "grad_norm": 3.2739977836608887,
      "learning_rate": 2.307394502516454e-06,
      "loss": 1.9274,
      "step": 596
    },
    {
      "epoch": 0.2311265969802555,
      "grad_norm": 3.0603384971618652,
      "learning_rate": 2.311265969802555e-06,
      "loss": 1.8668,
      "step": 597
    },
    {
      "epoch": 0.23151374370886565,
      "grad_norm": 3.2655792236328125,
      "learning_rate": 2.3151374370886566e-06,
      "loss": 1.997,
      "step": 598
    },
    {
      "epoch": 0.2319008904374758,
      "grad_norm": 2.7068700790405273,
      "learning_rate": 2.319008904374758e-06,
      "loss": 1.9316,
      "step": 599
    },
    {
      "epoch": 0.23228803716608595,
      "grad_norm": 2.8865816593170166,
      "learning_rate": 2.3228803716608596e-06,
      "loss": 1.8957,
      "step": 600
    },
    {
      "epoch": 0.2326751838946961,
      "grad_norm": 2.599088668823242,
      "learning_rate": 2.326751838946961e-06,
      "loss": 1.9323,
      "step": 601
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 3.4828672409057617,
      "learning_rate": 2.3306233062330626e-06,
      "loss": 1.9538,
      "step": 602
    },
    {
      "epoch": 0.23344947735191637,
      "grad_norm": 4.671082973480225,
      "learning_rate": 2.334494773519164e-06,
      "loss": 1.8551,
      "step": 603
    },
    {
      "epoch": 0.23383662408052652,
      "grad_norm": 3.0255954265594482,
      "learning_rate": 2.3383662408052656e-06,
      "loss": 1.892,
      "step": 604
    },
    {
      "epoch": 0.23422377080913667,
      "grad_norm": 3.1279845237731934,
      "learning_rate": 2.342237708091367e-06,
      "loss": 1.8258,
      "step": 605
    },
    {
      "epoch": 0.2346109175377468,
      "grad_norm": 3.1772384643554688,
      "learning_rate": 2.346109175377468e-06,
      "loss": 1.8204,
      "step": 606
    },
    {
      "epoch": 0.23499806426635694,
      "grad_norm": 3.6955058574676514,
      "learning_rate": 2.3499806426635697e-06,
      "loss": 1.9051,
      "step": 607
    },
    {
      "epoch": 0.2353852109949671,
      "grad_norm": 3.1887357234954834,
      "learning_rate": 2.353852109949671e-06,
      "loss": 1.8205,
      "step": 608
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 3.2619283199310303,
      "learning_rate": 2.3577235772357727e-06,
      "loss": 2.001,
      "step": 609
    },
    {
      "epoch": 0.2361595044521874,
      "grad_norm": 3.414287805557251,
      "learning_rate": 2.361595044521874e-06,
      "loss": 1.8904,
      "step": 610
    },
    {
      "epoch": 0.2365466511807975,
      "grad_norm": 3.365271806716919,
      "learning_rate": 2.3654665118079752e-06,
      "loss": 1.9885,
      "step": 611
    },
    {
      "epoch": 0.23693379790940766,
      "grad_norm": 2.9154138565063477,
      "learning_rate": 2.3693379790940767e-06,
      "loss": 1.8776,
      "step": 612
    },
    {
      "epoch": 0.2373209446380178,
      "grad_norm": 3.950515031814575,
      "learning_rate": 2.373209446380178e-06,
      "loss": 1.8804,
      "step": 613
    },
    {
      "epoch": 0.23770809136662796,
      "grad_norm": 3.3670907020568848,
      "learning_rate": 2.3770809136662797e-06,
      "loss": 1.896,
      "step": 614
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 3.382739543914795,
      "learning_rate": 2.380952380952381e-06,
      "loss": 1.8724,
      "step": 615
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 3.3065719604492188,
      "learning_rate": 2.3848238482384827e-06,
      "loss": 1.9434,
      "step": 616
    },
    {
      "epoch": 0.23886953155245838,
      "grad_norm": 3.561591148376465,
      "learning_rate": 2.388695315524584e-06,
      "loss": 1.8742,
      "step": 617
    },
    {
      "epoch": 0.23925667828106853,
      "grad_norm": 2.970308780670166,
      "learning_rate": 2.3925667828106857e-06,
      "loss": 1.9409,
      "step": 618
    },
    {
      "epoch": 0.23964382500967868,
      "grad_norm": 2.8836848735809326,
      "learning_rate": 2.396438250096787e-06,
      "loss": 1.8374,
      "step": 619
    },
    {
      "epoch": 0.2400309717382888,
      "grad_norm": 3.968465566635132,
      "learning_rate": 2.4003097173828882e-06,
      "loss": 1.8975,
      "step": 620
    },
    {
      "epoch": 0.24041811846689895,
      "grad_norm": 3.4351775646209717,
      "learning_rate": 2.4041811846689897e-06,
      "loss": 1.922,
      "step": 621
    },
    {
      "epoch": 0.2408052651955091,
      "grad_norm": 3.2049365043640137,
      "learning_rate": 2.4080526519550912e-06,
      "loss": 1.8614,
      "step": 622
    },
    {
      "epoch": 0.24119241192411925,
      "grad_norm": 3.821133852005005,
      "learning_rate": 2.4119241192411927e-06,
      "loss": 1.9453,
      "step": 623
    },
    {
      "epoch": 0.2415795586527294,
      "grad_norm": 3.1494576930999756,
      "learning_rate": 2.415795586527294e-06,
      "loss": 1.9489,
      "step": 624
    },
    {
      "epoch": 0.24196670538133952,
      "grad_norm": 5.582937240600586,
      "learning_rate": 2.4196670538133953e-06,
      "loss": 1.9362,
      "step": 625
    },
    {
      "epoch": 0.24235385210994967,
      "grad_norm": 3.1615118980407715,
      "learning_rate": 2.4235385210994968e-06,
      "loss": 1.9177,
      "step": 626
    },
    {
      "epoch": 0.24274099883855982,
      "grad_norm": 4.917486667633057,
      "learning_rate": 2.4274099883855983e-06,
      "loss": 1.9665,
      "step": 627
    },
    {
      "epoch": 0.24312814556716997,
      "grad_norm": 2.617466688156128,
      "learning_rate": 2.4312814556716998e-06,
      "loss": 1.9039,
      "step": 628
    },
    {
      "epoch": 0.2435152922957801,
      "grad_norm": 3.5294570922851562,
      "learning_rate": 2.4351529229578012e-06,
      "loss": 1.7998,
      "step": 629
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 3.3786113262176514,
      "learning_rate": 2.4390243902439027e-06,
      "loss": 1.8826,
      "step": 630
    },
    {
      "epoch": 0.2442895857530004,
      "grad_norm": 3.3518545627593994,
      "learning_rate": 2.4428958575300042e-06,
      "loss": 1.9907,
      "step": 631
    },
    {
      "epoch": 0.24467673248161054,
      "grad_norm": 3.9130935668945312,
      "learning_rate": 2.4467673248161057e-06,
      "loss": 1.9671,
      "step": 632
    },
    {
      "epoch": 0.2450638792102207,
      "grad_norm": 2.4000244140625,
      "learning_rate": 2.4506387921022072e-06,
      "loss": 1.9032,
      "step": 633
    },
    {
      "epoch": 0.2454510259388308,
      "grad_norm": 3.740856885910034,
      "learning_rate": 2.4545102593883083e-06,
      "loss": 1.9897,
      "step": 634
    },
    {
      "epoch": 0.24583817266744096,
      "grad_norm": 5.551656723022461,
      "learning_rate": 2.4583817266744098e-06,
      "loss": 2.0685,
      "step": 635
    },
    {
      "epoch": 0.2462253193960511,
      "grad_norm": 5.3994293212890625,
      "learning_rate": 2.4622531939605113e-06,
      "loss": 2.0431,
      "step": 636
    },
    {
      "epoch": 0.24661246612466126,
      "grad_norm": 3.578625202178955,
      "learning_rate": 2.4661246612466128e-06,
      "loss": 1.9106,
      "step": 637
    },
    {
      "epoch": 0.24699961285327138,
      "grad_norm": 3.429713010787964,
      "learning_rate": 2.469996128532714e-06,
      "loss": 2.009,
      "step": 638
    },
    {
      "epoch": 0.24738675958188153,
      "grad_norm": 3.784666061401367,
      "learning_rate": 2.4738675958188153e-06,
      "loss": 1.7985,
      "step": 639
    },
    {
      "epoch": 0.24777390631049168,
      "grad_norm": 3.3645927906036377,
      "learning_rate": 2.477739063104917e-06,
      "loss": 1.9087,
      "step": 640
    },
    {
      "epoch": 0.24816105303910183,
      "grad_norm": 3.9199585914611816,
      "learning_rate": 2.4816105303910183e-06,
      "loss": 1.8845,
      "step": 641
    },
    {
      "epoch": 0.24854819976771197,
      "grad_norm": 6.136801719665527,
      "learning_rate": 2.48548199767712e-06,
      "loss": 1.7943,
      "step": 642
    },
    {
      "epoch": 0.2489353464963221,
      "grad_norm": 3.523496389389038,
      "learning_rate": 2.4893534649632213e-06,
      "loss": 1.9407,
      "step": 643
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 3.8842272758483887,
      "learning_rate": 2.493224932249323e-06,
      "loss": 1.8769,
      "step": 644
    },
    {
      "epoch": 0.2497096399535424,
      "grad_norm": 4.059936046600342,
      "learning_rate": 2.4970963995354243e-06,
      "loss": 1.861,
      "step": 645
    },
    {
      "epoch": 0.2500967866821525,
      "grad_norm": 3.360710859298706,
      "learning_rate": 2.5009678668215254e-06,
      "loss": 1.8916,
      "step": 646
    },
    {
      "epoch": 0.2504839334107627,
      "grad_norm": 3.1754322052001953,
      "learning_rate": 2.5048393341076273e-06,
      "loss": 1.8353,
      "step": 647
    },
    {
      "epoch": 0.2508710801393728,
      "grad_norm": 3.7455263137817383,
      "learning_rate": 2.5087108013937284e-06,
      "loss": 1.8163,
      "step": 648
    },
    {
      "epoch": 0.251258226867983,
      "grad_norm": 3.599885940551758,
      "learning_rate": 2.5125822686798303e-06,
      "loss": 1.8072,
      "step": 649
    },
    {
      "epoch": 0.2516453735965931,
      "grad_norm": 4.261420726776123,
      "learning_rate": 2.5164537359659313e-06,
      "loss": 1.8518,
      "step": 650
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 3.8999245166778564,
      "learning_rate": 2.5203252032520324e-06,
      "loss": 1.9896,
      "step": 651
    },
    {
      "epoch": 0.2524196670538134,
      "grad_norm": 3.0409655570983887,
      "learning_rate": 2.5241966705381343e-06,
      "loss": 1.8783,
      "step": 652
    },
    {
      "epoch": 0.25280681378242353,
      "grad_norm": 3.44474720954895,
      "learning_rate": 2.5280681378242354e-06,
      "loss": 1.9036,
      "step": 653
    },
    {
      "epoch": 0.25319396051103366,
      "grad_norm": 3.290738582611084,
      "learning_rate": 2.531939605110337e-06,
      "loss": 1.8864,
      "step": 654
    },
    {
      "epoch": 0.25358110723964383,
      "grad_norm": 3.4059650897979736,
      "learning_rate": 2.5358110723964384e-06,
      "loss": 1.933,
      "step": 655
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 6.91644811630249,
      "learning_rate": 2.53968253968254e-06,
      "loss": 2.1221,
      "step": 656
    },
    {
      "epoch": 0.25435540069686413,
      "grad_norm": 4.318762302398682,
      "learning_rate": 2.5435540069686414e-06,
      "loss": 1.8058,
      "step": 657
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 6.063329696655273,
      "learning_rate": 2.547425474254743e-06,
      "loss": 1.8173,
      "step": 658
    },
    {
      "epoch": 0.2551296941540844,
      "grad_norm": 4.607699871063232,
      "learning_rate": 2.551296941540844e-06,
      "loss": 1.8468,
      "step": 659
    },
    {
      "epoch": 0.25551684088269455,
      "grad_norm": 4.768526077270508,
      "learning_rate": 2.555168408826946e-06,
      "loss": 1.9442,
      "step": 660
    },
    {
      "epoch": 0.2559039876113047,
      "grad_norm": 4.362678050994873,
      "learning_rate": 2.559039876113047e-06,
      "loss": 2.0183,
      "step": 661
    },
    {
      "epoch": 0.25629113433991485,
      "grad_norm": 6.192969799041748,
      "learning_rate": 2.562911343399149e-06,
      "loss": 1.7825,
      "step": 662
    },
    {
      "epoch": 0.25667828106852497,
      "grad_norm": 6.307344913482666,
      "learning_rate": 2.56678281068525e-06,
      "loss": 1.7833,
      "step": 663
    },
    {
      "epoch": 0.2570654277971351,
      "grad_norm": 4.242782115936279,
      "learning_rate": 2.570654277971351e-06,
      "loss": 1.8815,
      "step": 664
    },
    {
      "epoch": 0.25745257452574527,
      "grad_norm": 3.866044521331787,
      "learning_rate": 2.574525745257453e-06,
      "loss": 1.9273,
      "step": 665
    },
    {
      "epoch": 0.2578397212543554,
      "grad_norm": 4.032533168792725,
      "learning_rate": 2.578397212543554e-06,
      "loss": 2.0077,
      "step": 666
    },
    {
      "epoch": 0.25822686798296557,
      "grad_norm": 4.543755054473877,
      "learning_rate": 2.582268679829656e-06,
      "loss": 1.7515,
      "step": 667
    },
    {
      "epoch": 0.2586140147115757,
      "grad_norm": 4.582633972167969,
      "learning_rate": 2.586140147115757e-06,
      "loss": 1.9885,
      "step": 668
    },
    {
      "epoch": 0.2590011614401858,
      "grad_norm": 3.1893250942230225,
      "learning_rate": 2.5900116144018584e-06,
      "loss": 1.8189,
      "step": 669
    },
    {
      "epoch": 0.259388308168796,
      "grad_norm": 4.60728120803833,
      "learning_rate": 2.59388308168796e-06,
      "loss": 1.8458,
      "step": 670
    },
    {
      "epoch": 0.2597754548974061,
      "grad_norm": 4.773448944091797,
      "learning_rate": 2.5977545489740614e-06,
      "loss": 1.8498,
      "step": 671
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 4.272185325622559,
      "learning_rate": 2.601626016260163e-06,
      "loss": 1.7841,
      "step": 672
    },
    {
      "epoch": 0.2605497483546264,
      "grad_norm": 4.759449005126953,
      "learning_rate": 2.6054974835462644e-06,
      "loss": 1.7508,
      "step": 673
    },
    {
      "epoch": 0.26093689508323653,
      "grad_norm": 5.055673599243164,
      "learning_rate": 2.6093689508323655e-06,
      "loss": 2.0055,
      "step": 674
    },
    {
      "epoch": 0.2613240418118467,
      "grad_norm": 6.391668319702148,
      "learning_rate": 2.6132404181184674e-06,
      "loss": 1.7855,
      "step": 675
    },
    {
      "epoch": 0.26171118854045683,
      "grad_norm": 4.756046772003174,
      "learning_rate": 2.6171118854045685e-06,
      "loss": 1.7472,
      "step": 676
    },
    {
      "epoch": 0.26209833526906695,
      "grad_norm": 5.495229244232178,
      "learning_rate": 2.6209833526906695e-06,
      "loss": 2.0584,
      "step": 677
    },
    {
      "epoch": 0.26248548199767713,
      "grad_norm": 5.0100860595703125,
      "learning_rate": 2.6248548199767715e-06,
      "loss": 1.8883,
      "step": 678
    },
    {
      "epoch": 0.26287262872628725,
      "grad_norm": 5.162625312805176,
      "learning_rate": 2.6287262872628725e-06,
      "loss": 1.9899,
      "step": 679
    },
    {
      "epoch": 0.2632597754548974,
      "grad_norm": 3.8375680446624756,
      "learning_rate": 2.6325977545489744e-06,
      "loss": 1.8749,
      "step": 680
    },
    {
      "epoch": 0.26364692218350755,
      "grad_norm": 3.909369707107544,
      "learning_rate": 2.6364692218350755e-06,
      "loss": 1.9139,
      "step": 681
    },
    {
      "epoch": 0.26403406891211767,
      "grad_norm": 5.063611030578613,
      "learning_rate": 2.640340689121177e-06,
      "loss": 2.0069,
      "step": 682
    },
    {
      "epoch": 0.26442121564072785,
      "grad_norm": 4.642796993255615,
      "learning_rate": 2.6442121564072785e-06,
      "loss": 2.0197,
      "step": 683
    },
    {
      "epoch": 0.26480836236933797,
      "grad_norm": 6.2610697746276855,
      "learning_rate": 2.64808362369338e-06,
      "loss": 2.0715,
      "step": 684
    },
    {
      "epoch": 0.26519550909794815,
      "grad_norm": 5.662660598754883,
      "learning_rate": 2.6519550909794815e-06,
      "loss": 1.9479,
      "step": 685
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 4.154530048370361,
      "learning_rate": 2.655826558265583e-06,
      "loss": 1.9376,
      "step": 686
    },
    {
      "epoch": 0.2659698025551684,
      "grad_norm": 4.834386348724365,
      "learning_rate": 2.659698025551684e-06,
      "loss": 1.8541,
      "step": 687
    },
    {
      "epoch": 0.26635694928377857,
      "grad_norm": 3.6495752334594727,
      "learning_rate": 2.663569492837786e-06,
      "loss": 1.896,
      "step": 688
    },
    {
      "epoch": 0.2667440960123887,
      "grad_norm": 4.754018783569336,
      "learning_rate": 2.667440960123887e-06,
      "loss": 2.0248,
      "step": 689
    },
    {
      "epoch": 0.26713124274099886,
      "grad_norm": 4.64987850189209,
      "learning_rate": 2.671312427409989e-06,
      "loss": 1.9435,
      "step": 690
    },
    {
      "epoch": 0.267518389469609,
      "grad_norm": 7.222677230834961,
      "learning_rate": 2.67518389469609e-06,
      "loss": 1.8265,
      "step": 691
    },
    {
      "epoch": 0.2679055361982191,
      "grad_norm": 4.4423933029174805,
      "learning_rate": 2.679055361982191e-06,
      "loss": 2.0162,
      "step": 692
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 7.022895812988281,
      "learning_rate": 2.682926829268293e-06,
      "loss": 1.7694,
      "step": 693
    },
    {
      "epoch": 0.2686798296554394,
      "grad_norm": 4.531471252441406,
      "learning_rate": 2.686798296554394e-06,
      "loss": 1.7688,
      "step": 694
    },
    {
      "epoch": 0.2690669763840495,
      "grad_norm": 5.1545209884643555,
      "learning_rate": 2.6906697638404956e-06,
      "loss": 1.8752,
      "step": 695
    },
    {
      "epoch": 0.2694541231126597,
      "grad_norm": 5.3327412605285645,
      "learning_rate": 2.694541231126597e-06,
      "loss": 2.0328,
      "step": 696
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 4.146062850952148,
      "learning_rate": 2.6984126984126986e-06,
      "loss": 1.9438,
      "step": 697
    },
    {
      "epoch": 0.27022841656988,
      "grad_norm": 4.124353408813477,
      "learning_rate": 2.7022841656988e-06,
      "loss": 1.7708,
      "step": 698
    },
    {
      "epoch": 0.2706155632984901,
      "grad_norm": 4.504242420196533,
      "learning_rate": 2.7061556329849016e-06,
      "loss": 1.9064,
      "step": 699
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 7.229456424713135,
      "learning_rate": 2.7100271002710026e-06,
      "loss": 1.7583,
      "step": 700
    },
    {
      "epoch": 0.2713898567557104,
      "grad_norm": 4.642282485961914,
      "learning_rate": 2.7138985675571045e-06,
      "loss": 1.8314,
      "step": 701
    },
    {
      "epoch": 0.27177700348432055,
      "grad_norm": 5.135335445404053,
      "learning_rate": 2.7177700348432056e-06,
      "loss": 1.9238,
      "step": 702
    },
    {
      "epoch": 0.2721641502129307,
      "grad_norm": 2.8204410076141357,
      "learning_rate": 2.7216415021293075e-06,
      "loss": 1.8897,
      "step": 703
    },
    {
      "epoch": 0.27255129694154084,
      "grad_norm": 6.136299133300781,
      "learning_rate": 2.7255129694154086e-06,
      "loss": 1.9242,
      "step": 704
    },
    {
      "epoch": 0.27293844367015097,
      "grad_norm": 7.137617111206055,
      "learning_rate": 2.7293844367015097e-06,
      "loss": 1.787,
      "step": 705
    },
    {
      "epoch": 0.27332559039876114,
      "grad_norm": 3.5939252376556396,
      "learning_rate": 2.7332559039876116e-06,
      "loss": 1.8043,
      "step": 706
    },
    {
      "epoch": 0.27371273712737126,
      "grad_norm": 4.38499116897583,
      "learning_rate": 2.7371273712737127e-06,
      "loss": 1.858,
      "step": 707
    },
    {
      "epoch": 0.27409988385598144,
      "grad_norm": 4.360669136047363,
      "learning_rate": 2.7409988385598146e-06,
      "loss": 1.8495,
      "step": 708
    },
    {
      "epoch": 0.27448703058459156,
      "grad_norm": 4.928162097930908,
      "learning_rate": 2.7448703058459156e-06,
      "loss": 1.7899,
      "step": 709
    },
    {
      "epoch": 0.2748741773132017,
      "grad_norm": 5.019421577453613,
      "learning_rate": 2.748741773132017e-06,
      "loss": 1.852,
      "step": 710
    },
    {
      "epoch": 0.27526132404181186,
      "grad_norm": 4.517184257507324,
      "learning_rate": 2.7526132404181186e-06,
      "loss": 1.9816,
      "step": 711
    },
    {
      "epoch": 0.275648470770422,
      "grad_norm": 4.655425071716309,
      "learning_rate": 2.75648470770422e-06,
      "loss": 1.7883,
      "step": 712
    },
    {
      "epoch": 0.27603561749903216,
      "grad_norm": 5.299063682556152,
      "learning_rate": 2.7603561749903216e-06,
      "loss": 1.9563,
      "step": 713
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 3.487900495529175,
      "learning_rate": 2.764227642276423e-06,
      "loss": 1.8027,
      "step": 714
    },
    {
      "epoch": 0.2768099109562524,
      "grad_norm": 4.364028453826904,
      "learning_rate": 2.768099109562524e-06,
      "loss": 1.9442,
      "step": 715
    },
    {
      "epoch": 0.2771970576848626,
      "grad_norm": 5.812967300415039,
      "learning_rate": 2.771970576848626e-06,
      "loss": 1.8083,
      "step": 716
    },
    {
      "epoch": 0.2775842044134727,
      "grad_norm": 4.044814109802246,
      "learning_rate": 2.775842044134727e-06,
      "loss": 1.9015,
      "step": 717
    },
    {
      "epoch": 0.2779713511420828,
      "grad_norm": 3.7721736431121826,
      "learning_rate": 2.7797135114208287e-06,
      "loss": 1.8481,
      "step": 718
    },
    {
      "epoch": 0.278358497870693,
      "grad_norm": 4.931928634643555,
      "learning_rate": 2.78358497870693e-06,
      "loss": 1.7782,
      "step": 719
    },
    {
      "epoch": 0.2787456445993031,
      "grad_norm": 5.657633304595947,
      "learning_rate": 2.7874564459930316e-06,
      "loss": 1.8797,
      "step": 720
    },
    {
      "epoch": 0.2791327913279133,
      "grad_norm": 5.048424243927002,
      "learning_rate": 2.791327913279133e-06,
      "loss": 1.725,
      "step": 721
    },
    {
      "epoch": 0.2795199380565234,
      "grad_norm": 6.106997966766357,
      "learning_rate": 2.7951993805652346e-06,
      "loss": 1.8718,
      "step": 722
    },
    {
      "epoch": 0.27990708478513354,
      "grad_norm": 7.286074161529541,
      "learning_rate": 2.7990708478513357e-06,
      "loss": 1.7517,
      "step": 723
    },
    {
      "epoch": 0.2802942315137437,
      "grad_norm": 5.362709045410156,
      "learning_rate": 2.8029423151374376e-06,
      "loss": 1.7559,
      "step": 724
    },
    {
      "epoch": 0.28068137824235384,
      "grad_norm": 4.095361709594727,
      "learning_rate": 2.8068137824235387e-06,
      "loss": 1.8369,
      "step": 725
    },
    {
      "epoch": 0.281068524970964,
      "grad_norm": 4.322763919830322,
      "learning_rate": 2.8106852497096406e-06,
      "loss": 1.882,
      "step": 726
    },
    {
      "epoch": 0.28145567169957414,
      "grad_norm": 6.495409965515137,
      "learning_rate": 2.8145567169957417e-06,
      "loss": 2.022,
      "step": 727
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 6.4468183517456055,
      "learning_rate": 2.8184281842818427e-06,
      "loss": 2.1031,
      "step": 728
    },
    {
      "epoch": 0.28222996515679444,
      "grad_norm": 4.456118583679199,
      "learning_rate": 2.8222996515679447e-06,
      "loss": 1.9985,
      "step": 729
    },
    {
      "epoch": 0.28261711188540456,
      "grad_norm": 6.888878345489502,
      "learning_rate": 2.8261711188540457e-06,
      "loss": 1.9374,
      "step": 730
    },
    {
      "epoch": 0.28300425861401474,
      "grad_norm": 7.9537153244018555,
      "learning_rate": 2.8300425861401476e-06,
      "loss": 1.8008,
      "step": 731
    },
    {
      "epoch": 0.28339140534262486,
      "grad_norm": 5.529542446136475,
      "learning_rate": 2.8339140534262487e-06,
      "loss": 1.9633,
      "step": 732
    },
    {
      "epoch": 0.283778552071235,
      "grad_norm": 6.911525249481201,
      "learning_rate": 2.8377855207123502e-06,
      "loss": 1.7761,
      "step": 733
    },
    {
      "epoch": 0.28416569879984516,
      "grad_norm": 5.577544212341309,
      "learning_rate": 2.8416569879984517e-06,
      "loss": 1.884,
      "step": 734
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 5.366798400878906,
      "learning_rate": 2.845528455284553e-06,
      "loss": 2.029,
      "step": 735
    },
    {
      "epoch": 0.28493999225706546,
      "grad_norm": 9.168179512023926,
      "learning_rate": 2.8493999225706547e-06,
      "loss": 1.9563,
      "step": 736
    },
    {
      "epoch": 0.2853271389856756,
      "grad_norm": 4.902073383331299,
      "learning_rate": 2.853271389856756e-06,
      "loss": 1.984,
      "step": 737
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.704491138458252,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.7238,
      "step": 738
    },
    {
      "epoch": 0.2861014324428959,
      "grad_norm": 3.4613540172576904,
      "learning_rate": 2.861014324428959e-06,
      "loss": 1.9359,
      "step": 739
    },
    {
      "epoch": 0.286488579171506,
      "grad_norm": 5.451022148132324,
      "learning_rate": 2.8648857917150602e-06,
      "loss": 1.9373,
      "step": 740
    },
    {
      "epoch": 0.2868757259001161,
      "grad_norm": 5.204198837280273,
      "learning_rate": 2.8687572590011613e-06,
      "loss": 1.7742,
      "step": 741
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 5.681777000427246,
      "learning_rate": 2.8726287262872632e-06,
      "loss": 1.8316,
      "step": 742
    },
    {
      "epoch": 0.2876500193573364,
      "grad_norm": 4.054718494415283,
      "learning_rate": 2.8765001935733643e-06,
      "loss": 1.8907,
      "step": 743
    },
    {
      "epoch": 0.2880371660859466,
      "grad_norm": 5.634524822235107,
      "learning_rate": 2.8803716608594662e-06,
      "loss": 1.7384,
      "step": 744
    },
    {
      "epoch": 0.2884243128145567,
      "grad_norm": 7.47828483581543,
      "learning_rate": 2.8842431281455673e-06,
      "loss": 1.7447,
      "step": 745
    },
    {
      "epoch": 0.28881145954316684,
      "grad_norm": 4.801217079162598,
      "learning_rate": 2.8881145954316688e-06,
      "loss": 1.8459,
      "step": 746
    },
    {
      "epoch": 0.289198606271777,
      "grad_norm": 4.754260540008545,
      "learning_rate": 2.8919860627177703e-06,
      "loss": 1.8811,
      "step": 747
    },
    {
      "epoch": 0.28958575300038714,
      "grad_norm": 4.147353649139404,
      "learning_rate": 2.8958575300038718e-06,
      "loss": 1.875,
      "step": 748
    },
    {
      "epoch": 0.2899728997289973,
      "grad_norm": 4.6981000900268555,
      "learning_rate": 2.8997289972899733e-06,
      "loss": 1.8323,
      "step": 749
    },
    {
      "epoch": 0.29036004645760743,
      "grad_norm": 5.474301815032959,
      "learning_rate": 2.9036004645760748e-06,
      "loss": 1.8334,
      "step": 750
    },
    {
      "epoch": 0.29074719318621756,
      "grad_norm": 5.662237167358398,
      "learning_rate": 2.907471931862176e-06,
      "loss": 1.8387,
      "step": 751
    },
    {
      "epoch": 0.29113433991482773,
      "grad_norm": 4.950139999389648,
      "learning_rate": 2.9113433991482777e-06,
      "loss": 1.8388,
      "step": 752
    },
    {
      "epoch": 0.29152148664343785,
      "grad_norm": 5.912869930267334,
      "learning_rate": 2.915214866434379e-06,
      "loss": 2.0337,
      "step": 753
    },
    {
      "epoch": 0.29190863337204803,
      "grad_norm": 5.617253303527832,
      "learning_rate": 2.9190863337204807e-06,
      "loss": 2.0504,
      "step": 754
    },
    {
      "epoch": 0.29229578010065815,
      "grad_norm": 6.655825614929199,
      "learning_rate": 2.922957801006582e-06,
      "loss": 2.0543,
      "step": 755
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 4.767932415008545,
      "learning_rate": 2.926829268292683e-06,
      "loss": 1.8532,
      "step": 756
    },
    {
      "epoch": 0.29307007355787845,
      "grad_norm": 4.485767364501953,
      "learning_rate": 2.9307007355787848e-06,
      "loss": 1.798,
      "step": 757
    },
    {
      "epoch": 0.2934572202864886,
      "grad_norm": 4.872833251953125,
      "learning_rate": 2.934572202864886e-06,
      "loss": 1.8822,
      "step": 758
    },
    {
      "epoch": 0.2938443670150987,
      "grad_norm": 5.726745128631592,
      "learning_rate": 2.9384436701509873e-06,
      "loss": 1.8422,
      "step": 759
    },
    {
      "epoch": 0.2942315137437089,
      "grad_norm": 5.475102424621582,
      "learning_rate": 2.942315137437089e-06,
      "loss": 1.8736,
      "step": 760
    },
    {
      "epoch": 0.294618660472319,
      "grad_norm": 4.796927452087402,
      "learning_rate": 2.9461866047231903e-06,
      "loss": 1.8309,
      "step": 761
    },
    {
      "epoch": 0.29500580720092917,
      "grad_norm": 4.532254695892334,
      "learning_rate": 2.950058072009292e-06,
      "loss": 1.8391,
      "step": 762
    },
    {
      "epoch": 0.2953929539295393,
      "grad_norm": 4.085954189300537,
      "learning_rate": 2.9539295392953933e-06,
      "loss": 1.8699,
      "step": 763
    },
    {
      "epoch": 0.2957801006581494,
      "grad_norm": 5.225602626800537,
      "learning_rate": 2.9578010065814944e-06,
      "loss": 1.8301,
      "step": 764
    },
    {
      "epoch": 0.2961672473867596,
      "grad_norm": 5.530776500701904,
      "learning_rate": 2.9616724738675963e-06,
      "loss": 1.8712,
      "step": 765
    },
    {
      "epoch": 0.2965543941153697,
      "grad_norm": 4.6287841796875,
      "learning_rate": 2.9655439411536974e-06,
      "loss": 2.0258,
      "step": 766
    },
    {
      "epoch": 0.2969415408439799,
      "grad_norm": 6.171422004699707,
      "learning_rate": 2.9694154084397993e-06,
      "loss": 2.031,
      "step": 767
    },
    {
      "epoch": 0.29732868757259,
      "grad_norm": 5.255777359008789,
      "learning_rate": 2.9732868757259004e-06,
      "loss": 1.8541,
      "step": 768
    },
    {
      "epoch": 0.29771583430120013,
      "grad_norm": 4.348309516906738,
      "learning_rate": 2.9771583430120014e-06,
      "loss": 1.9124,
      "step": 769
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 5.602612495422363,
      "learning_rate": 2.9810298102981034e-06,
      "loss": 1.8359,
      "step": 770
    },
    {
      "epoch": 0.29849012775842043,
      "grad_norm": 4.787204265594482,
      "learning_rate": 2.9849012775842044e-06,
      "loss": 1.7659,
      "step": 771
    },
    {
      "epoch": 0.2988772744870306,
      "grad_norm": 5.784604072570801,
      "learning_rate": 2.9887727448703063e-06,
      "loss": 1.7458,
      "step": 772
    },
    {
      "epoch": 0.29926442121564073,
      "grad_norm": 7.5158233642578125,
      "learning_rate": 2.9926442121564074e-06,
      "loss": 1.9343,
      "step": 773
    },
    {
      "epoch": 0.29965156794425085,
      "grad_norm": 4.8036603927612305,
      "learning_rate": 2.996515679442509e-06,
      "loss": 1.9368,
      "step": 774
    },
    {
      "epoch": 0.30003871467286103,
      "grad_norm": 4.685634613037109,
      "learning_rate": 3.0003871467286104e-06,
      "loss": 1.7937,
      "step": 775
    },
    {
      "epoch": 0.30042586140147115,
      "grad_norm": 5.344221591949463,
      "learning_rate": 3.004258614014712e-06,
      "loss": 1.9383,
      "step": 776
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 6.42534065246582,
      "learning_rate": 3.0081300813008134e-06,
      "loss": 1.9676,
      "step": 777
    },
    {
      "epoch": 0.30120015485869145,
      "grad_norm": 6.802953720092773,
      "learning_rate": 3.012001548586915e-06,
      "loss": 1.8353,
      "step": 778
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 6.546514511108398,
      "learning_rate": 3.015873015873016e-06,
      "loss": 2.0512,
      "step": 779
    },
    {
      "epoch": 0.30197444831591175,
      "grad_norm": 6.816506862640381,
      "learning_rate": 3.019744483159118e-06,
      "loss": 1.8282,
      "step": 780
    },
    {
      "epoch": 0.30236159504452187,
      "grad_norm": 10.432145118713379,
      "learning_rate": 3.023615950445219e-06,
      "loss": 1.7776,
      "step": 781
    },
    {
      "epoch": 0.302748741773132,
      "grad_norm": 4.962366104125977,
      "learning_rate": 3.02748741773132e-06,
      "loss": 1.9361,
      "step": 782
    },
    {
      "epoch": 0.30313588850174217,
      "grad_norm": 5.64499568939209,
      "learning_rate": 3.031358885017422e-06,
      "loss": 1.9697,
      "step": 783
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 9.46323013305664,
      "learning_rate": 3.035230352303523e-06,
      "loss": 1.9652,
      "step": 784
    },
    {
      "epoch": 0.30391018195896247,
      "grad_norm": 6.745748996734619,
      "learning_rate": 3.039101819589625e-06,
      "loss": 1.7772,
      "step": 785
    },
    {
      "epoch": 0.3042973286875726,
      "grad_norm": 8.02359390258789,
      "learning_rate": 3.042973286875726e-06,
      "loss": 1.7514,
      "step": 786
    },
    {
      "epoch": 0.3046844754161827,
      "grad_norm": 5.675581455230713,
      "learning_rate": 3.0468447541618275e-06,
      "loss": 1.735,
      "step": 787
    },
    {
      "epoch": 0.3050716221447929,
      "grad_norm": 5.587117671966553,
      "learning_rate": 3.050716221447929e-06,
      "loss": 1.7456,
      "step": 788
    },
    {
      "epoch": 0.305458768873403,
      "grad_norm": 5.306910991668701,
      "learning_rate": 3.0545876887340305e-06,
      "loss": 1.988,
      "step": 789
    },
    {
      "epoch": 0.3058459156020132,
      "grad_norm": 4.185196876525879,
      "learning_rate": 3.058459156020132e-06,
      "loss": 1.9034,
      "step": 790
    },
    {
      "epoch": 0.3062330623306233,
      "grad_norm": 6.3735671043396,
      "learning_rate": 3.0623306233062334e-06,
      "loss": 1.8445,
      "step": 791
    },
    {
      "epoch": 0.30662020905923343,
      "grad_norm": 10.124629020690918,
      "learning_rate": 3.0662020905923345e-06,
      "loss": 1.6863,
      "step": 792
    },
    {
      "epoch": 0.3070073557878436,
      "grad_norm": 6.769291877746582,
      "learning_rate": 3.0700735578784364e-06,
      "loss": 2.0166,
      "step": 793
    },
    {
      "epoch": 0.3073945025164537,
      "grad_norm": 5.670444488525391,
      "learning_rate": 3.0739450251645375e-06,
      "loss": 1.8505,
      "step": 794
    },
    {
      "epoch": 0.3077816492450639,
      "grad_norm": 6.0421881675720215,
      "learning_rate": 3.0778164924506394e-06,
      "loss": 1.9222,
      "step": 795
    },
    {
      "epoch": 0.308168795973674,
      "grad_norm": 6.5627570152282715,
      "learning_rate": 3.0816879597367405e-06,
      "loss": 1.8231,
      "step": 796
    },
    {
      "epoch": 0.30855594270228415,
      "grad_norm": 4.4243645668029785,
      "learning_rate": 3.0855594270228416e-06,
      "loss": 1.7902,
      "step": 797
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 4.4525227546691895,
      "learning_rate": 3.0894308943089435e-06,
      "loss": 1.8462,
      "step": 798
    },
    {
      "epoch": 0.30933023615950445,
      "grad_norm": 4.4406514167785645,
      "learning_rate": 3.0933023615950445e-06,
      "loss": 1.8506,
      "step": 799
    },
    {
      "epoch": 0.3097173828881146,
      "grad_norm": 6.788020610809326,
      "learning_rate": 3.0971738288811465e-06,
      "loss": 1.8197,
      "step": 800
    },
    {
      "epoch": 0.31010452961672474,
      "grad_norm": 4.740174770355225,
      "learning_rate": 3.1010452961672475e-06,
      "loss": 1.8689,
      "step": 801
    },
    {
      "epoch": 0.31049167634533487,
      "grad_norm": 5.3511247634887695,
      "learning_rate": 3.104916763453349e-06,
      "loss": 1.8578,
      "step": 802
    },
    {
      "epoch": 0.31087882307394504,
      "grad_norm": 4.1916704177856445,
      "learning_rate": 3.1087882307394505e-06,
      "loss": 1.9015,
      "step": 803
    },
    {
      "epoch": 0.31126596980255516,
      "grad_norm": 5.017251491546631,
      "learning_rate": 3.112659698025552e-06,
      "loss": 1.8689,
      "step": 804
    },
    {
      "epoch": 0.3116531165311653,
      "grad_norm": 4.858780384063721,
      "learning_rate": 3.116531165311653e-06,
      "loss": 1.8752,
      "step": 805
    },
    {
      "epoch": 0.31204026325977546,
      "grad_norm": 7.455528736114502,
      "learning_rate": 3.120402632597755e-06,
      "loss": 2.0778,
      "step": 806
    },
    {
      "epoch": 0.3124274099883856,
      "grad_norm": 6.000278472900391,
      "learning_rate": 3.124274099883856e-06,
      "loss": 1.8841,
      "step": 807
    },
    {
      "epoch": 0.31281455671699576,
      "grad_norm": 6.417995929718018,
      "learning_rate": 3.128145567169958e-06,
      "loss": 1.9384,
      "step": 808
    },
    {
      "epoch": 0.3132017034456059,
      "grad_norm": 5.796879291534424,
      "learning_rate": 3.132017034456059e-06,
      "loss": 1.7489,
      "step": 809
    },
    {
      "epoch": 0.313588850174216,
      "grad_norm": 5.434488296508789,
      "learning_rate": 3.13588850174216e-06,
      "loss": 1.9463,
      "step": 810
    },
    {
      "epoch": 0.3139759969028262,
      "grad_norm": 4.301883697509766,
      "learning_rate": 3.139759969028262e-06,
      "loss": 1.8658,
      "step": 811
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 5.496555328369141,
      "learning_rate": 3.143631436314363e-06,
      "loss": 1.8655,
      "step": 812
    },
    {
      "epoch": 0.3147502903600465,
      "grad_norm": 6.712944507598877,
      "learning_rate": 3.147502903600465e-06,
      "loss": 1.876,
      "step": 813
    },
    {
      "epoch": 0.3151374370886566,
      "grad_norm": 6.929919719696045,
      "learning_rate": 3.151374370886566e-06,
      "loss": 1.942,
      "step": 814
    },
    {
      "epoch": 0.3155245838172667,
      "grad_norm": 7.619915962219238,
      "learning_rate": 3.1552458381726676e-06,
      "loss": 2.0467,
      "step": 815
    },
    {
      "epoch": 0.3159117305458769,
      "grad_norm": 6.238576412200928,
      "learning_rate": 3.159117305458769e-06,
      "loss": 2.0159,
      "step": 816
    },
    {
      "epoch": 0.316298877274487,
      "grad_norm": 6.187562465667725,
      "learning_rate": 3.1629887727448706e-06,
      "loss": 1.8343,
      "step": 817
    },
    {
      "epoch": 0.3166860240030972,
      "grad_norm": 5.7111663818359375,
      "learning_rate": 3.166860240030972e-06,
      "loss": 2.0185,
      "step": 818
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 6.155790328979492,
      "learning_rate": 3.1707317073170736e-06,
      "loss": 1.9357,
      "step": 819
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 6.024950981140137,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 1.8469,
      "step": 820
    },
    {
      "epoch": 0.3178474641889276,
      "grad_norm": 4.766077041625977,
      "learning_rate": 3.1784746418892766e-06,
      "loss": 1.9448,
      "step": 821
    },
    {
      "epoch": 0.31823461091753774,
      "grad_norm": 5.4873504638671875,
      "learning_rate": 3.1823461091753776e-06,
      "loss": 1.7459,
      "step": 822
    },
    {
      "epoch": 0.31862175764614786,
      "grad_norm": 6.2112579345703125,
      "learning_rate": 3.1862175764614787e-06,
      "loss": 1.8522,
      "step": 823
    },
    {
      "epoch": 0.31900890437475804,
      "grad_norm": 5.348487377166748,
      "learning_rate": 3.1900890437475806e-06,
      "loss": 1.8355,
      "step": 824
    },
    {
      "epoch": 0.31939605110336816,
      "grad_norm": 4.652820587158203,
      "learning_rate": 3.1939605110336817e-06,
      "loss": 1.8774,
      "step": 825
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 4.153924942016602,
      "learning_rate": 3.1978319783197836e-06,
      "loss": 1.9156,
      "step": 826
    },
    {
      "epoch": 0.32017034456058846,
      "grad_norm": 4.569142818450928,
      "learning_rate": 3.2017034456058847e-06,
      "loss": 1.7949,
      "step": 827
    },
    {
      "epoch": 0.3205574912891986,
      "grad_norm": 7.637098789215088,
      "learning_rate": 3.205574912891986e-06,
      "loss": 1.9284,
      "step": 828
    },
    {
      "epoch": 0.32094463801780876,
      "grad_norm": 4.78095817565918,
      "learning_rate": 3.2094463801780877e-06,
      "loss": 1.8912,
      "step": 829
    },
    {
      "epoch": 0.3213317847464189,
      "grad_norm": 4.840390205383301,
      "learning_rate": 3.213317847464189e-06,
      "loss": 1.9292,
      "step": 830
    },
    {
      "epoch": 0.32171893147502906,
      "grad_norm": 4.632334232330322,
      "learning_rate": 3.2171893147502906e-06,
      "loss": 1.7963,
      "step": 831
    },
    {
      "epoch": 0.3221060782036392,
      "grad_norm": 7.335258960723877,
      "learning_rate": 3.221060782036392e-06,
      "loss": 2.1439,
      "step": 832
    },
    {
      "epoch": 0.3224932249322493,
      "grad_norm": 6.618497848510742,
      "learning_rate": 3.224932249322493e-06,
      "loss": 1.8431,
      "step": 833
    },
    {
      "epoch": 0.3228803716608595,
      "grad_norm": 6.997554779052734,
      "learning_rate": 3.228803716608595e-06,
      "loss": 1.8194,
      "step": 834
    },
    {
      "epoch": 0.3232675183894696,
      "grad_norm": 7.6938018798828125,
      "learning_rate": 3.232675183894696e-06,
      "loss": 2.0969,
      "step": 835
    },
    {
      "epoch": 0.3236546651180798,
      "grad_norm": 6.193465232849121,
      "learning_rate": 3.236546651180798e-06,
      "loss": 1.8314,
      "step": 836
    },
    {
      "epoch": 0.3240418118466899,
      "grad_norm": 6.705842971801758,
      "learning_rate": 3.240418118466899e-06,
      "loss": 1.9696,
      "step": 837
    },
    {
      "epoch": 0.3244289585753,
      "grad_norm": 6.203885555267334,
      "learning_rate": 3.2442895857530007e-06,
      "loss": 1.8961,
      "step": 838
    },
    {
      "epoch": 0.3248161053039102,
      "grad_norm": 8.54855728149414,
      "learning_rate": 3.248161053039102e-06,
      "loss": 1.9223,
      "step": 839
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 5.910970211029053,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 1.9365,
      "step": 840
    },
    {
      "epoch": 0.3255903987611305,
      "grad_norm": 5.887923717498779,
      "learning_rate": 3.255903987611305e-06,
      "loss": 1.766,
      "step": 841
    },
    {
      "epoch": 0.3259775454897406,
      "grad_norm": 5.040539264678955,
      "learning_rate": 3.2597754548974066e-06,
      "loss": 1.9635,
      "step": 842
    },
    {
      "epoch": 0.32636469221835074,
      "grad_norm": 5.9049391746521,
      "learning_rate": 3.2636469221835077e-06,
      "loss": 1.8503,
      "step": 843
    },
    {
      "epoch": 0.3267518389469609,
      "grad_norm": 5.912818431854248,
      "learning_rate": 3.2675183894696096e-06,
      "loss": 1.7372,
      "step": 844
    },
    {
      "epoch": 0.32713898567557104,
      "grad_norm": 5.692553997039795,
      "learning_rate": 3.2713898567557107e-06,
      "loss": 1.8906,
      "step": 845
    },
    {
      "epoch": 0.32752613240418116,
      "grad_norm": 5.645511150360107,
      "learning_rate": 3.2752613240418118e-06,
      "loss": 1.9932,
      "step": 846
    },
    {
      "epoch": 0.32791327913279134,
      "grad_norm": 4.425200462341309,
      "learning_rate": 3.2791327913279137e-06,
      "loss": 1.7631,
      "step": 847
    },
    {
      "epoch": 0.32830042586140146,
      "grad_norm": 5.873213291168213,
      "learning_rate": 3.2830042586140148e-06,
      "loss": 1.9709,
      "step": 848
    },
    {
      "epoch": 0.32868757259001163,
      "grad_norm": 7.371757984161377,
      "learning_rate": 3.2868757259001167e-06,
      "loss": 1.7789,
      "step": 849
    },
    {
      "epoch": 0.32907471931862176,
      "grad_norm": 10.0683012008667,
      "learning_rate": 3.2907471931862177e-06,
      "loss": 1.7267,
      "step": 850
    },
    {
      "epoch": 0.3294618660472319,
      "grad_norm": 6.804142475128174,
      "learning_rate": 3.2946186604723192e-06,
      "loss": 1.991,
      "step": 851
    },
    {
      "epoch": 0.32984901277584205,
      "grad_norm": 5.716935634613037,
      "learning_rate": 3.2984901277584207e-06,
      "loss": 1.8856,
      "step": 852
    },
    {
      "epoch": 0.3302361595044522,
      "grad_norm": 5.3959059715271,
      "learning_rate": 3.3023615950445222e-06,
      "loss": 1.9214,
      "step": 853
    },
    {
      "epoch": 0.33062330623306235,
      "grad_norm": 8.169224739074707,
      "learning_rate": 3.3062330623306237e-06,
      "loss": 1.9224,
      "step": 854
    },
    {
      "epoch": 0.3310104529616725,
      "grad_norm": 5.549602508544922,
      "learning_rate": 3.310104529616725e-06,
      "loss": 1.7604,
      "step": 855
    },
    {
      "epoch": 0.3313975996902826,
      "grad_norm": 5.1741766929626465,
      "learning_rate": 3.3139759969028263e-06,
      "loss": 1.872,
      "step": 856
    },
    {
      "epoch": 0.3317847464188928,
      "grad_norm": 3.778778314590454,
      "learning_rate": 3.317847464188928e-06,
      "loss": 1.8409,
      "step": 857
    },
    {
      "epoch": 0.3321718931475029,
      "grad_norm": 5.4393439292907715,
      "learning_rate": 3.3217189314750293e-06,
      "loss": 1.8688,
      "step": 858
    },
    {
      "epoch": 0.33255903987611307,
      "grad_norm": 5.699974536895752,
      "learning_rate": 3.325590398761131e-06,
      "loss": 1.841,
      "step": 859
    },
    {
      "epoch": 0.3329461866047232,
      "grad_norm": 5.061371326446533,
      "learning_rate": 3.3294618660472323e-06,
      "loss": 1.9313,
      "step": 860
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 4.864750385284424,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.9648,
      "step": 861
    },
    {
      "epoch": 0.3337204800619435,
      "grad_norm": 7.423203468322754,
      "learning_rate": 3.3372048006194352e-06,
      "loss": 1.9577,
      "step": 862
    },
    {
      "epoch": 0.3341076267905536,
      "grad_norm": 6.4587860107421875,
      "learning_rate": 3.3410762679055363e-06,
      "loss": 1.7395,
      "step": 863
    },
    {
      "epoch": 0.3344947735191638,
      "grad_norm": 6.423622131347656,
      "learning_rate": 3.3449477351916382e-06,
      "loss": 1.8121,
      "step": 864
    },
    {
      "epoch": 0.3348819202477739,
      "grad_norm": 6.226582050323486,
      "learning_rate": 3.3488192024777393e-06,
      "loss": 1.8357,
      "step": 865
    },
    {
      "epoch": 0.33526906697638403,
      "grad_norm": 8.454564094543457,
      "learning_rate": 3.352690669763841e-06,
      "loss": 1.9223,
      "step": 866
    },
    {
      "epoch": 0.3356562137049942,
      "grad_norm": 6.044162750244141,
      "learning_rate": 3.3565621370499423e-06,
      "loss": 1.7265,
      "step": 867
    },
    {
      "epoch": 0.33604336043360433,
      "grad_norm": 4.222441673278809,
      "learning_rate": 3.3604336043360438e-06,
      "loss": 1.8772,
      "step": 868
    },
    {
      "epoch": 0.33643050716221445,
      "grad_norm": 4.663129806518555,
      "learning_rate": 3.364305071622145e-06,
      "loss": 1.9725,
      "step": 869
    },
    {
      "epoch": 0.33681765389082463,
      "grad_norm": 6.317265510559082,
      "learning_rate": 3.3681765389082468e-06,
      "loss": 1.7537,
      "step": 870
    },
    {
      "epoch": 0.33720480061943475,
      "grad_norm": 4.505406856536865,
      "learning_rate": 3.372048006194348e-06,
      "loss": 1.8973,
      "step": 871
    },
    {
      "epoch": 0.33759194734804493,
      "grad_norm": 6.944278240203857,
      "learning_rate": 3.3759194734804498e-06,
      "loss": 1.9139,
      "step": 872
    },
    {
      "epoch": 0.33797909407665505,
      "grad_norm": 5.991332530975342,
      "learning_rate": 3.379790940766551e-06,
      "loss": 2.007,
      "step": 873
    },
    {
      "epoch": 0.3383662408052652,
      "grad_norm": 5.562312126159668,
      "learning_rate": 3.383662408052652e-06,
      "loss": 1.9214,
      "step": 874
    },
    {
      "epoch": 0.33875338753387535,
      "grad_norm": 8.87118911743164,
      "learning_rate": 3.387533875338754e-06,
      "loss": 1.906,
      "step": 875
    },
    {
      "epoch": 0.33914053426248547,
      "grad_norm": 5.414921760559082,
      "learning_rate": 3.391405342624855e-06,
      "loss": 1.8234,
      "step": 876
    },
    {
      "epoch": 0.33952768099109565,
      "grad_norm": 6.100900650024414,
      "learning_rate": 3.395276809910957e-06,
      "loss": 1.8379,
      "step": 877
    },
    {
      "epoch": 0.33991482771970577,
      "grad_norm": 6.632890701293945,
      "learning_rate": 3.399148277197058e-06,
      "loss": 1.7059,
      "step": 878
    },
    {
      "epoch": 0.3403019744483159,
      "grad_norm": 5.403982639312744,
      "learning_rate": 3.4030197444831594e-06,
      "loss": 1.9368,
      "step": 879
    },
    {
      "epoch": 0.34068912117692607,
      "grad_norm": 6.948767185211182,
      "learning_rate": 3.406891211769261e-06,
      "loss": 1.9726,
      "step": 880
    },
    {
      "epoch": 0.3410762679055362,
      "grad_norm": 7.1643571853637695,
      "learning_rate": 3.4107626790553623e-06,
      "loss": 1.8223,
      "step": 881
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 4.706746578216553,
      "learning_rate": 3.414634146341464e-06,
      "loss": 1.8573,
      "step": 882
    },
    {
      "epoch": 0.3418505613627565,
      "grad_norm": 8.068329811096191,
      "learning_rate": 3.4185056136275653e-06,
      "loss": 1.7434,
      "step": 883
    },
    {
      "epoch": 0.3422377080913666,
      "grad_norm": 6.023092746734619,
      "learning_rate": 3.4223770809136664e-06,
      "loss": 1.8223,
      "step": 884
    },
    {
      "epoch": 0.3426248548199768,
      "grad_norm": 7.659391403198242,
      "learning_rate": 3.4262485481997683e-06,
      "loss": 1.8335,
      "step": 885
    },
    {
      "epoch": 0.3430120015485869,
      "grad_norm": 6.32168436050415,
      "learning_rate": 3.4301200154858694e-06,
      "loss": 1.8933,
      "step": 886
    },
    {
      "epoch": 0.34339914827719703,
      "grad_norm": 5.008774280548096,
      "learning_rate": 3.4339914827719705e-06,
      "loss": 1.9749,
      "step": 887
    },
    {
      "epoch": 0.3437862950058072,
      "grad_norm": 3.820782423019409,
      "learning_rate": 3.4378629500580724e-06,
      "loss": 1.8132,
      "step": 888
    },
    {
      "epoch": 0.34417344173441733,
      "grad_norm": 10.133663177490234,
      "learning_rate": 3.4417344173441734e-06,
      "loss": 1.9117,
      "step": 889
    },
    {
      "epoch": 0.3445605884630275,
      "grad_norm": 6.351448059082031,
      "learning_rate": 3.4456058846302754e-06,
      "loss": 1.9423,
      "step": 890
    },
    {
      "epoch": 0.34494773519163763,
      "grad_norm": 5.801458835601807,
      "learning_rate": 3.4494773519163764e-06,
      "loss": 1.9876,
      "step": 891
    },
    {
      "epoch": 0.34533488192024775,
      "grad_norm": 5.4739460945129395,
      "learning_rate": 3.453348819202478e-06,
      "loss": 1.8503,
      "step": 892
    },
    {
      "epoch": 0.3457220286488579,
      "grad_norm": 5.154107093811035,
      "learning_rate": 3.4572202864885794e-06,
      "loss": 1.8281,
      "step": 893
    },
    {
      "epoch": 0.34610917537746805,
      "grad_norm": 5.932253837585449,
      "learning_rate": 3.461091753774681e-06,
      "loss": 1.8432,
      "step": 894
    },
    {
      "epoch": 0.3464963221060782,
      "grad_norm": 10.667448043823242,
      "learning_rate": 3.4649632210607824e-06,
      "loss": 1.7225,
      "step": 895
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 7.351673126220703,
      "learning_rate": 3.468834688346884e-06,
      "loss": 1.7891,
      "step": 896
    },
    {
      "epoch": 0.34727061556329847,
      "grad_norm": 5.1575751304626465,
      "learning_rate": 3.472706155632985e-06,
      "loss": 1.8494,
      "step": 897
    },
    {
      "epoch": 0.34765776229190865,
      "grad_norm": 5.02217960357666,
      "learning_rate": 3.476577622919087e-06,
      "loss": 1.9592,
      "step": 898
    },
    {
      "epoch": 0.34804490902051877,
      "grad_norm": 6.5551886558532715,
      "learning_rate": 3.480449090205188e-06,
      "loss": 1.8134,
      "step": 899
    },
    {
      "epoch": 0.34843205574912894,
      "grad_norm": 10.820032119750977,
      "learning_rate": 3.48432055749129e-06,
      "loss": 1.9588,
      "step": 900
    },
    {
      "epoch": 0.34881920247773907,
      "grad_norm": 6.451162815093994,
      "learning_rate": 3.488192024777391e-06,
      "loss": 2.07,
      "step": 901
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 6.635438919067383,
      "learning_rate": 3.492063492063492e-06,
      "loss": 1.8382,
      "step": 902
    },
    {
      "epoch": 0.34959349593495936,
      "grad_norm": 7.011789798736572,
      "learning_rate": 3.495934959349594e-06,
      "loss": 1.6766,
      "step": 903
    },
    {
      "epoch": 0.3499806426635695,
      "grad_norm": 9.57318115234375,
      "learning_rate": 3.499806426635695e-06,
      "loss": 1.7279,
      "step": 904
    },
    {
      "epoch": 0.35036778939217966,
      "grad_norm": 6.355260372161865,
      "learning_rate": 3.503677893921797e-06,
      "loss": 1.9439,
      "step": 905
    },
    {
      "epoch": 0.3507549361207898,
      "grad_norm": 6.412200927734375,
      "learning_rate": 3.507549361207898e-06,
      "loss": 1.8079,
      "step": 906
    },
    {
      "epoch": 0.3511420828493999,
      "grad_norm": 6.9061479568481445,
      "learning_rate": 3.5114208284939995e-06,
      "loss": 1.7214,
      "step": 907
    },
    {
      "epoch": 0.3515292295780101,
      "grad_norm": 5.9676313400268555,
      "learning_rate": 3.515292295780101e-06,
      "loss": 1.7231,
      "step": 908
    },
    {
      "epoch": 0.3519163763066202,
      "grad_norm": 7.351244926452637,
      "learning_rate": 3.5191637630662025e-06,
      "loss": 1.9573,
      "step": 909
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 9.965896606445312,
      "learning_rate": 3.5230352303523035e-06,
      "loss": 1.9179,
      "step": 910
    },
    {
      "epoch": 0.3526906697638405,
      "grad_norm": 5.996851921081543,
      "learning_rate": 3.5269066976384055e-06,
      "loss": 1.8246,
      "step": 911
    },
    {
      "epoch": 0.3530778164924506,
      "grad_norm": 5.066661357879639,
      "learning_rate": 3.5307781649245065e-06,
      "loss": 1.8176,
      "step": 912
    },
    {
      "epoch": 0.3534649632210608,
      "grad_norm": 7.239879608154297,
      "learning_rate": 3.5346496322106084e-06,
      "loss": 1.7103,
      "step": 913
    },
    {
      "epoch": 0.3538521099496709,
      "grad_norm": 4.652872085571289,
      "learning_rate": 3.5385210994967095e-06,
      "loss": 1.8752,
      "step": 914
    },
    {
      "epoch": 0.35423925667828104,
      "grad_norm": 8.013227462768555,
      "learning_rate": 3.5423925667828106e-06,
      "loss": 1.9386,
      "step": 915
    },
    {
      "epoch": 0.3546264034068912,
      "grad_norm": 6.250703811645508,
      "learning_rate": 3.5462640340689125e-06,
      "loss": 1.8108,
      "step": 916
    },
    {
      "epoch": 0.35501355013550134,
      "grad_norm": 5.24884557723999,
      "learning_rate": 3.5501355013550136e-06,
      "loss": 1.7778,
      "step": 917
    },
    {
      "epoch": 0.3554006968641115,
      "grad_norm": 5.327972412109375,
      "learning_rate": 3.5540069686411155e-06,
      "loss": 1.8662,
      "step": 918
    },
    {
      "epoch": 0.35578784359272164,
      "grad_norm": 5.019577980041504,
      "learning_rate": 3.5578784359272166e-06,
      "loss": 1.8298,
      "step": 919
    },
    {
      "epoch": 0.35617499032133176,
      "grad_norm": 5.474857330322266,
      "learning_rate": 3.561749903213318e-06,
      "loss": 1.826,
      "step": 920
    },
    {
      "epoch": 0.35656213704994194,
      "grad_norm": 4.48022985458374,
      "learning_rate": 3.5656213704994195e-06,
      "loss": 1.9117,
      "step": 921
    },
    {
      "epoch": 0.35694928377855206,
      "grad_norm": 6.3672776222229,
      "learning_rate": 3.569492837785521e-06,
      "loss": 1.8216,
      "step": 922
    },
    {
      "epoch": 0.35733643050716224,
      "grad_norm": 6.9902849197387695,
      "learning_rate": 3.5733643050716225e-06,
      "loss": 2.0655,
      "step": 923
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 5.389644622802734,
      "learning_rate": 3.577235772357724e-06,
      "loss": 1.917,
      "step": 924
    },
    {
      "epoch": 0.3581107239643825,
      "grad_norm": 6.284972667694092,
      "learning_rate": 3.581107239643825e-06,
      "loss": 1.7176,
      "step": 925
    },
    {
      "epoch": 0.35849787069299266,
      "grad_norm": 7.1447954177856445,
      "learning_rate": 3.584978706929927e-06,
      "loss": 1.8124,
      "step": 926
    },
    {
      "epoch": 0.3588850174216028,
      "grad_norm": 9.958982467651367,
      "learning_rate": 3.588850174216028e-06,
      "loss": 1.9154,
      "step": 927
    },
    {
      "epoch": 0.35927216415021296,
      "grad_norm": 4.640240669250488,
      "learning_rate": 3.59272164150213e-06,
      "loss": 1.8529,
      "step": 928
    },
    {
      "epoch": 0.3596593108788231,
      "grad_norm": 10.382489204406738,
      "learning_rate": 3.596593108788231e-06,
      "loss": 1.9181,
      "step": 929
    },
    {
      "epoch": 0.3600464576074332,
      "grad_norm": 7.03015661239624,
      "learning_rate": 3.600464576074332e-06,
      "loss": 1.9232,
      "step": 930
    },
    {
      "epoch": 0.3604336043360434,
      "grad_norm": 5.814809799194336,
      "learning_rate": 3.604336043360434e-06,
      "loss": 1.863,
      "step": 931
    },
    {
      "epoch": 0.3608207510646535,
      "grad_norm": 6.6736860275268555,
      "learning_rate": 3.608207510646535e-06,
      "loss": 1.8235,
      "step": 932
    },
    {
      "epoch": 0.3612078977932636,
      "grad_norm": 5.057463645935059,
      "learning_rate": 3.6120789779326366e-06,
      "loss": 1.8298,
      "step": 933
    },
    {
      "epoch": 0.3615950445218738,
      "grad_norm": 10.918339729309082,
      "learning_rate": 3.615950445218738e-06,
      "loss": 1.6659,
      "step": 934
    },
    {
      "epoch": 0.3619821912504839,
      "grad_norm": 7.8552398681640625,
      "learning_rate": 3.6198219125048396e-06,
      "loss": 1.9181,
      "step": 935
    },
    {
      "epoch": 0.3623693379790941,
      "grad_norm": 6.742218971252441,
      "learning_rate": 3.623693379790941e-06,
      "loss": 1.9649,
      "step": 936
    },
    {
      "epoch": 0.3627564847077042,
      "grad_norm": 10.614937782287598,
      "learning_rate": 3.6275648470770426e-06,
      "loss": 1.658,
      "step": 937
    },
    {
      "epoch": 0.36314363143631434,
      "grad_norm": 6.994784355163574,
      "learning_rate": 3.6314363143631437e-06,
      "loss": 1.7901,
      "step": 938
    },
    {
      "epoch": 0.3635307781649245,
      "grad_norm": 7.651595592498779,
      "learning_rate": 3.6353077816492456e-06,
      "loss": 1.7745,
      "step": 939
    },
    {
      "epoch": 0.36391792489353464,
      "grad_norm": 7.180861949920654,
      "learning_rate": 3.6391792489353466e-06,
      "loss": 1.7663,
      "step": 940
    },
    {
      "epoch": 0.3643050716221448,
      "grad_norm": 7.757514476776123,
      "learning_rate": 3.6430507162214486e-06,
      "loss": 1.9243,
      "step": 941
    },
    {
      "epoch": 0.36469221835075494,
      "grad_norm": 7.771273612976074,
      "learning_rate": 3.6469221835075496e-06,
      "loss": 1.8037,
      "step": 942
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 6.205393314361572,
      "learning_rate": 3.6507936507936507e-06,
      "loss": 1.8512,
      "step": 943
    },
    {
      "epoch": 0.36546651180797524,
      "grad_norm": 7.9298014640808105,
      "learning_rate": 3.6546651180797526e-06,
      "loss": 2.0097,
      "step": 944
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 7.5006103515625,
      "learning_rate": 3.6585365853658537e-06,
      "loss": 2.0315,
      "step": 945
    },
    {
      "epoch": 0.36624080526519553,
      "grad_norm": 6.117129325866699,
      "learning_rate": 3.6624080526519556e-06,
      "loss": 1.8459,
      "step": 946
    },
    {
      "epoch": 0.36662795199380566,
      "grad_norm": 5.192980766296387,
      "learning_rate": 3.6662795199380567e-06,
      "loss": 1.8411,
      "step": 947
    },
    {
      "epoch": 0.3670150987224158,
      "grad_norm": 7.468575954437256,
      "learning_rate": 3.670150987224158e-06,
      "loss": 1.7749,
      "step": 948
    },
    {
      "epoch": 0.36740224545102595,
      "grad_norm": 10.083757400512695,
      "learning_rate": 3.6740224545102597e-06,
      "loss": 1.7704,
      "step": 949
    },
    {
      "epoch": 0.3677893921796361,
      "grad_norm": 6.403713703155518,
      "learning_rate": 3.677893921796361e-06,
      "loss": 1.8017,
      "step": 950
    },
    {
      "epoch": 0.3681765389082462,
      "grad_norm": 5.674198627471924,
      "learning_rate": 3.6817653890824622e-06,
      "loss": 2.0026,
      "step": 951
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 4.889875411987305,
      "learning_rate": 3.685636856368564e-06,
      "loss": 1.8509,
      "step": 952
    },
    {
      "epoch": 0.3689508323654665,
      "grad_norm": 6.990925312042236,
      "learning_rate": 3.6895083236546652e-06,
      "loss": 1.9853,
      "step": 953
    },
    {
      "epoch": 0.3693379790940767,
      "grad_norm": 11.994502067565918,
      "learning_rate": 3.693379790940767e-06,
      "loss": 1.7516,
      "step": 954
    },
    {
      "epoch": 0.3697251258226868,
      "grad_norm": 9.122444152832031,
      "learning_rate": 3.697251258226868e-06,
      "loss": 2.1057,
      "step": 955
    },
    {
      "epoch": 0.3701122725512969,
      "grad_norm": 7.620613098144531,
      "learning_rate": 3.7011227255129693e-06,
      "loss": 1.7136,
      "step": 956
    },
    {
      "epoch": 0.3704994192799071,
      "grad_norm": 6.132121562957764,
      "learning_rate": 3.704994192799071e-06,
      "loss": 1.9759,
      "step": 957
    },
    {
      "epoch": 0.3708865660085172,
      "grad_norm": 7.0659403800964355,
      "learning_rate": 3.7088656600851723e-06,
      "loss": 1.8104,
      "step": 958
    },
    {
      "epoch": 0.3712737127371274,
      "grad_norm": 6.117478370666504,
      "learning_rate": 3.712737127371274e-06,
      "loss": 1.7564,
      "step": 959
    },
    {
      "epoch": 0.3716608594657375,
      "grad_norm": 6.567720413208008,
      "learning_rate": 3.7166085946573752e-06,
      "loss": 1.8593,
      "step": 960
    },
    {
      "epoch": 0.37204800619434764,
      "grad_norm": 7.8149333000183105,
      "learning_rate": 3.7204800619434767e-06,
      "loss": 1.8281,
      "step": 961
    },
    {
      "epoch": 0.3724351529229578,
      "grad_norm": 8.909214973449707,
      "learning_rate": 3.7243515292295782e-06,
      "loss": 2.0803,
      "step": 962
    },
    {
      "epoch": 0.37282229965156793,
      "grad_norm": 5.039905548095703,
      "learning_rate": 3.7282229965156797e-06,
      "loss": 1.7746,
      "step": 963
    },
    {
      "epoch": 0.3732094463801781,
      "grad_norm": 5.5525641441345215,
      "learning_rate": 3.7320944638017812e-06,
      "loss": 1.826,
      "step": 964
    },
    {
      "epoch": 0.37359659310878823,
      "grad_norm": 11.596717834472656,
      "learning_rate": 3.7359659310878827e-06,
      "loss": 2.1175,
      "step": 965
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 6.0371856689453125,
      "learning_rate": 3.7398373983739838e-06,
      "loss": 1.9787,
      "step": 966
    },
    {
      "epoch": 0.37437088656600853,
      "grad_norm": 6.924212455749512,
      "learning_rate": 3.7437088656600857e-06,
      "loss": 1.8907,
      "step": 967
    },
    {
      "epoch": 0.37475803329461865,
      "grad_norm": 7.629073619842529,
      "learning_rate": 3.7475803329461868e-06,
      "loss": 1.9683,
      "step": 968
    },
    {
      "epoch": 0.37514518002322883,
      "grad_norm": 8.482142448425293,
      "learning_rate": 3.7514518002322887e-06,
      "loss": 1.7534,
      "step": 969
    },
    {
      "epoch": 0.37553232675183895,
      "grad_norm": 4.847962379455566,
      "learning_rate": 3.7553232675183898e-06,
      "loss": 1.9211,
      "step": 970
    },
    {
      "epoch": 0.3759194734804491,
      "grad_norm": 8.615931510925293,
      "learning_rate": 3.7591947348044912e-06,
      "loss": 1.9173,
      "step": 971
    },
    {
      "epoch": 0.37630662020905925,
      "grad_norm": 4.532628536224365,
      "learning_rate": 3.7630662020905927e-06,
      "loss": 1.8191,
      "step": 972
    },
    {
      "epoch": 0.37669376693766937,
      "grad_norm": 7.408926486968994,
      "learning_rate": 3.7669376693766942e-06,
      "loss": 1.9242,
      "step": 973
    },
    {
      "epoch": 0.3770809136662795,
      "grad_norm": 6.459501266479492,
      "learning_rate": 3.7708091366627953e-06,
      "loss": 1.8151,
      "step": 974
    },
    {
      "epoch": 0.37746806039488967,
      "grad_norm": 4.728452682495117,
      "learning_rate": 3.7746806039488972e-06,
      "loss": 1.8509,
      "step": 975
    },
    {
      "epoch": 0.3778552071234998,
      "grad_norm": 6.875911712646484,
      "learning_rate": 3.7785520712349983e-06,
      "loss": 1.8357,
      "step": 976
    },
    {
      "epoch": 0.37824235385210997,
      "grad_norm": 6.194357872009277,
      "learning_rate": 3.7824235385211e-06,
      "loss": 1.8507,
      "step": 977
    },
    {
      "epoch": 0.3786295005807201,
      "grad_norm": 6.441167831420898,
      "learning_rate": 3.7862950058072013e-06,
      "loss": 1.8902,
      "step": 978
    },
    {
      "epoch": 0.3790166473093302,
      "grad_norm": 7.9552693367004395,
      "learning_rate": 3.7901664730933023e-06,
      "loss": 1.7467,
      "step": 979
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 4.351224422454834,
      "learning_rate": 3.7940379403794043e-06,
      "loss": 1.9513,
      "step": 980
    },
    {
      "epoch": 0.3797909407665505,
      "grad_norm": 5.030082702636719,
      "learning_rate": 3.7979094076655053e-06,
      "loss": 1.9397,
      "step": 981
    },
    {
      "epoch": 0.3801780874951607,
      "grad_norm": 6.6347737312316895,
      "learning_rate": 3.8017808749516073e-06,
      "loss": 1.826,
      "step": 982
    },
    {
      "epoch": 0.3805652342237708,
      "grad_norm": 5.957674503326416,
      "learning_rate": 3.8056523422377083e-06,
      "loss": 1.8744,
      "step": 983
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 7.484006404876709,
      "learning_rate": 3.80952380952381e-06,
      "loss": 1.9361,
      "step": 984
    },
    {
      "epoch": 0.3813395276809911,
      "grad_norm": 5.486177921295166,
      "learning_rate": 3.8133952768099113e-06,
      "loss": 1.7396,
      "step": 985
    },
    {
      "epoch": 0.38172667440960123,
      "grad_norm": 6.526432514190674,
      "learning_rate": 3.817266744096012e-06,
      "loss": 1.805,
      "step": 986
    },
    {
      "epoch": 0.3821138211382114,
      "grad_norm": 6.485808372497559,
      "learning_rate": 3.821138211382115e-06,
      "loss": 1.8467,
      "step": 987
    },
    {
      "epoch": 0.38250096786682153,
      "grad_norm": 8.187198638916016,
      "learning_rate": 3.825009678668215e-06,
      "loss": 1.8837,
      "step": 988
    },
    {
      "epoch": 0.38288811459543165,
      "grad_norm": 6.070324897766113,
      "learning_rate": 3.828881145954317e-06,
      "loss": 1.8484,
      "step": 989
    },
    {
      "epoch": 0.3832752613240418,
      "grad_norm": 7.754283905029297,
      "learning_rate": 3.832752613240418e-06,
      "loss": 1.7662,
      "step": 990
    },
    {
      "epoch": 0.38366240805265195,
      "grad_norm": 5.667147159576416,
      "learning_rate": 3.83662408052652e-06,
      "loss": 1.7702,
      "step": 991
    },
    {
      "epoch": 0.3840495547812621,
      "grad_norm": 4.315986633300781,
      "learning_rate": 3.840495547812621e-06,
      "loss": 1.8675,
      "step": 992
    },
    {
      "epoch": 0.38443670150987225,
      "grad_norm": 5.951976776123047,
      "learning_rate": 3.844367015098723e-06,
      "loss": 1.8134,
      "step": 993
    },
    {
      "epoch": 0.38482384823848237,
      "grad_norm": 4.930840015411377,
      "learning_rate": 3.848238482384824e-06,
      "loss": 1.7918,
      "step": 994
    },
    {
      "epoch": 0.38521099496709255,
      "grad_norm": 6.285064697265625,
      "learning_rate": 3.852109949670926e-06,
      "loss": 1.9145,
      "step": 995
    },
    {
      "epoch": 0.38559814169570267,
      "grad_norm": 7.371286392211914,
      "learning_rate": 3.855981416957027e-06,
      "loss": 1.9535,
      "step": 996
    },
    {
      "epoch": 0.3859852884243128,
      "grad_norm": 6.694183826446533,
      "learning_rate": 3.859852884243128e-06,
      "loss": 1.7954,
      "step": 997
    },
    {
      "epoch": 0.38637243515292297,
      "grad_norm": 6.17716121673584,
      "learning_rate": 3.86372435152923e-06,
      "loss": 1.8989,
      "step": 998
    },
    {
      "epoch": 0.3867595818815331,
      "grad_norm": 4.890300273895264,
      "learning_rate": 3.867595818815331e-06,
      "loss": 1.8554,
      "step": 999
    },
    {
      "epoch": 0.38714672861014326,
      "grad_norm": 6.994903564453125,
      "learning_rate": 3.871467286101433e-06,
      "loss": 1.755,
      "step": 1000
    },
    {
      "epoch": 0.3875338753387534,
      "grad_norm": 6.921496868133545,
      "learning_rate": 3.875338753387534e-06,
      "loss": 1.8091,
      "step": 1001
    },
    {
      "epoch": 0.3879210220673635,
      "grad_norm": 10.440444946289062,
      "learning_rate": 3.8792102206736354e-06,
      "loss": 1.8656,
      "step": 1002
    },
    {
      "epoch": 0.3883081687959737,
      "grad_norm": 7.185372352600098,
      "learning_rate": 3.883081687959737e-06,
      "loss": 1.8885,
      "step": 1003
    },
    {
      "epoch": 0.3886953155245838,
      "grad_norm": 4.347341060638428,
      "learning_rate": 3.886953155245838e-06,
      "loss": 1.8515,
      "step": 1004
    },
    {
      "epoch": 0.389082462253194,
      "grad_norm": 10.460396766662598,
      "learning_rate": 3.89082462253194e-06,
      "loss": 1.914,
      "step": 1005
    },
    {
      "epoch": 0.3894696089818041,
      "grad_norm": 5.536220550537109,
      "learning_rate": 3.894696089818041e-06,
      "loss": 1.7757,
      "step": 1006
    },
    {
      "epoch": 0.3898567557104142,
      "grad_norm": 4.700542449951172,
      "learning_rate": 3.898567557104143e-06,
      "loss": 1.7862,
      "step": 1007
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 6.4426374435424805,
      "learning_rate": 3.902439024390244e-06,
      "loss": 1.9703,
      "step": 1008
    },
    {
      "epoch": 0.3906310491676345,
      "grad_norm": 7.4468817710876465,
      "learning_rate": 3.906310491676346e-06,
      "loss": 1.9811,
      "step": 1009
    },
    {
      "epoch": 0.3910181958962447,
      "grad_norm": 7.305410861968994,
      "learning_rate": 3.910181958962447e-06,
      "loss": 1.8045,
      "step": 1010
    },
    {
      "epoch": 0.3914053426248548,
      "grad_norm": 7.314209938049316,
      "learning_rate": 3.914053426248549e-06,
      "loss": 1.8096,
      "step": 1011
    },
    {
      "epoch": 0.39179248935346495,
      "grad_norm": 6.130899429321289,
      "learning_rate": 3.9179248935346495e-06,
      "loss": 1.839,
      "step": 1012
    },
    {
      "epoch": 0.3921796360820751,
      "grad_norm": 6.605877876281738,
      "learning_rate": 3.921796360820752e-06,
      "loss": 1.8013,
      "step": 1013
    },
    {
      "epoch": 0.39256678281068524,
      "grad_norm": 9.672568321228027,
      "learning_rate": 3.9256678281068525e-06,
      "loss": 1.8679,
      "step": 1014
    },
    {
      "epoch": 0.39295392953929537,
      "grad_norm": 7.296731948852539,
      "learning_rate": 3.929539295392954e-06,
      "loss": 1.8086,
      "step": 1015
    },
    {
      "epoch": 0.39334107626790554,
      "grad_norm": 7.263525485992432,
      "learning_rate": 3.9334107626790555e-06,
      "loss": 1.7227,
      "step": 1016
    },
    {
      "epoch": 0.39372822299651566,
      "grad_norm": 8.393779754638672,
      "learning_rate": 3.937282229965157e-06,
      "loss": 1.8701,
      "step": 1017
    },
    {
      "epoch": 0.39411536972512584,
      "grad_norm": 7.498469829559326,
      "learning_rate": 3.9411536972512585e-06,
      "loss": 1.9127,
      "step": 1018
    },
    {
      "epoch": 0.39450251645373596,
      "grad_norm": 7.705573558807373,
      "learning_rate": 3.94502516453736e-06,
      "loss": 1.8794,
      "step": 1019
    },
    {
      "epoch": 0.3948896631823461,
      "grad_norm": 5.393834114074707,
      "learning_rate": 3.9488966318234615e-06,
      "loss": 1.776,
      "step": 1020
    },
    {
      "epoch": 0.39527680991095626,
      "grad_norm": 6.81558895111084,
      "learning_rate": 3.952768099109563e-06,
      "loss": 1.9436,
      "step": 1021
    },
    {
      "epoch": 0.3956639566395664,
      "grad_norm": 10.730734825134277,
      "learning_rate": 3.9566395663956644e-06,
      "loss": 1.9126,
      "step": 1022
    },
    {
      "epoch": 0.39605110336817656,
      "grad_norm": 4.824930667877197,
      "learning_rate": 3.960511033681766e-06,
      "loss": 1.9485,
      "step": 1023
    },
    {
      "epoch": 0.3964382500967867,
      "grad_norm": 7.289262771606445,
      "learning_rate": 3.9643825009678674e-06,
      "loss": 1.7659,
      "step": 1024
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 8.324384689331055,
      "learning_rate": 3.968253968253968e-06,
      "loss": 1.822,
      "step": 1025
    },
    {
      "epoch": 0.397212543554007,
      "grad_norm": 7.1331682205200195,
      "learning_rate": 3.97212543554007e-06,
      "loss": 1.7658,
      "step": 1026
    },
    {
      "epoch": 0.3975996902826171,
      "grad_norm": 6.0450873374938965,
      "learning_rate": 3.975996902826171e-06,
      "loss": 1.9733,
      "step": 1027
    },
    {
      "epoch": 0.3979868370112273,
      "grad_norm": 8.693906784057617,
      "learning_rate": 3.979868370112273e-06,
      "loss": 2.041,
      "step": 1028
    },
    {
      "epoch": 0.3983739837398374,
      "grad_norm": 6.79072904586792,
      "learning_rate": 3.983739837398374e-06,
      "loss": 1.8285,
      "step": 1029
    },
    {
      "epoch": 0.3987611304684475,
      "grad_norm": 6.792149066925049,
      "learning_rate": 3.9876113046844755e-06,
      "loss": 1.6992,
      "step": 1030
    },
    {
      "epoch": 0.3991482771970577,
      "grad_norm": 10.180524826049805,
      "learning_rate": 3.991482771970577e-06,
      "loss": 1.7537,
      "step": 1031
    },
    {
      "epoch": 0.3995354239256678,
      "grad_norm": 6.90080451965332,
      "learning_rate": 3.9953542392566785e-06,
      "loss": 1.8543,
      "step": 1032
    },
    {
      "epoch": 0.399922570654278,
      "grad_norm": 8.782181739807129,
      "learning_rate": 3.99922570654278e-06,
      "loss": 1.9359,
      "step": 1033
    },
    {
      "epoch": 0.4003097173828881,
      "grad_norm": 4.588636875152588,
      "learning_rate": 4.0030971738288815e-06,
      "loss": 1.854,
      "step": 1034
    },
    {
      "epoch": 0.40069686411149824,
      "grad_norm": 5.694295406341553,
      "learning_rate": 4.006968641114983e-06,
      "loss": 1.9144,
      "step": 1035
    },
    {
      "epoch": 0.4010840108401084,
      "grad_norm": 5.34428071975708,
      "learning_rate": 4.0108401084010845e-06,
      "loss": 1.9145,
      "step": 1036
    },
    {
      "epoch": 0.40147115756871854,
      "grad_norm": 8.83871078491211,
      "learning_rate": 4.014711575687186e-06,
      "loss": 1.6739,
      "step": 1037
    },
    {
      "epoch": 0.40185830429732866,
      "grad_norm": 5.622236251831055,
      "learning_rate": 4.018583042973287e-06,
      "loss": 1.8569,
      "step": 1038
    },
    {
      "epoch": 0.40224545102593884,
      "grad_norm": 9.09410285949707,
      "learning_rate": 4.022454510259389e-06,
      "loss": 1.7797,
      "step": 1039
    },
    {
      "epoch": 0.40263259775454896,
      "grad_norm": 6.6376495361328125,
      "learning_rate": 4.02632597754549e-06,
      "loss": 1.9282,
      "step": 1040
    },
    {
      "epoch": 0.40301974448315914,
      "grad_norm": 6.3174028396606445,
      "learning_rate": 4.030197444831592e-06,
      "loss": 1.8154,
      "step": 1041
    },
    {
      "epoch": 0.40340689121176926,
      "grad_norm": 5.422191143035889,
      "learning_rate": 4.034068912117693e-06,
      "loss": 1.7916,
      "step": 1042
    },
    {
      "epoch": 0.4037940379403794,
      "grad_norm": 7.130708694458008,
      "learning_rate": 4.037940379403794e-06,
      "loss": 1.7154,
      "step": 1043
    },
    {
      "epoch": 0.40418118466898956,
      "grad_norm": 7.735983371734619,
      "learning_rate": 4.041811846689896e-06,
      "loss": 1.9298,
      "step": 1044
    },
    {
      "epoch": 0.4045683313975997,
      "grad_norm": 7.1909990310668945,
      "learning_rate": 4.045683313975997e-06,
      "loss": 1.7682,
      "step": 1045
    },
    {
      "epoch": 0.40495547812620986,
      "grad_norm": 5.878539562225342,
      "learning_rate": 4.049554781262099e-06,
      "loss": 1.8125,
      "step": 1046
    },
    {
      "epoch": 0.40534262485482,
      "grad_norm": 5.125941276550293,
      "learning_rate": 4.0534262485482e-06,
      "loss": 1.9141,
      "step": 1047
    },
    {
      "epoch": 0.4057297715834301,
      "grad_norm": 7.789982795715332,
      "learning_rate": 4.057297715834302e-06,
      "loss": 1.7673,
      "step": 1048
    },
    {
      "epoch": 0.4061169183120403,
      "grad_norm": 6.927610397338867,
      "learning_rate": 4.061169183120403e-06,
      "loss": 1.7171,
      "step": 1049
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 7.074440956115723,
      "learning_rate": 4.0650406504065046e-06,
      "loss": 1.8205,
      "step": 1050
    },
    {
      "epoch": 0.4068912117692606,
      "grad_norm": 10.519023895263672,
      "learning_rate": 4.068912117692606e-06,
      "loss": 1.9074,
      "step": 1051
    },
    {
      "epoch": 0.4072783584978707,
      "grad_norm": 6.531515121459961,
      "learning_rate": 4.0727835849787076e-06,
      "loss": 1.777,
      "step": 1052
    },
    {
      "epoch": 0.4076655052264808,
      "grad_norm": 5.757457733154297,
      "learning_rate": 4.076655052264808e-06,
      "loss": 1.824,
      "step": 1053
    },
    {
      "epoch": 0.408052651955091,
      "grad_norm": 5.75399923324585,
      "learning_rate": 4.0805265195509105e-06,
      "loss": 1.8079,
      "step": 1054
    },
    {
      "epoch": 0.4084397986837011,
      "grad_norm": 11.61396598815918,
      "learning_rate": 4.084397986837011e-06,
      "loss": 2.0023,
      "step": 1055
    },
    {
      "epoch": 0.4088269454123113,
      "grad_norm": 7.138791084289551,
      "learning_rate": 4.0882694541231135e-06,
      "loss": 1.8936,
      "step": 1056
    },
    {
      "epoch": 0.4092140921409214,
      "grad_norm": 7.854433536529541,
      "learning_rate": 4.092140921409214e-06,
      "loss": 1.8226,
      "step": 1057
    },
    {
      "epoch": 0.40960123886953154,
      "grad_norm": 11.446723937988281,
      "learning_rate": 4.096012388695316e-06,
      "loss": 2.0463,
      "step": 1058
    },
    {
      "epoch": 0.4099883855981417,
      "grad_norm": 6.197576999664307,
      "learning_rate": 4.099883855981417e-06,
      "loss": 1.9028,
      "step": 1059
    },
    {
      "epoch": 0.41037553232675184,
      "grad_norm": 8.371200561523438,
      "learning_rate": 4.103755323267519e-06,
      "loss": 1.9448,
      "step": 1060
    },
    {
      "epoch": 0.41076267905536196,
      "grad_norm": 8.966072082519531,
      "learning_rate": 4.10762679055362e-06,
      "loss": 2.0768,
      "step": 1061
    },
    {
      "epoch": 0.41114982578397213,
      "grad_norm": 5.86836576461792,
      "learning_rate": 4.111498257839722e-06,
      "loss": 1.8453,
      "step": 1062
    },
    {
      "epoch": 0.41153697251258226,
      "grad_norm": 6.762277126312256,
      "learning_rate": 4.115369725125823e-06,
      "loss": 1.6987,
      "step": 1063
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 6.063173294067383,
      "learning_rate": 4.119241192411925e-06,
      "loss": 1.8231,
      "step": 1064
    },
    {
      "epoch": 0.41231126596980255,
      "grad_norm": 6.219878673553467,
      "learning_rate": 4.123112659698026e-06,
      "loss": 1.8949,
      "step": 1065
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 11.741094589233398,
      "learning_rate": 4.126984126984127e-06,
      "loss": 1.8869,
      "step": 1066
    },
    {
      "epoch": 0.41308555942702285,
      "grad_norm": 9.171361923217773,
      "learning_rate": 4.130855594270229e-06,
      "loss": 2.1046,
      "step": 1067
    },
    {
      "epoch": 0.413472706155633,
      "grad_norm": 7.2616729736328125,
      "learning_rate": 4.13472706155633e-06,
      "loss": 1.8042,
      "step": 1068
    },
    {
      "epoch": 0.41385985288424315,
      "grad_norm": 7.731119632720947,
      "learning_rate": 4.138598528842432e-06,
      "loss": 1.833,
      "step": 1069
    },
    {
      "epoch": 0.4142469996128533,
      "grad_norm": 7.547011852264404,
      "learning_rate": 4.142469996128533e-06,
      "loss": 1.8938,
      "step": 1070
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 7.011690616607666,
      "learning_rate": 4.146341463414634e-06,
      "loss": 1.836,
      "step": 1071
    },
    {
      "epoch": 0.41502129307007357,
      "grad_norm": 12.237669944763184,
      "learning_rate": 4.150212930700736e-06,
      "loss": 1.8363,
      "step": 1072
    },
    {
      "epoch": 0.4154084397986837,
      "grad_norm": 5.191030979156494,
      "learning_rate": 4.154084397986837e-06,
      "loss": 1.8278,
      "step": 1073
    },
    {
      "epoch": 0.41579558652729387,
      "grad_norm": 7.0229597091674805,
      "learning_rate": 4.157955865272939e-06,
      "loss": 1.6721,
      "step": 1074
    },
    {
      "epoch": 0.416182733255904,
      "grad_norm": 11.699090003967285,
      "learning_rate": 4.16182733255904e-06,
      "loss": 2.0986,
      "step": 1075
    },
    {
      "epoch": 0.4165698799845141,
      "grad_norm": 8.181157112121582,
      "learning_rate": 4.165698799845142e-06,
      "loss": 1.9333,
      "step": 1076
    },
    {
      "epoch": 0.4169570267131243,
      "grad_norm": 7.733731746673584,
      "learning_rate": 4.169570267131243e-06,
      "loss": 1.8594,
      "step": 1077
    },
    {
      "epoch": 0.4173441734417344,
      "grad_norm": 5.851437568664551,
      "learning_rate": 4.173441734417345e-06,
      "loss": 1.822,
      "step": 1078
    },
    {
      "epoch": 0.41773132017034453,
      "grad_norm": 8.455815315246582,
      "learning_rate": 4.177313201703445e-06,
      "loss": 1.895,
      "step": 1079
    },
    {
      "epoch": 0.4181184668989547,
      "grad_norm": 7.864645957946777,
      "learning_rate": 4.181184668989548e-06,
      "loss": 1.7781,
      "step": 1080
    },
    {
      "epoch": 0.41850561362756483,
      "grad_norm": 8.117196083068848,
      "learning_rate": 4.185056136275648e-06,
      "loss": 1.8597,
      "step": 1081
    },
    {
      "epoch": 0.418892760356175,
      "grad_norm": 5.850574970245361,
      "learning_rate": 4.188927603561751e-06,
      "loss": 1.844,
      "step": 1082
    },
    {
      "epoch": 0.41927990708478513,
      "grad_norm": 7.6419525146484375,
      "learning_rate": 4.192799070847851e-06,
      "loss": 1.7653,
      "step": 1083
    },
    {
      "epoch": 0.41966705381339525,
      "grad_norm": 6.343409538269043,
      "learning_rate": 4.196670538133953e-06,
      "loss": 1.7293,
      "step": 1084
    },
    {
      "epoch": 0.42005420054200543,
      "grad_norm": 5.983706951141357,
      "learning_rate": 4.200542005420054e-06,
      "loss": 1.802,
      "step": 1085
    },
    {
      "epoch": 0.42044134727061555,
      "grad_norm": 7.5377092361450195,
      "learning_rate": 4.204413472706156e-06,
      "loss": 1.8169,
      "step": 1086
    },
    {
      "epoch": 0.42082849399922573,
      "grad_norm": 3.8462092876434326,
      "learning_rate": 4.208284939992257e-06,
      "loss": 1.8385,
      "step": 1087
    },
    {
      "epoch": 0.42121564072783585,
      "grad_norm": 5.379147529602051,
      "learning_rate": 4.212156407278359e-06,
      "loss": 1.7416,
      "step": 1088
    },
    {
      "epoch": 0.42160278745644597,
      "grad_norm": 7.648809432983398,
      "learning_rate": 4.21602787456446e-06,
      "loss": 1.8885,
      "step": 1089
    },
    {
      "epoch": 0.42198993418505615,
      "grad_norm": 7.084758758544922,
      "learning_rate": 4.219899341850562e-06,
      "loss": 1.8204,
      "step": 1090
    },
    {
      "epoch": 0.42237708091366627,
      "grad_norm": 8.979840278625488,
      "learning_rate": 4.223770809136663e-06,
      "loss": 2.0371,
      "step": 1091
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 5.410975456237793,
      "learning_rate": 4.227642276422765e-06,
      "loss": 1.8007,
      "step": 1092
    },
    {
      "epoch": 0.42315137437088657,
      "grad_norm": 6.89345645904541,
      "learning_rate": 4.231513743708866e-06,
      "loss": 1.8088,
      "step": 1093
    },
    {
      "epoch": 0.4235385210994967,
      "grad_norm": 5.9134521484375,
      "learning_rate": 4.235385210994967e-06,
      "loss": 1.7834,
      "step": 1094
    },
    {
      "epoch": 0.42392566782810687,
      "grad_norm": 5.889510631561279,
      "learning_rate": 4.239256678281069e-06,
      "loss": 1.7182,
      "step": 1095
    },
    {
      "epoch": 0.424312814556717,
      "grad_norm": 7.260437965393066,
      "learning_rate": 4.24312814556717e-06,
      "loss": 1.7903,
      "step": 1096
    },
    {
      "epoch": 0.42469996128532717,
      "grad_norm": 7.3325419425964355,
      "learning_rate": 4.246999612853272e-06,
      "loss": 1.7872,
      "step": 1097
    },
    {
      "epoch": 0.4250871080139373,
      "grad_norm": 6.902414321899414,
      "learning_rate": 4.250871080139373e-06,
      "loss": 1.8061,
      "step": 1098
    },
    {
      "epoch": 0.4254742547425474,
      "grad_norm": 9.584883689880371,
      "learning_rate": 4.254742547425474e-06,
      "loss": 1.7024,
      "step": 1099
    },
    {
      "epoch": 0.4258614014711576,
      "grad_norm": 7.092770099639893,
      "learning_rate": 4.258614014711576e-06,
      "loss": 1.8645,
      "step": 1100
    },
    {
      "epoch": 0.4262485481997677,
      "grad_norm": 10.605938911437988,
      "learning_rate": 4.262485481997677e-06,
      "loss": 2.0842,
      "step": 1101
    },
    {
      "epoch": 0.42663569492837783,
      "grad_norm": 7.272816181182861,
      "learning_rate": 4.266356949283779e-06,
      "loss": 1.7644,
      "step": 1102
    },
    {
      "epoch": 0.427022841656988,
      "grad_norm": 7.227027893066406,
      "learning_rate": 4.27022841656988e-06,
      "loss": 1.8143,
      "step": 1103
    },
    {
      "epoch": 0.4274099883855981,
      "grad_norm": 6.540299415588379,
      "learning_rate": 4.274099883855982e-06,
      "loss": 1.8091,
      "step": 1104
    },
    {
      "epoch": 0.4277971351142083,
      "grad_norm": 7.267534255981445,
      "learning_rate": 4.277971351142083e-06,
      "loss": 1.8152,
      "step": 1105
    },
    {
      "epoch": 0.4281842818428184,
      "grad_norm": 7.487703323364258,
      "learning_rate": 4.281842818428185e-06,
      "loss": 1.7344,
      "step": 1106
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.099993705749512,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 1.7852,
      "step": 1107
    },
    {
      "epoch": 0.4289585753000387,
      "grad_norm": 7.036769866943359,
      "learning_rate": 4.289585753000388e-06,
      "loss": 1.8193,
      "step": 1108
    },
    {
      "epoch": 0.42934572202864885,
      "grad_norm": 7.88557767868042,
      "learning_rate": 4.2934572202864884e-06,
      "loss": 1.7807,
      "step": 1109
    },
    {
      "epoch": 0.429732868757259,
      "grad_norm": 6.234097480773926,
      "learning_rate": 4.297328687572591e-06,
      "loss": 1.7211,
      "step": 1110
    },
    {
      "epoch": 0.43012001548586914,
      "grad_norm": 12.439189910888672,
      "learning_rate": 4.3012001548586914e-06,
      "loss": 1.6537,
      "step": 1111
    },
    {
      "epoch": 0.43050716221447927,
      "grad_norm": 8.169246673583984,
      "learning_rate": 4.305071622144793e-06,
      "loss": 1.8592,
      "step": 1112
    },
    {
      "epoch": 0.43089430894308944,
      "grad_norm": 4.973944187164307,
      "learning_rate": 4.308943089430894e-06,
      "loss": 1.8289,
      "step": 1113
    },
    {
      "epoch": 0.43128145567169957,
      "grad_norm": 8.851553916931152,
      "learning_rate": 4.312814556716996e-06,
      "loss": 1.9104,
      "step": 1114
    },
    {
      "epoch": 0.43166860240030974,
      "grad_norm": 8.162639617919922,
      "learning_rate": 4.316686024003097e-06,
      "loss": 1.7266,
      "step": 1115
    },
    {
      "epoch": 0.43205574912891986,
      "grad_norm": 8.516408920288086,
      "learning_rate": 4.320557491289199e-06,
      "loss": 1.8425,
      "step": 1116
    },
    {
      "epoch": 0.43244289585753,
      "grad_norm": 8.054035186767578,
      "learning_rate": 4.3244289585753e-06,
      "loss": 1.6528,
      "step": 1117
    },
    {
      "epoch": 0.43283004258614016,
      "grad_norm": 8.632983207702637,
      "learning_rate": 4.328300425861402e-06,
      "loss": 1.7698,
      "step": 1118
    },
    {
      "epoch": 0.4332171893147503,
      "grad_norm": 7.768477439880371,
      "learning_rate": 4.332171893147503e-06,
      "loss": 1.7886,
      "step": 1119
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 7.118156433105469,
      "learning_rate": 4.336043360433605e-06,
      "loss": 1.8985,
      "step": 1120
    },
    {
      "epoch": 0.4339914827719706,
      "grad_norm": 8.817588806152344,
      "learning_rate": 4.339914827719706e-06,
      "loss": 1.7758,
      "step": 1121
    },
    {
      "epoch": 0.4343786295005807,
      "grad_norm": 6.1306962966918945,
      "learning_rate": 4.343786295005807e-06,
      "loss": 1.7747,
      "step": 1122
    },
    {
      "epoch": 0.4347657762291909,
      "grad_norm": 8.011433601379395,
      "learning_rate": 4.347657762291909e-06,
      "loss": 1.822,
      "step": 1123
    },
    {
      "epoch": 0.435152922957801,
      "grad_norm": 7.646282196044922,
      "learning_rate": 4.35152922957801e-06,
      "loss": 1.7612,
      "step": 1124
    },
    {
      "epoch": 0.4355400696864111,
      "grad_norm": 6.916039943695068,
      "learning_rate": 4.3554006968641115e-06,
      "loss": 1.7448,
      "step": 1125
    },
    {
      "epoch": 0.4359272164150213,
      "grad_norm": 8.355910301208496,
      "learning_rate": 4.359272164150213e-06,
      "loss": 1.8102,
      "step": 1126
    },
    {
      "epoch": 0.4363143631436314,
      "grad_norm": 9.373740196228027,
      "learning_rate": 4.3631436314363145e-06,
      "loss": 1.8227,
      "step": 1127
    },
    {
      "epoch": 0.4367015098722416,
      "grad_norm": 8.126326560974121,
      "learning_rate": 4.367015098722416e-06,
      "loss": 1.8974,
      "step": 1128
    },
    {
      "epoch": 0.4370886566008517,
      "grad_norm": 7.3929033279418945,
      "learning_rate": 4.3708865660085175e-06,
      "loss": 1.7919,
      "step": 1129
    },
    {
      "epoch": 0.43747580332946184,
      "grad_norm": 7.894052505493164,
      "learning_rate": 4.374758033294619e-06,
      "loss": 1.9394,
      "step": 1130
    },
    {
      "epoch": 0.437862950058072,
      "grad_norm": 7.260898590087891,
      "learning_rate": 4.3786295005807205e-06,
      "loss": 1.9232,
      "step": 1131
    },
    {
      "epoch": 0.43825009678668214,
      "grad_norm": 6.294471263885498,
      "learning_rate": 4.382500967866822e-06,
      "loss": 1.7289,
      "step": 1132
    },
    {
      "epoch": 0.4386372435152923,
      "grad_norm": 7.442167282104492,
      "learning_rate": 4.3863724351529234e-06,
      "loss": 1.702,
      "step": 1133
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 10.481618881225586,
      "learning_rate": 4.390243902439025e-06,
      "loss": 1.8236,
      "step": 1134
    },
    {
      "epoch": 0.43941153697251256,
      "grad_norm": 10.772294998168945,
      "learning_rate": 4.394115369725126e-06,
      "loss": 1.9302,
      "step": 1135
    },
    {
      "epoch": 0.43979868370112274,
      "grad_norm": 9.008394241333008,
      "learning_rate": 4.397986837011228e-06,
      "loss": 1.8349,
      "step": 1136
    },
    {
      "epoch": 0.44018583042973286,
      "grad_norm": 10.050360679626465,
      "learning_rate": 4.4018583042973286e-06,
      "loss": 1.9866,
      "step": 1137
    },
    {
      "epoch": 0.44057297715834304,
      "grad_norm": 6.3553080558776855,
      "learning_rate": 4.405729771583431e-06,
      "loss": 1.8056,
      "step": 1138
    },
    {
      "epoch": 0.44096012388695316,
      "grad_norm": 10.872370719909668,
      "learning_rate": 4.4096012388695316e-06,
      "loss": 1.8464,
      "step": 1139
    },
    {
      "epoch": 0.4413472706155633,
      "grad_norm": 6.178465366363525,
      "learning_rate": 4.413472706155633e-06,
      "loss": 1.8874,
      "step": 1140
    },
    {
      "epoch": 0.44173441734417346,
      "grad_norm": 5.847365379333496,
      "learning_rate": 4.4173441734417345e-06,
      "loss": 1.7468,
      "step": 1141
    },
    {
      "epoch": 0.4421215640727836,
      "grad_norm": 5.362544059753418,
      "learning_rate": 4.421215640727836e-06,
      "loss": 1.8735,
      "step": 1142
    },
    {
      "epoch": 0.4425087108013937,
      "grad_norm": 7.363213539123535,
      "learning_rate": 4.4250871080139375e-06,
      "loss": 1.7748,
      "step": 1143
    },
    {
      "epoch": 0.4428958575300039,
      "grad_norm": 7.650078296661377,
      "learning_rate": 4.428958575300039e-06,
      "loss": 1.7209,
      "step": 1144
    },
    {
      "epoch": 0.443283004258614,
      "grad_norm": 9.157793045043945,
      "learning_rate": 4.4328300425861405e-06,
      "loss": 1.8174,
      "step": 1145
    },
    {
      "epoch": 0.4436701509872242,
      "grad_norm": 8.010568618774414,
      "learning_rate": 4.436701509872242e-06,
      "loss": 1.6716,
      "step": 1146
    },
    {
      "epoch": 0.4440572977158343,
      "grad_norm": 7.896255016326904,
      "learning_rate": 4.4405729771583435e-06,
      "loss": 1.9857,
      "step": 1147
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 7.9335432052612305,
      "learning_rate": 4.444444444444444e-06,
      "loss": 1.7966,
      "step": 1148
    },
    {
      "epoch": 0.4448315911730546,
      "grad_norm": 6.553643226623535,
      "learning_rate": 4.4483159117305465e-06,
      "loss": 1.8011,
      "step": 1149
    },
    {
      "epoch": 0.4452187379016647,
      "grad_norm": 7.421691417694092,
      "learning_rate": 4.452187379016647e-06,
      "loss": 1.6947,
      "step": 1150
    },
    {
      "epoch": 0.4456058846302749,
      "grad_norm": 5.758768558502197,
      "learning_rate": 4.4560588463027495e-06,
      "loss": 1.8463,
      "step": 1151
    },
    {
      "epoch": 0.445993031358885,
      "grad_norm": 8.436195373535156,
      "learning_rate": 4.45993031358885e-06,
      "loss": 1.7883,
      "step": 1152
    },
    {
      "epoch": 0.44638017808749514,
      "grad_norm": 6.468707084655762,
      "learning_rate": 4.463801780874952e-06,
      "loss": 1.8275,
      "step": 1153
    },
    {
      "epoch": 0.4467673248161053,
      "grad_norm": 5.621915340423584,
      "learning_rate": 4.467673248161053e-06,
      "loss": 1.8272,
      "step": 1154
    },
    {
      "epoch": 0.44715447154471544,
      "grad_norm": 6.155309677124023,
      "learning_rate": 4.471544715447155e-06,
      "loss": 1.8239,
      "step": 1155
    },
    {
      "epoch": 0.4475416182733256,
      "grad_norm": 16.646562576293945,
      "learning_rate": 4.475416182733256e-06,
      "loss": 1.8473,
      "step": 1156
    },
    {
      "epoch": 0.44792876500193574,
      "grad_norm": 10.270953178405762,
      "learning_rate": 4.479287650019358e-06,
      "loss": 2.0731,
      "step": 1157
    },
    {
      "epoch": 0.44831591173054586,
      "grad_norm": 7.724088668823242,
      "learning_rate": 4.483159117305459e-06,
      "loss": 1.8746,
      "step": 1158
    },
    {
      "epoch": 0.44870305845915603,
      "grad_norm": 7.1062140464782715,
      "learning_rate": 4.487030584591561e-06,
      "loss": 1.7333,
      "step": 1159
    },
    {
      "epoch": 0.44909020518776616,
      "grad_norm": 7.381711006164551,
      "learning_rate": 4.490902051877662e-06,
      "loss": 1.7291,
      "step": 1160
    },
    {
      "epoch": 0.44947735191637633,
      "grad_norm": 10.362436294555664,
      "learning_rate": 4.4947735191637636e-06,
      "loss": 1.9623,
      "step": 1161
    },
    {
      "epoch": 0.44986449864498645,
      "grad_norm": 10.222472190856934,
      "learning_rate": 4.498644986449865e-06,
      "loss": 1.6973,
      "step": 1162
    },
    {
      "epoch": 0.4502516453735966,
      "grad_norm": 7.345465183258057,
      "learning_rate": 4.5025164537359666e-06,
      "loss": 1.8314,
      "step": 1163
    },
    {
      "epoch": 0.45063879210220675,
      "grad_norm": 8.868804931640625,
      "learning_rate": 4.506387921022068e-06,
      "loss": 1.7175,
      "step": 1164
    },
    {
      "epoch": 0.4510259388308169,
      "grad_norm": 6.456099033355713,
      "learning_rate": 4.5102593883081695e-06,
      "loss": 1.8393,
      "step": 1165
    },
    {
      "epoch": 0.451413085559427,
      "grad_norm": 8.33167839050293,
      "learning_rate": 4.51413085559427e-06,
      "loss": 1.9257,
      "step": 1166
    },
    {
      "epoch": 0.4518002322880372,
      "grad_norm": 8.079118728637695,
      "learning_rate": 4.5180023228803725e-06,
      "loss": 1.6983,
      "step": 1167
    },
    {
      "epoch": 0.4521873790166473,
      "grad_norm": 6.416520595550537,
      "learning_rate": 4.521873790166473e-06,
      "loss": 1.7987,
      "step": 1168
    },
    {
      "epoch": 0.45257452574525747,
      "grad_norm": 8.079507827758789,
      "learning_rate": 4.5257452574525755e-06,
      "loss": 1.8103,
      "step": 1169
    },
    {
      "epoch": 0.4529616724738676,
      "grad_norm": 14.616253852844238,
      "learning_rate": 4.529616724738676e-06,
      "loss": 2.0165,
      "step": 1170
    },
    {
      "epoch": 0.4533488192024777,
      "grad_norm": 10.729535102844238,
      "learning_rate": 4.533488192024778e-06,
      "loss": 1.8941,
      "step": 1171
    },
    {
      "epoch": 0.4537359659310879,
      "grad_norm": 6.862905502319336,
      "learning_rate": 4.537359659310879e-06,
      "loss": 1.7868,
      "step": 1172
    },
    {
      "epoch": 0.454123112659698,
      "grad_norm": 7.891745567321777,
      "learning_rate": 4.541231126596981e-06,
      "loss": 1.7899,
      "step": 1173
    },
    {
      "epoch": 0.4545102593883082,
      "grad_norm": 7.428366661071777,
      "learning_rate": 4.545102593883082e-06,
      "loss": 1.718,
      "step": 1174
    },
    {
      "epoch": 0.4548974061169183,
      "grad_norm": 9.85200023651123,
      "learning_rate": 4.548974061169184e-06,
      "loss": 1.7185,
      "step": 1175
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 7.832864761352539,
      "learning_rate": 4.552845528455285e-06,
      "loss": 1.7691,
      "step": 1176
    },
    {
      "epoch": 0.4556716995741386,
      "grad_norm": 8.443107604980469,
      "learning_rate": 4.556716995741387e-06,
      "loss": 2.232,
      "step": 1177
    },
    {
      "epoch": 0.45605884630274873,
      "grad_norm": 8.465107917785645,
      "learning_rate": 4.560588463027488e-06,
      "loss": 1.8328,
      "step": 1178
    },
    {
      "epoch": 0.4564459930313589,
      "grad_norm": 7.087314128875732,
      "learning_rate": 4.56445993031359e-06,
      "loss": 1.7479,
      "step": 1179
    },
    {
      "epoch": 0.45683313975996903,
      "grad_norm": 7.591129779815674,
      "learning_rate": 4.568331397599691e-06,
      "loss": 1.7046,
      "step": 1180
    },
    {
      "epoch": 0.45722028648857915,
      "grad_norm": 5.6059651374816895,
      "learning_rate": 4.572202864885792e-06,
      "loss": 1.7802,
      "step": 1181
    },
    {
      "epoch": 0.45760743321718933,
      "grad_norm": 8.291529655456543,
      "learning_rate": 4.576074332171894e-06,
      "loss": 1.8031,
      "step": 1182
    },
    {
      "epoch": 0.45799457994579945,
      "grad_norm": 7.791135311126709,
      "learning_rate": 4.579945799457995e-06,
      "loss": 1.8618,
      "step": 1183
    },
    {
      "epoch": 0.45838172667440963,
      "grad_norm": 7.984649658203125,
      "learning_rate": 4.583817266744097e-06,
      "loss": 1.7971,
      "step": 1184
    },
    {
      "epoch": 0.45876887340301975,
      "grad_norm": 11.637068748474121,
      "learning_rate": 4.587688734030198e-06,
      "loss": 2.0359,
      "step": 1185
    },
    {
      "epoch": 0.45915602013162987,
      "grad_norm": 11.001853942871094,
      "learning_rate": 4.591560201316299e-06,
      "loss": 1.8609,
      "step": 1186
    },
    {
      "epoch": 0.45954316686024005,
      "grad_norm": 11.171842575073242,
      "learning_rate": 4.595431668602401e-06,
      "loss": 1.7055,
      "step": 1187
    },
    {
      "epoch": 0.45993031358885017,
      "grad_norm": 5.497411251068115,
      "learning_rate": 4.599303135888502e-06,
      "loss": 1.8262,
      "step": 1188
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 6.827698230743408,
      "learning_rate": 4.603174603174604e-06,
      "loss": 1.7899,
      "step": 1189
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 9.068109512329102,
      "learning_rate": 4.607046070460705e-06,
      "loss": 1.8165,
      "step": 1190
    },
    {
      "epoch": 0.4610917537746806,
      "grad_norm": 8.095277786254883,
      "learning_rate": 4.610917537746807e-06,
      "loss": 1.756,
      "step": 1191
    },
    {
      "epoch": 0.46147890050329077,
      "grad_norm": 10.41849136352539,
      "learning_rate": 4.614789005032908e-06,
      "loss": 1.7962,
      "step": 1192
    },
    {
      "epoch": 0.4618660472319009,
      "grad_norm": 7.548772811889648,
      "learning_rate": 4.61866047231901e-06,
      "loss": 1.7278,
      "step": 1193
    },
    {
      "epoch": 0.462253193960511,
      "grad_norm": 6.841695785522461,
      "learning_rate": 4.62253193960511e-06,
      "loss": 1.7784,
      "step": 1194
    },
    {
      "epoch": 0.4626403406891212,
      "grad_norm": 8.158512115478516,
      "learning_rate": 4.626403406891213e-06,
      "loss": 1.8571,
      "step": 1195
    },
    {
      "epoch": 0.4630274874177313,
      "grad_norm": 5.224759578704834,
      "learning_rate": 4.630274874177313e-06,
      "loss": 1.8073,
      "step": 1196
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 7.086298942565918,
      "learning_rate": 4.634146341463416e-06,
      "loss": 1.6694,
      "step": 1197
    },
    {
      "epoch": 0.4638017808749516,
      "grad_norm": 7.74200963973999,
      "learning_rate": 4.638017808749516e-06,
      "loss": 1.7286,
      "step": 1198
    },
    {
      "epoch": 0.46418892760356173,
      "grad_norm": 9.289406776428223,
      "learning_rate": 4.641889276035618e-06,
      "loss": 1.6876,
      "step": 1199
    },
    {
      "epoch": 0.4645760743321719,
      "grad_norm": 6.77566385269165,
      "learning_rate": 4.645760743321719e-06,
      "loss": 1.7614,
      "step": 1200
    },
    {
      "epoch": 0.46496322106078203,
      "grad_norm": 7.472706317901611,
      "learning_rate": 4.649632210607821e-06,
      "loss": 1.8944,
      "step": 1201
    },
    {
      "epoch": 0.4653503677893922,
      "grad_norm": 9.37161636352539,
      "learning_rate": 4.653503677893922e-06,
      "loss": 1.7653,
      "step": 1202
    },
    {
      "epoch": 0.4657375145180023,
      "grad_norm": 8.697610855102539,
      "learning_rate": 4.657375145180024e-06,
      "loss": 1.7986,
      "step": 1203
    },
    {
      "epoch": 0.46612466124661245,
      "grad_norm": 9.745418548583984,
      "learning_rate": 4.661246612466125e-06,
      "loss": 2.2016,
      "step": 1204
    },
    {
      "epoch": 0.4665118079752226,
      "grad_norm": 7.772027015686035,
      "learning_rate": 4.665118079752227e-06,
      "loss": 1.8722,
      "step": 1205
    },
    {
      "epoch": 0.46689895470383275,
      "grad_norm": 8.565866470336914,
      "learning_rate": 4.668989547038328e-06,
      "loss": 1.7909,
      "step": 1206
    },
    {
      "epoch": 0.46728610143244287,
      "grad_norm": 5.965920925140381,
      "learning_rate": 4.672861014324429e-06,
      "loss": 1.76,
      "step": 1207
    },
    {
      "epoch": 0.46767324816105305,
      "grad_norm": 11.530014991760254,
      "learning_rate": 4.676732481610531e-06,
      "loss": 1.8608,
      "step": 1208
    },
    {
      "epoch": 0.46806039488966317,
      "grad_norm": 6.076766490936279,
      "learning_rate": 4.680603948896632e-06,
      "loss": 1.7292,
      "step": 1209
    },
    {
      "epoch": 0.46844754161827334,
      "grad_norm": 8.431111335754395,
      "learning_rate": 4.684475416182734e-06,
      "loss": 1.7818,
      "step": 1210
    },
    {
      "epoch": 0.46883468834688347,
      "grad_norm": 7.297515869140625,
      "learning_rate": 4.688346883468835e-06,
      "loss": 1.7175,
      "step": 1211
    },
    {
      "epoch": 0.4692218350754936,
      "grad_norm": 12.616189956665039,
      "learning_rate": 4.692218350754936e-06,
      "loss": 1.8794,
      "step": 1212
    },
    {
      "epoch": 0.46960898180410376,
      "grad_norm": 6.422956466674805,
      "learning_rate": 4.696089818041038e-06,
      "loss": 1.6784,
      "step": 1213
    },
    {
      "epoch": 0.4699961285327139,
      "grad_norm": 9.145773887634277,
      "learning_rate": 4.699961285327139e-06,
      "loss": 1.8095,
      "step": 1214
    },
    {
      "epoch": 0.47038327526132406,
      "grad_norm": 9.226818084716797,
      "learning_rate": 4.703832752613241e-06,
      "loss": 1.7219,
      "step": 1215
    },
    {
      "epoch": 0.4707704219899342,
      "grad_norm": 6.9555158615112305,
      "learning_rate": 4.707704219899342e-06,
      "loss": 1.7958,
      "step": 1216
    },
    {
      "epoch": 0.4711575687185443,
      "grad_norm": 8.154215812683105,
      "learning_rate": 4.711575687185444e-06,
      "loss": 1.8573,
      "step": 1217
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 5.027792453765869,
      "learning_rate": 4.715447154471545e-06,
      "loss": 1.7501,
      "step": 1218
    },
    {
      "epoch": 0.4719318621757646,
      "grad_norm": 7.6340413093566895,
      "learning_rate": 4.719318621757647e-06,
      "loss": 1.8114,
      "step": 1219
    },
    {
      "epoch": 0.4723190089043748,
      "grad_norm": 8.009791374206543,
      "learning_rate": 4.723190089043748e-06,
      "loss": 1.7774,
      "step": 1220
    },
    {
      "epoch": 0.4727061556329849,
      "grad_norm": 5.859385967254639,
      "learning_rate": 4.72706155632985e-06,
      "loss": 1.7764,
      "step": 1221
    },
    {
      "epoch": 0.473093302361595,
      "grad_norm": 6.946610450744629,
      "learning_rate": 4.7309330236159504e-06,
      "loss": 1.754,
      "step": 1222
    },
    {
      "epoch": 0.4734804490902052,
      "grad_norm": 9.399944305419922,
      "learning_rate": 4.734804490902053e-06,
      "loss": 1.8098,
      "step": 1223
    },
    {
      "epoch": 0.4738675958188153,
      "grad_norm": 6.630936145782471,
      "learning_rate": 4.738675958188153e-06,
      "loss": 1.8501,
      "step": 1224
    },
    {
      "epoch": 0.4742547425474255,
      "grad_norm": 11.695547103881836,
      "learning_rate": 4.742547425474256e-06,
      "loss": 1.6851,
      "step": 1225
    },
    {
      "epoch": 0.4746418892760356,
      "grad_norm": 8.159245491027832,
      "learning_rate": 4.746418892760356e-06,
      "loss": 2.091,
      "step": 1226
    },
    {
      "epoch": 0.47502903600464574,
      "grad_norm": 6.82175350189209,
      "learning_rate": 4.750290360046458e-06,
      "loss": 1.8104,
      "step": 1227
    },
    {
      "epoch": 0.4754161827332559,
      "grad_norm": 6.956649303436279,
      "learning_rate": 4.754161827332559e-06,
      "loss": 1.7584,
      "step": 1228
    },
    {
      "epoch": 0.47580332946186604,
      "grad_norm": 8.740132331848145,
      "learning_rate": 4.758033294618661e-06,
      "loss": 1.8276,
      "step": 1229
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 6.581330299377441,
      "learning_rate": 4.761904761904762e-06,
      "loss": 1.7654,
      "step": 1230
    },
    {
      "epoch": 0.47657762291908634,
      "grad_norm": 6.841709136962891,
      "learning_rate": 4.765776229190864e-06,
      "loss": 1.8021,
      "step": 1231
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 9.53692626953125,
      "learning_rate": 4.769647696476965e-06,
      "loss": 1.8709,
      "step": 1232
    },
    {
      "epoch": 0.47735191637630664,
      "grad_norm": 28.54514503479004,
      "learning_rate": 4.773519163763067e-06,
      "loss": 2.039,
      "step": 1233
    },
    {
      "epoch": 0.47773906310491676,
      "grad_norm": 6.897552490234375,
      "learning_rate": 4.777390631049168e-06,
      "loss": 1.762,
      "step": 1234
    },
    {
      "epoch": 0.4781262098335269,
      "grad_norm": 8.126384735107422,
      "learning_rate": 4.781262098335269e-06,
      "loss": 2.1001,
      "step": 1235
    },
    {
      "epoch": 0.47851335656213706,
      "grad_norm": 11.268472671508789,
      "learning_rate": 4.785133565621371e-06,
      "loss": 1.6657,
      "step": 1236
    },
    {
      "epoch": 0.4789005032907472,
      "grad_norm": 6.6045613288879395,
      "learning_rate": 4.789005032907472e-06,
      "loss": 1.7459,
      "step": 1237
    },
    {
      "epoch": 0.47928765001935736,
      "grad_norm": 7.3218560218811035,
      "learning_rate": 4.792876500193574e-06,
      "loss": 1.7375,
      "step": 1238
    },
    {
      "epoch": 0.4796747967479675,
      "grad_norm": 12.644137382507324,
      "learning_rate": 4.796747967479675e-06,
      "loss": 1.9214,
      "step": 1239
    },
    {
      "epoch": 0.4800619434765776,
      "grad_norm": 12.779243469238281,
      "learning_rate": 4.8006194347657765e-06,
      "loss": 1.9184,
      "step": 1240
    },
    {
      "epoch": 0.4804490902051878,
      "grad_norm": 7.266574859619141,
      "learning_rate": 4.804490902051878e-06,
      "loss": 1.7182,
      "step": 1241
    },
    {
      "epoch": 0.4808362369337979,
      "grad_norm": 8.06868839263916,
      "learning_rate": 4.8083623693379794e-06,
      "loss": 1.7949,
      "step": 1242
    },
    {
      "epoch": 0.4812233836624081,
      "grad_norm": 16.04082679748535,
      "learning_rate": 4.812233836624081e-06,
      "loss": 2.0924,
      "step": 1243
    },
    {
      "epoch": 0.4816105303910182,
      "grad_norm": 7.495399475097656,
      "learning_rate": 4.8161053039101824e-06,
      "loss": 1.6522,
      "step": 1244
    },
    {
      "epoch": 0.4819976771196283,
      "grad_norm": 7.571193695068359,
      "learning_rate": 4.819976771196284e-06,
      "loss": 1.8176,
      "step": 1245
    },
    {
      "epoch": 0.4823848238482385,
      "grad_norm": 9.685017585754395,
      "learning_rate": 4.823848238482385e-06,
      "loss": 1.7813,
      "step": 1246
    },
    {
      "epoch": 0.4827719705768486,
      "grad_norm": 7.221957683563232,
      "learning_rate": 4.827719705768487e-06,
      "loss": 1.7539,
      "step": 1247
    },
    {
      "epoch": 0.4831591173054588,
      "grad_norm": 6.6510186195373535,
      "learning_rate": 4.831591173054588e-06,
      "loss": 1.7172,
      "step": 1248
    },
    {
      "epoch": 0.4835462640340689,
      "grad_norm": 8.394550323486328,
      "learning_rate": 4.83546264034069e-06,
      "loss": 1.8425,
      "step": 1249
    },
    {
      "epoch": 0.48393341076267904,
      "grad_norm": 8.16180419921875,
      "learning_rate": 4.8393341076267905e-06,
      "loss": 1.8468,
      "step": 1250
    },
    {
      "epoch": 0.4843205574912892,
      "grad_norm": 8.013097763061523,
      "learning_rate": 4.843205574912893e-06,
      "loss": 1.7693,
      "step": 1251
    },
    {
      "epoch": 0.48470770421989934,
      "grad_norm": 10.49217414855957,
      "learning_rate": 4.8470770421989935e-06,
      "loss": 1.7618,
      "step": 1252
    },
    {
      "epoch": 0.48509485094850946,
      "grad_norm": 8.79500961303711,
      "learning_rate": 4.850948509485095e-06,
      "loss": 1.8085,
      "step": 1253
    },
    {
      "epoch": 0.48548199767711964,
      "grad_norm": 8.855293273925781,
      "learning_rate": 4.8548199767711965e-06,
      "loss": 1.7781,
      "step": 1254
    },
    {
      "epoch": 0.48586914440572976,
      "grad_norm": 7.3895063400268555,
      "learning_rate": 4.858691444057298e-06,
      "loss": 1.8355,
      "step": 1255
    },
    {
      "epoch": 0.48625629113433994,
      "grad_norm": 9.0484619140625,
      "learning_rate": 4.8625629113433995e-06,
      "loss": 1.6794,
      "step": 1256
    },
    {
      "epoch": 0.48664343786295006,
      "grad_norm": 11.733718872070312,
      "learning_rate": 4.866434378629501e-06,
      "loss": 2.1144,
      "step": 1257
    },
    {
      "epoch": 0.4870305845915602,
      "grad_norm": 8.353759765625,
      "learning_rate": 4.8703058459156025e-06,
      "loss": 1.7139,
      "step": 1258
    },
    {
      "epoch": 0.48741773132017036,
      "grad_norm": 5.8226447105407715,
      "learning_rate": 4.874177313201704e-06,
      "loss": 1.7637,
      "step": 1259
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 7.501962184906006,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 1.7533,
      "step": 1260
    },
    {
      "epoch": 0.48819202477739065,
      "grad_norm": 8.486833572387695,
      "learning_rate": 4.881920247773907e-06,
      "loss": 1.704,
      "step": 1261
    },
    {
      "epoch": 0.4885791715060008,
      "grad_norm": 9.040925979614258,
      "learning_rate": 4.8857917150600085e-06,
      "loss": 1.9838,
      "step": 1262
    },
    {
      "epoch": 0.4889663182346109,
      "grad_norm": 5.881021976470947,
      "learning_rate": 4.889663182346109e-06,
      "loss": 1.7447,
      "step": 1263
    },
    {
      "epoch": 0.4893534649632211,
      "grad_norm": 7.9820098876953125,
      "learning_rate": 4.8935346496322115e-06,
      "loss": 2.048,
      "step": 1264
    },
    {
      "epoch": 0.4897406116918312,
      "grad_norm": 9.066625595092773,
      "learning_rate": 4.897406116918312e-06,
      "loss": 2.149,
      "step": 1265
    },
    {
      "epoch": 0.4901277584204414,
      "grad_norm": 8.0830078125,
      "learning_rate": 4.9012775842044144e-06,
      "loss": 1.8366,
      "step": 1266
    },
    {
      "epoch": 0.4905149051490515,
      "grad_norm": 7.364961624145508,
      "learning_rate": 4.905149051490515e-06,
      "loss": 1.6481,
      "step": 1267
    },
    {
      "epoch": 0.4909020518776616,
      "grad_norm": 10.621599197387695,
      "learning_rate": 4.909020518776617e-06,
      "loss": 1.8789,
      "step": 1268
    },
    {
      "epoch": 0.4912891986062718,
      "grad_norm": 8.669751167297363,
      "learning_rate": 4.912891986062718e-06,
      "loss": 1.8713,
      "step": 1269
    },
    {
      "epoch": 0.4916763453348819,
      "grad_norm": 8.815266609191895,
      "learning_rate": 4.9167634533488196e-06,
      "loss": 1.6935,
      "step": 1270
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 5.5545735359191895,
      "learning_rate": 4.920634920634921e-06,
      "loss": 1.8098,
      "step": 1271
    },
    {
      "epoch": 0.4924506387921022,
      "grad_norm": 6.907401084899902,
      "learning_rate": 4.9245063879210226e-06,
      "loss": 1.7447,
      "step": 1272
    },
    {
      "epoch": 0.49283778552071233,
      "grad_norm": 12.632711410522461,
      "learning_rate": 4.928377855207124e-06,
      "loss": 1.9561,
      "step": 1273
    },
    {
      "epoch": 0.4932249322493225,
      "grad_norm": 9.984132766723633,
      "learning_rate": 4.9322493224932255e-06,
      "loss": 1.7556,
      "step": 1274
    },
    {
      "epoch": 0.49361207897793263,
      "grad_norm": 6.085011959075928,
      "learning_rate": 4.936120789779327e-06,
      "loss": 1.7836,
      "step": 1275
    },
    {
      "epoch": 0.49399922570654275,
      "grad_norm": 6.727043151855469,
      "learning_rate": 4.939992257065428e-06,
      "loss": 1.6737,
      "step": 1276
    },
    {
      "epoch": 0.49438637243515293,
      "grad_norm": 6.552462100982666,
      "learning_rate": 4.94386372435153e-06,
      "loss": 1.8562,
      "step": 1277
    },
    {
      "epoch": 0.49477351916376305,
      "grad_norm": 6.996310710906982,
      "learning_rate": 4.947735191637631e-06,
      "loss": 1.774,
      "step": 1278
    },
    {
      "epoch": 0.49516066589237323,
      "grad_norm": 7.439060211181641,
      "learning_rate": 4.951606658923733e-06,
      "loss": 1.8645,
      "step": 1279
    },
    {
      "epoch": 0.49554781262098335,
      "grad_norm": 8.391182899475098,
      "learning_rate": 4.955478126209834e-06,
      "loss": 1.7161,
      "step": 1280
    },
    {
      "epoch": 0.4959349593495935,
      "grad_norm": 6.773946762084961,
      "learning_rate": 4.959349593495935e-06,
      "loss": 1.7727,
      "step": 1281
    },
    {
      "epoch": 0.49632210607820365,
      "grad_norm": 9.339591026306152,
      "learning_rate": 4.963221060782037e-06,
      "loss": 2.1467,
      "step": 1282
    },
    {
      "epoch": 0.4967092528068138,
      "grad_norm": 13.101534843444824,
      "learning_rate": 4.967092528068138e-06,
      "loss": 1.8156,
      "step": 1283
    },
    {
      "epoch": 0.49709639953542395,
      "grad_norm": 9.072443008422852,
      "learning_rate": 4.97096399535424e-06,
      "loss": 1.7877,
      "step": 1284
    },
    {
      "epoch": 0.49748354626403407,
      "grad_norm": 8.034409523010254,
      "learning_rate": 4.974835462640341e-06,
      "loss": 1.6724,
      "step": 1285
    },
    {
      "epoch": 0.4978706929926442,
      "grad_norm": 9.200090408325195,
      "learning_rate": 4.978706929926443e-06,
      "loss": 1.6717,
      "step": 1286
    },
    {
      "epoch": 0.49825783972125437,
      "grad_norm": 7.619313716888428,
      "learning_rate": 4.982578397212544e-06,
      "loss": 1.7391,
      "step": 1287
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 11.121085166931152,
      "learning_rate": 4.986449864498646e-06,
      "loss": 1.7499,
      "step": 1288
    },
    {
      "epoch": 0.49903213317847467,
      "grad_norm": 10.333406448364258,
      "learning_rate": 4.990321331784747e-06,
      "loss": 1.9308,
      "step": 1289
    },
    {
      "epoch": 0.4994192799070848,
      "grad_norm": 7.297568321228027,
      "learning_rate": 4.994192799070849e-06,
      "loss": 1.6901,
      "step": 1290
    },
    {
      "epoch": 0.4998064266356949,
      "grad_norm": 5.885211944580078,
      "learning_rate": 4.998064266356949e-06,
      "loss": 1.7462,
      "step": 1291
    },
    {
      "epoch": 0.500193573364305,
      "grad_norm": 8.989912033081055,
      "learning_rate": 5.001935733643051e-06,
      "loss": 1.7973,
      "step": 1292
    },
    {
      "epoch": 0.5005807200929152,
      "grad_norm": 14.319707870483398,
      "learning_rate": 5.005807200929152e-06,
      "loss": 1.7254,
      "step": 1293
    },
    {
      "epoch": 0.5009678668215254,
      "grad_norm": 8.638689994812012,
      "learning_rate": 5.0096786682152546e-06,
      "loss": 1.723,
      "step": 1294
    },
    {
      "epoch": 0.5013550135501355,
      "grad_norm": 6.493345260620117,
      "learning_rate": 5.013550135501355e-06,
      "loss": 1.7121,
      "step": 1295
    },
    {
      "epoch": 0.5017421602787456,
      "grad_norm": 7.5784478187561035,
      "learning_rate": 5.017421602787457e-06,
      "loss": 1.6619,
      "step": 1296
    },
    {
      "epoch": 0.5021293070073558,
      "grad_norm": 8.598311424255371,
      "learning_rate": 5.021293070073558e-06,
      "loss": 1.7179,
      "step": 1297
    },
    {
      "epoch": 0.502516453735966,
      "grad_norm": 9.280269622802734,
      "learning_rate": 5.0251645373596605e-06,
      "loss": 1.7426,
      "step": 1298
    },
    {
      "epoch": 0.502903600464576,
      "grad_norm": 7.828007698059082,
      "learning_rate": 5.029036004645761e-06,
      "loss": 1.6942,
      "step": 1299
    },
    {
      "epoch": 0.5032907471931862,
      "grad_norm": 8.416085243225098,
      "learning_rate": 5.032907471931863e-06,
      "loss": 1.7567,
      "step": 1300
    },
    {
      "epoch": 0.5036778939217964,
      "grad_norm": 4.9756622314453125,
      "learning_rate": 5.036778939217964e-06,
      "loss": 1.8594,
      "step": 1301
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 9.59684944152832,
      "learning_rate": 5.040650406504065e-06,
      "loss": 1.769,
      "step": 1302
    },
    {
      "epoch": 0.5044521873790166,
      "grad_norm": 7.622328758239746,
      "learning_rate": 5.044521873790167e-06,
      "loss": 1.7885,
      "step": 1303
    },
    {
      "epoch": 0.5048393341076268,
      "grad_norm": 5.3853759765625,
      "learning_rate": 5.048393341076269e-06,
      "loss": 1.9012,
      "step": 1304
    },
    {
      "epoch": 0.5052264808362369,
      "grad_norm": 12.7770414352417,
      "learning_rate": 5.052264808362369e-06,
      "loss": 1.5756,
      "step": 1305
    },
    {
      "epoch": 0.5056136275648471,
      "grad_norm": 9.566641807556152,
      "learning_rate": 5.056136275648471e-06,
      "loss": 1.8211,
      "step": 1306
    },
    {
      "epoch": 0.5060007742934572,
      "grad_norm": 7.381981372833252,
      "learning_rate": 5.060007742934573e-06,
      "loss": 1.7978,
      "step": 1307
    },
    {
      "epoch": 0.5063879210220673,
      "grad_norm": 13.096182823181152,
      "learning_rate": 5.063879210220674e-06,
      "loss": 1.5714,
      "step": 1308
    },
    {
      "epoch": 0.5067750677506775,
      "grad_norm": 7.9695634841918945,
      "learning_rate": 5.067750677506775e-06,
      "loss": 1.6306,
      "step": 1309
    },
    {
      "epoch": 0.5071622144792877,
      "grad_norm": 6.70554256439209,
      "learning_rate": 5.071622144792877e-06,
      "loss": 1.6707,
      "step": 1310
    },
    {
      "epoch": 0.5075493612078978,
      "grad_norm": 9.293648719787598,
      "learning_rate": 5.075493612078979e-06,
      "loss": 1.7442,
      "step": 1311
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 12.665901184082031,
      "learning_rate": 5.07936507936508e-06,
      "loss": 1.7302,
      "step": 1312
    },
    {
      "epoch": 0.5083236546651181,
      "grad_norm": 7.331841945648193,
      "learning_rate": 5.083236546651181e-06,
      "loss": 1.7705,
      "step": 1313
    },
    {
      "epoch": 0.5087108013937283,
      "grad_norm": 8.053418159484863,
      "learning_rate": 5.087108013937283e-06,
      "loss": 1.723,
      "step": 1314
    },
    {
      "epoch": 0.5090979481223383,
      "grad_norm": 16.229639053344727,
      "learning_rate": 5.090979481223383e-06,
      "loss": 1.7001,
      "step": 1315
    },
    {
      "epoch": 0.5094850948509485,
      "grad_norm": 7.097141742706299,
      "learning_rate": 5.094850948509486e-06,
      "loss": 1.6488,
      "step": 1316
    },
    {
      "epoch": 0.5098722415795587,
      "grad_norm": 8.260337829589844,
      "learning_rate": 5.098722415795587e-06,
      "loss": 1.7674,
      "step": 1317
    },
    {
      "epoch": 0.5102593883081687,
      "grad_norm": 7.707313060760498,
      "learning_rate": 5.102593883081688e-06,
      "loss": 1.6598,
      "step": 1318
    },
    {
      "epoch": 0.5106465350367789,
      "grad_norm": 9.216583251953125,
      "learning_rate": 5.106465350367789e-06,
      "loss": 1.7438,
      "step": 1319
    },
    {
      "epoch": 0.5110336817653891,
      "grad_norm": 8.76384449005127,
      "learning_rate": 5.110336817653892e-06,
      "loss": 1.66,
      "step": 1320
    },
    {
      "epoch": 0.5114208284939993,
      "grad_norm": 9.909137725830078,
      "learning_rate": 5.114208284939993e-06,
      "loss": 1.918,
      "step": 1321
    },
    {
      "epoch": 0.5118079752226093,
      "grad_norm": 15.340450286865234,
      "learning_rate": 5.118079752226094e-06,
      "loss": 1.9091,
      "step": 1322
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 14.217267990112305,
      "learning_rate": 5.121951219512195e-06,
      "loss": 1.6284,
      "step": 1323
    },
    {
      "epoch": 0.5125822686798297,
      "grad_norm": 9.805551528930664,
      "learning_rate": 5.125822686798298e-06,
      "loss": 1.9058,
      "step": 1324
    },
    {
      "epoch": 0.5129694154084398,
      "grad_norm": 7.676059246063232,
      "learning_rate": 5.129694154084398e-06,
      "loss": 1.7423,
      "step": 1325
    },
    {
      "epoch": 0.5133565621370499,
      "grad_norm": 7.661569118499756,
      "learning_rate": 5.1335656213705e-06,
      "loss": 1.8519,
      "step": 1326
    },
    {
      "epoch": 0.5137437088656601,
      "grad_norm": 10.50899600982666,
      "learning_rate": 5.137437088656601e-06,
      "loss": 1.7718,
      "step": 1327
    },
    {
      "epoch": 0.5141308555942702,
      "grad_norm": 9.510676383972168,
      "learning_rate": 5.141308555942702e-06,
      "loss": 1.5858,
      "step": 1328
    },
    {
      "epoch": 0.5145180023228804,
      "grad_norm": 8.900527954101562,
      "learning_rate": 5.145180023228804e-06,
      "loss": 1.7474,
      "step": 1329
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 8.274018287658691,
      "learning_rate": 5.149051490514906e-06,
      "loss": 1.709,
      "step": 1330
    },
    {
      "epoch": 0.5152922957801006,
      "grad_norm": 7.996002197265625,
      "learning_rate": 5.1529229578010064e-06,
      "loss": 1.9953,
      "step": 1331
    },
    {
      "epoch": 0.5156794425087108,
      "grad_norm": 8.977570533752441,
      "learning_rate": 5.156794425087108e-06,
      "loss": 1.6712,
      "step": 1332
    },
    {
      "epoch": 0.516066589237321,
      "grad_norm": 9.667548179626465,
      "learning_rate": 5.16066589237321e-06,
      "loss": 1.8049,
      "step": 1333
    },
    {
      "epoch": 0.5164537359659311,
      "grad_norm": 9.509604454040527,
      "learning_rate": 5.164537359659312e-06,
      "loss": 1.7928,
      "step": 1334
    },
    {
      "epoch": 0.5168408826945412,
      "grad_norm": 9.484648704528809,
      "learning_rate": 5.168408826945412e-06,
      "loss": 1.8007,
      "step": 1335
    },
    {
      "epoch": 0.5172280294231514,
      "grad_norm": 8.456109046936035,
      "learning_rate": 5.172280294231514e-06,
      "loss": 1.6151,
      "step": 1336
    },
    {
      "epoch": 0.5176151761517616,
      "grad_norm": 13.701489448547363,
      "learning_rate": 5.176151761517616e-06,
      "loss": 1.8202,
      "step": 1337
    },
    {
      "epoch": 0.5180023228803716,
      "grad_norm": 10.891124725341797,
      "learning_rate": 5.180023228803717e-06,
      "loss": 1.8602,
      "step": 1338
    },
    {
      "epoch": 0.5183894696089818,
      "grad_norm": 7.9399518966674805,
      "learning_rate": 5.183894696089818e-06,
      "loss": 1.7691,
      "step": 1339
    },
    {
      "epoch": 0.518776616337592,
      "grad_norm": 14.020740509033203,
      "learning_rate": 5.18776616337592e-06,
      "loss": 1.8357,
      "step": 1340
    },
    {
      "epoch": 0.519163763066202,
      "grad_norm": 11.67481803894043,
      "learning_rate": 5.1916376306620205e-06,
      "loss": 1.6927,
      "step": 1341
    },
    {
      "epoch": 0.5195509097948122,
      "grad_norm": 15.899163246154785,
      "learning_rate": 5.195509097948123e-06,
      "loss": 1.9472,
      "step": 1342
    },
    {
      "epoch": 0.5199380565234224,
      "grad_norm": 9.076348304748535,
      "learning_rate": 5.199380565234224e-06,
      "loss": 1.7944,
      "step": 1343
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 12.691703796386719,
      "learning_rate": 5.203252032520326e-06,
      "loss": 1.9228,
      "step": 1344
    },
    {
      "epoch": 0.5207123499806426,
      "grad_norm": 9.287591934204102,
      "learning_rate": 5.2071234998064265e-06,
      "loss": 1.5984,
      "step": 1345
    },
    {
      "epoch": 0.5210994967092528,
      "grad_norm": 7.831995964050293,
      "learning_rate": 5.210994967092529e-06,
      "loss": 1.6131,
      "step": 1346
    },
    {
      "epoch": 0.521486643437863,
      "grad_norm": 6.71788215637207,
      "learning_rate": 5.21486643437863e-06,
      "loss": 1.5869,
      "step": 1347
    },
    {
      "epoch": 0.5218737901664731,
      "grad_norm": 9.345585823059082,
      "learning_rate": 5.218737901664731e-06,
      "loss": 1.6317,
      "step": 1348
    },
    {
      "epoch": 0.5222609368950832,
      "grad_norm": 8.090785026550293,
      "learning_rate": 5.2226093689508325e-06,
      "loss": 1.5941,
      "step": 1349
    },
    {
      "epoch": 0.5226480836236934,
      "grad_norm": 12.546285629272461,
      "learning_rate": 5.226480836236935e-06,
      "loss": 1.7428,
      "step": 1350
    },
    {
      "epoch": 0.5230352303523035,
      "grad_norm": 11.025548934936523,
      "learning_rate": 5.2303523035230355e-06,
      "loss": 1.9032,
      "step": 1351
    },
    {
      "epoch": 0.5234223770809137,
      "grad_norm": 8.071944236755371,
      "learning_rate": 5.234223770809137e-06,
      "loss": 1.7206,
      "step": 1352
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 7.341916084289551,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 1.7799,
      "step": 1353
    },
    {
      "epoch": 0.5241966705381339,
      "grad_norm": 10.793625831604004,
      "learning_rate": 5.241966705381339e-06,
      "loss": 1.703,
      "step": 1354
    },
    {
      "epoch": 0.5245838172667441,
      "grad_norm": 7.954017162322998,
      "learning_rate": 5.2458381726674414e-06,
      "loss": 1.7322,
      "step": 1355
    },
    {
      "epoch": 0.5249709639953543,
      "grad_norm": 7.091125965118408,
      "learning_rate": 5.249709639953543e-06,
      "loss": 1.8452,
      "step": 1356
    },
    {
      "epoch": 0.5253581107239644,
      "grad_norm": 9.655097007751465,
      "learning_rate": 5.253581107239644e-06,
      "loss": 1.6682,
      "step": 1357
    },
    {
      "epoch": 0.5257452574525745,
      "grad_norm": 15.474637985229492,
      "learning_rate": 5.257452574525745e-06,
      "loss": 1.765,
      "step": 1358
    },
    {
      "epoch": 0.5261324041811847,
      "grad_norm": 11.418221473693848,
      "learning_rate": 5.261324041811847e-06,
      "loss": 1.7673,
      "step": 1359
    },
    {
      "epoch": 0.5265195509097949,
      "grad_norm": 9.937798500061035,
      "learning_rate": 5.265195509097949e-06,
      "loss": 1.7564,
      "step": 1360
    },
    {
      "epoch": 0.5269066976384049,
      "grad_norm": 6.057168483734131,
      "learning_rate": 5.2690669763840495e-06,
      "loss": 1.8477,
      "step": 1361
    },
    {
      "epoch": 0.5272938443670151,
      "grad_norm": 8.355338096618652,
      "learning_rate": 5.272938443670151e-06,
      "loss": 1.6224,
      "step": 1362
    },
    {
      "epoch": 0.5276809910956253,
      "grad_norm": 13.478459358215332,
      "learning_rate": 5.276809910956253e-06,
      "loss": 2.0608,
      "step": 1363
    },
    {
      "epoch": 0.5280681378242353,
      "grad_norm": 9.84047794342041,
      "learning_rate": 5.280681378242354e-06,
      "loss": 1.6636,
      "step": 1364
    },
    {
      "epoch": 0.5284552845528455,
      "grad_norm": 10.584559440612793,
      "learning_rate": 5.2845528455284555e-06,
      "loss": 1.7397,
      "step": 1365
    },
    {
      "epoch": 0.5288424312814557,
      "grad_norm": 14.384819030761719,
      "learning_rate": 5.288424312814557e-06,
      "loss": 1.7679,
      "step": 1366
    },
    {
      "epoch": 0.5292295780100658,
      "grad_norm": 12.933555603027344,
      "learning_rate": 5.292295780100658e-06,
      "loss": 1.7447,
      "step": 1367
    },
    {
      "epoch": 0.5296167247386759,
      "grad_norm": 8.521564483642578,
      "learning_rate": 5.29616724738676e-06,
      "loss": 1.7322,
      "step": 1368
    },
    {
      "epoch": 0.5300038714672861,
      "grad_norm": 8.680744171142578,
      "learning_rate": 5.3000387146728615e-06,
      "loss": 1.7035,
      "step": 1369
    },
    {
      "epoch": 0.5303910181958963,
      "grad_norm": 13.195222854614258,
      "learning_rate": 5.303910181958963e-06,
      "loss": 1.6419,
      "step": 1370
    },
    {
      "epoch": 0.5307781649245064,
      "grad_norm": 8.50390911102295,
      "learning_rate": 5.307781649245064e-06,
      "loss": 1.8746,
      "step": 1371
    },
    {
      "epoch": 0.5311653116531165,
      "grad_norm": 9.71081256866455,
      "learning_rate": 5.311653116531166e-06,
      "loss": 1.6766,
      "step": 1372
    },
    {
      "epoch": 0.5315524583817267,
      "grad_norm": 12.901923179626465,
      "learning_rate": 5.3155245838172675e-06,
      "loss": 1.8042,
      "step": 1373
    },
    {
      "epoch": 0.5319396051103368,
      "grad_norm": 7.895209312438965,
      "learning_rate": 5.319396051103368e-06,
      "loss": 1.6994,
      "step": 1374
    },
    {
      "epoch": 0.532326751838947,
      "grad_norm": 7.3983001708984375,
      "learning_rate": 5.32326751838947e-06,
      "loss": 1.7151,
      "step": 1375
    },
    {
      "epoch": 0.5327138985675571,
      "grad_norm": 15.921335220336914,
      "learning_rate": 5.327138985675572e-06,
      "loss": 1.7117,
      "step": 1376
    },
    {
      "epoch": 0.5331010452961672,
      "grad_norm": 9.918570518493652,
      "learning_rate": 5.331010452961673e-06,
      "loss": 1.9024,
      "step": 1377
    },
    {
      "epoch": 0.5334881920247774,
      "grad_norm": 7.98599100112915,
      "learning_rate": 5.334881920247774e-06,
      "loss": 1.6681,
      "step": 1378
    },
    {
      "epoch": 0.5338753387533876,
      "grad_norm": 7.361870288848877,
      "learning_rate": 5.338753387533876e-06,
      "loss": 1.6636,
      "step": 1379
    },
    {
      "epoch": 0.5342624854819977,
      "grad_norm": 8.02617073059082,
      "learning_rate": 5.342624854819978e-06,
      "loss": 1.6671,
      "step": 1380
    },
    {
      "epoch": 0.5346496322106078,
      "grad_norm": 13.056593894958496,
      "learning_rate": 5.3464963221060786e-06,
      "loss": 1.7162,
      "step": 1381
    },
    {
      "epoch": 0.535036778939218,
      "grad_norm": 9.123769760131836,
      "learning_rate": 5.35036778939218e-06,
      "loss": 1.6141,
      "step": 1382
    },
    {
      "epoch": 0.5354239256678281,
      "grad_norm": 7.697166442871094,
      "learning_rate": 5.3542392566782816e-06,
      "loss": 1.7032,
      "step": 1383
    },
    {
      "epoch": 0.5358110723964382,
      "grad_norm": 9.595966339111328,
      "learning_rate": 5.358110723964382e-06,
      "loss": 2.2018,
      "step": 1384
    },
    {
      "epoch": 0.5361982191250484,
      "grad_norm": 12.388055801391602,
      "learning_rate": 5.3619821912504845e-06,
      "loss": 1.791,
      "step": 1385
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 8.258294105529785,
      "learning_rate": 5.365853658536586e-06,
      "loss": 1.7152,
      "step": 1386
    },
    {
      "epoch": 0.5369725125822686,
      "grad_norm": 7.559144973754883,
      "learning_rate": 5.369725125822687e-06,
      "loss": 1.594,
      "step": 1387
    },
    {
      "epoch": 0.5373596593108788,
      "grad_norm": 6.790043354034424,
      "learning_rate": 5.373596593108788e-06,
      "loss": 1.7443,
      "step": 1388
    },
    {
      "epoch": 0.537746806039489,
      "grad_norm": 6.464563369750977,
      "learning_rate": 5.3774680603948905e-06,
      "loss": 1.5464,
      "step": 1389
    },
    {
      "epoch": 0.538133952768099,
      "grad_norm": 8.155656814575195,
      "learning_rate": 5.381339527680991e-06,
      "loss": 1.8746,
      "step": 1390
    },
    {
      "epoch": 0.5385210994967092,
      "grad_norm": 9.205279350280762,
      "learning_rate": 5.385210994967093e-06,
      "loss": 1.6912,
      "step": 1391
    },
    {
      "epoch": 0.5389082462253194,
      "grad_norm": 7.957200527191162,
      "learning_rate": 5.389082462253194e-06,
      "loss": 1.8355,
      "step": 1392
    },
    {
      "epoch": 0.5392953929539296,
      "grad_norm": 8.353617668151855,
      "learning_rate": 5.3929539295392965e-06,
      "loss": 1.7264,
      "step": 1393
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 8.412663459777832,
      "learning_rate": 5.396825396825397e-06,
      "loss": 1.796,
      "step": 1394
    },
    {
      "epoch": 0.5400696864111498,
      "grad_norm": 8.655327796936035,
      "learning_rate": 5.400696864111499e-06,
      "loss": 1.5663,
      "step": 1395
    },
    {
      "epoch": 0.54045683313976,
      "grad_norm": 9.140593528747559,
      "learning_rate": 5.4045683313976e-06,
      "loss": 1.7888,
      "step": 1396
    },
    {
      "epoch": 0.5408439798683701,
      "grad_norm": 12.821698188781738,
      "learning_rate": 5.408439798683701e-06,
      "loss": 1.7683,
      "step": 1397
    },
    {
      "epoch": 0.5412311265969802,
      "grad_norm": 10.31419849395752,
      "learning_rate": 5.412311265969803e-06,
      "loss": 1.8958,
      "step": 1398
    },
    {
      "epoch": 0.5416182733255904,
      "grad_norm": 11.663201332092285,
      "learning_rate": 5.416182733255905e-06,
      "loss": 1.6816,
      "step": 1399
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 17.67310905456543,
      "learning_rate": 5.420054200542005e-06,
      "loss": 1.8997,
      "step": 1400
    },
    {
      "epoch": 0.5423925667828107,
      "grad_norm": 7.681925296783447,
      "learning_rate": 5.423925667828107e-06,
      "loss": 1.6003,
      "step": 1401
    },
    {
      "epoch": 0.5427797135114208,
      "grad_norm": 8.727168083190918,
      "learning_rate": 5.427797135114209e-06,
      "loss": 1.592,
      "step": 1402
    },
    {
      "epoch": 0.543166860240031,
      "grad_norm": 9.692272186279297,
      "learning_rate": 5.4316686024003106e-06,
      "loss": 1.8054,
      "step": 1403
    },
    {
      "epoch": 0.5435540069686411,
      "grad_norm": 8.61725902557373,
      "learning_rate": 5.435540069686411e-06,
      "loss": 1.7554,
      "step": 1404
    },
    {
      "epoch": 0.5439411536972513,
      "grad_norm": 9.115333557128906,
      "learning_rate": 5.439411536972513e-06,
      "loss": 1.539,
      "step": 1405
    },
    {
      "epoch": 0.5443283004258614,
      "grad_norm": 9.260086059570312,
      "learning_rate": 5.443283004258615e-06,
      "loss": 1.6606,
      "step": 1406
    },
    {
      "epoch": 0.5447154471544715,
      "grad_norm": 11.25811767578125,
      "learning_rate": 5.447154471544716e-06,
      "loss": 1.6883,
      "step": 1407
    },
    {
      "epoch": 0.5451025938830817,
      "grad_norm": 9.235289573669434,
      "learning_rate": 5.451025938830817e-06,
      "loss": 1.5065,
      "step": 1408
    },
    {
      "epoch": 0.5454897406116919,
      "grad_norm": 23.70233154296875,
      "learning_rate": 5.454897406116919e-06,
      "loss": 1.9649,
      "step": 1409
    },
    {
      "epoch": 0.5458768873403019,
      "grad_norm": 10.807398796081543,
      "learning_rate": 5.458768873403019e-06,
      "loss": 1.8071,
      "step": 1410
    },
    {
      "epoch": 0.5462640340689121,
      "grad_norm": 9.413725852966309,
      "learning_rate": 5.462640340689122e-06,
      "loss": 1.5424,
      "step": 1411
    },
    {
      "epoch": 0.5466511807975223,
      "grad_norm": 15.844057083129883,
      "learning_rate": 5.466511807975223e-06,
      "loss": 2.2533,
      "step": 1412
    },
    {
      "epoch": 0.5470383275261324,
      "grad_norm": 8.093308448791504,
      "learning_rate": 5.470383275261324e-06,
      "loss": 1.6579,
      "step": 1413
    },
    {
      "epoch": 0.5474254742547425,
      "grad_norm": 8.075512886047363,
      "learning_rate": 5.474254742547425e-06,
      "loss": 1.761,
      "step": 1414
    },
    {
      "epoch": 0.5478126209833527,
      "grad_norm": 8.526487350463867,
      "learning_rate": 5.478126209833528e-06,
      "loss": 1.6375,
      "step": 1415
    },
    {
      "epoch": 0.5481997677119629,
      "grad_norm": 9.4284086227417,
      "learning_rate": 5.481997677119629e-06,
      "loss": 1.8597,
      "step": 1416
    },
    {
      "epoch": 0.548586914440573,
      "grad_norm": 7.141223430633545,
      "learning_rate": 5.48586914440573e-06,
      "loss": 1.4689,
      "step": 1417
    },
    {
      "epoch": 0.5489740611691831,
      "grad_norm": 10.1143217086792,
      "learning_rate": 5.489740611691831e-06,
      "loss": 1.5996,
      "step": 1418
    },
    {
      "epoch": 0.5493612078977933,
      "grad_norm": 9.950176239013672,
      "learning_rate": 5.493612078977934e-06,
      "loss": 1.8674,
      "step": 1419
    },
    {
      "epoch": 0.5497483546264034,
      "grad_norm": 8.04936408996582,
      "learning_rate": 5.497483546264034e-06,
      "loss": 1.6556,
      "step": 1420
    },
    {
      "epoch": 0.5501355013550135,
      "grad_norm": 12.24074649810791,
      "learning_rate": 5.501355013550136e-06,
      "loss": 1.6691,
      "step": 1421
    },
    {
      "epoch": 0.5505226480836237,
      "grad_norm": 11.19809627532959,
      "learning_rate": 5.505226480836237e-06,
      "loss": 1.7494,
      "step": 1422
    },
    {
      "epoch": 0.5509097948122338,
      "grad_norm": 7.17512845993042,
      "learning_rate": 5.509097948122339e-06,
      "loss": 1.7289,
      "step": 1423
    },
    {
      "epoch": 0.551296941540844,
      "grad_norm": 13.313982963562012,
      "learning_rate": 5.51296941540844e-06,
      "loss": 1.4654,
      "step": 1424
    },
    {
      "epoch": 0.5516840882694541,
      "grad_norm": 9.673526763916016,
      "learning_rate": 5.516840882694542e-06,
      "loss": 1.555,
      "step": 1425
    },
    {
      "epoch": 0.5520712349980643,
      "grad_norm": 9.7999906539917,
      "learning_rate": 5.520712349980643e-06,
      "loss": 1.6722,
      "step": 1426
    },
    {
      "epoch": 0.5524583817266744,
      "grad_norm": 8.88854694366455,
      "learning_rate": 5.524583817266745e-06,
      "loss": 1.8199,
      "step": 1427
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 10.987530708312988,
      "learning_rate": 5.528455284552846e-06,
      "loss": 1.6953,
      "step": 1428
    },
    {
      "epoch": 0.5532326751838947,
      "grad_norm": 9.979049682617188,
      "learning_rate": 5.532326751838948e-06,
      "loss": 1.5629,
      "step": 1429
    },
    {
      "epoch": 0.5536198219125048,
      "grad_norm": 7.817266941070557,
      "learning_rate": 5.536198219125048e-06,
      "loss": 1.5905,
      "step": 1430
    },
    {
      "epoch": 0.554006968641115,
      "grad_norm": 7.565865993499756,
      "learning_rate": 5.540069686411151e-06,
      "loss": 1.5959,
      "step": 1431
    },
    {
      "epoch": 0.5543941153697252,
      "grad_norm": 10.322434425354004,
      "learning_rate": 5.543941153697252e-06,
      "loss": 2.0315,
      "step": 1432
    },
    {
      "epoch": 0.5547812620983352,
      "grad_norm": 7.255078315734863,
      "learning_rate": 5.547812620983353e-06,
      "loss": 1.6373,
      "step": 1433
    },
    {
      "epoch": 0.5551684088269454,
      "grad_norm": 8.379776000976562,
      "learning_rate": 5.551684088269454e-06,
      "loss": 1.774,
      "step": 1434
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 9.933935165405273,
      "learning_rate": 5.555555555555557e-06,
      "loss": 2.0162,
      "step": 1435
    },
    {
      "epoch": 0.5559427022841656,
      "grad_norm": 8.29250431060791,
      "learning_rate": 5.559427022841657e-06,
      "loss": 1.7496,
      "step": 1436
    },
    {
      "epoch": 0.5563298490127758,
      "grad_norm": 12.468098640441895,
      "learning_rate": 5.563298490127759e-06,
      "loss": 1.6349,
      "step": 1437
    },
    {
      "epoch": 0.556716995741386,
      "grad_norm": 13.199508666992188,
      "learning_rate": 5.56716995741386e-06,
      "loss": 1.6256,
      "step": 1438
    },
    {
      "epoch": 0.5571041424699962,
      "grad_norm": 10.697770118713379,
      "learning_rate": 5.571041424699963e-06,
      "loss": 1.5869,
      "step": 1439
    },
    {
      "epoch": 0.5574912891986062,
      "grad_norm": 10.315377235412598,
      "learning_rate": 5.574912891986063e-06,
      "loss": 1.9821,
      "step": 1440
    },
    {
      "epoch": 0.5578784359272164,
      "grad_norm": 5.946903228759766,
      "learning_rate": 5.578784359272165e-06,
      "loss": 1.7128,
      "step": 1441
    },
    {
      "epoch": 0.5582655826558266,
      "grad_norm": 8.534420013427734,
      "learning_rate": 5.582655826558266e-06,
      "loss": 1.7344,
      "step": 1442
    },
    {
      "epoch": 0.5586527293844367,
      "grad_norm": 9.231522560119629,
      "learning_rate": 5.586527293844367e-06,
      "loss": 1.5411,
      "step": 1443
    },
    {
      "epoch": 0.5590398761130468,
      "grad_norm": 9.710814476013184,
      "learning_rate": 5.590398761130469e-06,
      "loss": 1.5228,
      "step": 1444
    },
    {
      "epoch": 0.559427022841657,
      "grad_norm": 9.185322761535645,
      "learning_rate": 5.594270228416571e-06,
      "loss": 1.7659,
      "step": 1445
    },
    {
      "epoch": 0.5598141695702671,
      "grad_norm": 9.67588996887207,
      "learning_rate": 5.598141695702671e-06,
      "loss": 1.7684,
      "step": 1446
    },
    {
      "epoch": 0.5602013162988773,
      "grad_norm": 9.422475814819336,
      "learning_rate": 5.602013162988773e-06,
      "loss": 1.7035,
      "step": 1447
    },
    {
      "epoch": 0.5605884630274874,
      "grad_norm": 9.273335456848145,
      "learning_rate": 5.605884630274875e-06,
      "loss": 1.7381,
      "step": 1448
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 10.089031219482422,
      "learning_rate": 5.609756097560977e-06,
      "loss": 1.6133,
      "step": 1449
    },
    {
      "epoch": 0.5613627564847077,
      "grad_norm": 10.175055503845215,
      "learning_rate": 5.613627564847077e-06,
      "loss": 1.7544,
      "step": 1450
    },
    {
      "epoch": 0.5617499032133179,
      "grad_norm": 8.147137641906738,
      "learning_rate": 5.617499032133179e-06,
      "loss": 1.7314,
      "step": 1451
    },
    {
      "epoch": 0.562137049941928,
      "grad_norm": 10.88351821899414,
      "learning_rate": 5.621370499419281e-06,
      "loss": 1.7979,
      "step": 1452
    },
    {
      "epoch": 0.5625241966705381,
      "grad_norm": 10.035901069641113,
      "learning_rate": 5.625241966705382e-06,
      "loss": 1.8173,
      "step": 1453
    },
    {
      "epoch": 0.5629113433991483,
      "grad_norm": 8.756290435791016,
      "learning_rate": 5.629113433991483e-06,
      "loss": 1.704,
      "step": 1454
    },
    {
      "epoch": 0.5632984901277585,
      "grad_norm": 11.939477920532227,
      "learning_rate": 5.632984901277585e-06,
      "loss": 1.8192,
      "step": 1455
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 9.53458023071289,
      "learning_rate": 5.6368563685636855e-06,
      "loss": 1.7223,
      "step": 1456
    },
    {
      "epoch": 0.5640727835849787,
      "grad_norm": 13.202938079833984,
      "learning_rate": 5.640727835849788e-06,
      "loss": 2.1213,
      "step": 1457
    },
    {
      "epoch": 0.5644599303135889,
      "grad_norm": 10.124244689941406,
      "learning_rate": 5.644599303135889e-06,
      "loss": 1.7353,
      "step": 1458
    },
    {
      "epoch": 0.5648470770421989,
      "grad_norm": 12.167207717895508,
      "learning_rate": 5.64847077042199e-06,
      "loss": 2.0167,
      "step": 1459
    },
    {
      "epoch": 0.5652342237708091,
      "grad_norm": 10.377643585205078,
      "learning_rate": 5.6523422377080915e-06,
      "loss": 1.5211,
      "step": 1460
    },
    {
      "epoch": 0.5656213704994193,
      "grad_norm": 13.694287300109863,
      "learning_rate": 5.656213704994194e-06,
      "loss": 1.6214,
      "step": 1461
    },
    {
      "epoch": 0.5660085172280295,
      "grad_norm": 12.551365852355957,
      "learning_rate": 5.660085172280295e-06,
      "loss": 1.6732,
      "step": 1462
    },
    {
      "epoch": 0.5663956639566395,
      "grad_norm": 10.562268257141113,
      "learning_rate": 5.663956639566396e-06,
      "loss": 1.7806,
      "step": 1463
    },
    {
      "epoch": 0.5667828106852497,
      "grad_norm": 7.578174114227295,
      "learning_rate": 5.6678281068524974e-06,
      "loss": 1.6514,
      "step": 1464
    },
    {
      "epoch": 0.5671699574138599,
      "grad_norm": 10.057487487792969,
      "learning_rate": 5.6716995741386e-06,
      "loss": 1.6932,
      "step": 1465
    },
    {
      "epoch": 0.56755710414247,
      "grad_norm": 17.37509536743164,
      "learning_rate": 5.6755710414247004e-06,
      "loss": 1.8797,
      "step": 1466
    },
    {
      "epoch": 0.5679442508710801,
      "grad_norm": 15.189029693603516,
      "learning_rate": 5.679442508710802e-06,
      "loss": 1.62,
      "step": 1467
    },
    {
      "epoch": 0.5683313975996903,
      "grad_norm": 15.259355545043945,
      "learning_rate": 5.683313975996903e-06,
      "loss": 1.8883,
      "step": 1468
    },
    {
      "epoch": 0.5687185443283004,
      "grad_norm": 11.407679557800293,
      "learning_rate": 5.687185443283004e-06,
      "loss": 1.6147,
      "step": 1469
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 13.385226249694824,
      "learning_rate": 5.691056910569106e-06,
      "loss": 2.015,
      "step": 1470
    },
    {
      "epoch": 0.5694928377855207,
      "grad_norm": 13.078021049499512,
      "learning_rate": 5.694928377855208e-06,
      "loss": 2.1355,
      "step": 1471
    },
    {
      "epoch": 0.5698799845141309,
      "grad_norm": 16.56485366821289,
      "learning_rate": 5.698799845141309e-06,
      "loss": 1.6212,
      "step": 1472
    },
    {
      "epoch": 0.570267131242741,
      "grad_norm": 10.44498062133789,
      "learning_rate": 5.70267131242741e-06,
      "loss": 1.5316,
      "step": 1473
    },
    {
      "epoch": 0.5706542779713512,
      "grad_norm": 11.248676300048828,
      "learning_rate": 5.706542779713512e-06,
      "loss": 1.7543,
      "step": 1474
    },
    {
      "epoch": 0.5710414246999613,
      "grad_norm": 8.887187957763672,
      "learning_rate": 5.710414246999614e-06,
      "loss": 1.6853,
      "step": 1475
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 14.102459907531738,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.4686,
      "step": 1476
    },
    {
      "epoch": 0.5718157181571816,
      "grad_norm": 16.885581970214844,
      "learning_rate": 5.718157181571816e-06,
      "loss": 1.8241,
      "step": 1477
    },
    {
      "epoch": 0.5722028648857918,
      "grad_norm": 10.367696762084961,
      "learning_rate": 5.722028648857918e-06,
      "loss": 1.7372,
      "step": 1478
    },
    {
      "epoch": 0.5725900116144018,
      "grad_norm": 10.994058609008789,
      "learning_rate": 5.725900116144019e-06,
      "loss": 1.5818,
      "step": 1479
    },
    {
      "epoch": 0.572977158343012,
      "grad_norm": 9.929997444152832,
      "learning_rate": 5.7297715834301205e-06,
      "loss": 1.596,
      "step": 1480
    },
    {
      "epoch": 0.5733643050716222,
      "grad_norm": 11.595735549926758,
      "learning_rate": 5.733643050716222e-06,
      "loss": 1.7573,
      "step": 1481
    },
    {
      "epoch": 0.5737514518002322,
      "grad_norm": 9.880404472351074,
      "learning_rate": 5.737514518002323e-06,
      "loss": 1.6642,
      "step": 1482
    },
    {
      "epoch": 0.5741385985288424,
      "grad_norm": 13.446292877197266,
      "learning_rate": 5.741385985288425e-06,
      "loss": 1.7607,
      "step": 1483
    },
    {
      "epoch": 0.5745257452574526,
      "grad_norm": 11.114376068115234,
      "learning_rate": 5.7452574525745265e-06,
      "loss": 1.8247,
      "step": 1484
    },
    {
      "epoch": 0.5749128919860628,
      "grad_norm": 11.16783618927002,
      "learning_rate": 5.749128919860628e-06,
      "loss": 1.6077,
      "step": 1485
    },
    {
      "epoch": 0.5753000387146728,
      "grad_norm": 8.862831115722656,
      "learning_rate": 5.753000387146729e-06,
      "loss": 1.7596,
      "step": 1486
    },
    {
      "epoch": 0.575687185443283,
      "grad_norm": 10.285255432128906,
      "learning_rate": 5.756871854432831e-06,
      "loss": 1.8237,
      "step": 1487
    },
    {
      "epoch": 0.5760743321718932,
      "grad_norm": 15.75640869140625,
      "learning_rate": 5.7607433217189324e-06,
      "loss": 1.5814,
      "step": 1488
    },
    {
      "epoch": 0.5764614789005033,
      "grad_norm": 12.1646146774292,
      "learning_rate": 5.764614789005033e-06,
      "loss": 2.1519,
      "step": 1489
    },
    {
      "epoch": 0.5768486256291134,
      "grad_norm": 10.039576530456543,
      "learning_rate": 5.7684862562911346e-06,
      "loss": 1.6119,
      "step": 1490
    },
    {
      "epoch": 0.5772357723577236,
      "grad_norm": 6.684099197387695,
      "learning_rate": 5.772357723577237e-06,
      "loss": 1.8163,
      "step": 1491
    },
    {
      "epoch": 0.5776229190863337,
      "grad_norm": 8.67487907409668,
      "learning_rate": 5.7762291908633376e-06,
      "loss": 1.4125,
      "step": 1492
    },
    {
      "epoch": 0.5780100658149439,
      "grad_norm": 10.82089614868164,
      "learning_rate": 5.780100658149439e-06,
      "loss": 1.5987,
      "step": 1493
    },
    {
      "epoch": 0.578397212543554,
      "grad_norm": 12.447325706481934,
      "learning_rate": 5.7839721254355405e-06,
      "loss": 1.9415,
      "step": 1494
    },
    {
      "epoch": 0.5787843592721641,
      "grad_norm": 11.678605079650879,
      "learning_rate": 5.787843592721641e-06,
      "loss": 1.7508,
      "step": 1495
    },
    {
      "epoch": 0.5791715060007743,
      "grad_norm": 8.295422554016113,
      "learning_rate": 5.7917150600077435e-06,
      "loss": 1.7493,
      "step": 1496
    },
    {
      "epoch": 0.5795586527293844,
      "grad_norm": 11.251540184020996,
      "learning_rate": 5.795586527293845e-06,
      "loss": 1.6122,
      "step": 1497
    },
    {
      "epoch": 0.5799457994579946,
      "grad_norm": 11.07530689239502,
      "learning_rate": 5.7994579945799465e-06,
      "loss": 1.8203,
      "step": 1498
    },
    {
      "epoch": 0.5803329461866047,
      "grad_norm": 7.696312427520752,
      "learning_rate": 5.803329461866047e-06,
      "loss": 1.6412,
      "step": 1499
    },
    {
      "epoch": 0.5807200929152149,
      "grad_norm": 8.422235488891602,
      "learning_rate": 5.8072009291521495e-06,
      "loss": 1.7239,
      "step": 1500
    },
    {
      "epoch": 0.581107239643825,
      "grad_norm": 8.833298683166504,
      "learning_rate": 5.811072396438251e-06,
      "loss": 1.7684,
      "step": 1501
    },
    {
      "epoch": 0.5814943863724351,
      "grad_norm": 12.904641151428223,
      "learning_rate": 5.814943863724352e-06,
      "loss": 1.582,
      "step": 1502
    },
    {
      "epoch": 0.5818815331010453,
      "grad_norm": 10.146073341369629,
      "learning_rate": 5.818815331010453e-06,
      "loss": 1.6466,
      "step": 1503
    },
    {
      "epoch": 0.5822686798296555,
      "grad_norm": 9.032862663269043,
      "learning_rate": 5.8226867982965555e-06,
      "loss": 1.6147,
      "step": 1504
    },
    {
      "epoch": 0.5826558265582655,
      "grad_norm": 13.819158554077148,
      "learning_rate": 5.826558265582656e-06,
      "loss": 1.6945,
      "step": 1505
    },
    {
      "epoch": 0.5830429732868757,
      "grad_norm": 10.75665283203125,
      "learning_rate": 5.830429732868758e-06,
      "loss": 1.9395,
      "step": 1506
    },
    {
      "epoch": 0.5834301200154859,
      "grad_norm": 8.8504638671875,
      "learning_rate": 5.834301200154859e-06,
      "loss": 1.8054,
      "step": 1507
    },
    {
      "epoch": 0.5838172667440961,
      "grad_norm": 18.66946792602539,
      "learning_rate": 5.8381726674409615e-06,
      "loss": 1.7902,
      "step": 1508
    },
    {
      "epoch": 0.5842044134727061,
      "grad_norm": 9.193771362304688,
      "learning_rate": 5.842044134727062e-06,
      "loss": 1.8253,
      "step": 1509
    },
    {
      "epoch": 0.5845915602013163,
      "grad_norm": 9.99756908416748,
      "learning_rate": 5.845915602013164e-06,
      "loss": 1.6649,
      "step": 1510
    },
    {
      "epoch": 0.5849787069299265,
      "grad_norm": 14.317920684814453,
      "learning_rate": 5.849787069299265e-06,
      "loss": 1.6757,
      "step": 1511
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 13.28160285949707,
      "learning_rate": 5.853658536585366e-06,
      "loss": 1.9246,
      "step": 1512
    },
    {
      "epoch": 0.5857530003871467,
      "grad_norm": 9.238507270812988,
      "learning_rate": 5.857530003871468e-06,
      "loss": 1.6773,
      "step": 1513
    },
    {
      "epoch": 0.5861401471157569,
      "grad_norm": 6.352151870727539,
      "learning_rate": 5.8614014711575696e-06,
      "loss": 1.7755,
      "step": 1514
    },
    {
      "epoch": 0.586527293844367,
      "grad_norm": 9.958553314208984,
      "learning_rate": 5.86527293844367e-06,
      "loss": 1.7826,
      "step": 1515
    },
    {
      "epoch": 0.5869144405729771,
      "grad_norm": 14.048624038696289,
      "learning_rate": 5.869144405729772e-06,
      "loss": 1.8408,
      "step": 1516
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 13.656244277954102,
      "learning_rate": 5.873015873015874e-06,
      "loss": 1.5944,
      "step": 1517
    },
    {
      "epoch": 0.5876887340301974,
      "grad_norm": 17.456457138061523,
      "learning_rate": 5.876887340301975e-06,
      "loss": 1.6646,
      "step": 1518
    },
    {
      "epoch": 0.5880758807588076,
      "grad_norm": 41.354862213134766,
      "learning_rate": 5.880758807588076e-06,
      "loss": 1.8757,
      "step": 1519
    },
    {
      "epoch": 0.5884630274874177,
      "grad_norm": 8.14378833770752,
      "learning_rate": 5.884630274874178e-06,
      "loss": 1.6647,
      "step": 1520
    },
    {
      "epoch": 0.5888501742160279,
      "grad_norm": 10.520184516906738,
      "learning_rate": 5.88850174216028e-06,
      "loss": 2.1739,
      "step": 1521
    },
    {
      "epoch": 0.589237320944638,
      "grad_norm": 8.685115814208984,
      "learning_rate": 5.892373209446381e-06,
      "loss": 1.7264,
      "step": 1522
    },
    {
      "epoch": 0.5896244676732482,
      "grad_norm": 10.867173194885254,
      "learning_rate": 5.896244676732482e-06,
      "loss": 1.8826,
      "step": 1523
    },
    {
      "epoch": 0.5900116144018583,
      "grad_norm": 14.543697357177734,
      "learning_rate": 5.900116144018584e-06,
      "loss": 1.5724,
      "step": 1524
    },
    {
      "epoch": 0.5903987611304684,
      "grad_norm": 14.55343246459961,
      "learning_rate": 5.903987611304684e-06,
      "loss": 1.5863,
      "step": 1525
    },
    {
      "epoch": 0.5907859078590786,
      "grad_norm": 22.200260162353516,
      "learning_rate": 5.907859078590787e-06,
      "loss": 2.027,
      "step": 1526
    },
    {
      "epoch": 0.5911730545876888,
      "grad_norm": 15.841262817382812,
      "learning_rate": 5.911730545876888e-06,
      "loss": 1.6265,
      "step": 1527
    },
    {
      "epoch": 0.5915602013162988,
      "grad_norm": 10.919628143310547,
      "learning_rate": 5.915602013162989e-06,
      "loss": 1.8709,
      "step": 1528
    },
    {
      "epoch": 0.591947348044909,
      "grad_norm": 9.772811889648438,
      "learning_rate": 5.91947348044909e-06,
      "loss": 1.7798,
      "step": 1529
    },
    {
      "epoch": 0.5923344947735192,
      "grad_norm": 12.474100112915039,
      "learning_rate": 5.923344947735193e-06,
      "loss": 1.6601,
      "step": 1530
    },
    {
      "epoch": 0.5927216415021294,
      "grad_norm": 12.051456451416016,
      "learning_rate": 5.927216415021294e-06,
      "loss": 1.5838,
      "step": 1531
    },
    {
      "epoch": 0.5931087882307394,
      "grad_norm": 13.413492202758789,
      "learning_rate": 5.931087882307395e-06,
      "loss": 1.8935,
      "step": 1532
    },
    {
      "epoch": 0.5934959349593496,
      "grad_norm": 7.277101516723633,
      "learning_rate": 5.934959349593496e-06,
      "loss": 1.3854,
      "step": 1533
    },
    {
      "epoch": 0.5938830816879598,
      "grad_norm": 18.329099655151367,
      "learning_rate": 5.938830816879599e-06,
      "loss": 1.8802,
      "step": 1534
    },
    {
      "epoch": 0.5942702284165698,
      "grad_norm": 11.395195007324219,
      "learning_rate": 5.942702284165699e-06,
      "loss": 1.7302,
      "step": 1535
    },
    {
      "epoch": 0.59465737514518,
      "grad_norm": 8.492464065551758,
      "learning_rate": 5.946573751451801e-06,
      "loss": 1.6016,
      "step": 1536
    },
    {
      "epoch": 0.5950445218737902,
      "grad_norm": 12.25341796875,
      "learning_rate": 5.950445218737902e-06,
      "loss": 1.8058,
      "step": 1537
    },
    {
      "epoch": 0.5954316686024003,
      "grad_norm": 10.197683334350586,
      "learning_rate": 5.954316686024003e-06,
      "loss": 1.5771,
      "step": 1538
    },
    {
      "epoch": 0.5958188153310104,
      "grad_norm": 10.790886878967285,
      "learning_rate": 5.958188153310105e-06,
      "loss": 1.5338,
      "step": 1539
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 9.665396690368652,
      "learning_rate": 5.962059620596207e-06,
      "loss": 1.5568,
      "step": 1540
    },
    {
      "epoch": 0.5965931087882307,
      "grad_norm": 10.372039794921875,
      "learning_rate": 5.965931087882307e-06,
      "loss": 1.5795,
      "step": 1541
    },
    {
      "epoch": 0.5969802555168409,
      "grad_norm": 9.478906631469727,
      "learning_rate": 5.969802555168409e-06,
      "loss": 1.7045,
      "step": 1542
    },
    {
      "epoch": 0.597367402245451,
      "grad_norm": 10.947084426879883,
      "learning_rate": 5.973674022454511e-06,
      "loss": 1.4489,
      "step": 1543
    },
    {
      "epoch": 0.5977545489740612,
      "grad_norm": 14.078129768371582,
      "learning_rate": 5.977545489740613e-06,
      "loss": 1.6344,
      "step": 1544
    },
    {
      "epoch": 0.5981416957026713,
      "grad_norm": 11.127141952514648,
      "learning_rate": 5.981416957026713e-06,
      "loss": 1.7046,
      "step": 1545
    },
    {
      "epoch": 0.5985288424312815,
      "grad_norm": 10.913918495178223,
      "learning_rate": 5.985288424312815e-06,
      "loss": 1.7891,
      "step": 1546
    },
    {
      "epoch": 0.5989159891598916,
      "grad_norm": 10.396374702453613,
      "learning_rate": 5.989159891598917e-06,
      "loss": 1.4717,
      "step": 1547
    },
    {
      "epoch": 0.5993031358885017,
      "grad_norm": 7.831698894500732,
      "learning_rate": 5.993031358885018e-06,
      "loss": 1.7413,
      "step": 1548
    },
    {
      "epoch": 0.5996902826171119,
      "grad_norm": 14.152691841125488,
      "learning_rate": 5.996902826171119e-06,
      "loss": 1.8132,
      "step": 1549
    },
    {
      "epoch": 0.6000774293457221,
      "grad_norm": 10.332314491271973,
      "learning_rate": 6.000774293457221e-06,
      "loss": 1.5192,
      "step": 1550
    },
    {
      "epoch": 0.6004645760743321,
      "grad_norm": 10.374032974243164,
      "learning_rate": 6.0046457607433214e-06,
      "loss": 1.8334,
      "step": 1551
    },
    {
      "epoch": 0.6008517228029423,
      "grad_norm": 13.717498779296875,
      "learning_rate": 6.008517228029424e-06,
      "loss": 1.8211,
      "step": 1552
    },
    {
      "epoch": 0.6012388695315525,
      "grad_norm": 12.885575294494629,
      "learning_rate": 6.012388695315525e-06,
      "loss": 1.6023,
      "step": 1553
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 8.932714462280273,
      "learning_rate": 6.016260162601627e-06,
      "loss": 1.5866,
      "step": 1554
    },
    {
      "epoch": 0.6020131629887727,
      "grad_norm": 12.031266212463379,
      "learning_rate": 6.020131629887727e-06,
      "loss": 1.3664,
      "step": 1555
    },
    {
      "epoch": 0.6024003097173829,
      "grad_norm": 15.150345802307129,
      "learning_rate": 6.02400309717383e-06,
      "loss": 1.6353,
      "step": 1556
    },
    {
      "epoch": 0.6027874564459931,
      "grad_norm": 14.332188606262207,
      "learning_rate": 6.027874564459931e-06,
      "loss": 1.5895,
      "step": 1557
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 13.231552124023438,
      "learning_rate": 6.031746031746032e-06,
      "loss": 1.7254,
      "step": 1558
    },
    {
      "epoch": 0.6035617499032133,
      "grad_norm": 8.915203094482422,
      "learning_rate": 6.035617499032133e-06,
      "loss": 1.6773,
      "step": 1559
    },
    {
      "epoch": 0.6039488966318235,
      "grad_norm": 8.854516983032227,
      "learning_rate": 6.039488966318236e-06,
      "loss": 1.613,
      "step": 1560
    },
    {
      "epoch": 0.6043360433604336,
      "grad_norm": 38.64569854736328,
      "learning_rate": 6.043360433604336e-06,
      "loss": 1.3755,
      "step": 1561
    },
    {
      "epoch": 0.6047231900890437,
      "grad_norm": 20.608163833618164,
      "learning_rate": 6.047231900890438e-06,
      "loss": 1.7736,
      "step": 1562
    },
    {
      "epoch": 0.6051103368176539,
      "grad_norm": 14.431235313415527,
      "learning_rate": 6.051103368176539e-06,
      "loss": 1.5949,
      "step": 1563
    },
    {
      "epoch": 0.605497483546264,
      "grad_norm": 9.450784683227539,
      "learning_rate": 6.05497483546264e-06,
      "loss": 1.5292,
      "step": 1564
    },
    {
      "epoch": 0.6058846302748742,
      "grad_norm": 9.671939849853516,
      "learning_rate": 6.058846302748742e-06,
      "loss": 1.5512,
      "step": 1565
    },
    {
      "epoch": 0.6062717770034843,
      "grad_norm": 12.98320198059082,
      "learning_rate": 6.062717770034844e-06,
      "loss": 1.577,
      "step": 1566
    },
    {
      "epoch": 0.6066589237320945,
      "grad_norm": 10.783313751220703,
      "learning_rate": 6.066589237320945e-06,
      "loss": 1.8383,
      "step": 1567
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 10.17593002319336,
      "learning_rate": 6.070460704607046e-06,
      "loss": 1.5238,
      "step": 1568
    },
    {
      "epoch": 0.6074332171893148,
      "grad_norm": 16.289405822753906,
      "learning_rate": 6.074332171893148e-06,
      "loss": 2.0461,
      "step": 1569
    },
    {
      "epoch": 0.6078203639179249,
      "grad_norm": 11.299612998962402,
      "learning_rate": 6.07820363917925e-06,
      "loss": 1.8521,
      "step": 1570
    },
    {
      "epoch": 0.608207510646535,
      "grad_norm": 11.739983558654785,
      "learning_rate": 6.0820751064653505e-06,
      "loss": 2.0838,
      "step": 1571
    },
    {
      "epoch": 0.6085946573751452,
      "grad_norm": 7.775533676147461,
      "learning_rate": 6.085946573751452e-06,
      "loss": 1.6605,
      "step": 1572
    },
    {
      "epoch": 0.6089818041037554,
      "grad_norm": 9.274157524108887,
      "learning_rate": 6.089818041037554e-06,
      "loss": 1.6843,
      "step": 1573
    },
    {
      "epoch": 0.6093689508323654,
      "grad_norm": 10.940832138061523,
      "learning_rate": 6.093689508323655e-06,
      "loss": 1.5896,
      "step": 1574
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 10.693075180053711,
      "learning_rate": 6.0975609756097564e-06,
      "loss": 1.7382,
      "step": 1575
    },
    {
      "epoch": 0.6101432442895858,
      "grad_norm": 12.609443664550781,
      "learning_rate": 6.101432442895858e-06,
      "loss": 2.5954,
      "step": 1576
    },
    {
      "epoch": 0.610530391018196,
      "grad_norm": 10.26055908203125,
      "learning_rate": 6.10530391018196e-06,
      "loss": 1.5159,
      "step": 1577
    },
    {
      "epoch": 0.610917537746806,
      "grad_norm": 16.145095825195312,
      "learning_rate": 6.109175377468061e-06,
      "loss": 1.915,
      "step": 1578
    },
    {
      "epoch": 0.6113046844754162,
      "grad_norm": 10.751679420471191,
      "learning_rate": 6.113046844754162e-06,
      "loss": 1.7961,
      "step": 1579
    },
    {
      "epoch": 0.6116918312040264,
      "grad_norm": 10.17103385925293,
      "learning_rate": 6.116918312040264e-06,
      "loss": 1.7543,
      "step": 1580
    },
    {
      "epoch": 0.6120789779326364,
      "grad_norm": 8.905314445495605,
      "learning_rate": 6.1207897793263645e-06,
      "loss": 1.4692,
      "step": 1581
    },
    {
      "epoch": 0.6124661246612466,
      "grad_norm": 12.344270706176758,
      "learning_rate": 6.124661246612467e-06,
      "loss": 1.5914,
      "step": 1582
    },
    {
      "epoch": 0.6128532713898568,
      "grad_norm": 12.150907516479492,
      "learning_rate": 6.128532713898568e-06,
      "loss": 1.6411,
      "step": 1583
    },
    {
      "epoch": 0.6132404181184669,
      "grad_norm": 8.09635066986084,
      "learning_rate": 6.132404181184669e-06,
      "loss": 1.6552,
      "step": 1584
    },
    {
      "epoch": 0.613627564847077,
      "grad_norm": 9.78537368774414,
      "learning_rate": 6.1362756484707705e-06,
      "loss": 1.9092,
      "step": 1585
    },
    {
      "epoch": 0.6140147115756872,
      "grad_norm": 10.028664588928223,
      "learning_rate": 6.140147115756873e-06,
      "loss": 1.4177,
      "step": 1586
    },
    {
      "epoch": 0.6144018583042973,
      "grad_norm": 22.991329193115234,
      "learning_rate": 6.1440185830429735e-06,
      "loss": 1.6709,
      "step": 1587
    },
    {
      "epoch": 0.6147890050329075,
      "grad_norm": 13.437554359436035,
      "learning_rate": 6.147890050329075e-06,
      "loss": 1.535,
      "step": 1588
    },
    {
      "epoch": 0.6151761517615176,
      "grad_norm": 8.078166007995605,
      "learning_rate": 6.1517615176151765e-06,
      "loss": 1.8083,
      "step": 1589
    },
    {
      "epoch": 0.6155632984901278,
      "grad_norm": 7.74685525894165,
      "learning_rate": 6.155632984901279e-06,
      "loss": 1.7143,
      "step": 1590
    },
    {
      "epoch": 0.6159504452187379,
      "grad_norm": 9.635074615478516,
      "learning_rate": 6.1595044521873795e-06,
      "loss": 1.3984,
      "step": 1591
    },
    {
      "epoch": 0.616337591947348,
      "grad_norm": 12.859708786010742,
      "learning_rate": 6.163375919473481e-06,
      "loss": 1.7372,
      "step": 1592
    },
    {
      "epoch": 0.6167247386759582,
      "grad_norm": 24.00034523010254,
      "learning_rate": 6.1672473867595825e-06,
      "loss": 1.494,
      "step": 1593
    },
    {
      "epoch": 0.6171118854045683,
      "grad_norm": 9.76987075805664,
      "learning_rate": 6.171118854045683e-06,
      "loss": 1.831,
      "step": 1594
    },
    {
      "epoch": 0.6174990321331785,
      "grad_norm": 13.735464096069336,
      "learning_rate": 6.1749903213317855e-06,
      "loss": 1.8629,
      "step": 1595
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 12.546796798706055,
      "learning_rate": 6.178861788617887e-06,
      "loss": 1.7484,
      "step": 1596
    },
    {
      "epoch": 0.6182733255903987,
      "grad_norm": 10.250710487365723,
      "learning_rate": 6.182733255903988e-06,
      "loss": 1.8899,
      "step": 1597
    },
    {
      "epoch": 0.6186604723190089,
      "grad_norm": 12.981754302978516,
      "learning_rate": 6.186604723190089e-06,
      "loss": 2.2793,
      "step": 1598
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 11.438450813293457,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 2.1992,
      "step": 1599
    },
    {
      "epoch": 0.6194347657762292,
      "grad_norm": 11.044639587402344,
      "learning_rate": 6.194347657762293e-06,
      "loss": 1.996,
      "step": 1600
    },
    {
      "epoch": 0.6198219125048393,
      "grad_norm": 9.229256629943848,
      "learning_rate": 6.1982191250483936e-06,
      "loss": 1.6605,
      "step": 1601
    },
    {
      "epoch": 0.6202090592334495,
      "grad_norm": 16.97092056274414,
      "learning_rate": 6.202090592334495e-06,
      "loss": 1.6276,
      "step": 1602
    },
    {
      "epoch": 0.6205962059620597,
      "grad_norm": 10.03750991821289,
      "learning_rate": 6.205962059620597e-06,
      "loss": 1.692,
      "step": 1603
    },
    {
      "epoch": 0.6209833526906697,
      "grad_norm": 11.449248313903809,
      "learning_rate": 6.209833526906698e-06,
      "loss": 1.9971,
      "step": 1604
    },
    {
      "epoch": 0.6213704994192799,
      "grad_norm": 9.563312530517578,
      "learning_rate": 6.2137049941927995e-06,
      "loss": 1.6856,
      "step": 1605
    },
    {
      "epoch": 0.6217576461478901,
      "grad_norm": 17.56059455871582,
      "learning_rate": 6.217576461478901e-06,
      "loss": 1.6123,
      "step": 1606
    },
    {
      "epoch": 0.6221447928765002,
      "grad_norm": 14.759915351867676,
      "learning_rate": 6.221447928765002e-06,
      "loss": 1.9823,
      "step": 1607
    },
    {
      "epoch": 0.6225319396051103,
      "grad_norm": 12.17438793182373,
      "learning_rate": 6.225319396051104e-06,
      "loss": 1.9422,
      "step": 1608
    },
    {
      "epoch": 0.6229190863337205,
      "grad_norm": 11.85266399383545,
      "learning_rate": 6.2291908633372055e-06,
      "loss": 1.8395,
      "step": 1609
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 9.19947338104248,
      "learning_rate": 6.233062330623306e-06,
      "loss": 1.6802,
      "step": 1610
    },
    {
      "epoch": 0.6236933797909407,
      "grad_norm": 14.721713066101074,
      "learning_rate": 6.236933797909408e-06,
      "loss": 1.18,
      "step": 1611
    },
    {
      "epoch": 0.6240805265195509,
      "grad_norm": 10.618770599365234,
      "learning_rate": 6.24080526519551e-06,
      "loss": 1.7229,
      "step": 1612
    },
    {
      "epoch": 0.6244676732481611,
      "grad_norm": 15.817943572998047,
      "learning_rate": 6.2446767324816115e-06,
      "loss": 1.5922,
      "step": 1613
    },
    {
      "epoch": 0.6248548199767712,
      "grad_norm": 14.560968399047852,
      "learning_rate": 6.248548199767712e-06,
      "loss": 1.6989,
      "step": 1614
    },
    {
      "epoch": 0.6252419667053813,
      "grad_norm": 10.002652168273926,
      "learning_rate": 6.252419667053814e-06,
      "loss": 1.4555,
      "step": 1615
    },
    {
      "epoch": 0.6256291134339915,
      "grad_norm": 13.65610122680664,
      "learning_rate": 6.256291134339916e-06,
      "loss": 1.4468,
      "step": 1616
    },
    {
      "epoch": 0.6260162601626016,
      "grad_norm": 12.421309471130371,
      "learning_rate": 6.260162601626017e-06,
      "loss": 1.8664,
      "step": 1617
    },
    {
      "epoch": 0.6264034068912118,
      "grad_norm": 9.310602188110352,
      "learning_rate": 6.264034068912118e-06,
      "loss": 1.5122,
      "step": 1618
    },
    {
      "epoch": 0.6267905536198219,
      "grad_norm": 9.932397842407227,
      "learning_rate": 6.26790553619822e-06,
      "loss": 1.552,
      "step": 1619
    },
    {
      "epoch": 0.627177700348432,
      "grad_norm": 11.499909400939941,
      "learning_rate": 6.27177700348432e-06,
      "loss": 1.8281,
      "step": 1620
    },
    {
      "epoch": 0.6275648470770422,
      "grad_norm": 13.386284828186035,
      "learning_rate": 6.275648470770423e-06,
      "loss": 1.5517,
      "step": 1621
    },
    {
      "epoch": 0.6279519938056524,
      "grad_norm": 14.677328109741211,
      "learning_rate": 6.279519938056524e-06,
      "loss": 1.6707,
      "step": 1622
    },
    {
      "epoch": 0.6283391405342624,
      "grad_norm": 9.997922897338867,
      "learning_rate": 6.283391405342625e-06,
      "loss": 1.7789,
      "step": 1623
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 9.93423080444336,
      "learning_rate": 6.287262872628726e-06,
      "loss": 1.5773,
      "step": 1624
    },
    {
      "epoch": 0.6291134339914828,
      "grad_norm": 11.95890998840332,
      "learning_rate": 6.2911343399148286e-06,
      "loss": 1.9148,
      "step": 1625
    },
    {
      "epoch": 0.629500580720093,
      "grad_norm": 9.166473388671875,
      "learning_rate": 6.29500580720093e-06,
      "loss": 1.7835,
      "step": 1626
    },
    {
      "epoch": 0.629887727448703,
      "grad_norm": 9.763961791992188,
      "learning_rate": 6.298877274487031e-06,
      "loss": 1.426,
      "step": 1627
    },
    {
      "epoch": 0.6302748741773132,
      "grad_norm": 11.169343948364258,
      "learning_rate": 6.302748741773132e-06,
      "loss": 1.7174,
      "step": 1628
    },
    {
      "epoch": 0.6306620209059234,
      "grad_norm": 9.525415420532227,
      "learning_rate": 6.3066202090592345e-06,
      "loss": 1.698,
      "step": 1629
    },
    {
      "epoch": 0.6310491676345334,
      "grad_norm": 11.631133079528809,
      "learning_rate": 6.310491676345335e-06,
      "loss": 1.8154,
      "step": 1630
    },
    {
      "epoch": 0.6314363143631436,
      "grad_norm": 12.042900085449219,
      "learning_rate": 6.314363143631437e-06,
      "loss": 1.8916,
      "step": 1631
    },
    {
      "epoch": 0.6318234610917538,
      "grad_norm": 12.48315715789795,
      "learning_rate": 6.318234610917538e-06,
      "loss": 1.7226,
      "step": 1632
    },
    {
      "epoch": 0.6322106078203639,
      "grad_norm": 13.990615844726562,
      "learning_rate": 6.322106078203639e-06,
      "loss": 1.8374,
      "step": 1633
    },
    {
      "epoch": 0.632597754548974,
      "grad_norm": 10.116567611694336,
      "learning_rate": 6.325977545489741e-06,
      "loss": 1.8486,
      "step": 1634
    },
    {
      "epoch": 0.6329849012775842,
      "grad_norm": 16.7943172454834,
      "learning_rate": 6.329849012775843e-06,
      "loss": 1.7482,
      "step": 1635
    },
    {
      "epoch": 0.6333720480061944,
      "grad_norm": 11.196146011352539,
      "learning_rate": 6.333720480061944e-06,
      "loss": 1.543,
      "step": 1636
    },
    {
      "epoch": 0.6337591947348045,
      "grad_norm": 9.938338279724121,
      "learning_rate": 6.337591947348045e-06,
      "loss": 1.5045,
      "step": 1637
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 9.68420124053955,
      "learning_rate": 6.341463414634147e-06,
      "loss": 1.6563,
      "step": 1638
    },
    {
      "epoch": 0.6345334881920248,
      "grad_norm": 15.46877384185791,
      "learning_rate": 6.345334881920249e-06,
      "loss": 1.5631,
      "step": 1639
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 10.867834091186523,
      "learning_rate": 6.349206349206349e-06,
      "loss": 1.7889,
      "step": 1640
    },
    {
      "epoch": 0.6353077816492451,
      "grad_norm": 12.051791191101074,
      "learning_rate": 6.353077816492451e-06,
      "loss": 1.5484,
      "step": 1641
    },
    {
      "epoch": 0.6356949283778552,
      "grad_norm": 9.796772956848145,
      "learning_rate": 6.356949283778553e-06,
      "loss": 2.1745,
      "step": 1642
    },
    {
      "epoch": 0.6360820751064653,
      "grad_norm": 10.983915328979492,
      "learning_rate": 6.360820751064654e-06,
      "loss": 1.3828,
      "step": 1643
    },
    {
      "epoch": 0.6364692218350755,
      "grad_norm": 15.645587921142578,
      "learning_rate": 6.364692218350755e-06,
      "loss": 1.5505,
      "step": 1644
    },
    {
      "epoch": 0.6368563685636857,
      "grad_norm": 15.705018997192383,
      "learning_rate": 6.368563685636857e-06,
      "loss": 1.4453,
      "step": 1645
    },
    {
      "epoch": 0.6372435152922957,
      "grad_norm": 14.376604080200195,
      "learning_rate": 6.372435152922957e-06,
      "loss": 1.7615,
      "step": 1646
    },
    {
      "epoch": 0.6376306620209059,
      "grad_norm": 10.845812797546387,
      "learning_rate": 6.37630662020906e-06,
      "loss": 1.6827,
      "step": 1647
    },
    {
      "epoch": 0.6380178087495161,
      "grad_norm": 10.224095344543457,
      "learning_rate": 6.380178087495161e-06,
      "loss": 1.9021,
      "step": 1648
    },
    {
      "epoch": 0.6384049554781263,
      "grad_norm": 16.573556900024414,
      "learning_rate": 6.384049554781263e-06,
      "loss": 1.7546,
      "step": 1649
    },
    {
      "epoch": 0.6387921022067363,
      "grad_norm": 10.449393272399902,
      "learning_rate": 6.387921022067363e-06,
      "loss": 1.6522,
      "step": 1650
    },
    {
      "epoch": 0.6391792489353465,
      "grad_norm": 18.235862731933594,
      "learning_rate": 6.391792489353466e-06,
      "loss": 1.599,
      "step": 1651
    },
    {
      "epoch": 0.6395663956639567,
      "grad_norm": 11.273093223571777,
      "learning_rate": 6.395663956639567e-06,
      "loss": 1.8063,
      "step": 1652
    },
    {
      "epoch": 0.6399535423925667,
      "grad_norm": 9.501378059387207,
      "learning_rate": 6.399535423925668e-06,
      "loss": 1.5975,
      "step": 1653
    },
    {
      "epoch": 0.6403406891211769,
      "grad_norm": 11.26749324798584,
      "learning_rate": 6.403406891211769e-06,
      "loss": 1.6983,
      "step": 1654
    },
    {
      "epoch": 0.6407278358497871,
      "grad_norm": 8.536458969116211,
      "learning_rate": 6.407278358497872e-06,
      "loss": 1.5714,
      "step": 1655
    },
    {
      "epoch": 0.6411149825783972,
      "grad_norm": 7.421027183532715,
      "learning_rate": 6.411149825783972e-06,
      "loss": 1.6369,
      "step": 1656
    },
    {
      "epoch": 0.6415021293070073,
      "grad_norm": 9.434779167175293,
      "learning_rate": 6.415021293070074e-06,
      "loss": 1.6161,
      "step": 1657
    },
    {
      "epoch": 0.6418892760356175,
      "grad_norm": 7.645019054412842,
      "learning_rate": 6.418892760356175e-06,
      "loss": 1.6453,
      "step": 1658
    },
    {
      "epoch": 0.6422764227642277,
      "grad_norm": 19.036104202270508,
      "learning_rate": 6.422764227642278e-06,
      "loss": 1.8505,
      "step": 1659
    },
    {
      "epoch": 0.6426635694928378,
      "grad_norm": 10.538844108581543,
      "learning_rate": 6.426635694928378e-06,
      "loss": 1.817,
      "step": 1660
    },
    {
      "epoch": 0.6430507162214479,
      "grad_norm": 9.730415344238281,
      "learning_rate": 6.43050716221448e-06,
      "loss": 1.6494,
      "step": 1661
    },
    {
      "epoch": 0.6434378629500581,
      "grad_norm": 11.24897289276123,
      "learning_rate": 6.434378629500581e-06,
      "loss": 1.3921,
      "step": 1662
    },
    {
      "epoch": 0.6438250096786682,
      "grad_norm": 13.17837905883789,
      "learning_rate": 6.438250096786682e-06,
      "loss": 2.4474,
      "step": 1663
    },
    {
      "epoch": 0.6442121564072784,
      "grad_norm": 10.572211265563965,
      "learning_rate": 6.442121564072784e-06,
      "loss": 1.3125,
      "step": 1664
    },
    {
      "epoch": 0.6445993031358885,
      "grad_norm": 10.286937713623047,
      "learning_rate": 6.445993031358886e-06,
      "loss": 1.4291,
      "step": 1665
    },
    {
      "epoch": 0.6449864498644986,
      "grad_norm": 11.037425994873047,
      "learning_rate": 6.449864498644986e-06,
      "loss": 1.7163,
      "step": 1666
    },
    {
      "epoch": 0.6453735965931088,
      "grad_norm": 10.855558395385742,
      "learning_rate": 6.453735965931088e-06,
      "loss": 1.639,
      "step": 1667
    },
    {
      "epoch": 0.645760743321719,
      "grad_norm": 9.719392776489258,
      "learning_rate": 6.45760743321719e-06,
      "loss": 1.6727,
      "step": 1668
    },
    {
      "epoch": 0.646147890050329,
      "grad_norm": 10.755160331726074,
      "learning_rate": 6.461478900503291e-06,
      "loss": 1.8768,
      "step": 1669
    },
    {
      "epoch": 0.6465350367789392,
      "grad_norm": 15.968979835510254,
      "learning_rate": 6.465350367789392e-06,
      "loss": 1.7733,
      "step": 1670
    },
    {
      "epoch": 0.6469221835075494,
      "grad_norm": 41.006656646728516,
      "learning_rate": 6.469221835075494e-06,
      "loss": 1.8769,
      "step": 1671
    },
    {
      "epoch": 0.6473093302361596,
      "grad_norm": 18.741825103759766,
      "learning_rate": 6.473093302361596e-06,
      "loss": 1.7571,
      "step": 1672
    },
    {
      "epoch": 0.6476964769647696,
      "grad_norm": 10.438446998596191,
      "learning_rate": 6.476964769647697e-06,
      "loss": 1.8701,
      "step": 1673
    },
    {
      "epoch": 0.6480836236933798,
      "grad_norm": 10.539752960205078,
      "learning_rate": 6.480836236933798e-06,
      "loss": 1.6691,
      "step": 1674
    },
    {
      "epoch": 0.64847077042199,
      "grad_norm": 9.968301773071289,
      "learning_rate": 6.4847077042199e-06,
      "loss": 1.511,
      "step": 1675
    },
    {
      "epoch": 0.6488579171506,
      "grad_norm": 12.567091941833496,
      "learning_rate": 6.488579171506001e-06,
      "loss": 1.3022,
      "step": 1676
    },
    {
      "epoch": 0.6492450638792102,
      "grad_norm": 9.34203815460205,
      "learning_rate": 6.492450638792103e-06,
      "loss": 1.7131,
      "step": 1677
    },
    {
      "epoch": 0.6496322106078204,
      "grad_norm": 10.73672866821289,
      "learning_rate": 6.496322106078204e-06,
      "loss": 1.4045,
      "step": 1678
    },
    {
      "epoch": 0.6500193573364305,
      "grad_norm": 9.564488410949707,
      "learning_rate": 6.500193573364305e-06,
      "loss": 1.5883,
      "step": 1679
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 16.660350799560547,
      "learning_rate": 6.504065040650407e-06,
      "loss": 2.0124,
      "step": 1680
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 9.59033489227295,
      "learning_rate": 6.507936507936509e-06,
      "loss": 1.6768,
      "step": 1681
    },
    {
      "epoch": 0.651180797522261,
      "grad_norm": 10.19639778137207,
      "learning_rate": 6.51180797522261e-06,
      "loss": 1.3712,
      "step": 1682
    },
    {
      "epoch": 0.6515679442508711,
      "grad_norm": 10.807090759277344,
      "learning_rate": 6.515679442508711e-06,
      "loss": 1.6953,
      "step": 1683
    },
    {
      "epoch": 0.6519550909794812,
      "grad_norm": 16.682161331176758,
      "learning_rate": 6.519550909794813e-06,
      "loss": 1.6858,
      "step": 1684
    },
    {
      "epoch": 0.6523422377080914,
      "grad_norm": 5.795151710510254,
      "learning_rate": 6.523422377080915e-06,
      "loss": 1.8299,
      "step": 1685
    },
    {
      "epoch": 0.6527293844367015,
      "grad_norm": 11.262455940246582,
      "learning_rate": 6.5272938443670154e-06,
      "loss": 1.2982,
      "step": 1686
    },
    {
      "epoch": 0.6531165311653117,
      "grad_norm": 18.409643173217773,
      "learning_rate": 6.531165311653117e-06,
      "loss": 1.8875,
      "step": 1687
    },
    {
      "epoch": 0.6535036778939218,
      "grad_norm": 12.072376251220703,
      "learning_rate": 6.535036778939219e-06,
      "loss": 1.7107,
      "step": 1688
    },
    {
      "epoch": 0.6538908246225319,
      "grad_norm": 9.39417839050293,
      "learning_rate": 6.53890824622532e-06,
      "loss": 1.7486,
      "step": 1689
    },
    {
      "epoch": 0.6542779713511421,
      "grad_norm": 14.508858680725098,
      "learning_rate": 6.542779713511421e-06,
      "loss": 1.4811,
      "step": 1690
    },
    {
      "epoch": 0.6546651180797523,
      "grad_norm": 11.4152250289917,
      "learning_rate": 6.546651180797523e-06,
      "loss": 1.8255,
      "step": 1691
    },
    {
      "epoch": 0.6550522648083623,
      "grad_norm": 14.729401588439941,
      "learning_rate": 6.5505226480836235e-06,
      "loss": 1.9383,
      "step": 1692
    },
    {
      "epoch": 0.6554394115369725,
      "grad_norm": 20.702415466308594,
      "learning_rate": 6.554394115369726e-06,
      "loss": 1.7911,
      "step": 1693
    },
    {
      "epoch": 0.6558265582655827,
      "grad_norm": 11.477478981018066,
      "learning_rate": 6.558265582655827e-06,
      "loss": 1.7735,
      "step": 1694
    },
    {
      "epoch": 0.6562137049941928,
      "grad_norm": 16.803979873657227,
      "learning_rate": 6.562137049941929e-06,
      "loss": 2.2658,
      "step": 1695
    },
    {
      "epoch": 0.6566008517228029,
      "grad_norm": 8.346403121948242,
      "learning_rate": 6.5660085172280295e-06,
      "loss": 1.6516,
      "step": 1696
    },
    {
      "epoch": 0.6569879984514131,
      "grad_norm": 10.942815780639648,
      "learning_rate": 6.569879984514132e-06,
      "loss": 1.3745,
      "step": 1697
    },
    {
      "epoch": 0.6573751451800233,
      "grad_norm": 9.2357177734375,
      "learning_rate": 6.573751451800233e-06,
      "loss": 1.5242,
      "step": 1698
    },
    {
      "epoch": 0.6577622919086333,
      "grad_norm": 9.346148490905762,
      "learning_rate": 6.577622919086334e-06,
      "loss": 1.5722,
      "step": 1699
    },
    {
      "epoch": 0.6581494386372435,
      "grad_norm": 14.438264846801758,
      "learning_rate": 6.5814943863724355e-06,
      "loss": 1.5598,
      "step": 1700
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 25.445663452148438,
      "learning_rate": 6.585365853658538e-06,
      "loss": 2.0345,
      "step": 1701
    },
    {
      "epoch": 0.6589237320944638,
      "grad_norm": 8.108845710754395,
      "learning_rate": 6.5892373209446385e-06,
      "loss": 1.5409,
      "step": 1702
    },
    {
      "epoch": 0.6593108788230739,
      "grad_norm": 9.686677932739258,
      "learning_rate": 6.59310878823074e-06,
      "loss": 1.6921,
      "step": 1703
    },
    {
      "epoch": 0.6596980255516841,
      "grad_norm": 10.099474906921387,
      "learning_rate": 6.5969802555168415e-06,
      "loss": 1.7747,
      "step": 1704
    },
    {
      "epoch": 0.6600851722802943,
      "grad_norm": 9.691216468811035,
      "learning_rate": 6.600851722802944e-06,
      "loss": 1.7017,
      "step": 1705
    },
    {
      "epoch": 0.6604723190089044,
      "grad_norm": 10.665839195251465,
      "learning_rate": 6.6047231900890444e-06,
      "loss": 1.7126,
      "step": 1706
    },
    {
      "epoch": 0.6608594657375145,
      "grad_norm": 9.948758125305176,
      "learning_rate": 6.608594657375146e-06,
      "loss": 1.6053,
      "step": 1707
    },
    {
      "epoch": 0.6612466124661247,
      "grad_norm": 9.160399436950684,
      "learning_rate": 6.6124661246612474e-06,
      "loss": 1.6367,
      "step": 1708
    },
    {
      "epoch": 0.6616337591947348,
      "grad_norm": 12.585317611694336,
      "learning_rate": 6.616337591947348e-06,
      "loss": 1.8175,
      "step": 1709
    },
    {
      "epoch": 0.662020905923345,
      "grad_norm": 9.440046310424805,
      "learning_rate": 6.62020905923345e-06,
      "loss": 1.7824,
      "step": 1710
    },
    {
      "epoch": 0.6624080526519551,
      "grad_norm": 10.115623474121094,
      "learning_rate": 6.624080526519552e-06,
      "loss": 1.2509,
      "step": 1711
    },
    {
      "epoch": 0.6627951993805652,
      "grad_norm": 10.586886405944824,
      "learning_rate": 6.6279519938056526e-06,
      "loss": 1.7563,
      "step": 1712
    },
    {
      "epoch": 0.6631823461091754,
      "grad_norm": 14.908645629882812,
      "learning_rate": 6.631823461091754e-06,
      "loss": 1.6461,
      "step": 1713
    },
    {
      "epoch": 0.6635694928377855,
      "grad_norm": 13.411029815673828,
      "learning_rate": 6.635694928377856e-06,
      "loss": 1.9984,
      "step": 1714
    },
    {
      "epoch": 0.6639566395663956,
      "grad_norm": 9.240623474121094,
      "learning_rate": 6.639566395663957e-06,
      "loss": 1.5382,
      "step": 1715
    },
    {
      "epoch": 0.6643437862950058,
      "grad_norm": 12.124184608459473,
      "learning_rate": 6.6434378629500585e-06,
      "loss": 1.5087,
      "step": 1716
    },
    {
      "epoch": 0.664730933023616,
      "grad_norm": 20.874603271484375,
      "learning_rate": 6.64730933023616e-06,
      "loss": 1.6838,
      "step": 1717
    },
    {
      "epoch": 0.6651180797522261,
      "grad_norm": 10.21602725982666,
      "learning_rate": 6.651180797522262e-06,
      "loss": 1.9602,
      "step": 1718
    },
    {
      "epoch": 0.6655052264808362,
      "grad_norm": 10.535090446472168,
      "learning_rate": 6.655052264808363e-06,
      "loss": 1.7679,
      "step": 1719
    },
    {
      "epoch": 0.6658923732094464,
      "grad_norm": 8.714552879333496,
      "learning_rate": 6.6589237320944645e-06,
      "loss": 1.6743,
      "step": 1720
    },
    {
      "epoch": 0.6662795199380566,
      "grad_norm": 16.960966110229492,
      "learning_rate": 6.662795199380566e-06,
      "loss": 1.7441,
      "step": 1721
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 8.444924354553223,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.2433,
      "step": 1722
    },
    {
      "epoch": 0.6670538133952768,
      "grad_norm": 14.322436332702637,
      "learning_rate": 6.670538133952769e-06,
      "loss": 1.4488,
      "step": 1723
    },
    {
      "epoch": 0.667440960123887,
      "grad_norm": 16.21245002746582,
      "learning_rate": 6.6744096012388705e-06,
      "loss": 1.6602,
      "step": 1724
    },
    {
      "epoch": 0.667828106852497,
      "grad_norm": 15.279275894165039,
      "learning_rate": 6.678281068524971e-06,
      "loss": 1.726,
      "step": 1725
    },
    {
      "epoch": 0.6682152535811072,
      "grad_norm": 9.365312576293945,
      "learning_rate": 6.682152535811073e-06,
      "loss": 1.5979,
      "step": 1726
    },
    {
      "epoch": 0.6686024003097174,
      "grad_norm": 7.099853515625,
      "learning_rate": 6.686024003097175e-06,
      "loss": 1.7253,
      "step": 1727
    },
    {
      "epoch": 0.6689895470383276,
      "grad_norm": 18.042552947998047,
      "learning_rate": 6.6898954703832765e-06,
      "loss": 2.1572,
      "step": 1728
    },
    {
      "epoch": 0.6693766937669376,
      "grad_norm": 12.843842506408691,
      "learning_rate": 6.693766937669377e-06,
      "loss": 1.428,
      "step": 1729
    },
    {
      "epoch": 0.6697638404955478,
      "grad_norm": 8.317930221557617,
      "learning_rate": 6.697638404955479e-06,
      "loss": 1.8253,
      "step": 1730
    },
    {
      "epoch": 0.670150987224158,
      "grad_norm": 12.043935775756836,
      "learning_rate": 6.701509872241581e-06,
      "loss": 1.7482,
      "step": 1731
    },
    {
      "epoch": 0.6705381339527681,
      "grad_norm": 15.44277572631836,
      "learning_rate": 6.705381339527682e-06,
      "loss": 1.6465,
      "step": 1732
    },
    {
      "epoch": 0.6709252806813782,
      "grad_norm": 14.590611457824707,
      "learning_rate": 6.709252806813783e-06,
      "loss": 1.6486,
      "step": 1733
    },
    {
      "epoch": 0.6713124274099884,
      "grad_norm": 12.6495943069458,
      "learning_rate": 6.7131242740998846e-06,
      "loss": 1.5811,
      "step": 1734
    },
    {
      "epoch": 0.6716995741385985,
      "grad_norm": 12.761370658874512,
      "learning_rate": 6.716995741385985e-06,
      "loss": 1.6173,
      "step": 1735
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 28.80975914001465,
      "learning_rate": 6.7208672086720876e-06,
      "loss": 2.128,
      "step": 1736
    },
    {
      "epoch": 0.6724738675958188,
      "grad_norm": 11.561868667602539,
      "learning_rate": 6.724738675958189e-06,
      "loss": 1.7178,
      "step": 1737
    },
    {
      "epoch": 0.6728610143244289,
      "grad_norm": 12.951156616210938,
      "learning_rate": 6.72861014324429e-06,
      "loss": 1.6467,
      "step": 1738
    },
    {
      "epoch": 0.6732481610530391,
      "grad_norm": 17.135419845581055,
      "learning_rate": 6.732481610530391e-06,
      "loss": 1.7315,
      "step": 1739
    },
    {
      "epoch": 0.6736353077816493,
      "grad_norm": 13.040820121765137,
      "learning_rate": 6.7363530778164935e-06,
      "loss": 1.6077,
      "step": 1740
    },
    {
      "epoch": 0.6740224545102594,
      "grad_norm": 9.455711364746094,
      "learning_rate": 6.740224545102595e-06,
      "loss": 1.6266,
      "step": 1741
    },
    {
      "epoch": 0.6744096012388695,
      "grad_norm": 9.62055778503418,
      "learning_rate": 6.744096012388696e-06,
      "loss": 1.6667,
      "step": 1742
    },
    {
      "epoch": 0.6747967479674797,
      "grad_norm": 7.899770736694336,
      "learning_rate": 6.747967479674797e-06,
      "loss": 1.2322,
      "step": 1743
    },
    {
      "epoch": 0.6751838946960899,
      "grad_norm": 9.839421272277832,
      "learning_rate": 6.7518389469608995e-06,
      "loss": 1.5933,
      "step": 1744
    },
    {
      "epoch": 0.6755710414246999,
      "grad_norm": 11.663894653320312,
      "learning_rate": 6.755710414247e-06,
      "loss": 1.6245,
      "step": 1745
    },
    {
      "epoch": 0.6759581881533101,
      "grad_norm": 8.384481430053711,
      "learning_rate": 6.759581881533102e-06,
      "loss": 1.6683,
      "step": 1746
    },
    {
      "epoch": 0.6763453348819203,
      "grad_norm": 16.135602951049805,
      "learning_rate": 6.763453348819203e-06,
      "loss": 1.7899,
      "step": 1747
    },
    {
      "epoch": 0.6767324816105303,
      "grad_norm": 10.426004409790039,
      "learning_rate": 6.767324816105304e-06,
      "loss": 1.3406,
      "step": 1748
    },
    {
      "epoch": 0.6771196283391405,
      "grad_norm": 12.822816848754883,
      "learning_rate": 6.771196283391406e-06,
      "loss": 1.5617,
      "step": 1749
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 14.033377647399902,
      "learning_rate": 6.775067750677508e-06,
      "loss": 2.1645,
      "step": 1750
    },
    {
      "epoch": 0.6778939217963608,
      "grad_norm": 30.764177322387695,
      "learning_rate": 6.778939217963608e-06,
      "loss": 1.9821,
      "step": 1751
    },
    {
      "epoch": 0.6782810685249709,
      "grad_norm": 7.057508945465088,
      "learning_rate": 6.78281068524971e-06,
      "loss": 1.229,
      "step": 1752
    },
    {
      "epoch": 0.6786682152535811,
      "grad_norm": 6.876107215881348,
      "learning_rate": 6.786682152535812e-06,
      "loss": 1.3643,
      "step": 1753
    },
    {
      "epoch": 0.6790553619821913,
      "grad_norm": 10.17329216003418,
      "learning_rate": 6.790553619821914e-06,
      "loss": 1.3381,
      "step": 1754
    },
    {
      "epoch": 0.6794425087108014,
      "grad_norm": 9.891817092895508,
      "learning_rate": 6.794425087108014e-06,
      "loss": 1.6959,
      "step": 1755
    },
    {
      "epoch": 0.6798296554394115,
      "grad_norm": 12.845528602600098,
      "learning_rate": 6.798296554394116e-06,
      "loss": 1.4437,
      "step": 1756
    },
    {
      "epoch": 0.6802168021680217,
      "grad_norm": 14.759862899780273,
      "learning_rate": 6.802168021680218e-06,
      "loss": 1.598,
      "step": 1757
    },
    {
      "epoch": 0.6806039488966318,
      "grad_norm": 14.325324058532715,
      "learning_rate": 6.806039488966319e-06,
      "loss": 1.6625,
      "step": 1758
    },
    {
      "epoch": 0.680991095625242,
      "grad_norm": 8.000545501708984,
      "learning_rate": 6.80991095625242e-06,
      "loss": 1.5641,
      "step": 1759
    },
    {
      "epoch": 0.6813782423538521,
      "grad_norm": 15.356788635253906,
      "learning_rate": 6.813782423538522e-06,
      "loss": 1.7632,
      "step": 1760
    },
    {
      "epoch": 0.6817653890824622,
      "grad_norm": 15.611788749694824,
      "learning_rate": 6.817653890824622e-06,
      "loss": 1.724,
      "step": 1761
    },
    {
      "epoch": 0.6821525358110724,
      "grad_norm": 13.338688850402832,
      "learning_rate": 6.821525358110725e-06,
      "loss": 1.4255,
      "step": 1762
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 9.277602195739746,
      "learning_rate": 6.825396825396826e-06,
      "loss": 1.1634,
      "step": 1763
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 10.77285099029541,
      "learning_rate": 6.829268292682928e-06,
      "loss": 1.2284,
      "step": 1764
    },
    {
      "epoch": 0.6833139759969028,
      "grad_norm": 10.962461471557617,
      "learning_rate": 6.833139759969028e-06,
      "loss": 1.5134,
      "step": 1765
    },
    {
      "epoch": 0.683701122725513,
      "grad_norm": 9.403000831604004,
      "learning_rate": 6.837011227255131e-06,
      "loss": 1.3402,
      "step": 1766
    },
    {
      "epoch": 0.6840882694541232,
      "grad_norm": 12.388096809387207,
      "learning_rate": 6.840882694541232e-06,
      "loss": 2.1403,
      "step": 1767
    },
    {
      "epoch": 0.6844754161827332,
      "grad_norm": 11.67637825012207,
      "learning_rate": 6.844754161827333e-06,
      "loss": 1.4719,
      "step": 1768
    },
    {
      "epoch": 0.6848625629113434,
      "grad_norm": 13.047776222229004,
      "learning_rate": 6.848625629113434e-06,
      "loss": 0.9589,
      "step": 1769
    },
    {
      "epoch": 0.6852497096399536,
      "grad_norm": 13.283334732055664,
      "learning_rate": 6.852497096399537e-06,
      "loss": 0.8937,
      "step": 1770
    },
    {
      "epoch": 0.6856368563685636,
      "grad_norm": 8.076489448547363,
      "learning_rate": 6.856368563685637e-06,
      "loss": 1.5734,
      "step": 1771
    },
    {
      "epoch": 0.6860240030971738,
      "grad_norm": 11.028087615966797,
      "learning_rate": 6.860240030971739e-06,
      "loss": 1.5626,
      "step": 1772
    },
    {
      "epoch": 0.686411149825784,
      "grad_norm": 10.923970222473145,
      "learning_rate": 6.86411149825784e-06,
      "loss": 1.556,
      "step": 1773
    },
    {
      "epoch": 0.6867982965543941,
      "grad_norm": 10.3269681930542,
      "learning_rate": 6.867982965543941e-06,
      "loss": 1.2788,
      "step": 1774
    },
    {
      "epoch": 0.6871854432830042,
      "grad_norm": 29.58184814453125,
      "learning_rate": 6.871854432830043e-06,
      "loss": 2.13,
      "step": 1775
    },
    {
      "epoch": 0.6875725900116144,
      "grad_norm": 13.688886642456055,
      "learning_rate": 6.875725900116145e-06,
      "loss": 1.5384,
      "step": 1776
    },
    {
      "epoch": 0.6879597367402246,
      "grad_norm": 9.623886108398438,
      "learning_rate": 6.879597367402246e-06,
      "loss": 1.6521,
      "step": 1777
    },
    {
      "epoch": 0.6883468834688347,
      "grad_norm": 8.089095115661621,
      "learning_rate": 6.883468834688347e-06,
      "loss": 1.4718,
      "step": 1778
    },
    {
      "epoch": 0.6887340301974448,
      "grad_norm": 13.436201095581055,
      "learning_rate": 6.887340301974449e-06,
      "loss": 1.4377,
      "step": 1779
    },
    {
      "epoch": 0.689121176926055,
      "grad_norm": 12.04425048828125,
      "learning_rate": 6.891211769260551e-06,
      "loss": 1.7137,
      "step": 1780
    },
    {
      "epoch": 0.6895083236546651,
      "grad_norm": 26.844402313232422,
      "learning_rate": 6.895083236546651e-06,
      "loss": 1.511,
      "step": 1781
    },
    {
      "epoch": 0.6898954703832753,
      "grad_norm": 15.213305473327637,
      "learning_rate": 6.898954703832753e-06,
      "loss": 1.3233,
      "step": 1782
    },
    {
      "epoch": 0.6902826171118854,
      "grad_norm": 11.502018928527832,
      "learning_rate": 6.902826171118855e-06,
      "loss": 1.8445,
      "step": 1783
    },
    {
      "epoch": 0.6906697638404955,
      "grad_norm": 9.076035499572754,
      "learning_rate": 6.906697638404956e-06,
      "loss": 1.578,
      "step": 1784
    },
    {
      "epoch": 0.6910569105691057,
      "grad_norm": 16.82877540588379,
      "learning_rate": 6.910569105691057e-06,
      "loss": 1.7182,
      "step": 1785
    },
    {
      "epoch": 0.6914440572977159,
      "grad_norm": 25.262794494628906,
      "learning_rate": 6.914440572977159e-06,
      "loss": 2.0436,
      "step": 1786
    },
    {
      "epoch": 0.691831204026326,
      "grad_norm": 11.96796703338623,
      "learning_rate": 6.918312040263261e-06,
      "loss": 1.9054,
      "step": 1787
    },
    {
      "epoch": 0.6922183507549361,
      "grad_norm": 10.857501029968262,
      "learning_rate": 6.922183507549362e-06,
      "loss": 1.7282,
      "step": 1788
    },
    {
      "epoch": 0.6926054974835463,
      "grad_norm": 13.52420711517334,
      "learning_rate": 6.926054974835463e-06,
      "loss": 1.7775,
      "step": 1789
    },
    {
      "epoch": 0.6929926442121565,
      "grad_norm": 10.676712036132812,
      "learning_rate": 6.929926442121565e-06,
      "loss": 1.2428,
      "step": 1790
    },
    {
      "epoch": 0.6933797909407665,
      "grad_norm": 8.852523803710938,
      "learning_rate": 6.9337979094076655e-06,
      "loss": 1.8614,
      "step": 1791
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 21.561601638793945,
      "learning_rate": 6.937669376693768e-06,
      "loss": 1.6159,
      "step": 1792
    },
    {
      "epoch": 0.6941540843979869,
      "grad_norm": 13.067060470581055,
      "learning_rate": 6.941540843979869e-06,
      "loss": 0.8923,
      "step": 1793
    },
    {
      "epoch": 0.6945412311265969,
      "grad_norm": 11.131730079650879,
      "learning_rate": 6.94541231126597e-06,
      "loss": 1.2655,
      "step": 1794
    },
    {
      "epoch": 0.6949283778552071,
      "grad_norm": 9.820119857788086,
      "learning_rate": 6.9492837785520714e-06,
      "loss": 1.3564,
      "step": 1795
    },
    {
      "epoch": 0.6953155245838173,
      "grad_norm": 10.39277172088623,
      "learning_rate": 6.953155245838174e-06,
      "loss": 1.2747,
      "step": 1796
    },
    {
      "epoch": 0.6957026713124274,
      "grad_norm": 20.97304344177246,
      "learning_rate": 6.957026713124274e-06,
      "loss": 1.9854,
      "step": 1797
    },
    {
      "epoch": 0.6960898180410375,
      "grad_norm": 10.669303894042969,
      "learning_rate": 6.960898180410376e-06,
      "loss": 1.2316,
      "step": 1798
    },
    {
      "epoch": 0.6964769647696477,
      "grad_norm": 10.355815887451172,
      "learning_rate": 6.964769647696477e-06,
      "loss": 1.6321,
      "step": 1799
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 18.02633285522461,
      "learning_rate": 6.96864111498258e-06,
      "loss": 1.7068,
      "step": 1800
    },
    {
      "epoch": 0.697251258226868,
      "grad_norm": 13.13068675994873,
      "learning_rate": 6.97251258226868e-06,
      "loss": 1.5696,
      "step": 1801
    },
    {
      "epoch": 0.6976384049554781,
      "grad_norm": 11.295896530151367,
      "learning_rate": 6.976384049554782e-06,
      "loss": 1.5582,
      "step": 1802
    },
    {
      "epoch": 0.6980255516840883,
      "grad_norm": 15.978182792663574,
      "learning_rate": 6.980255516840883e-06,
      "loss": 2.0441,
      "step": 1803
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 12.968184471130371,
      "learning_rate": 6.984126984126984e-06,
      "loss": 1.5884,
      "step": 1804
    },
    {
      "epoch": 0.6987998451413086,
      "grad_norm": 14.512860298156738,
      "learning_rate": 6.987998451413086e-06,
      "loss": 1.5692,
      "step": 1805
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 23.638874053955078,
      "learning_rate": 6.991869918699188e-06,
      "loss": 2.0531,
      "step": 1806
    },
    {
      "epoch": 0.6995741385985288,
      "grad_norm": 13.984369277954102,
      "learning_rate": 6.9957413859852885e-06,
      "loss": 1.4325,
      "step": 1807
    },
    {
      "epoch": 0.699961285327139,
      "grad_norm": 21.825904846191406,
      "learning_rate": 6.99961285327139e-06,
      "loss": 1.827,
      "step": 1808
    },
    {
      "epoch": 0.7003484320557491,
      "grad_norm": 11.423483848571777,
      "learning_rate": 7.003484320557492e-06,
      "loss": 1.6614,
      "step": 1809
    },
    {
      "epoch": 0.7007355787843593,
      "grad_norm": 11.648202896118164,
      "learning_rate": 7.007355787843594e-06,
      "loss": 1.2566,
      "step": 1810
    },
    {
      "epoch": 0.7011227255129694,
      "grad_norm": 9.72801399230957,
      "learning_rate": 7.0112272551296945e-06,
      "loss": 1.5388,
      "step": 1811
    },
    {
      "epoch": 0.7015098722415796,
      "grad_norm": 11.390835762023926,
      "learning_rate": 7.015098722415796e-06,
      "loss": 1.7097,
      "step": 1812
    },
    {
      "epoch": 0.7018970189701897,
      "grad_norm": 11.721148490905762,
      "learning_rate": 7.018970189701898e-06,
      "loss": 1.8019,
      "step": 1813
    },
    {
      "epoch": 0.7022841656987998,
      "grad_norm": 9.784533500671387,
      "learning_rate": 7.022841656987999e-06,
      "loss": 1.5691,
      "step": 1814
    },
    {
      "epoch": 0.70267131242741,
      "grad_norm": 24.900054931640625,
      "learning_rate": 7.0267131242741005e-06,
      "loss": 1.9059,
      "step": 1815
    },
    {
      "epoch": 0.7030584591560202,
      "grad_norm": 10.221701622009277,
      "learning_rate": 7.030584591560202e-06,
      "loss": 1.1204,
      "step": 1816
    },
    {
      "epoch": 0.7034456058846302,
      "grad_norm": 9.470000267028809,
      "learning_rate": 7.034456058846303e-06,
      "loss": 1.5417,
      "step": 1817
    },
    {
      "epoch": 0.7038327526132404,
      "grad_norm": 10.516840934753418,
      "learning_rate": 7.038327526132405e-06,
      "loss": 1.1856,
      "step": 1818
    },
    {
      "epoch": 0.7042198993418506,
      "grad_norm": 16.654998779296875,
      "learning_rate": 7.0421989934185064e-06,
      "loss": 1.5317,
      "step": 1819
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 21.645463943481445,
      "learning_rate": 7.046070460704607e-06,
      "loss": 2.1117,
      "step": 1820
    },
    {
      "epoch": 0.7049941927990708,
      "grad_norm": 10.257989883422852,
      "learning_rate": 7.0499419279907086e-06,
      "loss": 1.5766,
      "step": 1821
    },
    {
      "epoch": 0.705381339527681,
      "grad_norm": 9.5421724319458,
      "learning_rate": 7.053813395276811e-06,
      "loss": 1.6297,
      "step": 1822
    },
    {
      "epoch": 0.7057684862562912,
      "grad_norm": 9.262897491455078,
      "learning_rate": 7.057684862562912e-06,
      "loss": 1.5097,
      "step": 1823
    },
    {
      "epoch": 0.7061556329849012,
      "grad_norm": 10.464183807373047,
      "learning_rate": 7.061556329849013e-06,
      "loss": 1.6753,
      "step": 1824
    },
    {
      "epoch": 0.7065427797135114,
      "grad_norm": 9.965311050415039,
      "learning_rate": 7.0654277971351145e-06,
      "loss": 1.5756,
      "step": 1825
    },
    {
      "epoch": 0.7069299264421216,
      "grad_norm": 10.674848556518555,
      "learning_rate": 7.069299264421217e-06,
      "loss": 1.8911,
      "step": 1826
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 9.590476036071777,
      "learning_rate": 7.0731707317073175e-06,
      "loss": 1.5828,
      "step": 1827
    },
    {
      "epoch": 0.7077042198993418,
      "grad_norm": 15.461480140686035,
      "learning_rate": 7.077042198993419e-06,
      "loss": 1.8836,
      "step": 1828
    },
    {
      "epoch": 0.708091366627952,
      "grad_norm": 31.93998908996582,
      "learning_rate": 7.0809136662795205e-06,
      "loss": 1.9967,
      "step": 1829
    },
    {
      "epoch": 0.7084785133565621,
      "grad_norm": 12.130507469177246,
      "learning_rate": 7.084785133565621e-06,
      "loss": 1.9402,
      "step": 1830
    },
    {
      "epoch": 0.7088656600851723,
      "grad_norm": 14.148700714111328,
      "learning_rate": 7.0886566008517235e-06,
      "loss": 1.9952,
      "step": 1831
    },
    {
      "epoch": 0.7092528068137824,
      "grad_norm": 10.896751403808594,
      "learning_rate": 7.092528068137825e-06,
      "loss": 1.4159,
      "step": 1832
    },
    {
      "epoch": 0.7096399535423926,
      "grad_norm": 17.4697322845459,
      "learning_rate": 7.0963995354239265e-06,
      "loss": 1.6221,
      "step": 1833
    },
    {
      "epoch": 0.7100271002710027,
      "grad_norm": 13.097932815551758,
      "learning_rate": 7.100271002710027e-06,
      "loss": 2.3555,
      "step": 1834
    },
    {
      "epoch": 0.7104142469996129,
      "grad_norm": 12.204648971557617,
      "learning_rate": 7.1041424699961295e-06,
      "loss": 1.8923,
      "step": 1835
    },
    {
      "epoch": 0.710801393728223,
      "grad_norm": 17.001800537109375,
      "learning_rate": 7.108013937282231e-06,
      "loss": 1.6488,
      "step": 1836
    },
    {
      "epoch": 0.7111885404568331,
      "grad_norm": 12.56925106048584,
      "learning_rate": 7.111885404568332e-06,
      "loss": 0.7874,
      "step": 1837
    },
    {
      "epoch": 0.7115756871854433,
      "grad_norm": 11.63797664642334,
      "learning_rate": 7.115756871854433e-06,
      "loss": 1.2369,
      "step": 1838
    },
    {
      "epoch": 0.7119628339140535,
      "grad_norm": 10.654993057250977,
      "learning_rate": 7.1196283391405354e-06,
      "loss": 1.0972,
      "step": 1839
    },
    {
      "epoch": 0.7123499806426635,
      "grad_norm": 11.287028312683105,
      "learning_rate": 7.123499806426636e-06,
      "loss": 1.4792,
      "step": 1840
    },
    {
      "epoch": 0.7127371273712737,
      "grad_norm": 10.462878227233887,
      "learning_rate": 7.127371273712738e-06,
      "loss": 1.1993,
      "step": 1841
    },
    {
      "epoch": 0.7131242740998839,
      "grad_norm": 18.554828643798828,
      "learning_rate": 7.131242740998839e-06,
      "loss": 1.5575,
      "step": 1842
    },
    {
      "epoch": 0.713511420828494,
      "grad_norm": 12.615894317626953,
      "learning_rate": 7.13511420828494e-06,
      "loss": 1.8971,
      "step": 1843
    },
    {
      "epoch": 0.7138985675571041,
      "grad_norm": 13.771031379699707,
      "learning_rate": 7.138985675571042e-06,
      "loss": 1.5347,
      "step": 1844
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 10.006608963012695,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 1.5088,
      "step": 1845
    },
    {
      "epoch": 0.7146728610143245,
      "grad_norm": 12.236921310424805,
      "learning_rate": 7.146728610143245e-06,
      "loss": 1.6373,
      "step": 1846
    },
    {
      "epoch": 0.7150600077429345,
      "grad_norm": 9.374247550964355,
      "learning_rate": 7.150600077429346e-06,
      "loss": 1.5926,
      "step": 1847
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 17.186115264892578,
      "learning_rate": 7.154471544715448e-06,
      "loss": 1.5349,
      "step": 1848
    },
    {
      "epoch": 0.7158343012001549,
      "grad_norm": 12.181107521057129,
      "learning_rate": 7.1583430120015495e-06,
      "loss": 1.9095,
      "step": 1849
    },
    {
      "epoch": 0.716221447928765,
      "grad_norm": 11.187947273254395,
      "learning_rate": 7.16221447928765e-06,
      "loss": 1.3003,
      "step": 1850
    },
    {
      "epoch": 0.7166085946573751,
      "grad_norm": 12.072371482849121,
      "learning_rate": 7.166085946573752e-06,
      "loss": 2.349,
      "step": 1851
    },
    {
      "epoch": 0.7169957413859853,
      "grad_norm": 13.392343521118164,
      "learning_rate": 7.169957413859854e-06,
      "loss": 1.968,
      "step": 1852
    },
    {
      "epoch": 0.7173828881145954,
      "grad_norm": 30.259838104248047,
      "learning_rate": 7.173828881145955e-06,
      "loss": 2.4742,
      "step": 1853
    },
    {
      "epoch": 0.7177700348432056,
      "grad_norm": 11.19955062866211,
      "learning_rate": 7.177700348432056e-06,
      "loss": 1.1376,
      "step": 1854
    },
    {
      "epoch": 0.7181571815718157,
      "grad_norm": 13.725021362304688,
      "learning_rate": 7.181571815718158e-06,
      "loss": 2.4004,
      "step": 1855
    },
    {
      "epoch": 0.7185443283004259,
      "grad_norm": 9.970237731933594,
      "learning_rate": 7.18544328300426e-06,
      "loss": 1.2084,
      "step": 1856
    },
    {
      "epoch": 0.718931475029036,
      "grad_norm": 18.29294204711914,
      "learning_rate": 7.189314750290361e-06,
      "loss": 2.2405,
      "step": 1857
    },
    {
      "epoch": 0.7193186217576462,
      "grad_norm": 22.2736759185791,
      "learning_rate": 7.193186217576462e-06,
      "loss": 1.7058,
      "step": 1858
    },
    {
      "epoch": 0.7197057684862563,
      "grad_norm": 13.911639213562012,
      "learning_rate": 7.197057684862564e-06,
      "loss": 1.984,
      "step": 1859
    },
    {
      "epoch": 0.7200929152148664,
      "grad_norm": 12.247221946716309,
      "learning_rate": 7.200929152148664e-06,
      "loss": 1.348,
      "step": 1860
    },
    {
      "epoch": 0.7204800619434766,
      "grad_norm": 8.755260467529297,
      "learning_rate": 7.204800619434767e-06,
      "loss": 1.8339,
      "step": 1861
    },
    {
      "epoch": 0.7208672086720868,
      "grad_norm": 11.214261054992676,
      "learning_rate": 7.208672086720868e-06,
      "loss": 1.6176,
      "step": 1862
    },
    {
      "epoch": 0.7212543554006968,
      "grad_norm": 12.732138633728027,
      "learning_rate": 7.212543554006969e-06,
      "loss": 1.3676,
      "step": 1863
    },
    {
      "epoch": 0.721641502129307,
      "grad_norm": 10.964174270629883,
      "learning_rate": 7.21641502129307e-06,
      "loss": 1.1852,
      "step": 1864
    },
    {
      "epoch": 0.7220286488579172,
      "grad_norm": 14.338828086853027,
      "learning_rate": 7.220286488579173e-06,
      "loss": 2.0766,
      "step": 1865
    },
    {
      "epoch": 0.7224157955865272,
      "grad_norm": 10.876822471618652,
      "learning_rate": 7.224157955865273e-06,
      "loss": 1.1081,
      "step": 1866
    },
    {
      "epoch": 0.7228029423151374,
      "grad_norm": 18.84029769897461,
      "learning_rate": 7.228029423151375e-06,
      "loss": 1.7809,
      "step": 1867
    },
    {
      "epoch": 0.7231900890437476,
      "grad_norm": 33.96131134033203,
      "learning_rate": 7.231900890437476e-06,
      "loss": 1.7871,
      "step": 1868
    },
    {
      "epoch": 0.7235772357723578,
      "grad_norm": 17.277557373046875,
      "learning_rate": 7.2357723577235786e-06,
      "loss": 1.5063,
      "step": 1869
    },
    {
      "epoch": 0.7239643825009678,
      "grad_norm": 10.946242332458496,
      "learning_rate": 7.239643825009679e-06,
      "loss": 1.6909,
      "step": 1870
    },
    {
      "epoch": 0.724351529229578,
      "grad_norm": 10.621179580688477,
      "learning_rate": 7.243515292295781e-06,
      "loss": 1.13,
      "step": 1871
    },
    {
      "epoch": 0.7247386759581882,
      "grad_norm": 15.861169815063477,
      "learning_rate": 7.247386759581882e-06,
      "loss": 1.7049,
      "step": 1872
    },
    {
      "epoch": 0.7251258226867983,
      "grad_norm": 13.01651668548584,
      "learning_rate": 7.251258226867983e-06,
      "loss": 0.9608,
      "step": 1873
    },
    {
      "epoch": 0.7255129694154084,
      "grad_norm": 13.291890144348145,
      "learning_rate": 7.255129694154085e-06,
      "loss": 1.4186,
      "step": 1874
    },
    {
      "epoch": 0.7259001161440186,
      "grad_norm": 9.263340950012207,
      "learning_rate": 7.259001161440187e-06,
      "loss": 1.2941,
      "step": 1875
    },
    {
      "epoch": 0.7262872628726287,
      "grad_norm": 12.005663871765137,
      "learning_rate": 7.262872628726287e-06,
      "loss": 1.5537,
      "step": 1876
    },
    {
      "epoch": 0.7266744096012389,
      "grad_norm": 14.243425369262695,
      "learning_rate": 7.266744096012389e-06,
      "loss": 1.1913,
      "step": 1877
    },
    {
      "epoch": 0.727061556329849,
      "grad_norm": 17.449832916259766,
      "learning_rate": 7.270615563298491e-06,
      "loss": 2.2363,
      "step": 1878
    },
    {
      "epoch": 0.7274487030584591,
      "grad_norm": 16.841358184814453,
      "learning_rate": 7.274487030584592e-06,
      "loss": 1.6643,
      "step": 1879
    },
    {
      "epoch": 0.7278358497870693,
      "grad_norm": 26.13020896911621,
      "learning_rate": 7.278358497870693e-06,
      "loss": 2.0316,
      "step": 1880
    },
    {
      "epoch": 0.7282229965156795,
      "grad_norm": 29.318880081176758,
      "learning_rate": 7.282229965156795e-06,
      "loss": 0.8627,
      "step": 1881
    },
    {
      "epoch": 0.7286101432442896,
      "grad_norm": 17.875864028930664,
      "learning_rate": 7.286101432442897e-06,
      "loss": 1.4629,
      "step": 1882
    },
    {
      "epoch": 0.7289972899728997,
      "grad_norm": 11.419010162353516,
      "learning_rate": 7.289972899728998e-06,
      "loss": 1.5076,
      "step": 1883
    },
    {
      "epoch": 0.7293844367015099,
      "grad_norm": 18.90157699584961,
      "learning_rate": 7.293844367015099e-06,
      "loss": 1.829,
      "step": 1884
    },
    {
      "epoch": 0.72977158343012,
      "grad_norm": 9.7317533493042,
      "learning_rate": 7.297715834301201e-06,
      "loss": 1.5391,
      "step": 1885
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 12.53754711151123,
      "learning_rate": 7.301587301587301e-06,
      "loss": 1.5928,
      "step": 1886
    },
    {
      "epoch": 0.7305458768873403,
      "grad_norm": 12.51893424987793,
      "learning_rate": 7.305458768873404e-06,
      "loss": 1.007,
      "step": 1887
    },
    {
      "epoch": 0.7309330236159505,
      "grad_norm": 11.269881248474121,
      "learning_rate": 7.309330236159505e-06,
      "loss": 1.2101,
      "step": 1888
    },
    {
      "epoch": 0.7313201703445605,
      "grad_norm": 9.009419441223145,
      "learning_rate": 7.313201703445606e-06,
      "loss": 1.0086,
      "step": 1889
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 12.65145206451416,
      "learning_rate": 7.317073170731707e-06,
      "loss": 2.235,
      "step": 1890
    },
    {
      "epoch": 0.7320944638017809,
      "grad_norm": 10.209554672241211,
      "learning_rate": 7.32094463801781e-06,
      "loss": 1.6514,
      "step": 1891
    },
    {
      "epoch": 0.7324816105303911,
      "grad_norm": 11.349108695983887,
      "learning_rate": 7.324816105303911e-06,
      "loss": 1.1478,
      "step": 1892
    },
    {
      "epoch": 0.7328687572590011,
      "grad_norm": 9.583183288574219,
      "learning_rate": 7.328687572590012e-06,
      "loss": 1.7413,
      "step": 1893
    },
    {
      "epoch": 0.7332559039876113,
      "grad_norm": 7.993554592132568,
      "learning_rate": 7.332559039876113e-06,
      "loss": 1.7242,
      "step": 1894
    },
    {
      "epoch": 0.7336430507162215,
      "grad_norm": 10.382140159606934,
      "learning_rate": 7.336430507162216e-06,
      "loss": 1.5912,
      "step": 1895
    },
    {
      "epoch": 0.7340301974448316,
      "grad_norm": 15.78878116607666,
      "learning_rate": 7.340301974448316e-06,
      "loss": 2.377,
      "step": 1896
    },
    {
      "epoch": 0.7344173441734417,
      "grad_norm": 27.374418258666992,
      "learning_rate": 7.344173441734418e-06,
      "loss": 2.0899,
      "step": 1897
    },
    {
      "epoch": 0.7348044909020519,
      "grad_norm": 26.64742088317871,
      "learning_rate": 7.348044909020519e-06,
      "loss": 2.1844,
      "step": 1898
    },
    {
      "epoch": 0.735191637630662,
      "grad_norm": 14.361444473266602,
      "learning_rate": 7.35191637630662e-06,
      "loss": 1.3762,
      "step": 1899
    },
    {
      "epoch": 0.7355787843592722,
      "grad_norm": 49.87547302246094,
      "learning_rate": 7.355787843592722e-06,
      "loss": 1.5382,
      "step": 1900
    },
    {
      "epoch": 0.7359659310878823,
      "grad_norm": 11.756628036499023,
      "learning_rate": 7.359659310878824e-06,
      "loss": 1.7408,
      "step": 1901
    },
    {
      "epoch": 0.7363530778164924,
      "grad_norm": 8.663679122924805,
      "learning_rate": 7.3635307781649245e-06,
      "loss": 1.0826,
      "step": 1902
    },
    {
      "epoch": 0.7367402245451026,
      "grad_norm": 12.705883979797363,
      "learning_rate": 7.367402245451026e-06,
      "loss": 2.0997,
      "step": 1903
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 13.711716651916504,
      "learning_rate": 7.371273712737128e-06,
      "loss": 2.0705,
      "step": 1904
    },
    {
      "epoch": 0.7375145180023229,
      "grad_norm": 14.677573204040527,
      "learning_rate": 7.37514518002323e-06,
      "loss": 1.5943,
      "step": 1905
    },
    {
      "epoch": 0.737901664730933,
      "grad_norm": 12.284815788269043,
      "learning_rate": 7.3790166473093304e-06,
      "loss": 0.7107,
      "step": 1906
    },
    {
      "epoch": 0.7382888114595432,
      "grad_norm": 13.438404083251953,
      "learning_rate": 7.382888114595432e-06,
      "loss": 1.174,
      "step": 1907
    },
    {
      "epoch": 0.7386759581881533,
      "grad_norm": 17.576393127441406,
      "learning_rate": 7.386759581881534e-06,
      "loss": 1.816,
      "step": 1908
    },
    {
      "epoch": 0.7390631049167634,
      "grad_norm": 30.27766990661621,
      "learning_rate": 7.390631049167635e-06,
      "loss": 2.1301,
      "step": 1909
    },
    {
      "epoch": 0.7394502516453736,
      "grad_norm": 10.517561912536621,
      "learning_rate": 7.394502516453736e-06,
      "loss": 1.5646,
      "step": 1910
    },
    {
      "epoch": 0.7398373983739838,
      "grad_norm": 9.080708503723145,
      "learning_rate": 7.398373983739838e-06,
      "loss": 1.0292,
      "step": 1911
    },
    {
      "epoch": 0.7402245451025938,
      "grad_norm": 10.82419204711914,
      "learning_rate": 7.4022454510259385e-06,
      "loss": 1.2173,
      "step": 1912
    },
    {
      "epoch": 0.740611691831204,
      "grad_norm": 22.4649600982666,
      "learning_rate": 7.406116918312041e-06,
      "loss": 2.1408,
      "step": 1913
    },
    {
      "epoch": 0.7409988385598142,
      "grad_norm": 10.43198299407959,
      "learning_rate": 7.409988385598142e-06,
      "loss": 1.0836,
      "step": 1914
    },
    {
      "epoch": 0.7413859852884244,
      "grad_norm": 14.869677543640137,
      "learning_rate": 7.413859852884244e-06,
      "loss": 1.7277,
      "step": 1915
    },
    {
      "epoch": 0.7417731320170344,
      "grad_norm": 10.25045394897461,
      "learning_rate": 7.4177313201703445e-06,
      "loss": 1.5877,
      "step": 1916
    },
    {
      "epoch": 0.7421602787456446,
      "grad_norm": 12.514726638793945,
      "learning_rate": 7.421602787456447e-06,
      "loss": 0.7247,
      "step": 1917
    },
    {
      "epoch": 0.7425474254742548,
      "grad_norm": 7.74023962020874,
      "learning_rate": 7.425474254742548e-06,
      "loss": 1.1478,
      "step": 1918
    },
    {
      "epoch": 0.7429345722028649,
      "grad_norm": 16.95342254638672,
      "learning_rate": 7.429345722028649e-06,
      "loss": 2.1886,
      "step": 1919
    },
    {
      "epoch": 0.743321718931475,
      "grad_norm": 11.470868110656738,
      "learning_rate": 7.4332171893147505e-06,
      "loss": 1.3637,
      "step": 1920
    },
    {
      "epoch": 0.7437088656600852,
      "grad_norm": 18.00442886352539,
      "learning_rate": 7.437088656600853e-06,
      "loss": 1.5468,
      "step": 1921
    },
    {
      "epoch": 0.7440960123886953,
      "grad_norm": 13.240797996520996,
      "learning_rate": 7.4409601238869535e-06,
      "loss": 1.8159,
      "step": 1922
    },
    {
      "epoch": 0.7444831591173054,
      "grad_norm": 9.970535278320312,
      "learning_rate": 7.444831591173055e-06,
      "loss": 1.0039,
      "step": 1923
    },
    {
      "epoch": 0.7448703058459156,
      "grad_norm": 19.18270492553711,
      "learning_rate": 7.4487030584591565e-06,
      "loss": 1.7457,
      "step": 1924
    },
    {
      "epoch": 0.7452574525745257,
      "grad_norm": 40.182559967041016,
      "learning_rate": 7.452574525745257e-06,
      "loss": 1.679,
      "step": 1925
    },
    {
      "epoch": 0.7456445993031359,
      "grad_norm": 14.895502090454102,
      "learning_rate": 7.4564459930313594e-06,
      "loss": 1.7317,
      "step": 1926
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 20.992162704467773,
      "learning_rate": 7.460317460317461e-06,
      "loss": 2.1566,
      "step": 1927
    },
    {
      "epoch": 0.7464188927603562,
      "grad_norm": 15.920352935791016,
      "learning_rate": 7.4641889276035624e-06,
      "loss": 1.6818,
      "step": 1928
    },
    {
      "epoch": 0.7468060394889663,
      "grad_norm": 9.74396800994873,
      "learning_rate": 7.468060394889663e-06,
      "loss": 1.643,
      "step": 1929
    },
    {
      "epoch": 0.7471931862175765,
      "grad_norm": 16.795475006103516,
      "learning_rate": 7.471931862175765e-06,
      "loss": 2.1981,
      "step": 1930
    },
    {
      "epoch": 0.7475803329461866,
      "grad_norm": 12.765235900878906,
      "learning_rate": 7.475803329461867e-06,
      "loss": 1.688,
      "step": 1931
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 20.37154197692871,
      "learning_rate": 7.4796747967479676e-06,
      "loss": 1.6172,
      "step": 1932
    },
    {
      "epoch": 0.7483546264034069,
      "grad_norm": 14.455954551696777,
      "learning_rate": 7.483546264034069e-06,
      "loss": 1.4268,
      "step": 1933
    },
    {
      "epoch": 0.7487417731320171,
      "grad_norm": 16.610145568847656,
      "learning_rate": 7.487417731320171e-06,
      "loss": 1.373,
      "step": 1934
    },
    {
      "epoch": 0.7491289198606271,
      "grad_norm": 10.616569519042969,
      "learning_rate": 7.491289198606272e-06,
      "loss": 1.1483,
      "step": 1935
    },
    {
      "epoch": 0.7495160665892373,
      "grad_norm": 12.589751243591309,
      "learning_rate": 7.4951606658923735e-06,
      "loss": 1.6187,
      "step": 1936
    },
    {
      "epoch": 0.7499032133178475,
      "grad_norm": 17.257892608642578,
      "learning_rate": 7.499032133178475e-06,
      "loss": 1.6075,
      "step": 1937
    },
    {
      "epoch": 0.7502903600464577,
      "grad_norm": 11.136000633239746,
      "learning_rate": 7.502903600464577e-06,
      "loss": 1.6087,
      "step": 1938
    },
    {
      "epoch": 0.7506775067750677,
      "grad_norm": 25.43038558959961,
      "learning_rate": 7.506775067750678e-06,
      "loss": 1.8974,
      "step": 1939
    },
    {
      "epoch": 0.7510646535036779,
      "grad_norm": 12.164017677307129,
      "learning_rate": 7.5106465350367795e-06,
      "loss": 1.6371,
      "step": 1940
    },
    {
      "epoch": 0.7514518002322881,
      "grad_norm": 12.394525527954102,
      "learning_rate": 7.514518002322881e-06,
      "loss": 1.5595,
      "step": 1941
    },
    {
      "epoch": 0.7518389469608981,
      "grad_norm": 14.04058837890625,
      "learning_rate": 7.5183894696089825e-06,
      "loss": 1.9507,
      "step": 1942
    },
    {
      "epoch": 0.7522260936895083,
      "grad_norm": 14.971114158630371,
      "learning_rate": 7.522260936895084e-06,
      "loss": 1.7544,
      "step": 1943
    },
    {
      "epoch": 0.7526132404181185,
      "grad_norm": 12.41119384765625,
      "learning_rate": 7.5261324041811855e-06,
      "loss": 1.8071,
      "step": 1944
    },
    {
      "epoch": 0.7530003871467286,
      "grad_norm": 9.41286563873291,
      "learning_rate": 7.530003871467286e-06,
      "loss": 1.5262,
      "step": 1945
    },
    {
      "epoch": 0.7533875338753387,
      "grad_norm": 9.069091796875,
      "learning_rate": 7.5338753387533885e-06,
      "loss": 1.4887,
      "step": 1946
    },
    {
      "epoch": 0.7537746806039489,
      "grad_norm": 15.7218599319458,
      "learning_rate": 7.53774680603949e-06,
      "loss": 0.7631,
      "step": 1947
    },
    {
      "epoch": 0.754161827332559,
      "grad_norm": 15.03204345703125,
      "learning_rate": 7.541618273325591e-06,
      "loss": 1.7159,
      "step": 1948
    },
    {
      "epoch": 0.7545489740611692,
      "grad_norm": 11.664665222167969,
      "learning_rate": 7.545489740611692e-06,
      "loss": 1.3532,
      "step": 1949
    },
    {
      "epoch": 0.7549361207897793,
      "grad_norm": 11.342138290405273,
      "learning_rate": 7.5493612078977944e-06,
      "loss": 1.2463,
      "step": 1950
    },
    {
      "epoch": 0.7553232675183895,
      "grad_norm": 13.915107727050781,
      "learning_rate": 7.553232675183896e-06,
      "loss": 1.8022,
      "step": 1951
    },
    {
      "epoch": 0.7557104142469996,
      "grad_norm": 15.865480422973633,
      "learning_rate": 7.557104142469997e-06,
      "loss": 1.612,
      "step": 1952
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 26.49051856994629,
      "learning_rate": 7.560975609756098e-06,
      "loss": 1.9109,
      "step": 1953
    },
    {
      "epoch": 0.7564847077042199,
      "grad_norm": 8.516701698303223,
      "learning_rate": 7.5648470770422e-06,
      "loss": 1.4265,
      "step": 1954
    },
    {
      "epoch": 0.75687185443283,
      "grad_norm": 13.553561210632324,
      "learning_rate": 7.568718544328301e-06,
      "loss": 1.2738,
      "step": 1955
    },
    {
      "epoch": 0.7572590011614402,
      "grad_norm": 11.326796531677246,
      "learning_rate": 7.5725900116144026e-06,
      "loss": 1.543,
      "step": 1956
    },
    {
      "epoch": 0.7576461478900504,
      "grad_norm": 8.318197250366211,
      "learning_rate": 7.576461478900504e-06,
      "loss": 1.5966,
      "step": 1957
    },
    {
      "epoch": 0.7580332946186604,
      "grad_norm": 18.933923721313477,
      "learning_rate": 7.580332946186605e-06,
      "loss": 1.3129,
      "step": 1958
    },
    {
      "epoch": 0.7584204413472706,
      "grad_norm": 13.768263816833496,
      "learning_rate": 7.584204413472707e-06,
      "loss": 1.6965,
      "step": 1959
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 17.770965576171875,
      "learning_rate": 7.5880758807588085e-06,
      "loss": 1.3627,
      "step": 1960
    },
    {
      "epoch": 0.759194734804491,
      "grad_norm": 9.434476852416992,
      "learning_rate": 7.59194734804491e-06,
      "loss": 1.6,
      "step": 1961
    },
    {
      "epoch": 0.759581881533101,
      "grad_norm": 17.576566696166992,
      "learning_rate": 7.595818815331011e-06,
      "loss": 1.5023,
      "step": 1962
    },
    {
      "epoch": 0.7599690282617112,
      "grad_norm": 10.181289672851562,
      "learning_rate": 7.599690282617113e-06,
      "loss": 1.3008,
      "step": 1963
    },
    {
      "epoch": 0.7603561749903214,
      "grad_norm": 26.192546844482422,
      "learning_rate": 7.6035617499032145e-06,
      "loss": 1.8328,
      "step": 1964
    },
    {
      "epoch": 0.7607433217189314,
      "grad_norm": 15.818461418151855,
      "learning_rate": 7.607433217189315e-06,
      "loss": 1.6455,
      "step": 1965
    },
    {
      "epoch": 0.7611304684475416,
      "grad_norm": 9.226573944091797,
      "learning_rate": 7.611304684475417e-06,
      "loss": 1.0409,
      "step": 1966
    },
    {
      "epoch": 0.7615176151761518,
      "grad_norm": 11.794890403747559,
      "learning_rate": 7.615176151761519e-06,
      "loss": 1.5478,
      "step": 1967
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 10.868616104125977,
      "learning_rate": 7.61904761904762e-06,
      "loss": 1.5269,
      "step": 1968
    },
    {
      "epoch": 0.762291908633372,
      "grad_norm": 11.16672420501709,
      "learning_rate": 7.622919086333721e-06,
      "loss": 1.7585,
      "step": 1969
    },
    {
      "epoch": 0.7626790553619822,
      "grad_norm": 11.509918212890625,
      "learning_rate": 7.626790553619823e-06,
      "loss": 1.2451,
      "step": 1970
    },
    {
      "epoch": 0.7630662020905923,
      "grad_norm": 15.383813858032227,
      "learning_rate": 7.630662020905924e-06,
      "loss": 1.7136,
      "step": 1971
    },
    {
      "epoch": 0.7634533488192025,
      "grad_norm": 22.048492431640625,
      "learning_rate": 7.634533488192025e-06,
      "loss": 2.6516,
      "step": 1972
    },
    {
      "epoch": 0.7638404955478126,
      "grad_norm": 16.15256690979004,
      "learning_rate": 7.638404955478127e-06,
      "loss": 1.5158,
      "step": 1973
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 11.238784790039062,
      "learning_rate": 7.64227642276423e-06,
      "loss": 1.6091,
      "step": 1974
    },
    {
      "epoch": 0.7646147890050329,
      "grad_norm": 12.923303604125977,
      "learning_rate": 7.64614789005033e-06,
      "loss": 0.8492,
      "step": 1975
    },
    {
      "epoch": 0.7650019357336431,
      "grad_norm": 16.92905616760254,
      "learning_rate": 7.65001935733643e-06,
      "loss": 2.2878,
      "step": 1976
    },
    {
      "epoch": 0.7653890824622532,
      "grad_norm": 20.7362117767334,
      "learning_rate": 7.653890824622533e-06,
      "loss": 1.8082,
      "step": 1977
    },
    {
      "epoch": 0.7657762291908633,
      "grad_norm": 10.814109802246094,
      "learning_rate": 7.657762291908634e-06,
      "loss": 1.5725,
      "step": 1978
    },
    {
      "epoch": 0.7661633759194735,
      "grad_norm": 13.205665588378906,
      "learning_rate": 7.661633759194736e-06,
      "loss": 1.297,
      "step": 1979
    },
    {
      "epoch": 0.7665505226480837,
      "grad_norm": 14.721141815185547,
      "learning_rate": 7.665505226480837e-06,
      "loss": 1.5405,
      "step": 1980
    },
    {
      "epoch": 0.7669376693766937,
      "grad_norm": 11.625903129577637,
      "learning_rate": 7.669376693766937e-06,
      "loss": 1.6432,
      "step": 1981
    },
    {
      "epoch": 0.7673248161053039,
      "grad_norm": 22.504732131958008,
      "learning_rate": 7.67324816105304e-06,
      "loss": 2.3335,
      "step": 1982
    },
    {
      "epoch": 0.7677119628339141,
      "grad_norm": 21.97978401184082,
      "learning_rate": 7.677119628339142e-06,
      "loss": 1.7436,
      "step": 1983
    },
    {
      "epoch": 0.7680991095625243,
      "grad_norm": 21.98860740661621,
      "learning_rate": 7.680991095625243e-06,
      "loss": 1.7779,
      "step": 1984
    },
    {
      "epoch": 0.7684862562911343,
      "grad_norm": 10.59450912475586,
      "learning_rate": 7.684862562911343e-06,
      "loss": 1.0224,
      "step": 1985
    },
    {
      "epoch": 0.7688734030197445,
      "grad_norm": 11.559497833251953,
      "learning_rate": 7.688734030197446e-06,
      "loss": 1.5719,
      "step": 1986
    },
    {
      "epoch": 0.7692605497483547,
      "grad_norm": 13.249598503112793,
      "learning_rate": 7.692605497483548e-06,
      "loss": 1.4843,
      "step": 1987
    },
    {
      "epoch": 0.7696476964769647,
      "grad_norm": 9.416162490844727,
      "learning_rate": 7.696476964769649e-06,
      "loss": 0.9553,
      "step": 1988
    },
    {
      "epoch": 0.7700348432055749,
      "grad_norm": 13.389005661010742,
      "learning_rate": 7.70034843205575e-06,
      "loss": 1.5369,
      "step": 1989
    },
    {
      "epoch": 0.7704219899341851,
      "grad_norm": 12.763663291931152,
      "learning_rate": 7.704219899341852e-06,
      "loss": 1.3062,
      "step": 1990
    },
    {
      "epoch": 0.7708091366627952,
      "grad_norm": 11.993646621704102,
      "learning_rate": 7.708091366627952e-06,
      "loss": 1.6829,
      "step": 1991
    },
    {
      "epoch": 0.7711962833914053,
      "grad_norm": 11.38809871673584,
      "learning_rate": 7.711962833914055e-06,
      "loss": 1.5214,
      "step": 1992
    },
    {
      "epoch": 0.7715834301200155,
      "grad_norm": 10.566888809204102,
      "learning_rate": 7.715834301200155e-06,
      "loss": 1.2411,
      "step": 1993
    },
    {
      "epoch": 0.7719705768486256,
      "grad_norm": 8.767128944396973,
      "learning_rate": 7.719705768486256e-06,
      "loss": 1.4924,
      "step": 1994
    },
    {
      "epoch": 0.7723577235772358,
      "grad_norm": 12.787650108337402,
      "learning_rate": 7.723577235772358e-06,
      "loss": 0.6316,
      "step": 1995
    },
    {
      "epoch": 0.7727448703058459,
      "grad_norm": 12.293133735656738,
      "learning_rate": 7.72744870305846e-06,
      "loss": 1.1826,
      "step": 1996
    },
    {
      "epoch": 0.7731320170344561,
      "grad_norm": 12.385100364685059,
      "learning_rate": 7.731320170344561e-06,
      "loss": 1.8622,
      "step": 1997
    },
    {
      "epoch": 0.7735191637630662,
      "grad_norm": 56.399009704589844,
      "learning_rate": 7.735191637630662e-06,
      "loss": 1.9428,
      "step": 1998
    },
    {
      "epoch": 0.7739063104916764,
      "grad_norm": 13.542095184326172,
      "learning_rate": 7.739063104916764e-06,
      "loss": 0.6426,
      "step": 1999
    },
    {
      "epoch": 0.7742934572202865,
      "grad_norm": 14.81740665435791,
      "learning_rate": 7.742934572202867e-06,
      "loss": 1.6861,
      "step": 2000
    },
    {
      "epoch": 0.7746806039488966,
      "grad_norm": 21.877193450927734,
      "learning_rate": 7.746806039488967e-06,
      "loss": 1.9161,
      "step": 2001
    },
    {
      "epoch": 0.7750677506775068,
      "grad_norm": 11.67359447479248,
      "learning_rate": 7.750677506775068e-06,
      "loss": 1.1329,
      "step": 2002
    },
    {
      "epoch": 0.775454897406117,
      "grad_norm": 18.83820343017578,
      "learning_rate": 7.75454897406117e-06,
      "loss": 1.7693,
      "step": 2003
    },
    {
      "epoch": 0.775842044134727,
      "grad_norm": 24.510290145874023,
      "learning_rate": 7.758420441347271e-06,
      "loss": 1.5313,
      "step": 2004
    },
    {
      "epoch": 0.7762291908633372,
      "grad_norm": 15.44631290435791,
      "learning_rate": 7.762291908633373e-06,
      "loss": 2.6878,
      "step": 2005
    },
    {
      "epoch": 0.7766163375919474,
      "grad_norm": 24.364273071289062,
      "learning_rate": 7.766163375919474e-06,
      "loss": 1.29,
      "step": 2006
    },
    {
      "epoch": 0.7770034843205574,
      "grad_norm": 15.23204231262207,
      "learning_rate": 7.770034843205574e-06,
      "loss": 2.1408,
      "step": 2007
    },
    {
      "epoch": 0.7773906310491676,
      "grad_norm": 12.872687339782715,
      "learning_rate": 7.773906310491677e-06,
      "loss": 1.3539,
      "step": 2008
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 16.5174560546875,
      "learning_rate": 7.77777777777778e-06,
      "loss": 2.1497,
      "step": 2009
    },
    {
      "epoch": 0.778164924506388,
      "grad_norm": 16.77091407775879,
      "learning_rate": 7.78164924506388e-06,
      "loss": 1.8685,
      "step": 2010
    },
    {
      "epoch": 0.778552071234998,
      "grad_norm": 11.650288581848145,
      "learning_rate": 7.78552071234998e-06,
      "loss": 2.1979,
      "step": 2011
    },
    {
      "epoch": 0.7789392179636082,
      "grad_norm": 8.83530330657959,
      "learning_rate": 7.789392179636083e-06,
      "loss": 1.3675,
      "step": 2012
    },
    {
      "epoch": 0.7793263646922184,
      "grad_norm": 9.870386123657227,
      "learning_rate": 7.793263646922185e-06,
      "loss": 1.7351,
      "step": 2013
    },
    {
      "epoch": 0.7797135114208285,
      "grad_norm": 13.531003952026367,
      "learning_rate": 7.797135114208286e-06,
      "loss": 1.4164,
      "step": 2014
    },
    {
      "epoch": 0.7801006581494386,
      "grad_norm": 8.458600044250488,
      "learning_rate": 7.801006581494386e-06,
      "loss": 1.5804,
      "step": 2015
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 14.386209487915039,
      "learning_rate": 7.804878048780489e-06,
      "loss": 1.6219,
      "step": 2016
    },
    {
      "epoch": 0.7808749516066589,
      "grad_norm": 8.184931755065918,
      "learning_rate": 7.80874951606659e-06,
      "loss": 1.4114,
      "step": 2017
    },
    {
      "epoch": 0.781262098335269,
      "grad_norm": 19.261884689331055,
      "learning_rate": 7.812620983352692e-06,
      "loss": 1.4959,
      "step": 2018
    },
    {
      "epoch": 0.7816492450638792,
      "grad_norm": 15.873129844665527,
      "learning_rate": 7.816492450638792e-06,
      "loss": 1.8075,
      "step": 2019
    },
    {
      "epoch": 0.7820363917924894,
      "grad_norm": 9.34190845489502,
      "learning_rate": 7.820363917924895e-06,
      "loss": 1.4395,
      "step": 2020
    },
    {
      "epoch": 0.7824235385210995,
      "grad_norm": 21.857839584350586,
      "learning_rate": 7.824235385210995e-06,
      "loss": 1.4694,
      "step": 2021
    },
    {
      "epoch": 0.7828106852497096,
      "grad_norm": 14.552980422973633,
      "learning_rate": 7.828106852497098e-06,
      "loss": 1.3637,
      "step": 2022
    },
    {
      "epoch": 0.7831978319783198,
      "grad_norm": 17.919906616210938,
      "learning_rate": 7.831978319783198e-06,
      "loss": 1.469,
      "step": 2023
    },
    {
      "epoch": 0.7835849787069299,
      "grad_norm": 10.988612174987793,
      "learning_rate": 7.835849787069299e-06,
      "loss": 1.7048,
      "step": 2024
    },
    {
      "epoch": 0.7839721254355401,
      "grad_norm": 20.872093200683594,
      "learning_rate": 7.839721254355401e-06,
      "loss": 1.887,
      "step": 2025
    },
    {
      "epoch": 0.7843592721641502,
      "grad_norm": 13.368658065795898,
      "learning_rate": 7.843592721641504e-06,
      "loss": 1.5149,
      "step": 2026
    },
    {
      "epoch": 0.7847464188927603,
      "grad_norm": 8.91414737701416,
      "learning_rate": 7.847464188927604e-06,
      "loss": 1.6903,
      "step": 2027
    },
    {
      "epoch": 0.7851335656213705,
      "grad_norm": 19.51024055480957,
      "learning_rate": 7.851335656213705e-06,
      "loss": 1.7898,
      "step": 2028
    },
    {
      "epoch": 0.7855207123499807,
      "grad_norm": 16.086170196533203,
      "learning_rate": 7.855207123499807e-06,
      "loss": 1.2394,
      "step": 2029
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 8.621088981628418,
      "learning_rate": 7.859078590785908e-06,
      "loss": 1.0175,
      "step": 2030
    },
    {
      "epoch": 0.7862950058072009,
      "grad_norm": 21.321758270263672,
      "learning_rate": 7.86295005807201e-06,
      "loss": 1.303,
      "step": 2031
    },
    {
      "epoch": 0.7866821525358111,
      "grad_norm": 16.430667877197266,
      "learning_rate": 7.866821525358111e-06,
      "loss": 1.7531,
      "step": 2032
    },
    {
      "epoch": 0.7870692992644213,
      "grad_norm": 16.769088745117188,
      "learning_rate": 7.870692992644213e-06,
      "loss": 1.8649,
      "step": 2033
    },
    {
      "epoch": 0.7874564459930313,
      "grad_norm": 16.180889129638672,
      "learning_rate": 7.874564459930314e-06,
      "loss": 1.6573,
      "step": 2034
    },
    {
      "epoch": 0.7878435927216415,
      "grad_norm": 9.34954833984375,
      "learning_rate": 7.878435927216416e-06,
      "loss": 1.6688,
      "step": 2035
    },
    {
      "epoch": 0.7882307394502517,
      "grad_norm": 11.340555191040039,
      "learning_rate": 7.882307394502517e-06,
      "loss": 1.7212,
      "step": 2036
    },
    {
      "epoch": 0.7886178861788617,
      "grad_norm": 10.588753700256348,
      "learning_rate": 7.886178861788618e-06,
      "loss": 1.2372,
      "step": 2037
    },
    {
      "epoch": 0.7890050329074719,
      "grad_norm": 10.633072853088379,
      "learning_rate": 7.89005032907472e-06,
      "loss": 1.4803,
      "step": 2038
    },
    {
      "epoch": 0.7893921796360821,
      "grad_norm": 18.350297927856445,
      "learning_rate": 7.893921796360822e-06,
      "loss": 1.8844,
      "step": 2039
    },
    {
      "epoch": 0.7897793263646922,
      "grad_norm": 12.915491104125977,
      "learning_rate": 7.897793263646923e-06,
      "loss": 1.4727,
      "step": 2040
    },
    {
      "epoch": 0.7901664730933023,
      "grad_norm": 12.982916831970215,
      "learning_rate": 7.901664730933024e-06,
      "loss": 1.404,
      "step": 2041
    },
    {
      "epoch": 0.7905536198219125,
      "grad_norm": 9.356689453125,
      "learning_rate": 7.905536198219126e-06,
      "loss": 1.5784,
      "step": 2042
    },
    {
      "epoch": 0.7909407665505227,
      "grad_norm": 14.527385711669922,
      "learning_rate": 7.909407665505228e-06,
      "loss": 2.1243,
      "step": 2043
    },
    {
      "epoch": 0.7913279132791328,
      "grad_norm": 8.472180366516113,
      "learning_rate": 7.913279132791329e-06,
      "loss": 1.5957,
      "step": 2044
    },
    {
      "epoch": 0.7917150600077429,
      "grad_norm": 16.374982833862305,
      "learning_rate": 7.91715060007743e-06,
      "loss": 1.7892,
      "step": 2045
    },
    {
      "epoch": 0.7921022067363531,
      "grad_norm": 9.015387535095215,
      "learning_rate": 7.921022067363532e-06,
      "loss": 1.3636,
      "step": 2046
    },
    {
      "epoch": 0.7924893534649632,
      "grad_norm": 10.786067962646484,
      "learning_rate": 7.924893534649633e-06,
      "loss": 2.2642,
      "step": 2047
    },
    {
      "epoch": 0.7928765001935734,
      "grad_norm": 26.038461685180664,
      "learning_rate": 7.928765001935735e-06,
      "loss": 1.1573,
      "step": 2048
    },
    {
      "epoch": 0.7932636469221835,
      "grad_norm": 25.198867797851562,
      "learning_rate": 7.932636469221836e-06,
      "loss": 2.2964,
      "step": 2049
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 13.941287994384766,
      "learning_rate": 7.936507936507936e-06,
      "loss": 1.297,
      "step": 2050
    },
    {
      "epoch": 0.7940379403794038,
      "grad_norm": 9.05423641204834,
      "learning_rate": 7.940379403794039e-06,
      "loss": 1.4229,
      "step": 2051
    },
    {
      "epoch": 0.794425087108014,
      "grad_norm": 10.740323066711426,
      "learning_rate": 7.94425087108014e-06,
      "loss": 1.5449,
      "step": 2052
    },
    {
      "epoch": 0.794812233836624,
      "grad_norm": 10.183048248291016,
      "learning_rate": 7.948122338366241e-06,
      "loss": 1.5528,
      "step": 2053
    },
    {
      "epoch": 0.7951993805652342,
      "grad_norm": 15.2091703414917,
      "learning_rate": 7.951993805652342e-06,
      "loss": 1.3319,
      "step": 2054
    },
    {
      "epoch": 0.7955865272938444,
      "grad_norm": 28.51121711730957,
      "learning_rate": 7.955865272938444e-06,
      "loss": 1.2127,
      "step": 2055
    },
    {
      "epoch": 0.7959736740224546,
      "grad_norm": 10.28612232208252,
      "learning_rate": 7.959736740224547e-06,
      "loss": 1.5281,
      "step": 2056
    },
    {
      "epoch": 0.7963608207510646,
      "grad_norm": 27.027082443237305,
      "learning_rate": 7.963608207510647e-06,
      "loss": 1.8897,
      "step": 2057
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 11.64136028289795,
      "learning_rate": 7.967479674796748e-06,
      "loss": 1.6135,
      "step": 2058
    },
    {
      "epoch": 0.797135114208285,
      "grad_norm": 12.317622184753418,
      "learning_rate": 7.97135114208285e-06,
      "loss": 1.137,
      "step": 2059
    },
    {
      "epoch": 0.797522260936895,
      "grad_norm": 18.41493797302246,
      "learning_rate": 7.975222609368951e-06,
      "loss": 1.428,
      "step": 2060
    },
    {
      "epoch": 0.7979094076655052,
      "grad_norm": 9.622678756713867,
      "learning_rate": 7.979094076655053e-06,
      "loss": 0.9097,
      "step": 2061
    },
    {
      "epoch": 0.7982965543941154,
      "grad_norm": 17.047924041748047,
      "learning_rate": 7.982965543941154e-06,
      "loss": 2.5142,
      "step": 2062
    },
    {
      "epoch": 0.7986837011227255,
      "grad_norm": 18.034547805786133,
      "learning_rate": 7.986837011227255e-06,
      "loss": 1.6548,
      "step": 2063
    },
    {
      "epoch": 0.7990708478513356,
      "grad_norm": 11.33230972290039,
      "learning_rate": 7.990708478513357e-06,
      "loss": 1.1807,
      "step": 2064
    },
    {
      "epoch": 0.7994579945799458,
      "grad_norm": 14.44831371307373,
      "learning_rate": 7.99457994579946e-06,
      "loss": 1.574,
      "step": 2065
    },
    {
      "epoch": 0.799845141308556,
      "grad_norm": 23.73099708557129,
      "learning_rate": 7.99845141308556e-06,
      "loss": 1.827,
      "step": 2066
    },
    {
      "epoch": 0.8002322880371661,
      "grad_norm": 10.230483055114746,
      "learning_rate": 8.00232288037166e-06,
      "loss": 1.4317,
      "step": 2067
    },
    {
      "epoch": 0.8006194347657762,
      "grad_norm": 17.13132095336914,
      "learning_rate": 8.006194347657763e-06,
      "loss": 2.127,
      "step": 2068
    },
    {
      "epoch": 0.8010065814943864,
      "grad_norm": 14.252950668334961,
      "learning_rate": 8.010065814943865e-06,
      "loss": 1.7097,
      "step": 2069
    },
    {
      "epoch": 0.8013937282229965,
      "grad_norm": 10.91816234588623,
      "learning_rate": 8.013937282229966e-06,
      "loss": 1.5602,
      "step": 2070
    },
    {
      "epoch": 0.8017808749516067,
      "grad_norm": 17.23995018005371,
      "learning_rate": 8.017808749516067e-06,
      "loss": 1.8387,
      "step": 2071
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 10.96123218536377,
      "learning_rate": 8.021680216802169e-06,
      "loss": 1.3474,
      "step": 2072
    },
    {
      "epoch": 0.8025551684088269,
      "grad_norm": 59.898433685302734,
      "learning_rate": 8.02555168408827e-06,
      "loss": 2.0066,
      "step": 2073
    },
    {
      "epoch": 0.8029423151374371,
      "grad_norm": 9.275467872619629,
      "learning_rate": 8.029423151374372e-06,
      "loss": 1.5191,
      "step": 2074
    },
    {
      "epoch": 0.8033294618660473,
      "grad_norm": 18.352914810180664,
      "learning_rate": 8.033294618660473e-06,
      "loss": 1.7544,
      "step": 2075
    },
    {
      "epoch": 0.8037166085946573,
      "grad_norm": 10.870415687561035,
      "learning_rate": 8.037166085946573e-06,
      "loss": 1.5455,
      "step": 2076
    },
    {
      "epoch": 0.8041037553232675,
      "grad_norm": 10.928840637207031,
      "learning_rate": 8.041037553232676e-06,
      "loss": 1.4908,
      "step": 2077
    },
    {
      "epoch": 0.8044909020518777,
      "grad_norm": 17.364770889282227,
      "learning_rate": 8.044909020518778e-06,
      "loss": 1.3619,
      "step": 2078
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 13.622734069824219,
      "learning_rate": 8.048780487804879e-06,
      "loss": 1.4025,
      "step": 2079
    },
    {
      "epoch": 0.8052651955090979,
      "grad_norm": 23.949111938476562,
      "learning_rate": 8.05265195509098e-06,
      "loss": 1.2608,
      "step": 2080
    },
    {
      "epoch": 0.8056523422377081,
      "grad_norm": 14.556358337402344,
      "learning_rate": 8.056523422377082e-06,
      "loss": 1.7842,
      "step": 2081
    },
    {
      "epoch": 0.8060394889663183,
      "grad_norm": 20.568449020385742,
      "learning_rate": 8.060394889663184e-06,
      "loss": 1.3438,
      "step": 2082
    },
    {
      "epoch": 0.8064266356949283,
      "grad_norm": 16.196861267089844,
      "learning_rate": 8.064266356949285e-06,
      "loss": 1.5254,
      "step": 2083
    },
    {
      "epoch": 0.8068137824235385,
      "grad_norm": 28.71220588684082,
      "learning_rate": 8.068137824235385e-06,
      "loss": 2.3215,
      "step": 2084
    },
    {
      "epoch": 0.8072009291521487,
      "grad_norm": 15.975860595703125,
      "learning_rate": 8.072009291521488e-06,
      "loss": 1.6948,
      "step": 2085
    },
    {
      "epoch": 0.8075880758807588,
      "grad_norm": 14.02998161315918,
      "learning_rate": 8.075880758807588e-06,
      "loss": 1.717,
      "step": 2086
    },
    {
      "epoch": 0.8079752226093689,
      "grad_norm": 41.80805969238281,
      "learning_rate": 8.07975222609369e-06,
      "loss": 1.6576,
      "step": 2087
    },
    {
      "epoch": 0.8083623693379791,
      "grad_norm": 21.00052261352539,
      "learning_rate": 8.083623693379791e-06,
      "loss": 2.0447,
      "step": 2088
    },
    {
      "epoch": 0.8087495160665893,
      "grad_norm": 44.310699462890625,
      "learning_rate": 8.087495160665894e-06,
      "loss": 1.9461,
      "step": 2089
    },
    {
      "epoch": 0.8091366627951994,
      "grad_norm": 10.55066204071045,
      "learning_rate": 8.091366627951994e-06,
      "loss": 0.9652,
      "step": 2090
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 11.417497634887695,
      "learning_rate": 8.095238095238097e-06,
      "loss": 1.5785,
      "step": 2091
    },
    {
      "epoch": 0.8099109562524197,
      "grad_norm": 11.924347877502441,
      "learning_rate": 8.099109562524197e-06,
      "loss": 1.4805,
      "step": 2092
    },
    {
      "epoch": 0.8102981029810298,
      "grad_norm": 18.078807830810547,
      "learning_rate": 8.102981029810298e-06,
      "loss": 1.6493,
      "step": 2093
    },
    {
      "epoch": 0.81068524970964,
      "grad_norm": 13.243597984313965,
      "learning_rate": 8.1068524970964e-06,
      "loss": 1.5893,
      "step": 2094
    },
    {
      "epoch": 0.8110723964382501,
      "grad_norm": 11.638444900512695,
      "learning_rate": 8.110723964382503e-06,
      "loss": 1.4378,
      "step": 2095
    },
    {
      "epoch": 0.8114595431668602,
      "grad_norm": 19.294845581054688,
      "learning_rate": 8.114595431668603e-06,
      "loss": 1.7136,
      "step": 2096
    },
    {
      "epoch": 0.8118466898954704,
      "grad_norm": 13.344961166381836,
      "learning_rate": 8.118466898954704e-06,
      "loss": 1.2045,
      "step": 2097
    },
    {
      "epoch": 0.8122338366240806,
      "grad_norm": 16.976343154907227,
      "learning_rate": 8.122338366240806e-06,
      "loss": 1.8163,
      "step": 2098
    },
    {
      "epoch": 0.8126209833526906,
      "grad_norm": 14.334086418151855,
      "learning_rate": 8.126209833526907e-06,
      "loss": 1.2376,
      "step": 2099
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 10.489884376525879,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.2154,
      "step": 2100
    },
    {
      "epoch": 0.813395276809911,
      "grad_norm": 24.210803985595703,
      "learning_rate": 8.13395276809911e-06,
      "loss": 2.2648,
      "step": 2101
    },
    {
      "epoch": 0.8137824235385211,
      "grad_norm": 15.344009399414062,
      "learning_rate": 8.137824235385212e-06,
      "loss": 1.7779,
      "step": 2102
    },
    {
      "epoch": 0.8141695702671312,
      "grad_norm": 10.385869026184082,
      "learning_rate": 8.141695702671313e-06,
      "loss": 1.7158,
      "step": 2103
    },
    {
      "epoch": 0.8145567169957414,
      "grad_norm": 16.728370666503906,
      "learning_rate": 8.145567169957415e-06,
      "loss": 2.1645,
      "step": 2104
    },
    {
      "epoch": 0.8149438637243516,
      "grad_norm": 11.507981300354004,
      "learning_rate": 8.149438637243516e-06,
      "loss": 1.6753,
      "step": 2105
    },
    {
      "epoch": 0.8153310104529616,
      "grad_norm": 25.624345779418945,
      "learning_rate": 8.153310104529616e-06,
      "loss": 2.1438,
      "step": 2106
    },
    {
      "epoch": 0.8157181571815718,
      "grad_norm": 16.021713256835938,
      "learning_rate": 8.157181571815719e-06,
      "loss": 1.8879,
      "step": 2107
    },
    {
      "epoch": 0.816105303910182,
      "grad_norm": 14.28944206237793,
      "learning_rate": 8.161053039101821e-06,
      "loss": 1.7357,
      "step": 2108
    },
    {
      "epoch": 0.8164924506387921,
      "grad_norm": 13.740686416625977,
      "learning_rate": 8.164924506387922e-06,
      "loss": 1.1798,
      "step": 2109
    },
    {
      "epoch": 0.8168795973674022,
      "grad_norm": 8.679463386535645,
      "learning_rate": 8.168795973674022e-06,
      "loss": 0.998,
      "step": 2110
    },
    {
      "epoch": 0.8172667440960124,
      "grad_norm": 14.888792037963867,
      "learning_rate": 8.172667440960125e-06,
      "loss": 1.8089,
      "step": 2111
    },
    {
      "epoch": 0.8176538908246226,
      "grad_norm": 12.143692970275879,
      "learning_rate": 8.176538908246227e-06,
      "loss": 1.5368,
      "step": 2112
    },
    {
      "epoch": 0.8180410375532327,
      "grad_norm": 25.47806167602539,
      "learning_rate": 8.180410375532328e-06,
      "loss": 1.699,
      "step": 2113
    },
    {
      "epoch": 0.8184281842818428,
      "grad_norm": 14.56418228149414,
      "learning_rate": 8.184281842818428e-06,
      "loss": 1.8969,
      "step": 2114
    },
    {
      "epoch": 0.818815331010453,
      "grad_norm": 11.470760345458984,
      "learning_rate": 8.18815331010453e-06,
      "loss": 1.2484,
      "step": 2115
    },
    {
      "epoch": 0.8192024777390631,
      "grad_norm": 15.581243515014648,
      "learning_rate": 8.192024777390631e-06,
      "loss": 1.5506,
      "step": 2116
    },
    {
      "epoch": 0.8195896244676733,
      "grad_norm": 17.52492904663086,
      "learning_rate": 8.195896244676734e-06,
      "loss": 1.4061,
      "step": 2117
    },
    {
      "epoch": 0.8199767711962834,
      "grad_norm": 12.266321182250977,
      "learning_rate": 8.199767711962834e-06,
      "loss": 1.6297,
      "step": 2118
    },
    {
      "epoch": 0.8203639179248935,
      "grad_norm": 13.054709434509277,
      "learning_rate": 8.203639179248935e-06,
      "loss": 1.6799,
      "step": 2119
    },
    {
      "epoch": 0.8207510646535037,
      "grad_norm": 10.41582202911377,
      "learning_rate": 8.207510646535037e-06,
      "loss": 1.771,
      "step": 2120
    },
    {
      "epoch": 0.8211382113821138,
      "grad_norm": 11.430129051208496,
      "learning_rate": 8.21138211382114e-06,
      "loss": 1.6314,
      "step": 2121
    },
    {
      "epoch": 0.8215253581107239,
      "grad_norm": 11.911107063293457,
      "learning_rate": 8.21525358110724e-06,
      "loss": 1.3811,
      "step": 2122
    },
    {
      "epoch": 0.8219125048393341,
      "grad_norm": 19.053007125854492,
      "learning_rate": 8.219125048393341e-06,
      "loss": 1.6573,
      "step": 2123
    },
    {
      "epoch": 0.8222996515679443,
      "grad_norm": 14.288761138916016,
      "learning_rate": 8.222996515679443e-06,
      "loss": 1.6902,
      "step": 2124
    },
    {
      "epoch": 0.8226867982965544,
      "grad_norm": 20.283615112304688,
      "learning_rate": 8.226867982965546e-06,
      "loss": 1.5305,
      "step": 2125
    },
    {
      "epoch": 0.8230739450251645,
      "grad_norm": 15.44629192352295,
      "learning_rate": 8.230739450251646e-06,
      "loss": 1.5435,
      "step": 2126
    },
    {
      "epoch": 0.8234610917537747,
      "grad_norm": 13.141427040100098,
      "learning_rate": 8.234610917537747e-06,
      "loss": 1.5959,
      "step": 2127
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 11.349234580993652,
      "learning_rate": 8.23848238482385e-06,
      "loss": 2.1449,
      "step": 2128
    },
    {
      "epoch": 0.8242353852109949,
      "grad_norm": 20.24184226989746,
      "learning_rate": 8.24235385210995e-06,
      "loss": 1.7835,
      "step": 2129
    },
    {
      "epoch": 0.8246225319396051,
      "grad_norm": 7.774609088897705,
      "learning_rate": 8.246225319396052e-06,
      "loss": 1.7144,
      "step": 2130
    },
    {
      "epoch": 0.8250096786682153,
      "grad_norm": 10.912466049194336,
      "learning_rate": 8.250096786682153e-06,
      "loss": 1.7148,
      "step": 2131
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 11.331450462341309,
      "learning_rate": 8.253968253968254e-06,
      "loss": 1.6081,
      "step": 2132
    },
    {
      "epoch": 0.8257839721254355,
      "grad_norm": 10.165979385375977,
      "learning_rate": 8.257839721254356e-06,
      "loss": 1.5764,
      "step": 2133
    },
    {
      "epoch": 0.8261711188540457,
      "grad_norm": 11.920113563537598,
      "learning_rate": 8.261711188540458e-06,
      "loss": 1.6907,
      "step": 2134
    },
    {
      "epoch": 0.8265582655826558,
      "grad_norm": 13.684804916381836,
      "learning_rate": 8.265582655826559e-06,
      "loss": 1.5503,
      "step": 2135
    },
    {
      "epoch": 0.826945412311266,
      "grad_norm": 14.786256790161133,
      "learning_rate": 8.26945412311266e-06,
      "loss": 1.6055,
      "step": 2136
    },
    {
      "epoch": 0.8273325590398761,
      "grad_norm": 19.786827087402344,
      "learning_rate": 8.273325590398762e-06,
      "loss": 2.1582,
      "step": 2137
    },
    {
      "epoch": 0.8277197057684863,
      "grad_norm": 17.46035385131836,
      "learning_rate": 8.277197057684864e-06,
      "loss": 1.3962,
      "step": 2138
    },
    {
      "epoch": 0.8281068524970964,
      "grad_norm": 15.019089698791504,
      "learning_rate": 8.281068524970965e-06,
      "loss": 1.8448,
      "step": 2139
    },
    {
      "epoch": 0.8284939992257065,
      "grad_norm": 13.277210235595703,
      "learning_rate": 8.284939992257065e-06,
      "loss": 1.7468,
      "step": 2140
    },
    {
      "epoch": 0.8288811459543167,
      "grad_norm": 13.310478210449219,
      "learning_rate": 8.288811459543168e-06,
      "loss": 1.6477,
      "step": 2141
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 15.190135955810547,
      "learning_rate": 8.292682926829268e-06,
      "loss": 1.6926,
      "step": 2142
    },
    {
      "epoch": 0.829655439411537,
      "grad_norm": 53.67072296142578,
      "learning_rate": 8.29655439411537e-06,
      "loss": 1.4221,
      "step": 2143
    },
    {
      "epoch": 0.8300425861401471,
      "grad_norm": 13.836723327636719,
      "learning_rate": 8.300425861401471e-06,
      "loss": 1.1746,
      "step": 2144
    },
    {
      "epoch": 0.8304297328687572,
      "grad_norm": 9.867587089538574,
      "learning_rate": 8.304297328687572e-06,
      "loss": 1.1726,
      "step": 2145
    },
    {
      "epoch": 0.8308168795973674,
      "grad_norm": 26.833621978759766,
      "learning_rate": 8.308168795973674e-06,
      "loss": 2.722,
      "step": 2146
    },
    {
      "epoch": 0.8312040263259776,
      "grad_norm": 12.140603065490723,
      "learning_rate": 8.312040263259777e-06,
      "loss": 1.0798,
      "step": 2147
    },
    {
      "epoch": 0.8315911730545877,
      "grad_norm": 10.524012565612793,
      "learning_rate": 8.315911730545877e-06,
      "loss": 1.1014,
      "step": 2148
    },
    {
      "epoch": 0.8319783197831978,
      "grad_norm": 14.24618911743164,
      "learning_rate": 8.319783197831978e-06,
      "loss": 1.598,
      "step": 2149
    },
    {
      "epoch": 0.832365466511808,
      "grad_norm": 10.414746284484863,
      "learning_rate": 8.32365466511808e-06,
      "loss": 1.4065,
      "step": 2150
    },
    {
      "epoch": 0.8327526132404182,
      "grad_norm": 23.890275955200195,
      "learning_rate": 8.327526132404183e-06,
      "loss": 1.6112,
      "step": 2151
    },
    {
      "epoch": 0.8331397599690282,
      "grad_norm": 14.585893630981445,
      "learning_rate": 8.331397599690283e-06,
      "loss": 1.189,
      "step": 2152
    },
    {
      "epoch": 0.8335269066976384,
      "grad_norm": 17.004106521606445,
      "learning_rate": 8.335269066976384e-06,
      "loss": 1.6558,
      "step": 2153
    },
    {
      "epoch": 0.8339140534262486,
      "grad_norm": 18.965782165527344,
      "learning_rate": 8.339140534262486e-06,
      "loss": 1.5459,
      "step": 2154
    },
    {
      "epoch": 0.8343012001548586,
      "grad_norm": 31.639196395874023,
      "learning_rate": 8.343012001548587e-06,
      "loss": 1.8591,
      "step": 2155
    },
    {
      "epoch": 0.8346883468834688,
      "grad_norm": 26.23279571533203,
      "learning_rate": 8.34688346883469e-06,
      "loss": 1.4141,
      "step": 2156
    },
    {
      "epoch": 0.835075493612079,
      "grad_norm": 8.058737754821777,
      "learning_rate": 8.35075493612079e-06,
      "loss": 1.4096,
      "step": 2157
    },
    {
      "epoch": 0.8354626403406891,
      "grad_norm": 19.554719924926758,
      "learning_rate": 8.35462640340689e-06,
      "loss": 1.8641,
      "step": 2158
    },
    {
      "epoch": 0.8358497870692992,
      "grad_norm": 18.35394287109375,
      "learning_rate": 8.358497870692993e-06,
      "loss": 1.591,
      "step": 2159
    },
    {
      "epoch": 0.8362369337979094,
      "grad_norm": 9.319986343383789,
      "learning_rate": 8.362369337979095e-06,
      "loss": 0.9274,
      "step": 2160
    },
    {
      "epoch": 0.8366240805265196,
      "grad_norm": 19.622718811035156,
      "learning_rate": 8.366240805265196e-06,
      "loss": 1.7086,
      "step": 2161
    },
    {
      "epoch": 0.8370112272551297,
      "grad_norm": 15.330463409423828,
      "learning_rate": 8.370112272551297e-06,
      "loss": 2.1132,
      "step": 2162
    },
    {
      "epoch": 0.8373983739837398,
      "grad_norm": 14.79594612121582,
      "learning_rate": 8.373983739837399e-06,
      "loss": 2.1258,
      "step": 2163
    },
    {
      "epoch": 0.83778552071235,
      "grad_norm": 9.78537368774414,
      "learning_rate": 8.377855207123501e-06,
      "loss": 1.1433,
      "step": 2164
    },
    {
      "epoch": 0.8381726674409601,
      "grad_norm": 8.926616668701172,
      "learning_rate": 8.381726674409602e-06,
      "loss": 1.5136,
      "step": 2165
    },
    {
      "epoch": 0.8385598141695703,
      "grad_norm": 15.533157348632812,
      "learning_rate": 8.385598141695703e-06,
      "loss": 1.629,
      "step": 2166
    },
    {
      "epoch": 0.8389469608981804,
      "grad_norm": 17.258729934692383,
      "learning_rate": 8.389469608981805e-06,
      "loss": 2.4779,
      "step": 2167
    },
    {
      "epoch": 0.8393341076267905,
      "grad_norm": 20.144193649291992,
      "learning_rate": 8.393341076267906e-06,
      "loss": 1.7957,
      "step": 2168
    },
    {
      "epoch": 0.8397212543554007,
      "grad_norm": 12.98810863494873,
      "learning_rate": 8.397212543554008e-06,
      "loss": 1.2313,
      "step": 2169
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 54.01066970825195,
      "learning_rate": 8.401084010840109e-06,
      "loss": 1.0123,
      "step": 2170
    },
    {
      "epoch": 0.840495547812621,
      "grad_norm": 14.167572021484375,
      "learning_rate": 8.404955478126211e-06,
      "loss": 0.6615,
      "step": 2171
    },
    {
      "epoch": 0.8408826945412311,
      "grad_norm": 9.89950942993164,
      "learning_rate": 8.408826945412312e-06,
      "loss": 1.4337,
      "step": 2172
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 11.962043762207031,
      "learning_rate": 8.412698412698414e-06,
      "loss": 1.7105,
      "step": 2173
    },
    {
      "epoch": 0.8416569879984515,
      "grad_norm": 13.910822868347168,
      "learning_rate": 8.416569879984515e-06,
      "loss": 1.96,
      "step": 2174
    },
    {
      "epoch": 0.8420441347270615,
      "grad_norm": 19.704757690429688,
      "learning_rate": 8.420441347270615e-06,
      "loss": 3.1724,
      "step": 2175
    },
    {
      "epoch": 0.8424312814556717,
      "grad_norm": 13.390996932983398,
      "learning_rate": 8.424312814556718e-06,
      "loss": 1.7029,
      "step": 2176
    },
    {
      "epoch": 0.8428184281842819,
      "grad_norm": 11.462701797485352,
      "learning_rate": 8.42818428184282e-06,
      "loss": 1.1609,
      "step": 2177
    },
    {
      "epoch": 0.8432055749128919,
      "grad_norm": 20.789403915405273,
      "learning_rate": 8.43205574912892e-06,
      "loss": 2.0266,
      "step": 2178
    },
    {
      "epoch": 0.8435927216415021,
      "grad_norm": 10.42020320892334,
      "learning_rate": 8.435927216415021e-06,
      "loss": 1.6771,
      "step": 2179
    },
    {
      "epoch": 0.8439798683701123,
      "grad_norm": 18.41660499572754,
      "learning_rate": 8.439798683701124e-06,
      "loss": 1.6697,
      "step": 2180
    },
    {
      "epoch": 0.8443670150987224,
      "grad_norm": 14.073445320129395,
      "learning_rate": 8.443670150987224e-06,
      "loss": 1.1859,
      "step": 2181
    },
    {
      "epoch": 0.8447541618273325,
      "grad_norm": 15.706205368041992,
      "learning_rate": 8.447541618273327e-06,
      "loss": 1.4689,
      "step": 2182
    },
    {
      "epoch": 0.8451413085559427,
      "grad_norm": 8.167473793029785,
      "learning_rate": 8.451413085559427e-06,
      "loss": 1.3869,
      "step": 2183
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 12.855470657348633,
      "learning_rate": 8.45528455284553e-06,
      "loss": 1.4704,
      "step": 2184
    },
    {
      "epoch": 0.845915602013163,
      "grad_norm": 37.359683990478516,
      "learning_rate": 8.45915602013163e-06,
      "loss": 2.1863,
      "step": 2185
    },
    {
      "epoch": 0.8463027487417731,
      "grad_norm": 16.507675170898438,
      "learning_rate": 8.463027487417732e-06,
      "loss": 1.2882,
      "step": 2186
    },
    {
      "epoch": 0.8466898954703833,
      "grad_norm": 13.112751007080078,
      "learning_rate": 8.466898954703833e-06,
      "loss": 1.6452,
      "step": 2187
    },
    {
      "epoch": 0.8470770421989934,
      "grad_norm": 18.63865852355957,
      "learning_rate": 8.470770421989934e-06,
      "loss": 2.1938,
      "step": 2188
    },
    {
      "epoch": 0.8474641889276036,
      "grad_norm": 19.7222900390625,
      "learning_rate": 8.474641889276036e-06,
      "loss": 2.2125,
      "step": 2189
    },
    {
      "epoch": 0.8478513356562137,
      "grad_norm": 23.135313034057617,
      "learning_rate": 8.478513356562138e-06,
      "loss": 1.8557,
      "step": 2190
    },
    {
      "epoch": 0.8482384823848238,
      "grad_norm": 9.902103424072266,
      "learning_rate": 8.482384823848239e-06,
      "loss": 1.1417,
      "step": 2191
    },
    {
      "epoch": 0.848625629113434,
      "grad_norm": 11.430632591247559,
      "learning_rate": 8.48625629113434e-06,
      "loss": 1.2018,
      "step": 2192
    },
    {
      "epoch": 0.8490127758420442,
      "grad_norm": 24.98682975769043,
      "learning_rate": 8.490127758420442e-06,
      "loss": 1.4995,
      "step": 2193
    },
    {
      "epoch": 0.8493999225706543,
      "grad_norm": 35.070133209228516,
      "learning_rate": 8.493999225706544e-06,
      "loss": 1.4302,
      "step": 2194
    },
    {
      "epoch": 0.8497870692992644,
      "grad_norm": 29.556665420532227,
      "learning_rate": 8.497870692992645e-06,
      "loss": 1.2417,
      "step": 2195
    },
    {
      "epoch": 0.8501742160278746,
      "grad_norm": 14.488005638122559,
      "learning_rate": 8.501742160278746e-06,
      "loss": 1.3895,
      "step": 2196
    },
    {
      "epoch": 0.8505613627564848,
      "grad_norm": 9.084390640258789,
      "learning_rate": 8.505613627564848e-06,
      "loss": 1.4808,
      "step": 2197
    },
    {
      "epoch": 0.8509485094850948,
      "grad_norm": 9.788613319396973,
      "learning_rate": 8.509485094850949e-06,
      "loss": 1.5267,
      "step": 2198
    },
    {
      "epoch": 0.851335656213705,
      "grad_norm": 11.75755786895752,
      "learning_rate": 8.513356562137051e-06,
      "loss": 1.6979,
      "step": 2199
    },
    {
      "epoch": 0.8517228029423152,
      "grad_norm": 13.124393463134766,
      "learning_rate": 8.517228029423152e-06,
      "loss": 1.6381,
      "step": 2200
    },
    {
      "epoch": 0.8521099496709252,
      "grad_norm": 11.733028411865234,
      "learning_rate": 8.521099496709252e-06,
      "loss": 1.6207,
      "step": 2201
    },
    {
      "epoch": 0.8524970963995354,
      "grad_norm": 19.760021209716797,
      "learning_rate": 8.524970963995355e-06,
      "loss": 1.5988,
      "step": 2202
    },
    {
      "epoch": 0.8528842431281456,
      "grad_norm": 15.45322036743164,
      "learning_rate": 8.528842431281457e-06,
      "loss": 1.612,
      "step": 2203
    },
    {
      "epoch": 0.8532713898567557,
      "grad_norm": 25.96436309814453,
      "learning_rate": 8.532713898567558e-06,
      "loss": 2.0123,
      "step": 2204
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 18.135671615600586,
      "learning_rate": 8.536585365853658e-06,
      "loss": 1.4325,
      "step": 2205
    },
    {
      "epoch": 0.854045683313976,
      "grad_norm": 9.199226379394531,
      "learning_rate": 8.54045683313976e-06,
      "loss": 0.9241,
      "step": 2206
    },
    {
      "epoch": 0.8544328300425862,
      "grad_norm": 31.8306941986084,
      "learning_rate": 8.544328300425863e-06,
      "loss": 1.6128,
      "step": 2207
    },
    {
      "epoch": 0.8548199767711963,
      "grad_norm": 10.949952125549316,
      "learning_rate": 8.548199767711964e-06,
      "loss": 1.2165,
      "step": 2208
    },
    {
      "epoch": 0.8552071234998064,
      "grad_norm": 17.183326721191406,
      "learning_rate": 8.552071234998064e-06,
      "loss": 1.9418,
      "step": 2209
    },
    {
      "epoch": 0.8555942702284166,
      "grad_norm": 17.843435287475586,
      "learning_rate": 8.555942702284167e-06,
      "loss": 1.6941,
      "step": 2210
    },
    {
      "epoch": 0.8559814169570267,
      "grad_norm": 17.529129028320312,
      "learning_rate": 8.559814169570267e-06,
      "loss": 1.7653,
      "step": 2211
    },
    {
      "epoch": 0.8563685636856369,
      "grad_norm": 15.642940521240234,
      "learning_rate": 8.56368563685637e-06,
      "loss": 1.5423,
      "step": 2212
    },
    {
      "epoch": 0.856755710414247,
      "grad_norm": 15.752030372619629,
      "learning_rate": 8.56755710414247e-06,
      "loss": 1.7215,
      "step": 2213
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 23.798702239990234,
      "learning_rate": 8.571428571428571e-06,
      "loss": 1.5234,
      "step": 2214
    },
    {
      "epoch": 0.8575300038714673,
      "grad_norm": 24.3959903717041,
      "learning_rate": 8.575300038714673e-06,
      "loss": 1.217,
      "step": 2215
    },
    {
      "epoch": 0.8579171506000774,
      "grad_norm": 9.18332576751709,
      "learning_rate": 8.579171506000776e-06,
      "loss": 1.6293,
      "step": 2216
    },
    {
      "epoch": 0.8583042973286876,
      "grad_norm": 11.722647666931152,
      "learning_rate": 8.583042973286876e-06,
      "loss": 1.6938,
      "step": 2217
    },
    {
      "epoch": 0.8586914440572977,
      "grad_norm": 14.896014213562012,
      "learning_rate": 8.586914440572977e-06,
      "loss": 2.1324,
      "step": 2218
    },
    {
      "epoch": 0.8590785907859079,
      "grad_norm": 15.223676681518555,
      "learning_rate": 8.59078590785908e-06,
      "loss": 1.2524,
      "step": 2219
    },
    {
      "epoch": 0.859465737514518,
      "grad_norm": 12.06539535522461,
      "learning_rate": 8.594657375145182e-06,
      "loss": 1.5714,
      "step": 2220
    },
    {
      "epoch": 0.8598528842431281,
      "grad_norm": 16.286516189575195,
      "learning_rate": 8.598528842431282e-06,
      "loss": 2.1073,
      "step": 2221
    },
    {
      "epoch": 0.8602400309717383,
      "grad_norm": 13.37855339050293,
      "learning_rate": 8.602400309717383e-06,
      "loss": 1.2769,
      "step": 2222
    },
    {
      "epoch": 0.8606271777003485,
      "grad_norm": 14.750328063964844,
      "learning_rate": 8.606271777003485e-06,
      "loss": 1.1355,
      "step": 2223
    },
    {
      "epoch": 0.8610143244289585,
      "grad_norm": 12.071784973144531,
      "learning_rate": 8.610143244289586e-06,
      "loss": 1.6066,
      "step": 2224
    },
    {
      "epoch": 0.8614014711575687,
      "grad_norm": 16.02212905883789,
      "learning_rate": 8.614014711575688e-06,
      "loss": 1.8746,
      "step": 2225
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 10.740756034851074,
      "learning_rate": 8.617886178861789e-06,
      "loss": 1.1276,
      "step": 2226
    },
    {
      "epoch": 0.862175764614789,
      "grad_norm": 14.076102256774902,
      "learning_rate": 8.62175764614789e-06,
      "loss": 1.3813,
      "step": 2227
    },
    {
      "epoch": 0.8625629113433991,
      "grad_norm": 11.554852485656738,
      "learning_rate": 8.625629113433992e-06,
      "loss": 1.6069,
      "step": 2228
    },
    {
      "epoch": 0.8629500580720093,
      "grad_norm": 13.958612442016602,
      "learning_rate": 8.629500580720094e-06,
      "loss": 1.5389,
      "step": 2229
    },
    {
      "epoch": 0.8633372048006195,
      "grad_norm": 14.800100326538086,
      "learning_rate": 8.633372048006195e-06,
      "loss": 1.1257,
      "step": 2230
    },
    {
      "epoch": 0.8637243515292296,
      "grad_norm": 12.484302520751953,
      "learning_rate": 8.637243515292295e-06,
      "loss": 2.019,
      "step": 2231
    },
    {
      "epoch": 0.8641114982578397,
      "grad_norm": 12.822870254516602,
      "learning_rate": 8.641114982578398e-06,
      "loss": 1.6302,
      "step": 2232
    },
    {
      "epoch": 0.8644986449864499,
      "grad_norm": 16.397939682006836,
      "learning_rate": 8.6449864498645e-06,
      "loss": 1.9165,
      "step": 2233
    },
    {
      "epoch": 0.86488579171506,
      "grad_norm": 17.83855628967285,
      "learning_rate": 8.6488579171506e-06,
      "loss": 1.4446,
      "step": 2234
    },
    {
      "epoch": 0.8652729384436701,
      "grad_norm": 18.972755432128906,
      "learning_rate": 8.652729384436701e-06,
      "loss": 1.8924,
      "step": 2235
    },
    {
      "epoch": 0.8656600851722803,
      "grad_norm": 21.031139373779297,
      "learning_rate": 8.656600851722804e-06,
      "loss": 2.4871,
      "step": 2236
    },
    {
      "epoch": 0.8660472319008904,
      "grad_norm": 17.726720809936523,
      "learning_rate": 8.660472319008904e-06,
      "loss": 2.0166,
      "step": 2237
    },
    {
      "epoch": 0.8664343786295006,
      "grad_norm": 14.419464111328125,
      "learning_rate": 8.664343786295007e-06,
      "loss": 2.0454,
      "step": 2238
    },
    {
      "epoch": 0.8668215253581107,
      "grad_norm": 20.74118995666504,
      "learning_rate": 8.668215253581107e-06,
      "loss": 1.7399,
      "step": 2239
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 20.333431243896484,
      "learning_rate": 8.67208672086721e-06,
      "loss": 1.3718,
      "step": 2240
    },
    {
      "epoch": 0.867595818815331,
      "grad_norm": 20.801889419555664,
      "learning_rate": 8.67595818815331e-06,
      "loss": 1.4769,
      "step": 2241
    },
    {
      "epoch": 0.8679829655439412,
      "grad_norm": 15.907133102416992,
      "learning_rate": 8.679829655439413e-06,
      "loss": 1.4795,
      "step": 2242
    },
    {
      "epoch": 0.8683701122725513,
      "grad_norm": 12.965600967407227,
      "learning_rate": 8.683701122725513e-06,
      "loss": 0.904,
      "step": 2243
    },
    {
      "epoch": 0.8687572590011614,
      "grad_norm": 12.582987785339355,
      "learning_rate": 8.687572590011614e-06,
      "loss": 1.1121,
      "step": 2244
    },
    {
      "epoch": 0.8691444057297716,
      "grad_norm": 23.34198570251465,
      "learning_rate": 8.691444057297716e-06,
      "loss": 1.1769,
      "step": 2245
    },
    {
      "epoch": 0.8695315524583818,
      "grad_norm": 12.891820907592773,
      "learning_rate": 8.695315524583819e-06,
      "loss": 0.7337,
      "step": 2246
    },
    {
      "epoch": 0.8699186991869918,
      "grad_norm": 24.68622398376465,
      "learning_rate": 8.69918699186992e-06,
      "loss": 2.071,
      "step": 2247
    },
    {
      "epoch": 0.870305845915602,
      "grad_norm": 14.691848754882812,
      "learning_rate": 8.70305845915602e-06,
      "loss": 2.1575,
      "step": 2248
    },
    {
      "epoch": 0.8706929926442122,
      "grad_norm": 20.639516830444336,
      "learning_rate": 8.706929926442122e-06,
      "loss": 2.0549,
      "step": 2249
    },
    {
      "epoch": 0.8710801393728222,
      "grad_norm": 12.297811508178711,
      "learning_rate": 8.710801393728223e-06,
      "loss": 1.4962,
      "step": 2250
    },
    {
      "epoch": 0.8714672861014324,
      "grad_norm": 17.77300262451172,
      "learning_rate": 8.714672861014325e-06,
      "loss": 2.23,
      "step": 2251
    },
    {
      "epoch": 0.8718544328300426,
      "grad_norm": 11.60481071472168,
      "learning_rate": 8.718544328300426e-06,
      "loss": 1.6528,
      "step": 2252
    },
    {
      "epoch": 0.8722415795586528,
      "grad_norm": 15.060504913330078,
      "learning_rate": 8.722415795586528e-06,
      "loss": 1.5089,
      "step": 2253
    },
    {
      "epoch": 0.8726287262872628,
      "grad_norm": 14.47146224975586,
      "learning_rate": 8.726287262872629e-06,
      "loss": 1.4909,
      "step": 2254
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 11.070438385009766,
      "learning_rate": 8.730158730158731e-06,
      "loss": 1.4419,
      "step": 2255
    },
    {
      "epoch": 0.8734030197444832,
      "grad_norm": 12.088664054870605,
      "learning_rate": 8.734030197444832e-06,
      "loss": 1.1327,
      "step": 2256
    },
    {
      "epoch": 0.8737901664730933,
      "grad_norm": 17.66997528076172,
      "learning_rate": 8.737901664730933e-06,
      "loss": 1.4289,
      "step": 2257
    },
    {
      "epoch": 0.8741773132017034,
      "grad_norm": 14.155426025390625,
      "learning_rate": 8.741773132017035e-06,
      "loss": 1.535,
      "step": 2258
    },
    {
      "epoch": 0.8745644599303136,
      "grad_norm": 12.567061424255371,
      "learning_rate": 8.745644599303137e-06,
      "loss": 1.4027,
      "step": 2259
    },
    {
      "epoch": 0.8749516066589237,
      "grad_norm": 15.7269926071167,
      "learning_rate": 8.749516066589238e-06,
      "loss": 1.4648,
      "step": 2260
    },
    {
      "epoch": 0.8753387533875339,
      "grad_norm": 20.81868553161621,
      "learning_rate": 8.753387533875339e-06,
      "loss": 1.9668,
      "step": 2261
    },
    {
      "epoch": 0.875725900116144,
      "grad_norm": 12.825963973999023,
      "learning_rate": 8.757259001161441e-06,
      "loss": 1.662,
      "step": 2262
    },
    {
      "epoch": 0.8761130468447541,
      "grad_norm": 13.30033016204834,
      "learning_rate": 8.761130468447542e-06,
      "loss": 1.7352,
      "step": 2263
    },
    {
      "epoch": 0.8765001935733643,
      "grad_norm": 16.659343719482422,
      "learning_rate": 8.765001935733644e-06,
      "loss": 1.3876,
      "step": 2264
    },
    {
      "epoch": 0.8768873403019745,
      "grad_norm": 16.27847671508789,
      "learning_rate": 8.768873403019745e-06,
      "loss": 1.5372,
      "step": 2265
    },
    {
      "epoch": 0.8772744870305846,
      "grad_norm": 19.99696922302246,
      "learning_rate": 8.772744870305847e-06,
      "loss": 0.9447,
      "step": 2266
    },
    {
      "epoch": 0.8776616337591947,
      "grad_norm": 15.020037651062012,
      "learning_rate": 8.776616337591948e-06,
      "loss": 1.401,
      "step": 2267
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 13.22045612335205,
      "learning_rate": 8.78048780487805e-06,
      "loss": 1.5589,
      "step": 2268
    },
    {
      "epoch": 0.8784359272164151,
      "grad_norm": 28.87611198425293,
      "learning_rate": 8.78435927216415e-06,
      "loss": 1.5489,
      "step": 2269
    },
    {
      "epoch": 0.8788230739450251,
      "grad_norm": 14.399734497070312,
      "learning_rate": 8.788230739450251e-06,
      "loss": 1.6196,
      "step": 2270
    },
    {
      "epoch": 0.8792102206736353,
      "grad_norm": 18.10629653930664,
      "learning_rate": 8.792102206736354e-06,
      "loss": 1.4175,
      "step": 2271
    },
    {
      "epoch": 0.8795973674022455,
      "grad_norm": 11.851116180419922,
      "learning_rate": 8.795973674022456e-06,
      "loss": 1.1562,
      "step": 2272
    },
    {
      "epoch": 0.8799845141308555,
      "grad_norm": 16.055912017822266,
      "learning_rate": 8.799845141308556e-06,
      "loss": 1.5998,
      "step": 2273
    },
    {
      "epoch": 0.8803716608594657,
      "grad_norm": 32.25862503051758,
      "learning_rate": 8.803716608594657e-06,
      "loss": 1.5831,
      "step": 2274
    },
    {
      "epoch": 0.8807588075880759,
      "grad_norm": 15.607915878295898,
      "learning_rate": 8.80758807588076e-06,
      "loss": 1.5792,
      "step": 2275
    },
    {
      "epoch": 0.8811459543166861,
      "grad_norm": 18.523290634155273,
      "learning_rate": 8.811459543166862e-06,
      "loss": 1.6403,
      "step": 2276
    },
    {
      "epoch": 0.8815331010452961,
      "grad_norm": 23.760475158691406,
      "learning_rate": 8.815331010452962e-06,
      "loss": 1.4693,
      "step": 2277
    },
    {
      "epoch": 0.8819202477739063,
      "grad_norm": 17.157516479492188,
      "learning_rate": 8.819202477739063e-06,
      "loss": 1.7005,
      "step": 2278
    },
    {
      "epoch": 0.8823073945025165,
      "grad_norm": 17.168176651000977,
      "learning_rate": 8.823073945025165e-06,
      "loss": 1.5906,
      "step": 2279
    },
    {
      "epoch": 0.8826945412311266,
      "grad_norm": 16.34307861328125,
      "learning_rate": 8.826945412311266e-06,
      "loss": 1.4369,
      "step": 2280
    },
    {
      "epoch": 0.8830816879597367,
      "grad_norm": 14.74492073059082,
      "learning_rate": 8.830816879597368e-06,
      "loss": 1.2281,
      "step": 2281
    },
    {
      "epoch": 0.8834688346883469,
      "grad_norm": 11.97536563873291,
      "learning_rate": 8.834688346883469e-06,
      "loss": 1.3218,
      "step": 2282
    },
    {
      "epoch": 0.883855981416957,
      "grad_norm": 24.429309844970703,
      "learning_rate": 8.83855981416957e-06,
      "loss": 1.4704,
      "step": 2283
    },
    {
      "epoch": 0.8842431281455672,
      "grad_norm": 12.320389747619629,
      "learning_rate": 8.842431281455672e-06,
      "loss": 1.1616,
      "step": 2284
    },
    {
      "epoch": 0.8846302748741773,
      "grad_norm": 14.957619667053223,
      "learning_rate": 8.846302748741774e-06,
      "loss": 1.5386,
      "step": 2285
    },
    {
      "epoch": 0.8850174216027874,
      "grad_norm": 12.924386978149414,
      "learning_rate": 8.850174216027875e-06,
      "loss": 1.4798,
      "step": 2286
    },
    {
      "epoch": 0.8854045683313976,
      "grad_norm": 16.73602867126465,
      "learning_rate": 8.854045683313976e-06,
      "loss": 1.3266,
      "step": 2287
    },
    {
      "epoch": 0.8857917150600078,
      "grad_norm": 11.90524959564209,
      "learning_rate": 8.857917150600078e-06,
      "loss": 1.1304,
      "step": 2288
    },
    {
      "epoch": 0.8861788617886179,
      "grad_norm": 13.111261367797852,
      "learning_rate": 8.86178861788618e-06,
      "loss": 1.612,
      "step": 2289
    },
    {
      "epoch": 0.886566008517228,
      "grad_norm": 10.81860065460205,
      "learning_rate": 8.865660085172281e-06,
      "loss": 1.0047,
      "step": 2290
    },
    {
      "epoch": 0.8869531552458382,
      "grad_norm": 11.84177303314209,
      "learning_rate": 8.869531552458382e-06,
      "loss": 0.9812,
      "step": 2291
    },
    {
      "epoch": 0.8873403019744484,
      "grad_norm": 20.840534210205078,
      "learning_rate": 8.873403019744484e-06,
      "loss": 1.7371,
      "step": 2292
    },
    {
      "epoch": 0.8877274487030584,
      "grad_norm": 22.989032745361328,
      "learning_rate": 8.877274487030585e-06,
      "loss": 1.6599,
      "step": 2293
    },
    {
      "epoch": 0.8881145954316686,
      "grad_norm": 12.005914688110352,
      "learning_rate": 8.881145954316687e-06,
      "loss": 1.1745,
      "step": 2294
    },
    {
      "epoch": 0.8885017421602788,
      "grad_norm": 13.745367050170898,
      "learning_rate": 8.885017421602788e-06,
      "loss": 1.7005,
      "step": 2295
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 13.400556564331055,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.4364,
      "step": 2296
    },
    {
      "epoch": 0.889276035617499,
      "grad_norm": 15.58880615234375,
      "learning_rate": 8.89276035617499e-06,
      "loss": 1.2581,
      "step": 2297
    },
    {
      "epoch": 0.8896631823461092,
      "grad_norm": 15.218204498291016,
      "learning_rate": 8.896631823461093e-06,
      "loss": 1.6279,
      "step": 2298
    },
    {
      "epoch": 0.8900503290747194,
      "grad_norm": 10.251004219055176,
      "learning_rate": 8.900503290747194e-06,
      "loss": 1.5468,
      "step": 2299
    },
    {
      "epoch": 0.8904374758033294,
      "grad_norm": 11.572959899902344,
      "learning_rate": 8.904374758033294e-06,
      "loss": 1.119,
      "step": 2300
    },
    {
      "epoch": 0.8908246225319396,
      "grad_norm": 13.885424613952637,
      "learning_rate": 8.908246225319397e-06,
      "loss": 2.0078,
      "step": 2301
    },
    {
      "epoch": 0.8912117692605498,
      "grad_norm": 8.547845840454102,
      "learning_rate": 8.912117692605499e-06,
      "loss": 1.403,
      "step": 2302
    },
    {
      "epoch": 0.8915989159891599,
      "grad_norm": 19.928274154663086,
      "learning_rate": 8.9159891598916e-06,
      "loss": 2.2332,
      "step": 2303
    },
    {
      "epoch": 0.89198606271777,
      "grad_norm": 10.80008602142334,
      "learning_rate": 8.9198606271777e-06,
      "loss": 1.5958,
      "step": 2304
    },
    {
      "epoch": 0.8923732094463802,
      "grad_norm": 10.781627655029297,
      "learning_rate": 8.923732094463803e-06,
      "loss": 1.1257,
      "step": 2305
    },
    {
      "epoch": 0.8927603561749903,
      "grad_norm": 18.424612045288086,
      "learning_rate": 8.927603561749903e-06,
      "loss": 1.6353,
      "step": 2306
    },
    {
      "epoch": 0.8931475029036005,
      "grad_norm": 8.803183555603027,
      "learning_rate": 8.931475029036006e-06,
      "loss": 1.4344,
      "step": 2307
    },
    {
      "epoch": 0.8935346496322106,
      "grad_norm": 10.109408378601074,
      "learning_rate": 8.935346496322106e-06,
      "loss": 1.4473,
      "step": 2308
    },
    {
      "epoch": 0.8939217963608207,
      "grad_norm": 10.411210060119629,
      "learning_rate": 8.939217963608207e-06,
      "loss": 1.6122,
      "step": 2309
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 12.85700798034668,
      "learning_rate": 8.94308943089431e-06,
      "loss": 1.2788,
      "step": 2310
    },
    {
      "epoch": 0.894696089818041,
      "grad_norm": 23.017993927001953,
      "learning_rate": 8.946960898180412e-06,
      "loss": 1.8537,
      "step": 2311
    },
    {
      "epoch": 0.8950832365466512,
      "grad_norm": 23.13499641418457,
      "learning_rate": 8.950832365466512e-06,
      "loss": 3.1293,
      "step": 2312
    },
    {
      "epoch": 0.8954703832752613,
      "grad_norm": 18.346111297607422,
      "learning_rate": 8.954703832752613e-06,
      "loss": 2.1261,
      "step": 2313
    },
    {
      "epoch": 0.8958575300038715,
      "grad_norm": 16.246475219726562,
      "learning_rate": 8.958575300038715e-06,
      "loss": 1.6187,
      "step": 2314
    },
    {
      "epoch": 0.8962446767324816,
      "grad_norm": 14.044034004211426,
      "learning_rate": 8.962446767324818e-06,
      "loss": 2.0409,
      "step": 2315
    },
    {
      "epoch": 0.8966318234610917,
      "grad_norm": 9.911795616149902,
      "learning_rate": 8.966318234610918e-06,
      "loss": 1.4129,
      "step": 2316
    },
    {
      "epoch": 0.8970189701897019,
      "grad_norm": 22.82571029663086,
      "learning_rate": 8.970189701897019e-06,
      "loss": 1.9633,
      "step": 2317
    },
    {
      "epoch": 0.8974061169183121,
      "grad_norm": 12.343034744262695,
      "learning_rate": 8.974061169183121e-06,
      "loss": 1.1637,
      "step": 2318
    },
    {
      "epoch": 0.8977932636469221,
      "grad_norm": 10.994402885437012,
      "learning_rate": 8.977932636469222e-06,
      "loss": 1.592,
      "step": 2319
    },
    {
      "epoch": 0.8981804103755323,
      "grad_norm": 23.1755313873291,
      "learning_rate": 8.981804103755324e-06,
      "loss": 1.6409,
      "step": 2320
    },
    {
      "epoch": 0.8985675571041425,
      "grad_norm": 11.940260887145996,
      "learning_rate": 8.985675571041425e-06,
      "loss": 1.6626,
      "step": 2321
    },
    {
      "epoch": 0.8989547038327527,
      "grad_norm": 19.94672203063965,
      "learning_rate": 8.989547038327527e-06,
      "loss": 1.5697,
      "step": 2322
    },
    {
      "epoch": 0.8993418505613627,
      "grad_norm": 17.600189208984375,
      "learning_rate": 8.993418505613628e-06,
      "loss": 1.1746,
      "step": 2323
    },
    {
      "epoch": 0.8997289972899729,
      "grad_norm": 12.956334114074707,
      "learning_rate": 8.99728997289973e-06,
      "loss": 1.6042,
      "step": 2324
    },
    {
      "epoch": 0.9001161440185831,
      "grad_norm": 23.419052124023438,
      "learning_rate": 9.00116144018583e-06,
      "loss": 1.4831,
      "step": 2325
    },
    {
      "epoch": 0.9005032907471932,
      "grad_norm": 16.746448516845703,
      "learning_rate": 9.005032907471933e-06,
      "loss": 1.9065,
      "step": 2326
    },
    {
      "epoch": 0.9008904374758033,
      "grad_norm": 13.007083892822266,
      "learning_rate": 9.008904374758034e-06,
      "loss": 0.6093,
      "step": 2327
    },
    {
      "epoch": 0.9012775842044135,
      "grad_norm": 12.367864608764648,
      "learning_rate": 9.012775842044136e-06,
      "loss": 1.4398,
      "step": 2328
    },
    {
      "epoch": 0.9016647309330236,
      "grad_norm": 12.033641815185547,
      "learning_rate": 9.016647309330237e-06,
      "loss": 1.1713,
      "step": 2329
    },
    {
      "epoch": 0.9020518776616337,
      "grad_norm": 14.2890043258667,
      "learning_rate": 9.020518776616339e-06,
      "loss": 1.5398,
      "step": 2330
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 13.913548469543457,
      "learning_rate": 9.02439024390244e-06,
      "loss": 1.4976,
      "step": 2331
    },
    {
      "epoch": 0.902826171118854,
      "grad_norm": 15.085039138793945,
      "learning_rate": 9.02826171118854e-06,
      "loss": 1.6067,
      "step": 2332
    },
    {
      "epoch": 0.9032133178474642,
      "grad_norm": 17.061906814575195,
      "learning_rate": 9.032133178474643e-06,
      "loss": 1.9206,
      "step": 2333
    },
    {
      "epoch": 0.9036004645760743,
      "grad_norm": 23.209362030029297,
      "learning_rate": 9.036004645760745e-06,
      "loss": 1.588,
      "step": 2334
    },
    {
      "epoch": 0.9039876113046845,
      "grad_norm": 26.369630813598633,
      "learning_rate": 9.039876113046846e-06,
      "loss": 2.1319,
      "step": 2335
    },
    {
      "epoch": 0.9043747580332946,
      "grad_norm": 13.020764350891113,
      "learning_rate": 9.043747580332946e-06,
      "loss": 1.4821,
      "step": 2336
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 15.711220741271973,
      "learning_rate": 9.047619047619049e-06,
      "loss": 1.5899,
      "step": 2337
    },
    {
      "epoch": 0.9051490514905149,
      "grad_norm": 18.32108497619629,
      "learning_rate": 9.051490514905151e-06,
      "loss": 1.6117,
      "step": 2338
    },
    {
      "epoch": 0.905536198219125,
      "grad_norm": 13.442981719970703,
      "learning_rate": 9.055361982191252e-06,
      "loss": 1.5049,
      "step": 2339
    },
    {
      "epoch": 0.9059233449477352,
      "grad_norm": 12.958024024963379,
      "learning_rate": 9.059233449477352e-06,
      "loss": 1.5989,
      "step": 2340
    },
    {
      "epoch": 0.9063104916763454,
      "grad_norm": 23.134050369262695,
      "learning_rate": 9.063104916763455e-06,
      "loss": 1.7602,
      "step": 2341
    },
    {
      "epoch": 0.9066976384049554,
      "grad_norm": 34.762451171875,
      "learning_rate": 9.066976384049555e-06,
      "loss": 1.477,
      "step": 2342
    },
    {
      "epoch": 0.9070847851335656,
      "grad_norm": 14.378808975219727,
      "learning_rate": 9.070847851335658e-06,
      "loss": 1.0421,
      "step": 2343
    },
    {
      "epoch": 0.9074719318621758,
      "grad_norm": 20.486675262451172,
      "learning_rate": 9.074719318621758e-06,
      "loss": 1.9366,
      "step": 2344
    },
    {
      "epoch": 0.907859078590786,
      "grad_norm": 13.179032325744629,
      "learning_rate": 9.07859078590786e-06,
      "loss": 1.5512,
      "step": 2345
    },
    {
      "epoch": 0.908246225319396,
      "grad_norm": 31.507125854492188,
      "learning_rate": 9.082462253193961e-06,
      "loss": 1.4161,
      "step": 2346
    },
    {
      "epoch": 0.9086333720480062,
      "grad_norm": 20.894813537597656,
      "learning_rate": 9.086333720480064e-06,
      "loss": 1.5576,
      "step": 2347
    },
    {
      "epoch": 0.9090205187766164,
      "grad_norm": 11.242952346801758,
      "learning_rate": 9.090205187766164e-06,
      "loss": 1.4449,
      "step": 2348
    },
    {
      "epoch": 0.9094076655052264,
      "grad_norm": 15.805705070495605,
      "learning_rate": 9.094076655052265e-06,
      "loss": 1.2177,
      "step": 2349
    },
    {
      "epoch": 0.9097948122338366,
      "grad_norm": 23.7022705078125,
      "learning_rate": 9.097948122338367e-06,
      "loss": 1.3178,
      "step": 2350
    },
    {
      "epoch": 0.9101819589624468,
      "grad_norm": 11.574383735656738,
      "learning_rate": 9.10181958962447e-06,
      "loss": 1.6262,
      "step": 2351
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 16.93019676208496,
      "learning_rate": 9.10569105691057e-06,
      "loss": 1.8163,
      "step": 2352
    },
    {
      "epoch": 0.910956252419667,
      "grad_norm": 17.140581130981445,
      "learning_rate": 9.109562524196671e-06,
      "loss": 2.0023,
      "step": 2353
    },
    {
      "epoch": 0.9113433991482772,
      "grad_norm": 19.254098892211914,
      "learning_rate": 9.113433991482773e-06,
      "loss": 1.5814,
      "step": 2354
    },
    {
      "epoch": 0.9117305458768873,
      "grad_norm": 13.242351531982422,
      "learning_rate": 9.117305458768874e-06,
      "loss": 1.7686,
      "step": 2355
    },
    {
      "epoch": 0.9121176926054975,
      "grad_norm": 18.668685913085938,
      "learning_rate": 9.121176926054976e-06,
      "loss": 1.6695,
      "step": 2356
    },
    {
      "epoch": 0.9125048393341076,
      "grad_norm": 13.012821197509766,
      "learning_rate": 9.125048393341077e-06,
      "loss": 0.6488,
      "step": 2357
    },
    {
      "epoch": 0.9128919860627178,
      "grad_norm": 10.107338905334473,
      "learning_rate": 9.12891986062718e-06,
      "loss": 1.4543,
      "step": 2358
    },
    {
      "epoch": 0.9132791327913279,
      "grad_norm": 19.161630630493164,
      "learning_rate": 9.13279132791328e-06,
      "loss": 1.5762,
      "step": 2359
    },
    {
      "epoch": 0.9136662795199381,
      "grad_norm": 22.690185546875,
      "learning_rate": 9.136662795199382e-06,
      "loss": 2.2651,
      "step": 2360
    },
    {
      "epoch": 0.9140534262485482,
      "grad_norm": 11.013131141662598,
      "learning_rate": 9.140534262485483e-06,
      "loss": 1.7743,
      "step": 2361
    },
    {
      "epoch": 0.9144405729771583,
      "grad_norm": 30.30308723449707,
      "learning_rate": 9.144405729771583e-06,
      "loss": 1.5577,
      "step": 2362
    },
    {
      "epoch": 0.9148277197057685,
      "grad_norm": 16.0367374420166,
      "learning_rate": 9.148277197057686e-06,
      "loss": 1.589,
      "step": 2363
    },
    {
      "epoch": 0.9152148664343787,
      "grad_norm": 18.354352951049805,
      "learning_rate": 9.152148664343788e-06,
      "loss": 1.9855,
      "step": 2364
    },
    {
      "epoch": 0.9156020131629887,
      "grad_norm": 12.190423965454102,
      "learning_rate": 9.156020131629889e-06,
      "loss": 1.6008,
      "step": 2365
    },
    {
      "epoch": 0.9159891598915989,
      "grad_norm": 20.238805770874023,
      "learning_rate": 9.15989159891599e-06,
      "loss": 1.4064,
      "step": 2366
    },
    {
      "epoch": 0.9163763066202091,
      "grad_norm": 20.29241943359375,
      "learning_rate": 9.163763066202092e-06,
      "loss": 1.4082,
      "step": 2367
    },
    {
      "epoch": 0.9167634533488193,
      "grad_norm": 13.115478515625,
      "learning_rate": 9.167634533488194e-06,
      "loss": 1.7288,
      "step": 2368
    },
    {
      "epoch": 0.9171506000774293,
      "grad_norm": 13.040194511413574,
      "learning_rate": 9.171506000774295e-06,
      "loss": 1.1587,
      "step": 2369
    },
    {
      "epoch": 0.9175377468060395,
      "grad_norm": 23.43735694885254,
      "learning_rate": 9.175377468060395e-06,
      "loss": 1.3228,
      "step": 2370
    },
    {
      "epoch": 0.9179248935346497,
      "grad_norm": 16.442445755004883,
      "learning_rate": 9.179248935346498e-06,
      "loss": 1.5597,
      "step": 2371
    },
    {
      "epoch": 0.9183120402632597,
      "grad_norm": 13.542684555053711,
      "learning_rate": 9.183120402632598e-06,
      "loss": 0.6425,
      "step": 2372
    },
    {
      "epoch": 0.9186991869918699,
      "grad_norm": 22.42603302001953,
      "learning_rate": 9.1869918699187e-06,
      "loss": 1.9084,
      "step": 2373
    },
    {
      "epoch": 0.9190863337204801,
      "grad_norm": 12.764554023742676,
      "learning_rate": 9.190863337204801e-06,
      "loss": 1.6014,
      "step": 2374
    },
    {
      "epoch": 0.9194734804490902,
      "grad_norm": 15.377429008483887,
      "learning_rate": 9.194734804490902e-06,
      "loss": 1.5523,
      "step": 2375
    },
    {
      "epoch": 0.9198606271777003,
      "grad_norm": 16.361726760864258,
      "learning_rate": 9.198606271777004e-06,
      "loss": 1.6989,
      "step": 2376
    },
    {
      "epoch": 0.9202477739063105,
      "grad_norm": 17.958755493164062,
      "learning_rate": 9.202477739063107e-06,
      "loss": 2.0709,
      "step": 2377
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 13.025574684143066,
      "learning_rate": 9.206349206349207e-06,
      "loss": 1.5105,
      "step": 2378
    },
    {
      "epoch": 0.9210220673635308,
      "grad_norm": 11.80859661102295,
      "learning_rate": 9.210220673635308e-06,
      "loss": 1.0666,
      "step": 2379
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 13.937124252319336,
      "learning_rate": 9.21409214092141e-06,
      "loss": 1.5833,
      "step": 2380
    },
    {
      "epoch": 0.9217963608207511,
      "grad_norm": 16.18181610107422,
      "learning_rate": 9.217963608207513e-06,
      "loss": 1.9268,
      "step": 2381
    },
    {
      "epoch": 0.9221835075493612,
      "grad_norm": 18.005353927612305,
      "learning_rate": 9.221835075493613e-06,
      "loss": 1.9648,
      "step": 2382
    },
    {
      "epoch": 0.9225706542779714,
      "grad_norm": 16.683561325073242,
      "learning_rate": 9.225706542779714e-06,
      "loss": 1.5371,
      "step": 2383
    },
    {
      "epoch": 0.9229578010065815,
      "grad_norm": 19.468503952026367,
      "learning_rate": 9.229578010065816e-06,
      "loss": 2.0823,
      "step": 2384
    },
    {
      "epoch": 0.9233449477351916,
      "grad_norm": 40.522422790527344,
      "learning_rate": 9.233449477351917e-06,
      "loss": 1.791,
      "step": 2385
    },
    {
      "epoch": 0.9237320944638018,
      "grad_norm": 17.683931350708008,
      "learning_rate": 9.23732094463802e-06,
      "loss": 1.5831,
      "step": 2386
    },
    {
      "epoch": 0.924119241192412,
      "grad_norm": 21.762582778930664,
      "learning_rate": 9.24119241192412e-06,
      "loss": 1.8657,
      "step": 2387
    },
    {
      "epoch": 0.924506387921022,
      "grad_norm": 16.8353271484375,
      "learning_rate": 9.24506387921022e-06,
      "loss": 2.2314,
      "step": 2388
    },
    {
      "epoch": 0.9248935346496322,
      "grad_norm": 13.438691139221191,
      "learning_rate": 9.248935346496323e-06,
      "loss": 2.0699,
      "step": 2389
    },
    {
      "epoch": 0.9252806813782424,
      "grad_norm": 9.743907928466797,
      "learning_rate": 9.252806813782425e-06,
      "loss": 1.3659,
      "step": 2390
    },
    {
      "epoch": 0.9256678281068524,
      "grad_norm": 16.128942489624023,
      "learning_rate": 9.256678281068526e-06,
      "loss": 1.666,
      "step": 2391
    },
    {
      "epoch": 0.9260549748354626,
      "grad_norm": 21.764951705932617,
      "learning_rate": 9.260549748354627e-06,
      "loss": 1.3506,
      "step": 2392
    },
    {
      "epoch": 0.9264421215640728,
      "grad_norm": 14.598368644714355,
      "learning_rate": 9.264421215640729e-06,
      "loss": 1.2683,
      "step": 2393
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 19.210384368896484,
      "learning_rate": 9.268292682926831e-06,
      "loss": 1.8057,
      "step": 2394
    },
    {
      "epoch": 0.927216415021293,
      "grad_norm": 16.101091384887695,
      "learning_rate": 9.272164150212932e-06,
      "loss": 1.1264,
      "step": 2395
    },
    {
      "epoch": 0.9276035617499032,
      "grad_norm": 12.29494857788086,
      "learning_rate": 9.276035617499033e-06,
      "loss": 1.5805,
      "step": 2396
    },
    {
      "epoch": 0.9279907084785134,
      "grad_norm": 9.9263334274292,
      "learning_rate": 9.279907084785135e-06,
      "loss": 1.3774,
      "step": 2397
    },
    {
      "epoch": 0.9283778552071235,
      "grad_norm": 13.792848587036133,
      "learning_rate": 9.283778552071236e-06,
      "loss": 1.2696,
      "step": 2398
    },
    {
      "epoch": 0.9287650019357336,
      "grad_norm": 20.09490203857422,
      "learning_rate": 9.287650019357338e-06,
      "loss": 1.885,
      "step": 2399
    },
    {
      "epoch": 0.9291521486643438,
      "grad_norm": 11.56367015838623,
      "learning_rate": 9.291521486643439e-06,
      "loss": 1.0915,
      "step": 2400
    },
    {
      "epoch": 0.9295392953929539,
      "grad_norm": 13.34237289428711,
      "learning_rate": 9.29539295392954e-06,
      "loss": 1.1436,
      "step": 2401
    },
    {
      "epoch": 0.9299264421215641,
      "grad_norm": 11.43200397491455,
      "learning_rate": 9.299264421215642e-06,
      "loss": 1.4022,
      "step": 2402
    },
    {
      "epoch": 0.9303135888501742,
      "grad_norm": 20.76986312866211,
      "learning_rate": 9.303135888501744e-06,
      "loss": 1.4049,
      "step": 2403
    },
    {
      "epoch": 0.9307007355787844,
      "grad_norm": 14.892484664916992,
      "learning_rate": 9.307007355787845e-06,
      "loss": 1.477,
      "step": 2404
    },
    {
      "epoch": 0.9310878823073945,
      "grad_norm": 25.995710372924805,
      "learning_rate": 9.310878823073945e-06,
      "loss": 1.9349,
      "step": 2405
    },
    {
      "epoch": 0.9314750290360047,
      "grad_norm": 27.663976669311523,
      "learning_rate": 9.314750290360047e-06,
      "loss": 1.6966,
      "step": 2406
    },
    {
      "epoch": 0.9318621757646148,
      "grad_norm": 16.943809509277344,
      "learning_rate": 9.31862175764615e-06,
      "loss": 1.4204,
      "step": 2407
    },
    {
      "epoch": 0.9322493224932249,
      "grad_norm": 14.838353157043457,
      "learning_rate": 9.32249322493225e-06,
      "loss": 1.4813,
      "step": 2408
    },
    {
      "epoch": 0.9326364692218351,
      "grad_norm": 10.75448989868164,
      "learning_rate": 9.326364692218351e-06,
      "loss": 1.4801,
      "step": 2409
    },
    {
      "epoch": 0.9330236159504453,
      "grad_norm": 16.601646423339844,
      "learning_rate": 9.330236159504453e-06,
      "loss": 0.7622,
      "step": 2410
    },
    {
      "epoch": 0.9334107626790553,
      "grad_norm": 9.349617958068848,
      "learning_rate": 9.334107626790554e-06,
      "loss": 1.4584,
      "step": 2411
    },
    {
      "epoch": 0.9337979094076655,
      "grad_norm": 31.27602195739746,
      "learning_rate": 9.337979094076656e-06,
      "loss": 1.4303,
      "step": 2412
    },
    {
      "epoch": 0.9341850561362757,
      "grad_norm": 17.315326690673828,
      "learning_rate": 9.341850561362757e-06,
      "loss": 1.7213,
      "step": 2413
    },
    {
      "epoch": 0.9345722028648857,
      "grad_norm": 20.14692497253418,
      "learning_rate": 9.345722028648858e-06,
      "loss": 1.5622,
      "step": 2414
    },
    {
      "epoch": 0.9349593495934959,
      "grad_norm": 12.344671249389648,
      "learning_rate": 9.34959349593496e-06,
      "loss": 1.2256,
      "step": 2415
    },
    {
      "epoch": 0.9353464963221061,
      "grad_norm": 18.859813690185547,
      "learning_rate": 9.353464963221062e-06,
      "loss": 1.0116,
      "step": 2416
    },
    {
      "epoch": 0.9357336430507163,
      "grad_norm": 25.69373893737793,
      "learning_rate": 9.357336430507163e-06,
      "loss": 2.3313,
      "step": 2417
    },
    {
      "epoch": 0.9361207897793263,
      "grad_norm": 10.889843940734863,
      "learning_rate": 9.361207897793264e-06,
      "loss": 1.3481,
      "step": 2418
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 16.764297485351562,
      "learning_rate": 9.365079365079366e-06,
      "loss": 1.8266,
      "step": 2419
    },
    {
      "epoch": 0.9368950832365467,
      "grad_norm": 17.193193435668945,
      "learning_rate": 9.368950832365468e-06,
      "loss": 2.1259,
      "step": 2420
    },
    {
      "epoch": 0.9372822299651568,
      "grad_norm": 14.0260009765625,
      "learning_rate": 9.372822299651569e-06,
      "loss": 1.6465,
      "step": 2421
    },
    {
      "epoch": 0.9376693766937669,
      "grad_norm": 15.246230125427246,
      "learning_rate": 9.37669376693767e-06,
      "loss": 1.2291,
      "step": 2422
    },
    {
      "epoch": 0.9380565234223771,
      "grad_norm": 13.033675193786621,
      "learning_rate": 9.380565234223772e-06,
      "loss": 1.3934,
      "step": 2423
    },
    {
      "epoch": 0.9384436701509872,
      "grad_norm": 21.061824798583984,
      "learning_rate": 9.384436701509873e-06,
      "loss": 1.4074,
      "step": 2424
    },
    {
      "epoch": 0.9388308168795974,
      "grad_norm": 14.433723449707031,
      "learning_rate": 9.388308168795975e-06,
      "loss": 1.3989,
      "step": 2425
    },
    {
      "epoch": 0.9392179636082075,
      "grad_norm": 14.267729759216309,
      "learning_rate": 9.392179636082076e-06,
      "loss": 1.0865,
      "step": 2426
    },
    {
      "epoch": 0.9396051103368177,
      "grad_norm": 24.80900764465332,
      "learning_rate": 9.396051103368178e-06,
      "loss": 2.0322,
      "step": 2427
    },
    {
      "epoch": 0.9399922570654278,
      "grad_norm": 9.134562492370605,
      "learning_rate": 9.399922570654279e-06,
      "loss": 0.9409,
      "step": 2428
    },
    {
      "epoch": 0.940379403794038,
      "grad_norm": 37.82712936401367,
      "learning_rate": 9.403794037940381e-06,
      "loss": 1.7059,
      "step": 2429
    },
    {
      "epoch": 0.9407665505226481,
      "grad_norm": 22.591991424560547,
      "learning_rate": 9.407665505226482e-06,
      "loss": 1.5855,
      "step": 2430
    },
    {
      "epoch": 0.9411536972512582,
      "grad_norm": 13.572936058044434,
      "learning_rate": 9.411536972512582e-06,
      "loss": 1.4127,
      "step": 2431
    },
    {
      "epoch": 0.9415408439798684,
      "grad_norm": 18.473112106323242,
      "learning_rate": 9.415408439798685e-06,
      "loss": 2.1885,
      "step": 2432
    },
    {
      "epoch": 0.9419279907084785,
      "grad_norm": 14.272032737731934,
      "learning_rate": 9.419279907084787e-06,
      "loss": 1.419,
      "step": 2433
    },
    {
      "epoch": 0.9423151374370886,
      "grad_norm": 24.690614700317383,
      "learning_rate": 9.423151374370888e-06,
      "loss": 1.3399,
      "step": 2434
    },
    {
      "epoch": 0.9427022841656988,
      "grad_norm": 7.785721302032471,
      "learning_rate": 9.427022841656988e-06,
      "loss": 1.7068,
      "step": 2435
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 17.01764488220215,
      "learning_rate": 9.43089430894309e-06,
      "loss": 1.4329,
      "step": 2436
    },
    {
      "epoch": 0.943476577622919,
      "grad_norm": 25.782939910888672,
      "learning_rate": 9.434765776229191e-06,
      "loss": 1.5691,
      "step": 2437
    },
    {
      "epoch": 0.9438637243515292,
      "grad_norm": 15.145503044128418,
      "learning_rate": 9.438637243515294e-06,
      "loss": 1.7079,
      "step": 2438
    },
    {
      "epoch": 0.9442508710801394,
      "grad_norm": 15.970056533813477,
      "learning_rate": 9.442508710801394e-06,
      "loss": 1.0651,
      "step": 2439
    },
    {
      "epoch": 0.9446380178087496,
      "grad_norm": 9.232255935668945,
      "learning_rate": 9.446380178087497e-06,
      "loss": 1.3785,
      "step": 2440
    },
    {
      "epoch": 0.9450251645373596,
      "grad_norm": 15.347969055175781,
      "learning_rate": 9.450251645373597e-06,
      "loss": 1.9582,
      "step": 2441
    },
    {
      "epoch": 0.9454123112659698,
      "grad_norm": 13.718271255493164,
      "learning_rate": 9.4541231126597e-06,
      "loss": 1.6878,
      "step": 2442
    },
    {
      "epoch": 0.94579945799458,
      "grad_norm": 12.794452667236328,
      "learning_rate": 9.4579945799458e-06,
      "loss": 1.6623,
      "step": 2443
    },
    {
      "epoch": 0.94618660472319,
      "grad_norm": 12.400466918945312,
      "learning_rate": 9.461866047231901e-06,
      "loss": 1.0793,
      "step": 2444
    },
    {
      "epoch": 0.9465737514518002,
      "grad_norm": 18.931352615356445,
      "learning_rate": 9.465737514518003e-06,
      "loss": 1.6084,
      "step": 2445
    },
    {
      "epoch": 0.9469608981804104,
      "grad_norm": 38.066829681396484,
      "learning_rate": 9.469608981804106e-06,
      "loss": 2.7413,
      "step": 2446
    },
    {
      "epoch": 0.9473480449090205,
      "grad_norm": 25.894744873046875,
      "learning_rate": 9.473480449090206e-06,
      "loss": 1.7575,
      "step": 2447
    },
    {
      "epoch": 0.9477351916376306,
      "grad_norm": 42.95613098144531,
      "learning_rate": 9.477351916376307e-06,
      "loss": 1.134,
      "step": 2448
    },
    {
      "epoch": 0.9481223383662408,
      "grad_norm": 13.600801467895508,
      "learning_rate": 9.48122338366241e-06,
      "loss": 1.3171,
      "step": 2449
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 11.727331161499023,
      "learning_rate": 9.485094850948512e-06,
      "loss": 1.7117,
      "step": 2450
    },
    {
      "epoch": 0.9488966318234611,
      "grad_norm": 15.058943748474121,
      "learning_rate": 9.488966318234612e-06,
      "loss": 1.73,
      "step": 2451
    },
    {
      "epoch": 0.9492837785520712,
      "grad_norm": 19.389455795288086,
      "learning_rate": 9.492837785520713e-06,
      "loss": 1.7607,
      "step": 2452
    },
    {
      "epoch": 0.9496709252806814,
      "grad_norm": 12.034249305725098,
      "learning_rate": 9.496709252806815e-06,
      "loss": 1.7567,
      "step": 2453
    },
    {
      "epoch": 0.9500580720092915,
      "grad_norm": 30.04859161376953,
      "learning_rate": 9.500580720092916e-06,
      "loss": 1.8477,
      "step": 2454
    },
    {
      "epoch": 0.9504452187379017,
      "grad_norm": 17.20099449157715,
      "learning_rate": 9.504452187379018e-06,
      "loss": 1.4302,
      "step": 2455
    },
    {
      "epoch": 0.9508323654665118,
      "grad_norm": 8.845955848693848,
      "learning_rate": 9.508323654665119e-06,
      "loss": 1.3515,
      "step": 2456
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 21.175443649291992,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.3648,
      "step": 2457
    },
    {
      "epoch": 0.9516066589237321,
      "grad_norm": 22.235788345336914,
      "learning_rate": 9.516066589237322e-06,
      "loss": 1.6389,
      "step": 2458
    },
    {
      "epoch": 0.9519938056523423,
      "grad_norm": 16.619800567626953,
      "learning_rate": 9.519938056523424e-06,
      "loss": 2.0895,
      "step": 2459
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 13.149585723876953,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.1648,
      "step": 2460
    },
    {
      "epoch": 0.9527680991095625,
      "grad_norm": 23.319137573242188,
      "learning_rate": 9.527680991095625e-06,
      "loss": 2.8377,
      "step": 2461
    },
    {
      "epoch": 0.9531552458381727,
      "grad_norm": 14.723240852355957,
      "learning_rate": 9.531552458381728e-06,
      "loss": 1.5215,
      "step": 2462
    },
    {
      "epoch": 0.9535423925667829,
      "grad_norm": 19.70477867126465,
      "learning_rate": 9.53542392566783e-06,
      "loss": 1.5722,
      "step": 2463
    },
    {
      "epoch": 0.9539295392953929,
      "grad_norm": 27.534887313842773,
      "learning_rate": 9.53929539295393e-06,
      "loss": 1.6698,
      "step": 2464
    },
    {
      "epoch": 0.9543166860240031,
      "grad_norm": 20.85808753967285,
      "learning_rate": 9.543166860240031e-06,
      "loss": 1.5283,
      "step": 2465
    },
    {
      "epoch": 0.9547038327526133,
      "grad_norm": 19.407377243041992,
      "learning_rate": 9.547038327526134e-06,
      "loss": 1.8617,
      "step": 2466
    },
    {
      "epoch": 0.9550909794812233,
      "grad_norm": 47.65606689453125,
      "learning_rate": 9.550909794812234e-06,
      "loss": 2.9037,
      "step": 2467
    },
    {
      "epoch": 0.9554781262098335,
      "grad_norm": 16.20706558227539,
      "learning_rate": 9.554781262098337e-06,
      "loss": 1.7534,
      "step": 2468
    },
    {
      "epoch": 0.9558652729384437,
      "grad_norm": 12.767754554748535,
      "learning_rate": 9.558652729384437e-06,
      "loss": 1.4429,
      "step": 2469
    },
    {
      "epoch": 0.9562524196670538,
      "grad_norm": 20.498306274414062,
      "learning_rate": 9.562524196670538e-06,
      "loss": 1.3395,
      "step": 2470
    },
    {
      "epoch": 0.9566395663956639,
      "grad_norm": 15.765213012695312,
      "learning_rate": 9.56639566395664e-06,
      "loss": 1.4481,
      "step": 2471
    },
    {
      "epoch": 0.9570267131242741,
      "grad_norm": 15.332927703857422,
      "learning_rate": 9.570267131242743e-06,
      "loss": 1.6043,
      "step": 2472
    },
    {
      "epoch": 0.9574138598528843,
      "grad_norm": 29.2025203704834,
      "learning_rate": 9.574138598528843e-06,
      "loss": 1.1671,
      "step": 2473
    },
    {
      "epoch": 0.9578010065814944,
      "grad_norm": 26.906808853149414,
      "learning_rate": 9.578010065814944e-06,
      "loss": 1.8244,
      "step": 2474
    },
    {
      "epoch": 0.9581881533101045,
      "grad_norm": 22.60091781616211,
      "learning_rate": 9.581881533101046e-06,
      "loss": 1.788,
      "step": 2475
    },
    {
      "epoch": 0.9585753000387147,
      "grad_norm": 15.026811599731445,
      "learning_rate": 9.585753000387149e-06,
      "loss": 1.4295,
      "step": 2476
    },
    {
      "epoch": 0.9589624467673248,
      "grad_norm": 16.645891189575195,
      "learning_rate": 9.58962446767325e-06,
      "loss": 1.5895,
      "step": 2477
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 9.505697250366211,
      "learning_rate": 9.59349593495935e-06,
      "loss": 1.5302,
      "step": 2478
    },
    {
      "epoch": 0.9597367402245451,
      "grad_norm": 14.520769119262695,
      "learning_rate": 9.597367402245452e-06,
      "loss": 1.7381,
      "step": 2479
    },
    {
      "epoch": 0.9601238869531552,
      "grad_norm": 24.15897560119629,
      "learning_rate": 9.601238869531553e-06,
      "loss": 2.2318,
      "step": 2480
    },
    {
      "epoch": 0.9605110336817654,
      "grad_norm": 14.324383735656738,
      "learning_rate": 9.605110336817655e-06,
      "loss": 1.243,
      "step": 2481
    },
    {
      "epoch": 0.9608981804103756,
      "grad_norm": 17.041664123535156,
      "learning_rate": 9.608981804103756e-06,
      "loss": 1.8038,
      "step": 2482
    },
    {
      "epoch": 0.9612853271389856,
      "grad_norm": 13.885501861572266,
      "learning_rate": 9.612853271389857e-06,
      "loss": 1.3225,
      "step": 2483
    },
    {
      "epoch": 0.9616724738675958,
      "grad_norm": 29.35101318359375,
      "learning_rate": 9.616724738675959e-06,
      "loss": 1.497,
      "step": 2484
    },
    {
      "epoch": 0.962059620596206,
      "grad_norm": 10.921612739562988,
      "learning_rate": 9.620596205962061e-06,
      "loss": 1.6965,
      "step": 2485
    },
    {
      "epoch": 0.9624467673248162,
      "grad_norm": 16.019756317138672,
      "learning_rate": 9.624467673248162e-06,
      "loss": 1.3167,
      "step": 2486
    },
    {
      "epoch": 0.9628339140534262,
      "grad_norm": 12.54230785369873,
      "learning_rate": 9.628339140534263e-06,
      "loss": 1.1435,
      "step": 2487
    },
    {
      "epoch": 0.9632210607820364,
      "grad_norm": 9.355463027954102,
      "learning_rate": 9.632210607820365e-06,
      "loss": 1.4365,
      "step": 2488
    },
    {
      "epoch": 0.9636082075106466,
      "grad_norm": 10.55498218536377,
      "learning_rate": 9.636082075106467e-06,
      "loss": 1.6198,
      "step": 2489
    },
    {
      "epoch": 0.9639953542392566,
      "grad_norm": 13.701261520385742,
      "learning_rate": 9.639953542392568e-06,
      "loss": 2.2337,
      "step": 2490
    },
    {
      "epoch": 0.9643825009678668,
      "grad_norm": 40.77080535888672,
      "learning_rate": 9.643825009678669e-06,
      "loss": 1.3366,
      "step": 2491
    },
    {
      "epoch": 0.964769647696477,
      "grad_norm": 21.886022567749023,
      "learning_rate": 9.64769647696477e-06,
      "loss": 1.4343,
      "step": 2492
    },
    {
      "epoch": 0.9651567944250871,
      "grad_norm": 10.636088371276855,
      "learning_rate": 9.651567944250871e-06,
      "loss": 1.1051,
      "step": 2493
    },
    {
      "epoch": 0.9655439411536972,
      "grad_norm": 17.435609817504883,
      "learning_rate": 9.655439411536974e-06,
      "loss": 1.1707,
      "step": 2494
    },
    {
      "epoch": 0.9659310878823074,
      "grad_norm": 9.50867748260498,
      "learning_rate": 9.659310878823074e-06,
      "loss": 1.4348,
      "step": 2495
    },
    {
      "epoch": 0.9663182346109176,
      "grad_norm": 17.800506591796875,
      "learning_rate": 9.663182346109177e-06,
      "loss": 1.6305,
      "step": 2496
    },
    {
      "epoch": 0.9667053813395277,
      "grad_norm": 11.626612663269043,
      "learning_rate": 9.667053813395277e-06,
      "loss": 1.6394,
      "step": 2497
    },
    {
      "epoch": 0.9670925280681378,
      "grad_norm": 14.243045806884766,
      "learning_rate": 9.67092528068138e-06,
      "loss": 1.4224,
      "step": 2498
    },
    {
      "epoch": 0.967479674796748,
      "grad_norm": 13.251462936401367,
      "learning_rate": 9.67479674796748e-06,
      "loss": 1.3466,
      "step": 2499
    },
    {
      "epoch": 0.9678668215253581,
      "grad_norm": 8.359162330627441,
      "learning_rate": 9.678668215253581e-06,
      "loss": 1.0328,
      "step": 2500
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 17.624109268188477,
      "learning_rate": 9.682539682539683e-06,
      "loss": 1.6187,
      "step": 2501
    },
    {
      "epoch": 0.9686411149825784,
      "grad_norm": 15.328519821166992,
      "learning_rate": 9.686411149825786e-06,
      "loss": 1.8545,
      "step": 2502
    },
    {
      "epoch": 0.9690282617111885,
      "grad_norm": 13.258936882019043,
      "learning_rate": 9.690282617111886e-06,
      "loss": 2.1023,
      "step": 2503
    },
    {
      "epoch": 0.9694154084397987,
      "grad_norm": 13.75233268737793,
      "learning_rate": 9.694154084397987e-06,
      "loss": 1.3671,
      "step": 2504
    },
    {
      "epoch": 0.9698025551684089,
      "grad_norm": 9.982054710388184,
      "learning_rate": 9.69802555168409e-06,
      "loss": 0.9629,
      "step": 2505
    },
    {
      "epoch": 0.9701897018970189,
      "grad_norm": 11.263108253479004,
      "learning_rate": 9.70189701897019e-06,
      "loss": 1.6017,
      "step": 2506
    },
    {
      "epoch": 0.9705768486256291,
      "grad_norm": 16.134258270263672,
      "learning_rate": 9.705768486256292e-06,
      "loss": 1.5041,
      "step": 2507
    },
    {
      "epoch": 0.9709639953542393,
      "grad_norm": 14.935892105102539,
      "learning_rate": 9.709639953542393e-06,
      "loss": 1.4109,
      "step": 2508
    },
    {
      "epoch": 0.9713511420828495,
      "grad_norm": 9.89331340789795,
      "learning_rate": 9.713511420828495e-06,
      "loss": 1.601,
      "step": 2509
    },
    {
      "epoch": 0.9717382888114595,
      "grad_norm": 30.166852951049805,
      "learning_rate": 9.717382888114596e-06,
      "loss": 1.6253,
      "step": 2510
    },
    {
      "epoch": 0.9721254355400697,
      "grad_norm": 15.278709411621094,
      "learning_rate": 9.721254355400698e-06,
      "loss": 1.2424,
      "step": 2511
    },
    {
      "epoch": 0.9725125822686799,
      "grad_norm": 16.50385856628418,
      "learning_rate": 9.725125822686799e-06,
      "loss": 1.8546,
      "step": 2512
    },
    {
      "epoch": 0.9728997289972899,
      "grad_norm": 19.038211822509766,
      "learning_rate": 9.7289972899729e-06,
      "loss": 1.5566,
      "step": 2513
    },
    {
      "epoch": 0.9732868757259001,
      "grad_norm": 19.280607223510742,
      "learning_rate": 9.732868757259002e-06,
      "loss": 1.9039,
      "step": 2514
    },
    {
      "epoch": 0.9736740224545103,
      "grad_norm": 27.72847557067871,
      "learning_rate": 9.736740224545104e-06,
      "loss": 1.0504,
      "step": 2515
    },
    {
      "epoch": 0.9740611691831204,
      "grad_norm": 15.415870666503906,
      "learning_rate": 9.740611691831205e-06,
      "loss": 1.4687,
      "step": 2516
    },
    {
      "epoch": 0.9744483159117305,
      "grad_norm": 10.676066398620605,
      "learning_rate": 9.744483159117306e-06,
      "loss": 1.4078,
      "step": 2517
    },
    {
      "epoch": 0.9748354626403407,
      "grad_norm": 13.590201377868652,
      "learning_rate": 9.748354626403408e-06,
      "loss": 1.1165,
      "step": 2518
    },
    {
      "epoch": 0.9752226093689508,
      "grad_norm": 12.787718772888184,
      "learning_rate": 9.752226093689509e-06,
      "loss": 1.5834,
      "step": 2519
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 34.39543533325195,
      "learning_rate": 9.756097560975611e-06,
      "loss": 1.6226,
      "step": 2520
    },
    {
      "epoch": 0.9759969028261711,
      "grad_norm": 22.205814361572266,
      "learning_rate": 9.759969028261712e-06,
      "loss": 1.8868,
      "step": 2521
    },
    {
      "epoch": 0.9763840495547813,
      "grad_norm": 14.953932762145996,
      "learning_rate": 9.763840495547814e-06,
      "loss": 1.5732,
      "step": 2522
    },
    {
      "epoch": 0.9767711962833914,
      "grad_norm": 24.26693344116211,
      "learning_rate": 9.767711962833915e-06,
      "loss": 1.6035,
      "step": 2523
    },
    {
      "epoch": 0.9771583430120016,
      "grad_norm": 25.152528762817383,
      "learning_rate": 9.771583430120017e-06,
      "loss": 1.9118,
      "step": 2524
    },
    {
      "epoch": 0.9775454897406117,
      "grad_norm": 12.62132740020752,
      "learning_rate": 9.775454897406118e-06,
      "loss": 1.565,
      "step": 2525
    },
    {
      "epoch": 0.9779326364692218,
      "grad_norm": 14.87847900390625,
      "learning_rate": 9.779326364692218e-06,
      "loss": 1.5118,
      "step": 2526
    },
    {
      "epoch": 0.978319783197832,
      "grad_norm": 15.64487075805664,
      "learning_rate": 9.78319783197832e-06,
      "loss": 1.3423,
      "step": 2527
    },
    {
      "epoch": 0.9787069299264421,
      "grad_norm": 13.70876407623291,
      "learning_rate": 9.787069299264423e-06,
      "loss": 1.4081,
      "step": 2528
    },
    {
      "epoch": 0.9790940766550522,
      "grad_norm": 15.113752365112305,
      "learning_rate": 9.790940766550524e-06,
      "loss": 1.6401,
      "step": 2529
    },
    {
      "epoch": 0.9794812233836624,
      "grad_norm": 14.120649337768555,
      "learning_rate": 9.794812233836624e-06,
      "loss": 1.1461,
      "step": 2530
    },
    {
      "epoch": 0.9798683701122726,
      "grad_norm": 27.340917587280273,
      "learning_rate": 9.798683701122727e-06,
      "loss": 2.1994,
      "step": 2531
    },
    {
      "epoch": 0.9802555168408827,
      "grad_norm": 17.262170791625977,
      "learning_rate": 9.802555168408829e-06,
      "loss": 1.5305,
      "step": 2532
    },
    {
      "epoch": 0.9806426635694928,
      "grad_norm": 13.281227111816406,
      "learning_rate": 9.80642663569493e-06,
      "loss": 0.9674,
      "step": 2533
    },
    {
      "epoch": 0.981029810298103,
      "grad_norm": 20.432065963745117,
      "learning_rate": 9.81029810298103e-06,
      "loss": 1.5451,
      "step": 2534
    },
    {
      "epoch": 0.9814169570267132,
      "grad_norm": 14.50888729095459,
      "learning_rate": 9.814169570267133e-06,
      "loss": 2.1751,
      "step": 2535
    },
    {
      "epoch": 0.9818041037553232,
      "grad_norm": 7.63320779800415,
      "learning_rate": 9.818041037553233e-06,
      "loss": 0.9587,
      "step": 2536
    },
    {
      "epoch": 0.9821912504839334,
      "grad_norm": 13.159502983093262,
      "learning_rate": 9.821912504839336e-06,
      "loss": 1.6428,
      "step": 2537
    },
    {
      "epoch": 0.9825783972125436,
      "grad_norm": 20.119869232177734,
      "learning_rate": 9.825783972125436e-06,
      "loss": 1.5929,
      "step": 2538
    },
    {
      "epoch": 0.9829655439411537,
      "grad_norm": 8.515982627868652,
      "learning_rate": 9.829655439411537e-06,
      "loss": 0.9254,
      "step": 2539
    },
    {
      "epoch": 0.9833526906697638,
      "grad_norm": 12.554961204528809,
      "learning_rate": 9.833526906697639e-06,
      "loss": 1.1249,
      "step": 2540
    },
    {
      "epoch": 0.983739837398374,
      "grad_norm": 13.83731460571289,
      "learning_rate": 9.837398373983741e-06,
      "loss": 1.428,
      "step": 2541
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 15.132648468017578,
      "learning_rate": 9.841269841269842e-06,
      "loss": 2.0606,
      "step": 2542
    },
    {
      "epoch": 0.9845141308555942,
      "grad_norm": 13.795101165771484,
      "learning_rate": 9.845141308555943e-06,
      "loss": 1.1191,
      "step": 2543
    },
    {
      "epoch": 0.9849012775842044,
      "grad_norm": 32.88582992553711,
      "learning_rate": 9.849012775842045e-06,
      "loss": 2.1854,
      "step": 2544
    },
    {
      "epoch": 0.9852884243128146,
      "grad_norm": 17.08940887451172,
      "learning_rate": 9.852884243128147e-06,
      "loss": 1.5562,
      "step": 2545
    },
    {
      "epoch": 0.9856755710414247,
      "grad_norm": 12.026966094970703,
      "learning_rate": 9.856755710414248e-06,
      "loss": 1.5602,
      "step": 2546
    },
    {
      "epoch": 0.9860627177700348,
      "grad_norm": 19.02543067932129,
      "learning_rate": 9.860627177700349e-06,
      "loss": 2.2267,
      "step": 2547
    },
    {
      "epoch": 0.986449864498645,
      "grad_norm": 16.640535354614258,
      "learning_rate": 9.864498644986451e-06,
      "loss": 1.4911,
      "step": 2548
    },
    {
      "epoch": 0.9868370112272551,
      "grad_norm": 19.505102157592773,
      "learning_rate": 9.868370112272552e-06,
      "loss": 1.8137,
      "step": 2549
    },
    {
      "epoch": 0.9872241579558653,
      "grad_norm": 10.739411354064941,
      "learning_rate": 9.872241579558654e-06,
      "loss": 1.5136,
      "step": 2550
    },
    {
      "epoch": 0.9876113046844754,
      "grad_norm": 13.492147445678711,
      "learning_rate": 9.876113046844755e-06,
      "loss": 1.5452,
      "step": 2551
    },
    {
      "epoch": 0.9879984514130855,
      "grad_norm": 18.027143478393555,
      "learning_rate": 9.879984514130855e-06,
      "loss": 1.6421,
      "step": 2552
    },
    {
      "epoch": 0.9883855981416957,
      "grad_norm": 14.806660652160645,
      "learning_rate": 9.883855981416958e-06,
      "loss": 1.4613,
      "step": 2553
    },
    {
      "epoch": 0.9887727448703059,
      "grad_norm": 10.595080375671387,
      "learning_rate": 9.88772744870306e-06,
      "loss": 1.3687,
      "step": 2554
    },
    {
      "epoch": 0.989159891598916,
      "grad_norm": 16.189510345458984,
      "learning_rate": 9.89159891598916e-06,
      "loss": 1.6571,
      "step": 2555
    },
    {
      "epoch": 0.9895470383275261,
      "grad_norm": 21.16738510131836,
      "learning_rate": 9.895470383275261e-06,
      "loss": 2.5705,
      "step": 2556
    },
    {
      "epoch": 0.9899341850561363,
      "grad_norm": 18.801536560058594,
      "learning_rate": 9.899341850561364e-06,
      "loss": 1.7222,
      "step": 2557
    },
    {
      "epoch": 0.9903213317847465,
      "grad_norm": 14.925885200500488,
      "learning_rate": 9.903213317847466e-06,
      "loss": 1.5635,
      "step": 2558
    },
    {
      "epoch": 0.9907084785133565,
      "grad_norm": 19.014347076416016,
      "learning_rate": 9.907084785133567e-06,
      "loss": 1.6099,
      "step": 2559
    },
    {
      "epoch": 0.9910956252419667,
      "grad_norm": 23.29602813720703,
      "learning_rate": 9.910956252419667e-06,
      "loss": 1.86,
      "step": 2560
    },
    {
      "epoch": 0.9914827719705769,
      "grad_norm": 14.084688186645508,
      "learning_rate": 9.91482771970577e-06,
      "loss": 1.6384,
      "step": 2561
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 14.673308372497559,
      "learning_rate": 9.91869918699187e-06,
      "loss": 2.1152,
      "step": 2562
    },
    {
      "epoch": 0.9922570654277971,
      "grad_norm": 13.005427360534668,
      "learning_rate": 9.922570654277973e-06,
      "loss": 1.5282,
      "step": 2563
    },
    {
      "epoch": 0.9926442121564073,
      "grad_norm": 16.916603088378906,
      "learning_rate": 9.926442121564073e-06,
      "loss": 1.6518,
      "step": 2564
    },
    {
      "epoch": 0.9930313588850174,
      "grad_norm": 17.782567977905273,
      "learning_rate": 9.930313588850174e-06,
      "loss": 1.1994,
      "step": 2565
    },
    {
      "epoch": 0.9934185056136275,
      "grad_norm": 21.232084274291992,
      "learning_rate": 9.934185056136276e-06,
      "loss": 1.4839,
      "step": 2566
    },
    {
      "epoch": 0.9938056523422377,
      "grad_norm": 24.868227005004883,
      "learning_rate": 9.938056523422379e-06,
      "loss": 1.9744,
      "step": 2567
    },
    {
      "epoch": 0.9941927990708479,
      "grad_norm": 17.786909103393555,
      "learning_rate": 9.94192799070848e-06,
      "loss": 2.0374,
      "step": 2568
    },
    {
      "epoch": 0.994579945799458,
      "grad_norm": 15.702722549438477,
      "learning_rate": 9.94579945799458e-06,
      "loss": 1.3156,
      "step": 2569
    },
    {
      "epoch": 0.9949670925280681,
      "grad_norm": 17.65581703186035,
      "learning_rate": 9.949670925280682e-06,
      "loss": 1.6388,
      "step": 2570
    },
    {
      "epoch": 0.9953542392566783,
      "grad_norm": 21.25937843322754,
      "learning_rate": 9.953542392566785e-06,
      "loss": 1.8243,
      "step": 2571
    },
    {
      "epoch": 0.9957413859852884,
      "grad_norm": 16.396568298339844,
      "learning_rate": 9.957413859852885e-06,
      "loss": 1.3511,
      "step": 2572
    },
    {
      "epoch": 0.9961285327138986,
      "grad_norm": 22.33350372314453,
      "learning_rate": 9.961285327138986e-06,
      "loss": 1.5456,
      "step": 2573
    },
    {
      "epoch": 0.9965156794425087,
      "grad_norm": 23.696367263793945,
      "learning_rate": 9.965156794425088e-06,
      "loss": 2.4264,
      "step": 2574
    },
    {
      "epoch": 0.9969028261711188,
      "grad_norm": 19.24443244934082,
      "learning_rate": 9.969028261711189e-06,
      "loss": 1.7842,
      "step": 2575
    },
    {
      "epoch": 0.997289972899729,
      "grad_norm": 17.70773696899414,
      "learning_rate": 9.972899728997291e-06,
      "loss": 1.8099,
      "step": 2576
    },
    {
      "epoch": 0.9976771196283392,
      "grad_norm": 14.81180191040039,
      "learning_rate": 9.976771196283392e-06,
      "loss": 1.4985,
      "step": 2577
    },
    {
      "epoch": 0.9980642663569493,
      "grad_norm": 23.634035110473633,
      "learning_rate": 9.980642663569494e-06,
      "loss": 1.5511,
      "step": 2578
    },
    {
      "epoch": 0.9984514130855594,
      "grad_norm": 14.349963188171387,
      "learning_rate": 9.984514130855595e-06,
      "loss": 1.4931,
      "step": 2579
    },
    {
      "epoch": 0.9988385598141696,
      "grad_norm": 16.540782928466797,
      "learning_rate": 9.988385598141697e-06,
      "loss": 1.6154,
      "step": 2580
    },
    {
      "epoch": 0.9992257065427798,
      "grad_norm": 21.973140716552734,
      "learning_rate": 9.992257065427798e-06,
      "loss": 1.5554,
      "step": 2581
    },
    {
      "epoch": 0.9996128532713898,
      "grad_norm": 14.765721321105957,
      "learning_rate": 9.996128532713898e-06,
      "loss": 1.2196,
      "step": 2582
    },
    {
      "epoch": 1.0,
      "grad_norm": 23.19845962524414,
      "learning_rate": 1e-05,
      "loss": 1.4839,
      "step": 2583
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.30566037735849055,
      "eval_f1": 0.24708313558012496,
      "eval_loss": 1.6493834257125854,
      "eval_runtime": 421.8005,
      "eval_samples_per_second": 2.513,
      "eval_steps_per_second": 1.257,
      "step": 2583
    }
  ],
  "logging_steps": 1,
  "max_steps": 25830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4067534355776e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
