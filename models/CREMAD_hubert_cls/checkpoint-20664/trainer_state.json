{
  "best_metric": 0.5795620120939399,
  "best_model_checkpoint": "models\\CREMAD_hubert_cls\\checkpoint-20664",
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 20664,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00038714672861014324,
      "grad_norm": 1.8722529411315918,
      "learning_rate": 3.871467286101433e-09,
      "loss": 1.9665,
      "step": 1
    },
    {
      "epoch": 0.0007742934572202865,
      "grad_norm": 1.3827552795410156,
      "learning_rate": 7.742934572202866e-09,
      "loss": 1.9174,
      "step": 2
    },
    {
      "epoch": 0.0011614401858304297,
      "grad_norm": 1.2626818418502808,
      "learning_rate": 1.1614401858304298e-08,
      "loss": 1.9406,
      "step": 3
    },
    {
      "epoch": 0.001548586914440573,
      "grad_norm": 1.2638156414031982,
      "learning_rate": 1.5485869144405732e-08,
      "loss": 1.9347,
      "step": 4
    },
    {
      "epoch": 0.0019357336430507162,
      "grad_norm": 1.4256352186203003,
      "learning_rate": 1.9357336430507163e-08,
      "loss": 1.941,
      "step": 5
    },
    {
      "epoch": 0.0023228803716608595,
      "grad_norm": 1.1354379653930664,
      "learning_rate": 2.3228803716608597e-08,
      "loss": 1.9343,
      "step": 6
    },
    {
      "epoch": 0.0027100271002710027,
      "grad_norm": 1.5203808546066284,
      "learning_rate": 2.710027100271003e-08,
      "loss": 1.9437,
      "step": 7
    },
    {
      "epoch": 0.003097173828881146,
      "grad_norm": 1.2510936260223389,
      "learning_rate": 3.0971738288811464e-08,
      "loss": 1.9503,
      "step": 8
    },
    {
      "epoch": 0.003484320557491289,
      "grad_norm": 1.2681835889816284,
      "learning_rate": 3.484320557491289e-08,
      "loss": 1.9602,
      "step": 9
    },
    {
      "epoch": 0.0038714672861014324,
      "grad_norm": 1.2337955236434937,
      "learning_rate": 3.8714672861014325e-08,
      "loss": 1.9297,
      "step": 10
    },
    {
      "epoch": 0.004258614014711576,
      "grad_norm": 1.2799679040908813,
      "learning_rate": 4.258614014711576e-08,
      "loss": 1.9389,
      "step": 11
    },
    {
      "epoch": 0.004645760743321719,
      "grad_norm": 1.1558623313903809,
      "learning_rate": 4.645760743321719e-08,
      "loss": 1.9505,
      "step": 12
    },
    {
      "epoch": 0.005032907471931862,
      "grad_norm": 1.2249882221221924,
      "learning_rate": 5.032907471931863e-08,
      "loss": 1.9634,
      "step": 13
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 1.39509117603302,
      "learning_rate": 5.420054200542006e-08,
      "loss": 1.943,
      "step": 14
    },
    {
      "epoch": 0.005807200929152149,
      "grad_norm": 2.282097816467285,
      "learning_rate": 5.807200929152149e-08,
      "loss": 1.9743,
      "step": 15
    },
    {
      "epoch": 0.006194347657762292,
      "grad_norm": 1.5924534797668457,
      "learning_rate": 6.194347657762293e-08,
      "loss": 1.9034,
      "step": 16
    },
    {
      "epoch": 0.006581494386372435,
      "grad_norm": 1.3129477500915527,
      "learning_rate": 6.581494386372436e-08,
      "loss": 1.9399,
      "step": 17
    },
    {
      "epoch": 0.006968641114982578,
      "grad_norm": 1.948675274848938,
      "learning_rate": 6.968641114982578e-08,
      "loss": 1.921,
      "step": 18
    },
    {
      "epoch": 0.007355787843592722,
      "grad_norm": 1.161292552947998,
      "learning_rate": 7.355787843592722e-08,
      "loss": 1.9295,
      "step": 19
    },
    {
      "epoch": 0.007742934572202865,
      "grad_norm": 1.602184772491455,
      "learning_rate": 7.742934572202865e-08,
      "loss": 1.9164,
      "step": 20
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 1.4578857421875,
      "learning_rate": 8.130081300813009e-08,
      "loss": 1.9829,
      "step": 21
    },
    {
      "epoch": 0.008517228029423151,
      "grad_norm": 1.7535805702209473,
      "learning_rate": 8.517228029423152e-08,
      "loss": 2.0017,
      "step": 22
    },
    {
      "epoch": 0.008904374758033295,
      "grad_norm": 1.3390156030654907,
      "learning_rate": 8.904374758033296e-08,
      "loss": 1.9736,
      "step": 23
    },
    {
      "epoch": 0.009291521486643438,
      "grad_norm": 1.3785299062728882,
      "learning_rate": 9.291521486643439e-08,
      "loss": 1.9085,
      "step": 24
    },
    {
      "epoch": 0.009678668215253582,
      "grad_norm": 1.6784433126449585,
      "learning_rate": 9.678668215253583e-08,
      "loss": 1.9873,
      "step": 25
    },
    {
      "epoch": 0.010065814943863724,
      "grad_norm": 1.1994949579238892,
      "learning_rate": 1.0065814943863725e-07,
      "loss": 1.9368,
      "step": 26
    },
    {
      "epoch": 0.010452961672473868,
      "grad_norm": 2.8482437133789062,
      "learning_rate": 1.045296167247387e-07,
      "loss": 2.0242,
      "step": 27
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 1.3148632049560547,
      "learning_rate": 1.0840108401084012e-07,
      "loss": 1.9134,
      "step": 28
    },
    {
      "epoch": 0.011227255129694155,
      "grad_norm": 1.808053731918335,
      "learning_rate": 1.1227255129694156e-07,
      "loss": 1.9751,
      "step": 29
    },
    {
      "epoch": 0.011614401858304297,
      "grad_norm": 1.7089858055114746,
      "learning_rate": 1.1614401858304298e-07,
      "loss": 1.9722,
      "step": 30
    },
    {
      "epoch": 0.012001548586914441,
      "grad_norm": 1.437395453453064,
      "learning_rate": 1.2001548586914442e-07,
      "loss": 1.9501,
      "step": 31
    },
    {
      "epoch": 0.012388695315524584,
      "grad_norm": 1.1888842582702637,
      "learning_rate": 1.2388695315524586e-07,
      "loss": 1.9494,
      "step": 32
    },
    {
      "epoch": 0.012775842044134728,
      "grad_norm": 1.641129732131958,
      "learning_rate": 1.277584204413473e-07,
      "loss": 1.9476,
      "step": 33
    },
    {
      "epoch": 0.01316298877274487,
      "grad_norm": 1.4256542921066284,
      "learning_rate": 1.316298877274487e-07,
      "loss": 1.9882,
      "step": 34
    },
    {
      "epoch": 0.013550135501355014,
      "grad_norm": 1.4232455492019653,
      "learning_rate": 1.3550135501355015e-07,
      "loss": 1.9382,
      "step": 35
    },
    {
      "epoch": 0.013937282229965157,
      "grad_norm": 1.3072998523712158,
      "learning_rate": 1.3937282229965157e-07,
      "loss": 1.9046,
      "step": 36
    },
    {
      "epoch": 0.014324428958575301,
      "grad_norm": 1.658265471458435,
      "learning_rate": 1.4324428958575303e-07,
      "loss": 1.9523,
      "step": 37
    },
    {
      "epoch": 0.014711575687185443,
      "grad_norm": 2.552384853363037,
      "learning_rate": 1.4711575687185445e-07,
      "loss": 1.9937,
      "step": 38
    },
    {
      "epoch": 0.015098722415795587,
      "grad_norm": 1.3056846857070923,
      "learning_rate": 1.509872241579559e-07,
      "loss": 1.9564,
      "step": 39
    },
    {
      "epoch": 0.01548586914440573,
      "grad_norm": 1.5663657188415527,
      "learning_rate": 1.548586914440573e-07,
      "loss": 1.9578,
      "step": 40
    },
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 1.4655976295471191,
      "learning_rate": 1.5873015873015874e-07,
      "loss": 1.9609,
      "step": 41
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 1.363230586051941,
      "learning_rate": 1.6260162601626018e-07,
      "loss": 1.9456,
      "step": 42
    },
    {
      "epoch": 0.01664730933023616,
      "grad_norm": 1.4018268585205078,
      "learning_rate": 1.6647309330236162e-07,
      "loss": 1.974,
      "step": 43
    },
    {
      "epoch": 0.017034456058846303,
      "grad_norm": 1.313948631286621,
      "learning_rate": 1.7034456058846304e-07,
      "loss": 1.9217,
      "step": 44
    },
    {
      "epoch": 0.017421602787456445,
      "grad_norm": 1.3514381647109985,
      "learning_rate": 1.7421602787456448e-07,
      "loss": 1.9363,
      "step": 45
    },
    {
      "epoch": 0.01780874951606659,
      "grad_norm": 1.3341065645217896,
      "learning_rate": 1.7808749516066592e-07,
      "loss": 1.947,
      "step": 46
    },
    {
      "epoch": 0.018195896244676733,
      "grad_norm": 1.6700693368911743,
      "learning_rate": 1.8195896244676736e-07,
      "loss": 1.9013,
      "step": 47
    },
    {
      "epoch": 0.018583042973286876,
      "grad_norm": 1.316781997680664,
      "learning_rate": 1.8583042973286877e-07,
      "loss": 1.9705,
      "step": 48
    },
    {
      "epoch": 0.018970189701897018,
      "grad_norm": 1.6118602752685547,
      "learning_rate": 1.897018970189702e-07,
      "loss": 1.945,
      "step": 49
    },
    {
      "epoch": 0.019357336430507164,
      "grad_norm": 1.5270217657089233,
      "learning_rate": 1.9357336430507165e-07,
      "loss": 1.9544,
      "step": 50
    },
    {
      "epoch": 0.019744483159117306,
      "grad_norm": 1.4832324981689453,
      "learning_rate": 1.9744483159117307e-07,
      "loss": 1.9545,
      "step": 51
    },
    {
      "epoch": 0.02013162988772745,
      "grad_norm": 1.3447526693344116,
      "learning_rate": 2.013162988772745e-07,
      "loss": 1.9503,
      "step": 52
    },
    {
      "epoch": 0.02051877661633759,
      "grad_norm": 1.3612908124923706,
      "learning_rate": 2.0518776616337592e-07,
      "loss": 1.9433,
      "step": 53
    },
    {
      "epoch": 0.020905923344947737,
      "grad_norm": 1.390224814414978,
      "learning_rate": 2.090592334494774e-07,
      "loss": 1.94,
      "step": 54
    },
    {
      "epoch": 0.02129307007355788,
      "grad_norm": 1.323393702507019,
      "learning_rate": 2.129307007355788e-07,
      "loss": 1.9492,
      "step": 55
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 1.2235162258148193,
      "learning_rate": 2.1680216802168024e-07,
      "loss": 1.9142,
      "step": 56
    },
    {
      "epoch": 0.022067363530778164,
      "grad_norm": 1.4587013721466064,
      "learning_rate": 2.2067363530778166e-07,
      "loss": 1.9667,
      "step": 57
    },
    {
      "epoch": 0.02245451025938831,
      "grad_norm": 1.5683465003967285,
      "learning_rate": 2.2454510259388312e-07,
      "loss": 1.9902,
      "step": 58
    },
    {
      "epoch": 0.022841656987998452,
      "grad_norm": 1.1734521389007568,
      "learning_rate": 2.2841656987998454e-07,
      "loss": 1.9399,
      "step": 59
    },
    {
      "epoch": 0.023228803716608595,
      "grad_norm": 1.223766803741455,
      "learning_rate": 2.3228803716608595e-07,
      "loss": 1.9228,
      "step": 60
    },
    {
      "epoch": 0.023615950445218737,
      "grad_norm": 1.4083489179611206,
      "learning_rate": 2.361595044521874e-07,
      "loss": 1.9386,
      "step": 61
    },
    {
      "epoch": 0.024003097173828883,
      "grad_norm": 1.222882628440857,
      "learning_rate": 2.4003097173828883e-07,
      "loss": 1.9436,
      "step": 62
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 1.1779512166976929,
      "learning_rate": 2.439024390243903e-07,
      "loss": 1.9587,
      "step": 63
    },
    {
      "epoch": 0.024777390631049168,
      "grad_norm": 1.3427867889404297,
      "learning_rate": 2.477739063104917e-07,
      "loss": 1.9423,
      "step": 64
    },
    {
      "epoch": 0.02516453735965931,
      "grad_norm": 1.3166260719299316,
      "learning_rate": 2.516453735965931e-07,
      "loss": 1.9297,
      "step": 65
    },
    {
      "epoch": 0.025551684088269456,
      "grad_norm": 1.241493582725525,
      "learning_rate": 2.555168408826946e-07,
      "loss": 1.9211,
      "step": 66
    },
    {
      "epoch": 0.025938830816879598,
      "grad_norm": 1.08889901638031,
      "learning_rate": 2.59388308168796e-07,
      "loss": 1.9327,
      "step": 67
    },
    {
      "epoch": 0.02632597754548974,
      "grad_norm": 1.323351502418518,
      "learning_rate": 2.632597754548974e-07,
      "loss": 1.9433,
      "step": 68
    },
    {
      "epoch": 0.026713124274099883,
      "grad_norm": 1.6102895736694336,
      "learning_rate": 2.6713124274099886e-07,
      "loss": 1.9331,
      "step": 69
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 1.2116812467575073,
      "learning_rate": 2.710027100271003e-07,
      "loss": 1.9339,
      "step": 70
    },
    {
      "epoch": 0.02748741773132017,
      "grad_norm": 1.355094075202942,
      "learning_rate": 2.7487417731320175e-07,
      "loss": 1.9562,
      "step": 71
    },
    {
      "epoch": 0.027874564459930314,
      "grad_norm": 1.508041262626648,
      "learning_rate": 2.7874564459930313e-07,
      "loss": 1.9267,
      "step": 72
    },
    {
      "epoch": 0.028261711188540456,
      "grad_norm": 1.884343147277832,
      "learning_rate": 2.8261711188540457e-07,
      "loss": 1.9387,
      "step": 73
    },
    {
      "epoch": 0.028648857917150602,
      "grad_norm": 1.177363395690918,
      "learning_rate": 2.8648857917150607e-07,
      "loss": 1.9102,
      "step": 74
    },
    {
      "epoch": 0.029036004645760744,
      "grad_norm": 1.6059622764587402,
      "learning_rate": 2.9036004645760745e-07,
      "loss": 1.966,
      "step": 75
    },
    {
      "epoch": 0.029423151374370887,
      "grad_norm": 1.9946407079696655,
      "learning_rate": 2.942315137437089e-07,
      "loss": 1.9223,
      "step": 76
    },
    {
      "epoch": 0.02981029810298103,
      "grad_norm": 1.584816575050354,
      "learning_rate": 2.9810298102981034e-07,
      "loss": 1.9353,
      "step": 77
    },
    {
      "epoch": 0.030197444831591175,
      "grad_norm": 1.761682152748108,
      "learning_rate": 3.019744483159118e-07,
      "loss": 1.9959,
      "step": 78
    },
    {
      "epoch": 0.030584591560201317,
      "grad_norm": 1.2681838274002075,
      "learning_rate": 3.058459156020132e-07,
      "loss": 1.9638,
      "step": 79
    },
    {
      "epoch": 0.03097173828881146,
      "grad_norm": 1.2599982023239136,
      "learning_rate": 3.097173828881146e-07,
      "loss": 1.9294,
      "step": 80
    },
    {
      "epoch": 0.0313588850174216,
      "grad_norm": 1.2641021013259888,
      "learning_rate": 3.1358885017421604e-07,
      "loss": 1.9599,
      "step": 81
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 1.806406855583191,
      "learning_rate": 3.174603174603175e-07,
      "loss": 1.9721,
      "step": 82
    },
    {
      "epoch": 0.03213317847464189,
      "grad_norm": 1.8139973878860474,
      "learning_rate": 3.2133178474641887e-07,
      "loss": 1.9249,
      "step": 83
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 1.4414734840393066,
      "learning_rate": 3.2520325203252037e-07,
      "loss": 1.9418,
      "step": 84
    },
    {
      "epoch": 0.03290747193186218,
      "grad_norm": 1.3878464698791504,
      "learning_rate": 3.290747193186218e-07,
      "loss": 1.9529,
      "step": 85
    },
    {
      "epoch": 0.03329461866047232,
      "grad_norm": 1.3376766443252563,
      "learning_rate": 3.3294618660472325e-07,
      "loss": 1.9536,
      "step": 86
    },
    {
      "epoch": 0.03368176538908246,
      "grad_norm": 1.1098401546478271,
      "learning_rate": 3.3681765389082463e-07,
      "loss": 1.9283,
      "step": 87
    },
    {
      "epoch": 0.034068912117692605,
      "grad_norm": 1.6624932289123535,
      "learning_rate": 3.406891211769261e-07,
      "loss": 1.9864,
      "step": 88
    },
    {
      "epoch": 0.03445605884630275,
      "grad_norm": 1.6171804666519165,
      "learning_rate": 3.445605884630275e-07,
      "loss": 1.9468,
      "step": 89
    },
    {
      "epoch": 0.03484320557491289,
      "grad_norm": 1.2598140239715576,
      "learning_rate": 3.4843205574912896e-07,
      "loss": 1.9234,
      "step": 90
    },
    {
      "epoch": 0.03523035230352303,
      "grad_norm": 1.2662670612335205,
      "learning_rate": 3.5230352303523034e-07,
      "loss": 1.9764,
      "step": 91
    },
    {
      "epoch": 0.03561749903213318,
      "grad_norm": 1.4466278553009033,
      "learning_rate": 3.5617499032133184e-07,
      "loss": 1.9249,
      "step": 92
    },
    {
      "epoch": 0.036004645760743324,
      "grad_norm": 1.3765774965286255,
      "learning_rate": 3.600464576074333e-07,
      "loss": 1.9697,
      "step": 93
    },
    {
      "epoch": 0.03639179248935347,
      "grad_norm": 1.7024292945861816,
      "learning_rate": 3.639179248935347e-07,
      "loss": 1.9322,
      "step": 94
    },
    {
      "epoch": 0.03677893921796361,
      "grad_norm": 1.2493489980697632,
      "learning_rate": 3.677893921796361e-07,
      "loss": 1.9433,
      "step": 95
    },
    {
      "epoch": 0.03716608594657375,
      "grad_norm": 1.2947214841842651,
      "learning_rate": 3.7166085946573755e-07,
      "loss": 1.9419,
      "step": 96
    },
    {
      "epoch": 0.037553232675183894,
      "grad_norm": 1.1667033433914185,
      "learning_rate": 3.75532326751839e-07,
      "loss": 1.9536,
      "step": 97
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 1.2410893440246582,
      "learning_rate": 3.794037940379404e-07,
      "loss": 1.9438,
      "step": 98
    },
    {
      "epoch": 0.03832752613240418,
      "grad_norm": 1.519470453262329,
      "learning_rate": 3.832752613240418e-07,
      "loss": 1.9765,
      "step": 99
    },
    {
      "epoch": 0.03871467286101433,
      "grad_norm": 1.4211254119873047,
      "learning_rate": 3.871467286101433e-07,
      "loss": 1.9347,
      "step": 100
    },
    {
      "epoch": 0.03910181958962447,
      "grad_norm": 1.6876616477966309,
      "learning_rate": 3.9101819589624475e-07,
      "loss": 1.9288,
      "step": 101
    },
    {
      "epoch": 0.03948896631823461,
      "grad_norm": 1.8199079036712646,
      "learning_rate": 3.9488966318234614e-07,
      "loss": 1.9188,
      "step": 102
    },
    {
      "epoch": 0.039876113046844755,
      "grad_norm": 1.3045974969863892,
      "learning_rate": 3.987611304684476e-07,
      "loss": 1.9071,
      "step": 103
    },
    {
      "epoch": 0.0402632597754549,
      "grad_norm": 1.272830605506897,
      "learning_rate": 4.02632597754549e-07,
      "loss": 1.9353,
      "step": 104
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 1.3459570407867432,
      "learning_rate": 4.0650406504065046e-07,
      "loss": 1.9307,
      "step": 105
    },
    {
      "epoch": 0.04103755323267518,
      "grad_norm": 1.138325572013855,
      "learning_rate": 4.1037553232675184e-07,
      "loss": 1.9277,
      "step": 106
    },
    {
      "epoch": 0.041424699961285325,
      "grad_norm": 1.2090156078338623,
      "learning_rate": 4.142469996128533e-07,
      "loss": 1.9336,
      "step": 107
    },
    {
      "epoch": 0.041811846689895474,
      "grad_norm": 1.7125427722930908,
      "learning_rate": 4.181184668989548e-07,
      "loss": 1.9698,
      "step": 108
    },
    {
      "epoch": 0.042198993418505616,
      "grad_norm": 1.2674199342727661,
      "learning_rate": 4.219899341850562e-07,
      "loss": 1.9546,
      "step": 109
    },
    {
      "epoch": 0.04258614014711576,
      "grad_norm": 1.2682015895843506,
      "learning_rate": 4.258614014711576e-07,
      "loss": 1.9724,
      "step": 110
    },
    {
      "epoch": 0.0429732868757259,
      "grad_norm": 1.6381535530090332,
      "learning_rate": 4.2973286875725905e-07,
      "loss": 1.9327,
      "step": 111
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 1.373987078666687,
      "learning_rate": 4.336043360433605e-07,
      "loss": 1.9685,
      "step": 112
    },
    {
      "epoch": 0.043747580332946186,
      "grad_norm": 1.551061987876892,
      "learning_rate": 4.374758033294619e-07,
      "loss": 1.9154,
      "step": 113
    },
    {
      "epoch": 0.04413472706155633,
      "grad_norm": 1.1360738277435303,
      "learning_rate": 4.413472706155633e-07,
      "loss": 1.9266,
      "step": 114
    },
    {
      "epoch": 0.04452187379016647,
      "grad_norm": 1.3859291076660156,
      "learning_rate": 4.4521873790166476e-07,
      "loss": 1.9895,
      "step": 115
    },
    {
      "epoch": 0.04490902051877662,
      "grad_norm": 1.1683683395385742,
      "learning_rate": 4.4909020518776625e-07,
      "loss": 1.9103,
      "step": 116
    },
    {
      "epoch": 0.04529616724738676,
      "grad_norm": 1.4732296466827393,
      "learning_rate": 4.5296167247386764e-07,
      "loss": 1.9578,
      "step": 117
    },
    {
      "epoch": 0.045683313975996905,
      "grad_norm": 1.3103270530700684,
      "learning_rate": 4.568331397599691e-07,
      "loss": 1.9643,
      "step": 118
    },
    {
      "epoch": 0.04607046070460705,
      "grad_norm": 1.541742205619812,
      "learning_rate": 4.607046070460705e-07,
      "loss": 1.9323,
      "step": 119
    },
    {
      "epoch": 0.04645760743321719,
      "grad_norm": 1.4031747579574585,
      "learning_rate": 4.645760743321719e-07,
      "loss": 1.9424,
      "step": 120
    },
    {
      "epoch": 0.04684475416182733,
      "grad_norm": 1.2545462846755981,
      "learning_rate": 4.6844754161827335e-07,
      "loss": 1.9171,
      "step": 121
    },
    {
      "epoch": 0.047231900890437474,
      "grad_norm": 1.7230219841003418,
      "learning_rate": 4.723190089043748e-07,
      "loss": 1.9284,
      "step": 122
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.366240382194519,
      "learning_rate": 4.7619047619047623e-07,
      "loss": 1.9595,
      "step": 123
    },
    {
      "epoch": 0.048006194347657766,
      "grad_norm": 1.2956231832504272,
      "learning_rate": 4.800619434765777e-07,
      "loss": 1.9364,
      "step": 124
    },
    {
      "epoch": 0.04839334107626791,
      "grad_norm": 1.6009840965270996,
      "learning_rate": 4.839334107626792e-07,
      "loss": 1.9576,
      "step": 125
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 1.8668384552001953,
      "learning_rate": 4.878048780487805e-07,
      "loss": 1.9437,
      "step": 126
    },
    {
      "epoch": 0.04916763453348819,
      "grad_norm": 1.2493070363998413,
      "learning_rate": 4.916763453348819e-07,
      "loss": 1.9069,
      "step": 127
    },
    {
      "epoch": 0.049554781262098335,
      "grad_norm": 1.4693695306777954,
      "learning_rate": 4.955478126209834e-07,
      "loss": 1.9378,
      "step": 128
    },
    {
      "epoch": 0.04994192799070848,
      "grad_norm": 1.3185383081436157,
      "learning_rate": 4.994192799070848e-07,
      "loss": 1.9629,
      "step": 129
    },
    {
      "epoch": 0.05032907471931862,
      "grad_norm": 1.330763339996338,
      "learning_rate": 5.032907471931862e-07,
      "loss": 1.9106,
      "step": 130
    },
    {
      "epoch": 0.05071622144792876,
      "grad_norm": 1.402917742729187,
      "learning_rate": 5.071622144792877e-07,
      "loss": 1.9535,
      "step": 131
    },
    {
      "epoch": 0.05110336817653891,
      "grad_norm": 1.5598068237304688,
      "learning_rate": 5.110336817653892e-07,
      "loss": 1.9177,
      "step": 132
    },
    {
      "epoch": 0.051490514905149054,
      "grad_norm": 1.5118436813354492,
      "learning_rate": 5.149051490514906e-07,
      "loss": 1.9613,
      "step": 133
    },
    {
      "epoch": 0.051877661633759196,
      "grad_norm": 1.3305963277816772,
      "learning_rate": 5.18776616337592e-07,
      "loss": 1.9311,
      "step": 134
    },
    {
      "epoch": 0.05226480836236934,
      "grad_norm": 1.5102858543395996,
      "learning_rate": 5.226480836236935e-07,
      "loss": 1.9032,
      "step": 135
    },
    {
      "epoch": 0.05265195509097948,
      "grad_norm": 1.3898465633392334,
      "learning_rate": 5.265195509097948e-07,
      "loss": 1.9022,
      "step": 136
    },
    {
      "epoch": 0.053039101819589624,
      "grad_norm": 1.5764615535736084,
      "learning_rate": 5.303910181958962e-07,
      "loss": 1.9854,
      "step": 137
    },
    {
      "epoch": 0.053426248548199766,
      "grad_norm": 1.2301346063613892,
      "learning_rate": 5.342624854819977e-07,
      "loss": 1.9294,
      "step": 138
    },
    {
      "epoch": 0.05381339527680991,
      "grad_norm": 1.7448948621749878,
      "learning_rate": 5.381339527680991e-07,
      "loss": 1.9606,
      "step": 139
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 1.4438631534576416,
      "learning_rate": 5.420054200542006e-07,
      "loss": 1.9549,
      "step": 140
    },
    {
      "epoch": 0.0545876887340302,
      "grad_norm": 1.266018033027649,
      "learning_rate": 5.45876887340302e-07,
      "loss": 1.9444,
      "step": 141
    },
    {
      "epoch": 0.05497483546264034,
      "grad_norm": 1.7858085632324219,
      "learning_rate": 5.497483546264035e-07,
      "loss": 1.965,
      "step": 142
    },
    {
      "epoch": 0.055361982191250485,
      "grad_norm": 1.4917521476745605,
      "learning_rate": 5.536198219125049e-07,
      "loss": 1.9479,
      "step": 143
    },
    {
      "epoch": 0.05574912891986063,
      "grad_norm": 1.7303102016448975,
      "learning_rate": 5.574912891986063e-07,
      "loss": 1.9638,
      "step": 144
    },
    {
      "epoch": 0.05613627564847077,
      "grad_norm": 1.435689091682434,
      "learning_rate": 5.613627564847078e-07,
      "loss": 1.914,
      "step": 145
    },
    {
      "epoch": 0.05652342237708091,
      "grad_norm": 1.178758978843689,
      "learning_rate": 5.652342237708091e-07,
      "loss": 1.9394,
      "step": 146
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 1.3719794750213623,
      "learning_rate": 5.691056910569106e-07,
      "loss": 1.9192,
      "step": 147
    },
    {
      "epoch": 0.057297715834301204,
      "grad_norm": 1.689508318901062,
      "learning_rate": 5.729771583430121e-07,
      "loss": 1.9621,
      "step": 148
    },
    {
      "epoch": 0.057684862562911346,
      "grad_norm": 1.4846851825714111,
      "learning_rate": 5.768486256291135e-07,
      "loss": 1.9338,
      "step": 149
    },
    {
      "epoch": 0.05807200929152149,
      "grad_norm": 2.1773600578308105,
      "learning_rate": 5.807200929152149e-07,
      "loss": 1.954,
      "step": 150
    },
    {
      "epoch": 0.05845915602013163,
      "grad_norm": 1.3258416652679443,
      "learning_rate": 5.845915602013164e-07,
      "loss": 1.9391,
      "step": 151
    },
    {
      "epoch": 0.05884630274874177,
      "grad_norm": 1.0761454105377197,
      "learning_rate": 5.884630274874178e-07,
      "loss": 1.9304,
      "step": 152
    },
    {
      "epoch": 0.059233449477351915,
      "grad_norm": 1.9608542919158936,
      "learning_rate": 5.923344947735192e-07,
      "loss": 1.9314,
      "step": 153
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 1.4622232913970947,
      "learning_rate": 5.962059620596207e-07,
      "loss": 1.9449,
      "step": 154
    },
    {
      "epoch": 0.0600077429345722,
      "grad_norm": 1.386207103729248,
      "learning_rate": 6.000774293457221e-07,
      "loss": 1.9475,
      "step": 155
    },
    {
      "epoch": 0.06039488966318235,
      "grad_norm": 1.374375343322754,
      "learning_rate": 6.039488966318236e-07,
      "loss": 1.9563,
      "step": 156
    },
    {
      "epoch": 0.06078203639179249,
      "grad_norm": 1.9986577033996582,
      "learning_rate": 6.078203639179249e-07,
      "loss": 1.8984,
      "step": 157
    },
    {
      "epoch": 0.061169183120402634,
      "grad_norm": 1.337410807609558,
      "learning_rate": 6.116918312040264e-07,
      "loss": 1.9413,
      "step": 158
    },
    {
      "epoch": 0.06155632984901278,
      "grad_norm": 1.569469928741455,
      "learning_rate": 6.155632984901278e-07,
      "loss": 1.9382,
      "step": 159
    },
    {
      "epoch": 0.06194347657762292,
      "grad_norm": 1.4656800031661987,
      "learning_rate": 6.194347657762292e-07,
      "loss": 1.9587,
      "step": 160
    },
    {
      "epoch": 0.06233062330623306,
      "grad_norm": 1.5173181295394897,
      "learning_rate": 6.233062330623307e-07,
      "loss": 1.8969,
      "step": 161
    },
    {
      "epoch": 0.0627177700348432,
      "grad_norm": 1.3920223712921143,
      "learning_rate": 6.271777003484321e-07,
      "loss": 1.9535,
      "step": 162
    },
    {
      "epoch": 0.06310491676345335,
      "grad_norm": 1.5692684650421143,
      "learning_rate": 6.310491676345336e-07,
      "loss": 1.964,
      "step": 163
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 1.865328073501587,
      "learning_rate": 6.34920634920635e-07,
      "loss": 1.9307,
      "step": 164
    },
    {
      "epoch": 0.06387921022067364,
      "grad_norm": 1.199167013168335,
      "learning_rate": 6.387921022067365e-07,
      "loss": 1.9431,
      "step": 165
    },
    {
      "epoch": 0.06426635694928377,
      "grad_norm": 1.1244466304779053,
      "learning_rate": 6.426635694928377e-07,
      "loss": 1.961,
      "step": 166
    },
    {
      "epoch": 0.06465350367789392,
      "grad_norm": 1.2851150035858154,
      "learning_rate": 6.465350367789392e-07,
      "loss": 1.9451,
      "step": 167
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 1.5376777648925781,
      "learning_rate": 6.504065040650407e-07,
      "loss": 1.9772,
      "step": 168
    },
    {
      "epoch": 0.06542779713511421,
      "grad_norm": 1.279203176498413,
      "learning_rate": 6.542779713511421e-07,
      "loss": 1.9595,
      "step": 169
    },
    {
      "epoch": 0.06581494386372436,
      "grad_norm": 1.3015878200531006,
      "learning_rate": 6.581494386372436e-07,
      "loss": 1.9246,
      "step": 170
    },
    {
      "epoch": 0.06620209059233449,
      "grad_norm": 1.3563429117202759,
      "learning_rate": 6.62020905923345e-07,
      "loss": 1.9266,
      "step": 171
    },
    {
      "epoch": 0.06658923732094464,
      "grad_norm": 1.6971479654312134,
      "learning_rate": 6.658923732094465e-07,
      "loss": 1.9665,
      "step": 172
    },
    {
      "epoch": 0.06697638404955478,
      "grad_norm": 1.6323630809783936,
      "learning_rate": 6.697638404955478e-07,
      "loss": 1.9093,
      "step": 173
    },
    {
      "epoch": 0.06736353077816493,
      "grad_norm": 2.0553784370422363,
      "learning_rate": 6.736353077816493e-07,
      "loss": 1.9728,
      "step": 174
    },
    {
      "epoch": 0.06775067750677506,
      "grad_norm": 1.456234335899353,
      "learning_rate": 6.775067750677507e-07,
      "loss": 1.9113,
      "step": 175
    },
    {
      "epoch": 0.06813782423538521,
      "grad_norm": 1.291272759437561,
      "learning_rate": 6.813782423538521e-07,
      "loss": 1.9161,
      "step": 176
    },
    {
      "epoch": 0.06852497096399536,
      "grad_norm": 2.527912139892578,
      "learning_rate": 6.852497096399536e-07,
      "loss": 2.01,
      "step": 177
    },
    {
      "epoch": 0.0689121176926055,
      "grad_norm": 1.593970537185669,
      "learning_rate": 6.89121176926055e-07,
      "loss": 1.9579,
      "step": 178
    },
    {
      "epoch": 0.06929926442121565,
      "grad_norm": 1.4862563610076904,
      "learning_rate": 6.929926442121565e-07,
      "loss": 1.9594,
      "step": 179
    },
    {
      "epoch": 0.06968641114982578,
      "grad_norm": 1.3331727981567383,
      "learning_rate": 6.968641114982579e-07,
      "loss": 1.9385,
      "step": 180
    },
    {
      "epoch": 0.07007355787843593,
      "grad_norm": 1.440492868423462,
      "learning_rate": 7.007355787843594e-07,
      "loss": 1.9491,
      "step": 181
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 1.2666289806365967,
      "learning_rate": 7.046070460704607e-07,
      "loss": 1.9182,
      "step": 182
    },
    {
      "epoch": 0.07084785133565621,
      "grad_norm": 2.116395950317383,
      "learning_rate": 7.084785133565622e-07,
      "loss": 1.8837,
      "step": 183
    },
    {
      "epoch": 0.07123499806426636,
      "grad_norm": 1.2691996097564697,
      "learning_rate": 7.123499806426637e-07,
      "loss": 1.9147,
      "step": 184
    },
    {
      "epoch": 0.0716221447928765,
      "grad_norm": 1.5789631605148315,
      "learning_rate": 7.162214479287651e-07,
      "loss": 1.9514,
      "step": 185
    },
    {
      "epoch": 0.07200929152148665,
      "grad_norm": 1.7871671915054321,
      "learning_rate": 7.200929152148666e-07,
      "loss": 1.9124,
      "step": 186
    },
    {
      "epoch": 0.07239643825009678,
      "grad_norm": 1.3387380838394165,
      "learning_rate": 7.239643825009679e-07,
      "loss": 1.9585,
      "step": 187
    },
    {
      "epoch": 0.07278358497870693,
      "grad_norm": 1.8699023723602295,
      "learning_rate": 7.278358497870694e-07,
      "loss": 2.0036,
      "step": 188
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 1.2714886665344238,
      "learning_rate": 7.317073170731707e-07,
      "loss": 1.9415,
      "step": 189
    },
    {
      "epoch": 0.07355787843592722,
      "grad_norm": 1.475311517715454,
      "learning_rate": 7.355787843592722e-07,
      "loss": 1.9552,
      "step": 190
    },
    {
      "epoch": 0.07394502516453735,
      "grad_norm": 2.807035446166992,
      "learning_rate": 7.394502516453736e-07,
      "loss": 2.012,
      "step": 191
    },
    {
      "epoch": 0.0743321718931475,
      "grad_norm": 1.132474422454834,
      "learning_rate": 7.433217189314751e-07,
      "loss": 1.9424,
      "step": 192
    },
    {
      "epoch": 0.07471931862175765,
      "grad_norm": 1.295541524887085,
      "learning_rate": 7.471931862175766e-07,
      "loss": 1.923,
      "step": 193
    },
    {
      "epoch": 0.07510646535036779,
      "grad_norm": 1.4100650548934937,
      "learning_rate": 7.51064653503678e-07,
      "loss": 1.9477,
      "step": 194
    },
    {
      "epoch": 0.07549361207897794,
      "grad_norm": 2.203207492828369,
      "learning_rate": 7.549361207897795e-07,
      "loss": 1.9149,
      "step": 195
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 1.3338545560836792,
      "learning_rate": 7.588075880758807e-07,
      "loss": 1.9155,
      "step": 196
    },
    {
      "epoch": 0.07626790553619822,
      "grad_norm": 1.1864159107208252,
      "learning_rate": 7.626790553619822e-07,
      "loss": 1.9412,
      "step": 197
    },
    {
      "epoch": 0.07665505226480836,
      "grad_norm": 1.3234816789627075,
      "learning_rate": 7.665505226480836e-07,
      "loss": 1.9523,
      "step": 198
    },
    {
      "epoch": 0.0770421989934185,
      "grad_norm": 1.321771502494812,
      "learning_rate": 7.704219899341851e-07,
      "loss": 1.9225,
      "step": 199
    },
    {
      "epoch": 0.07742934572202866,
      "grad_norm": 1.8656835556030273,
      "learning_rate": 7.742934572202866e-07,
      "loss": 1.9862,
      "step": 200
    },
    {
      "epoch": 0.07781649245063879,
      "grad_norm": 1.1836506128311157,
      "learning_rate": 7.78164924506388e-07,
      "loss": 1.9598,
      "step": 201
    },
    {
      "epoch": 0.07820363917924894,
      "grad_norm": 1.8203198909759521,
      "learning_rate": 7.820363917924895e-07,
      "loss": 1.9778,
      "step": 202
    },
    {
      "epoch": 0.07859078590785908,
      "grad_norm": 1.5249191522598267,
      "learning_rate": 7.859078590785908e-07,
      "loss": 1.9573,
      "step": 203
    },
    {
      "epoch": 0.07897793263646923,
      "grad_norm": 3.275322437286377,
      "learning_rate": 7.897793263646923e-07,
      "loss": 2.0267,
      "step": 204
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 1.334331750869751,
      "learning_rate": 7.936507936507937e-07,
      "loss": 1.9509,
      "step": 205
    },
    {
      "epoch": 0.07975222609368951,
      "grad_norm": 1.411104440689087,
      "learning_rate": 7.975222609368952e-07,
      "loss": 1.9031,
      "step": 206
    },
    {
      "epoch": 0.08013937282229965,
      "grad_norm": 1.5724178552627563,
      "learning_rate": 8.013937282229965e-07,
      "loss": 1.9363,
      "step": 207
    },
    {
      "epoch": 0.0805265195509098,
      "grad_norm": 1.317803144454956,
      "learning_rate": 8.05265195509098e-07,
      "loss": 1.8998,
      "step": 208
    },
    {
      "epoch": 0.08091366627951994,
      "grad_norm": 1.4644187688827515,
      "learning_rate": 8.091366627951995e-07,
      "loss": 1.9177,
      "step": 209
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 1.3745898008346558,
      "learning_rate": 8.130081300813009e-07,
      "loss": 1.9465,
      "step": 210
    },
    {
      "epoch": 0.08168795973674023,
      "grad_norm": 1.160555124282837,
      "learning_rate": 8.168795973674024e-07,
      "loss": 1.9458,
      "step": 211
    },
    {
      "epoch": 0.08207510646535036,
      "grad_norm": 1.3809980154037476,
      "learning_rate": 8.207510646535037e-07,
      "loss": 1.9491,
      "step": 212
    },
    {
      "epoch": 0.08246225319396051,
      "grad_norm": 1.1839203834533691,
      "learning_rate": 8.246225319396052e-07,
      "loss": 1.9471,
      "step": 213
    },
    {
      "epoch": 0.08284939992257065,
      "grad_norm": 1.617287516593933,
      "learning_rate": 8.284939992257066e-07,
      "loss": 1.9789,
      "step": 214
    },
    {
      "epoch": 0.0832365466511808,
      "grad_norm": 1.344551920890808,
      "learning_rate": 8.323654665118081e-07,
      "loss": 1.9604,
      "step": 215
    },
    {
      "epoch": 0.08362369337979095,
      "grad_norm": 1.2424730062484741,
      "learning_rate": 8.362369337979096e-07,
      "loss": 1.9451,
      "step": 216
    },
    {
      "epoch": 0.08401084010840108,
      "grad_norm": 2.60916805267334,
      "learning_rate": 8.401084010840109e-07,
      "loss": 2.007,
      "step": 217
    },
    {
      "epoch": 0.08439798683701123,
      "grad_norm": 1.6512129306793213,
      "learning_rate": 8.439798683701124e-07,
      "loss": 1.9516,
      "step": 218
    },
    {
      "epoch": 0.08478513356562137,
      "grad_norm": 1.2789881229400635,
      "learning_rate": 8.478513356562137e-07,
      "loss": 1.9559,
      "step": 219
    },
    {
      "epoch": 0.08517228029423152,
      "grad_norm": 1.200788140296936,
      "learning_rate": 8.517228029423152e-07,
      "loss": 1.9446,
      "step": 220
    },
    {
      "epoch": 0.08555942702284165,
      "grad_norm": 2.3394548892974854,
      "learning_rate": 8.555942702284166e-07,
      "loss": 1.9619,
      "step": 221
    },
    {
      "epoch": 0.0859465737514518,
      "grad_norm": 1.5540934801101685,
      "learning_rate": 8.594657375145181e-07,
      "loss": 1.9772,
      "step": 222
    },
    {
      "epoch": 0.08633372048006194,
      "grad_norm": 1.2303707599639893,
      "learning_rate": 8.633372048006195e-07,
      "loss": 1.9447,
      "step": 223
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 2.3388192653656006,
      "learning_rate": 8.67208672086721e-07,
      "loss": 1.9759,
      "step": 224
    },
    {
      "epoch": 0.08710801393728224,
      "grad_norm": 1.3377221822738647,
      "learning_rate": 8.710801393728225e-07,
      "loss": 1.938,
      "step": 225
    },
    {
      "epoch": 0.08749516066589237,
      "grad_norm": 1.5286308526992798,
      "learning_rate": 8.749516066589237e-07,
      "loss": 1.967,
      "step": 226
    },
    {
      "epoch": 0.08788230739450252,
      "grad_norm": 1.6421114206314087,
      "learning_rate": 8.788230739450252e-07,
      "loss": 1.9575,
      "step": 227
    },
    {
      "epoch": 0.08826945412311266,
      "grad_norm": 1.849982500076294,
      "learning_rate": 8.826945412311266e-07,
      "loss": 1.8846,
      "step": 228
    },
    {
      "epoch": 0.0886566008517228,
      "grad_norm": 1.2569493055343628,
      "learning_rate": 8.865660085172281e-07,
      "loss": 1.9301,
      "step": 229
    },
    {
      "epoch": 0.08904374758033294,
      "grad_norm": 1.2114485502243042,
      "learning_rate": 8.904374758033295e-07,
      "loss": 1.9306,
      "step": 230
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 1.7603652477264404,
      "learning_rate": 8.94308943089431e-07,
      "loss": 1.9647,
      "step": 231
    },
    {
      "epoch": 0.08981804103755324,
      "grad_norm": 2.010676145553589,
      "learning_rate": 8.981804103755325e-07,
      "loss": 1.8844,
      "step": 232
    },
    {
      "epoch": 0.09020518776616337,
      "grad_norm": 2.038668394088745,
      "learning_rate": 9.020518776616338e-07,
      "loss": 1.9139,
      "step": 233
    },
    {
      "epoch": 0.09059233449477352,
      "grad_norm": 1.405318260192871,
      "learning_rate": 9.059233449477353e-07,
      "loss": 1.9412,
      "step": 234
    },
    {
      "epoch": 0.09097948122338366,
      "grad_norm": 1.33861243724823,
      "learning_rate": 9.097948122338367e-07,
      "loss": 1.9402,
      "step": 235
    },
    {
      "epoch": 0.09136662795199381,
      "grad_norm": 1.2831127643585205,
      "learning_rate": 9.136662795199382e-07,
      "loss": 1.9616,
      "step": 236
    },
    {
      "epoch": 0.09175377468060394,
      "grad_norm": 1.9888291358947754,
      "learning_rate": 9.175377468060395e-07,
      "loss": 1.955,
      "step": 237
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 1.2296912670135498,
      "learning_rate": 9.21409214092141e-07,
      "loss": 1.9453,
      "step": 238
    },
    {
      "epoch": 0.09252806813782423,
      "grad_norm": 1.3826355934143066,
      "learning_rate": 9.252806813782423e-07,
      "loss": 1.914,
      "step": 239
    },
    {
      "epoch": 0.09291521486643438,
      "grad_norm": 1.311548113822937,
      "learning_rate": 9.291521486643438e-07,
      "loss": 1.9442,
      "step": 240
    },
    {
      "epoch": 0.09330236159504453,
      "grad_norm": 1.4872076511383057,
      "learning_rate": 9.330236159504453e-07,
      "loss": 1.9136,
      "step": 241
    },
    {
      "epoch": 0.09368950832365466,
      "grad_norm": 1.64584481716156,
      "learning_rate": 9.368950832365467e-07,
      "loss": 1.8918,
      "step": 242
    },
    {
      "epoch": 0.09407665505226481,
      "grad_norm": 1.434674859046936,
      "learning_rate": 9.407665505226482e-07,
      "loss": 1.9021,
      "step": 243
    },
    {
      "epoch": 0.09446380178087495,
      "grad_norm": 1.2531490325927734,
      "learning_rate": 9.446380178087496e-07,
      "loss": 1.9471,
      "step": 244
    },
    {
      "epoch": 0.0948509485094851,
      "grad_norm": 1.3403780460357666,
      "learning_rate": 9.485094850948511e-07,
      "loss": 1.957,
      "step": 245
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 1.2302957773208618,
      "learning_rate": 9.523809523809525e-07,
      "loss": 1.9588,
      "step": 246
    },
    {
      "epoch": 0.09562524196670538,
      "grad_norm": 2.386007308959961,
      "learning_rate": 9.562524196670538e-07,
      "loss": 1.9031,
      "step": 247
    },
    {
      "epoch": 0.09601238869531553,
      "grad_norm": 2.1216280460357666,
      "learning_rate": 9.601238869531553e-07,
      "loss": 1.8761,
      "step": 248
    },
    {
      "epoch": 0.09639953542392567,
      "grad_norm": 1.5414080619812012,
      "learning_rate": 9.639953542392568e-07,
      "loss": 1.9508,
      "step": 249
    },
    {
      "epoch": 0.09678668215253582,
      "grad_norm": 1.1540870666503906,
      "learning_rate": 9.678668215253583e-07,
      "loss": 1.9154,
      "step": 250
    },
    {
      "epoch": 0.09717382888114595,
      "grad_norm": 1.7975988388061523,
      "learning_rate": 9.717382888114596e-07,
      "loss": 1.8962,
      "step": 251
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.2319954633712769,
      "learning_rate": 9.75609756097561e-07,
      "loss": 1.9131,
      "step": 252
    },
    {
      "epoch": 0.09794812233836624,
      "grad_norm": 1.3950631618499756,
      "learning_rate": 9.794812233836624e-07,
      "loss": 1.9454,
      "step": 253
    },
    {
      "epoch": 0.09833526906697639,
      "grad_norm": 1.2658807039260864,
      "learning_rate": 9.833526906697639e-07,
      "loss": 1.9204,
      "step": 254
    },
    {
      "epoch": 0.09872241579558652,
      "grad_norm": 1.4267122745513916,
      "learning_rate": 9.872241579558654e-07,
      "loss": 1.9249,
      "step": 255
    },
    {
      "epoch": 0.09910956252419667,
      "grad_norm": 1.1770508289337158,
      "learning_rate": 9.910956252419669e-07,
      "loss": 1.9455,
      "step": 256
    },
    {
      "epoch": 0.09949670925280682,
      "grad_norm": 1.8326971530914307,
      "learning_rate": 9.949670925280684e-07,
      "loss": 1.9653,
      "step": 257
    },
    {
      "epoch": 0.09988385598141696,
      "grad_norm": 1.2412647008895874,
      "learning_rate": 9.988385598141696e-07,
      "loss": 1.9064,
      "step": 258
    },
    {
      "epoch": 0.1002710027100271,
      "grad_norm": 1.4522029161453247,
      "learning_rate": 1.0027100271002711e-06,
      "loss": 1.9688,
      "step": 259
    },
    {
      "epoch": 0.10065814943863724,
      "grad_norm": 1.323971152305603,
      "learning_rate": 1.0065814943863724e-06,
      "loss": 1.9651,
      "step": 260
    },
    {
      "epoch": 0.10104529616724739,
      "grad_norm": 1.4504698514938354,
      "learning_rate": 1.010452961672474e-06,
      "loss": 1.968,
      "step": 261
    },
    {
      "epoch": 0.10143244289585752,
      "grad_norm": 1.3116633892059326,
      "learning_rate": 1.0143244289585754e-06,
      "loss": 1.9339,
      "step": 262
    },
    {
      "epoch": 0.10181958962446767,
      "grad_norm": 1.3598530292510986,
      "learning_rate": 1.0181958962446769e-06,
      "loss": 1.9549,
      "step": 263
    },
    {
      "epoch": 0.10220673635307782,
      "grad_norm": 1.6924139261245728,
      "learning_rate": 1.0220673635307784e-06,
      "loss": 1.9765,
      "step": 264
    },
    {
      "epoch": 0.10259388308168796,
      "grad_norm": 1.5049668550491333,
      "learning_rate": 1.0259388308168797e-06,
      "loss": 1.9353,
      "step": 265
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 1.8106191158294678,
      "learning_rate": 1.0298102981029812e-06,
      "loss": 1.9957,
      "step": 266
    },
    {
      "epoch": 0.10336817653890824,
      "grad_norm": 1.6184200048446655,
      "learning_rate": 1.0336817653890824e-06,
      "loss": 1.9578,
      "step": 267
    },
    {
      "epoch": 0.10375532326751839,
      "grad_norm": 1.5791020393371582,
      "learning_rate": 1.037553232675184e-06,
      "loss": 1.9382,
      "step": 268
    },
    {
      "epoch": 0.10414246999612853,
      "grad_norm": 2.790430784225464,
      "learning_rate": 1.0414246999612854e-06,
      "loss": 2.014,
      "step": 269
    },
    {
      "epoch": 0.10452961672473868,
      "grad_norm": 1.7857729196548462,
      "learning_rate": 1.045296167247387e-06,
      "loss": 1.9737,
      "step": 270
    },
    {
      "epoch": 0.10491676345334881,
      "grad_norm": 1.3903334140777588,
      "learning_rate": 1.0491676345334882e-06,
      "loss": 1.9677,
      "step": 271
    },
    {
      "epoch": 0.10530391018195896,
      "grad_norm": 1.6178395748138428,
      "learning_rate": 1.0530391018195897e-06,
      "loss": 1.9698,
      "step": 272
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 1.5775846242904663,
      "learning_rate": 1.0569105691056912e-06,
      "loss": 1.9566,
      "step": 273
    },
    {
      "epoch": 0.10607820363917925,
      "grad_norm": 1.633928894996643,
      "learning_rate": 1.0607820363917925e-06,
      "loss": 1.9469,
      "step": 274
    },
    {
      "epoch": 0.1064653503677894,
      "grad_norm": 1.3794456720352173,
      "learning_rate": 1.064653503677894e-06,
      "loss": 1.9286,
      "step": 275
    },
    {
      "epoch": 0.10685249709639953,
      "grad_norm": 1.7416026592254639,
      "learning_rate": 1.0685249709639955e-06,
      "loss": 1.9669,
      "step": 276
    },
    {
      "epoch": 0.10723964382500968,
      "grad_norm": 1.34681236743927,
      "learning_rate": 1.072396438250097e-06,
      "loss": 1.9763,
      "step": 277
    },
    {
      "epoch": 0.10762679055361982,
      "grad_norm": 1.1881003379821777,
      "learning_rate": 1.0762679055361982e-06,
      "loss": 1.9483,
      "step": 278
    },
    {
      "epoch": 0.10801393728222997,
      "grad_norm": 1.3797640800476074,
      "learning_rate": 1.0801393728222997e-06,
      "loss": 1.9368,
      "step": 279
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 1.714133620262146,
      "learning_rate": 1.0840108401084012e-06,
      "loss": 1.959,
      "step": 280
    },
    {
      "epoch": 0.10878823073945025,
      "grad_norm": 1.6516321897506714,
      "learning_rate": 1.0878823073945025e-06,
      "loss": 1.9496,
      "step": 281
    },
    {
      "epoch": 0.1091753774680604,
      "grad_norm": 1.2354130744934082,
      "learning_rate": 1.091753774680604e-06,
      "loss": 1.911,
      "step": 282
    },
    {
      "epoch": 0.10956252419667054,
      "grad_norm": 1.9756921529769897,
      "learning_rate": 1.0956252419667055e-06,
      "loss": 1.9926,
      "step": 283
    },
    {
      "epoch": 0.10994967092528068,
      "grad_norm": 1.754900574684143,
      "learning_rate": 1.099496709252807e-06,
      "loss": 1.9661,
      "step": 284
    },
    {
      "epoch": 0.11033681765389082,
      "grad_norm": 1.5594189167022705,
      "learning_rate": 1.1033681765389083e-06,
      "loss": 1.9244,
      "step": 285
    },
    {
      "epoch": 0.11072396438250097,
      "grad_norm": 1.324192762374878,
      "learning_rate": 1.1072396438250098e-06,
      "loss": 1.9389,
      "step": 286
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.5862911939620972,
      "learning_rate": 1.111111111111111e-06,
      "loss": 1.9498,
      "step": 287
    },
    {
      "epoch": 0.11149825783972125,
      "grad_norm": 1.4328017234802246,
      "learning_rate": 1.1149825783972125e-06,
      "loss": 1.9242,
      "step": 288
    },
    {
      "epoch": 0.1118854045683314,
      "grad_norm": 1.3288697004318237,
      "learning_rate": 1.118854045683314e-06,
      "loss": 1.9258,
      "step": 289
    },
    {
      "epoch": 0.11227255129694154,
      "grad_norm": 1.9579726457595825,
      "learning_rate": 1.1227255129694155e-06,
      "loss": 1.9294,
      "step": 290
    },
    {
      "epoch": 0.11265969802555169,
      "grad_norm": 1.5679636001586914,
      "learning_rate": 1.126596980255517e-06,
      "loss": 1.9553,
      "step": 291
    },
    {
      "epoch": 0.11304684475416182,
      "grad_norm": 1.6586045026779175,
      "learning_rate": 1.1304684475416183e-06,
      "loss": 1.9197,
      "step": 292
    },
    {
      "epoch": 0.11343399148277197,
      "grad_norm": 1.574398398399353,
      "learning_rate": 1.1343399148277198e-06,
      "loss": 1.9067,
      "step": 293
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 1.2593274116516113,
      "learning_rate": 1.1382113821138213e-06,
      "loss": 1.9165,
      "step": 294
    },
    {
      "epoch": 0.11420828493999226,
      "grad_norm": 1.6787760257720947,
      "learning_rate": 1.1420828493999228e-06,
      "loss": 1.9367,
      "step": 295
    },
    {
      "epoch": 0.11459543166860241,
      "grad_norm": 1.301345705986023,
      "learning_rate": 1.1459543166860243e-06,
      "loss": 1.931,
      "step": 296
    },
    {
      "epoch": 0.11498257839721254,
      "grad_norm": 1.7856166362762451,
      "learning_rate": 1.1498257839721255e-06,
      "loss": 1.9596,
      "step": 297
    },
    {
      "epoch": 0.11536972512582269,
      "grad_norm": 2.10019588470459,
      "learning_rate": 1.153697251258227e-06,
      "loss": 1.9118,
      "step": 298
    },
    {
      "epoch": 0.11575687185443283,
      "grad_norm": 1.2561169862747192,
      "learning_rate": 1.1575687185443283e-06,
      "loss": 1.9156,
      "step": 299
    },
    {
      "epoch": 0.11614401858304298,
      "grad_norm": 1.9176281690597534,
      "learning_rate": 1.1614401858304298e-06,
      "loss": 2.0002,
      "step": 300
    },
    {
      "epoch": 0.11653116531165311,
      "grad_norm": 1.355483889579773,
      "learning_rate": 1.1653116531165313e-06,
      "loss": 1.9141,
      "step": 301
    },
    {
      "epoch": 0.11691831204026326,
      "grad_norm": 1.5007543563842773,
      "learning_rate": 1.1691831204026328e-06,
      "loss": 1.9266,
      "step": 302
    },
    {
      "epoch": 0.1173054587688734,
      "grad_norm": 1.4830043315887451,
      "learning_rate": 1.173054587688734e-06,
      "loss": 1.9098,
      "step": 303
    },
    {
      "epoch": 0.11769260549748355,
      "grad_norm": 1.8125677108764648,
      "learning_rate": 1.1769260549748356e-06,
      "loss": 1.9666,
      "step": 304
    },
    {
      "epoch": 0.1180797522260937,
      "grad_norm": 1.7718329429626465,
      "learning_rate": 1.180797522260937e-06,
      "loss": 1.9011,
      "step": 305
    },
    {
      "epoch": 0.11846689895470383,
      "grad_norm": 1.7778793573379517,
      "learning_rate": 1.1846689895470384e-06,
      "loss": 1.9549,
      "step": 306
    },
    {
      "epoch": 0.11885404568331398,
      "grad_norm": 1.3464010953903198,
      "learning_rate": 1.1885404568331398e-06,
      "loss": 1.9262,
      "step": 307
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 1.5550071001052856,
      "learning_rate": 1.1924119241192413e-06,
      "loss": 1.9432,
      "step": 308
    },
    {
      "epoch": 0.11962833914053426,
      "grad_norm": 1.3861435651779175,
      "learning_rate": 1.1962833914053428e-06,
      "loss": 1.9498,
      "step": 309
    },
    {
      "epoch": 0.1200154858691444,
      "grad_norm": 1.558093547821045,
      "learning_rate": 1.2001548586914441e-06,
      "loss": 1.9309,
      "step": 310
    },
    {
      "epoch": 0.12040263259775455,
      "grad_norm": 1.7558469772338867,
      "learning_rate": 1.2040263259775456e-06,
      "loss": 1.9673,
      "step": 311
    },
    {
      "epoch": 0.1207897793263647,
      "grad_norm": 1.3375784158706665,
      "learning_rate": 1.207897793263647e-06,
      "loss": 1.9214,
      "step": 312
    },
    {
      "epoch": 0.12117692605497483,
      "grad_norm": 1.4449307918548584,
      "learning_rate": 1.2117692605497484e-06,
      "loss": 1.9161,
      "step": 313
    },
    {
      "epoch": 0.12156407278358498,
      "grad_norm": 2.441638708114624,
      "learning_rate": 1.2156407278358499e-06,
      "loss": 1.9059,
      "step": 314
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 1.353731632232666,
      "learning_rate": 1.2195121951219514e-06,
      "loss": 1.9085,
      "step": 315
    },
    {
      "epoch": 0.12233836624080527,
      "grad_norm": 3.030303716659546,
      "learning_rate": 1.2233836624080529e-06,
      "loss": 1.994,
      "step": 316
    },
    {
      "epoch": 0.1227255129694154,
      "grad_norm": 1.4710289239883423,
      "learning_rate": 1.2272551296941541e-06,
      "loss": 1.9479,
      "step": 317
    },
    {
      "epoch": 0.12311265969802555,
      "grad_norm": 2.2766623497009277,
      "learning_rate": 1.2311265969802556e-06,
      "loss": 1.9691,
      "step": 318
    },
    {
      "epoch": 0.12349980642663569,
      "grad_norm": 1.7645599842071533,
      "learning_rate": 1.234998064266357e-06,
      "loss": 1.9268,
      "step": 319
    },
    {
      "epoch": 0.12388695315524584,
      "grad_norm": 1.4728820323944092,
      "learning_rate": 1.2388695315524584e-06,
      "loss": 1.9227,
      "step": 320
    },
    {
      "epoch": 0.12427409988385599,
      "grad_norm": 1.4844386577606201,
      "learning_rate": 1.24274099883856e-06,
      "loss": 1.9095,
      "step": 321
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 1.8163331747055054,
      "learning_rate": 1.2466124661246614e-06,
      "loss": 1.9141,
      "step": 322
    },
    {
      "epoch": 0.12504839334107626,
      "grad_norm": 1.5501534938812256,
      "learning_rate": 1.2504839334107627e-06,
      "loss": 1.9188,
      "step": 323
    },
    {
      "epoch": 0.1254355400696864,
      "grad_norm": 1.6929230690002441,
      "learning_rate": 1.2543554006968642e-06,
      "loss": 1.9111,
      "step": 324
    },
    {
      "epoch": 0.12582268679829656,
      "grad_norm": 1.8505001068115234,
      "learning_rate": 1.2582268679829657e-06,
      "loss": 1.9889,
      "step": 325
    },
    {
      "epoch": 0.1262098335269067,
      "grad_norm": 1.5359790325164795,
      "learning_rate": 1.2620983352690672e-06,
      "loss": 1.9704,
      "step": 326
    },
    {
      "epoch": 0.12659698025551683,
      "grad_norm": 2.2799971103668213,
      "learning_rate": 1.2659698025551684e-06,
      "loss": 1.9299,
      "step": 327
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 1.4610644578933716,
      "learning_rate": 1.26984126984127e-06,
      "loss": 1.9008,
      "step": 328
    },
    {
      "epoch": 0.12737127371273713,
      "grad_norm": 1.7973026037216187,
      "learning_rate": 1.2737127371273714e-06,
      "loss": 1.9837,
      "step": 329
    },
    {
      "epoch": 0.12775842044134728,
      "grad_norm": 1.6464828252792358,
      "learning_rate": 1.277584204413473e-06,
      "loss": 1.9364,
      "step": 330
    },
    {
      "epoch": 0.12814556716995743,
      "grad_norm": 1.63655686378479,
      "learning_rate": 1.2814556716995744e-06,
      "loss": 1.9066,
      "step": 331
    },
    {
      "epoch": 0.12853271389856755,
      "grad_norm": 1.439244270324707,
      "learning_rate": 1.2853271389856755e-06,
      "loss": 1.9623,
      "step": 332
    },
    {
      "epoch": 0.1289198606271777,
      "grad_norm": 1.2896493673324585,
      "learning_rate": 1.289198606271777e-06,
      "loss": 1.9274,
      "step": 333
    },
    {
      "epoch": 0.12930700735578785,
      "grad_norm": 1.627769947052002,
      "learning_rate": 1.2930700735578785e-06,
      "loss": 1.9523,
      "step": 334
    },
    {
      "epoch": 0.129694154084398,
      "grad_norm": 1.3762844800949097,
      "learning_rate": 1.29694154084398e-06,
      "loss": 1.9705,
      "step": 335
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 2.214961051940918,
      "learning_rate": 1.3008130081300815e-06,
      "loss": 1.9149,
      "step": 336
    },
    {
      "epoch": 0.13046844754161827,
      "grad_norm": 1.923017144203186,
      "learning_rate": 1.3046844754161827e-06,
      "loss": 1.9748,
      "step": 337
    },
    {
      "epoch": 0.13085559427022841,
      "grad_norm": 1.5595788955688477,
      "learning_rate": 1.3085559427022842e-06,
      "loss": 1.9196,
      "step": 338
    },
    {
      "epoch": 0.13124274099883856,
      "grad_norm": 1.6769872903823853,
      "learning_rate": 1.3124274099883857e-06,
      "loss": 1.9044,
      "step": 339
    },
    {
      "epoch": 0.1316298877274487,
      "grad_norm": 1.5886876583099365,
      "learning_rate": 1.3162988772744872e-06,
      "loss": 1.9429,
      "step": 340
    },
    {
      "epoch": 0.13201703445605883,
      "grad_norm": 1.8425449132919312,
      "learning_rate": 1.3201703445605885e-06,
      "loss": 1.9779,
      "step": 341
    },
    {
      "epoch": 0.13240418118466898,
      "grad_norm": 2.574467897415161,
      "learning_rate": 1.32404181184669e-06,
      "loss": 1.9092,
      "step": 342
    },
    {
      "epoch": 0.13279132791327913,
      "grad_norm": 1.966910719871521,
      "learning_rate": 1.3279132791327915e-06,
      "loss": 1.8938,
      "step": 343
    },
    {
      "epoch": 0.13317847464188928,
      "grad_norm": 1.4091365337371826,
      "learning_rate": 1.331784746418893e-06,
      "loss": 1.9338,
      "step": 344
    },
    {
      "epoch": 0.13356562137049943,
      "grad_norm": 1.697117805480957,
      "learning_rate": 1.3356562137049945e-06,
      "loss": 1.9069,
      "step": 345
    },
    {
      "epoch": 0.13395276809910955,
      "grad_norm": 1.5179247856140137,
      "learning_rate": 1.3395276809910955e-06,
      "loss": 1.9147,
      "step": 346
    },
    {
      "epoch": 0.1343399148277197,
      "grad_norm": 2.0698840618133545,
      "learning_rate": 1.343399148277197e-06,
      "loss": 1.9749,
      "step": 347
    },
    {
      "epoch": 0.13472706155632985,
      "grad_norm": 1.9285082817077637,
      "learning_rate": 1.3472706155632985e-06,
      "loss": 1.8833,
      "step": 348
    },
    {
      "epoch": 0.13511420828494,
      "grad_norm": 1.9182261228561401,
      "learning_rate": 1.3511420828494e-06,
      "loss": 1.8858,
      "step": 349
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 1.635923981666565,
      "learning_rate": 1.3550135501355013e-06,
      "loss": 1.9989,
      "step": 350
    },
    {
      "epoch": 0.13588850174216027,
      "grad_norm": 2.0719873905181885,
      "learning_rate": 1.3588850174216028e-06,
      "loss": 1.9787,
      "step": 351
    },
    {
      "epoch": 0.13627564847077042,
      "grad_norm": 1.3286869525909424,
      "learning_rate": 1.3627564847077043e-06,
      "loss": 1.9266,
      "step": 352
    },
    {
      "epoch": 0.13666279519938057,
      "grad_norm": 1.9464876651763916,
      "learning_rate": 1.3666279519938058e-06,
      "loss": 1.9644,
      "step": 353
    },
    {
      "epoch": 0.13704994192799072,
      "grad_norm": 1.4783718585968018,
      "learning_rate": 1.3704994192799073e-06,
      "loss": 1.9031,
      "step": 354
    },
    {
      "epoch": 0.13743708865660084,
      "grad_norm": 1.8775720596313477,
      "learning_rate": 1.3743708865660086e-06,
      "loss": 1.9766,
      "step": 355
    },
    {
      "epoch": 0.137824235385211,
      "grad_norm": 1.3642946481704712,
      "learning_rate": 1.37824235385211e-06,
      "loss": 1.9622,
      "step": 356
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 1.9857633113861084,
      "learning_rate": 1.3821138211382116e-06,
      "loss": 1.9673,
      "step": 357
    },
    {
      "epoch": 0.1385985288424313,
      "grad_norm": 2.044436454772949,
      "learning_rate": 1.385985288424313e-06,
      "loss": 1.9704,
      "step": 358
    },
    {
      "epoch": 0.1389856755710414,
      "grad_norm": 1.88657546043396,
      "learning_rate": 1.3898567557104143e-06,
      "loss": 1.9593,
      "step": 359
    },
    {
      "epoch": 0.13937282229965156,
      "grad_norm": 1.629560112953186,
      "learning_rate": 1.3937282229965158e-06,
      "loss": 1.9291,
      "step": 360
    },
    {
      "epoch": 0.1397599690282617,
      "grad_norm": 2.096426010131836,
      "learning_rate": 1.3975996902826173e-06,
      "loss": 1.8883,
      "step": 361
    },
    {
      "epoch": 0.14014711575687186,
      "grad_norm": 1.6367053985595703,
      "learning_rate": 1.4014711575687188e-06,
      "loss": 1.9506,
      "step": 362
    },
    {
      "epoch": 0.140534262485482,
      "grad_norm": 1.7556641101837158,
      "learning_rate": 1.4053426248548203e-06,
      "loss": 1.882,
      "step": 363
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 2.1869797706604004,
      "learning_rate": 1.4092140921409214e-06,
      "loss": 1.9229,
      "step": 364
    },
    {
      "epoch": 0.14130855594270228,
      "grad_norm": 1.9259355068206787,
      "learning_rate": 1.4130855594270229e-06,
      "loss": 1.8881,
      "step": 365
    },
    {
      "epoch": 0.14169570267131243,
      "grad_norm": 1.702422857284546,
      "learning_rate": 1.4169570267131244e-06,
      "loss": 1.8975,
      "step": 366
    },
    {
      "epoch": 0.14208284939992258,
      "grad_norm": 1.7902804613113403,
      "learning_rate": 1.4208284939992259e-06,
      "loss": 1.9469,
      "step": 367
    },
    {
      "epoch": 0.14246999612853273,
      "grad_norm": 1.5810487270355225,
      "learning_rate": 1.4246999612853273e-06,
      "loss": 1.9212,
      "step": 368
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.1189210414886475,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 1.9797,
      "step": 369
    },
    {
      "epoch": 0.143244289585753,
      "grad_norm": 1.8121310472488403,
      "learning_rate": 1.4324428958575301e-06,
      "loss": 1.9687,
      "step": 370
    },
    {
      "epoch": 0.14363143631436315,
      "grad_norm": 1.6179672479629517,
      "learning_rate": 1.4363143631436316e-06,
      "loss": 1.9583,
      "step": 371
    },
    {
      "epoch": 0.1440185830429733,
      "grad_norm": 1.5939662456512451,
      "learning_rate": 1.4401858304297331e-06,
      "loss": 1.8932,
      "step": 372
    },
    {
      "epoch": 0.14440572977158342,
      "grad_norm": 1.6596579551696777,
      "learning_rate": 1.4440572977158344e-06,
      "loss": 1.9112,
      "step": 373
    },
    {
      "epoch": 0.14479287650019357,
      "grad_norm": 2.0239057540893555,
      "learning_rate": 1.4479287650019359e-06,
      "loss": 1.9901,
      "step": 374
    },
    {
      "epoch": 0.14518002322880372,
      "grad_norm": 1.7644284963607788,
      "learning_rate": 1.4518002322880374e-06,
      "loss": 1.9437,
      "step": 375
    },
    {
      "epoch": 0.14556716995741387,
      "grad_norm": 1.8518065214157104,
      "learning_rate": 1.4556716995741389e-06,
      "loss": 1.9919,
      "step": 376
    },
    {
      "epoch": 0.14595431668602402,
      "grad_norm": 1.8030755519866943,
      "learning_rate": 1.4595431668602404e-06,
      "loss": 1.9337,
      "step": 377
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 1.7847497463226318,
      "learning_rate": 1.4634146341463414e-06,
      "loss": 1.958,
      "step": 378
    },
    {
      "epoch": 0.1467286101432443,
      "grad_norm": 1.6623131036758423,
      "learning_rate": 1.467286101432443e-06,
      "loss": 1.946,
      "step": 379
    },
    {
      "epoch": 0.14711575687185444,
      "grad_norm": 1.6354752779006958,
      "learning_rate": 1.4711575687185444e-06,
      "loss": 1.8925,
      "step": 380
    },
    {
      "epoch": 0.14750290360046459,
      "grad_norm": 2.0448474884033203,
      "learning_rate": 1.475029036004646e-06,
      "loss": 1.9933,
      "step": 381
    },
    {
      "epoch": 0.1478900503290747,
      "grad_norm": 1.1962838172912598,
      "learning_rate": 1.4789005032907472e-06,
      "loss": 1.9323,
      "step": 382
    },
    {
      "epoch": 0.14827719705768486,
      "grad_norm": 1.7342759370803833,
      "learning_rate": 1.4827719705768487e-06,
      "loss": 1.9459,
      "step": 383
    },
    {
      "epoch": 0.148664343786295,
      "grad_norm": 2.4976603984832764,
      "learning_rate": 1.4866434378629502e-06,
      "loss": 1.9026,
      "step": 384
    },
    {
      "epoch": 0.14905149051490515,
      "grad_norm": 2.0613508224487305,
      "learning_rate": 1.4905149051490517e-06,
      "loss": 1.9633,
      "step": 385
    },
    {
      "epoch": 0.1494386372435153,
      "grad_norm": 1.9321515560150146,
      "learning_rate": 1.4943863724351532e-06,
      "loss": 1.9258,
      "step": 386
    },
    {
      "epoch": 0.14982578397212543,
      "grad_norm": 1.7624300718307495,
      "learning_rate": 1.4982578397212545e-06,
      "loss": 1.9071,
      "step": 387
    },
    {
      "epoch": 0.15021293070073558,
      "grad_norm": 1.7151979207992554,
      "learning_rate": 1.502129307007356e-06,
      "loss": 1.9735,
      "step": 388
    },
    {
      "epoch": 0.15060007742934572,
      "grad_norm": 1.619200587272644,
      "learning_rate": 1.5060007742934574e-06,
      "loss": 1.8988,
      "step": 389
    },
    {
      "epoch": 0.15098722415795587,
      "grad_norm": 1.7529816627502441,
      "learning_rate": 1.509872241579559e-06,
      "loss": 1.8931,
      "step": 390
    },
    {
      "epoch": 0.151374370886566,
      "grad_norm": 1.6528289318084717,
      "learning_rate": 1.51374370886566e-06,
      "loss": 1.9409,
      "step": 391
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 1.6155864000320435,
      "learning_rate": 1.5176151761517615e-06,
      "loss": 1.9482,
      "step": 392
    },
    {
      "epoch": 0.1521486643437863,
      "grad_norm": 2.1943047046661377,
      "learning_rate": 1.521486643437863e-06,
      "loss": 1.9432,
      "step": 393
    },
    {
      "epoch": 0.15253581107239644,
      "grad_norm": 2.2073628902435303,
      "learning_rate": 1.5253581107239645e-06,
      "loss": 1.9736,
      "step": 394
    },
    {
      "epoch": 0.1529229578010066,
      "grad_norm": 2.9721994400024414,
      "learning_rate": 1.529229578010066e-06,
      "loss": 1.8918,
      "step": 395
    },
    {
      "epoch": 0.15331010452961671,
      "grad_norm": 2.8253636360168457,
      "learning_rate": 1.5331010452961673e-06,
      "loss": 1.9,
      "step": 396
    },
    {
      "epoch": 0.15369725125822686,
      "grad_norm": 1.879274606704712,
      "learning_rate": 1.5369725125822687e-06,
      "loss": 1.8827,
      "step": 397
    },
    {
      "epoch": 0.154084397986837,
      "grad_norm": 1.8416578769683838,
      "learning_rate": 1.5408439798683702e-06,
      "loss": 1.9564,
      "step": 398
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 2.139995813369751,
      "learning_rate": 1.5447154471544717e-06,
      "loss": 1.948,
      "step": 399
    },
    {
      "epoch": 0.1548586914440573,
      "grad_norm": 1.5404483079910278,
      "learning_rate": 1.5485869144405732e-06,
      "loss": 1.8985,
      "step": 400
    },
    {
      "epoch": 0.15524583817266743,
      "grad_norm": 1.5433573722839355,
      "learning_rate": 1.5524583817266745e-06,
      "loss": 1.887,
      "step": 401
    },
    {
      "epoch": 0.15563298490127758,
      "grad_norm": 1.6600062847137451,
      "learning_rate": 1.556329849012776e-06,
      "loss": 1.8934,
      "step": 402
    },
    {
      "epoch": 0.15602013162988773,
      "grad_norm": 1.8858287334442139,
      "learning_rate": 1.5602013162988775e-06,
      "loss": 1.9519,
      "step": 403
    },
    {
      "epoch": 0.15640727835849788,
      "grad_norm": 3.4226059913635254,
      "learning_rate": 1.564072783584979e-06,
      "loss": 2.0058,
      "step": 404
    },
    {
      "epoch": 0.156794425087108,
      "grad_norm": 1.5807850360870361,
      "learning_rate": 1.56794425087108e-06,
      "loss": 1.9268,
      "step": 405
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 1.7989859580993652,
      "learning_rate": 1.5718157181571816e-06,
      "loss": 1.9184,
      "step": 406
    },
    {
      "epoch": 0.1575687185443283,
      "grad_norm": 1.710532546043396,
      "learning_rate": 1.575687185443283e-06,
      "loss": 1.9577,
      "step": 407
    },
    {
      "epoch": 0.15795586527293845,
      "grad_norm": 2.220198154449463,
      "learning_rate": 1.5795586527293845e-06,
      "loss": 1.9777,
      "step": 408
    },
    {
      "epoch": 0.1583430120015486,
      "grad_norm": 1.4623024463653564,
      "learning_rate": 1.583430120015486e-06,
      "loss": 1.9544,
      "step": 409
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 1.648970365524292,
      "learning_rate": 1.5873015873015873e-06,
      "loss": 1.8919,
      "step": 410
    },
    {
      "epoch": 0.15911730545876887,
      "grad_norm": 3.689943552017212,
      "learning_rate": 1.5911730545876888e-06,
      "loss": 2.0226,
      "step": 411
    },
    {
      "epoch": 0.15950445218737902,
      "grad_norm": 1.7296627759933472,
      "learning_rate": 1.5950445218737903e-06,
      "loss": 1.9595,
      "step": 412
    },
    {
      "epoch": 0.15989159891598917,
      "grad_norm": 1.6155670881271362,
      "learning_rate": 1.5989159891598918e-06,
      "loss": 1.9146,
      "step": 413
    },
    {
      "epoch": 0.1602787456445993,
      "grad_norm": 1.8420119285583496,
      "learning_rate": 1.602787456445993e-06,
      "loss": 1.8759,
      "step": 414
    },
    {
      "epoch": 0.16066589237320944,
      "grad_norm": 2.0066492557525635,
      "learning_rate": 1.6066589237320946e-06,
      "loss": 1.9496,
      "step": 415
    },
    {
      "epoch": 0.1610530391018196,
      "grad_norm": 1.871625304222107,
      "learning_rate": 1.610530391018196e-06,
      "loss": 1.9185,
      "step": 416
    },
    {
      "epoch": 0.16144018583042974,
      "grad_norm": 1.4670833349227905,
      "learning_rate": 1.6144018583042976e-06,
      "loss": 1.9508,
      "step": 417
    },
    {
      "epoch": 0.1618273325590399,
      "grad_norm": 3.789684772491455,
      "learning_rate": 1.618273325590399e-06,
      "loss": 2.0372,
      "step": 418
    },
    {
      "epoch": 0.16221447928765,
      "grad_norm": 2.0267770290374756,
      "learning_rate": 1.6221447928765003e-06,
      "loss": 1.9542,
      "step": 419
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 2.1309025287628174,
      "learning_rate": 1.6260162601626018e-06,
      "loss": 1.93,
      "step": 420
    },
    {
      "epoch": 0.1629887727448703,
      "grad_norm": 1.7066789865493774,
      "learning_rate": 1.6298877274487033e-06,
      "loss": 1.9887,
      "step": 421
    },
    {
      "epoch": 0.16337591947348046,
      "grad_norm": 1.6124634742736816,
      "learning_rate": 1.6337591947348048e-06,
      "loss": 1.9384,
      "step": 422
    },
    {
      "epoch": 0.16376306620209058,
      "grad_norm": 2.008396863937378,
      "learning_rate": 1.6376306620209059e-06,
      "loss": 1.9237,
      "step": 423
    },
    {
      "epoch": 0.16415021293070073,
      "grad_norm": 3.0317580699920654,
      "learning_rate": 1.6415021293070074e-06,
      "loss": 1.9999,
      "step": 424
    },
    {
      "epoch": 0.16453735965931088,
      "grad_norm": 1.720217227935791,
      "learning_rate": 1.6453735965931089e-06,
      "loss": 1.8905,
      "step": 425
    },
    {
      "epoch": 0.16492450638792103,
      "grad_norm": 1.7010760307312012,
      "learning_rate": 1.6492450638792104e-06,
      "loss": 1.8893,
      "step": 426
    },
    {
      "epoch": 0.16531165311653118,
      "grad_norm": 1.8502633571624756,
      "learning_rate": 1.6531165311653119e-06,
      "loss": 1.8761,
      "step": 427
    },
    {
      "epoch": 0.1656987998451413,
      "grad_norm": 1.9518791437149048,
      "learning_rate": 1.6569879984514131e-06,
      "loss": 1.9272,
      "step": 428
    },
    {
      "epoch": 0.16608594657375145,
      "grad_norm": 1.5597469806671143,
      "learning_rate": 1.6608594657375146e-06,
      "loss": 1.9212,
      "step": 429
    },
    {
      "epoch": 0.1664730933023616,
      "grad_norm": 1.6594756841659546,
      "learning_rate": 1.6647309330236161e-06,
      "loss": 1.9082,
      "step": 430
    },
    {
      "epoch": 0.16686024003097175,
      "grad_norm": 2.568570137023926,
      "learning_rate": 1.6686024003097176e-06,
      "loss": 1.9329,
      "step": 431
    },
    {
      "epoch": 0.1672473867595819,
      "grad_norm": 1.8203858137130737,
      "learning_rate": 1.6724738675958191e-06,
      "loss": 1.977,
      "step": 432
    },
    {
      "epoch": 0.16763453348819202,
      "grad_norm": 1.8662112951278687,
      "learning_rate": 1.6763453348819204e-06,
      "loss": 1.9073,
      "step": 433
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 2.7789127826690674,
      "learning_rate": 1.6802168021680219e-06,
      "loss": 1.9687,
      "step": 434
    },
    {
      "epoch": 0.16840882694541232,
      "grad_norm": 1.8848450183868408,
      "learning_rate": 1.6840882694541234e-06,
      "loss": 1.9468,
      "step": 435
    },
    {
      "epoch": 0.16879597367402246,
      "grad_norm": 1.9737122058868408,
      "learning_rate": 1.6879597367402249e-06,
      "loss": 1.9407,
      "step": 436
    },
    {
      "epoch": 0.1691831204026326,
      "grad_norm": 1.8369290828704834,
      "learning_rate": 1.691831204026326e-06,
      "loss": 1.8792,
      "step": 437
    },
    {
      "epoch": 0.16957026713124274,
      "grad_norm": 2.665700912475586,
      "learning_rate": 1.6957026713124274e-06,
      "loss": 1.898,
      "step": 438
    },
    {
      "epoch": 0.16995741385985288,
      "grad_norm": 2.0381131172180176,
      "learning_rate": 1.699574138598529e-06,
      "loss": 1.9852,
      "step": 439
    },
    {
      "epoch": 0.17034456058846303,
      "grad_norm": 2.0582683086395264,
      "learning_rate": 1.7034456058846304e-06,
      "loss": 1.8832,
      "step": 440
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 2.215968370437622,
      "learning_rate": 1.707317073170732e-06,
      "loss": 1.9686,
      "step": 441
    },
    {
      "epoch": 0.1711188540456833,
      "grad_norm": 2.136892318725586,
      "learning_rate": 1.7111885404568332e-06,
      "loss": 1.9417,
      "step": 442
    },
    {
      "epoch": 0.17150600077429345,
      "grad_norm": 1.8776793479919434,
      "learning_rate": 1.7150600077429347e-06,
      "loss": 1.8849,
      "step": 443
    },
    {
      "epoch": 0.1718931475029036,
      "grad_norm": 2.1167948246002197,
      "learning_rate": 1.7189314750290362e-06,
      "loss": 1.9597,
      "step": 444
    },
    {
      "epoch": 0.17228029423151375,
      "grad_norm": 1.548703908920288,
      "learning_rate": 1.7228029423151377e-06,
      "loss": 1.9166,
      "step": 445
    },
    {
      "epoch": 0.17266744096012387,
      "grad_norm": 1.735309362411499,
      "learning_rate": 1.726674409601239e-06,
      "loss": 1.9208,
      "step": 446
    },
    {
      "epoch": 0.17305458768873402,
      "grad_norm": 2.2364118099212646,
      "learning_rate": 1.7305458768873405e-06,
      "loss": 1.9475,
      "step": 447
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 1.9087903499603271,
      "learning_rate": 1.734417344173442e-06,
      "loss": 1.9559,
      "step": 448
    },
    {
      "epoch": 0.17382888114595432,
      "grad_norm": 1.683915376663208,
      "learning_rate": 1.7382888114595434e-06,
      "loss": 1.8875,
      "step": 449
    },
    {
      "epoch": 0.17421602787456447,
      "grad_norm": 1.8590646982192993,
      "learning_rate": 1.742160278745645e-06,
      "loss": 1.9165,
      "step": 450
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 1.775416374206543,
      "learning_rate": 1.746031746031746e-06,
      "loss": 1.9376,
      "step": 451
    },
    {
      "epoch": 0.17499032133178474,
      "grad_norm": 1.5585452318191528,
      "learning_rate": 1.7499032133178475e-06,
      "loss": 1.899,
      "step": 452
    },
    {
      "epoch": 0.1753774680603949,
      "grad_norm": 1.9003162384033203,
      "learning_rate": 1.753774680603949e-06,
      "loss": 1.9676,
      "step": 453
    },
    {
      "epoch": 0.17576461478900504,
      "grad_norm": 2.2428629398345947,
      "learning_rate": 1.7576461478900505e-06,
      "loss": 1.9091,
      "step": 454
    },
    {
      "epoch": 0.17615176151761516,
      "grad_norm": 2.1124215126037598,
      "learning_rate": 1.7615176151761518e-06,
      "loss": 1.9249,
      "step": 455
    },
    {
      "epoch": 0.1765389082462253,
      "grad_norm": 2.023526430130005,
      "learning_rate": 1.7653890824622533e-06,
      "loss": 1.9182,
      "step": 456
    },
    {
      "epoch": 0.17692605497483546,
      "grad_norm": 2.263871192932129,
      "learning_rate": 1.7692605497483548e-06,
      "loss": 1.8959,
      "step": 457
    },
    {
      "epoch": 0.1773132017034456,
      "grad_norm": 1.727168083190918,
      "learning_rate": 1.7731320170344562e-06,
      "loss": 1.9249,
      "step": 458
    },
    {
      "epoch": 0.17770034843205576,
      "grad_norm": 2.3711769580841064,
      "learning_rate": 1.7770034843205577e-06,
      "loss": 1.89,
      "step": 459
    },
    {
      "epoch": 0.17808749516066588,
      "grad_norm": 1.6670210361480713,
      "learning_rate": 1.780874951606659e-06,
      "loss": 1.915,
      "step": 460
    },
    {
      "epoch": 0.17847464188927603,
      "grad_norm": 3.0449299812316895,
      "learning_rate": 1.7847464188927605e-06,
      "loss": 1.8581,
      "step": 461
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 2.191378116607666,
      "learning_rate": 1.788617886178862e-06,
      "loss": 1.8564,
      "step": 462
    },
    {
      "epoch": 0.17924893534649633,
      "grad_norm": 1.9549640417099,
      "learning_rate": 1.7924893534649635e-06,
      "loss": 1.9251,
      "step": 463
    },
    {
      "epoch": 0.17963608207510648,
      "grad_norm": 2.2771153450012207,
      "learning_rate": 1.796360820751065e-06,
      "loss": 1.9291,
      "step": 464
    },
    {
      "epoch": 0.1800232288037166,
      "grad_norm": 1.8234041929244995,
      "learning_rate": 1.800232288037166e-06,
      "loss": 1.8824,
      "step": 465
    },
    {
      "epoch": 0.18041037553232675,
      "grad_norm": 2.7391409873962402,
      "learning_rate": 1.8041037553232676e-06,
      "loss": 1.9647,
      "step": 466
    },
    {
      "epoch": 0.1807975222609369,
      "grad_norm": 2.333094358444214,
      "learning_rate": 1.807975222609369e-06,
      "loss": 1.9167,
      "step": 467
    },
    {
      "epoch": 0.18118466898954705,
      "grad_norm": 1.7818106412887573,
      "learning_rate": 1.8118466898954705e-06,
      "loss": 1.9099,
      "step": 468
    },
    {
      "epoch": 0.18157181571815717,
      "grad_norm": 2.2323849201202393,
      "learning_rate": 1.8157181571815718e-06,
      "loss": 1.9488,
      "step": 469
    },
    {
      "epoch": 0.18195896244676732,
      "grad_norm": 2.204543352127075,
      "learning_rate": 1.8195896244676733e-06,
      "loss": 1.9422,
      "step": 470
    },
    {
      "epoch": 0.18234610917537747,
      "grad_norm": 3.3988256454467773,
      "learning_rate": 1.8234610917537748e-06,
      "loss": 1.8778,
      "step": 471
    },
    {
      "epoch": 0.18273325590398762,
      "grad_norm": 1.9499329328536987,
      "learning_rate": 1.8273325590398763e-06,
      "loss": 1.8762,
      "step": 472
    },
    {
      "epoch": 0.18312040263259777,
      "grad_norm": 2.0696651935577393,
      "learning_rate": 1.8312040263259778e-06,
      "loss": 1.8734,
      "step": 473
    },
    {
      "epoch": 0.1835075493612079,
      "grad_norm": 1.943961501121521,
      "learning_rate": 1.835075493612079e-06,
      "loss": 1.8755,
      "step": 474
    },
    {
      "epoch": 0.18389469608981804,
      "grad_norm": 2.222642660140991,
      "learning_rate": 1.8389469608981806e-06,
      "loss": 1.8614,
      "step": 475
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 1.7302738428115845,
      "learning_rate": 1.842818428184282e-06,
      "loss": 1.95,
      "step": 476
    },
    {
      "epoch": 0.18466898954703834,
      "grad_norm": 2.9227631092071533,
      "learning_rate": 1.8466898954703836e-06,
      "loss": 1.8966,
      "step": 477
    },
    {
      "epoch": 0.18505613627564846,
      "grad_norm": 2.488569498062134,
      "learning_rate": 1.8505613627564846e-06,
      "loss": 1.8509,
      "step": 478
    },
    {
      "epoch": 0.1854432830042586,
      "grad_norm": 2.4713664054870605,
      "learning_rate": 1.8544328300425861e-06,
      "loss": 1.8599,
      "step": 479
    },
    {
      "epoch": 0.18583042973286876,
      "grad_norm": 2.2355926036834717,
      "learning_rate": 1.8583042973286876e-06,
      "loss": 1.8724,
      "step": 480
    },
    {
      "epoch": 0.1862175764614789,
      "grad_norm": 1.877783179283142,
      "learning_rate": 1.8621757646147891e-06,
      "loss": 1.9127,
      "step": 481
    },
    {
      "epoch": 0.18660472319008906,
      "grad_norm": 1.7124749422073364,
      "learning_rate": 1.8660472319008906e-06,
      "loss": 1.9015,
      "step": 482
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 2.6785898208618164,
      "learning_rate": 1.8699186991869919e-06,
      "loss": 1.9304,
      "step": 483
    },
    {
      "epoch": 0.18737901664730933,
      "grad_norm": 2.280778169631958,
      "learning_rate": 1.8737901664730934e-06,
      "loss": 1.9159,
      "step": 484
    },
    {
      "epoch": 0.18776616337591948,
      "grad_norm": 1.917722463607788,
      "learning_rate": 1.8776616337591949e-06,
      "loss": 1.9146,
      "step": 485
    },
    {
      "epoch": 0.18815331010452963,
      "grad_norm": 1.9566518068313599,
      "learning_rate": 1.8815331010452964e-06,
      "loss": 1.9506,
      "step": 486
    },
    {
      "epoch": 0.18854045683313975,
      "grad_norm": 2.3477766513824463,
      "learning_rate": 1.8854045683313977e-06,
      "loss": 1.9187,
      "step": 487
    },
    {
      "epoch": 0.1889276035617499,
      "grad_norm": 2.4909584522247314,
      "learning_rate": 1.8892760356174991e-06,
      "loss": 1.9382,
      "step": 488
    },
    {
      "epoch": 0.18931475029036005,
      "grad_norm": 2.0873820781707764,
      "learning_rate": 1.8931475029036006e-06,
      "loss": 1.9359,
      "step": 489
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 2.571516990661621,
      "learning_rate": 1.8970189701897021e-06,
      "loss": 1.9586,
      "step": 490
    },
    {
      "epoch": 0.19008904374758034,
      "grad_norm": 2.38154935836792,
      "learning_rate": 1.9008904374758036e-06,
      "loss": 1.947,
      "step": 491
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 2.5822691917419434,
      "learning_rate": 1.904761904761905e-06,
      "loss": 1.8536,
      "step": 492
    },
    {
      "epoch": 0.19086333720480061,
      "grad_norm": 1.8056399822235107,
      "learning_rate": 1.908633372048006e-06,
      "loss": 1.9063,
      "step": 493
    },
    {
      "epoch": 0.19125048393341076,
      "grad_norm": 3.5948235988616943,
      "learning_rate": 1.9125048393341077e-06,
      "loss": 1.967,
      "step": 494
    },
    {
      "epoch": 0.1916376306620209,
      "grad_norm": 2.4224820137023926,
      "learning_rate": 1.916376306620209e-06,
      "loss": 1.9639,
      "step": 495
    },
    {
      "epoch": 0.19202477739063106,
      "grad_norm": 2.342782974243164,
      "learning_rate": 1.9202477739063107e-06,
      "loss": 1.9245,
      "step": 496
    },
    {
      "epoch": 0.19241192411924118,
      "grad_norm": 3.6940810680389404,
      "learning_rate": 1.924119241192412e-06,
      "loss": 2.0253,
      "step": 497
    },
    {
      "epoch": 0.19279907084785133,
      "grad_norm": 3.257767915725708,
      "learning_rate": 1.9279907084785137e-06,
      "loss": 1.8736,
      "step": 498
    },
    {
      "epoch": 0.19318621757646148,
      "grad_norm": 3.9524102210998535,
      "learning_rate": 1.931862175764615e-06,
      "loss": 1.9619,
      "step": 499
    },
    {
      "epoch": 0.19357336430507163,
      "grad_norm": 3.4401321411132812,
      "learning_rate": 1.9357336430507166e-06,
      "loss": 2.0274,
      "step": 500
    },
    {
      "epoch": 0.19396051103368175,
      "grad_norm": 2.4816038608551025,
      "learning_rate": 1.9396051103368177e-06,
      "loss": 1.8443,
      "step": 501
    },
    {
      "epoch": 0.1943476577622919,
      "grad_norm": 3.461838960647583,
      "learning_rate": 1.943476577622919e-06,
      "loss": 1.8317,
      "step": 502
    },
    {
      "epoch": 0.19473480449090205,
      "grad_norm": 2.773894786834717,
      "learning_rate": 1.9473480449090207e-06,
      "loss": 1.9651,
      "step": 503
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 2.1346523761749268,
      "learning_rate": 1.951219512195122e-06,
      "loss": 1.948,
      "step": 504
    },
    {
      "epoch": 0.19550909794812235,
      "grad_norm": 2.639450788497925,
      "learning_rate": 1.9550909794812237e-06,
      "loss": 1.955,
      "step": 505
    },
    {
      "epoch": 0.19589624467673247,
      "grad_norm": 2.8137636184692383,
      "learning_rate": 1.9589624467673248e-06,
      "loss": 1.9477,
      "step": 506
    },
    {
      "epoch": 0.19628339140534262,
      "grad_norm": 2.0274181365966797,
      "learning_rate": 1.9628339140534263e-06,
      "loss": 1.8955,
      "step": 507
    },
    {
      "epoch": 0.19667053813395277,
      "grad_norm": 2.113105058670044,
      "learning_rate": 1.9667053813395277e-06,
      "loss": 1.8924,
      "step": 508
    },
    {
      "epoch": 0.19705768486256292,
      "grad_norm": 2.6175010204315186,
      "learning_rate": 1.9705768486256292e-06,
      "loss": 1.9068,
      "step": 509
    },
    {
      "epoch": 0.19744483159117304,
      "grad_norm": 2.2982449531555176,
      "learning_rate": 1.9744483159117307e-06,
      "loss": 1.9035,
      "step": 510
    },
    {
      "epoch": 0.1978319783197832,
      "grad_norm": 2.0805115699768066,
      "learning_rate": 1.9783197831978322e-06,
      "loss": 1.9404,
      "step": 511
    },
    {
      "epoch": 0.19821912504839334,
      "grad_norm": 2.186197519302368,
      "learning_rate": 1.9821912504839337e-06,
      "loss": 1.8626,
      "step": 512
    },
    {
      "epoch": 0.1986062717770035,
      "grad_norm": 2.702214002609253,
      "learning_rate": 1.986062717770035e-06,
      "loss": 1.8421,
      "step": 513
    },
    {
      "epoch": 0.19899341850561364,
      "grad_norm": 2.377042293548584,
      "learning_rate": 1.9899341850561367e-06,
      "loss": 1.9021,
      "step": 514
    },
    {
      "epoch": 0.19938056523422376,
      "grad_norm": 2.3623898029327393,
      "learning_rate": 1.9938056523422378e-06,
      "loss": 1.8568,
      "step": 515
    },
    {
      "epoch": 0.1997677119628339,
      "grad_norm": 2.3873507976531982,
      "learning_rate": 1.9976771196283393e-06,
      "loss": 1.9689,
      "step": 516
    },
    {
      "epoch": 0.20015485869144406,
      "grad_norm": 2.4235477447509766,
      "learning_rate": 2.0015485869144408e-06,
      "loss": 1.9833,
      "step": 517
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 2.501016139984131,
      "learning_rate": 2.0054200542005423e-06,
      "loss": 1.9063,
      "step": 518
    },
    {
      "epoch": 0.20092915214866433,
      "grad_norm": 2.4915363788604736,
      "learning_rate": 2.0092915214866433e-06,
      "loss": 1.8524,
      "step": 519
    },
    {
      "epoch": 0.20131629887727448,
      "grad_norm": 2.2108585834503174,
      "learning_rate": 2.013162988772745e-06,
      "loss": 1.9087,
      "step": 520
    },
    {
      "epoch": 0.20170344560588463,
      "grad_norm": 2.323448896408081,
      "learning_rate": 2.0170344560588463e-06,
      "loss": 1.9437,
      "step": 521
    },
    {
      "epoch": 0.20209059233449478,
      "grad_norm": 2.4101457595825195,
      "learning_rate": 2.020905923344948e-06,
      "loss": 1.8875,
      "step": 522
    },
    {
      "epoch": 0.20247773906310493,
      "grad_norm": 2.8812882900238037,
      "learning_rate": 2.0247773906310493e-06,
      "loss": 2.0077,
      "step": 523
    },
    {
      "epoch": 0.20286488579171505,
      "grad_norm": 2.8530945777893066,
      "learning_rate": 2.028648857917151e-06,
      "loss": 1.882,
      "step": 524
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 3.3358843326568604,
      "learning_rate": 2.0325203252032523e-06,
      "loss": 1.8882,
      "step": 525
    },
    {
      "epoch": 0.20363917924893535,
      "grad_norm": 2.7170238494873047,
      "learning_rate": 2.0363917924893538e-06,
      "loss": 1.8855,
      "step": 526
    },
    {
      "epoch": 0.2040263259775455,
      "grad_norm": 3.7087786197662354,
      "learning_rate": 2.0402632597754553e-06,
      "loss": 1.882,
      "step": 527
    },
    {
      "epoch": 0.20441347270615565,
      "grad_norm": 2.993332862854004,
      "learning_rate": 2.0441347270615568e-06,
      "loss": 1.9938,
      "step": 528
    },
    {
      "epoch": 0.20480061943476577,
      "grad_norm": 2.101775884628296,
      "learning_rate": 2.048006194347658e-06,
      "loss": 1.921,
      "step": 529
    },
    {
      "epoch": 0.20518776616337592,
      "grad_norm": 2.6562321186065674,
      "learning_rate": 2.0518776616337593e-06,
      "loss": 1.8606,
      "step": 530
    },
    {
      "epoch": 0.20557491289198607,
      "grad_norm": 2.7955009937286377,
      "learning_rate": 2.055749128919861e-06,
      "loss": 1.8412,
      "step": 531
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 3.4236927032470703,
      "learning_rate": 2.0596205962059623e-06,
      "loss": 1.83,
      "step": 532
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 3.0495593547821045,
      "learning_rate": 2.0634920634920634e-06,
      "loss": 2.0067,
      "step": 533
    },
    {
      "epoch": 0.2067363530778165,
      "grad_norm": 2.8688130378723145,
      "learning_rate": 2.067363530778165e-06,
      "loss": 1.9912,
      "step": 534
    },
    {
      "epoch": 0.20712349980642664,
      "grad_norm": 2.4124221801757812,
      "learning_rate": 2.0712349980642664e-06,
      "loss": 1.8411,
      "step": 535
    },
    {
      "epoch": 0.20751064653503679,
      "grad_norm": 3.1382622718811035,
      "learning_rate": 2.075106465350368e-06,
      "loss": 1.9271,
      "step": 536
    },
    {
      "epoch": 0.20789779326364694,
      "grad_norm": 2.927093505859375,
      "learning_rate": 2.0789779326364694e-06,
      "loss": 1.9559,
      "step": 537
    },
    {
      "epoch": 0.20828493999225706,
      "grad_norm": 2.526332139968872,
      "learning_rate": 2.082849399922571e-06,
      "loss": 1.902,
      "step": 538
    },
    {
      "epoch": 0.2086720867208672,
      "grad_norm": 2.562246084213257,
      "learning_rate": 2.0867208672086723e-06,
      "loss": 1.9005,
      "step": 539
    },
    {
      "epoch": 0.20905923344947736,
      "grad_norm": 4.358397483825684,
      "learning_rate": 2.090592334494774e-06,
      "loss": 1.9562,
      "step": 540
    },
    {
      "epoch": 0.2094463801780875,
      "grad_norm": 3.723266839981079,
      "learning_rate": 2.0944638017808753e-06,
      "loss": 1.8856,
      "step": 541
    },
    {
      "epoch": 0.20983352690669763,
      "grad_norm": 3.3886048793792725,
      "learning_rate": 2.0983352690669764e-06,
      "loss": 2.002,
      "step": 542
    },
    {
      "epoch": 0.21022067363530778,
      "grad_norm": 2.269824743270874,
      "learning_rate": 2.102206736353078e-06,
      "loss": 1.9524,
      "step": 543
    },
    {
      "epoch": 0.21060782036391792,
      "grad_norm": 2.7479076385498047,
      "learning_rate": 2.1060782036391794e-06,
      "loss": 1.8612,
      "step": 544
    },
    {
      "epoch": 0.21099496709252807,
      "grad_norm": 2.758011817932129,
      "learning_rate": 2.109949670925281e-06,
      "loss": 1.8876,
      "step": 545
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 2.6552813053131104,
      "learning_rate": 2.1138211382113824e-06,
      "loss": 1.974,
      "step": 546
    },
    {
      "epoch": 0.21176926054974834,
      "grad_norm": 3.232708692550659,
      "learning_rate": 2.1176926054974834e-06,
      "loss": 1.9336,
      "step": 547
    },
    {
      "epoch": 0.2121564072783585,
      "grad_norm": 2.7354931831359863,
      "learning_rate": 2.121564072783585e-06,
      "loss": 1.9484,
      "step": 548
    },
    {
      "epoch": 0.21254355400696864,
      "grad_norm": 2.8682477474212646,
      "learning_rate": 2.1254355400696864e-06,
      "loss": 1.9574,
      "step": 549
    },
    {
      "epoch": 0.2129307007355788,
      "grad_norm": 2.6743104457855225,
      "learning_rate": 2.129307007355788e-06,
      "loss": 1.9425,
      "step": 550
    },
    {
      "epoch": 0.21331784746418891,
      "grad_norm": 2.7308857440948486,
      "learning_rate": 2.1331784746418894e-06,
      "loss": 1.9423,
      "step": 551
    },
    {
      "epoch": 0.21370499419279906,
      "grad_norm": 4.331692218780518,
      "learning_rate": 2.137049941927991e-06,
      "loss": 1.8594,
      "step": 552
    },
    {
      "epoch": 0.2140921409214092,
      "grad_norm": 2.5042083263397217,
      "learning_rate": 2.1409214092140924e-06,
      "loss": 1.9768,
      "step": 553
    },
    {
      "epoch": 0.21447928765001936,
      "grad_norm": 2.335200071334839,
      "learning_rate": 2.144792876500194e-06,
      "loss": 1.8498,
      "step": 554
    },
    {
      "epoch": 0.2148664343786295,
      "grad_norm": 2.799895763397217,
      "learning_rate": 2.1486643437862954e-06,
      "loss": 1.8374,
      "step": 555
    },
    {
      "epoch": 0.21525358110723963,
      "grad_norm": 2.5952205657958984,
      "learning_rate": 2.1525358110723965e-06,
      "loss": 1.8815,
      "step": 556
    },
    {
      "epoch": 0.21564072783584978,
      "grad_norm": 3.546342134475708,
      "learning_rate": 2.156407278358498e-06,
      "loss": 1.9167,
      "step": 557
    },
    {
      "epoch": 0.21602787456445993,
      "grad_norm": 2.8184549808502197,
      "learning_rate": 2.1602787456445995e-06,
      "loss": 1.8496,
      "step": 558
    },
    {
      "epoch": 0.21641502129307008,
      "grad_norm": 2.996811866760254,
      "learning_rate": 2.164150212930701e-06,
      "loss": 1.8741,
      "step": 559
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 2.5355756282806396,
      "learning_rate": 2.1680216802168024e-06,
      "loss": 1.8463,
      "step": 560
    },
    {
      "epoch": 0.21718931475029035,
      "grad_norm": 2.502490282058716,
      "learning_rate": 2.1718931475029035e-06,
      "loss": 1.9196,
      "step": 561
    },
    {
      "epoch": 0.2175764614789005,
      "grad_norm": 2.6291375160217285,
      "learning_rate": 2.175764614789005e-06,
      "loss": 1.8721,
      "step": 562
    },
    {
      "epoch": 0.21796360820751065,
      "grad_norm": 1.9296364784240723,
      "learning_rate": 2.1796360820751065e-06,
      "loss": 1.9146,
      "step": 563
    },
    {
      "epoch": 0.2183507549361208,
      "grad_norm": 2.586912155151367,
      "learning_rate": 2.183507549361208e-06,
      "loss": 1.8551,
      "step": 564
    },
    {
      "epoch": 0.21873790166473092,
      "grad_norm": 3.1770145893096924,
      "learning_rate": 2.1873790166473095e-06,
      "loss": 1.9061,
      "step": 565
    },
    {
      "epoch": 0.21912504839334107,
      "grad_norm": 2.5562517642974854,
      "learning_rate": 2.191250483933411e-06,
      "loss": 1.9023,
      "step": 566
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 3.6946306228637695,
      "learning_rate": 2.1951219512195125e-06,
      "loss": 1.8845,
      "step": 567
    },
    {
      "epoch": 0.21989934185056137,
      "grad_norm": 2.6644959449768066,
      "learning_rate": 2.198993418505614e-06,
      "loss": 1.8888,
      "step": 568
    },
    {
      "epoch": 0.22028648857917152,
      "grad_norm": 3.0481698513031006,
      "learning_rate": 2.2028648857917155e-06,
      "loss": 1.9299,
      "step": 569
    },
    {
      "epoch": 0.22067363530778164,
      "grad_norm": 3.1044862270355225,
      "learning_rate": 2.2067363530778165e-06,
      "loss": 1.9954,
      "step": 570
    },
    {
      "epoch": 0.2210607820363918,
      "grad_norm": 2.7377381324768066,
      "learning_rate": 2.210607820363918e-06,
      "loss": 1.9922,
      "step": 571
    },
    {
      "epoch": 0.22144792876500194,
      "grad_norm": 3.2715463638305664,
      "learning_rate": 2.2144792876500195e-06,
      "loss": 1.8587,
      "step": 572
    },
    {
      "epoch": 0.2218350754936121,
      "grad_norm": 2.9851653575897217,
      "learning_rate": 2.218350754936121e-06,
      "loss": 1.873,
      "step": 573
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 3.0437581539154053,
      "learning_rate": 2.222222222222222e-06,
      "loss": 1.9325,
      "step": 574
    },
    {
      "epoch": 0.22260936895083236,
      "grad_norm": 3.1831600666046143,
      "learning_rate": 2.2260936895083236e-06,
      "loss": 1.8842,
      "step": 575
    },
    {
      "epoch": 0.2229965156794425,
      "grad_norm": 2.4917285442352295,
      "learning_rate": 2.229965156794425e-06,
      "loss": 1.8764,
      "step": 576
    },
    {
      "epoch": 0.22338366240805266,
      "grad_norm": 3.124706745147705,
      "learning_rate": 2.2338366240805266e-06,
      "loss": 1.8457,
      "step": 577
    },
    {
      "epoch": 0.2237708091366628,
      "grad_norm": 2.5710504055023193,
      "learning_rate": 2.237708091366628e-06,
      "loss": 1.867,
      "step": 578
    },
    {
      "epoch": 0.22415795586527293,
      "grad_norm": 3.237506628036499,
      "learning_rate": 2.2415795586527295e-06,
      "loss": 2.0212,
      "step": 579
    },
    {
      "epoch": 0.22454510259388308,
      "grad_norm": 3.0819568634033203,
      "learning_rate": 2.245451025938831e-06,
      "loss": 1.9654,
      "step": 580
    },
    {
      "epoch": 0.22493224932249323,
      "grad_norm": 3.028522253036499,
      "learning_rate": 2.2493224932249325e-06,
      "loss": 1.9505,
      "step": 581
    },
    {
      "epoch": 0.22531939605110338,
      "grad_norm": 3.6128528118133545,
      "learning_rate": 2.253193960511034e-06,
      "loss": 1.8228,
      "step": 582
    },
    {
      "epoch": 0.2257065427797135,
      "grad_norm": 3.3251895904541016,
      "learning_rate": 2.257065427797135e-06,
      "loss": 1.8851,
      "step": 583
    },
    {
      "epoch": 0.22609368950832365,
      "grad_norm": 2.2347934246063232,
      "learning_rate": 2.2609368950832366e-06,
      "loss": 1.9214,
      "step": 584
    },
    {
      "epoch": 0.2264808362369338,
      "grad_norm": 4.391674518585205,
      "learning_rate": 2.264808362369338e-06,
      "loss": 1.9317,
      "step": 585
    },
    {
      "epoch": 0.22686798296554395,
      "grad_norm": 3.3873775005340576,
      "learning_rate": 2.2686798296554396e-06,
      "loss": 1.8221,
      "step": 586
    },
    {
      "epoch": 0.2272551296941541,
      "grad_norm": 3.1601979732513428,
      "learning_rate": 2.272551296941541e-06,
      "loss": 1.8219,
      "step": 587
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 3.4796934127807617,
      "learning_rate": 2.2764227642276426e-06,
      "loss": 1.87,
      "step": 588
    },
    {
      "epoch": 0.22802942315137437,
      "grad_norm": 2.8448221683502197,
      "learning_rate": 2.280294231513744e-06,
      "loss": 1.969,
      "step": 589
    },
    {
      "epoch": 0.22841656987998452,
      "grad_norm": 3.1195778846740723,
      "learning_rate": 2.2841656987998455e-06,
      "loss": 1.9873,
      "step": 590
    },
    {
      "epoch": 0.22880371660859466,
      "grad_norm": 5.096436023712158,
      "learning_rate": 2.288037166085947e-06,
      "loss": 1.9543,
      "step": 591
    },
    {
      "epoch": 0.22919086333720481,
      "grad_norm": 2.8748507499694824,
      "learning_rate": 2.2919086333720485e-06,
      "loss": 1.9509,
      "step": 592
    },
    {
      "epoch": 0.22957801006581494,
      "grad_norm": 2.957690477371216,
      "learning_rate": 2.2957801006581496e-06,
      "loss": 1.8318,
      "step": 593
    },
    {
      "epoch": 0.22996515679442509,
      "grad_norm": 3.318359613418579,
      "learning_rate": 2.299651567944251e-06,
      "loss": 1.906,
      "step": 594
    },
    {
      "epoch": 0.23035230352303523,
      "grad_norm": 3.4155304431915283,
      "learning_rate": 2.3035230352303526e-06,
      "loss": 1.9993,
      "step": 595
    },
    {
      "epoch": 0.23073945025164538,
      "grad_norm": 3.2739977836608887,
      "learning_rate": 2.307394502516454e-06,
      "loss": 1.9274,
      "step": 596
    },
    {
      "epoch": 0.2311265969802555,
      "grad_norm": 3.0603384971618652,
      "learning_rate": 2.311265969802555e-06,
      "loss": 1.8668,
      "step": 597
    },
    {
      "epoch": 0.23151374370886565,
      "grad_norm": 3.2655792236328125,
      "learning_rate": 2.3151374370886566e-06,
      "loss": 1.997,
      "step": 598
    },
    {
      "epoch": 0.2319008904374758,
      "grad_norm": 2.7068700790405273,
      "learning_rate": 2.319008904374758e-06,
      "loss": 1.9316,
      "step": 599
    },
    {
      "epoch": 0.23228803716608595,
      "grad_norm": 2.8865816593170166,
      "learning_rate": 2.3228803716608596e-06,
      "loss": 1.8957,
      "step": 600
    },
    {
      "epoch": 0.2326751838946961,
      "grad_norm": 2.599088668823242,
      "learning_rate": 2.326751838946961e-06,
      "loss": 1.9323,
      "step": 601
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 3.4828672409057617,
      "learning_rate": 2.3306233062330626e-06,
      "loss": 1.9538,
      "step": 602
    },
    {
      "epoch": 0.23344947735191637,
      "grad_norm": 4.671082973480225,
      "learning_rate": 2.334494773519164e-06,
      "loss": 1.8551,
      "step": 603
    },
    {
      "epoch": 0.23383662408052652,
      "grad_norm": 3.0255954265594482,
      "learning_rate": 2.3383662408052656e-06,
      "loss": 1.892,
      "step": 604
    },
    {
      "epoch": 0.23422377080913667,
      "grad_norm": 3.1279845237731934,
      "learning_rate": 2.342237708091367e-06,
      "loss": 1.8258,
      "step": 605
    },
    {
      "epoch": 0.2346109175377468,
      "grad_norm": 3.1772384643554688,
      "learning_rate": 2.346109175377468e-06,
      "loss": 1.8204,
      "step": 606
    },
    {
      "epoch": 0.23499806426635694,
      "grad_norm": 3.6955058574676514,
      "learning_rate": 2.3499806426635697e-06,
      "loss": 1.9051,
      "step": 607
    },
    {
      "epoch": 0.2353852109949671,
      "grad_norm": 3.1887357234954834,
      "learning_rate": 2.353852109949671e-06,
      "loss": 1.8205,
      "step": 608
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 3.2619283199310303,
      "learning_rate": 2.3577235772357727e-06,
      "loss": 2.001,
      "step": 609
    },
    {
      "epoch": 0.2361595044521874,
      "grad_norm": 3.414287805557251,
      "learning_rate": 2.361595044521874e-06,
      "loss": 1.8904,
      "step": 610
    },
    {
      "epoch": 0.2365466511807975,
      "grad_norm": 3.365271806716919,
      "learning_rate": 2.3654665118079752e-06,
      "loss": 1.9885,
      "step": 611
    },
    {
      "epoch": 0.23693379790940766,
      "grad_norm": 2.9154138565063477,
      "learning_rate": 2.3693379790940767e-06,
      "loss": 1.8776,
      "step": 612
    },
    {
      "epoch": 0.2373209446380178,
      "grad_norm": 3.950515031814575,
      "learning_rate": 2.373209446380178e-06,
      "loss": 1.8804,
      "step": 613
    },
    {
      "epoch": 0.23770809136662796,
      "grad_norm": 3.3670907020568848,
      "learning_rate": 2.3770809136662797e-06,
      "loss": 1.896,
      "step": 614
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 3.382739543914795,
      "learning_rate": 2.380952380952381e-06,
      "loss": 1.8724,
      "step": 615
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 3.3065719604492188,
      "learning_rate": 2.3848238482384827e-06,
      "loss": 1.9434,
      "step": 616
    },
    {
      "epoch": 0.23886953155245838,
      "grad_norm": 3.561591148376465,
      "learning_rate": 2.388695315524584e-06,
      "loss": 1.8742,
      "step": 617
    },
    {
      "epoch": 0.23925667828106853,
      "grad_norm": 2.970308780670166,
      "learning_rate": 2.3925667828106857e-06,
      "loss": 1.9409,
      "step": 618
    },
    {
      "epoch": 0.23964382500967868,
      "grad_norm": 2.8836848735809326,
      "learning_rate": 2.396438250096787e-06,
      "loss": 1.8374,
      "step": 619
    },
    {
      "epoch": 0.2400309717382888,
      "grad_norm": 3.968465566635132,
      "learning_rate": 2.4003097173828882e-06,
      "loss": 1.8975,
      "step": 620
    },
    {
      "epoch": 0.24041811846689895,
      "grad_norm": 3.4351775646209717,
      "learning_rate": 2.4041811846689897e-06,
      "loss": 1.922,
      "step": 621
    },
    {
      "epoch": 0.2408052651955091,
      "grad_norm": 3.2049365043640137,
      "learning_rate": 2.4080526519550912e-06,
      "loss": 1.8614,
      "step": 622
    },
    {
      "epoch": 0.24119241192411925,
      "grad_norm": 3.821133852005005,
      "learning_rate": 2.4119241192411927e-06,
      "loss": 1.9453,
      "step": 623
    },
    {
      "epoch": 0.2415795586527294,
      "grad_norm": 3.1494576930999756,
      "learning_rate": 2.415795586527294e-06,
      "loss": 1.9489,
      "step": 624
    },
    {
      "epoch": 0.24196670538133952,
      "grad_norm": 5.582937240600586,
      "learning_rate": 2.4196670538133953e-06,
      "loss": 1.9362,
      "step": 625
    },
    {
      "epoch": 0.24235385210994967,
      "grad_norm": 3.1615118980407715,
      "learning_rate": 2.4235385210994968e-06,
      "loss": 1.9177,
      "step": 626
    },
    {
      "epoch": 0.24274099883855982,
      "grad_norm": 4.917486667633057,
      "learning_rate": 2.4274099883855983e-06,
      "loss": 1.9665,
      "step": 627
    },
    {
      "epoch": 0.24312814556716997,
      "grad_norm": 2.617466688156128,
      "learning_rate": 2.4312814556716998e-06,
      "loss": 1.9039,
      "step": 628
    },
    {
      "epoch": 0.2435152922957801,
      "grad_norm": 3.5294570922851562,
      "learning_rate": 2.4351529229578012e-06,
      "loss": 1.7998,
      "step": 629
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 3.3786113262176514,
      "learning_rate": 2.4390243902439027e-06,
      "loss": 1.8826,
      "step": 630
    },
    {
      "epoch": 0.2442895857530004,
      "grad_norm": 3.3518545627593994,
      "learning_rate": 2.4428958575300042e-06,
      "loss": 1.9907,
      "step": 631
    },
    {
      "epoch": 0.24467673248161054,
      "grad_norm": 3.9130935668945312,
      "learning_rate": 2.4467673248161057e-06,
      "loss": 1.9671,
      "step": 632
    },
    {
      "epoch": 0.2450638792102207,
      "grad_norm": 2.4000244140625,
      "learning_rate": 2.4506387921022072e-06,
      "loss": 1.9032,
      "step": 633
    },
    {
      "epoch": 0.2454510259388308,
      "grad_norm": 3.740856885910034,
      "learning_rate": 2.4545102593883083e-06,
      "loss": 1.9897,
      "step": 634
    },
    {
      "epoch": 0.24583817266744096,
      "grad_norm": 5.551656723022461,
      "learning_rate": 2.4583817266744098e-06,
      "loss": 2.0685,
      "step": 635
    },
    {
      "epoch": 0.2462253193960511,
      "grad_norm": 5.3994293212890625,
      "learning_rate": 2.4622531939605113e-06,
      "loss": 2.0431,
      "step": 636
    },
    {
      "epoch": 0.24661246612466126,
      "grad_norm": 3.578625202178955,
      "learning_rate": 2.4661246612466128e-06,
      "loss": 1.9106,
      "step": 637
    },
    {
      "epoch": 0.24699961285327138,
      "grad_norm": 3.429713010787964,
      "learning_rate": 2.469996128532714e-06,
      "loss": 2.009,
      "step": 638
    },
    {
      "epoch": 0.24738675958188153,
      "grad_norm": 3.784666061401367,
      "learning_rate": 2.4738675958188153e-06,
      "loss": 1.7985,
      "step": 639
    },
    {
      "epoch": 0.24777390631049168,
      "grad_norm": 3.3645927906036377,
      "learning_rate": 2.477739063104917e-06,
      "loss": 1.9087,
      "step": 640
    },
    {
      "epoch": 0.24816105303910183,
      "grad_norm": 3.9199585914611816,
      "learning_rate": 2.4816105303910183e-06,
      "loss": 1.8845,
      "step": 641
    },
    {
      "epoch": 0.24854819976771197,
      "grad_norm": 6.136801719665527,
      "learning_rate": 2.48548199767712e-06,
      "loss": 1.7943,
      "step": 642
    },
    {
      "epoch": 0.2489353464963221,
      "grad_norm": 3.523496389389038,
      "learning_rate": 2.4893534649632213e-06,
      "loss": 1.9407,
      "step": 643
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 3.8842272758483887,
      "learning_rate": 2.493224932249323e-06,
      "loss": 1.8769,
      "step": 644
    },
    {
      "epoch": 0.2497096399535424,
      "grad_norm": 4.059936046600342,
      "learning_rate": 2.4970963995354243e-06,
      "loss": 1.861,
      "step": 645
    },
    {
      "epoch": 0.2500967866821525,
      "grad_norm": 3.360710859298706,
      "learning_rate": 2.5009678668215254e-06,
      "loss": 1.8916,
      "step": 646
    },
    {
      "epoch": 0.2504839334107627,
      "grad_norm": 3.1754322052001953,
      "learning_rate": 2.5048393341076273e-06,
      "loss": 1.8353,
      "step": 647
    },
    {
      "epoch": 0.2508710801393728,
      "grad_norm": 3.7455263137817383,
      "learning_rate": 2.5087108013937284e-06,
      "loss": 1.8163,
      "step": 648
    },
    {
      "epoch": 0.251258226867983,
      "grad_norm": 3.599885940551758,
      "learning_rate": 2.5125822686798303e-06,
      "loss": 1.8072,
      "step": 649
    },
    {
      "epoch": 0.2516453735965931,
      "grad_norm": 4.261420726776123,
      "learning_rate": 2.5164537359659313e-06,
      "loss": 1.8518,
      "step": 650
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 3.8999245166778564,
      "learning_rate": 2.5203252032520324e-06,
      "loss": 1.9896,
      "step": 651
    },
    {
      "epoch": 0.2524196670538134,
      "grad_norm": 3.0409655570983887,
      "learning_rate": 2.5241966705381343e-06,
      "loss": 1.8783,
      "step": 652
    },
    {
      "epoch": 0.25280681378242353,
      "grad_norm": 3.44474720954895,
      "learning_rate": 2.5280681378242354e-06,
      "loss": 1.9036,
      "step": 653
    },
    {
      "epoch": 0.25319396051103366,
      "grad_norm": 3.290738582611084,
      "learning_rate": 2.531939605110337e-06,
      "loss": 1.8864,
      "step": 654
    },
    {
      "epoch": 0.25358110723964383,
      "grad_norm": 3.4059650897979736,
      "learning_rate": 2.5358110723964384e-06,
      "loss": 1.933,
      "step": 655
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 6.91644811630249,
      "learning_rate": 2.53968253968254e-06,
      "loss": 2.1221,
      "step": 656
    },
    {
      "epoch": 0.25435540069686413,
      "grad_norm": 4.318762302398682,
      "learning_rate": 2.5435540069686414e-06,
      "loss": 1.8058,
      "step": 657
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 6.063329696655273,
      "learning_rate": 2.547425474254743e-06,
      "loss": 1.8173,
      "step": 658
    },
    {
      "epoch": 0.2551296941540844,
      "grad_norm": 4.607699871063232,
      "learning_rate": 2.551296941540844e-06,
      "loss": 1.8468,
      "step": 659
    },
    {
      "epoch": 0.25551684088269455,
      "grad_norm": 4.768526077270508,
      "learning_rate": 2.555168408826946e-06,
      "loss": 1.9442,
      "step": 660
    },
    {
      "epoch": 0.2559039876113047,
      "grad_norm": 4.362678050994873,
      "learning_rate": 2.559039876113047e-06,
      "loss": 2.0183,
      "step": 661
    },
    {
      "epoch": 0.25629113433991485,
      "grad_norm": 6.192969799041748,
      "learning_rate": 2.562911343399149e-06,
      "loss": 1.7825,
      "step": 662
    },
    {
      "epoch": 0.25667828106852497,
      "grad_norm": 6.307344913482666,
      "learning_rate": 2.56678281068525e-06,
      "loss": 1.7833,
      "step": 663
    },
    {
      "epoch": 0.2570654277971351,
      "grad_norm": 4.242782115936279,
      "learning_rate": 2.570654277971351e-06,
      "loss": 1.8815,
      "step": 664
    },
    {
      "epoch": 0.25745257452574527,
      "grad_norm": 3.866044521331787,
      "learning_rate": 2.574525745257453e-06,
      "loss": 1.9273,
      "step": 665
    },
    {
      "epoch": 0.2578397212543554,
      "grad_norm": 4.032533168792725,
      "learning_rate": 2.578397212543554e-06,
      "loss": 2.0077,
      "step": 666
    },
    {
      "epoch": 0.25822686798296557,
      "grad_norm": 4.543755054473877,
      "learning_rate": 2.582268679829656e-06,
      "loss": 1.7515,
      "step": 667
    },
    {
      "epoch": 0.2586140147115757,
      "grad_norm": 4.582633972167969,
      "learning_rate": 2.586140147115757e-06,
      "loss": 1.9885,
      "step": 668
    },
    {
      "epoch": 0.2590011614401858,
      "grad_norm": 3.1893250942230225,
      "learning_rate": 2.5900116144018584e-06,
      "loss": 1.8189,
      "step": 669
    },
    {
      "epoch": 0.259388308168796,
      "grad_norm": 4.60728120803833,
      "learning_rate": 2.59388308168796e-06,
      "loss": 1.8458,
      "step": 670
    },
    {
      "epoch": 0.2597754548974061,
      "grad_norm": 4.773448944091797,
      "learning_rate": 2.5977545489740614e-06,
      "loss": 1.8498,
      "step": 671
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 4.272185325622559,
      "learning_rate": 2.601626016260163e-06,
      "loss": 1.7841,
      "step": 672
    },
    {
      "epoch": 0.2605497483546264,
      "grad_norm": 4.759449005126953,
      "learning_rate": 2.6054974835462644e-06,
      "loss": 1.7508,
      "step": 673
    },
    {
      "epoch": 0.26093689508323653,
      "grad_norm": 5.055673599243164,
      "learning_rate": 2.6093689508323655e-06,
      "loss": 2.0055,
      "step": 674
    },
    {
      "epoch": 0.2613240418118467,
      "grad_norm": 6.391668319702148,
      "learning_rate": 2.6132404181184674e-06,
      "loss": 1.7855,
      "step": 675
    },
    {
      "epoch": 0.26171118854045683,
      "grad_norm": 4.756046772003174,
      "learning_rate": 2.6171118854045685e-06,
      "loss": 1.7472,
      "step": 676
    },
    {
      "epoch": 0.26209833526906695,
      "grad_norm": 5.495229244232178,
      "learning_rate": 2.6209833526906695e-06,
      "loss": 2.0584,
      "step": 677
    },
    {
      "epoch": 0.26248548199767713,
      "grad_norm": 5.0100860595703125,
      "learning_rate": 2.6248548199767715e-06,
      "loss": 1.8883,
      "step": 678
    },
    {
      "epoch": 0.26287262872628725,
      "grad_norm": 5.162625312805176,
      "learning_rate": 2.6287262872628725e-06,
      "loss": 1.9899,
      "step": 679
    },
    {
      "epoch": 0.2632597754548974,
      "grad_norm": 3.8375680446624756,
      "learning_rate": 2.6325977545489744e-06,
      "loss": 1.8749,
      "step": 680
    },
    {
      "epoch": 0.26364692218350755,
      "grad_norm": 3.909369707107544,
      "learning_rate": 2.6364692218350755e-06,
      "loss": 1.9139,
      "step": 681
    },
    {
      "epoch": 0.26403406891211767,
      "grad_norm": 5.063611030578613,
      "learning_rate": 2.640340689121177e-06,
      "loss": 2.0069,
      "step": 682
    },
    {
      "epoch": 0.26442121564072785,
      "grad_norm": 4.642796993255615,
      "learning_rate": 2.6442121564072785e-06,
      "loss": 2.0197,
      "step": 683
    },
    {
      "epoch": 0.26480836236933797,
      "grad_norm": 6.2610697746276855,
      "learning_rate": 2.64808362369338e-06,
      "loss": 2.0715,
      "step": 684
    },
    {
      "epoch": 0.26519550909794815,
      "grad_norm": 5.662660598754883,
      "learning_rate": 2.6519550909794815e-06,
      "loss": 1.9479,
      "step": 685
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 4.154530048370361,
      "learning_rate": 2.655826558265583e-06,
      "loss": 1.9376,
      "step": 686
    },
    {
      "epoch": 0.2659698025551684,
      "grad_norm": 4.834386348724365,
      "learning_rate": 2.659698025551684e-06,
      "loss": 1.8541,
      "step": 687
    },
    {
      "epoch": 0.26635694928377857,
      "grad_norm": 3.6495752334594727,
      "learning_rate": 2.663569492837786e-06,
      "loss": 1.896,
      "step": 688
    },
    {
      "epoch": 0.2667440960123887,
      "grad_norm": 4.754018783569336,
      "learning_rate": 2.667440960123887e-06,
      "loss": 2.0248,
      "step": 689
    },
    {
      "epoch": 0.26713124274099886,
      "grad_norm": 4.64987850189209,
      "learning_rate": 2.671312427409989e-06,
      "loss": 1.9435,
      "step": 690
    },
    {
      "epoch": 0.267518389469609,
      "grad_norm": 7.222677230834961,
      "learning_rate": 2.67518389469609e-06,
      "loss": 1.8265,
      "step": 691
    },
    {
      "epoch": 0.2679055361982191,
      "grad_norm": 4.4423933029174805,
      "learning_rate": 2.679055361982191e-06,
      "loss": 2.0162,
      "step": 692
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 7.022895812988281,
      "learning_rate": 2.682926829268293e-06,
      "loss": 1.7694,
      "step": 693
    },
    {
      "epoch": 0.2686798296554394,
      "grad_norm": 4.531471252441406,
      "learning_rate": 2.686798296554394e-06,
      "loss": 1.7688,
      "step": 694
    },
    {
      "epoch": 0.2690669763840495,
      "grad_norm": 5.1545209884643555,
      "learning_rate": 2.6906697638404956e-06,
      "loss": 1.8752,
      "step": 695
    },
    {
      "epoch": 0.2694541231126597,
      "grad_norm": 5.3327412605285645,
      "learning_rate": 2.694541231126597e-06,
      "loss": 2.0328,
      "step": 696
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 4.146062850952148,
      "learning_rate": 2.6984126984126986e-06,
      "loss": 1.9438,
      "step": 697
    },
    {
      "epoch": 0.27022841656988,
      "grad_norm": 4.124353408813477,
      "learning_rate": 2.7022841656988e-06,
      "loss": 1.7708,
      "step": 698
    },
    {
      "epoch": 0.2706155632984901,
      "grad_norm": 4.504242420196533,
      "learning_rate": 2.7061556329849016e-06,
      "loss": 1.9064,
      "step": 699
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 7.229456424713135,
      "learning_rate": 2.7100271002710026e-06,
      "loss": 1.7583,
      "step": 700
    },
    {
      "epoch": 0.2713898567557104,
      "grad_norm": 4.642282485961914,
      "learning_rate": 2.7138985675571045e-06,
      "loss": 1.8314,
      "step": 701
    },
    {
      "epoch": 0.27177700348432055,
      "grad_norm": 5.135335445404053,
      "learning_rate": 2.7177700348432056e-06,
      "loss": 1.9238,
      "step": 702
    },
    {
      "epoch": 0.2721641502129307,
      "grad_norm": 2.8204410076141357,
      "learning_rate": 2.7216415021293075e-06,
      "loss": 1.8897,
      "step": 703
    },
    {
      "epoch": 0.27255129694154084,
      "grad_norm": 6.136299133300781,
      "learning_rate": 2.7255129694154086e-06,
      "loss": 1.9242,
      "step": 704
    },
    {
      "epoch": 0.27293844367015097,
      "grad_norm": 7.137617111206055,
      "learning_rate": 2.7293844367015097e-06,
      "loss": 1.787,
      "step": 705
    },
    {
      "epoch": 0.27332559039876114,
      "grad_norm": 3.5939252376556396,
      "learning_rate": 2.7332559039876116e-06,
      "loss": 1.8043,
      "step": 706
    },
    {
      "epoch": 0.27371273712737126,
      "grad_norm": 4.38499116897583,
      "learning_rate": 2.7371273712737127e-06,
      "loss": 1.858,
      "step": 707
    },
    {
      "epoch": 0.27409988385598144,
      "grad_norm": 4.360669136047363,
      "learning_rate": 2.7409988385598146e-06,
      "loss": 1.8495,
      "step": 708
    },
    {
      "epoch": 0.27448703058459156,
      "grad_norm": 4.928162097930908,
      "learning_rate": 2.7448703058459156e-06,
      "loss": 1.7899,
      "step": 709
    },
    {
      "epoch": 0.2748741773132017,
      "grad_norm": 5.019421577453613,
      "learning_rate": 2.748741773132017e-06,
      "loss": 1.852,
      "step": 710
    },
    {
      "epoch": 0.27526132404181186,
      "grad_norm": 4.517184257507324,
      "learning_rate": 2.7526132404181186e-06,
      "loss": 1.9816,
      "step": 711
    },
    {
      "epoch": 0.275648470770422,
      "grad_norm": 4.655425071716309,
      "learning_rate": 2.75648470770422e-06,
      "loss": 1.7883,
      "step": 712
    },
    {
      "epoch": 0.27603561749903216,
      "grad_norm": 5.299063682556152,
      "learning_rate": 2.7603561749903216e-06,
      "loss": 1.9563,
      "step": 713
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 3.487900495529175,
      "learning_rate": 2.764227642276423e-06,
      "loss": 1.8027,
      "step": 714
    },
    {
      "epoch": 0.2768099109562524,
      "grad_norm": 4.364028453826904,
      "learning_rate": 2.768099109562524e-06,
      "loss": 1.9442,
      "step": 715
    },
    {
      "epoch": 0.2771970576848626,
      "grad_norm": 5.812967300415039,
      "learning_rate": 2.771970576848626e-06,
      "loss": 1.8083,
      "step": 716
    },
    {
      "epoch": 0.2775842044134727,
      "grad_norm": 4.044814109802246,
      "learning_rate": 2.775842044134727e-06,
      "loss": 1.9015,
      "step": 717
    },
    {
      "epoch": 0.2779713511420828,
      "grad_norm": 3.7721736431121826,
      "learning_rate": 2.7797135114208287e-06,
      "loss": 1.8481,
      "step": 718
    },
    {
      "epoch": 0.278358497870693,
      "grad_norm": 4.931928634643555,
      "learning_rate": 2.78358497870693e-06,
      "loss": 1.7782,
      "step": 719
    },
    {
      "epoch": 0.2787456445993031,
      "grad_norm": 5.657633304595947,
      "learning_rate": 2.7874564459930316e-06,
      "loss": 1.8797,
      "step": 720
    },
    {
      "epoch": 0.2791327913279133,
      "grad_norm": 5.048424243927002,
      "learning_rate": 2.791327913279133e-06,
      "loss": 1.725,
      "step": 721
    },
    {
      "epoch": 0.2795199380565234,
      "grad_norm": 6.106997966766357,
      "learning_rate": 2.7951993805652346e-06,
      "loss": 1.8718,
      "step": 722
    },
    {
      "epoch": 0.27990708478513354,
      "grad_norm": 7.286074161529541,
      "learning_rate": 2.7990708478513357e-06,
      "loss": 1.7517,
      "step": 723
    },
    {
      "epoch": 0.2802942315137437,
      "grad_norm": 5.362709045410156,
      "learning_rate": 2.8029423151374376e-06,
      "loss": 1.7559,
      "step": 724
    },
    {
      "epoch": 0.28068137824235384,
      "grad_norm": 4.095361709594727,
      "learning_rate": 2.8068137824235387e-06,
      "loss": 1.8369,
      "step": 725
    },
    {
      "epoch": 0.281068524970964,
      "grad_norm": 4.322763919830322,
      "learning_rate": 2.8106852497096406e-06,
      "loss": 1.882,
      "step": 726
    },
    {
      "epoch": 0.28145567169957414,
      "grad_norm": 6.495409965515137,
      "learning_rate": 2.8145567169957417e-06,
      "loss": 2.022,
      "step": 727
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 6.4468183517456055,
      "learning_rate": 2.8184281842818427e-06,
      "loss": 2.1031,
      "step": 728
    },
    {
      "epoch": 0.28222996515679444,
      "grad_norm": 4.456118583679199,
      "learning_rate": 2.8222996515679447e-06,
      "loss": 1.9985,
      "step": 729
    },
    {
      "epoch": 0.28261711188540456,
      "grad_norm": 6.888878345489502,
      "learning_rate": 2.8261711188540457e-06,
      "loss": 1.9374,
      "step": 730
    },
    {
      "epoch": 0.28300425861401474,
      "grad_norm": 7.9537153244018555,
      "learning_rate": 2.8300425861401476e-06,
      "loss": 1.8008,
      "step": 731
    },
    {
      "epoch": 0.28339140534262486,
      "grad_norm": 5.529542446136475,
      "learning_rate": 2.8339140534262487e-06,
      "loss": 1.9633,
      "step": 732
    },
    {
      "epoch": 0.283778552071235,
      "grad_norm": 6.911525249481201,
      "learning_rate": 2.8377855207123502e-06,
      "loss": 1.7761,
      "step": 733
    },
    {
      "epoch": 0.28416569879984516,
      "grad_norm": 5.577544212341309,
      "learning_rate": 2.8416569879984517e-06,
      "loss": 1.884,
      "step": 734
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 5.366798400878906,
      "learning_rate": 2.845528455284553e-06,
      "loss": 2.029,
      "step": 735
    },
    {
      "epoch": 0.28493999225706546,
      "grad_norm": 9.168179512023926,
      "learning_rate": 2.8493999225706547e-06,
      "loss": 1.9563,
      "step": 736
    },
    {
      "epoch": 0.2853271389856756,
      "grad_norm": 4.902073383331299,
      "learning_rate": 2.853271389856756e-06,
      "loss": 1.984,
      "step": 737
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.704491138458252,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.7238,
      "step": 738
    },
    {
      "epoch": 0.2861014324428959,
      "grad_norm": 3.4613540172576904,
      "learning_rate": 2.861014324428959e-06,
      "loss": 1.9359,
      "step": 739
    },
    {
      "epoch": 0.286488579171506,
      "grad_norm": 5.451022148132324,
      "learning_rate": 2.8648857917150602e-06,
      "loss": 1.9373,
      "step": 740
    },
    {
      "epoch": 0.2868757259001161,
      "grad_norm": 5.204198837280273,
      "learning_rate": 2.8687572590011613e-06,
      "loss": 1.7742,
      "step": 741
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 5.681777000427246,
      "learning_rate": 2.8726287262872632e-06,
      "loss": 1.8316,
      "step": 742
    },
    {
      "epoch": 0.2876500193573364,
      "grad_norm": 4.054718494415283,
      "learning_rate": 2.8765001935733643e-06,
      "loss": 1.8907,
      "step": 743
    },
    {
      "epoch": 0.2880371660859466,
      "grad_norm": 5.634524822235107,
      "learning_rate": 2.8803716608594662e-06,
      "loss": 1.7384,
      "step": 744
    },
    {
      "epoch": 0.2884243128145567,
      "grad_norm": 7.47828483581543,
      "learning_rate": 2.8842431281455673e-06,
      "loss": 1.7447,
      "step": 745
    },
    {
      "epoch": 0.28881145954316684,
      "grad_norm": 4.801217079162598,
      "learning_rate": 2.8881145954316688e-06,
      "loss": 1.8459,
      "step": 746
    },
    {
      "epoch": 0.289198606271777,
      "grad_norm": 4.754260540008545,
      "learning_rate": 2.8919860627177703e-06,
      "loss": 1.8811,
      "step": 747
    },
    {
      "epoch": 0.28958575300038714,
      "grad_norm": 4.147353649139404,
      "learning_rate": 2.8958575300038718e-06,
      "loss": 1.875,
      "step": 748
    },
    {
      "epoch": 0.2899728997289973,
      "grad_norm": 4.6981000900268555,
      "learning_rate": 2.8997289972899733e-06,
      "loss": 1.8323,
      "step": 749
    },
    {
      "epoch": 0.29036004645760743,
      "grad_norm": 5.474301815032959,
      "learning_rate": 2.9036004645760748e-06,
      "loss": 1.8334,
      "step": 750
    },
    {
      "epoch": 0.29074719318621756,
      "grad_norm": 5.662237167358398,
      "learning_rate": 2.907471931862176e-06,
      "loss": 1.8387,
      "step": 751
    },
    {
      "epoch": 0.29113433991482773,
      "grad_norm": 4.950139999389648,
      "learning_rate": 2.9113433991482777e-06,
      "loss": 1.8388,
      "step": 752
    },
    {
      "epoch": 0.29152148664343785,
      "grad_norm": 5.912869930267334,
      "learning_rate": 2.915214866434379e-06,
      "loss": 2.0337,
      "step": 753
    },
    {
      "epoch": 0.29190863337204803,
      "grad_norm": 5.617253303527832,
      "learning_rate": 2.9190863337204807e-06,
      "loss": 2.0504,
      "step": 754
    },
    {
      "epoch": 0.29229578010065815,
      "grad_norm": 6.655825614929199,
      "learning_rate": 2.922957801006582e-06,
      "loss": 2.0543,
      "step": 755
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 4.767932415008545,
      "learning_rate": 2.926829268292683e-06,
      "loss": 1.8532,
      "step": 756
    },
    {
      "epoch": 0.29307007355787845,
      "grad_norm": 4.485767364501953,
      "learning_rate": 2.9307007355787848e-06,
      "loss": 1.798,
      "step": 757
    },
    {
      "epoch": 0.2934572202864886,
      "grad_norm": 4.872833251953125,
      "learning_rate": 2.934572202864886e-06,
      "loss": 1.8822,
      "step": 758
    },
    {
      "epoch": 0.2938443670150987,
      "grad_norm": 5.726745128631592,
      "learning_rate": 2.9384436701509873e-06,
      "loss": 1.8422,
      "step": 759
    },
    {
      "epoch": 0.2942315137437089,
      "grad_norm": 5.475102424621582,
      "learning_rate": 2.942315137437089e-06,
      "loss": 1.8736,
      "step": 760
    },
    {
      "epoch": 0.294618660472319,
      "grad_norm": 4.796927452087402,
      "learning_rate": 2.9461866047231903e-06,
      "loss": 1.8309,
      "step": 761
    },
    {
      "epoch": 0.29500580720092917,
      "grad_norm": 4.532254695892334,
      "learning_rate": 2.950058072009292e-06,
      "loss": 1.8391,
      "step": 762
    },
    {
      "epoch": 0.2953929539295393,
      "grad_norm": 4.085954189300537,
      "learning_rate": 2.9539295392953933e-06,
      "loss": 1.8699,
      "step": 763
    },
    {
      "epoch": 0.2957801006581494,
      "grad_norm": 5.225602626800537,
      "learning_rate": 2.9578010065814944e-06,
      "loss": 1.8301,
      "step": 764
    },
    {
      "epoch": 0.2961672473867596,
      "grad_norm": 5.530776500701904,
      "learning_rate": 2.9616724738675963e-06,
      "loss": 1.8712,
      "step": 765
    },
    {
      "epoch": 0.2965543941153697,
      "grad_norm": 4.6287841796875,
      "learning_rate": 2.9655439411536974e-06,
      "loss": 2.0258,
      "step": 766
    },
    {
      "epoch": 0.2969415408439799,
      "grad_norm": 6.171422004699707,
      "learning_rate": 2.9694154084397993e-06,
      "loss": 2.031,
      "step": 767
    },
    {
      "epoch": 0.29732868757259,
      "grad_norm": 5.255777359008789,
      "learning_rate": 2.9732868757259004e-06,
      "loss": 1.8541,
      "step": 768
    },
    {
      "epoch": 0.29771583430120013,
      "grad_norm": 4.348309516906738,
      "learning_rate": 2.9771583430120014e-06,
      "loss": 1.9124,
      "step": 769
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 5.602612495422363,
      "learning_rate": 2.9810298102981034e-06,
      "loss": 1.8359,
      "step": 770
    },
    {
      "epoch": 0.29849012775842043,
      "grad_norm": 4.787204265594482,
      "learning_rate": 2.9849012775842044e-06,
      "loss": 1.7659,
      "step": 771
    },
    {
      "epoch": 0.2988772744870306,
      "grad_norm": 5.784604072570801,
      "learning_rate": 2.9887727448703063e-06,
      "loss": 1.7458,
      "step": 772
    },
    {
      "epoch": 0.29926442121564073,
      "grad_norm": 7.5158233642578125,
      "learning_rate": 2.9926442121564074e-06,
      "loss": 1.9343,
      "step": 773
    },
    {
      "epoch": 0.29965156794425085,
      "grad_norm": 4.8036603927612305,
      "learning_rate": 2.996515679442509e-06,
      "loss": 1.9368,
      "step": 774
    },
    {
      "epoch": 0.30003871467286103,
      "grad_norm": 4.685634613037109,
      "learning_rate": 3.0003871467286104e-06,
      "loss": 1.7937,
      "step": 775
    },
    {
      "epoch": 0.30042586140147115,
      "grad_norm": 5.344221591949463,
      "learning_rate": 3.004258614014712e-06,
      "loss": 1.9383,
      "step": 776
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 6.42534065246582,
      "learning_rate": 3.0081300813008134e-06,
      "loss": 1.9676,
      "step": 777
    },
    {
      "epoch": 0.30120015485869145,
      "grad_norm": 6.802953720092773,
      "learning_rate": 3.012001548586915e-06,
      "loss": 1.8353,
      "step": 778
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 6.546514511108398,
      "learning_rate": 3.015873015873016e-06,
      "loss": 2.0512,
      "step": 779
    },
    {
      "epoch": 0.30197444831591175,
      "grad_norm": 6.816506862640381,
      "learning_rate": 3.019744483159118e-06,
      "loss": 1.8282,
      "step": 780
    },
    {
      "epoch": 0.30236159504452187,
      "grad_norm": 10.432145118713379,
      "learning_rate": 3.023615950445219e-06,
      "loss": 1.7776,
      "step": 781
    },
    {
      "epoch": 0.302748741773132,
      "grad_norm": 4.962366104125977,
      "learning_rate": 3.02748741773132e-06,
      "loss": 1.9361,
      "step": 782
    },
    {
      "epoch": 0.30313588850174217,
      "grad_norm": 5.64499568939209,
      "learning_rate": 3.031358885017422e-06,
      "loss": 1.9697,
      "step": 783
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 9.46323013305664,
      "learning_rate": 3.035230352303523e-06,
      "loss": 1.9652,
      "step": 784
    },
    {
      "epoch": 0.30391018195896247,
      "grad_norm": 6.745748996734619,
      "learning_rate": 3.039101819589625e-06,
      "loss": 1.7772,
      "step": 785
    },
    {
      "epoch": 0.3042973286875726,
      "grad_norm": 8.02359390258789,
      "learning_rate": 3.042973286875726e-06,
      "loss": 1.7514,
      "step": 786
    },
    {
      "epoch": 0.3046844754161827,
      "grad_norm": 5.675581455230713,
      "learning_rate": 3.0468447541618275e-06,
      "loss": 1.735,
      "step": 787
    },
    {
      "epoch": 0.3050716221447929,
      "grad_norm": 5.587117671966553,
      "learning_rate": 3.050716221447929e-06,
      "loss": 1.7456,
      "step": 788
    },
    {
      "epoch": 0.305458768873403,
      "grad_norm": 5.306910991668701,
      "learning_rate": 3.0545876887340305e-06,
      "loss": 1.988,
      "step": 789
    },
    {
      "epoch": 0.3058459156020132,
      "grad_norm": 4.185196876525879,
      "learning_rate": 3.058459156020132e-06,
      "loss": 1.9034,
      "step": 790
    },
    {
      "epoch": 0.3062330623306233,
      "grad_norm": 6.3735671043396,
      "learning_rate": 3.0623306233062334e-06,
      "loss": 1.8445,
      "step": 791
    },
    {
      "epoch": 0.30662020905923343,
      "grad_norm": 10.124629020690918,
      "learning_rate": 3.0662020905923345e-06,
      "loss": 1.6863,
      "step": 792
    },
    {
      "epoch": 0.3070073557878436,
      "grad_norm": 6.769291877746582,
      "learning_rate": 3.0700735578784364e-06,
      "loss": 2.0166,
      "step": 793
    },
    {
      "epoch": 0.3073945025164537,
      "grad_norm": 5.670444488525391,
      "learning_rate": 3.0739450251645375e-06,
      "loss": 1.8505,
      "step": 794
    },
    {
      "epoch": 0.3077816492450639,
      "grad_norm": 6.0421881675720215,
      "learning_rate": 3.0778164924506394e-06,
      "loss": 1.9222,
      "step": 795
    },
    {
      "epoch": 0.308168795973674,
      "grad_norm": 6.5627570152282715,
      "learning_rate": 3.0816879597367405e-06,
      "loss": 1.8231,
      "step": 796
    },
    {
      "epoch": 0.30855594270228415,
      "grad_norm": 4.4243645668029785,
      "learning_rate": 3.0855594270228416e-06,
      "loss": 1.7902,
      "step": 797
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 4.4525227546691895,
      "learning_rate": 3.0894308943089435e-06,
      "loss": 1.8462,
      "step": 798
    },
    {
      "epoch": 0.30933023615950445,
      "grad_norm": 4.4406514167785645,
      "learning_rate": 3.0933023615950445e-06,
      "loss": 1.8506,
      "step": 799
    },
    {
      "epoch": 0.3097173828881146,
      "grad_norm": 6.788020610809326,
      "learning_rate": 3.0971738288811465e-06,
      "loss": 1.8197,
      "step": 800
    },
    {
      "epoch": 0.31010452961672474,
      "grad_norm": 4.740174770355225,
      "learning_rate": 3.1010452961672475e-06,
      "loss": 1.8689,
      "step": 801
    },
    {
      "epoch": 0.31049167634533487,
      "grad_norm": 5.3511247634887695,
      "learning_rate": 3.104916763453349e-06,
      "loss": 1.8578,
      "step": 802
    },
    {
      "epoch": 0.31087882307394504,
      "grad_norm": 4.1916704177856445,
      "learning_rate": 3.1087882307394505e-06,
      "loss": 1.9015,
      "step": 803
    },
    {
      "epoch": 0.31126596980255516,
      "grad_norm": 5.017251491546631,
      "learning_rate": 3.112659698025552e-06,
      "loss": 1.8689,
      "step": 804
    },
    {
      "epoch": 0.3116531165311653,
      "grad_norm": 4.858780384063721,
      "learning_rate": 3.116531165311653e-06,
      "loss": 1.8752,
      "step": 805
    },
    {
      "epoch": 0.31204026325977546,
      "grad_norm": 7.455528736114502,
      "learning_rate": 3.120402632597755e-06,
      "loss": 2.0778,
      "step": 806
    },
    {
      "epoch": 0.3124274099883856,
      "grad_norm": 6.000278472900391,
      "learning_rate": 3.124274099883856e-06,
      "loss": 1.8841,
      "step": 807
    },
    {
      "epoch": 0.31281455671699576,
      "grad_norm": 6.417995929718018,
      "learning_rate": 3.128145567169958e-06,
      "loss": 1.9384,
      "step": 808
    },
    {
      "epoch": 0.3132017034456059,
      "grad_norm": 5.796879291534424,
      "learning_rate": 3.132017034456059e-06,
      "loss": 1.7489,
      "step": 809
    },
    {
      "epoch": 0.313588850174216,
      "grad_norm": 5.434488296508789,
      "learning_rate": 3.13588850174216e-06,
      "loss": 1.9463,
      "step": 810
    },
    {
      "epoch": 0.3139759969028262,
      "grad_norm": 4.301883697509766,
      "learning_rate": 3.139759969028262e-06,
      "loss": 1.8658,
      "step": 811
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 5.496555328369141,
      "learning_rate": 3.143631436314363e-06,
      "loss": 1.8655,
      "step": 812
    },
    {
      "epoch": 0.3147502903600465,
      "grad_norm": 6.712944507598877,
      "learning_rate": 3.147502903600465e-06,
      "loss": 1.876,
      "step": 813
    },
    {
      "epoch": 0.3151374370886566,
      "grad_norm": 6.929919719696045,
      "learning_rate": 3.151374370886566e-06,
      "loss": 1.942,
      "step": 814
    },
    {
      "epoch": 0.3155245838172667,
      "grad_norm": 7.619915962219238,
      "learning_rate": 3.1552458381726676e-06,
      "loss": 2.0467,
      "step": 815
    },
    {
      "epoch": 0.3159117305458769,
      "grad_norm": 6.238576412200928,
      "learning_rate": 3.159117305458769e-06,
      "loss": 2.0159,
      "step": 816
    },
    {
      "epoch": 0.316298877274487,
      "grad_norm": 6.187562465667725,
      "learning_rate": 3.1629887727448706e-06,
      "loss": 1.8343,
      "step": 817
    },
    {
      "epoch": 0.3166860240030972,
      "grad_norm": 5.7111663818359375,
      "learning_rate": 3.166860240030972e-06,
      "loss": 2.0185,
      "step": 818
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 6.155790328979492,
      "learning_rate": 3.1707317073170736e-06,
      "loss": 1.9357,
      "step": 819
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 6.024950981140137,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 1.8469,
      "step": 820
    },
    {
      "epoch": 0.3178474641889276,
      "grad_norm": 4.766077041625977,
      "learning_rate": 3.1784746418892766e-06,
      "loss": 1.9448,
      "step": 821
    },
    {
      "epoch": 0.31823461091753774,
      "grad_norm": 5.4873504638671875,
      "learning_rate": 3.1823461091753776e-06,
      "loss": 1.7459,
      "step": 822
    },
    {
      "epoch": 0.31862175764614786,
      "grad_norm": 6.2112579345703125,
      "learning_rate": 3.1862175764614787e-06,
      "loss": 1.8522,
      "step": 823
    },
    {
      "epoch": 0.31900890437475804,
      "grad_norm": 5.348487377166748,
      "learning_rate": 3.1900890437475806e-06,
      "loss": 1.8355,
      "step": 824
    },
    {
      "epoch": 0.31939605110336816,
      "grad_norm": 4.652820587158203,
      "learning_rate": 3.1939605110336817e-06,
      "loss": 1.8774,
      "step": 825
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 4.153924942016602,
      "learning_rate": 3.1978319783197836e-06,
      "loss": 1.9156,
      "step": 826
    },
    {
      "epoch": 0.32017034456058846,
      "grad_norm": 4.569142818450928,
      "learning_rate": 3.2017034456058847e-06,
      "loss": 1.7949,
      "step": 827
    },
    {
      "epoch": 0.3205574912891986,
      "grad_norm": 7.637098789215088,
      "learning_rate": 3.205574912891986e-06,
      "loss": 1.9284,
      "step": 828
    },
    {
      "epoch": 0.32094463801780876,
      "grad_norm": 4.78095817565918,
      "learning_rate": 3.2094463801780877e-06,
      "loss": 1.8912,
      "step": 829
    },
    {
      "epoch": 0.3213317847464189,
      "grad_norm": 4.840390205383301,
      "learning_rate": 3.213317847464189e-06,
      "loss": 1.9292,
      "step": 830
    },
    {
      "epoch": 0.32171893147502906,
      "grad_norm": 4.632334232330322,
      "learning_rate": 3.2171893147502906e-06,
      "loss": 1.7963,
      "step": 831
    },
    {
      "epoch": 0.3221060782036392,
      "grad_norm": 7.335258960723877,
      "learning_rate": 3.221060782036392e-06,
      "loss": 2.1439,
      "step": 832
    },
    {
      "epoch": 0.3224932249322493,
      "grad_norm": 6.618497848510742,
      "learning_rate": 3.224932249322493e-06,
      "loss": 1.8431,
      "step": 833
    },
    {
      "epoch": 0.3228803716608595,
      "grad_norm": 6.997554779052734,
      "learning_rate": 3.228803716608595e-06,
      "loss": 1.8194,
      "step": 834
    },
    {
      "epoch": 0.3232675183894696,
      "grad_norm": 7.6938018798828125,
      "learning_rate": 3.232675183894696e-06,
      "loss": 2.0969,
      "step": 835
    },
    {
      "epoch": 0.3236546651180798,
      "grad_norm": 6.193465232849121,
      "learning_rate": 3.236546651180798e-06,
      "loss": 1.8314,
      "step": 836
    },
    {
      "epoch": 0.3240418118466899,
      "grad_norm": 6.705842971801758,
      "learning_rate": 3.240418118466899e-06,
      "loss": 1.9696,
      "step": 837
    },
    {
      "epoch": 0.3244289585753,
      "grad_norm": 6.203885555267334,
      "learning_rate": 3.2442895857530007e-06,
      "loss": 1.8961,
      "step": 838
    },
    {
      "epoch": 0.3248161053039102,
      "grad_norm": 8.54855728149414,
      "learning_rate": 3.248161053039102e-06,
      "loss": 1.9223,
      "step": 839
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 5.910970211029053,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 1.9365,
      "step": 840
    },
    {
      "epoch": 0.3255903987611305,
      "grad_norm": 5.887923717498779,
      "learning_rate": 3.255903987611305e-06,
      "loss": 1.766,
      "step": 841
    },
    {
      "epoch": 0.3259775454897406,
      "grad_norm": 5.040539264678955,
      "learning_rate": 3.2597754548974066e-06,
      "loss": 1.9635,
      "step": 842
    },
    {
      "epoch": 0.32636469221835074,
      "grad_norm": 5.9049391746521,
      "learning_rate": 3.2636469221835077e-06,
      "loss": 1.8503,
      "step": 843
    },
    {
      "epoch": 0.3267518389469609,
      "grad_norm": 5.912818431854248,
      "learning_rate": 3.2675183894696096e-06,
      "loss": 1.7372,
      "step": 844
    },
    {
      "epoch": 0.32713898567557104,
      "grad_norm": 5.692553997039795,
      "learning_rate": 3.2713898567557107e-06,
      "loss": 1.8906,
      "step": 845
    },
    {
      "epoch": 0.32752613240418116,
      "grad_norm": 5.645511150360107,
      "learning_rate": 3.2752613240418118e-06,
      "loss": 1.9932,
      "step": 846
    },
    {
      "epoch": 0.32791327913279134,
      "grad_norm": 4.425200462341309,
      "learning_rate": 3.2791327913279137e-06,
      "loss": 1.7631,
      "step": 847
    },
    {
      "epoch": 0.32830042586140146,
      "grad_norm": 5.873213291168213,
      "learning_rate": 3.2830042586140148e-06,
      "loss": 1.9709,
      "step": 848
    },
    {
      "epoch": 0.32868757259001163,
      "grad_norm": 7.371757984161377,
      "learning_rate": 3.2868757259001167e-06,
      "loss": 1.7789,
      "step": 849
    },
    {
      "epoch": 0.32907471931862176,
      "grad_norm": 10.0683012008667,
      "learning_rate": 3.2907471931862177e-06,
      "loss": 1.7267,
      "step": 850
    },
    {
      "epoch": 0.3294618660472319,
      "grad_norm": 6.804142475128174,
      "learning_rate": 3.2946186604723192e-06,
      "loss": 1.991,
      "step": 851
    },
    {
      "epoch": 0.32984901277584205,
      "grad_norm": 5.716935634613037,
      "learning_rate": 3.2984901277584207e-06,
      "loss": 1.8856,
      "step": 852
    },
    {
      "epoch": 0.3302361595044522,
      "grad_norm": 5.3959059715271,
      "learning_rate": 3.3023615950445222e-06,
      "loss": 1.9214,
      "step": 853
    },
    {
      "epoch": 0.33062330623306235,
      "grad_norm": 8.169224739074707,
      "learning_rate": 3.3062330623306237e-06,
      "loss": 1.9224,
      "step": 854
    },
    {
      "epoch": 0.3310104529616725,
      "grad_norm": 5.549602508544922,
      "learning_rate": 3.310104529616725e-06,
      "loss": 1.7604,
      "step": 855
    },
    {
      "epoch": 0.3313975996902826,
      "grad_norm": 5.1741766929626465,
      "learning_rate": 3.3139759969028263e-06,
      "loss": 1.872,
      "step": 856
    },
    {
      "epoch": 0.3317847464188928,
      "grad_norm": 3.778778314590454,
      "learning_rate": 3.317847464188928e-06,
      "loss": 1.8409,
      "step": 857
    },
    {
      "epoch": 0.3321718931475029,
      "grad_norm": 5.4393439292907715,
      "learning_rate": 3.3217189314750293e-06,
      "loss": 1.8688,
      "step": 858
    },
    {
      "epoch": 0.33255903987611307,
      "grad_norm": 5.699974536895752,
      "learning_rate": 3.325590398761131e-06,
      "loss": 1.841,
      "step": 859
    },
    {
      "epoch": 0.3329461866047232,
      "grad_norm": 5.061371326446533,
      "learning_rate": 3.3294618660472323e-06,
      "loss": 1.9313,
      "step": 860
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 4.864750385284424,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.9648,
      "step": 861
    },
    {
      "epoch": 0.3337204800619435,
      "grad_norm": 7.423203468322754,
      "learning_rate": 3.3372048006194352e-06,
      "loss": 1.9577,
      "step": 862
    },
    {
      "epoch": 0.3341076267905536,
      "grad_norm": 6.4587860107421875,
      "learning_rate": 3.3410762679055363e-06,
      "loss": 1.7395,
      "step": 863
    },
    {
      "epoch": 0.3344947735191638,
      "grad_norm": 6.423622131347656,
      "learning_rate": 3.3449477351916382e-06,
      "loss": 1.8121,
      "step": 864
    },
    {
      "epoch": 0.3348819202477739,
      "grad_norm": 6.226582050323486,
      "learning_rate": 3.3488192024777393e-06,
      "loss": 1.8357,
      "step": 865
    },
    {
      "epoch": 0.33526906697638403,
      "grad_norm": 8.454564094543457,
      "learning_rate": 3.352690669763841e-06,
      "loss": 1.9223,
      "step": 866
    },
    {
      "epoch": 0.3356562137049942,
      "grad_norm": 6.044162750244141,
      "learning_rate": 3.3565621370499423e-06,
      "loss": 1.7265,
      "step": 867
    },
    {
      "epoch": 0.33604336043360433,
      "grad_norm": 4.222441673278809,
      "learning_rate": 3.3604336043360438e-06,
      "loss": 1.8772,
      "step": 868
    },
    {
      "epoch": 0.33643050716221445,
      "grad_norm": 4.663129806518555,
      "learning_rate": 3.364305071622145e-06,
      "loss": 1.9725,
      "step": 869
    },
    {
      "epoch": 0.33681765389082463,
      "grad_norm": 6.317265510559082,
      "learning_rate": 3.3681765389082468e-06,
      "loss": 1.7537,
      "step": 870
    },
    {
      "epoch": 0.33720480061943475,
      "grad_norm": 4.505406856536865,
      "learning_rate": 3.372048006194348e-06,
      "loss": 1.8973,
      "step": 871
    },
    {
      "epoch": 0.33759194734804493,
      "grad_norm": 6.944278240203857,
      "learning_rate": 3.3759194734804498e-06,
      "loss": 1.9139,
      "step": 872
    },
    {
      "epoch": 0.33797909407665505,
      "grad_norm": 5.991332530975342,
      "learning_rate": 3.379790940766551e-06,
      "loss": 2.007,
      "step": 873
    },
    {
      "epoch": 0.3383662408052652,
      "grad_norm": 5.562312126159668,
      "learning_rate": 3.383662408052652e-06,
      "loss": 1.9214,
      "step": 874
    },
    {
      "epoch": 0.33875338753387535,
      "grad_norm": 8.87118911743164,
      "learning_rate": 3.387533875338754e-06,
      "loss": 1.906,
      "step": 875
    },
    {
      "epoch": 0.33914053426248547,
      "grad_norm": 5.414921760559082,
      "learning_rate": 3.391405342624855e-06,
      "loss": 1.8234,
      "step": 876
    },
    {
      "epoch": 0.33952768099109565,
      "grad_norm": 6.100900650024414,
      "learning_rate": 3.395276809910957e-06,
      "loss": 1.8379,
      "step": 877
    },
    {
      "epoch": 0.33991482771970577,
      "grad_norm": 6.632890701293945,
      "learning_rate": 3.399148277197058e-06,
      "loss": 1.7059,
      "step": 878
    },
    {
      "epoch": 0.3403019744483159,
      "grad_norm": 5.403982639312744,
      "learning_rate": 3.4030197444831594e-06,
      "loss": 1.9368,
      "step": 879
    },
    {
      "epoch": 0.34068912117692607,
      "grad_norm": 6.948767185211182,
      "learning_rate": 3.406891211769261e-06,
      "loss": 1.9726,
      "step": 880
    },
    {
      "epoch": 0.3410762679055362,
      "grad_norm": 7.1643571853637695,
      "learning_rate": 3.4107626790553623e-06,
      "loss": 1.8223,
      "step": 881
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 4.706746578216553,
      "learning_rate": 3.414634146341464e-06,
      "loss": 1.8573,
      "step": 882
    },
    {
      "epoch": 0.3418505613627565,
      "grad_norm": 8.068329811096191,
      "learning_rate": 3.4185056136275653e-06,
      "loss": 1.7434,
      "step": 883
    },
    {
      "epoch": 0.3422377080913666,
      "grad_norm": 6.023092746734619,
      "learning_rate": 3.4223770809136664e-06,
      "loss": 1.8223,
      "step": 884
    },
    {
      "epoch": 0.3426248548199768,
      "grad_norm": 7.659391403198242,
      "learning_rate": 3.4262485481997683e-06,
      "loss": 1.8335,
      "step": 885
    },
    {
      "epoch": 0.3430120015485869,
      "grad_norm": 6.32168436050415,
      "learning_rate": 3.4301200154858694e-06,
      "loss": 1.8933,
      "step": 886
    },
    {
      "epoch": 0.34339914827719703,
      "grad_norm": 5.008774280548096,
      "learning_rate": 3.4339914827719705e-06,
      "loss": 1.9749,
      "step": 887
    },
    {
      "epoch": 0.3437862950058072,
      "grad_norm": 3.820782423019409,
      "learning_rate": 3.4378629500580724e-06,
      "loss": 1.8132,
      "step": 888
    },
    {
      "epoch": 0.34417344173441733,
      "grad_norm": 10.133663177490234,
      "learning_rate": 3.4417344173441734e-06,
      "loss": 1.9117,
      "step": 889
    },
    {
      "epoch": 0.3445605884630275,
      "grad_norm": 6.351448059082031,
      "learning_rate": 3.4456058846302754e-06,
      "loss": 1.9423,
      "step": 890
    },
    {
      "epoch": 0.34494773519163763,
      "grad_norm": 5.801458835601807,
      "learning_rate": 3.4494773519163764e-06,
      "loss": 1.9876,
      "step": 891
    },
    {
      "epoch": 0.34533488192024775,
      "grad_norm": 5.4739460945129395,
      "learning_rate": 3.453348819202478e-06,
      "loss": 1.8503,
      "step": 892
    },
    {
      "epoch": 0.3457220286488579,
      "grad_norm": 5.154107093811035,
      "learning_rate": 3.4572202864885794e-06,
      "loss": 1.8281,
      "step": 893
    },
    {
      "epoch": 0.34610917537746805,
      "grad_norm": 5.932253837585449,
      "learning_rate": 3.461091753774681e-06,
      "loss": 1.8432,
      "step": 894
    },
    {
      "epoch": 0.3464963221060782,
      "grad_norm": 10.667448043823242,
      "learning_rate": 3.4649632210607824e-06,
      "loss": 1.7225,
      "step": 895
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 7.351673126220703,
      "learning_rate": 3.468834688346884e-06,
      "loss": 1.7891,
      "step": 896
    },
    {
      "epoch": 0.34727061556329847,
      "grad_norm": 5.1575751304626465,
      "learning_rate": 3.472706155632985e-06,
      "loss": 1.8494,
      "step": 897
    },
    {
      "epoch": 0.34765776229190865,
      "grad_norm": 5.02217960357666,
      "learning_rate": 3.476577622919087e-06,
      "loss": 1.9592,
      "step": 898
    },
    {
      "epoch": 0.34804490902051877,
      "grad_norm": 6.5551886558532715,
      "learning_rate": 3.480449090205188e-06,
      "loss": 1.8134,
      "step": 899
    },
    {
      "epoch": 0.34843205574912894,
      "grad_norm": 10.820032119750977,
      "learning_rate": 3.48432055749129e-06,
      "loss": 1.9588,
      "step": 900
    },
    {
      "epoch": 0.34881920247773907,
      "grad_norm": 6.451162815093994,
      "learning_rate": 3.488192024777391e-06,
      "loss": 2.07,
      "step": 901
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 6.635438919067383,
      "learning_rate": 3.492063492063492e-06,
      "loss": 1.8382,
      "step": 902
    },
    {
      "epoch": 0.34959349593495936,
      "grad_norm": 7.011789798736572,
      "learning_rate": 3.495934959349594e-06,
      "loss": 1.6766,
      "step": 903
    },
    {
      "epoch": 0.3499806426635695,
      "grad_norm": 9.57318115234375,
      "learning_rate": 3.499806426635695e-06,
      "loss": 1.7279,
      "step": 904
    },
    {
      "epoch": 0.35036778939217966,
      "grad_norm": 6.355260372161865,
      "learning_rate": 3.503677893921797e-06,
      "loss": 1.9439,
      "step": 905
    },
    {
      "epoch": 0.3507549361207898,
      "grad_norm": 6.412200927734375,
      "learning_rate": 3.507549361207898e-06,
      "loss": 1.8079,
      "step": 906
    },
    {
      "epoch": 0.3511420828493999,
      "grad_norm": 6.9061479568481445,
      "learning_rate": 3.5114208284939995e-06,
      "loss": 1.7214,
      "step": 907
    },
    {
      "epoch": 0.3515292295780101,
      "grad_norm": 5.9676313400268555,
      "learning_rate": 3.515292295780101e-06,
      "loss": 1.7231,
      "step": 908
    },
    {
      "epoch": 0.3519163763066202,
      "grad_norm": 7.351244926452637,
      "learning_rate": 3.5191637630662025e-06,
      "loss": 1.9573,
      "step": 909
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 9.965896606445312,
      "learning_rate": 3.5230352303523035e-06,
      "loss": 1.9179,
      "step": 910
    },
    {
      "epoch": 0.3526906697638405,
      "grad_norm": 5.996851921081543,
      "learning_rate": 3.5269066976384055e-06,
      "loss": 1.8246,
      "step": 911
    },
    {
      "epoch": 0.3530778164924506,
      "grad_norm": 5.066661357879639,
      "learning_rate": 3.5307781649245065e-06,
      "loss": 1.8176,
      "step": 912
    },
    {
      "epoch": 0.3534649632210608,
      "grad_norm": 7.239879608154297,
      "learning_rate": 3.5346496322106084e-06,
      "loss": 1.7103,
      "step": 913
    },
    {
      "epoch": 0.3538521099496709,
      "grad_norm": 4.652872085571289,
      "learning_rate": 3.5385210994967095e-06,
      "loss": 1.8752,
      "step": 914
    },
    {
      "epoch": 0.35423925667828104,
      "grad_norm": 8.013227462768555,
      "learning_rate": 3.5423925667828106e-06,
      "loss": 1.9386,
      "step": 915
    },
    {
      "epoch": 0.3546264034068912,
      "grad_norm": 6.250703811645508,
      "learning_rate": 3.5462640340689125e-06,
      "loss": 1.8108,
      "step": 916
    },
    {
      "epoch": 0.35501355013550134,
      "grad_norm": 5.24884557723999,
      "learning_rate": 3.5501355013550136e-06,
      "loss": 1.7778,
      "step": 917
    },
    {
      "epoch": 0.3554006968641115,
      "grad_norm": 5.327972412109375,
      "learning_rate": 3.5540069686411155e-06,
      "loss": 1.8662,
      "step": 918
    },
    {
      "epoch": 0.35578784359272164,
      "grad_norm": 5.019577980041504,
      "learning_rate": 3.5578784359272166e-06,
      "loss": 1.8298,
      "step": 919
    },
    {
      "epoch": 0.35617499032133176,
      "grad_norm": 5.474857330322266,
      "learning_rate": 3.561749903213318e-06,
      "loss": 1.826,
      "step": 920
    },
    {
      "epoch": 0.35656213704994194,
      "grad_norm": 4.48022985458374,
      "learning_rate": 3.5656213704994195e-06,
      "loss": 1.9117,
      "step": 921
    },
    {
      "epoch": 0.35694928377855206,
      "grad_norm": 6.3672776222229,
      "learning_rate": 3.569492837785521e-06,
      "loss": 1.8216,
      "step": 922
    },
    {
      "epoch": 0.35733643050716224,
      "grad_norm": 6.9902849197387695,
      "learning_rate": 3.5733643050716225e-06,
      "loss": 2.0655,
      "step": 923
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 5.389644622802734,
      "learning_rate": 3.577235772357724e-06,
      "loss": 1.917,
      "step": 924
    },
    {
      "epoch": 0.3581107239643825,
      "grad_norm": 6.284972667694092,
      "learning_rate": 3.581107239643825e-06,
      "loss": 1.7176,
      "step": 925
    },
    {
      "epoch": 0.35849787069299266,
      "grad_norm": 7.1447954177856445,
      "learning_rate": 3.584978706929927e-06,
      "loss": 1.8124,
      "step": 926
    },
    {
      "epoch": 0.3588850174216028,
      "grad_norm": 9.958982467651367,
      "learning_rate": 3.588850174216028e-06,
      "loss": 1.9154,
      "step": 927
    },
    {
      "epoch": 0.35927216415021296,
      "grad_norm": 4.640240669250488,
      "learning_rate": 3.59272164150213e-06,
      "loss": 1.8529,
      "step": 928
    },
    {
      "epoch": 0.3596593108788231,
      "grad_norm": 10.382489204406738,
      "learning_rate": 3.596593108788231e-06,
      "loss": 1.9181,
      "step": 929
    },
    {
      "epoch": 0.3600464576074332,
      "grad_norm": 7.03015661239624,
      "learning_rate": 3.600464576074332e-06,
      "loss": 1.9232,
      "step": 930
    },
    {
      "epoch": 0.3604336043360434,
      "grad_norm": 5.814809799194336,
      "learning_rate": 3.604336043360434e-06,
      "loss": 1.863,
      "step": 931
    },
    {
      "epoch": 0.3608207510646535,
      "grad_norm": 6.6736860275268555,
      "learning_rate": 3.608207510646535e-06,
      "loss": 1.8235,
      "step": 932
    },
    {
      "epoch": 0.3612078977932636,
      "grad_norm": 5.057463645935059,
      "learning_rate": 3.6120789779326366e-06,
      "loss": 1.8298,
      "step": 933
    },
    {
      "epoch": 0.3615950445218738,
      "grad_norm": 10.918339729309082,
      "learning_rate": 3.615950445218738e-06,
      "loss": 1.6659,
      "step": 934
    },
    {
      "epoch": 0.3619821912504839,
      "grad_norm": 7.8552398681640625,
      "learning_rate": 3.6198219125048396e-06,
      "loss": 1.9181,
      "step": 935
    },
    {
      "epoch": 0.3623693379790941,
      "grad_norm": 6.742218971252441,
      "learning_rate": 3.623693379790941e-06,
      "loss": 1.9649,
      "step": 936
    },
    {
      "epoch": 0.3627564847077042,
      "grad_norm": 10.614937782287598,
      "learning_rate": 3.6275648470770426e-06,
      "loss": 1.658,
      "step": 937
    },
    {
      "epoch": 0.36314363143631434,
      "grad_norm": 6.994784355163574,
      "learning_rate": 3.6314363143631437e-06,
      "loss": 1.7901,
      "step": 938
    },
    {
      "epoch": 0.3635307781649245,
      "grad_norm": 7.651595592498779,
      "learning_rate": 3.6353077816492456e-06,
      "loss": 1.7745,
      "step": 939
    },
    {
      "epoch": 0.36391792489353464,
      "grad_norm": 7.180861949920654,
      "learning_rate": 3.6391792489353466e-06,
      "loss": 1.7663,
      "step": 940
    },
    {
      "epoch": 0.3643050716221448,
      "grad_norm": 7.757514476776123,
      "learning_rate": 3.6430507162214486e-06,
      "loss": 1.9243,
      "step": 941
    },
    {
      "epoch": 0.36469221835075494,
      "grad_norm": 7.771273612976074,
      "learning_rate": 3.6469221835075496e-06,
      "loss": 1.8037,
      "step": 942
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 6.205393314361572,
      "learning_rate": 3.6507936507936507e-06,
      "loss": 1.8512,
      "step": 943
    },
    {
      "epoch": 0.36546651180797524,
      "grad_norm": 7.9298014640808105,
      "learning_rate": 3.6546651180797526e-06,
      "loss": 2.0097,
      "step": 944
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 7.5006103515625,
      "learning_rate": 3.6585365853658537e-06,
      "loss": 2.0315,
      "step": 945
    },
    {
      "epoch": 0.36624080526519553,
      "grad_norm": 6.117129325866699,
      "learning_rate": 3.6624080526519556e-06,
      "loss": 1.8459,
      "step": 946
    },
    {
      "epoch": 0.36662795199380566,
      "grad_norm": 5.192980766296387,
      "learning_rate": 3.6662795199380567e-06,
      "loss": 1.8411,
      "step": 947
    },
    {
      "epoch": 0.3670150987224158,
      "grad_norm": 7.468575954437256,
      "learning_rate": 3.670150987224158e-06,
      "loss": 1.7749,
      "step": 948
    },
    {
      "epoch": 0.36740224545102595,
      "grad_norm": 10.083757400512695,
      "learning_rate": 3.6740224545102597e-06,
      "loss": 1.7704,
      "step": 949
    },
    {
      "epoch": 0.3677893921796361,
      "grad_norm": 6.403713703155518,
      "learning_rate": 3.677893921796361e-06,
      "loss": 1.8017,
      "step": 950
    },
    {
      "epoch": 0.3681765389082462,
      "grad_norm": 5.674198627471924,
      "learning_rate": 3.6817653890824622e-06,
      "loss": 2.0026,
      "step": 951
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 4.889875411987305,
      "learning_rate": 3.685636856368564e-06,
      "loss": 1.8509,
      "step": 952
    },
    {
      "epoch": 0.3689508323654665,
      "grad_norm": 6.990925312042236,
      "learning_rate": 3.6895083236546652e-06,
      "loss": 1.9853,
      "step": 953
    },
    {
      "epoch": 0.3693379790940767,
      "grad_norm": 11.994502067565918,
      "learning_rate": 3.693379790940767e-06,
      "loss": 1.7516,
      "step": 954
    },
    {
      "epoch": 0.3697251258226868,
      "grad_norm": 9.122444152832031,
      "learning_rate": 3.697251258226868e-06,
      "loss": 2.1057,
      "step": 955
    },
    {
      "epoch": 0.3701122725512969,
      "grad_norm": 7.620613098144531,
      "learning_rate": 3.7011227255129693e-06,
      "loss": 1.7136,
      "step": 956
    },
    {
      "epoch": 0.3704994192799071,
      "grad_norm": 6.132121562957764,
      "learning_rate": 3.704994192799071e-06,
      "loss": 1.9759,
      "step": 957
    },
    {
      "epoch": 0.3708865660085172,
      "grad_norm": 7.0659403800964355,
      "learning_rate": 3.7088656600851723e-06,
      "loss": 1.8104,
      "step": 958
    },
    {
      "epoch": 0.3712737127371274,
      "grad_norm": 6.117478370666504,
      "learning_rate": 3.712737127371274e-06,
      "loss": 1.7564,
      "step": 959
    },
    {
      "epoch": 0.3716608594657375,
      "grad_norm": 6.567720413208008,
      "learning_rate": 3.7166085946573752e-06,
      "loss": 1.8593,
      "step": 960
    },
    {
      "epoch": 0.37204800619434764,
      "grad_norm": 7.8149333000183105,
      "learning_rate": 3.7204800619434767e-06,
      "loss": 1.8281,
      "step": 961
    },
    {
      "epoch": 0.3724351529229578,
      "grad_norm": 8.909214973449707,
      "learning_rate": 3.7243515292295782e-06,
      "loss": 2.0803,
      "step": 962
    },
    {
      "epoch": 0.37282229965156793,
      "grad_norm": 5.039905548095703,
      "learning_rate": 3.7282229965156797e-06,
      "loss": 1.7746,
      "step": 963
    },
    {
      "epoch": 0.3732094463801781,
      "grad_norm": 5.5525641441345215,
      "learning_rate": 3.7320944638017812e-06,
      "loss": 1.826,
      "step": 964
    },
    {
      "epoch": 0.37359659310878823,
      "grad_norm": 11.596717834472656,
      "learning_rate": 3.7359659310878827e-06,
      "loss": 2.1175,
      "step": 965
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 6.0371856689453125,
      "learning_rate": 3.7398373983739838e-06,
      "loss": 1.9787,
      "step": 966
    },
    {
      "epoch": 0.37437088656600853,
      "grad_norm": 6.924212455749512,
      "learning_rate": 3.7437088656600857e-06,
      "loss": 1.8907,
      "step": 967
    },
    {
      "epoch": 0.37475803329461865,
      "grad_norm": 7.629073619842529,
      "learning_rate": 3.7475803329461868e-06,
      "loss": 1.9683,
      "step": 968
    },
    {
      "epoch": 0.37514518002322883,
      "grad_norm": 8.482142448425293,
      "learning_rate": 3.7514518002322887e-06,
      "loss": 1.7534,
      "step": 969
    },
    {
      "epoch": 0.37553232675183895,
      "grad_norm": 4.847962379455566,
      "learning_rate": 3.7553232675183898e-06,
      "loss": 1.9211,
      "step": 970
    },
    {
      "epoch": 0.3759194734804491,
      "grad_norm": 8.615931510925293,
      "learning_rate": 3.7591947348044912e-06,
      "loss": 1.9173,
      "step": 971
    },
    {
      "epoch": 0.37630662020905925,
      "grad_norm": 4.532628536224365,
      "learning_rate": 3.7630662020905927e-06,
      "loss": 1.8191,
      "step": 972
    },
    {
      "epoch": 0.37669376693766937,
      "grad_norm": 7.408926486968994,
      "learning_rate": 3.7669376693766942e-06,
      "loss": 1.9242,
      "step": 973
    },
    {
      "epoch": 0.3770809136662795,
      "grad_norm": 6.459501266479492,
      "learning_rate": 3.7708091366627953e-06,
      "loss": 1.8151,
      "step": 974
    },
    {
      "epoch": 0.37746806039488967,
      "grad_norm": 4.728452682495117,
      "learning_rate": 3.7746806039488972e-06,
      "loss": 1.8509,
      "step": 975
    },
    {
      "epoch": 0.3778552071234998,
      "grad_norm": 6.875911712646484,
      "learning_rate": 3.7785520712349983e-06,
      "loss": 1.8357,
      "step": 976
    },
    {
      "epoch": 0.37824235385210997,
      "grad_norm": 6.194357872009277,
      "learning_rate": 3.7824235385211e-06,
      "loss": 1.8507,
      "step": 977
    },
    {
      "epoch": 0.3786295005807201,
      "grad_norm": 6.441167831420898,
      "learning_rate": 3.7862950058072013e-06,
      "loss": 1.8902,
      "step": 978
    },
    {
      "epoch": 0.3790166473093302,
      "grad_norm": 7.9552693367004395,
      "learning_rate": 3.7901664730933023e-06,
      "loss": 1.7467,
      "step": 979
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 4.351224422454834,
      "learning_rate": 3.7940379403794043e-06,
      "loss": 1.9513,
      "step": 980
    },
    {
      "epoch": 0.3797909407665505,
      "grad_norm": 5.030082702636719,
      "learning_rate": 3.7979094076655053e-06,
      "loss": 1.9397,
      "step": 981
    },
    {
      "epoch": 0.3801780874951607,
      "grad_norm": 6.6347737312316895,
      "learning_rate": 3.8017808749516073e-06,
      "loss": 1.826,
      "step": 982
    },
    {
      "epoch": 0.3805652342237708,
      "grad_norm": 5.957674503326416,
      "learning_rate": 3.8056523422377083e-06,
      "loss": 1.8744,
      "step": 983
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 7.484006404876709,
      "learning_rate": 3.80952380952381e-06,
      "loss": 1.9361,
      "step": 984
    },
    {
      "epoch": 0.3813395276809911,
      "grad_norm": 5.486177921295166,
      "learning_rate": 3.8133952768099113e-06,
      "loss": 1.7396,
      "step": 985
    },
    {
      "epoch": 0.38172667440960123,
      "grad_norm": 6.526432514190674,
      "learning_rate": 3.817266744096012e-06,
      "loss": 1.805,
      "step": 986
    },
    {
      "epoch": 0.3821138211382114,
      "grad_norm": 6.485808372497559,
      "learning_rate": 3.821138211382115e-06,
      "loss": 1.8467,
      "step": 987
    },
    {
      "epoch": 0.38250096786682153,
      "grad_norm": 8.187198638916016,
      "learning_rate": 3.825009678668215e-06,
      "loss": 1.8837,
      "step": 988
    },
    {
      "epoch": 0.38288811459543165,
      "grad_norm": 6.070324897766113,
      "learning_rate": 3.828881145954317e-06,
      "loss": 1.8484,
      "step": 989
    },
    {
      "epoch": 0.3832752613240418,
      "grad_norm": 7.754283905029297,
      "learning_rate": 3.832752613240418e-06,
      "loss": 1.7662,
      "step": 990
    },
    {
      "epoch": 0.38366240805265195,
      "grad_norm": 5.667147159576416,
      "learning_rate": 3.83662408052652e-06,
      "loss": 1.7702,
      "step": 991
    },
    {
      "epoch": 0.3840495547812621,
      "grad_norm": 4.315986633300781,
      "learning_rate": 3.840495547812621e-06,
      "loss": 1.8675,
      "step": 992
    },
    {
      "epoch": 0.38443670150987225,
      "grad_norm": 5.951976776123047,
      "learning_rate": 3.844367015098723e-06,
      "loss": 1.8134,
      "step": 993
    },
    {
      "epoch": 0.38482384823848237,
      "grad_norm": 4.930840015411377,
      "learning_rate": 3.848238482384824e-06,
      "loss": 1.7918,
      "step": 994
    },
    {
      "epoch": 0.38521099496709255,
      "grad_norm": 6.285064697265625,
      "learning_rate": 3.852109949670926e-06,
      "loss": 1.9145,
      "step": 995
    },
    {
      "epoch": 0.38559814169570267,
      "grad_norm": 7.371286392211914,
      "learning_rate": 3.855981416957027e-06,
      "loss": 1.9535,
      "step": 996
    },
    {
      "epoch": 0.3859852884243128,
      "grad_norm": 6.694183826446533,
      "learning_rate": 3.859852884243128e-06,
      "loss": 1.7954,
      "step": 997
    },
    {
      "epoch": 0.38637243515292297,
      "grad_norm": 6.17716121673584,
      "learning_rate": 3.86372435152923e-06,
      "loss": 1.8989,
      "step": 998
    },
    {
      "epoch": 0.3867595818815331,
      "grad_norm": 4.890300273895264,
      "learning_rate": 3.867595818815331e-06,
      "loss": 1.8554,
      "step": 999
    },
    {
      "epoch": 0.38714672861014326,
      "grad_norm": 6.994903564453125,
      "learning_rate": 3.871467286101433e-06,
      "loss": 1.755,
      "step": 1000
    },
    {
      "epoch": 0.3875338753387534,
      "grad_norm": 6.921496868133545,
      "learning_rate": 3.875338753387534e-06,
      "loss": 1.8091,
      "step": 1001
    },
    {
      "epoch": 0.3879210220673635,
      "grad_norm": 10.440444946289062,
      "learning_rate": 3.8792102206736354e-06,
      "loss": 1.8656,
      "step": 1002
    },
    {
      "epoch": 0.3883081687959737,
      "grad_norm": 7.185372352600098,
      "learning_rate": 3.883081687959737e-06,
      "loss": 1.8885,
      "step": 1003
    },
    {
      "epoch": 0.3886953155245838,
      "grad_norm": 4.347341060638428,
      "learning_rate": 3.886953155245838e-06,
      "loss": 1.8515,
      "step": 1004
    },
    {
      "epoch": 0.389082462253194,
      "grad_norm": 10.460396766662598,
      "learning_rate": 3.89082462253194e-06,
      "loss": 1.914,
      "step": 1005
    },
    {
      "epoch": 0.3894696089818041,
      "grad_norm": 5.536220550537109,
      "learning_rate": 3.894696089818041e-06,
      "loss": 1.7757,
      "step": 1006
    },
    {
      "epoch": 0.3898567557104142,
      "grad_norm": 4.700542449951172,
      "learning_rate": 3.898567557104143e-06,
      "loss": 1.7862,
      "step": 1007
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 6.4426374435424805,
      "learning_rate": 3.902439024390244e-06,
      "loss": 1.9703,
      "step": 1008
    },
    {
      "epoch": 0.3906310491676345,
      "grad_norm": 7.4468817710876465,
      "learning_rate": 3.906310491676346e-06,
      "loss": 1.9811,
      "step": 1009
    },
    {
      "epoch": 0.3910181958962447,
      "grad_norm": 7.305410861968994,
      "learning_rate": 3.910181958962447e-06,
      "loss": 1.8045,
      "step": 1010
    },
    {
      "epoch": 0.3914053426248548,
      "grad_norm": 7.314209938049316,
      "learning_rate": 3.914053426248549e-06,
      "loss": 1.8096,
      "step": 1011
    },
    {
      "epoch": 0.39179248935346495,
      "grad_norm": 6.130899429321289,
      "learning_rate": 3.9179248935346495e-06,
      "loss": 1.839,
      "step": 1012
    },
    {
      "epoch": 0.3921796360820751,
      "grad_norm": 6.605877876281738,
      "learning_rate": 3.921796360820752e-06,
      "loss": 1.8013,
      "step": 1013
    },
    {
      "epoch": 0.39256678281068524,
      "grad_norm": 9.672568321228027,
      "learning_rate": 3.9256678281068525e-06,
      "loss": 1.8679,
      "step": 1014
    },
    {
      "epoch": 0.39295392953929537,
      "grad_norm": 7.296731948852539,
      "learning_rate": 3.929539295392954e-06,
      "loss": 1.8086,
      "step": 1015
    },
    {
      "epoch": 0.39334107626790554,
      "grad_norm": 7.263525485992432,
      "learning_rate": 3.9334107626790555e-06,
      "loss": 1.7227,
      "step": 1016
    },
    {
      "epoch": 0.39372822299651566,
      "grad_norm": 8.393779754638672,
      "learning_rate": 3.937282229965157e-06,
      "loss": 1.8701,
      "step": 1017
    },
    {
      "epoch": 0.39411536972512584,
      "grad_norm": 7.498469829559326,
      "learning_rate": 3.9411536972512585e-06,
      "loss": 1.9127,
      "step": 1018
    },
    {
      "epoch": 0.39450251645373596,
      "grad_norm": 7.705573558807373,
      "learning_rate": 3.94502516453736e-06,
      "loss": 1.8794,
      "step": 1019
    },
    {
      "epoch": 0.3948896631823461,
      "grad_norm": 5.393834114074707,
      "learning_rate": 3.9488966318234615e-06,
      "loss": 1.776,
      "step": 1020
    },
    {
      "epoch": 0.39527680991095626,
      "grad_norm": 6.81558895111084,
      "learning_rate": 3.952768099109563e-06,
      "loss": 1.9436,
      "step": 1021
    },
    {
      "epoch": 0.3956639566395664,
      "grad_norm": 10.730734825134277,
      "learning_rate": 3.9566395663956644e-06,
      "loss": 1.9126,
      "step": 1022
    },
    {
      "epoch": 0.39605110336817656,
      "grad_norm": 4.824930667877197,
      "learning_rate": 3.960511033681766e-06,
      "loss": 1.9485,
      "step": 1023
    },
    {
      "epoch": 0.3964382500967867,
      "grad_norm": 7.289262771606445,
      "learning_rate": 3.9643825009678674e-06,
      "loss": 1.7659,
      "step": 1024
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 8.324384689331055,
      "learning_rate": 3.968253968253968e-06,
      "loss": 1.822,
      "step": 1025
    },
    {
      "epoch": 0.397212543554007,
      "grad_norm": 7.1331682205200195,
      "learning_rate": 3.97212543554007e-06,
      "loss": 1.7658,
      "step": 1026
    },
    {
      "epoch": 0.3975996902826171,
      "grad_norm": 6.0450873374938965,
      "learning_rate": 3.975996902826171e-06,
      "loss": 1.9733,
      "step": 1027
    },
    {
      "epoch": 0.3979868370112273,
      "grad_norm": 8.693906784057617,
      "learning_rate": 3.979868370112273e-06,
      "loss": 2.041,
      "step": 1028
    },
    {
      "epoch": 0.3983739837398374,
      "grad_norm": 6.79072904586792,
      "learning_rate": 3.983739837398374e-06,
      "loss": 1.8285,
      "step": 1029
    },
    {
      "epoch": 0.3987611304684475,
      "grad_norm": 6.792149066925049,
      "learning_rate": 3.9876113046844755e-06,
      "loss": 1.6992,
      "step": 1030
    },
    {
      "epoch": 0.3991482771970577,
      "grad_norm": 10.180524826049805,
      "learning_rate": 3.991482771970577e-06,
      "loss": 1.7537,
      "step": 1031
    },
    {
      "epoch": 0.3995354239256678,
      "grad_norm": 6.90080451965332,
      "learning_rate": 3.9953542392566785e-06,
      "loss": 1.8543,
      "step": 1032
    },
    {
      "epoch": 0.399922570654278,
      "grad_norm": 8.782181739807129,
      "learning_rate": 3.99922570654278e-06,
      "loss": 1.9359,
      "step": 1033
    },
    {
      "epoch": 0.4003097173828881,
      "grad_norm": 4.588636875152588,
      "learning_rate": 4.0030971738288815e-06,
      "loss": 1.854,
      "step": 1034
    },
    {
      "epoch": 0.40069686411149824,
      "grad_norm": 5.694295406341553,
      "learning_rate": 4.006968641114983e-06,
      "loss": 1.9144,
      "step": 1035
    },
    {
      "epoch": 0.4010840108401084,
      "grad_norm": 5.34428071975708,
      "learning_rate": 4.0108401084010845e-06,
      "loss": 1.9145,
      "step": 1036
    },
    {
      "epoch": 0.40147115756871854,
      "grad_norm": 8.83871078491211,
      "learning_rate": 4.014711575687186e-06,
      "loss": 1.6739,
      "step": 1037
    },
    {
      "epoch": 0.40185830429732866,
      "grad_norm": 5.622236251831055,
      "learning_rate": 4.018583042973287e-06,
      "loss": 1.8569,
      "step": 1038
    },
    {
      "epoch": 0.40224545102593884,
      "grad_norm": 9.09410285949707,
      "learning_rate": 4.022454510259389e-06,
      "loss": 1.7797,
      "step": 1039
    },
    {
      "epoch": 0.40263259775454896,
      "grad_norm": 6.6376495361328125,
      "learning_rate": 4.02632597754549e-06,
      "loss": 1.9282,
      "step": 1040
    },
    {
      "epoch": 0.40301974448315914,
      "grad_norm": 6.3174028396606445,
      "learning_rate": 4.030197444831592e-06,
      "loss": 1.8154,
      "step": 1041
    },
    {
      "epoch": 0.40340689121176926,
      "grad_norm": 5.422191143035889,
      "learning_rate": 4.034068912117693e-06,
      "loss": 1.7916,
      "step": 1042
    },
    {
      "epoch": 0.4037940379403794,
      "grad_norm": 7.130708694458008,
      "learning_rate": 4.037940379403794e-06,
      "loss": 1.7154,
      "step": 1043
    },
    {
      "epoch": 0.40418118466898956,
      "grad_norm": 7.735983371734619,
      "learning_rate": 4.041811846689896e-06,
      "loss": 1.9298,
      "step": 1044
    },
    {
      "epoch": 0.4045683313975997,
      "grad_norm": 7.1909990310668945,
      "learning_rate": 4.045683313975997e-06,
      "loss": 1.7682,
      "step": 1045
    },
    {
      "epoch": 0.40495547812620986,
      "grad_norm": 5.878539562225342,
      "learning_rate": 4.049554781262099e-06,
      "loss": 1.8125,
      "step": 1046
    },
    {
      "epoch": 0.40534262485482,
      "grad_norm": 5.125941276550293,
      "learning_rate": 4.0534262485482e-06,
      "loss": 1.9141,
      "step": 1047
    },
    {
      "epoch": 0.4057297715834301,
      "grad_norm": 7.789982795715332,
      "learning_rate": 4.057297715834302e-06,
      "loss": 1.7673,
      "step": 1048
    },
    {
      "epoch": 0.4061169183120403,
      "grad_norm": 6.927610397338867,
      "learning_rate": 4.061169183120403e-06,
      "loss": 1.7171,
      "step": 1049
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 7.074440956115723,
      "learning_rate": 4.0650406504065046e-06,
      "loss": 1.8205,
      "step": 1050
    },
    {
      "epoch": 0.4068912117692606,
      "grad_norm": 10.519023895263672,
      "learning_rate": 4.068912117692606e-06,
      "loss": 1.9074,
      "step": 1051
    },
    {
      "epoch": 0.4072783584978707,
      "grad_norm": 6.531515121459961,
      "learning_rate": 4.0727835849787076e-06,
      "loss": 1.777,
      "step": 1052
    },
    {
      "epoch": 0.4076655052264808,
      "grad_norm": 5.757457733154297,
      "learning_rate": 4.076655052264808e-06,
      "loss": 1.824,
      "step": 1053
    },
    {
      "epoch": 0.408052651955091,
      "grad_norm": 5.75399923324585,
      "learning_rate": 4.0805265195509105e-06,
      "loss": 1.8079,
      "step": 1054
    },
    {
      "epoch": 0.4084397986837011,
      "grad_norm": 11.61396598815918,
      "learning_rate": 4.084397986837011e-06,
      "loss": 2.0023,
      "step": 1055
    },
    {
      "epoch": 0.4088269454123113,
      "grad_norm": 7.138791084289551,
      "learning_rate": 4.0882694541231135e-06,
      "loss": 1.8936,
      "step": 1056
    },
    {
      "epoch": 0.4092140921409214,
      "grad_norm": 7.854433536529541,
      "learning_rate": 4.092140921409214e-06,
      "loss": 1.8226,
      "step": 1057
    },
    {
      "epoch": 0.40960123886953154,
      "grad_norm": 11.446723937988281,
      "learning_rate": 4.096012388695316e-06,
      "loss": 2.0463,
      "step": 1058
    },
    {
      "epoch": 0.4099883855981417,
      "grad_norm": 6.197576999664307,
      "learning_rate": 4.099883855981417e-06,
      "loss": 1.9028,
      "step": 1059
    },
    {
      "epoch": 0.41037553232675184,
      "grad_norm": 8.371200561523438,
      "learning_rate": 4.103755323267519e-06,
      "loss": 1.9448,
      "step": 1060
    },
    {
      "epoch": 0.41076267905536196,
      "grad_norm": 8.966072082519531,
      "learning_rate": 4.10762679055362e-06,
      "loss": 2.0768,
      "step": 1061
    },
    {
      "epoch": 0.41114982578397213,
      "grad_norm": 5.86836576461792,
      "learning_rate": 4.111498257839722e-06,
      "loss": 1.8453,
      "step": 1062
    },
    {
      "epoch": 0.41153697251258226,
      "grad_norm": 6.762277126312256,
      "learning_rate": 4.115369725125823e-06,
      "loss": 1.6987,
      "step": 1063
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 6.063173294067383,
      "learning_rate": 4.119241192411925e-06,
      "loss": 1.8231,
      "step": 1064
    },
    {
      "epoch": 0.41231126596980255,
      "grad_norm": 6.219878673553467,
      "learning_rate": 4.123112659698026e-06,
      "loss": 1.8949,
      "step": 1065
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 11.741094589233398,
      "learning_rate": 4.126984126984127e-06,
      "loss": 1.8869,
      "step": 1066
    },
    {
      "epoch": 0.41308555942702285,
      "grad_norm": 9.171361923217773,
      "learning_rate": 4.130855594270229e-06,
      "loss": 2.1046,
      "step": 1067
    },
    {
      "epoch": 0.413472706155633,
      "grad_norm": 7.2616729736328125,
      "learning_rate": 4.13472706155633e-06,
      "loss": 1.8042,
      "step": 1068
    },
    {
      "epoch": 0.41385985288424315,
      "grad_norm": 7.731119632720947,
      "learning_rate": 4.138598528842432e-06,
      "loss": 1.833,
      "step": 1069
    },
    {
      "epoch": 0.4142469996128533,
      "grad_norm": 7.547011852264404,
      "learning_rate": 4.142469996128533e-06,
      "loss": 1.8938,
      "step": 1070
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 7.011690616607666,
      "learning_rate": 4.146341463414634e-06,
      "loss": 1.836,
      "step": 1071
    },
    {
      "epoch": 0.41502129307007357,
      "grad_norm": 12.237669944763184,
      "learning_rate": 4.150212930700736e-06,
      "loss": 1.8363,
      "step": 1072
    },
    {
      "epoch": 0.4154084397986837,
      "grad_norm": 5.191030979156494,
      "learning_rate": 4.154084397986837e-06,
      "loss": 1.8278,
      "step": 1073
    },
    {
      "epoch": 0.41579558652729387,
      "grad_norm": 7.0229597091674805,
      "learning_rate": 4.157955865272939e-06,
      "loss": 1.6721,
      "step": 1074
    },
    {
      "epoch": 0.416182733255904,
      "grad_norm": 11.699090003967285,
      "learning_rate": 4.16182733255904e-06,
      "loss": 2.0986,
      "step": 1075
    },
    {
      "epoch": 0.4165698799845141,
      "grad_norm": 8.181157112121582,
      "learning_rate": 4.165698799845142e-06,
      "loss": 1.9333,
      "step": 1076
    },
    {
      "epoch": 0.4169570267131243,
      "grad_norm": 7.733731746673584,
      "learning_rate": 4.169570267131243e-06,
      "loss": 1.8594,
      "step": 1077
    },
    {
      "epoch": 0.4173441734417344,
      "grad_norm": 5.851437568664551,
      "learning_rate": 4.173441734417345e-06,
      "loss": 1.822,
      "step": 1078
    },
    {
      "epoch": 0.41773132017034453,
      "grad_norm": 8.455815315246582,
      "learning_rate": 4.177313201703445e-06,
      "loss": 1.895,
      "step": 1079
    },
    {
      "epoch": 0.4181184668989547,
      "grad_norm": 7.864645957946777,
      "learning_rate": 4.181184668989548e-06,
      "loss": 1.7781,
      "step": 1080
    },
    {
      "epoch": 0.41850561362756483,
      "grad_norm": 8.117196083068848,
      "learning_rate": 4.185056136275648e-06,
      "loss": 1.8597,
      "step": 1081
    },
    {
      "epoch": 0.418892760356175,
      "grad_norm": 5.850574970245361,
      "learning_rate": 4.188927603561751e-06,
      "loss": 1.844,
      "step": 1082
    },
    {
      "epoch": 0.41927990708478513,
      "grad_norm": 7.6419525146484375,
      "learning_rate": 4.192799070847851e-06,
      "loss": 1.7653,
      "step": 1083
    },
    {
      "epoch": 0.41966705381339525,
      "grad_norm": 6.343409538269043,
      "learning_rate": 4.196670538133953e-06,
      "loss": 1.7293,
      "step": 1084
    },
    {
      "epoch": 0.42005420054200543,
      "grad_norm": 5.983706951141357,
      "learning_rate": 4.200542005420054e-06,
      "loss": 1.802,
      "step": 1085
    },
    {
      "epoch": 0.42044134727061555,
      "grad_norm": 7.5377092361450195,
      "learning_rate": 4.204413472706156e-06,
      "loss": 1.8169,
      "step": 1086
    },
    {
      "epoch": 0.42082849399922573,
      "grad_norm": 3.8462092876434326,
      "learning_rate": 4.208284939992257e-06,
      "loss": 1.8385,
      "step": 1087
    },
    {
      "epoch": 0.42121564072783585,
      "grad_norm": 5.379147529602051,
      "learning_rate": 4.212156407278359e-06,
      "loss": 1.7416,
      "step": 1088
    },
    {
      "epoch": 0.42160278745644597,
      "grad_norm": 7.648809432983398,
      "learning_rate": 4.21602787456446e-06,
      "loss": 1.8885,
      "step": 1089
    },
    {
      "epoch": 0.42198993418505615,
      "grad_norm": 7.084758758544922,
      "learning_rate": 4.219899341850562e-06,
      "loss": 1.8204,
      "step": 1090
    },
    {
      "epoch": 0.42237708091366627,
      "grad_norm": 8.979840278625488,
      "learning_rate": 4.223770809136663e-06,
      "loss": 2.0371,
      "step": 1091
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 5.410975456237793,
      "learning_rate": 4.227642276422765e-06,
      "loss": 1.8007,
      "step": 1092
    },
    {
      "epoch": 0.42315137437088657,
      "grad_norm": 6.89345645904541,
      "learning_rate": 4.231513743708866e-06,
      "loss": 1.8088,
      "step": 1093
    },
    {
      "epoch": 0.4235385210994967,
      "grad_norm": 5.9134521484375,
      "learning_rate": 4.235385210994967e-06,
      "loss": 1.7834,
      "step": 1094
    },
    {
      "epoch": 0.42392566782810687,
      "grad_norm": 5.889510631561279,
      "learning_rate": 4.239256678281069e-06,
      "loss": 1.7182,
      "step": 1095
    },
    {
      "epoch": 0.424312814556717,
      "grad_norm": 7.260437965393066,
      "learning_rate": 4.24312814556717e-06,
      "loss": 1.7903,
      "step": 1096
    },
    {
      "epoch": 0.42469996128532717,
      "grad_norm": 7.3325419425964355,
      "learning_rate": 4.246999612853272e-06,
      "loss": 1.7872,
      "step": 1097
    },
    {
      "epoch": 0.4250871080139373,
      "grad_norm": 6.902414321899414,
      "learning_rate": 4.250871080139373e-06,
      "loss": 1.8061,
      "step": 1098
    },
    {
      "epoch": 0.4254742547425474,
      "grad_norm": 9.584883689880371,
      "learning_rate": 4.254742547425474e-06,
      "loss": 1.7024,
      "step": 1099
    },
    {
      "epoch": 0.4258614014711576,
      "grad_norm": 7.092770099639893,
      "learning_rate": 4.258614014711576e-06,
      "loss": 1.8645,
      "step": 1100
    },
    {
      "epoch": 0.4262485481997677,
      "grad_norm": 10.605938911437988,
      "learning_rate": 4.262485481997677e-06,
      "loss": 2.0842,
      "step": 1101
    },
    {
      "epoch": 0.42663569492837783,
      "grad_norm": 7.272816181182861,
      "learning_rate": 4.266356949283779e-06,
      "loss": 1.7644,
      "step": 1102
    },
    {
      "epoch": 0.427022841656988,
      "grad_norm": 7.227027893066406,
      "learning_rate": 4.27022841656988e-06,
      "loss": 1.8143,
      "step": 1103
    },
    {
      "epoch": 0.4274099883855981,
      "grad_norm": 6.540299415588379,
      "learning_rate": 4.274099883855982e-06,
      "loss": 1.8091,
      "step": 1104
    },
    {
      "epoch": 0.4277971351142083,
      "grad_norm": 7.267534255981445,
      "learning_rate": 4.277971351142083e-06,
      "loss": 1.8152,
      "step": 1105
    },
    {
      "epoch": 0.4281842818428184,
      "grad_norm": 7.487703323364258,
      "learning_rate": 4.281842818428185e-06,
      "loss": 1.7344,
      "step": 1106
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.099993705749512,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 1.7852,
      "step": 1107
    },
    {
      "epoch": 0.4289585753000387,
      "grad_norm": 7.036769866943359,
      "learning_rate": 4.289585753000388e-06,
      "loss": 1.8193,
      "step": 1108
    },
    {
      "epoch": 0.42934572202864885,
      "grad_norm": 7.88557767868042,
      "learning_rate": 4.2934572202864884e-06,
      "loss": 1.7807,
      "step": 1109
    },
    {
      "epoch": 0.429732868757259,
      "grad_norm": 6.234097480773926,
      "learning_rate": 4.297328687572591e-06,
      "loss": 1.7211,
      "step": 1110
    },
    {
      "epoch": 0.43012001548586914,
      "grad_norm": 12.439189910888672,
      "learning_rate": 4.3012001548586914e-06,
      "loss": 1.6537,
      "step": 1111
    },
    {
      "epoch": 0.43050716221447927,
      "grad_norm": 8.169246673583984,
      "learning_rate": 4.305071622144793e-06,
      "loss": 1.8592,
      "step": 1112
    },
    {
      "epoch": 0.43089430894308944,
      "grad_norm": 4.973944187164307,
      "learning_rate": 4.308943089430894e-06,
      "loss": 1.8289,
      "step": 1113
    },
    {
      "epoch": 0.43128145567169957,
      "grad_norm": 8.851553916931152,
      "learning_rate": 4.312814556716996e-06,
      "loss": 1.9104,
      "step": 1114
    },
    {
      "epoch": 0.43166860240030974,
      "grad_norm": 8.162639617919922,
      "learning_rate": 4.316686024003097e-06,
      "loss": 1.7266,
      "step": 1115
    },
    {
      "epoch": 0.43205574912891986,
      "grad_norm": 8.516408920288086,
      "learning_rate": 4.320557491289199e-06,
      "loss": 1.8425,
      "step": 1116
    },
    {
      "epoch": 0.43244289585753,
      "grad_norm": 8.054035186767578,
      "learning_rate": 4.3244289585753e-06,
      "loss": 1.6528,
      "step": 1117
    },
    {
      "epoch": 0.43283004258614016,
      "grad_norm": 8.632983207702637,
      "learning_rate": 4.328300425861402e-06,
      "loss": 1.7698,
      "step": 1118
    },
    {
      "epoch": 0.4332171893147503,
      "grad_norm": 7.768477439880371,
      "learning_rate": 4.332171893147503e-06,
      "loss": 1.7886,
      "step": 1119
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 7.118156433105469,
      "learning_rate": 4.336043360433605e-06,
      "loss": 1.8985,
      "step": 1120
    },
    {
      "epoch": 0.4339914827719706,
      "grad_norm": 8.817588806152344,
      "learning_rate": 4.339914827719706e-06,
      "loss": 1.7758,
      "step": 1121
    },
    {
      "epoch": 0.4343786295005807,
      "grad_norm": 6.1306962966918945,
      "learning_rate": 4.343786295005807e-06,
      "loss": 1.7747,
      "step": 1122
    },
    {
      "epoch": 0.4347657762291909,
      "grad_norm": 8.011433601379395,
      "learning_rate": 4.347657762291909e-06,
      "loss": 1.822,
      "step": 1123
    },
    {
      "epoch": 0.435152922957801,
      "grad_norm": 7.646282196044922,
      "learning_rate": 4.35152922957801e-06,
      "loss": 1.7612,
      "step": 1124
    },
    {
      "epoch": 0.4355400696864111,
      "grad_norm": 6.916039943695068,
      "learning_rate": 4.3554006968641115e-06,
      "loss": 1.7448,
      "step": 1125
    },
    {
      "epoch": 0.4359272164150213,
      "grad_norm": 8.355910301208496,
      "learning_rate": 4.359272164150213e-06,
      "loss": 1.8102,
      "step": 1126
    },
    {
      "epoch": 0.4363143631436314,
      "grad_norm": 9.373740196228027,
      "learning_rate": 4.3631436314363145e-06,
      "loss": 1.8227,
      "step": 1127
    },
    {
      "epoch": 0.4367015098722416,
      "grad_norm": 8.126326560974121,
      "learning_rate": 4.367015098722416e-06,
      "loss": 1.8974,
      "step": 1128
    },
    {
      "epoch": 0.4370886566008517,
      "grad_norm": 7.3929033279418945,
      "learning_rate": 4.3708865660085175e-06,
      "loss": 1.7919,
      "step": 1129
    },
    {
      "epoch": 0.43747580332946184,
      "grad_norm": 7.894052505493164,
      "learning_rate": 4.374758033294619e-06,
      "loss": 1.9394,
      "step": 1130
    },
    {
      "epoch": 0.437862950058072,
      "grad_norm": 7.260898590087891,
      "learning_rate": 4.3786295005807205e-06,
      "loss": 1.9232,
      "step": 1131
    },
    {
      "epoch": 0.43825009678668214,
      "grad_norm": 6.294471263885498,
      "learning_rate": 4.382500967866822e-06,
      "loss": 1.7289,
      "step": 1132
    },
    {
      "epoch": 0.4386372435152923,
      "grad_norm": 7.442167282104492,
      "learning_rate": 4.3863724351529234e-06,
      "loss": 1.702,
      "step": 1133
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 10.481618881225586,
      "learning_rate": 4.390243902439025e-06,
      "loss": 1.8236,
      "step": 1134
    },
    {
      "epoch": 0.43941153697251256,
      "grad_norm": 10.772294998168945,
      "learning_rate": 4.394115369725126e-06,
      "loss": 1.9302,
      "step": 1135
    },
    {
      "epoch": 0.43979868370112274,
      "grad_norm": 9.008394241333008,
      "learning_rate": 4.397986837011228e-06,
      "loss": 1.8349,
      "step": 1136
    },
    {
      "epoch": 0.44018583042973286,
      "grad_norm": 10.050360679626465,
      "learning_rate": 4.4018583042973286e-06,
      "loss": 1.9866,
      "step": 1137
    },
    {
      "epoch": 0.44057297715834304,
      "grad_norm": 6.3553080558776855,
      "learning_rate": 4.405729771583431e-06,
      "loss": 1.8056,
      "step": 1138
    },
    {
      "epoch": 0.44096012388695316,
      "grad_norm": 10.872370719909668,
      "learning_rate": 4.4096012388695316e-06,
      "loss": 1.8464,
      "step": 1139
    },
    {
      "epoch": 0.4413472706155633,
      "grad_norm": 6.178465366363525,
      "learning_rate": 4.413472706155633e-06,
      "loss": 1.8874,
      "step": 1140
    },
    {
      "epoch": 0.44173441734417346,
      "grad_norm": 5.847365379333496,
      "learning_rate": 4.4173441734417345e-06,
      "loss": 1.7468,
      "step": 1141
    },
    {
      "epoch": 0.4421215640727836,
      "grad_norm": 5.362544059753418,
      "learning_rate": 4.421215640727836e-06,
      "loss": 1.8735,
      "step": 1142
    },
    {
      "epoch": 0.4425087108013937,
      "grad_norm": 7.363213539123535,
      "learning_rate": 4.4250871080139375e-06,
      "loss": 1.7748,
      "step": 1143
    },
    {
      "epoch": 0.4428958575300039,
      "grad_norm": 7.650078296661377,
      "learning_rate": 4.428958575300039e-06,
      "loss": 1.7209,
      "step": 1144
    },
    {
      "epoch": 0.443283004258614,
      "grad_norm": 9.157793045043945,
      "learning_rate": 4.4328300425861405e-06,
      "loss": 1.8174,
      "step": 1145
    },
    {
      "epoch": 0.4436701509872242,
      "grad_norm": 8.010568618774414,
      "learning_rate": 4.436701509872242e-06,
      "loss": 1.6716,
      "step": 1146
    },
    {
      "epoch": 0.4440572977158343,
      "grad_norm": 7.896255016326904,
      "learning_rate": 4.4405729771583435e-06,
      "loss": 1.9857,
      "step": 1147
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 7.9335432052612305,
      "learning_rate": 4.444444444444444e-06,
      "loss": 1.7966,
      "step": 1148
    },
    {
      "epoch": 0.4448315911730546,
      "grad_norm": 6.553643226623535,
      "learning_rate": 4.4483159117305465e-06,
      "loss": 1.8011,
      "step": 1149
    },
    {
      "epoch": 0.4452187379016647,
      "grad_norm": 7.421691417694092,
      "learning_rate": 4.452187379016647e-06,
      "loss": 1.6947,
      "step": 1150
    },
    {
      "epoch": 0.4456058846302749,
      "grad_norm": 5.758768558502197,
      "learning_rate": 4.4560588463027495e-06,
      "loss": 1.8463,
      "step": 1151
    },
    {
      "epoch": 0.445993031358885,
      "grad_norm": 8.436195373535156,
      "learning_rate": 4.45993031358885e-06,
      "loss": 1.7883,
      "step": 1152
    },
    {
      "epoch": 0.44638017808749514,
      "grad_norm": 6.468707084655762,
      "learning_rate": 4.463801780874952e-06,
      "loss": 1.8275,
      "step": 1153
    },
    {
      "epoch": 0.4467673248161053,
      "grad_norm": 5.621915340423584,
      "learning_rate": 4.467673248161053e-06,
      "loss": 1.8272,
      "step": 1154
    },
    {
      "epoch": 0.44715447154471544,
      "grad_norm": 6.155309677124023,
      "learning_rate": 4.471544715447155e-06,
      "loss": 1.8239,
      "step": 1155
    },
    {
      "epoch": 0.4475416182733256,
      "grad_norm": 16.646562576293945,
      "learning_rate": 4.475416182733256e-06,
      "loss": 1.8473,
      "step": 1156
    },
    {
      "epoch": 0.44792876500193574,
      "grad_norm": 10.270953178405762,
      "learning_rate": 4.479287650019358e-06,
      "loss": 2.0731,
      "step": 1157
    },
    {
      "epoch": 0.44831591173054586,
      "grad_norm": 7.724088668823242,
      "learning_rate": 4.483159117305459e-06,
      "loss": 1.8746,
      "step": 1158
    },
    {
      "epoch": 0.44870305845915603,
      "grad_norm": 7.1062140464782715,
      "learning_rate": 4.487030584591561e-06,
      "loss": 1.7333,
      "step": 1159
    },
    {
      "epoch": 0.44909020518776616,
      "grad_norm": 7.381711006164551,
      "learning_rate": 4.490902051877662e-06,
      "loss": 1.7291,
      "step": 1160
    },
    {
      "epoch": 0.44947735191637633,
      "grad_norm": 10.362436294555664,
      "learning_rate": 4.4947735191637636e-06,
      "loss": 1.9623,
      "step": 1161
    },
    {
      "epoch": 0.44986449864498645,
      "grad_norm": 10.222472190856934,
      "learning_rate": 4.498644986449865e-06,
      "loss": 1.6973,
      "step": 1162
    },
    {
      "epoch": 0.4502516453735966,
      "grad_norm": 7.345465183258057,
      "learning_rate": 4.5025164537359666e-06,
      "loss": 1.8314,
      "step": 1163
    },
    {
      "epoch": 0.45063879210220675,
      "grad_norm": 8.868804931640625,
      "learning_rate": 4.506387921022068e-06,
      "loss": 1.7175,
      "step": 1164
    },
    {
      "epoch": 0.4510259388308169,
      "grad_norm": 6.456099033355713,
      "learning_rate": 4.5102593883081695e-06,
      "loss": 1.8393,
      "step": 1165
    },
    {
      "epoch": 0.451413085559427,
      "grad_norm": 8.33167839050293,
      "learning_rate": 4.51413085559427e-06,
      "loss": 1.9257,
      "step": 1166
    },
    {
      "epoch": 0.4518002322880372,
      "grad_norm": 8.079118728637695,
      "learning_rate": 4.5180023228803725e-06,
      "loss": 1.6983,
      "step": 1167
    },
    {
      "epoch": 0.4521873790166473,
      "grad_norm": 6.416520595550537,
      "learning_rate": 4.521873790166473e-06,
      "loss": 1.7987,
      "step": 1168
    },
    {
      "epoch": 0.45257452574525747,
      "grad_norm": 8.079507827758789,
      "learning_rate": 4.5257452574525755e-06,
      "loss": 1.8103,
      "step": 1169
    },
    {
      "epoch": 0.4529616724738676,
      "grad_norm": 14.616253852844238,
      "learning_rate": 4.529616724738676e-06,
      "loss": 2.0165,
      "step": 1170
    },
    {
      "epoch": 0.4533488192024777,
      "grad_norm": 10.729535102844238,
      "learning_rate": 4.533488192024778e-06,
      "loss": 1.8941,
      "step": 1171
    },
    {
      "epoch": 0.4537359659310879,
      "grad_norm": 6.862905502319336,
      "learning_rate": 4.537359659310879e-06,
      "loss": 1.7868,
      "step": 1172
    },
    {
      "epoch": 0.454123112659698,
      "grad_norm": 7.891745567321777,
      "learning_rate": 4.541231126596981e-06,
      "loss": 1.7899,
      "step": 1173
    },
    {
      "epoch": 0.4545102593883082,
      "grad_norm": 7.428366661071777,
      "learning_rate": 4.545102593883082e-06,
      "loss": 1.718,
      "step": 1174
    },
    {
      "epoch": 0.4548974061169183,
      "grad_norm": 9.85200023651123,
      "learning_rate": 4.548974061169184e-06,
      "loss": 1.7185,
      "step": 1175
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 7.832864761352539,
      "learning_rate": 4.552845528455285e-06,
      "loss": 1.7691,
      "step": 1176
    },
    {
      "epoch": 0.4556716995741386,
      "grad_norm": 8.443107604980469,
      "learning_rate": 4.556716995741387e-06,
      "loss": 2.232,
      "step": 1177
    },
    {
      "epoch": 0.45605884630274873,
      "grad_norm": 8.465107917785645,
      "learning_rate": 4.560588463027488e-06,
      "loss": 1.8328,
      "step": 1178
    },
    {
      "epoch": 0.4564459930313589,
      "grad_norm": 7.087314128875732,
      "learning_rate": 4.56445993031359e-06,
      "loss": 1.7479,
      "step": 1179
    },
    {
      "epoch": 0.45683313975996903,
      "grad_norm": 7.591129779815674,
      "learning_rate": 4.568331397599691e-06,
      "loss": 1.7046,
      "step": 1180
    },
    {
      "epoch": 0.45722028648857915,
      "grad_norm": 5.6059651374816895,
      "learning_rate": 4.572202864885792e-06,
      "loss": 1.7802,
      "step": 1181
    },
    {
      "epoch": 0.45760743321718933,
      "grad_norm": 8.291529655456543,
      "learning_rate": 4.576074332171894e-06,
      "loss": 1.8031,
      "step": 1182
    },
    {
      "epoch": 0.45799457994579945,
      "grad_norm": 7.791135311126709,
      "learning_rate": 4.579945799457995e-06,
      "loss": 1.8618,
      "step": 1183
    },
    {
      "epoch": 0.45838172667440963,
      "grad_norm": 7.984649658203125,
      "learning_rate": 4.583817266744097e-06,
      "loss": 1.7971,
      "step": 1184
    },
    {
      "epoch": 0.45876887340301975,
      "grad_norm": 11.637068748474121,
      "learning_rate": 4.587688734030198e-06,
      "loss": 2.0359,
      "step": 1185
    },
    {
      "epoch": 0.45915602013162987,
      "grad_norm": 11.001853942871094,
      "learning_rate": 4.591560201316299e-06,
      "loss": 1.8609,
      "step": 1186
    },
    {
      "epoch": 0.45954316686024005,
      "grad_norm": 11.171842575073242,
      "learning_rate": 4.595431668602401e-06,
      "loss": 1.7055,
      "step": 1187
    },
    {
      "epoch": 0.45993031358885017,
      "grad_norm": 5.497411251068115,
      "learning_rate": 4.599303135888502e-06,
      "loss": 1.8262,
      "step": 1188
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 6.827698230743408,
      "learning_rate": 4.603174603174604e-06,
      "loss": 1.7899,
      "step": 1189
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 9.068109512329102,
      "learning_rate": 4.607046070460705e-06,
      "loss": 1.8165,
      "step": 1190
    },
    {
      "epoch": 0.4610917537746806,
      "grad_norm": 8.095277786254883,
      "learning_rate": 4.610917537746807e-06,
      "loss": 1.756,
      "step": 1191
    },
    {
      "epoch": 0.46147890050329077,
      "grad_norm": 10.41849136352539,
      "learning_rate": 4.614789005032908e-06,
      "loss": 1.7962,
      "step": 1192
    },
    {
      "epoch": 0.4618660472319009,
      "grad_norm": 7.548772811889648,
      "learning_rate": 4.61866047231901e-06,
      "loss": 1.7278,
      "step": 1193
    },
    {
      "epoch": 0.462253193960511,
      "grad_norm": 6.841695785522461,
      "learning_rate": 4.62253193960511e-06,
      "loss": 1.7784,
      "step": 1194
    },
    {
      "epoch": 0.4626403406891212,
      "grad_norm": 8.158512115478516,
      "learning_rate": 4.626403406891213e-06,
      "loss": 1.8571,
      "step": 1195
    },
    {
      "epoch": 0.4630274874177313,
      "grad_norm": 5.224759578704834,
      "learning_rate": 4.630274874177313e-06,
      "loss": 1.8073,
      "step": 1196
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 7.086298942565918,
      "learning_rate": 4.634146341463416e-06,
      "loss": 1.6694,
      "step": 1197
    },
    {
      "epoch": 0.4638017808749516,
      "grad_norm": 7.74200963973999,
      "learning_rate": 4.638017808749516e-06,
      "loss": 1.7286,
      "step": 1198
    },
    {
      "epoch": 0.46418892760356173,
      "grad_norm": 9.289406776428223,
      "learning_rate": 4.641889276035618e-06,
      "loss": 1.6876,
      "step": 1199
    },
    {
      "epoch": 0.4645760743321719,
      "grad_norm": 6.77566385269165,
      "learning_rate": 4.645760743321719e-06,
      "loss": 1.7614,
      "step": 1200
    },
    {
      "epoch": 0.46496322106078203,
      "grad_norm": 7.472706317901611,
      "learning_rate": 4.649632210607821e-06,
      "loss": 1.8944,
      "step": 1201
    },
    {
      "epoch": 0.4653503677893922,
      "grad_norm": 9.37161636352539,
      "learning_rate": 4.653503677893922e-06,
      "loss": 1.7653,
      "step": 1202
    },
    {
      "epoch": 0.4657375145180023,
      "grad_norm": 8.697610855102539,
      "learning_rate": 4.657375145180024e-06,
      "loss": 1.7986,
      "step": 1203
    },
    {
      "epoch": 0.46612466124661245,
      "grad_norm": 9.745418548583984,
      "learning_rate": 4.661246612466125e-06,
      "loss": 2.2016,
      "step": 1204
    },
    {
      "epoch": 0.4665118079752226,
      "grad_norm": 7.772027015686035,
      "learning_rate": 4.665118079752227e-06,
      "loss": 1.8722,
      "step": 1205
    },
    {
      "epoch": 0.46689895470383275,
      "grad_norm": 8.565866470336914,
      "learning_rate": 4.668989547038328e-06,
      "loss": 1.7909,
      "step": 1206
    },
    {
      "epoch": 0.46728610143244287,
      "grad_norm": 5.965920925140381,
      "learning_rate": 4.672861014324429e-06,
      "loss": 1.76,
      "step": 1207
    },
    {
      "epoch": 0.46767324816105305,
      "grad_norm": 11.530014991760254,
      "learning_rate": 4.676732481610531e-06,
      "loss": 1.8608,
      "step": 1208
    },
    {
      "epoch": 0.46806039488966317,
      "grad_norm": 6.076766490936279,
      "learning_rate": 4.680603948896632e-06,
      "loss": 1.7292,
      "step": 1209
    },
    {
      "epoch": 0.46844754161827334,
      "grad_norm": 8.431111335754395,
      "learning_rate": 4.684475416182734e-06,
      "loss": 1.7818,
      "step": 1210
    },
    {
      "epoch": 0.46883468834688347,
      "grad_norm": 7.297515869140625,
      "learning_rate": 4.688346883468835e-06,
      "loss": 1.7175,
      "step": 1211
    },
    {
      "epoch": 0.4692218350754936,
      "grad_norm": 12.616189956665039,
      "learning_rate": 4.692218350754936e-06,
      "loss": 1.8794,
      "step": 1212
    },
    {
      "epoch": 0.46960898180410376,
      "grad_norm": 6.422956466674805,
      "learning_rate": 4.696089818041038e-06,
      "loss": 1.6784,
      "step": 1213
    },
    {
      "epoch": 0.4699961285327139,
      "grad_norm": 9.145773887634277,
      "learning_rate": 4.699961285327139e-06,
      "loss": 1.8095,
      "step": 1214
    },
    {
      "epoch": 0.47038327526132406,
      "grad_norm": 9.226818084716797,
      "learning_rate": 4.703832752613241e-06,
      "loss": 1.7219,
      "step": 1215
    },
    {
      "epoch": 0.4707704219899342,
      "grad_norm": 6.9555158615112305,
      "learning_rate": 4.707704219899342e-06,
      "loss": 1.7958,
      "step": 1216
    },
    {
      "epoch": 0.4711575687185443,
      "grad_norm": 8.154215812683105,
      "learning_rate": 4.711575687185444e-06,
      "loss": 1.8573,
      "step": 1217
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 5.027792453765869,
      "learning_rate": 4.715447154471545e-06,
      "loss": 1.7501,
      "step": 1218
    },
    {
      "epoch": 0.4719318621757646,
      "grad_norm": 7.6340413093566895,
      "learning_rate": 4.719318621757647e-06,
      "loss": 1.8114,
      "step": 1219
    },
    {
      "epoch": 0.4723190089043748,
      "grad_norm": 8.009791374206543,
      "learning_rate": 4.723190089043748e-06,
      "loss": 1.7774,
      "step": 1220
    },
    {
      "epoch": 0.4727061556329849,
      "grad_norm": 5.859385967254639,
      "learning_rate": 4.72706155632985e-06,
      "loss": 1.7764,
      "step": 1221
    },
    {
      "epoch": 0.473093302361595,
      "grad_norm": 6.946610450744629,
      "learning_rate": 4.7309330236159504e-06,
      "loss": 1.754,
      "step": 1222
    },
    {
      "epoch": 0.4734804490902052,
      "grad_norm": 9.399944305419922,
      "learning_rate": 4.734804490902053e-06,
      "loss": 1.8098,
      "step": 1223
    },
    {
      "epoch": 0.4738675958188153,
      "grad_norm": 6.630936145782471,
      "learning_rate": 4.738675958188153e-06,
      "loss": 1.8501,
      "step": 1224
    },
    {
      "epoch": 0.4742547425474255,
      "grad_norm": 11.695547103881836,
      "learning_rate": 4.742547425474256e-06,
      "loss": 1.6851,
      "step": 1225
    },
    {
      "epoch": 0.4746418892760356,
      "grad_norm": 8.159245491027832,
      "learning_rate": 4.746418892760356e-06,
      "loss": 2.091,
      "step": 1226
    },
    {
      "epoch": 0.47502903600464574,
      "grad_norm": 6.82175350189209,
      "learning_rate": 4.750290360046458e-06,
      "loss": 1.8104,
      "step": 1227
    },
    {
      "epoch": 0.4754161827332559,
      "grad_norm": 6.956649303436279,
      "learning_rate": 4.754161827332559e-06,
      "loss": 1.7584,
      "step": 1228
    },
    {
      "epoch": 0.47580332946186604,
      "grad_norm": 8.740132331848145,
      "learning_rate": 4.758033294618661e-06,
      "loss": 1.8276,
      "step": 1229
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 6.581330299377441,
      "learning_rate": 4.761904761904762e-06,
      "loss": 1.7654,
      "step": 1230
    },
    {
      "epoch": 0.47657762291908634,
      "grad_norm": 6.841709136962891,
      "learning_rate": 4.765776229190864e-06,
      "loss": 1.8021,
      "step": 1231
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 9.53692626953125,
      "learning_rate": 4.769647696476965e-06,
      "loss": 1.8709,
      "step": 1232
    },
    {
      "epoch": 0.47735191637630664,
      "grad_norm": 28.54514503479004,
      "learning_rate": 4.773519163763067e-06,
      "loss": 2.039,
      "step": 1233
    },
    {
      "epoch": 0.47773906310491676,
      "grad_norm": 6.897552490234375,
      "learning_rate": 4.777390631049168e-06,
      "loss": 1.762,
      "step": 1234
    },
    {
      "epoch": 0.4781262098335269,
      "grad_norm": 8.126384735107422,
      "learning_rate": 4.781262098335269e-06,
      "loss": 2.1001,
      "step": 1235
    },
    {
      "epoch": 0.47851335656213706,
      "grad_norm": 11.268472671508789,
      "learning_rate": 4.785133565621371e-06,
      "loss": 1.6657,
      "step": 1236
    },
    {
      "epoch": 0.4789005032907472,
      "grad_norm": 6.6045613288879395,
      "learning_rate": 4.789005032907472e-06,
      "loss": 1.7459,
      "step": 1237
    },
    {
      "epoch": 0.47928765001935736,
      "grad_norm": 7.3218560218811035,
      "learning_rate": 4.792876500193574e-06,
      "loss": 1.7375,
      "step": 1238
    },
    {
      "epoch": 0.4796747967479675,
      "grad_norm": 12.644137382507324,
      "learning_rate": 4.796747967479675e-06,
      "loss": 1.9214,
      "step": 1239
    },
    {
      "epoch": 0.4800619434765776,
      "grad_norm": 12.779243469238281,
      "learning_rate": 4.8006194347657765e-06,
      "loss": 1.9184,
      "step": 1240
    },
    {
      "epoch": 0.4804490902051878,
      "grad_norm": 7.266574859619141,
      "learning_rate": 4.804490902051878e-06,
      "loss": 1.7182,
      "step": 1241
    },
    {
      "epoch": 0.4808362369337979,
      "grad_norm": 8.06868839263916,
      "learning_rate": 4.8083623693379794e-06,
      "loss": 1.7949,
      "step": 1242
    },
    {
      "epoch": 0.4812233836624081,
      "grad_norm": 16.04082679748535,
      "learning_rate": 4.812233836624081e-06,
      "loss": 2.0924,
      "step": 1243
    },
    {
      "epoch": 0.4816105303910182,
      "grad_norm": 7.495399475097656,
      "learning_rate": 4.8161053039101824e-06,
      "loss": 1.6522,
      "step": 1244
    },
    {
      "epoch": 0.4819976771196283,
      "grad_norm": 7.571193695068359,
      "learning_rate": 4.819976771196284e-06,
      "loss": 1.8176,
      "step": 1245
    },
    {
      "epoch": 0.4823848238482385,
      "grad_norm": 9.685017585754395,
      "learning_rate": 4.823848238482385e-06,
      "loss": 1.7813,
      "step": 1246
    },
    {
      "epoch": 0.4827719705768486,
      "grad_norm": 7.221957683563232,
      "learning_rate": 4.827719705768487e-06,
      "loss": 1.7539,
      "step": 1247
    },
    {
      "epoch": 0.4831591173054588,
      "grad_norm": 6.6510186195373535,
      "learning_rate": 4.831591173054588e-06,
      "loss": 1.7172,
      "step": 1248
    },
    {
      "epoch": 0.4835462640340689,
      "grad_norm": 8.394550323486328,
      "learning_rate": 4.83546264034069e-06,
      "loss": 1.8425,
      "step": 1249
    },
    {
      "epoch": 0.48393341076267904,
      "grad_norm": 8.16180419921875,
      "learning_rate": 4.8393341076267905e-06,
      "loss": 1.8468,
      "step": 1250
    },
    {
      "epoch": 0.4843205574912892,
      "grad_norm": 8.013097763061523,
      "learning_rate": 4.843205574912893e-06,
      "loss": 1.7693,
      "step": 1251
    },
    {
      "epoch": 0.48470770421989934,
      "grad_norm": 10.49217414855957,
      "learning_rate": 4.8470770421989935e-06,
      "loss": 1.7618,
      "step": 1252
    },
    {
      "epoch": 0.48509485094850946,
      "grad_norm": 8.79500961303711,
      "learning_rate": 4.850948509485095e-06,
      "loss": 1.8085,
      "step": 1253
    },
    {
      "epoch": 0.48548199767711964,
      "grad_norm": 8.855293273925781,
      "learning_rate": 4.8548199767711965e-06,
      "loss": 1.7781,
      "step": 1254
    },
    {
      "epoch": 0.48586914440572976,
      "grad_norm": 7.3895063400268555,
      "learning_rate": 4.858691444057298e-06,
      "loss": 1.8355,
      "step": 1255
    },
    {
      "epoch": 0.48625629113433994,
      "grad_norm": 9.0484619140625,
      "learning_rate": 4.8625629113433995e-06,
      "loss": 1.6794,
      "step": 1256
    },
    {
      "epoch": 0.48664343786295006,
      "grad_norm": 11.733718872070312,
      "learning_rate": 4.866434378629501e-06,
      "loss": 2.1144,
      "step": 1257
    },
    {
      "epoch": 0.4870305845915602,
      "grad_norm": 8.353759765625,
      "learning_rate": 4.8703058459156025e-06,
      "loss": 1.7139,
      "step": 1258
    },
    {
      "epoch": 0.48741773132017036,
      "grad_norm": 5.8226447105407715,
      "learning_rate": 4.874177313201704e-06,
      "loss": 1.7637,
      "step": 1259
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 7.501962184906006,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 1.7533,
      "step": 1260
    },
    {
      "epoch": 0.48819202477739065,
      "grad_norm": 8.486833572387695,
      "learning_rate": 4.881920247773907e-06,
      "loss": 1.704,
      "step": 1261
    },
    {
      "epoch": 0.4885791715060008,
      "grad_norm": 9.040925979614258,
      "learning_rate": 4.8857917150600085e-06,
      "loss": 1.9838,
      "step": 1262
    },
    {
      "epoch": 0.4889663182346109,
      "grad_norm": 5.881021976470947,
      "learning_rate": 4.889663182346109e-06,
      "loss": 1.7447,
      "step": 1263
    },
    {
      "epoch": 0.4893534649632211,
      "grad_norm": 7.9820098876953125,
      "learning_rate": 4.8935346496322115e-06,
      "loss": 2.048,
      "step": 1264
    },
    {
      "epoch": 0.4897406116918312,
      "grad_norm": 9.066625595092773,
      "learning_rate": 4.897406116918312e-06,
      "loss": 2.149,
      "step": 1265
    },
    {
      "epoch": 0.4901277584204414,
      "grad_norm": 8.0830078125,
      "learning_rate": 4.9012775842044144e-06,
      "loss": 1.8366,
      "step": 1266
    },
    {
      "epoch": 0.4905149051490515,
      "grad_norm": 7.364961624145508,
      "learning_rate": 4.905149051490515e-06,
      "loss": 1.6481,
      "step": 1267
    },
    {
      "epoch": 0.4909020518776616,
      "grad_norm": 10.621599197387695,
      "learning_rate": 4.909020518776617e-06,
      "loss": 1.8789,
      "step": 1268
    },
    {
      "epoch": 0.4912891986062718,
      "grad_norm": 8.669751167297363,
      "learning_rate": 4.912891986062718e-06,
      "loss": 1.8713,
      "step": 1269
    },
    {
      "epoch": 0.4916763453348819,
      "grad_norm": 8.815266609191895,
      "learning_rate": 4.9167634533488196e-06,
      "loss": 1.6935,
      "step": 1270
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 5.5545735359191895,
      "learning_rate": 4.920634920634921e-06,
      "loss": 1.8098,
      "step": 1271
    },
    {
      "epoch": 0.4924506387921022,
      "grad_norm": 6.907401084899902,
      "learning_rate": 4.9245063879210226e-06,
      "loss": 1.7447,
      "step": 1272
    },
    {
      "epoch": 0.49283778552071233,
      "grad_norm": 12.632711410522461,
      "learning_rate": 4.928377855207124e-06,
      "loss": 1.9561,
      "step": 1273
    },
    {
      "epoch": 0.4932249322493225,
      "grad_norm": 9.984132766723633,
      "learning_rate": 4.9322493224932255e-06,
      "loss": 1.7556,
      "step": 1274
    },
    {
      "epoch": 0.49361207897793263,
      "grad_norm": 6.085011959075928,
      "learning_rate": 4.936120789779327e-06,
      "loss": 1.7836,
      "step": 1275
    },
    {
      "epoch": 0.49399922570654275,
      "grad_norm": 6.727043151855469,
      "learning_rate": 4.939992257065428e-06,
      "loss": 1.6737,
      "step": 1276
    },
    {
      "epoch": 0.49438637243515293,
      "grad_norm": 6.552462100982666,
      "learning_rate": 4.94386372435153e-06,
      "loss": 1.8562,
      "step": 1277
    },
    {
      "epoch": 0.49477351916376305,
      "grad_norm": 6.996310710906982,
      "learning_rate": 4.947735191637631e-06,
      "loss": 1.774,
      "step": 1278
    },
    {
      "epoch": 0.49516066589237323,
      "grad_norm": 7.439060211181641,
      "learning_rate": 4.951606658923733e-06,
      "loss": 1.8645,
      "step": 1279
    },
    {
      "epoch": 0.49554781262098335,
      "grad_norm": 8.391182899475098,
      "learning_rate": 4.955478126209834e-06,
      "loss": 1.7161,
      "step": 1280
    },
    {
      "epoch": 0.4959349593495935,
      "grad_norm": 6.773946762084961,
      "learning_rate": 4.959349593495935e-06,
      "loss": 1.7727,
      "step": 1281
    },
    {
      "epoch": 0.49632210607820365,
      "grad_norm": 9.339591026306152,
      "learning_rate": 4.963221060782037e-06,
      "loss": 2.1467,
      "step": 1282
    },
    {
      "epoch": 0.4967092528068138,
      "grad_norm": 13.101534843444824,
      "learning_rate": 4.967092528068138e-06,
      "loss": 1.8156,
      "step": 1283
    },
    {
      "epoch": 0.49709639953542395,
      "grad_norm": 9.072443008422852,
      "learning_rate": 4.97096399535424e-06,
      "loss": 1.7877,
      "step": 1284
    },
    {
      "epoch": 0.49748354626403407,
      "grad_norm": 8.034409523010254,
      "learning_rate": 4.974835462640341e-06,
      "loss": 1.6724,
      "step": 1285
    },
    {
      "epoch": 0.4978706929926442,
      "grad_norm": 9.200090408325195,
      "learning_rate": 4.978706929926443e-06,
      "loss": 1.6717,
      "step": 1286
    },
    {
      "epoch": 0.49825783972125437,
      "grad_norm": 7.619313716888428,
      "learning_rate": 4.982578397212544e-06,
      "loss": 1.7391,
      "step": 1287
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 11.121085166931152,
      "learning_rate": 4.986449864498646e-06,
      "loss": 1.7499,
      "step": 1288
    },
    {
      "epoch": 0.49903213317847467,
      "grad_norm": 10.333406448364258,
      "learning_rate": 4.990321331784747e-06,
      "loss": 1.9308,
      "step": 1289
    },
    {
      "epoch": 0.4994192799070848,
      "grad_norm": 7.297568321228027,
      "learning_rate": 4.994192799070849e-06,
      "loss": 1.6901,
      "step": 1290
    },
    {
      "epoch": 0.4998064266356949,
      "grad_norm": 5.885211944580078,
      "learning_rate": 4.998064266356949e-06,
      "loss": 1.7462,
      "step": 1291
    },
    {
      "epoch": 0.500193573364305,
      "grad_norm": 8.989912033081055,
      "learning_rate": 5.001935733643051e-06,
      "loss": 1.7973,
      "step": 1292
    },
    {
      "epoch": 0.5005807200929152,
      "grad_norm": 14.319707870483398,
      "learning_rate": 5.005807200929152e-06,
      "loss": 1.7254,
      "step": 1293
    },
    {
      "epoch": 0.5009678668215254,
      "grad_norm": 8.638689994812012,
      "learning_rate": 5.0096786682152546e-06,
      "loss": 1.723,
      "step": 1294
    },
    {
      "epoch": 0.5013550135501355,
      "grad_norm": 6.493345260620117,
      "learning_rate": 5.013550135501355e-06,
      "loss": 1.7121,
      "step": 1295
    },
    {
      "epoch": 0.5017421602787456,
      "grad_norm": 7.5784478187561035,
      "learning_rate": 5.017421602787457e-06,
      "loss": 1.6619,
      "step": 1296
    },
    {
      "epoch": 0.5021293070073558,
      "grad_norm": 8.598311424255371,
      "learning_rate": 5.021293070073558e-06,
      "loss": 1.7179,
      "step": 1297
    },
    {
      "epoch": 0.502516453735966,
      "grad_norm": 9.280269622802734,
      "learning_rate": 5.0251645373596605e-06,
      "loss": 1.7426,
      "step": 1298
    },
    {
      "epoch": 0.502903600464576,
      "grad_norm": 7.828007698059082,
      "learning_rate": 5.029036004645761e-06,
      "loss": 1.6942,
      "step": 1299
    },
    {
      "epoch": 0.5032907471931862,
      "grad_norm": 8.416085243225098,
      "learning_rate": 5.032907471931863e-06,
      "loss": 1.7567,
      "step": 1300
    },
    {
      "epoch": 0.5036778939217964,
      "grad_norm": 4.9756622314453125,
      "learning_rate": 5.036778939217964e-06,
      "loss": 1.8594,
      "step": 1301
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 9.59684944152832,
      "learning_rate": 5.040650406504065e-06,
      "loss": 1.769,
      "step": 1302
    },
    {
      "epoch": 0.5044521873790166,
      "grad_norm": 7.622328758239746,
      "learning_rate": 5.044521873790167e-06,
      "loss": 1.7885,
      "step": 1303
    },
    {
      "epoch": 0.5048393341076268,
      "grad_norm": 5.3853759765625,
      "learning_rate": 5.048393341076269e-06,
      "loss": 1.9012,
      "step": 1304
    },
    {
      "epoch": 0.5052264808362369,
      "grad_norm": 12.7770414352417,
      "learning_rate": 5.052264808362369e-06,
      "loss": 1.5756,
      "step": 1305
    },
    {
      "epoch": 0.5056136275648471,
      "grad_norm": 9.566641807556152,
      "learning_rate": 5.056136275648471e-06,
      "loss": 1.8211,
      "step": 1306
    },
    {
      "epoch": 0.5060007742934572,
      "grad_norm": 7.381981372833252,
      "learning_rate": 5.060007742934573e-06,
      "loss": 1.7978,
      "step": 1307
    },
    {
      "epoch": 0.5063879210220673,
      "grad_norm": 13.096182823181152,
      "learning_rate": 5.063879210220674e-06,
      "loss": 1.5714,
      "step": 1308
    },
    {
      "epoch": 0.5067750677506775,
      "grad_norm": 7.9695634841918945,
      "learning_rate": 5.067750677506775e-06,
      "loss": 1.6306,
      "step": 1309
    },
    {
      "epoch": 0.5071622144792877,
      "grad_norm": 6.70554256439209,
      "learning_rate": 5.071622144792877e-06,
      "loss": 1.6707,
      "step": 1310
    },
    {
      "epoch": 0.5075493612078978,
      "grad_norm": 9.293648719787598,
      "learning_rate": 5.075493612078979e-06,
      "loss": 1.7442,
      "step": 1311
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 12.665901184082031,
      "learning_rate": 5.07936507936508e-06,
      "loss": 1.7302,
      "step": 1312
    },
    {
      "epoch": 0.5083236546651181,
      "grad_norm": 7.331841945648193,
      "learning_rate": 5.083236546651181e-06,
      "loss": 1.7705,
      "step": 1313
    },
    {
      "epoch": 0.5087108013937283,
      "grad_norm": 8.053418159484863,
      "learning_rate": 5.087108013937283e-06,
      "loss": 1.723,
      "step": 1314
    },
    {
      "epoch": 0.5090979481223383,
      "grad_norm": 16.229639053344727,
      "learning_rate": 5.090979481223383e-06,
      "loss": 1.7001,
      "step": 1315
    },
    {
      "epoch": 0.5094850948509485,
      "grad_norm": 7.097141742706299,
      "learning_rate": 5.094850948509486e-06,
      "loss": 1.6488,
      "step": 1316
    },
    {
      "epoch": 0.5098722415795587,
      "grad_norm": 8.260337829589844,
      "learning_rate": 5.098722415795587e-06,
      "loss": 1.7674,
      "step": 1317
    },
    {
      "epoch": 0.5102593883081687,
      "grad_norm": 7.707313060760498,
      "learning_rate": 5.102593883081688e-06,
      "loss": 1.6598,
      "step": 1318
    },
    {
      "epoch": 0.5106465350367789,
      "grad_norm": 9.216583251953125,
      "learning_rate": 5.106465350367789e-06,
      "loss": 1.7438,
      "step": 1319
    },
    {
      "epoch": 0.5110336817653891,
      "grad_norm": 8.76384449005127,
      "learning_rate": 5.110336817653892e-06,
      "loss": 1.66,
      "step": 1320
    },
    {
      "epoch": 0.5114208284939993,
      "grad_norm": 9.909137725830078,
      "learning_rate": 5.114208284939993e-06,
      "loss": 1.918,
      "step": 1321
    },
    {
      "epoch": 0.5118079752226093,
      "grad_norm": 15.340450286865234,
      "learning_rate": 5.118079752226094e-06,
      "loss": 1.9091,
      "step": 1322
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 14.217267990112305,
      "learning_rate": 5.121951219512195e-06,
      "loss": 1.6284,
      "step": 1323
    },
    {
      "epoch": 0.5125822686798297,
      "grad_norm": 9.805551528930664,
      "learning_rate": 5.125822686798298e-06,
      "loss": 1.9058,
      "step": 1324
    },
    {
      "epoch": 0.5129694154084398,
      "grad_norm": 7.676059246063232,
      "learning_rate": 5.129694154084398e-06,
      "loss": 1.7423,
      "step": 1325
    },
    {
      "epoch": 0.5133565621370499,
      "grad_norm": 7.661569118499756,
      "learning_rate": 5.1335656213705e-06,
      "loss": 1.8519,
      "step": 1326
    },
    {
      "epoch": 0.5137437088656601,
      "grad_norm": 10.50899600982666,
      "learning_rate": 5.137437088656601e-06,
      "loss": 1.7718,
      "step": 1327
    },
    {
      "epoch": 0.5141308555942702,
      "grad_norm": 9.510676383972168,
      "learning_rate": 5.141308555942702e-06,
      "loss": 1.5858,
      "step": 1328
    },
    {
      "epoch": 0.5145180023228804,
      "grad_norm": 8.900527954101562,
      "learning_rate": 5.145180023228804e-06,
      "loss": 1.7474,
      "step": 1329
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 8.274018287658691,
      "learning_rate": 5.149051490514906e-06,
      "loss": 1.709,
      "step": 1330
    },
    {
      "epoch": 0.5152922957801006,
      "grad_norm": 7.996002197265625,
      "learning_rate": 5.1529229578010064e-06,
      "loss": 1.9953,
      "step": 1331
    },
    {
      "epoch": 0.5156794425087108,
      "grad_norm": 8.977570533752441,
      "learning_rate": 5.156794425087108e-06,
      "loss": 1.6712,
      "step": 1332
    },
    {
      "epoch": 0.516066589237321,
      "grad_norm": 9.667548179626465,
      "learning_rate": 5.16066589237321e-06,
      "loss": 1.8049,
      "step": 1333
    },
    {
      "epoch": 0.5164537359659311,
      "grad_norm": 9.509604454040527,
      "learning_rate": 5.164537359659312e-06,
      "loss": 1.7928,
      "step": 1334
    },
    {
      "epoch": 0.5168408826945412,
      "grad_norm": 9.484648704528809,
      "learning_rate": 5.168408826945412e-06,
      "loss": 1.8007,
      "step": 1335
    },
    {
      "epoch": 0.5172280294231514,
      "grad_norm": 8.456109046936035,
      "learning_rate": 5.172280294231514e-06,
      "loss": 1.6151,
      "step": 1336
    },
    {
      "epoch": 0.5176151761517616,
      "grad_norm": 13.701489448547363,
      "learning_rate": 5.176151761517616e-06,
      "loss": 1.8202,
      "step": 1337
    },
    {
      "epoch": 0.5180023228803716,
      "grad_norm": 10.891124725341797,
      "learning_rate": 5.180023228803717e-06,
      "loss": 1.8602,
      "step": 1338
    },
    {
      "epoch": 0.5183894696089818,
      "grad_norm": 7.9399518966674805,
      "learning_rate": 5.183894696089818e-06,
      "loss": 1.7691,
      "step": 1339
    },
    {
      "epoch": 0.518776616337592,
      "grad_norm": 14.020740509033203,
      "learning_rate": 5.18776616337592e-06,
      "loss": 1.8357,
      "step": 1340
    },
    {
      "epoch": 0.519163763066202,
      "grad_norm": 11.67481803894043,
      "learning_rate": 5.1916376306620205e-06,
      "loss": 1.6927,
      "step": 1341
    },
    {
      "epoch": 0.5195509097948122,
      "grad_norm": 15.899163246154785,
      "learning_rate": 5.195509097948123e-06,
      "loss": 1.9472,
      "step": 1342
    },
    {
      "epoch": 0.5199380565234224,
      "grad_norm": 9.076348304748535,
      "learning_rate": 5.199380565234224e-06,
      "loss": 1.7944,
      "step": 1343
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 12.691703796386719,
      "learning_rate": 5.203252032520326e-06,
      "loss": 1.9228,
      "step": 1344
    },
    {
      "epoch": 0.5207123499806426,
      "grad_norm": 9.287591934204102,
      "learning_rate": 5.2071234998064265e-06,
      "loss": 1.5984,
      "step": 1345
    },
    {
      "epoch": 0.5210994967092528,
      "grad_norm": 7.831995964050293,
      "learning_rate": 5.210994967092529e-06,
      "loss": 1.6131,
      "step": 1346
    },
    {
      "epoch": 0.521486643437863,
      "grad_norm": 6.71788215637207,
      "learning_rate": 5.21486643437863e-06,
      "loss": 1.5869,
      "step": 1347
    },
    {
      "epoch": 0.5218737901664731,
      "grad_norm": 9.345585823059082,
      "learning_rate": 5.218737901664731e-06,
      "loss": 1.6317,
      "step": 1348
    },
    {
      "epoch": 0.5222609368950832,
      "grad_norm": 8.090785026550293,
      "learning_rate": 5.2226093689508325e-06,
      "loss": 1.5941,
      "step": 1349
    },
    {
      "epoch": 0.5226480836236934,
      "grad_norm": 12.546285629272461,
      "learning_rate": 5.226480836236935e-06,
      "loss": 1.7428,
      "step": 1350
    },
    {
      "epoch": 0.5230352303523035,
      "grad_norm": 11.025548934936523,
      "learning_rate": 5.2303523035230355e-06,
      "loss": 1.9032,
      "step": 1351
    },
    {
      "epoch": 0.5234223770809137,
      "grad_norm": 8.071944236755371,
      "learning_rate": 5.234223770809137e-06,
      "loss": 1.7206,
      "step": 1352
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 7.341916084289551,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 1.7799,
      "step": 1353
    },
    {
      "epoch": 0.5241966705381339,
      "grad_norm": 10.793625831604004,
      "learning_rate": 5.241966705381339e-06,
      "loss": 1.703,
      "step": 1354
    },
    {
      "epoch": 0.5245838172667441,
      "grad_norm": 7.954017162322998,
      "learning_rate": 5.2458381726674414e-06,
      "loss": 1.7322,
      "step": 1355
    },
    {
      "epoch": 0.5249709639953543,
      "grad_norm": 7.091125965118408,
      "learning_rate": 5.249709639953543e-06,
      "loss": 1.8452,
      "step": 1356
    },
    {
      "epoch": 0.5253581107239644,
      "grad_norm": 9.655097007751465,
      "learning_rate": 5.253581107239644e-06,
      "loss": 1.6682,
      "step": 1357
    },
    {
      "epoch": 0.5257452574525745,
      "grad_norm": 15.474637985229492,
      "learning_rate": 5.257452574525745e-06,
      "loss": 1.765,
      "step": 1358
    },
    {
      "epoch": 0.5261324041811847,
      "grad_norm": 11.418221473693848,
      "learning_rate": 5.261324041811847e-06,
      "loss": 1.7673,
      "step": 1359
    },
    {
      "epoch": 0.5265195509097949,
      "grad_norm": 9.937798500061035,
      "learning_rate": 5.265195509097949e-06,
      "loss": 1.7564,
      "step": 1360
    },
    {
      "epoch": 0.5269066976384049,
      "grad_norm": 6.057168483734131,
      "learning_rate": 5.2690669763840495e-06,
      "loss": 1.8477,
      "step": 1361
    },
    {
      "epoch": 0.5272938443670151,
      "grad_norm": 8.355338096618652,
      "learning_rate": 5.272938443670151e-06,
      "loss": 1.6224,
      "step": 1362
    },
    {
      "epoch": 0.5276809910956253,
      "grad_norm": 13.478459358215332,
      "learning_rate": 5.276809910956253e-06,
      "loss": 2.0608,
      "step": 1363
    },
    {
      "epoch": 0.5280681378242353,
      "grad_norm": 9.84047794342041,
      "learning_rate": 5.280681378242354e-06,
      "loss": 1.6636,
      "step": 1364
    },
    {
      "epoch": 0.5284552845528455,
      "grad_norm": 10.584559440612793,
      "learning_rate": 5.2845528455284555e-06,
      "loss": 1.7397,
      "step": 1365
    },
    {
      "epoch": 0.5288424312814557,
      "grad_norm": 14.384819030761719,
      "learning_rate": 5.288424312814557e-06,
      "loss": 1.7679,
      "step": 1366
    },
    {
      "epoch": 0.5292295780100658,
      "grad_norm": 12.933555603027344,
      "learning_rate": 5.292295780100658e-06,
      "loss": 1.7447,
      "step": 1367
    },
    {
      "epoch": 0.5296167247386759,
      "grad_norm": 8.521564483642578,
      "learning_rate": 5.29616724738676e-06,
      "loss": 1.7322,
      "step": 1368
    },
    {
      "epoch": 0.5300038714672861,
      "grad_norm": 8.680744171142578,
      "learning_rate": 5.3000387146728615e-06,
      "loss": 1.7035,
      "step": 1369
    },
    {
      "epoch": 0.5303910181958963,
      "grad_norm": 13.195222854614258,
      "learning_rate": 5.303910181958963e-06,
      "loss": 1.6419,
      "step": 1370
    },
    {
      "epoch": 0.5307781649245064,
      "grad_norm": 8.50390911102295,
      "learning_rate": 5.307781649245064e-06,
      "loss": 1.8746,
      "step": 1371
    },
    {
      "epoch": 0.5311653116531165,
      "grad_norm": 9.71081256866455,
      "learning_rate": 5.311653116531166e-06,
      "loss": 1.6766,
      "step": 1372
    },
    {
      "epoch": 0.5315524583817267,
      "grad_norm": 12.901923179626465,
      "learning_rate": 5.3155245838172675e-06,
      "loss": 1.8042,
      "step": 1373
    },
    {
      "epoch": 0.5319396051103368,
      "grad_norm": 7.895209312438965,
      "learning_rate": 5.319396051103368e-06,
      "loss": 1.6994,
      "step": 1374
    },
    {
      "epoch": 0.532326751838947,
      "grad_norm": 7.3983001708984375,
      "learning_rate": 5.32326751838947e-06,
      "loss": 1.7151,
      "step": 1375
    },
    {
      "epoch": 0.5327138985675571,
      "grad_norm": 15.921335220336914,
      "learning_rate": 5.327138985675572e-06,
      "loss": 1.7117,
      "step": 1376
    },
    {
      "epoch": 0.5331010452961672,
      "grad_norm": 9.918570518493652,
      "learning_rate": 5.331010452961673e-06,
      "loss": 1.9024,
      "step": 1377
    },
    {
      "epoch": 0.5334881920247774,
      "grad_norm": 7.98599100112915,
      "learning_rate": 5.334881920247774e-06,
      "loss": 1.6681,
      "step": 1378
    },
    {
      "epoch": 0.5338753387533876,
      "grad_norm": 7.361870288848877,
      "learning_rate": 5.338753387533876e-06,
      "loss": 1.6636,
      "step": 1379
    },
    {
      "epoch": 0.5342624854819977,
      "grad_norm": 8.02617073059082,
      "learning_rate": 5.342624854819978e-06,
      "loss": 1.6671,
      "step": 1380
    },
    {
      "epoch": 0.5346496322106078,
      "grad_norm": 13.056593894958496,
      "learning_rate": 5.3464963221060786e-06,
      "loss": 1.7162,
      "step": 1381
    },
    {
      "epoch": 0.535036778939218,
      "grad_norm": 9.123769760131836,
      "learning_rate": 5.35036778939218e-06,
      "loss": 1.6141,
      "step": 1382
    },
    {
      "epoch": 0.5354239256678281,
      "grad_norm": 7.697166442871094,
      "learning_rate": 5.3542392566782816e-06,
      "loss": 1.7032,
      "step": 1383
    },
    {
      "epoch": 0.5358110723964382,
      "grad_norm": 9.595966339111328,
      "learning_rate": 5.358110723964382e-06,
      "loss": 2.2018,
      "step": 1384
    },
    {
      "epoch": 0.5361982191250484,
      "grad_norm": 12.388055801391602,
      "learning_rate": 5.3619821912504845e-06,
      "loss": 1.791,
      "step": 1385
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 8.258294105529785,
      "learning_rate": 5.365853658536586e-06,
      "loss": 1.7152,
      "step": 1386
    },
    {
      "epoch": 0.5369725125822686,
      "grad_norm": 7.559144973754883,
      "learning_rate": 5.369725125822687e-06,
      "loss": 1.594,
      "step": 1387
    },
    {
      "epoch": 0.5373596593108788,
      "grad_norm": 6.790043354034424,
      "learning_rate": 5.373596593108788e-06,
      "loss": 1.7443,
      "step": 1388
    },
    {
      "epoch": 0.537746806039489,
      "grad_norm": 6.464563369750977,
      "learning_rate": 5.3774680603948905e-06,
      "loss": 1.5464,
      "step": 1389
    },
    {
      "epoch": 0.538133952768099,
      "grad_norm": 8.155656814575195,
      "learning_rate": 5.381339527680991e-06,
      "loss": 1.8746,
      "step": 1390
    },
    {
      "epoch": 0.5385210994967092,
      "grad_norm": 9.205279350280762,
      "learning_rate": 5.385210994967093e-06,
      "loss": 1.6912,
      "step": 1391
    },
    {
      "epoch": 0.5389082462253194,
      "grad_norm": 7.957200527191162,
      "learning_rate": 5.389082462253194e-06,
      "loss": 1.8355,
      "step": 1392
    },
    {
      "epoch": 0.5392953929539296,
      "grad_norm": 8.353617668151855,
      "learning_rate": 5.3929539295392965e-06,
      "loss": 1.7264,
      "step": 1393
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 8.412663459777832,
      "learning_rate": 5.396825396825397e-06,
      "loss": 1.796,
      "step": 1394
    },
    {
      "epoch": 0.5400696864111498,
      "grad_norm": 8.655327796936035,
      "learning_rate": 5.400696864111499e-06,
      "loss": 1.5663,
      "step": 1395
    },
    {
      "epoch": 0.54045683313976,
      "grad_norm": 9.140593528747559,
      "learning_rate": 5.4045683313976e-06,
      "loss": 1.7888,
      "step": 1396
    },
    {
      "epoch": 0.5408439798683701,
      "grad_norm": 12.821698188781738,
      "learning_rate": 5.408439798683701e-06,
      "loss": 1.7683,
      "step": 1397
    },
    {
      "epoch": 0.5412311265969802,
      "grad_norm": 10.31419849395752,
      "learning_rate": 5.412311265969803e-06,
      "loss": 1.8958,
      "step": 1398
    },
    {
      "epoch": 0.5416182733255904,
      "grad_norm": 11.663201332092285,
      "learning_rate": 5.416182733255905e-06,
      "loss": 1.6816,
      "step": 1399
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 17.67310905456543,
      "learning_rate": 5.420054200542005e-06,
      "loss": 1.8997,
      "step": 1400
    },
    {
      "epoch": 0.5423925667828107,
      "grad_norm": 7.681925296783447,
      "learning_rate": 5.423925667828107e-06,
      "loss": 1.6003,
      "step": 1401
    },
    {
      "epoch": 0.5427797135114208,
      "grad_norm": 8.727168083190918,
      "learning_rate": 5.427797135114209e-06,
      "loss": 1.592,
      "step": 1402
    },
    {
      "epoch": 0.543166860240031,
      "grad_norm": 9.692272186279297,
      "learning_rate": 5.4316686024003106e-06,
      "loss": 1.8054,
      "step": 1403
    },
    {
      "epoch": 0.5435540069686411,
      "grad_norm": 8.61725902557373,
      "learning_rate": 5.435540069686411e-06,
      "loss": 1.7554,
      "step": 1404
    },
    {
      "epoch": 0.5439411536972513,
      "grad_norm": 9.115333557128906,
      "learning_rate": 5.439411536972513e-06,
      "loss": 1.539,
      "step": 1405
    },
    {
      "epoch": 0.5443283004258614,
      "grad_norm": 9.260086059570312,
      "learning_rate": 5.443283004258615e-06,
      "loss": 1.6606,
      "step": 1406
    },
    {
      "epoch": 0.5447154471544715,
      "grad_norm": 11.25811767578125,
      "learning_rate": 5.447154471544716e-06,
      "loss": 1.6883,
      "step": 1407
    },
    {
      "epoch": 0.5451025938830817,
      "grad_norm": 9.235289573669434,
      "learning_rate": 5.451025938830817e-06,
      "loss": 1.5065,
      "step": 1408
    },
    {
      "epoch": 0.5454897406116919,
      "grad_norm": 23.70233154296875,
      "learning_rate": 5.454897406116919e-06,
      "loss": 1.9649,
      "step": 1409
    },
    {
      "epoch": 0.5458768873403019,
      "grad_norm": 10.807398796081543,
      "learning_rate": 5.458768873403019e-06,
      "loss": 1.8071,
      "step": 1410
    },
    {
      "epoch": 0.5462640340689121,
      "grad_norm": 9.413725852966309,
      "learning_rate": 5.462640340689122e-06,
      "loss": 1.5424,
      "step": 1411
    },
    {
      "epoch": 0.5466511807975223,
      "grad_norm": 15.844057083129883,
      "learning_rate": 5.466511807975223e-06,
      "loss": 2.2533,
      "step": 1412
    },
    {
      "epoch": 0.5470383275261324,
      "grad_norm": 8.093308448791504,
      "learning_rate": 5.470383275261324e-06,
      "loss": 1.6579,
      "step": 1413
    },
    {
      "epoch": 0.5474254742547425,
      "grad_norm": 8.075512886047363,
      "learning_rate": 5.474254742547425e-06,
      "loss": 1.761,
      "step": 1414
    },
    {
      "epoch": 0.5478126209833527,
      "grad_norm": 8.526487350463867,
      "learning_rate": 5.478126209833528e-06,
      "loss": 1.6375,
      "step": 1415
    },
    {
      "epoch": 0.5481997677119629,
      "grad_norm": 9.4284086227417,
      "learning_rate": 5.481997677119629e-06,
      "loss": 1.8597,
      "step": 1416
    },
    {
      "epoch": 0.548586914440573,
      "grad_norm": 7.141223430633545,
      "learning_rate": 5.48586914440573e-06,
      "loss": 1.4689,
      "step": 1417
    },
    {
      "epoch": 0.5489740611691831,
      "grad_norm": 10.1143217086792,
      "learning_rate": 5.489740611691831e-06,
      "loss": 1.5996,
      "step": 1418
    },
    {
      "epoch": 0.5493612078977933,
      "grad_norm": 9.950176239013672,
      "learning_rate": 5.493612078977934e-06,
      "loss": 1.8674,
      "step": 1419
    },
    {
      "epoch": 0.5497483546264034,
      "grad_norm": 8.04936408996582,
      "learning_rate": 5.497483546264034e-06,
      "loss": 1.6556,
      "step": 1420
    },
    {
      "epoch": 0.5501355013550135,
      "grad_norm": 12.24074649810791,
      "learning_rate": 5.501355013550136e-06,
      "loss": 1.6691,
      "step": 1421
    },
    {
      "epoch": 0.5505226480836237,
      "grad_norm": 11.19809627532959,
      "learning_rate": 5.505226480836237e-06,
      "loss": 1.7494,
      "step": 1422
    },
    {
      "epoch": 0.5509097948122338,
      "grad_norm": 7.17512845993042,
      "learning_rate": 5.509097948122339e-06,
      "loss": 1.7289,
      "step": 1423
    },
    {
      "epoch": 0.551296941540844,
      "grad_norm": 13.313982963562012,
      "learning_rate": 5.51296941540844e-06,
      "loss": 1.4654,
      "step": 1424
    },
    {
      "epoch": 0.5516840882694541,
      "grad_norm": 9.673526763916016,
      "learning_rate": 5.516840882694542e-06,
      "loss": 1.555,
      "step": 1425
    },
    {
      "epoch": 0.5520712349980643,
      "grad_norm": 9.7999906539917,
      "learning_rate": 5.520712349980643e-06,
      "loss": 1.6722,
      "step": 1426
    },
    {
      "epoch": 0.5524583817266744,
      "grad_norm": 8.88854694366455,
      "learning_rate": 5.524583817266745e-06,
      "loss": 1.8199,
      "step": 1427
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 10.987530708312988,
      "learning_rate": 5.528455284552846e-06,
      "loss": 1.6953,
      "step": 1428
    },
    {
      "epoch": 0.5532326751838947,
      "grad_norm": 9.979049682617188,
      "learning_rate": 5.532326751838948e-06,
      "loss": 1.5629,
      "step": 1429
    },
    {
      "epoch": 0.5536198219125048,
      "grad_norm": 7.817266941070557,
      "learning_rate": 5.536198219125048e-06,
      "loss": 1.5905,
      "step": 1430
    },
    {
      "epoch": 0.554006968641115,
      "grad_norm": 7.565865993499756,
      "learning_rate": 5.540069686411151e-06,
      "loss": 1.5959,
      "step": 1431
    },
    {
      "epoch": 0.5543941153697252,
      "grad_norm": 10.322434425354004,
      "learning_rate": 5.543941153697252e-06,
      "loss": 2.0315,
      "step": 1432
    },
    {
      "epoch": 0.5547812620983352,
      "grad_norm": 7.255078315734863,
      "learning_rate": 5.547812620983353e-06,
      "loss": 1.6373,
      "step": 1433
    },
    {
      "epoch": 0.5551684088269454,
      "grad_norm": 8.379776000976562,
      "learning_rate": 5.551684088269454e-06,
      "loss": 1.774,
      "step": 1434
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 9.933935165405273,
      "learning_rate": 5.555555555555557e-06,
      "loss": 2.0162,
      "step": 1435
    },
    {
      "epoch": 0.5559427022841656,
      "grad_norm": 8.29250431060791,
      "learning_rate": 5.559427022841657e-06,
      "loss": 1.7496,
      "step": 1436
    },
    {
      "epoch": 0.5563298490127758,
      "grad_norm": 12.468098640441895,
      "learning_rate": 5.563298490127759e-06,
      "loss": 1.6349,
      "step": 1437
    },
    {
      "epoch": 0.556716995741386,
      "grad_norm": 13.199508666992188,
      "learning_rate": 5.56716995741386e-06,
      "loss": 1.6256,
      "step": 1438
    },
    {
      "epoch": 0.5571041424699962,
      "grad_norm": 10.697770118713379,
      "learning_rate": 5.571041424699963e-06,
      "loss": 1.5869,
      "step": 1439
    },
    {
      "epoch": 0.5574912891986062,
      "grad_norm": 10.315377235412598,
      "learning_rate": 5.574912891986063e-06,
      "loss": 1.9821,
      "step": 1440
    },
    {
      "epoch": 0.5578784359272164,
      "grad_norm": 5.946903228759766,
      "learning_rate": 5.578784359272165e-06,
      "loss": 1.7128,
      "step": 1441
    },
    {
      "epoch": 0.5582655826558266,
      "grad_norm": 8.534420013427734,
      "learning_rate": 5.582655826558266e-06,
      "loss": 1.7344,
      "step": 1442
    },
    {
      "epoch": 0.5586527293844367,
      "grad_norm": 9.231522560119629,
      "learning_rate": 5.586527293844367e-06,
      "loss": 1.5411,
      "step": 1443
    },
    {
      "epoch": 0.5590398761130468,
      "grad_norm": 9.710814476013184,
      "learning_rate": 5.590398761130469e-06,
      "loss": 1.5228,
      "step": 1444
    },
    {
      "epoch": 0.559427022841657,
      "grad_norm": 9.185322761535645,
      "learning_rate": 5.594270228416571e-06,
      "loss": 1.7659,
      "step": 1445
    },
    {
      "epoch": 0.5598141695702671,
      "grad_norm": 9.67588996887207,
      "learning_rate": 5.598141695702671e-06,
      "loss": 1.7684,
      "step": 1446
    },
    {
      "epoch": 0.5602013162988773,
      "grad_norm": 9.422475814819336,
      "learning_rate": 5.602013162988773e-06,
      "loss": 1.7035,
      "step": 1447
    },
    {
      "epoch": 0.5605884630274874,
      "grad_norm": 9.273335456848145,
      "learning_rate": 5.605884630274875e-06,
      "loss": 1.7381,
      "step": 1448
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 10.089031219482422,
      "learning_rate": 5.609756097560977e-06,
      "loss": 1.6133,
      "step": 1449
    },
    {
      "epoch": 0.5613627564847077,
      "grad_norm": 10.175055503845215,
      "learning_rate": 5.613627564847077e-06,
      "loss": 1.7544,
      "step": 1450
    },
    {
      "epoch": 0.5617499032133179,
      "grad_norm": 8.147137641906738,
      "learning_rate": 5.617499032133179e-06,
      "loss": 1.7314,
      "step": 1451
    },
    {
      "epoch": 0.562137049941928,
      "grad_norm": 10.88351821899414,
      "learning_rate": 5.621370499419281e-06,
      "loss": 1.7979,
      "step": 1452
    },
    {
      "epoch": 0.5625241966705381,
      "grad_norm": 10.035901069641113,
      "learning_rate": 5.625241966705382e-06,
      "loss": 1.8173,
      "step": 1453
    },
    {
      "epoch": 0.5629113433991483,
      "grad_norm": 8.756290435791016,
      "learning_rate": 5.629113433991483e-06,
      "loss": 1.704,
      "step": 1454
    },
    {
      "epoch": 0.5632984901277585,
      "grad_norm": 11.939477920532227,
      "learning_rate": 5.632984901277585e-06,
      "loss": 1.8192,
      "step": 1455
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 9.53458023071289,
      "learning_rate": 5.6368563685636855e-06,
      "loss": 1.7223,
      "step": 1456
    },
    {
      "epoch": 0.5640727835849787,
      "grad_norm": 13.202938079833984,
      "learning_rate": 5.640727835849788e-06,
      "loss": 2.1213,
      "step": 1457
    },
    {
      "epoch": 0.5644599303135889,
      "grad_norm": 10.124244689941406,
      "learning_rate": 5.644599303135889e-06,
      "loss": 1.7353,
      "step": 1458
    },
    {
      "epoch": 0.5648470770421989,
      "grad_norm": 12.167207717895508,
      "learning_rate": 5.64847077042199e-06,
      "loss": 2.0167,
      "step": 1459
    },
    {
      "epoch": 0.5652342237708091,
      "grad_norm": 10.377643585205078,
      "learning_rate": 5.6523422377080915e-06,
      "loss": 1.5211,
      "step": 1460
    },
    {
      "epoch": 0.5656213704994193,
      "grad_norm": 13.694287300109863,
      "learning_rate": 5.656213704994194e-06,
      "loss": 1.6214,
      "step": 1461
    },
    {
      "epoch": 0.5660085172280295,
      "grad_norm": 12.551365852355957,
      "learning_rate": 5.660085172280295e-06,
      "loss": 1.6732,
      "step": 1462
    },
    {
      "epoch": 0.5663956639566395,
      "grad_norm": 10.562268257141113,
      "learning_rate": 5.663956639566396e-06,
      "loss": 1.7806,
      "step": 1463
    },
    {
      "epoch": 0.5667828106852497,
      "grad_norm": 7.578174114227295,
      "learning_rate": 5.6678281068524974e-06,
      "loss": 1.6514,
      "step": 1464
    },
    {
      "epoch": 0.5671699574138599,
      "grad_norm": 10.057487487792969,
      "learning_rate": 5.6716995741386e-06,
      "loss": 1.6932,
      "step": 1465
    },
    {
      "epoch": 0.56755710414247,
      "grad_norm": 17.37509536743164,
      "learning_rate": 5.6755710414247004e-06,
      "loss": 1.8797,
      "step": 1466
    },
    {
      "epoch": 0.5679442508710801,
      "grad_norm": 15.189029693603516,
      "learning_rate": 5.679442508710802e-06,
      "loss": 1.62,
      "step": 1467
    },
    {
      "epoch": 0.5683313975996903,
      "grad_norm": 15.259355545043945,
      "learning_rate": 5.683313975996903e-06,
      "loss": 1.8883,
      "step": 1468
    },
    {
      "epoch": 0.5687185443283004,
      "grad_norm": 11.407679557800293,
      "learning_rate": 5.687185443283004e-06,
      "loss": 1.6147,
      "step": 1469
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 13.385226249694824,
      "learning_rate": 5.691056910569106e-06,
      "loss": 2.015,
      "step": 1470
    },
    {
      "epoch": 0.5694928377855207,
      "grad_norm": 13.078021049499512,
      "learning_rate": 5.694928377855208e-06,
      "loss": 2.1355,
      "step": 1471
    },
    {
      "epoch": 0.5698799845141309,
      "grad_norm": 16.56485366821289,
      "learning_rate": 5.698799845141309e-06,
      "loss": 1.6212,
      "step": 1472
    },
    {
      "epoch": 0.570267131242741,
      "grad_norm": 10.44498062133789,
      "learning_rate": 5.70267131242741e-06,
      "loss": 1.5316,
      "step": 1473
    },
    {
      "epoch": 0.5706542779713512,
      "grad_norm": 11.248676300048828,
      "learning_rate": 5.706542779713512e-06,
      "loss": 1.7543,
      "step": 1474
    },
    {
      "epoch": 0.5710414246999613,
      "grad_norm": 8.887187957763672,
      "learning_rate": 5.710414246999614e-06,
      "loss": 1.6853,
      "step": 1475
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 14.102459907531738,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.4686,
      "step": 1476
    },
    {
      "epoch": 0.5718157181571816,
      "grad_norm": 16.885581970214844,
      "learning_rate": 5.718157181571816e-06,
      "loss": 1.8241,
      "step": 1477
    },
    {
      "epoch": 0.5722028648857918,
      "grad_norm": 10.367696762084961,
      "learning_rate": 5.722028648857918e-06,
      "loss": 1.7372,
      "step": 1478
    },
    {
      "epoch": 0.5725900116144018,
      "grad_norm": 10.994058609008789,
      "learning_rate": 5.725900116144019e-06,
      "loss": 1.5818,
      "step": 1479
    },
    {
      "epoch": 0.572977158343012,
      "grad_norm": 9.929997444152832,
      "learning_rate": 5.7297715834301205e-06,
      "loss": 1.596,
      "step": 1480
    },
    {
      "epoch": 0.5733643050716222,
      "grad_norm": 11.595735549926758,
      "learning_rate": 5.733643050716222e-06,
      "loss": 1.7573,
      "step": 1481
    },
    {
      "epoch": 0.5737514518002322,
      "grad_norm": 9.880404472351074,
      "learning_rate": 5.737514518002323e-06,
      "loss": 1.6642,
      "step": 1482
    },
    {
      "epoch": 0.5741385985288424,
      "grad_norm": 13.446292877197266,
      "learning_rate": 5.741385985288425e-06,
      "loss": 1.7607,
      "step": 1483
    },
    {
      "epoch": 0.5745257452574526,
      "grad_norm": 11.114376068115234,
      "learning_rate": 5.7452574525745265e-06,
      "loss": 1.8247,
      "step": 1484
    },
    {
      "epoch": 0.5749128919860628,
      "grad_norm": 11.16783618927002,
      "learning_rate": 5.749128919860628e-06,
      "loss": 1.6077,
      "step": 1485
    },
    {
      "epoch": 0.5753000387146728,
      "grad_norm": 8.862831115722656,
      "learning_rate": 5.753000387146729e-06,
      "loss": 1.7596,
      "step": 1486
    },
    {
      "epoch": 0.575687185443283,
      "grad_norm": 10.285255432128906,
      "learning_rate": 5.756871854432831e-06,
      "loss": 1.8237,
      "step": 1487
    },
    {
      "epoch": 0.5760743321718932,
      "grad_norm": 15.75640869140625,
      "learning_rate": 5.7607433217189324e-06,
      "loss": 1.5814,
      "step": 1488
    },
    {
      "epoch": 0.5764614789005033,
      "grad_norm": 12.1646146774292,
      "learning_rate": 5.764614789005033e-06,
      "loss": 2.1519,
      "step": 1489
    },
    {
      "epoch": 0.5768486256291134,
      "grad_norm": 10.039576530456543,
      "learning_rate": 5.7684862562911346e-06,
      "loss": 1.6119,
      "step": 1490
    },
    {
      "epoch": 0.5772357723577236,
      "grad_norm": 6.684099197387695,
      "learning_rate": 5.772357723577237e-06,
      "loss": 1.8163,
      "step": 1491
    },
    {
      "epoch": 0.5776229190863337,
      "grad_norm": 8.67487907409668,
      "learning_rate": 5.7762291908633376e-06,
      "loss": 1.4125,
      "step": 1492
    },
    {
      "epoch": 0.5780100658149439,
      "grad_norm": 10.82089614868164,
      "learning_rate": 5.780100658149439e-06,
      "loss": 1.5987,
      "step": 1493
    },
    {
      "epoch": 0.578397212543554,
      "grad_norm": 12.447325706481934,
      "learning_rate": 5.7839721254355405e-06,
      "loss": 1.9415,
      "step": 1494
    },
    {
      "epoch": 0.5787843592721641,
      "grad_norm": 11.678605079650879,
      "learning_rate": 5.787843592721641e-06,
      "loss": 1.7508,
      "step": 1495
    },
    {
      "epoch": 0.5791715060007743,
      "grad_norm": 8.295422554016113,
      "learning_rate": 5.7917150600077435e-06,
      "loss": 1.7493,
      "step": 1496
    },
    {
      "epoch": 0.5795586527293844,
      "grad_norm": 11.251540184020996,
      "learning_rate": 5.795586527293845e-06,
      "loss": 1.6122,
      "step": 1497
    },
    {
      "epoch": 0.5799457994579946,
      "grad_norm": 11.07530689239502,
      "learning_rate": 5.7994579945799465e-06,
      "loss": 1.8203,
      "step": 1498
    },
    {
      "epoch": 0.5803329461866047,
      "grad_norm": 7.696312427520752,
      "learning_rate": 5.803329461866047e-06,
      "loss": 1.6412,
      "step": 1499
    },
    {
      "epoch": 0.5807200929152149,
      "grad_norm": 8.422235488891602,
      "learning_rate": 5.8072009291521495e-06,
      "loss": 1.7239,
      "step": 1500
    },
    {
      "epoch": 0.581107239643825,
      "grad_norm": 8.833298683166504,
      "learning_rate": 5.811072396438251e-06,
      "loss": 1.7684,
      "step": 1501
    },
    {
      "epoch": 0.5814943863724351,
      "grad_norm": 12.904641151428223,
      "learning_rate": 5.814943863724352e-06,
      "loss": 1.582,
      "step": 1502
    },
    {
      "epoch": 0.5818815331010453,
      "grad_norm": 10.146073341369629,
      "learning_rate": 5.818815331010453e-06,
      "loss": 1.6466,
      "step": 1503
    },
    {
      "epoch": 0.5822686798296555,
      "grad_norm": 9.032862663269043,
      "learning_rate": 5.8226867982965555e-06,
      "loss": 1.6147,
      "step": 1504
    },
    {
      "epoch": 0.5826558265582655,
      "grad_norm": 13.819158554077148,
      "learning_rate": 5.826558265582656e-06,
      "loss": 1.6945,
      "step": 1505
    },
    {
      "epoch": 0.5830429732868757,
      "grad_norm": 10.75665283203125,
      "learning_rate": 5.830429732868758e-06,
      "loss": 1.9395,
      "step": 1506
    },
    {
      "epoch": 0.5834301200154859,
      "grad_norm": 8.8504638671875,
      "learning_rate": 5.834301200154859e-06,
      "loss": 1.8054,
      "step": 1507
    },
    {
      "epoch": 0.5838172667440961,
      "grad_norm": 18.66946792602539,
      "learning_rate": 5.8381726674409615e-06,
      "loss": 1.7902,
      "step": 1508
    },
    {
      "epoch": 0.5842044134727061,
      "grad_norm": 9.193771362304688,
      "learning_rate": 5.842044134727062e-06,
      "loss": 1.8253,
      "step": 1509
    },
    {
      "epoch": 0.5845915602013163,
      "grad_norm": 9.99756908416748,
      "learning_rate": 5.845915602013164e-06,
      "loss": 1.6649,
      "step": 1510
    },
    {
      "epoch": 0.5849787069299265,
      "grad_norm": 14.317920684814453,
      "learning_rate": 5.849787069299265e-06,
      "loss": 1.6757,
      "step": 1511
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 13.28160285949707,
      "learning_rate": 5.853658536585366e-06,
      "loss": 1.9246,
      "step": 1512
    },
    {
      "epoch": 0.5857530003871467,
      "grad_norm": 9.238507270812988,
      "learning_rate": 5.857530003871468e-06,
      "loss": 1.6773,
      "step": 1513
    },
    {
      "epoch": 0.5861401471157569,
      "grad_norm": 6.352151870727539,
      "learning_rate": 5.8614014711575696e-06,
      "loss": 1.7755,
      "step": 1514
    },
    {
      "epoch": 0.586527293844367,
      "grad_norm": 9.958553314208984,
      "learning_rate": 5.86527293844367e-06,
      "loss": 1.7826,
      "step": 1515
    },
    {
      "epoch": 0.5869144405729771,
      "grad_norm": 14.048624038696289,
      "learning_rate": 5.869144405729772e-06,
      "loss": 1.8408,
      "step": 1516
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 13.656244277954102,
      "learning_rate": 5.873015873015874e-06,
      "loss": 1.5944,
      "step": 1517
    },
    {
      "epoch": 0.5876887340301974,
      "grad_norm": 17.456457138061523,
      "learning_rate": 5.876887340301975e-06,
      "loss": 1.6646,
      "step": 1518
    },
    {
      "epoch": 0.5880758807588076,
      "grad_norm": 41.354862213134766,
      "learning_rate": 5.880758807588076e-06,
      "loss": 1.8757,
      "step": 1519
    },
    {
      "epoch": 0.5884630274874177,
      "grad_norm": 8.14378833770752,
      "learning_rate": 5.884630274874178e-06,
      "loss": 1.6647,
      "step": 1520
    },
    {
      "epoch": 0.5888501742160279,
      "grad_norm": 10.520184516906738,
      "learning_rate": 5.88850174216028e-06,
      "loss": 2.1739,
      "step": 1521
    },
    {
      "epoch": 0.589237320944638,
      "grad_norm": 8.685115814208984,
      "learning_rate": 5.892373209446381e-06,
      "loss": 1.7264,
      "step": 1522
    },
    {
      "epoch": 0.5896244676732482,
      "grad_norm": 10.867173194885254,
      "learning_rate": 5.896244676732482e-06,
      "loss": 1.8826,
      "step": 1523
    },
    {
      "epoch": 0.5900116144018583,
      "grad_norm": 14.543697357177734,
      "learning_rate": 5.900116144018584e-06,
      "loss": 1.5724,
      "step": 1524
    },
    {
      "epoch": 0.5903987611304684,
      "grad_norm": 14.55343246459961,
      "learning_rate": 5.903987611304684e-06,
      "loss": 1.5863,
      "step": 1525
    },
    {
      "epoch": 0.5907859078590786,
      "grad_norm": 22.200260162353516,
      "learning_rate": 5.907859078590787e-06,
      "loss": 2.027,
      "step": 1526
    },
    {
      "epoch": 0.5911730545876888,
      "grad_norm": 15.841262817382812,
      "learning_rate": 5.911730545876888e-06,
      "loss": 1.6265,
      "step": 1527
    },
    {
      "epoch": 0.5915602013162988,
      "grad_norm": 10.919628143310547,
      "learning_rate": 5.915602013162989e-06,
      "loss": 1.8709,
      "step": 1528
    },
    {
      "epoch": 0.591947348044909,
      "grad_norm": 9.772811889648438,
      "learning_rate": 5.91947348044909e-06,
      "loss": 1.7798,
      "step": 1529
    },
    {
      "epoch": 0.5923344947735192,
      "grad_norm": 12.474100112915039,
      "learning_rate": 5.923344947735193e-06,
      "loss": 1.6601,
      "step": 1530
    },
    {
      "epoch": 0.5927216415021294,
      "grad_norm": 12.051456451416016,
      "learning_rate": 5.927216415021294e-06,
      "loss": 1.5838,
      "step": 1531
    },
    {
      "epoch": 0.5931087882307394,
      "grad_norm": 13.413492202758789,
      "learning_rate": 5.931087882307395e-06,
      "loss": 1.8935,
      "step": 1532
    },
    {
      "epoch": 0.5934959349593496,
      "grad_norm": 7.277101516723633,
      "learning_rate": 5.934959349593496e-06,
      "loss": 1.3854,
      "step": 1533
    },
    {
      "epoch": 0.5938830816879598,
      "grad_norm": 18.329099655151367,
      "learning_rate": 5.938830816879599e-06,
      "loss": 1.8802,
      "step": 1534
    },
    {
      "epoch": 0.5942702284165698,
      "grad_norm": 11.395195007324219,
      "learning_rate": 5.942702284165699e-06,
      "loss": 1.7302,
      "step": 1535
    },
    {
      "epoch": 0.59465737514518,
      "grad_norm": 8.492464065551758,
      "learning_rate": 5.946573751451801e-06,
      "loss": 1.6016,
      "step": 1536
    },
    {
      "epoch": 0.5950445218737902,
      "grad_norm": 12.25341796875,
      "learning_rate": 5.950445218737902e-06,
      "loss": 1.8058,
      "step": 1537
    },
    {
      "epoch": 0.5954316686024003,
      "grad_norm": 10.197683334350586,
      "learning_rate": 5.954316686024003e-06,
      "loss": 1.5771,
      "step": 1538
    },
    {
      "epoch": 0.5958188153310104,
      "grad_norm": 10.790886878967285,
      "learning_rate": 5.958188153310105e-06,
      "loss": 1.5338,
      "step": 1539
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 9.665396690368652,
      "learning_rate": 5.962059620596207e-06,
      "loss": 1.5568,
      "step": 1540
    },
    {
      "epoch": 0.5965931087882307,
      "grad_norm": 10.372039794921875,
      "learning_rate": 5.965931087882307e-06,
      "loss": 1.5795,
      "step": 1541
    },
    {
      "epoch": 0.5969802555168409,
      "grad_norm": 9.478906631469727,
      "learning_rate": 5.969802555168409e-06,
      "loss": 1.7045,
      "step": 1542
    },
    {
      "epoch": 0.597367402245451,
      "grad_norm": 10.947084426879883,
      "learning_rate": 5.973674022454511e-06,
      "loss": 1.4489,
      "step": 1543
    },
    {
      "epoch": 0.5977545489740612,
      "grad_norm": 14.078129768371582,
      "learning_rate": 5.977545489740613e-06,
      "loss": 1.6344,
      "step": 1544
    },
    {
      "epoch": 0.5981416957026713,
      "grad_norm": 11.127141952514648,
      "learning_rate": 5.981416957026713e-06,
      "loss": 1.7046,
      "step": 1545
    },
    {
      "epoch": 0.5985288424312815,
      "grad_norm": 10.913918495178223,
      "learning_rate": 5.985288424312815e-06,
      "loss": 1.7891,
      "step": 1546
    },
    {
      "epoch": 0.5989159891598916,
      "grad_norm": 10.396374702453613,
      "learning_rate": 5.989159891598917e-06,
      "loss": 1.4717,
      "step": 1547
    },
    {
      "epoch": 0.5993031358885017,
      "grad_norm": 7.831698894500732,
      "learning_rate": 5.993031358885018e-06,
      "loss": 1.7413,
      "step": 1548
    },
    {
      "epoch": 0.5996902826171119,
      "grad_norm": 14.152691841125488,
      "learning_rate": 5.996902826171119e-06,
      "loss": 1.8132,
      "step": 1549
    },
    {
      "epoch": 0.6000774293457221,
      "grad_norm": 10.332314491271973,
      "learning_rate": 6.000774293457221e-06,
      "loss": 1.5192,
      "step": 1550
    },
    {
      "epoch": 0.6004645760743321,
      "grad_norm": 10.374032974243164,
      "learning_rate": 6.0046457607433214e-06,
      "loss": 1.8334,
      "step": 1551
    },
    {
      "epoch": 0.6008517228029423,
      "grad_norm": 13.717498779296875,
      "learning_rate": 6.008517228029424e-06,
      "loss": 1.8211,
      "step": 1552
    },
    {
      "epoch": 0.6012388695315525,
      "grad_norm": 12.885575294494629,
      "learning_rate": 6.012388695315525e-06,
      "loss": 1.6023,
      "step": 1553
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 8.932714462280273,
      "learning_rate": 6.016260162601627e-06,
      "loss": 1.5866,
      "step": 1554
    },
    {
      "epoch": 0.6020131629887727,
      "grad_norm": 12.031266212463379,
      "learning_rate": 6.020131629887727e-06,
      "loss": 1.3664,
      "step": 1555
    },
    {
      "epoch": 0.6024003097173829,
      "grad_norm": 15.150345802307129,
      "learning_rate": 6.02400309717383e-06,
      "loss": 1.6353,
      "step": 1556
    },
    {
      "epoch": 0.6027874564459931,
      "grad_norm": 14.332188606262207,
      "learning_rate": 6.027874564459931e-06,
      "loss": 1.5895,
      "step": 1557
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 13.231552124023438,
      "learning_rate": 6.031746031746032e-06,
      "loss": 1.7254,
      "step": 1558
    },
    {
      "epoch": 0.6035617499032133,
      "grad_norm": 8.915203094482422,
      "learning_rate": 6.035617499032133e-06,
      "loss": 1.6773,
      "step": 1559
    },
    {
      "epoch": 0.6039488966318235,
      "grad_norm": 8.854516983032227,
      "learning_rate": 6.039488966318236e-06,
      "loss": 1.613,
      "step": 1560
    },
    {
      "epoch": 0.6043360433604336,
      "grad_norm": 38.64569854736328,
      "learning_rate": 6.043360433604336e-06,
      "loss": 1.3755,
      "step": 1561
    },
    {
      "epoch": 0.6047231900890437,
      "grad_norm": 20.608163833618164,
      "learning_rate": 6.047231900890438e-06,
      "loss": 1.7736,
      "step": 1562
    },
    {
      "epoch": 0.6051103368176539,
      "grad_norm": 14.431235313415527,
      "learning_rate": 6.051103368176539e-06,
      "loss": 1.5949,
      "step": 1563
    },
    {
      "epoch": 0.605497483546264,
      "grad_norm": 9.450784683227539,
      "learning_rate": 6.05497483546264e-06,
      "loss": 1.5292,
      "step": 1564
    },
    {
      "epoch": 0.6058846302748742,
      "grad_norm": 9.671939849853516,
      "learning_rate": 6.058846302748742e-06,
      "loss": 1.5512,
      "step": 1565
    },
    {
      "epoch": 0.6062717770034843,
      "grad_norm": 12.98320198059082,
      "learning_rate": 6.062717770034844e-06,
      "loss": 1.577,
      "step": 1566
    },
    {
      "epoch": 0.6066589237320945,
      "grad_norm": 10.783313751220703,
      "learning_rate": 6.066589237320945e-06,
      "loss": 1.8383,
      "step": 1567
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 10.17593002319336,
      "learning_rate": 6.070460704607046e-06,
      "loss": 1.5238,
      "step": 1568
    },
    {
      "epoch": 0.6074332171893148,
      "grad_norm": 16.289405822753906,
      "learning_rate": 6.074332171893148e-06,
      "loss": 2.0461,
      "step": 1569
    },
    {
      "epoch": 0.6078203639179249,
      "grad_norm": 11.299612998962402,
      "learning_rate": 6.07820363917925e-06,
      "loss": 1.8521,
      "step": 1570
    },
    {
      "epoch": 0.608207510646535,
      "grad_norm": 11.739983558654785,
      "learning_rate": 6.0820751064653505e-06,
      "loss": 2.0838,
      "step": 1571
    },
    {
      "epoch": 0.6085946573751452,
      "grad_norm": 7.775533676147461,
      "learning_rate": 6.085946573751452e-06,
      "loss": 1.6605,
      "step": 1572
    },
    {
      "epoch": 0.6089818041037554,
      "grad_norm": 9.274157524108887,
      "learning_rate": 6.089818041037554e-06,
      "loss": 1.6843,
      "step": 1573
    },
    {
      "epoch": 0.6093689508323654,
      "grad_norm": 10.940832138061523,
      "learning_rate": 6.093689508323655e-06,
      "loss": 1.5896,
      "step": 1574
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 10.693075180053711,
      "learning_rate": 6.0975609756097564e-06,
      "loss": 1.7382,
      "step": 1575
    },
    {
      "epoch": 0.6101432442895858,
      "grad_norm": 12.609443664550781,
      "learning_rate": 6.101432442895858e-06,
      "loss": 2.5954,
      "step": 1576
    },
    {
      "epoch": 0.610530391018196,
      "grad_norm": 10.26055908203125,
      "learning_rate": 6.10530391018196e-06,
      "loss": 1.5159,
      "step": 1577
    },
    {
      "epoch": 0.610917537746806,
      "grad_norm": 16.145095825195312,
      "learning_rate": 6.109175377468061e-06,
      "loss": 1.915,
      "step": 1578
    },
    {
      "epoch": 0.6113046844754162,
      "grad_norm": 10.751679420471191,
      "learning_rate": 6.113046844754162e-06,
      "loss": 1.7961,
      "step": 1579
    },
    {
      "epoch": 0.6116918312040264,
      "grad_norm": 10.17103385925293,
      "learning_rate": 6.116918312040264e-06,
      "loss": 1.7543,
      "step": 1580
    },
    {
      "epoch": 0.6120789779326364,
      "grad_norm": 8.905314445495605,
      "learning_rate": 6.1207897793263645e-06,
      "loss": 1.4692,
      "step": 1581
    },
    {
      "epoch": 0.6124661246612466,
      "grad_norm": 12.344270706176758,
      "learning_rate": 6.124661246612467e-06,
      "loss": 1.5914,
      "step": 1582
    },
    {
      "epoch": 0.6128532713898568,
      "grad_norm": 12.150907516479492,
      "learning_rate": 6.128532713898568e-06,
      "loss": 1.6411,
      "step": 1583
    },
    {
      "epoch": 0.6132404181184669,
      "grad_norm": 8.09635066986084,
      "learning_rate": 6.132404181184669e-06,
      "loss": 1.6552,
      "step": 1584
    },
    {
      "epoch": 0.613627564847077,
      "grad_norm": 9.78537368774414,
      "learning_rate": 6.1362756484707705e-06,
      "loss": 1.9092,
      "step": 1585
    },
    {
      "epoch": 0.6140147115756872,
      "grad_norm": 10.028664588928223,
      "learning_rate": 6.140147115756873e-06,
      "loss": 1.4177,
      "step": 1586
    },
    {
      "epoch": 0.6144018583042973,
      "grad_norm": 22.991329193115234,
      "learning_rate": 6.1440185830429735e-06,
      "loss": 1.6709,
      "step": 1587
    },
    {
      "epoch": 0.6147890050329075,
      "grad_norm": 13.437554359436035,
      "learning_rate": 6.147890050329075e-06,
      "loss": 1.535,
      "step": 1588
    },
    {
      "epoch": 0.6151761517615176,
      "grad_norm": 8.078166007995605,
      "learning_rate": 6.1517615176151765e-06,
      "loss": 1.8083,
      "step": 1589
    },
    {
      "epoch": 0.6155632984901278,
      "grad_norm": 7.74685525894165,
      "learning_rate": 6.155632984901279e-06,
      "loss": 1.7143,
      "step": 1590
    },
    {
      "epoch": 0.6159504452187379,
      "grad_norm": 9.635074615478516,
      "learning_rate": 6.1595044521873795e-06,
      "loss": 1.3984,
      "step": 1591
    },
    {
      "epoch": 0.616337591947348,
      "grad_norm": 12.859708786010742,
      "learning_rate": 6.163375919473481e-06,
      "loss": 1.7372,
      "step": 1592
    },
    {
      "epoch": 0.6167247386759582,
      "grad_norm": 24.00034523010254,
      "learning_rate": 6.1672473867595825e-06,
      "loss": 1.494,
      "step": 1593
    },
    {
      "epoch": 0.6171118854045683,
      "grad_norm": 9.76987075805664,
      "learning_rate": 6.171118854045683e-06,
      "loss": 1.831,
      "step": 1594
    },
    {
      "epoch": 0.6174990321331785,
      "grad_norm": 13.735464096069336,
      "learning_rate": 6.1749903213317855e-06,
      "loss": 1.8629,
      "step": 1595
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 12.546796798706055,
      "learning_rate": 6.178861788617887e-06,
      "loss": 1.7484,
      "step": 1596
    },
    {
      "epoch": 0.6182733255903987,
      "grad_norm": 10.250710487365723,
      "learning_rate": 6.182733255903988e-06,
      "loss": 1.8899,
      "step": 1597
    },
    {
      "epoch": 0.6186604723190089,
      "grad_norm": 12.981754302978516,
      "learning_rate": 6.186604723190089e-06,
      "loss": 2.2793,
      "step": 1598
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 11.438450813293457,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 2.1992,
      "step": 1599
    },
    {
      "epoch": 0.6194347657762292,
      "grad_norm": 11.044639587402344,
      "learning_rate": 6.194347657762293e-06,
      "loss": 1.996,
      "step": 1600
    },
    {
      "epoch": 0.6198219125048393,
      "grad_norm": 9.229256629943848,
      "learning_rate": 6.1982191250483936e-06,
      "loss": 1.6605,
      "step": 1601
    },
    {
      "epoch": 0.6202090592334495,
      "grad_norm": 16.97092056274414,
      "learning_rate": 6.202090592334495e-06,
      "loss": 1.6276,
      "step": 1602
    },
    {
      "epoch": 0.6205962059620597,
      "grad_norm": 10.03750991821289,
      "learning_rate": 6.205962059620597e-06,
      "loss": 1.692,
      "step": 1603
    },
    {
      "epoch": 0.6209833526906697,
      "grad_norm": 11.449248313903809,
      "learning_rate": 6.209833526906698e-06,
      "loss": 1.9971,
      "step": 1604
    },
    {
      "epoch": 0.6213704994192799,
      "grad_norm": 9.563312530517578,
      "learning_rate": 6.2137049941927995e-06,
      "loss": 1.6856,
      "step": 1605
    },
    {
      "epoch": 0.6217576461478901,
      "grad_norm": 17.56059455871582,
      "learning_rate": 6.217576461478901e-06,
      "loss": 1.6123,
      "step": 1606
    },
    {
      "epoch": 0.6221447928765002,
      "grad_norm": 14.759915351867676,
      "learning_rate": 6.221447928765002e-06,
      "loss": 1.9823,
      "step": 1607
    },
    {
      "epoch": 0.6225319396051103,
      "grad_norm": 12.17438793182373,
      "learning_rate": 6.225319396051104e-06,
      "loss": 1.9422,
      "step": 1608
    },
    {
      "epoch": 0.6229190863337205,
      "grad_norm": 11.85266399383545,
      "learning_rate": 6.2291908633372055e-06,
      "loss": 1.8395,
      "step": 1609
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 9.19947338104248,
      "learning_rate": 6.233062330623306e-06,
      "loss": 1.6802,
      "step": 1610
    },
    {
      "epoch": 0.6236933797909407,
      "grad_norm": 14.721713066101074,
      "learning_rate": 6.236933797909408e-06,
      "loss": 1.18,
      "step": 1611
    },
    {
      "epoch": 0.6240805265195509,
      "grad_norm": 10.618770599365234,
      "learning_rate": 6.24080526519551e-06,
      "loss": 1.7229,
      "step": 1612
    },
    {
      "epoch": 0.6244676732481611,
      "grad_norm": 15.817943572998047,
      "learning_rate": 6.2446767324816115e-06,
      "loss": 1.5922,
      "step": 1613
    },
    {
      "epoch": 0.6248548199767712,
      "grad_norm": 14.560968399047852,
      "learning_rate": 6.248548199767712e-06,
      "loss": 1.6989,
      "step": 1614
    },
    {
      "epoch": 0.6252419667053813,
      "grad_norm": 10.002652168273926,
      "learning_rate": 6.252419667053814e-06,
      "loss": 1.4555,
      "step": 1615
    },
    {
      "epoch": 0.6256291134339915,
      "grad_norm": 13.65610122680664,
      "learning_rate": 6.256291134339916e-06,
      "loss": 1.4468,
      "step": 1616
    },
    {
      "epoch": 0.6260162601626016,
      "grad_norm": 12.421309471130371,
      "learning_rate": 6.260162601626017e-06,
      "loss": 1.8664,
      "step": 1617
    },
    {
      "epoch": 0.6264034068912118,
      "grad_norm": 9.310602188110352,
      "learning_rate": 6.264034068912118e-06,
      "loss": 1.5122,
      "step": 1618
    },
    {
      "epoch": 0.6267905536198219,
      "grad_norm": 9.932397842407227,
      "learning_rate": 6.26790553619822e-06,
      "loss": 1.552,
      "step": 1619
    },
    {
      "epoch": 0.627177700348432,
      "grad_norm": 11.499909400939941,
      "learning_rate": 6.27177700348432e-06,
      "loss": 1.8281,
      "step": 1620
    },
    {
      "epoch": 0.6275648470770422,
      "grad_norm": 13.386284828186035,
      "learning_rate": 6.275648470770423e-06,
      "loss": 1.5517,
      "step": 1621
    },
    {
      "epoch": 0.6279519938056524,
      "grad_norm": 14.677328109741211,
      "learning_rate": 6.279519938056524e-06,
      "loss": 1.6707,
      "step": 1622
    },
    {
      "epoch": 0.6283391405342624,
      "grad_norm": 9.997922897338867,
      "learning_rate": 6.283391405342625e-06,
      "loss": 1.7789,
      "step": 1623
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 9.93423080444336,
      "learning_rate": 6.287262872628726e-06,
      "loss": 1.5773,
      "step": 1624
    },
    {
      "epoch": 0.6291134339914828,
      "grad_norm": 11.95890998840332,
      "learning_rate": 6.2911343399148286e-06,
      "loss": 1.9148,
      "step": 1625
    },
    {
      "epoch": 0.629500580720093,
      "grad_norm": 9.166473388671875,
      "learning_rate": 6.29500580720093e-06,
      "loss": 1.7835,
      "step": 1626
    },
    {
      "epoch": 0.629887727448703,
      "grad_norm": 9.763961791992188,
      "learning_rate": 6.298877274487031e-06,
      "loss": 1.426,
      "step": 1627
    },
    {
      "epoch": 0.6302748741773132,
      "grad_norm": 11.169343948364258,
      "learning_rate": 6.302748741773132e-06,
      "loss": 1.7174,
      "step": 1628
    },
    {
      "epoch": 0.6306620209059234,
      "grad_norm": 9.525415420532227,
      "learning_rate": 6.3066202090592345e-06,
      "loss": 1.698,
      "step": 1629
    },
    {
      "epoch": 0.6310491676345334,
      "grad_norm": 11.631133079528809,
      "learning_rate": 6.310491676345335e-06,
      "loss": 1.8154,
      "step": 1630
    },
    {
      "epoch": 0.6314363143631436,
      "grad_norm": 12.042900085449219,
      "learning_rate": 6.314363143631437e-06,
      "loss": 1.8916,
      "step": 1631
    },
    {
      "epoch": 0.6318234610917538,
      "grad_norm": 12.48315715789795,
      "learning_rate": 6.318234610917538e-06,
      "loss": 1.7226,
      "step": 1632
    },
    {
      "epoch": 0.6322106078203639,
      "grad_norm": 13.990615844726562,
      "learning_rate": 6.322106078203639e-06,
      "loss": 1.8374,
      "step": 1633
    },
    {
      "epoch": 0.632597754548974,
      "grad_norm": 10.116567611694336,
      "learning_rate": 6.325977545489741e-06,
      "loss": 1.8486,
      "step": 1634
    },
    {
      "epoch": 0.6329849012775842,
      "grad_norm": 16.7943172454834,
      "learning_rate": 6.329849012775843e-06,
      "loss": 1.7482,
      "step": 1635
    },
    {
      "epoch": 0.6333720480061944,
      "grad_norm": 11.196146011352539,
      "learning_rate": 6.333720480061944e-06,
      "loss": 1.543,
      "step": 1636
    },
    {
      "epoch": 0.6337591947348045,
      "grad_norm": 9.938338279724121,
      "learning_rate": 6.337591947348045e-06,
      "loss": 1.5045,
      "step": 1637
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 9.68420124053955,
      "learning_rate": 6.341463414634147e-06,
      "loss": 1.6563,
      "step": 1638
    },
    {
      "epoch": 0.6345334881920248,
      "grad_norm": 15.46877384185791,
      "learning_rate": 6.345334881920249e-06,
      "loss": 1.5631,
      "step": 1639
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 10.867834091186523,
      "learning_rate": 6.349206349206349e-06,
      "loss": 1.7889,
      "step": 1640
    },
    {
      "epoch": 0.6353077816492451,
      "grad_norm": 12.051791191101074,
      "learning_rate": 6.353077816492451e-06,
      "loss": 1.5484,
      "step": 1641
    },
    {
      "epoch": 0.6356949283778552,
      "grad_norm": 9.796772956848145,
      "learning_rate": 6.356949283778553e-06,
      "loss": 2.1745,
      "step": 1642
    },
    {
      "epoch": 0.6360820751064653,
      "grad_norm": 10.983915328979492,
      "learning_rate": 6.360820751064654e-06,
      "loss": 1.3828,
      "step": 1643
    },
    {
      "epoch": 0.6364692218350755,
      "grad_norm": 15.645587921142578,
      "learning_rate": 6.364692218350755e-06,
      "loss": 1.5505,
      "step": 1644
    },
    {
      "epoch": 0.6368563685636857,
      "grad_norm": 15.705018997192383,
      "learning_rate": 6.368563685636857e-06,
      "loss": 1.4453,
      "step": 1645
    },
    {
      "epoch": 0.6372435152922957,
      "grad_norm": 14.376604080200195,
      "learning_rate": 6.372435152922957e-06,
      "loss": 1.7615,
      "step": 1646
    },
    {
      "epoch": 0.6376306620209059,
      "grad_norm": 10.845812797546387,
      "learning_rate": 6.37630662020906e-06,
      "loss": 1.6827,
      "step": 1647
    },
    {
      "epoch": 0.6380178087495161,
      "grad_norm": 10.224095344543457,
      "learning_rate": 6.380178087495161e-06,
      "loss": 1.9021,
      "step": 1648
    },
    {
      "epoch": 0.6384049554781263,
      "grad_norm": 16.573556900024414,
      "learning_rate": 6.384049554781263e-06,
      "loss": 1.7546,
      "step": 1649
    },
    {
      "epoch": 0.6387921022067363,
      "grad_norm": 10.449393272399902,
      "learning_rate": 6.387921022067363e-06,
      "loss": 1.6522,
      "step": 1650
    },
    {
      "epoch": 0.6391792489353465,
      "grad_norm": 18.235862731933594,
      "learning_rate": 6.391792489353466e-06,
      "loss": 1.599,
      "step": 1651
    },
    {
      "epoch": 0.6395663956639567,
      "grad_norm": 11.273093223571777,
      "learning_rate": 6.395663956639567e-06,
      "loss": 1.8063,
      "step": 1652
    },
    {
      "epoch": 0.6399535423925667,
      "grad_norm": 9.501378059387207,
      "learning_rate": 6.399535423925668e-06,
      "loss": 1.5975,
      "step": 1653
    },
    {
      "epoch": 0.6403406891211769,
      "grad_norm": 11.26749324798584,
      "learning_rate": 6.403406891211769e-06,
      "loss": 1.6983,
      "step": 1654
    },
    {
      "epoch": 0.6407278358497871,
      "grad_norm": 8.536458969116211,
      "learning_rate": 6.407278358497872e-06,
      "loss": 1.5714,
      "step": 1655
    },
    {
      "epoch": 0.6411149825783972,
      "grad_norm": 7.421027183532715,
      "learning_rate": 6.411149825783972e-06,
      "loss": 1.6369,
      "step": 1656
    },
    {
      "epoch": 0.6415021293070073,
      "grad_norm": 9.434779167175293,
      "learning_rate": 6.415021293070074e-06,
      "loss": 1.6161,
      "step": 1657
    },
    {
      "epoch": 0.6418892760356175,
      "grad_norm": 7.645019054412842,
      "learning_rate": 6.418892760356175e-06,
      "loss": 1.6453,
      "step": 1658
    },
    {
      "epoch": 0.6422764227642277,
      "grad_norm": 19.036104202270508,
      "learning_rate": 6.422764227642278e-06,
      "loss": 1.8505,
      "step": 1659
    },
    {
      "epoch": 0.6426635694928378,
      "grad_norm": 10.538844108581543,
      "learning_rate": 6.426635694928378e-06,
      "loss": 1.817,
      "step": 1660
    },
    {
      "epoch": 0.6430507162214479,
      "grad_norm": 9.730415344238281,
      "learning_rate": 6.43050716221448e-06,
      "loss": 1.6494,
      "step": 1661
    },
    {
      "epoch": 0.6434378629500581,
      "grad_norm": 11.24897289276123,
      "learning_rate": 6.434378629500581e-06,
      "loss": 1.3921,
      "step": 1662
    },
    {
      "epoch": 0.6438250096786682,
      "grad_norm": 13.17837905883789,
      "learning_rate": 6.438250096786682e-06,
      "loss": 2.4474,
      "step": 1663
    },
    {
      "epoch": 0.6442121564072784,
      "grad_norm": 10.572211265563965,
      "learning_rate": 6.442121564072784e-06,
      "loss": 1.3125,
      "step": 1664
    },
    {
      "epoch": 0.6445993031358885,
      "grad_norm": 10.286937713623047,
      "learning_rate": 6.445993031358886e-06,
      "loss": 1.4291,
      "step": 1665
    },
    {
      "epoch": 0.6449864498644986,
      "grad_norm": 11.037425994873047,
      "learning_rate": 6.449864498644986e-06,
      "loss": 1.7163,
      "step": 1666
    },
    {
      "epoch": 0.6453735965931088,
      "grad_norm": 10.855558395385742,
      "learning_rate": 6.453735965931088e-06,
      "loss": 1.639,
      "step": 1667
    },
    {
      "epoch": 0.645760743321719,
      "grad_norm": 9.719392776489258,
      "learning_rate": 6.45760743321719e-06,
      "loss": 1.6727,
      "step": 1668
    },
    {
      "epoch": 0.646147890050329,
      "grad_norm": 10.755160331726074,
      "learning_rate": 6.461478900503291e-06,
      "loss": 1.8768,
      "step": 1669
    },
    {
      "epoch": 0.6465350367789392,
      "grad_norm": 15.968979835510254,
      "learning_rate": 6.465350367789392e-06,
      "loss": 1.7733,
      "step": 1670
    },
    {
      "epoch": 0.6469221835075494,
      "grad_norm": 41.006656646728516,
      "learning_rate": 6.469221835075494e-06,
      "loss": 1.8769,
      "step": 1671
    },
    {
      "epoch": 0.6473093302361596,
      "grad_norm": 18.741825103759766,
      "learning_rate": 6.473093302361596e-06,
      "loss": 1.7571,
      "step": 1672
    },
    {
      "epoch": 0.6476964769647696,
      "grad_norm": 10.438446998596191,
      "learning_rate": 6.476964769647697e-06,
      "loss": 1.8701,
      "step": 1673
    },
    {
      "epoch": 0.6480836236933798,
      "grad_norm": 10.539752960205078,
      "learning_rate": 6.480836236933798e-06,
      "loss": 1.6691,
      "step": 1674
    },
    {
      "epoch": 0.64847077042199,
      "grad_norm": 9.968301773071289,
      "learning_rate": 6.4847077042199e-06,
      "loss": 1.511,
      "step": 1675
    },
    {
      "epoch": 0.6488579171506,
      "grad_norm": 12.567091941833496,
      "learning_rate": 6.488579171506001e-06,
      "loss": 1.3022,
      "step": 1676
    },
    {
      "epoch": 0.6492450638792102,
      "grad_norm": 9.34203815460205,
      "learning_rate": 6.492450638792103e-06,
      "loss": 1.7131,
      "step": 1677
    },
    {
      "epoch": 0.6496322106078204,
      "grad_norm": 10.73672866821289,
      "learning_rate": 6.496322106078204e-06,
      "loss": 1.4045,
      "step": 1678
    },
    {
      "epoch": 0.6500193573364305,
      "grad_norm": 9.564488410949707,
      "learning_rate": 6.500193573364305e-06,
      "loss": 1.5883,
      "step": 1679
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 16.660350799560547,
      "learning_rate": 6.504065040650407e-06,
      "loss": 2.0124,
      "step": 1680
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 9.59033489227295,
      "learning_rate": 6.507936507936509e-06,
      "loss": 1.6768,
      "step": 1681
    },
    {
      "epoch": 0.651180797522261,
      "grad_norm": 10.19639778137207,
      "learning_rate": 6.51180797522261e-06,
      "loss": 1.3712,
      "step": 1682
    },
    {
      "epoch": 0.6515679442508711,
      "grad_norm": 10.807090759277344,
      "learning_rate": 6.515679442508711e-06,
      "loss": 1.6953,
      "step": 1683
    },
    {
      "epoch": 0.6519550909794812,
      "grad_norm": 16.682161331176758,
      "learning_rate": 6.519550909794813e-06,
      "loss": 1.6858,
      "step": 1684
    },
    {
      "epoch": 0.6523422377080914,
      "grad_norm": 5.795151710510254,
      "learning_rate": 6.523422377080915e-06,
      "loss": 1.8299,
      "step": 1685
    },
    {
      "epoch": 0.6527293844367015,
      "grad_norm": 11.262455940246582,
      "learning_rate": 6.5272938443670154e-06,
      "loss": 1.2982,
      "step": 1686
    },
    {
      "epoch": 0.6531165311653117,
      "grad_norm": 18.409643173217773,
      "learning_rate": 6.531165311653117e-06,
      "loss": 1.8875,
      "step": 1687
    },
    {
      "epoch": 0.6535036778939218,
      "grad_norm": 12.072376251220703,
      "learning_rate": 6.535036778939219e-06,
      "loss": 1.7107,
      "step": 1688
    },
    {
      "epoch": 0.6538908246225319,
      "grad_norm": 9.39417839050293,
      "learning_rate": 6.53890824622532e-06,
      "loss": 1.7486,
      "step": 1689
    },
    {
      "epoch": 0.6542779713511421,
      "grad_norm": 14.508858680725098,
      "learning_rate": 6.542779713511421e-06,
      "loss": 1.4811,
      "step": 1690
    },
    {
      "epoch": 0.6546651180797523,
      "grad_norm": 11.4152250289917,
      "learning_rate": 6.546651180797523e-06,
      "loss": 1.8255,
      "step": 1691
    },
    {
      "epoch": 0.6550522648083623,
      "grad_norm": 14.729401588439941,
      "learning_rate": 6.5505226480836235e-06,
      "loss": 1.9383,
      "step": 1692
    },
    {
      "epoch": 0.6554394115369725,
      "grad_norm": 20.702415466308594,
      "learning_rate": 6.554394115369726e-06,
      "loss": 1.7911,
      "step": 1693
    },
    {
      "epoch": 0.6558265582655827,
      "grad_norm": 11.477478981018066,
      "learning_rate": 6.558265582655827e-06,
      "loss": 1.7735,
      "step": 1694
    },
    {
      "epoch": 0.6562137049941928,
      "grad_norm": 16.803979873657227,
      "learning_rate": 6.562137049941929e-06,
      "loss": 2.2658,
      "step": 1695
    },
    {
      "epoch": 0.6566008517228029,
      "grad_norm": 8.346403121948242,
      "learning_rate": 6.5660085172280295e-06,
      "loss": 1.6516,
      "step": 1696
    },
    {
      "epoch": 0.6569879984514131,
      "grad_norm": 10.942815780639648,
      "learning_rate": 6.569879984514132e-06,
      "loss": 1.3745,
      "step": 1697
    },
    {
      "epoch": 0.6573751451800233,
      "grad_norm": 9.2357177734375,
      "learning_rate": 6.573751451800233e-06,
      "loss": 1.5242,
      "step": 1698
    },
    {
      "epoch": 0.6577622919086333,
      "grad_norm": 9.346148490905762,
      "learning_rate": 6.577622919086334e-06,
      "loss": 1.5722,
      "step": 1699
    },
    {
      "epoch": 0.6581494386372435,
      "grad_norm": 14.438264846801758,
      "learning_rate": 6.5814943863724355e-06,
      "loss": 1.5598,
      "step": 1700
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 25.445663452148438,
      "learning_rate": 6.585365853658538e-06,
      "loss": 2.0345,
      "step": 1701
    },
    {
      "epoch": 0.6589237320944638,
      "grad_norm": 8.108845710754395,
      "learning_rate": 6.5892373209446385e-06,
      "loss": 1.5409,
      "step": 1702
    },
    {
      "epoch": 0.6593108788230739,
      "grad_norm": 9.686677932739258,
      "learning_rate": 6.59310878823074e-06,
      "loss": 1.6921,
      "step": 1703
    },
    {
      "epoch": 0.6596980255516841,
      "grad_norm": 10.099474906921387,
      "learning_rate": 6.5969802555168415e-06,
      "loss": 1.7747,
      "step": 1704
    },
    {
      "epoch": 0.6600851722802943,
      "grad_norm": 9.691216468811035,
      "learning_rate": 6.600851722802944e-06,
      "loss": 1.7017,
      "step": 1705
    },
    {
      "epoch": 0.6604723190089044,
      "grad_norm": 10.665839195251465,
      "learning_rate": 6.6047231900890444e-06,
      "loss": 1.7126,
      "step": 1706
    },
    {
      "epoch": 0.6608594657375145,
      "grad_norm": 9.948758125305176,
      "learning_rate": 6.608594657375146e-06,
      "loss": 1.6053,
      "step": 1707
    },
    {
      "epoch": 0.6612466124661247,
      "grad_norm": 9.160399436950684,
      "learning_rate": 6.6124661246612474e-06,
      "loss": 1.6367,
      "step": 1708
    },
    {
      "epoch": 0.6616337591947348,
      "grad_norm": 12.585317611694336,
      "learning_rate": 6.616337591947348e-06,
      "loss": 1.8175,
      "step": 1709
    },
    {
      "epoch": 0.662020905923345,
      "grad_norm": 9.440046310424805,
      "learning_rate": 6.62020905923345e-06,
      "loss": 1.7824,
      "step": 1710
    },
    {
      "epoch": 0.6624080526519551,
      "grad_norm": 10.115623474121094,
      "learning_rate": 6.624080526519552e-06,
      "loss": 1.2509,
      "step": 1711
    },
    {
      "epoch": 0.6627951993805652,
      "grad_norm": 10.586886405944824,
      "learning_rate": 6.6279519938056526e-06,
      "loss": 1.7563,
      "step": 1712
    },
    {
      "epoch": 0.6631823461091754,
      "grad_norm": 14.908645629882812,
      "learning_rate": 6.631823461091754e-06,
      "loss": 1.6461,
      "step": 1713
    },
    {
      "epoch": 0.6635694928377855,
      "grad_norm": 13.411029815673828,
      "learning_rate": 6.635694928377856e-06,
      "loss": 1.9984,
      "step": 1714
    },
    {
      "epoch": 0.6639566395663956,
      "grad_norm": 9.240623474121094,
      "learning_rate": 6.639566395663957e-06,
      "loss": 1.5382,
      "step": 1715
    },
    {
      "epoch": 0.6643437862950058,
      "grad_norm": 12.124184608459473,
      "learning_rate": 6.6434378629500585e-06,
      "loss": 1.5087,
      "step": 1716
    },
    {
      "epoch": 0.664730933023616,
      "grad_norm": 20.874603271484375,
      "learning_rate": 6.64730933023616e-06,
      "loss": 1.6838,
      "step": 1717
    },
    {
      "epoch": 0.6651180797522261,
      "grad_norm": 10.21602725982666,
      "learning_rate": 6.651180797522262e-06,
      "loss": 1.9602,
      "step": 1718
    },
    {
      "epoch": 0.6655052264808362,
      "grad_norm": 10.535090446472168,
      "learning_rate": 6.655052264808363e-06,
      "loss": 1.7679,
      "step": 1719
    },
    {
      "epoch": 0.6658923732094464,
      "grad_norm": 8.714552879333496,
      "learning_rate": 6.6589237320944645e-06,
      "loss": 1.6743,
      "step": 1720
    },
    {
      "epoch": 0.6662795199380566,
      "grad_norm": 16.960966110229492,
      "learning_rate": 6.662795199380566e-06,
      "loss": 1.7441,
      "step": 1721
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 8.444924354553223,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.2433,
      "step": 1722
    },
    {
      "epoch": 0.6670538133952768,
      "grad_norm": 14.322436332702637,
      "learning_rate": 6.670538133952769e-06,
      "loss": 1.4488,
      "step": 1723
    },
    {
      "epoch": 0.667440960123887,
      "grad_norm": 16.21245002746582,
      "learning_rate": 6.6744096012388705e-06,
      "loss": 1.6602,
      "step": 1724
    },
    {
      "epoch": 0.667828106852497,
      "grad_norm": 15.279275894165039,
      "learning_rate": 6.678281068524971e-06,
      "loss": 1.726,
      "step": 1725
    },
    {
      "epoch": 0.6682152535811072,
      "grad_norm": 9.365312576293945,
      "learning_rate": 6.682152535811073e-06,
      "loss": 1.5979,
      "step": 1726
    },
    {
      "epoch": 0.6686024003097174,
      "grad_norm": 7.099853515625,
      "learning_rate": 6.686024003097175e-06,
      "loss": 1.7253,
      "step": 1727
    },
    {
      "epoch": 0.6689895470383276,
      "grad_norm": 18.042552947998047,
      "learning_rate": 6.6898954703832765e-06,
      "loss": 2.1572,
      "step": 1728
    },
    {
      "epoch": 0.6693766937669376,
      "grad_norm": 12.843842506408691,
      "learning_rate": 6.693766937669377e-06,
      "loss": 1.428,
      "step": 1729
    },
    {
      "epoch": 0.6697638404955478,
      "grad_norm": 8.317930221557617,
      "learning_rate": 6.697638404955479e-06,
      "loss": 1.8253,
      "step": 1730
    },
    {
      "epoch": 0.670150987224158,
      "grad_norm": 12.043935775756836,
      "learning_rate": 6.701509872241581e-06,
      "loss": 1.7482,
      "step": 1731
    },
    {
      "epoch": 0.6705381339527681,
      "grad_norm": 15.44277572631836,
      "learning_rate": 6.705381339527682e-06,
      "loss": 1.6465,
      "step": 1732
    },
    {
      "epoch": 0.6709252806813782,
      "grad_norm": 14.590611457824707,
      "learning_rate": 6.709252806813783e-06,
      "loss": 1.6486,
      "step": 1733
    },
    {
      "epoch": 0.6713124274099884,
      "grad_norm": 12.6495943069458,
      "learning_rate": 6.7131242740998846e-06,
      "loss": 1.5811,
      "step": 1734
    },
    {
      "epoch": 0.6716995741385985,
      "grad_norm": 12.761370658874512,
      "learning_rate": 6.716995741385985e-06,
      "loss": 1.6173,
      "step": 1735
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 28.80975914001465,
      "learning_rate": 6.7208672086720876e-06,
      "loss": 2.128,
      "step": 1736
    },
    {
      "epoch": 0.6724738675958188,
      "grad_norm": 11.561868667602539,
      "learning_rate": 6.724738675958189e-06,
      "loss": 1.7178,
      "step": 1737
    },
    {
      "epoch": 0.6728610143244289,
      "grad_norm": 12.951156616210938,
      "learning_rate": 6.72861014324429e-06,
      "loss": 1.6467,
      "step": 1738
    },
    {
      "epoch": 0.6732481610530391,
      "grad_norm": 17.135419845581055,
      "learning_rate": 6.732481610530391e-06,
      "loss": 1.7315,
      "step": 1739
    },
    {
      "epoch": 0.6736353077816493,
      "grad_norm": 13.040820121765137,
      "learning_rate": 6.7363530778164935e-06,
      "loss": 1.6077,
      "step": 1740
    },
    {
      "epoch": 0.6740224545102594,
      "grad_norm": 9.455711364746094,
      "learning_rate": 6.740224545102595e-06,
      "loss": 1.6266,
      "step": 1741
    },
    {
      "epoch": 0.6744096012388695,
      "grad_norm": 9.62055778503418,
      "learning_rate": 6.744096012388696e-06,
      "loss": 1.6667,
      "step": 1742
    },
    {
      "epoch": 0.6747967479674797,
      "grad_norm": 7.899770736694336,
      "learning_rate": 6.747967479674797e-06,
      "loss": 1.2322,
      "step": 1743
    },
    {
      "epoch": 0.6751838946960899,
      "grad_norm": 9.839421272277832,
      "learning_rate": 6.7518389469608995e-06,
      "loss": 1.5933,
      "step": 1744
    },
    {
      "epoch": 0.6755710414246999,
      "grad_norm": 11.663894653320312,
      "learning_rate": 6.755710414247e-06,
      "loss": 1.6245,
      "step": 1745
    },
    {
      "epoch": 0.6759581881533101,
      "grad_norm": 8.384481430053711,
      "learning_rate": 6.759581881533102e-06,
      "loss": 1.6683,
      "step": 1746
    },
    {
      "epoch": 0.6763453348819203,
      "grad_norm": 16.135602951049805,
      "learning_rate": 6.763453348819203e-06,
      "loss": 1.7899,
      "step": 1747
    },
    {
      "epoch": 0.6767324816105303,
      "grad_norm": 10.426004409790039,
      "learning_rate": 6.767324816105304e-06,
      "loss": 1.3406,
      "step": 1748
    },
    {
      "epoch": 0.6771196283391405,
      "grad_norm": 12.822816848754883,
      "learning_rate": 6.771196283391406e-06,
      "loss": 1.5617,
      "step": 1749
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 14.033377647399902,
      "learning_rate": 6.775067750677508e-06,
      "loss": 2.1645,
      "step": 1750
    },
    {
      "epoch": 0.6778939217963608,
      "grad_norm": 30.764177322387695,
      "learning_rate": 6.778939217963608e-06,
      "loss": 1.9821,
      "step": 1751
    },
    {
      "epoch": 0.6782810685249709,
      "grad_norm": 7.057508945465088,
      "learning_rate": 6.78281068524971e-06,
      "loss": 1.229,
      "step": 1752
    },
    {
      "epoch": 0.6786682152535811,
      "grad_norm": 6.876107215881348,
      "learning_rate": 6.786682152535812e-06,
      "loss": 1.3643,
      "step": 1753
    },
    {
      "epoch": 0.6790553619821913,
      "grad_norm": 10.17329216003418,
      "learning_rate": 6.790553619821914e-06,
      "loss": 1.3381,
      "step": 1754
    },
    {
      "epoch": 0.6794425087108014,
      "grad_norm": 9.891817092895508,
      "learning_rate": 6.794425087108014e-06,
      "loss": 1.6959,
      "step": 1755
    },
    {
      "epoch": 0.6798296554394115,
      "grad_norm": 12.845528602600098,
      "learning_rate": 6.798296554394116e-06,
      "loss": 1.4437,
      "step": 1756
    },
    {
      "epoch": 0.6802168021680217,
      "grad_norm": 14.759862899780273,
      "learning_rate": 6.802168021680218e-06,
      "loss": 1.598,
      "step": 1757
    },
    {
      "epoch": 0.6806039488966318,
      "grad_norm": 14.325324058532715,
      "learning_rate": 6.806039488966319e-06,
      "loss": 1.6625,
      "step": 1758
    },
    {
      "epoch": 0.680991095625242,
      "grad_norm": 8.000545501708984,
      "learning_rate": 6.80991095625242e-06,
      "loss": 1.5641,
      "step": 1759
    },
    {
      "epoch": 0.6813782423538521,
      "grad_norm": 15.356788635253906,
      "learning_rate": 6.813782423538522e-06,
      "loss": 1.7632,
      "step": 1760
    },
    {
      "epoch": 0.6817653890824622,
      "grad_norm": 15.611788749694824,
      "learning_rate": 6.817653890824622e-06,
      "loss": 1.724,
      "step": 1761
    },
    {
      "epoch": 0.6821525358110724,
      "grad_norm": 13.338688850402832,
      "learning_rate": 6.821525358110725e-06,
      "loss": 1.4255,
      "step": 1762
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 9.277602195739746,
      "learning_rate": 6.825396825396826e-06,
      "loss": 1.1634,
      "step": 1763
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 10.77285099029541,
      "learning_rate": 6.829268292682928e-06,
      "loss": 1.2284,
      "step": 1764
    },
    {
      "epoch": 0.6833139759969028,
      "grad_norm": 10.962461471557617,
      "learning_rate": 6.833139759969028e-06,
      "loss": 1.5134,
      "step": 1765
    },
    {
      "epoch": 0.683701122725513,
      "grad_norm": 9.403000831604004,
      "learning_rate": 6.837011227255131e-06,
      "loss": 1.3402,
      "step": 1766
    },
    {
      "epoch": 0.6840882694541232,
      "grad_norm": 12.388096809387207,
      "learning_rate": 6.840882694541232e-06,
      "loss": 2.1403,
      "step": 1767
    },
    {
      "epoch": 0.6844754161827332,
      "grad_norm": 11.67637825012207,
      "learning_rate": 6.844754161827333e-06,
      "loss": 1.4719,
      "step": 1768
    },
    {
      "epoch": 0.6848625629113434,
      "grad_norm": 13.047776222229004,
      "learning_rate": 6.848625629113434e-06,
      "loss": 0.9589,
      "step": 1769
    },
    {
      "epoch": 0.6852497096399536,
      "grad_norm": 13.283334732055664,
      "learning_rate": 6.852497096399537e-06,
      "loss": 0.8937,
      "step": 1770
    },
    {
      "epoch": 0.6856368563685636,
      "grad_norm": 8.076489448547363,
      "learning_rate": 6.856368563685637e-06,
      "loss": 1.5734,
      "step": 1771
    },
    {
      "epoch": 0.6860240030971738,
      "grad_norm": 11.028087615966797,
      "learning_rate": 6.860240030971739e-06,
      "loss": 1.5626,
      "step": 1772
    },
    {
      "epoch": 0.686411149825784,
      "grad_norm": 10.923970222473145,
      "learning_rate": 6.86411149825784e-06,
      "loss": 1.556,
      "step": 1773
    },
    {
      "epoch": 0.6867982965543941,
      "grad_norm": 10.3269681930542,
      "learning_rate": 6.867982965543941e-06,
      "loss": 1.2788,
      "step": 1774
    },
    {
      "epoch": 0.6871854432830042,
      "grad_norm": 29.58184814453125,
      "learning_rate": 6.871854432830043e-06,
      "loss": 2.13,
      "step": 1775
    },
    {
      "epoch": 0.6875725900116144,
      "grad_norm": 13.688886642456055,
      "learning_rate": 6.875725900116145e-06,
      "loss": 1.5384,
      "step": 1776
    },
    {
      "epoch": 0.6879597367402246,
      "grad_norm": 9.623886108398438,
      "learning_rate": 6.879597367402246e-06,
      "loss": 1.6521,
      "step": 1777
    },
    {
      "epoch": 0.6883468834688347,
      "grad_norm": 8.089095115661621,
      "learning_rate": 6.883468834688347e-06,
      "loss": 1.4718,
      "step": 1778
    },
    {
      "epoch": 0.6887340301974448,
      "grad_norm": 13.436201095581055,
      "learning_rate": 6.887340301974449e-06,
      "loss": 1.4377,
      "step": 1779
    },
    {
      "epoch": 0.689121176926055,
      "grad_norm": 12.04425048828125,
      "learning_rate": 6.891211769260551e-06,
      "loss": 1.7137,
      "step": 1780
    },
    {
      "epoch": 0.6895083236546651,
      "grad_norm": 26.844402313232422,
      "learning_rate": 6.895083236546651e-06,
      "loss": 1.511,
      "step": 1781
    },
    {
      "epoch": 0.6898954703832753,
      "grad_norm": 15.213305473327637,
      "learning_rate": 6.898954703832753e-06,
      "loss": 1.3233,
      "step": 1782
    },
    {
      "epoch": 0.6902826171118854,
      "grad_norm": 11.502018928527832,
      "learning_rate": 6.902826171118855e-06,
      "loss": 1.8445,
      "step": 1783
    },
    {
      "epoch": 0.6906697638404955,
      "grad_norm": 9.076035499572754,
      "learning_rate": 6.906697638404956e-06,
      "loss": 1.578,
      "step": 1784
    },
    {
      "epoch": 0.6910569105691057,
      "grad_norm": 16.82877540588379,
      "learning_rate": 6.910569105691057e-06,
      "loss": 1.7182,
      "step": 1785
    },
    {
      "epoch": 0.6914440572977159,
      "grad_norm": 25.262794494628906,
      "learning_rate": 6.914440572977159e-06,
      "loss": 2.0436,
      "step": 1786
    },
    {
      "epoch": 0.691831204026326,
      "grad_norm": 11.96796703338623,
      "learning_rate": 6.918312040263261e-06,
      "loss": 1.9054,
      "step": 1787
    },
    {
      "epoch": 0.6922183507549361,
      "grad_norm": 10.857501029968262,
      "learning_rate": 6.922183507549362e-06,
      "loss": 1.7282,
      "step": 1788
    },
    {
      "epoch": 0.6926054974835463,
      "grad_norm": 13.52420711517334,
      "learning_rate": 6.926054974835463e-06,
      "loss": 1.7775,
      "step": 1789
    },
    {
      "epoch": 0.6929926442121565,
      "grad_norm": 10.676712036132812,
      "learning_rate": 6.929926442121565e-06,
      "loss": 1.2428,
      "step": 1790
    },
    {
      "epoch": 0.6933797909407665,
      "grad_norm": 8.852523803710938,
      "learning_rate": 6.9337979094076655e-06,
      "loss": 1.8614,
      "step": 1791
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 21.561601638793945,
      "learning_rate": 6.937669376693768e-06,
      "loss": 1.6159,
      "step": 1792
    },
    {
      "epoch": 0.6941540843979869,
      "grad_norm": 13.067060470581055,
      "learning_rate": 6.941540843979869e-06,
      "loss": 0.8923,
      "step": 1793
    },
    {
      "epoch": 0.6945412311265969,
      "grad_norm": 11.131730079650879,
      "learning_rate": 6.94541231126597e-06,
      "loss": 1.2655,
      "step": 1794
    },
    {
      "epoch": 0.6949283778552071,
      "grad_norm": 9.820119857788086,
      "learning_rate": 6.9492837785520714e-06,
      "loss": 1.3564,
      "step": 1795
    },
    {
      "epoch": 0.6953155245838173,
      "grad_norm": 10.39277172088623,
      "learning_rate": 6.953155245838174e-06,
      "loss": 1.2747,
      "step": 1796
    },
    {
      "epoch": 0.6957026713124274,
      "grad_norm": 20.97304344177246,
      "learning_rate": 6.957026713124274e-06,
      "loss": 1.9854,
      "step": 1797
    },
    {
      "epoch": 0.6960898180410375,
      "grad_norm": 10.669303894042969,
      "learning_rate": 6.960898180410376e-06,
      "loss": 1.2316,
      "step": 1798
    },
    {
      "epoch": 0.6964769647696477,
      "grad_norm": 10.355815887451172,
      "learning_rate": 6.964769647696477e-06,
      "loss": 1.6321,
      "step": 1799
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 18.02633285522461,
      "learning_rate": 6.96864111498258e-06,
      "loss": 1.7068,
      "step": 1800
    },
    {
      "epoch": 0.697251258226868,
      "grad_norm": 13.13068675994873,
      "learning_rate": 6.97251258226868e-06,
      "loss": 1.5696,
      "step": 1801
    },
    {
      "epoch": 0.6976384049554781,
      "grad_norm": 11.295896530151367,
      "learning_rate": 6.976384049554782e-06,
      "loss": 1.5582,
      "step": 1802
    },
    {
      "epoch": 0.6980255516840883,
      "grad_norm": 15.978182792663574,
      "learning_rate": 6.980255516840883e-06,
      "loss": 2.0441,
      "step": 1803
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 12.968184471130371,
      "learning_rate": 6.984126984126984e-06,
      "loss": 1.5884,
      "step": 1804
    },
    {
      "epoch": 0.6987998451413086,
      "grad_norm": 14.512860298156738,
      "learning_rate": 6.987998451413086e-06,
      "loss": 1.5692,
      "step": 1805
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 23.638874053955078,
      "learning_rate": 6.991869918699188e-06,
      "loss": 2.0531,
      "step": 1806
    },
    {
      "epoch": 0.6995741385985288,
      "grad_norm": 13.984369277954102,
      "learning_rate": 6.9957413859852885e-06,
      "loss": 1.4325,
      "step": 1807
    },
    {
      "epoch": 0.699961285327139,
      "grad_norm": 21.825904846191406,
      "learning_rate": 6.99961285327139e-06,
      "loss": 1.827,
      "step": 1808
    },
    {
      "epoch": 0.7003484320557491,
      "grad_norm": 11.423483848571777,
      "learning_rate": 7.003484320557492e-06,
      "loss": 1.6614,
      "step": 1809
    },
    {
      "epoch": 0.7007355787843593,
      "grad_norm": 11.648202896118164,
      "learning_rate": 7.007355787843594e-06,
      "loss": 1.2566,
      "step": 1810
    },
    {
      "epoch": 0.7011227255129694,
      "grad_norm": 9.72801399230957,
      "learning_rate": 7.0112272551296945e-06,
      "loss": 1.5388,
      "step": 1811
    },
    {
      "epoch": 0.7015098722415796,
      "grad_norm": 11.390835762023926,
      "learning_rate": 7.015098722415796e-06,
      "loss": 1.7097,
      "step": 1812
    },
    {
      "epoch": 0.7018970189701897,
      "grad_norm": 11.721148490905762,
      "learning_rate": 7.018970189701898e-06,
      "loss": 1.8019,
      "step": 1813
    },
    {
      "epoch": 0.7022841656987998,
      "grad_norm": 9.784533500671387,
      "learning_rate": 7.022841656987999e-06,
      "loss": 1.5691,
      "step": 1814
    },
    {
      "epoch": 0.70267131242741,
      "grad_norm": 24.900054931640625,
      "learning_rate": 7.0267131242741005e-06,
      "loss": 1.9059,
      "step": 1815
    },
    {
      "epoch": 0.7030584591560202,
      "grad_norm": 10.221701622009277,
      "learning_rate": 7.030584591560202e-06,
      "loss": 1.1204,
      "step": 1816
    },
    {
      "epoch": 0.7034456058846302,
      "grad_norm": 9.470000267028809,
      "learning_rate": 7.034456058846303e-06,
      "loss": 1.5417,
      "step": 1817
    },
    {
      "epoch": 0.7038327526132404,
      "grad_norm": 10.516840934753418,
      "learning_rate": 7.038327526132405e-06,
      "loss": 1.1856,
      "step": 1818
    },
    {
      "epoch": 0.7042198993418506,
      "grad_norm": 16.654998779296875,
      "learning_rate": 7.0421989934185064e-06,
      "loss": 1.5317,
      "step": 1819
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 21.645463943481445,
      "learning_rate": 7.046070460704607e-06,
      "loss": 2.1117,
      "step": 1820
    },
    {
      "epoch": 0.7049941927990708,
      "grad_norm": 10.257989883422852,
      "learning_rate": 7.0499419279907086e-06,
      "loss": 1.5766,
      "step": 1821
    },
    {
      "epoch": 0.705381339527681,
      "grad_norm": 9.5421724319458,
      "learning_rate": 7.053813395276811e-06,
      "loss": 1.6297,
      "step": 1822
    },
    {
      "epoch": 0.7057684862562912,
      "grad_norm": 9.262897491455078,
      "learning_rate": 7.057684862562912e-06,
      "loss": 1.5097,
      "step": 1823
    },
    {
      "epoch": 0.7061556329849012,
      "grad_norm": 10.464183807373047,
      "learning_rate": 7.061556329849013e-06,
      "loss": 1.6753,
      "step": 1824
    },
    {
      "epoch": 0.7065427797135114,
      "grad_norm": 9.965311050415039,
      "learning_rate": 7.0654277971351145e-06,
      "loss": 1.5756,
      "step": 1825
    },
    {
      "epoch": 0.7069299264421216,
      "grad_norm": 10.674848556518555,
      "learning_rate": 7.069299264421217e-06,
      "loss": 1.8911,
      "step": 1826
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 9.590476036071777,
      "learning_rate": 7.0731707317073175e-06,
      "loss": 1.5828,
      "step": 1827
    },
    {
      "epoch": 0.7077042198993418,
      "grad_norm": 15.461480140686035,
      "learning_rate": 7.077042198993419e-06,
      "loss": 1.8836,
      "step": 1828
    },
    {
      "epoch": 0.708091366627952,
      "grad_norm": 31.93998908996582,
      "learning_rate": 7.0809136662795205e-06,
      "loss": 1.9967,
      "step": 1829
    },
    {
      "epoch": 0.7084785133565621,
      "grad_norm": 12.130507469177246,
      "learning_rate": 7.084785133565621e-06,
      "loss": 1.9402,
      "step": 1830
    },
    {
      "epoch": 0.7088656600851723,
      "grad_norm": 14.148700714111328,
      "learning_rate": 7.0886566008517235e-06,
      "loss": 1.9952,
      "step": 1831
    },
    {
      "epoch": 0.7092528068137824,
      "grad_norm": 10.896751403808594,
      "learning_rate": 7.092528068137825e-06,
      "loss": 1.4159,
      "step": 1832
    },
    {
      "epoch": 0.7096399535423926,
      "grad_norm": 17.4697322845459,
      "learning_rate": 7.0963995354239265e-06,
      "loss": 1.6221,
      "step": 1833
    },
    {
      "epoch": 0.7100271002710027,
      "grad_norm": 13.097932815551758,
      "learning_rate": 7.100271002710027e-06,
      "loss": 2.3555,
      "step": 1834
    },
    {
      "epoch": 0.7104142469996129,
      "grad_norm": 12.204648971557617,
      "learning_rate": 7.1041424699961295e-06,
      "loss": 1.8923,
      "step": 1835
    },
    {
      "epoch": 0.710801393728223,
      "grad_norm": 17.001800537109375,
      "learning_rate": 7.108013937282231e-06,
      "loss": 1.6488,
      "step": 1836
    },
    {
      "epoch": 0.7111885404568331,
      "grad_norm": 12.56925106048584,
      "learning_rate": 7.111885404568332e-06,
      "loss": 0.7874,
      "step": 1837
    },
    {
      "epoch": 0.7115756871854433,
      "grad_norm": 11.63797664642334,
      "learning_rate": 7.115756871854433e-06,
      "loss": 1.2369,
      "step": 1838
    },
    {
      "epoch": 0.7119628339140535,
      "grad_norm": 10.654993057250977,
      "learning_rate": 7.1196283391405354e-06,
      "loss": 1.0972,
      "step": 1839
    },
    {
      "epoch": 0.7123499806426635,
      "grad_norm": 11.287028312683105,
      "learning_rate": 7.123499806426636e-06,
      "loss": 1.4792,
      "step": 1840
    },
    {
      "epoch": 0.7127371273712737,
      "grad_norm": 10.462878227233887,
      "learning_rate": 7.127371273712738e-06,
      "loss": 1.1993,
      "step": 1841
    },
    {
      "epoch": 0.7131242740998839,
      "grad_norm": 18.554828643798828,
      "learning_rate": 7.131242740998839e-06,
      "loss": 1.5575,
      "step": 1842
    },
    {
      "epoch": 0.713511420828494,
      "grad_norm": 12.615894317626953,
      "learning_rate": 7.13511420828494e-06,
      "loss": 1.8971,
      "step": 1843
    },
    {
      "epoch": 0.7138985675571041,
      "grad_norm": 13.771031379699707,
      "learning_rate": 7.138985675571042e-06,
      "loss": 1.5347,
      "step": 1844
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 10.006608963012695,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 1.5088,
      "step": 1845
    },
    {
      "epoch": 0.7146728610143245,
      "grad_norm": 12.236921310424805,
      "learning_rate": 7.146728610143245e-06,
      "loss": 1.6373,
      "step": 1846
    },
    {
      "epoch": 0.7150600077429345,
      "grad_norm": 9.374247550964355,
      "learning_rate": 7.150600077429346e-06,
      "loss": 1.5926,
      "step": 1847
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 17.186115264892578,
      "learning_rate": 7.154471544715448e-06,
      "loss": 1.5349,
      "step": 1848
    },
    {
      "epoch": 0.7158343012001549,
      "grad_norm": 12.181107521057129,
      "learning_rate": 7.1583430120015495e-06,
      "loss": 1.9095,
      "step": 1849
    },
    {
      "epoch": 0.716221447928765,
      "grad_norm": 11.187947273254395,
      "learning_rate": 7.16221447928765e-06,
      "loss": 1.3003,
      "step": 1850
    },
    {
      "epoch": 0.7166085946573751,
      "grad_norm": 12.072371482849121,
      "learning_rate": 7.166085946573752e-06,
      "loss": 2.349,
      "step": 1851
    },
    {
      "epoch": 0.7169957413859853,
      "grad_norm": 13.392343521118164,
      "learning_rate": 7.169957413859854e-06,
      "loss": 1.968,
      "step": 1852
    },
    {
      "epoch": 0.7173828881145954,
      "grad_norm": 30.259838104248047,
      "learning_rate": 7.173828881145955e-06,
      "loss": 2.4742,
      "step": 1853
    },
    {
      "epoch": 0.7177700348432056,
      "grad_norm": 11.19955062866211,
      "learning_rate": 7.177700348432056e-06,
      "loss": 1.1376,
      "step": 1854
    },
    {
      "epoch": 0.7181571815718157,
      "grad_norm": 13.725021362304688,
      "learning_rate": 7.181571815718158e-06,
      "loss": 2.4004,
      "step": 1855
    },
    {
      "epoch": 0.7185443283004259,
      "grad_norm": 9.970237731933594,
      "learning_rate": 7.18544328300426e-06,
      "loss": 1.2084,
      "step": 1856
    },
    {
      "epoch": 0.718931475029036,
      "grad_norm": 18.29294204711914,
      "learning_rate": 7.189314750290361e-06,
      "loss": 2.2405,
      "step": 1857
    },
    {
      "epoch": 0.7193186217576462,
      "grad_norm": 22.2736759185791,
      "learning_rate": 7.193186217576462e-06,
      "loss": 1.7058,
      "step": 1858
    },
    {
      "epoch": 0.7197057684862563,
      "grad_norm": 13.911639213562012,
      "learning_rate": 7.197057684862564e-06,
      "loss": 1.984,
      "step": 1859
    },
    {
      "epoch": 0.7200929152148664,
      "grad_norm": 12.247221946716309,
      "learning_rate": 7.200929152148664e-06,
      "loss": 1.348,
      "step": 1860
    },
    {
      "epoch": 0.7204800619434766,
      "grad_norm": 8.755260467529297,
      "learning_rate": 7.204800619434767e-06,
      "loss": 1.8339,
      "step": 1861
    },
    {
      "epoch": 0.7208672086720868,
      "grad_norm": 11.214261054992676,
      "learning_rate": 7.208672086720868e-06,
      "loss": 1.6176,
      "step": 1862
    },
    {
      "epoch": 0.7212543554006968,
      "grad_norm": 12.732138633728027,
      "learning_rate": 7.212543554006969e-06,
      "loss": 1.3676,
      "step": 1863
    },
    {
      "epoch": 0.721641502129307,
      "grad_norm": 10.964174270629883,
      "learning_rate": 7.21641502129307e-06,
      "loss": 1.1852,
      "step": 1864
    },
    {
      "epoch": 0.7220286488579172,
      "grad_norm": 14.338828086853027,
      "learning_rate": 7.220286488579173e-06,
      "loss": 2.0766,
      "step": 1865
    },
    {
      "epoch": 0.7224157955865272,
      "grad_norm": 10.876822471618652,
      "learning_rate": 7.224157955865273e-06,
      "loss": 1.1081,
      "step": 1866
    },
    {
      "epoch": 0.7228029423151374,
      "grad_norm": 18.84029769897461,
      "learning_rate": 7.228029423151375e-06,
      "loss": 1.7809,
      "step": 1867
    },
    {
      "epoch": 0.7231900890437476,
      "grad_norm": 33.96131134033203,
      "learning_rate": 7.231900890437476e-06,
      "loss": 1.7871,
      "step": 1868
    },
    {
      "epoch": 0.7235772357723578,
      "grad_norm": 17.277557373046875,
      "learning_rate": 7.2357723577235786e-06,
      "loss": 1.5063,
      "step": 1869
    },
    {
      "epoch": 0.7239643825009678,
      "grad_norm": 10.946242332458496,
      "learning_rate": 7.239643825009679e-06,
      "loss": 1.6909,
      "step": 1870
    },
    {
      "epoch": 0.724351529229578,
      "grad_norm": 10.621179580688477,
      "learning_rate": 7.243515292295781e-06,
      "loss": 1.13,
      "step": 1871
    },
    {
      "epoch": 0.7247386759581882,
      "grad_norm": 15.861169815063477,
      "learning_rate": 7.247386759581882e-06,
      "loss": 1.7049,
      "step": 1872
    },
    {
      "epoch": 0.7251258226867983,
      "grad_norm": 13.01651668548584,
      "learning_rate": 7.251258226867983e-06,
      "loss": 0.9608,
      "step": 1873
    },
    {
      "epoch": 0.7255129694154084,
      "grad_norm": 13.291890144348145,
      "learning_rate": 7.255129694154085e-06,
      "loss": 1.4186,
      "step": 1874
    },
    {
      "epoch": 0.7259001161440186,
      "grad_norm": 9.263340950012207,
      "learning_rate": 7.259001161440187e-06,
      "loss": 1.2941,
      "step": 1875
    },
    {
      "epoch": 0.7262872628726287,
      "grad_norm": 12.005663871765137,
      "learning_rate": 7.262872628726287e-06,
      "loss": 1.5537,
      "step": 1876
    },
    {
      "epoch": 0.7266744096012389,
      "grad_norm": 14.243425369262695,
      "learning_rate": 7.266744096012389e-06,
      "loss": 1.1913,
      "step": 1877
    },
    {
      "epoch": 0.727061556329849,
      "grad_norm": 17.449832916259766,
      "learning_rate": 7.270615563298491e-06,
      "loss": 2.2363,
      "step": 1878
    },
    {
      "epoch": 0.7274487030584591,
      "grad_norm": 16.841358184814453,
      "learning_rate": 7.274487030584592e-06,
      "loss": 1.6643,
      "step": 1879
    },
    {
      "epoch": 0.7278358497870693,
      "grad_norm": 26.13020896911621,
      "learning_rate": 7.278358497870693e-06,
      "loss": 2.0316,
      "step": 1880
    },
    {
      "epoch": 0.7282229965156795,
      "grad_norm": 29.318880081176758,
      "learning_rate": 7.282229965156795e-06,
      "loss": 0.8627,
      "step": 1881
    },
    {
      "epoch": 0.7286101432442896,
      "grad_norm": 17.875864028930664,
      "learning_rate": 7.286101432442897e-06,
      "loss": 1.4629,
      "step": 1882
    },
    {
      "epoch": 0.7289972899728997,
      "grad_norm": 11.419010162353516,
      "learning_rate": 7.289972899728998e-06,
      "loss": 1.5076,
      "step": 1883
    },
    {
      "epoch": 0.7293844367015099,
      "grad_norm": 18.90157699584961,
      "learning_rate": 7.293844367015099e-06,
      "loss": 1.829,
      "step": 1884
    },
    {
      "epoch": 0.72977158343012,
      "grad_norm": 9.7317533493042,
      "learning_rate": 7.297715834301201e-06,
      "loss": 1.5391,
      "step": 1885
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 12.53754711151123,
      "learning_rate": 7.301587301587301e-06,
      "loss": 1.5928,
      "step": 1886
    },
    {
      "epoch": 0.7305458768873403,
      "grad_norm": 12.51893424987793,
      "learning_rate": 7.305458768873404e-06,
      "loss": 1.007,
      "step": 1887
    },
    {
      "epoch": 0.7309330236159505,
      "grad_norm": 11.269881248474121,
      "learning_rate": 7.309330236159505e-06,
      "loss": 1.2101,
      "step": 1888
    },
    {
      "epoch": 0.7313201703445605,
      "grad_norm": 9.009419441223145,
      "learning_rate": 7.313201703445606e-06,
      "loss": 1.0086,
      "step": 1889
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 12.65145206451416,
      "learning_rate": 7.317073170731707e-06,
      "loss": 2.235,
      "step": 1890
    },
    {
      "epoch": 0.7320944638017809,
      "grad_norm": 10.209554672241211,
      "learning_rate": 7.32094463801781e-06,
      "loss": 1.6514,
      "step": 1891
    },
    {
      "epoch": 0.7324816105303911,
      "grad_norm": 11.349108695983887,
      "learning_rate": 7.324816105303911e-06,
      "loss": 1.1478,
      "step": 1892
    },
    {
      "epoch": 0.7328687572590011,
      "grad_norm": 9.583183288574219,
      "learning_rate": 7.328687572590012e-06,
      "loss": 1.7413,
      "step": 1893
    },
    {
      "epoch": 0.7332559039876113,
      "grad_norm": 7.993554592132568,
      "learning_rate": 7.332559039876113e-06,
      "loss": 1.7242,
      "step": 1894
    },
    {
      "epoch": 0.7336430507162215,
      "grad_norm": 10.382140159606934,
      "learning_rate": 7.336430507162216e-06,
      "loss": 1.5912,
      "step": 1895
    },
    {
      "epoch": 0.7340301974448316,
      "grad_norm": 15.78878116607666,
      "learning_rate": 7.340301974448316e-06,
      "loss": 2.377,
      "step": 1896
    },
    {
      "epoch": 0.7344173441734417,
      "grad_norm": 27.374418258666992,
      "learning_rate": 7.344173441734418e-06,
      "loss": 2.0899,
      "step": 1897
    },
    {
      "epoch": 0.7348044909020519,
      "grad_norm": 26.64742088317871,
      "learning_rate": 7.348044909020519e-06,
      "loss": 2.1844,
      "step": 1898
    },
    {
      "epoch": 0.735191637630662,
      "grad_norm": 14.361444473266602,
      "learning_rate": 7.35191637630662e-06,
      "loss": 1.3762,
      "step": 1899
    },
    {
      "epoch": 0.7355787843592722,
      "grad_norm": 49.87547302246094,
      "learning_rate": 7.355787843592722e-06,
      "loss": 1.5382,
      "step": 1900
    },
    {
      "epoch": 0.7359659310878823,
      "grad_norm": 11.756628036499023,
      "learning_rate": 7.359659310878824e-06,
      "loss": 1.7408,
      "step": 1901
    },
    {
      "epoch": 0.7363530778164924,
      "grad_norm": 8.663679122924805,
      "learning_rate": 7.3635307781649245e-06,
      "loss": 1.0826,
      "step": 1902
    },
    {
      "epoch": 0.7367402245451026,
      "grad_norm": 12.705883979797363,
      "learning_rate": 7.367402245451026e-06,
      "loss": 2.0997,
      "step": 1903
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 13.711716651916504,
      "learning_rate": 7.371273712737128e-06,
      "loss": 2.0705,
      "step": 1904
    },
    {
      "epoch": 0.7375145180023229,
      "grad_norm": 14.677573204040527,
      "learning_rate": 7.37514518002323e-06,
      "loss": 1.5943,
      "step": 1905
    },
    {
      "epoch": 0.737901664730933,
      "grad_norm": 12.284815788269043,
      "learning_rate": 7.3790166473093304e-06,
      "loss": 0.7107,
      "step": 1906
    },
    {
      "epoch": 0.7382888114595432,
      "grad_norm": 13.438404083251953,
      "learning_rate": 7.382888114595432e-06,
      "loss": 1.174,
      "step": 1907
    },
    {
      "epoch": 0.7386759581881533,
      "grad_norm": 17.576393127441406,
      "learning_rate": 7.386759581881534e-06,
      "loss": 1.816,
      "step": 1908
    },
    {
      "epoch": 0.7390631049167634,
      "grad_norm": 30.27766990661621,
      "learning_rate": 7.390631049167635e-06,
      "loss": 2.1301,
      "step": 1909
    },
    {
      "epoch": 0.7394502516453736,
      "grad_norm": 10.517561912536621,
      "learning_rate": 7.394502516453736e-06,
      "loss": 1.5646,
      "step": 1910
    },
    {
      "epoch": 0.7398373983739838,
      "grad_norm": 9.080708503723145,
      "learning_rate": 7.398373983739838e-06,
      "loss": 1.0292,
      "step": 1911
    },
    {
      "epoch": 0.7402245451025938,
      "grad_norm": 10.82419204711914,
      "learning_rate": 7.4022454510259385e-06,
      "loss": 1.2173,
      "step": 1912
    },
    {
      "epoch": 0.740611691831204,
      "grad_norm": 22.4649600982666,
      "learning_rate": 7.406116918312041e-06,
      "loss": 2.1408,
      "step": 1913
    },
    {
      "epoch": 0.7409988385598142,
      "grad_norm": 10.43198299407959,
      "learning_rate": 7.409988385598142e-06,
      "loss": 1.0836,
      "step": 1914
    },
    {
      "epoch": 0.7413859852884244,
      "grad_norm": 14.869677543640137,
      "learning_rate": 7.413859852884244e-06,
      "loss": 1.7277,
      "step": 1915
    },
    {
      "epoch": 0.7417731320170344,
      "grad_norm": 10.25045394897461,
      "learning_rate": 7.4177313201703445e-06,
      "loss": 1.5877,
      "step": 1916
    },
    {
      "epoch": 0.7421602787456446,
      "grad_norm": 12.514726638793945,
      "learning_rate": 7.421602787456447e-06,
      "loss": 0.7247,
      "step": 1917
    },
    {
      "epoch": 0.7425474254742548,
      "grad_norm": 7.74023962020874,
      "learning_rate": 7.425474254742548e-06,
      "loss": 1.1478,
      "step": 1918
    },
    {
      "epoch": 0.7429345722028649,
      "grad_norm": 16.95342254638672,
      "learning_rate": 7.429345722028649e-06,
      "loss": 2.1886,
      "step": 1919
    },
    {
      "epoch": 0.743321718931475,
      "grad_norm": 11.470868110656738,
      "learning_rate": 7.4332171893147505e-06,
      "loss": 1.3637,
      "step": 1920
    },
    {
      "epoch": 0.7437088656600852,
      "grad_norm": 18.00442886352539,
      "learning_rate": 7.437088656600853e-06,
      "loss": 1.5468,
      "step": 1921
    },
    {
      "epoch": 0.7440960123886953,
      "grad_norm": 13.240797996520996,
      "learning_rate": 7.4409601238869535e-06,
      "loss": 1.8159,
      "step": 1922
    },
    {
      "epoch": 0.7444831591173054,
      "grad_norm": 9.970535278320312,
      "learning_rate": 7.444831591173055e-06,
      "loss": 1.0039,
      "step": 1923
    },
    {
      "epoch": 0.7448703058459156,
      "grad_norm": 19.18270492553711,
      "learning_rate": 7.4487030584591565e-06,
      "loss": 1.7457,
      "step": 1924
    },
    {
      "epoch": 0.7452574525745257,
      "grad_norm": 40.182559967041016,
      "learning_rate": 7.452574525745257e-06,
      "loss": 1.679,
      "step": 1925
    },
    {
      "epoch": 0.7456445993031359,
      "grad_norm": 14.895502090454102,
      "learning_rate": 7.4564459930313594e-06,
      "loss": 1.7317,
      "step": 1926
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 20.992162704467773,
      "learning_rate": 7.460317460317461e-06,
      "loss": 2.1566,
      "step": 1927
    },
    {
      "epoch": 0.7464188927603562,
      "grad_norm": 15.920352935791016,
      "learning_rate": 7.4641889276035624e-06,
      "loss": 1.6818,
      "step": 1928
    },
    {
      "epoch": 0.7468060394889663,
      "grad_norm": 9.74396800994873,
      "learning_rate": 7.468060394889663e-06,
      "loss": 1.643,
      "step": 1929
    },
    {
      "epoch": 0.7471931862175765,
      "grad_norm": 16.795475006103516,
      "learning_rate": 7.471931862175765e-06,
      "loss": 2.1981,
      "step": 1930
    },
    {
      "epoch": 0.7475803329461866,
      "grad_norm": 12.765235900878906,
      "learning_rate": 7.475803329461867e-06,
      "loss": 1.688,
      "step": 1931
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 20.37154197692871,
      "learning_rate": 7.4796747967479676e-06,
      "loss": 1.6172,
      "step": 1932
    },
    {
      "epoch": 0.7483546264034069,
      "grad_norm": 14.455954551696777,
      "learning_rate": 7.483546264034069e-06,
      "loss": 1.4268,
      "step": 1933
    },
    {
      "epoch": 0.7487417731320171,
      "grad_norm": 16.610145568847656,
      "learning_rate": 7.487417731320171e-06,
      "loss": 1.373,
      "step": 1934
    },
    {
      "epoch": 0.7491289198606271,
      "grad_norm": 10.616569519042969,
      "learning_rate": 7.491289198606272e-06,
      "loss": 1.1483,
      "step": 1935
    },
    {
      "epoch": 0.7495160665892373,
      "grad_norm": 12.589751243591309,
      "learning_rate": 7.4951606658923735e-06,
      "loss": 1.6187,
      "step": 1936
    },
    {
      "epoch": 0.7499032133178475,
      "grad_norm": 17.257892608642578,
      "learning_rate": 7.499032133178475e-06,
      "loss": 1.6075,
      "step": 1937
    },
    {
      "epoch": 0.7502903600464577,
      "grad_norm": 11.136000633239746,
      "learning_rate": 7.502903600464577e-06,
      "loss": 1.6087,
      "step": 1938
    },
    {
      "epoch": 0.7506775067750677,
      "grad_norm": 25.43038558959961,
      "learning_rate": 7.506775067750678e-06,
      "loss": 1.8974,
      "step": 1939
    },
    {
      "epoch": 0.7510646535036779,
      "grad_norm": 12.164017677307129,
      "learning_rate": 7.5106465350367795e-06,
      "loss": 1.6371,
      "step": 1940
    },
    {
      "epoch": 0.7514518002322881,
      "grad_norm": 12.394525527954102,
      "learning_rate": 7.514518002322881e-06,
      "loss": 1.5595,
      "step": 1941
    },
    {
      "epoch": 0.7518389469608981,
      "grad_norm": 14.04058837890625,
      "learning_rate": 7.5183894696089825e-06,
      "loss": 1.9507,
      "step": 1942
    },
    {
      "epoch": 0.7522260936895083,
      "grad_norm": 14.971114158630371,
      "learning_rate": 7.522260936895084e-06,
      "loss": 1.7544,
      "step": 1943
    },
    {
      "epoch": 0.7526132404181185,
      "grad_norm": 12.41119384765625,
      "learning_rate": 7.5261324041811855e-06,
      "loss": 1.8071,
      "step": 1944
    },
    {
      "epoch": 0.7530003871467286,
      "grad_norm": 9.41286563873291,
      "learning_rate": 7.530003871467286e-06,
      "loss": 1.5262,
      "step": 1945
    },
    {
      "epoch": 0.7533875338753387,
      "grad_norm": 9.069091796875,
      "learning_rate": 7.5338753387533885e-06,
      "loss": 1.4887,
      "step": 1946
    },
    {
      "epoch": 0.7537746806039489,
      "grad_norm": 15.7218599319458,
      "learning_rate": 7.53774680603949e-06,
      "loss": 0.7631,
      "step": 1947
    },
    {
      "epoch": 0.754161827332559,
      "grad_norm": 15.03204345703125,
      "learning_rate": 7.541618273325591e-06,
      "loss": 1.7159,
      "step": 1948
    },
    {
      "epoch": 0.7545489740611692,
      "grad_norm": 11.664665222167969,
      "learning_rate": 7.545489740611692e-06,
      "loss": 1.3532,
      "step": 1949
    },
    {
      "epoch": 0.7549361207897793,
      "grad_norm": 11.342138290405273,
      "learning_rate": 7.5493612078977944e-06,
      "loss": 1.2463,
      "step": 1950
    },
    {
      "epoch": 0.7553232675183895,
      "grad_norm": 13.915107727050781,
      "learning_rate": 7.553232675183896e-06,
      "loss": 1.8022,
      "step": 1951
    },
    {
      "epoch": 0.7557104142469996,
      "grad_norm": 15.865480422973633,
      "learning_rate": 7.557104142469997e-06,
      "loss": 1.612,
      "step": 1952
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 26.49051856994629,
      "learning_rate": 7.560975609756098e-06,
      "loss": 1.9109,
      "step": 1953
    },
    {
      "epoch": 0.7564847077042199,
      "grad_norm": 8.516701698303223,
      "learning_rate": 7.5648470770422e-06,
      "loss": 1.4265,
      "step": 1954
    },
    {
      "epoch": 0.75687185443283,
      "grad_norm": 13.553561210632324,
      "learning_rate": 7.568718544328301e-06,
      "loss": 1.2738,
      "step": 1955
    },
    {
      "epoch": 0.7572590011614402,
      "grad_norm": 11.326796531677246,
      "learning_rate": 7.5725900116144026e-06,
      "loss": 1.543,
      "step": 1956
    },
    {
      "epoch": 0.7576461478900504,
      "grad_norm": 8.318197250366211,
      "learning_rate": 7.576461478900504e-06,
      "loss": 1.5966,
      "step": 1957
    },
    {
      "epoch": 0.7580332946186604,
      "grad_norm": 18.933923721313477,
      "learning_rate": 7.580332946186605e-06,
      "loss": 1.3129,
      "step": 1958
    },
    {
      "epoch": 0.7584204413472706,
      "grad_norm": 13.768263816833496,
      "learning_rate": 7.584204413472707e-06,
      "loss": 1.6965,
      "step": 1959
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 17.770965576171875,
      "learning_rate": 7.5880758807588085e-06,
      "loss": 1.3627,
      "step": 1960
    },
    {
      "epoch": 0.759194734804491,
      "grad_norm": 9.434476852416992,
      "learning_rate": 7.59194734804491e-06,
      "loss": 1.6,
      "step": 1961
    },
    {
      "epoch": 0.759581881533101,
      "grad_norm": 17.576566696166992,
      "learning_rate": 7.595818815331011e-06,
      "loss": 1.5023,
      "step": 1962
    },
    {
      "epoch": 0.7599690282617112,
      "grad_norm": 10.181289672851562,
      "learning_rate": 7.599690282617113e-06,
      "loss": 1.3008,
      "step": 1963
    },
    {
      "epoch": 0.7603561749903214,
      "grad_norm": 26.192546844482422,
      "learning_rate": 7.6035617499032145e-06,
      "loss": 1.8328,
      "step": 1964
    },
    {
      "epoch": 0.7607433217189314,
      "grad_norm": 15.818461418151855,
      "learning_rate": 7.607433217189315e-06,
      "loss": 1.6455,
      "step": 1965
    },
    {
      "epoch": 0.7611304684475416,
      "grad_norm": 9.226573944091797,
      "learning_rate": 7.611304684475417e-06,
      "loss": 1.0409,
      "step": 1966
    },
    {
      "epoch": 0.7615176151761518,
      "grad_norm": 11.794890403747559,
      "learning_rate": 7.615176151761519e-06,
      "loss": 1.5478,
      "step": 1967
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 10.868616104125977,
      "learning_rate": 7.61904761904762e-06,
      "loss": 1.5269,
      "step": 1968
    },
    {
      "epoch": 0.762291908633372,
      "grad_norm": 11.16672420501709,
      "learning_rate": 7.622919086333721e-06,
      "loss": 1.7585,
      "step": 1969
    },
    {
      "epoch": 0.7626790553619822,
      "grad_norm": 11.509918212890625,
      "learning_rate": 7.626790553619823e-06,
      "loss": 1.2451,
      "step": 1970
    },
    {
      "epoch": 0.7630662020905923,
      "grad_norm": 15.383813858032227,
      "learning_rate": 7.630662020905924e-06,
      "loss": 1.7136,
      "step": 1971
    },
    {
      "epoch": 0.7634533488192025,
      "grad_norm": 22.048492431640625,
      "learning_rate": 7.634533488192025e-06,
      "loss": 2.6516,
      "step": 1972
    },
    {
      "epoch": 0.7638404955478126,
      "grad_norm": 16.15256690979004,
      "learning_rate": 7.638404955478127e-06,
      "loss": 1.5158,
      "step": 1973
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 11.238784790039062,
      "learning_rate": 7.64227642276423e-06,
      "loss": 1.6091,
      "step": 1974
    },
    {
      "epoch": 0.7646147890050329,
      "grad_norm": 12.923303604125977,
      "learning_rate": 7.64614789005033e-06,
      "loss": 0.8492,
      "step": 1975
    },
    {
      "epoch": 0.7650019357336431,
      "grad_norm": 16.92905616760254,
      "learning_rate": 7.65001935733643e-06,
      "loss": 2.2878,
      "step": 1976
    },
    {
      "epoch": 0.7653890824622532,
      "grad_norm": 20.7362117767334,
      "learning_rate": 7.653890824622533e-06,
      "loss": 1.8082,
      "step": 1977
    },
    {
      "epoch": 0.7657762291908633,
      "grad_norm": 10.814109802246094,
      "learning_rate": 7.657762291908634e-06,
      "loss": 1.5725,
      "step": 1978
    },
    {
      "epoch": 0.7661633759194735,
      "grad_norm": 13.205665588378906,
      "learning_rate": 7.661633759194736e-06,
      "loss": 1.297,
      "step": 1979
    },
    {
      "epoch": 0.7665505226480837,
      "grad_norm": 14.721141815185547,
      "learning_rate": 7.665505226480837e-06,
      "loss": 1.5405,
      "step": 1980
    },
    {
      "epoch": 0.7669376693766937,
      "grad_norm": 11.625903129577637,
      "learning_rate": 7.669376693766937e-06,
      "loss": 1.6432,
      "step": 1981
    },
    {
      "epoch": 0.7673248161053039,
      "grad_norm": 22.504732131958008,
      "learning_rate": 7.67324816105304e-06,
      "loss": 2.3335,
      "step": 1982
    },
    {
      "epoch": 0.7677119628339141,
      "grad_norm": 21.97978401184082,
      "learning_rate": 7.677119628339142e-06,
      "loss": 1.7436,
      "step": 1983
    },
    {
      "epoch": 0.7680991095625243,
      "grad_norm": 21.98860740661621,
      "learning_rate": 7.680991095625243e-06,
      "loss": 1.7779,
      "step": 1984
    },
    {
      "epoch": 0.7684862562911343,
      "grad_norm": 10.59450912475586,
      "learning_rate": 7.684862562911343e-06,
      "loss": 1.0224,
      "step": 1985
    },
    {
      "epoch": 0.7688734030197445,
      "grad_norm": 11.559497833251953,
      "learning_rate": 7.688734030197446e-06,
      "loss": 1.5719,
      "step": 1986
    },
    {
      "epoch": 0.7692605497483547,
      "grad_norm": 13.249598503112793,
      "learning_rate": 7.692605497483548e-06,
      "loss": 1.4843,
      "step": 1987
    },
    {
      "epoch": 0.7696476964769647,
      "grad_norm": 9.416162490844727,
      "learning_rate": 7.696476964769649e-06,
      "loss": 0.9553,
      "step": 1988
    },
    {
      "epoch": 0.7700348432055749,
      "grad_norm": 13.389005661010742,
      "learning_rate": 7.70034843205575e-06,
      "loss": 1.5369,
      "step": 1989
    },
    {
      "epoch": 0.7704219899341851,
      "grad_norm": 12.763663291931152,
      "learning_rate": 7.704219899341852e-06,
      "loss": 1.3062,
      "step": 1990
    },
    {
      "epoch": 0.7708091366627952,
      "grad_norm": 11.993646621704102,
      "learning_rate": 7.708091366627952e-06,
      "loss": 1.6829,
      "step": 1991
    },
    {
      "epoch": 0.7711962833914053,
      "grad_norm": 11.38809871673584,
      "learning_rate": 7.711962833914055e-06,
      "loss": 1.5214,
      "step": 1992
    },
    {
      "epoch": 0.7715834301200155,
      "grad_norm": 10.566888809204102,
      "learning_rate": 7.715834301200155e-06,
      "loss": 1.2411,
      "step": 1993
    },
    {
      "epoch": 0.7719705768486256,
      "grad_norm": 8.767128944396973,
      "learning_rate": 7.719705768486256e-06,
      "loss": 1.4924,
      "step": 1994
    },
    {
      "epoch": 0.7723577235772358,
      "grad_norm": 12.787650108337402,
      "learning_rate": 7.723577235772358e-06,
      "loss": 0.6316,
      "step": 1995
    },
    {
      "epoch": 0.7727448703058459,
      "grad_norm": 12.293133735656738,
      "learning_rate": 7.72744870305846e-06,
      "loss": 1.1826,
      "step": 1996
    },
    {
      "epoch": 0.7731320170344561,
      "grad_norm": 12.385100364685059,
      "learning_rate": 7.731320170344561e-06,
      "loss": 1.8622,
      "step": 1997
    },
    {
      "epoch": 0.7735191637630662,
      "grad_norm": 56.399009704589844,
      "learning_rate": 7.735191637630662e-06,
      "loss": 1.9428,
      "step": 1998
    },
    {
      "epoch": 0.7739063104916764,
      "grad_norm": 13.542095184326172,
      "learning_rate": 7.739063104916764e-06,
      "loss": 0.6426,
      "step": 1999
    },
    {
      "epoch": 0.7742934572202865,
      "grad_norm": 14.81740665435791,
      "learning_rate": 7.742934572202867e-06,
      "loss": 1.6861,
      "step": 2000
    },
    {
      "epoch": 0.7746806039488966,
      "grad_norm": 21.877193450927734,
      "learning_rate": 7.746806039488967e-06,
      "loss": 1.9161,
      "step": 2001
    },
    {
      "epoch": 0.7750677506775068,
      "grad_norm": 11.67359447479248,
      "learning_rate": 7.750677506775068e-06,
      "loss": 1.1329,
      "step": 2002
    },
    {
      "epoch": 0.775454897406117,
      "grad_norm": 18.83820343017578,
      "learning_rate": 7.75454897406117e-06,
      "loss": 1.7693,
      "step": 2003
    },
    {
      "epoch": 0.775842044134727,
      "grad_norm": 24.510290145874023,
      "learning_rate": 7.758420441347271e-06,
      "loss": 1.5313,
      "step": 2004
    },
    {
      "epoch": 0.7762291908633372,
      "grad_norm": 15.44631290435791,
      "learning_rate": 7.762291908633373e-06,
      "loss": 2.6878,
      "step": 2005
    },
    {
      "epoch": 0.7766163375919474,
      "grad_norm": 24.364273071289062,
      "learning_rate": 7.766163375919474e-06,
      "loss": 1.29,
      "step": 2006
    },
    {
      "epoch": 0.7770034843205574,
      "grad_norm": 15.23204231262207,
      "learning_rate": 7.770034843205574e-06,
      "loss": 2.1408,
      "step": 2007
    },
    {
      "epoch": 0.7773906310491676,
      "grad_norm": 12.872687339782715,
      "learning_rate": 7.773906310491677e-06,
      "loss": 1.3539,
      "step": 2008
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 16.5174560546875,
      "learning_rate": 7.77777777777778e-06,
      "loss": 2.1497,
      "step": 2009
    },
    {
      "epoch": 0.778164924506388,
      "grad_norm": 16.77091407775879,
      "learning_rate": 7.78164924506388e-06,
      "loss": 1.8685,
      "step": 2010
    },
    {
      "epoch": 0.778552071234998,
      "grad_norm": 11.650288581848145,
      "learning_rate": 7.78552071234998e-06,
      "loss": 2.1979,
      "step": 2011
    },
    {
      "epoch": 0.7789392179636082,
      "grad_norm": 8.83530330657959,
      "learning_rate": 7.789392179636083e-06,
      "loss": 1.3675,
      "step": 2012
    },
    {
      "epoch": 0.7793263646922184,
      "grad_norm": 9.870386123657227,
      "learning_rate": 7.793263646922185e-06,
      "loss": 1.7351,
      "step": 2013
    },
    {
      "epoch": 0.7797135114208285,
      "grad_norm": 13.531003952026367,
      "learning_rate": 7.797135114208286e-06,
      "loss": 1.4164,
      "step": 2014
    },
    {
      "epoch": 0.7801006581494386,
      "grad_norm": 8.458600044250488,
      "learning_rate": 7.801006581494386e-06,
      "loss": 1.5804,
      "step": 2015
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 14.386209487915039,
      "learning_rate": 7.804878048780489e-06,
      "loss": 1.6219,
      "step": 2016
    },
    {
      "epoch": 0.7808749516066589,
      "grad_norm": 8.184931755065918,
      "learning_rate": 7.80874951606659e-06,
      "loss": 1.4114,
      "step": 2017
    },
    {
      "epoch": 0.781262098335269,
      "grad_norm": 19.261884689331055,
      "learning_rate": 7.812620983352692e-06,
      "loss": 1.4959,
      "step": 2018
    },
    {
      "epoch": 0.7816492450638792,
      "grad_norm": 15.873129844665527,
      "learning_rate": 7.816492450638792e-06,
      "loss": 1.8075,
      "step": 2019
    },
    {
      "epoch": 0.7820363917924894,
      "grad_norm": 9.34190845489502,
      "learning_rate": 7.820363917924895e-06,
      "loss": 1.4395,
      "step": 2020
    },
    {
      "epoch": 0.7824235385210995,
      "grad_norm": 21.857839584350586,
      "learning_rate": 7.824235385210995e-06,
      "loss": 1.4694,
      "step": 2021
    },
    {
      "epoch": 0.7828106852497096,
      "grad_norm": 14.552980422973633,
      "learning_rate": 7.828106852497098e-06,
      "loss": 1.3637,
      "step": 2022
    },
    {
      "epoch": 0.7831978319783198,
      "grad_norm": 17.919906616210938,
      "learning_rate": 7.831978319783198e-06,
      "loss": 1.469,
      "step": 2023
    },
    {
      "epoch": 0.7835849787069299,
      "grad_norm": 10.988612174987793,
      "learning_rate": 7.835849787069299e-06,
      "loss": 1.7048,
      "step": 2024
    },
    {
      "epoch": 0.7839721254355401,
      "grad_norm": 20.872093200683594,
      "learning_rate": 7.839721254355401e-06,
      "loss": 1.887,
      "step": 2025
    },
    {
      "epoch": 0.7843592721641502,
      "grad_norm": 13.368658065795898,
      "learning_rate": 7.843592721641504e-06,
      "loss": 1.5149,
      "step": 2026
    },
    {
      "epoch": 0.7847464188927603,
      "grad_norm": 8.91414737701416,
      "learning_rate": 7.847464188927604e-06,
      "loss": 1.6903,
      "step": 2027
    },
    {
      "epoch": 0.7851335656213705,
      "grad_norm": 19.51024055480957,
      "learning_rate": 7.851335656213705e-06,
      "loss": 1.7898,
      "step": 2028
    },
    {
      "epoch": 0.7855207123499807,
      "grad_norm": 16.086170196533203,
      "learning_rate": 7.855207123499807e-06,
      "loss": 1.2394,
      "step": 2029
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 8.621088981628418,
      "learning_rate": 7.859078590785908e-06,
      "loss": 1.0175,
      "step": 2030
    },
    {
      "epoch": 0.7862950058072009,
      "grad_norm": 21.321758270263672,
      "learning_rate": 7.86295005807201e-06,
      "loss": 1.303,
      "step": 2031
    },
    {
      "epoch": 0.7866821525358111,
      "grad_norm": 16.430667877197266,
      "learning_rate": 7.866821525358111e-06,
      "loss": 1.7531,
      "step": 2032
    },
    {
      "epoch": 0.7870692992644213,
      "grad_norm": 16.769088745117188,
      "learning_rate": 7.870692992644213e-06,
      "loss": 1.8649,
      "step": 2033
    },
    {
      "epoch": 0.7874564459930313,
      "grad_norm": 16.180889129638672,
      "learning_rate": 7.874564459930314e-06,
      "loss": 1.6573,
      "step": 2034
    },
    {
      "epoch": 0.7878435927216415,
      "grad_norm": 9.34954833984375,
      "learning_rate": 7.878435927216416e-06,
      "loss": 1.6688,
      "step": 2035
    },
    {
      "epoch": 0.7882307394502517,
      "grad_norm": 11.340555191040039,
      "learning_rate": 7.882307394502517e-06,
      "loss": 1.7212,
      "step": 2036
    },
    {
      "epoch": 0.7886178861788617,
      "grad_norm": 10.588753700256348,
      "learning_rate": 7.886178861788618e-06,
      "loss": 1.2372,
      "step": 2037
    },
    {
      "epoch": 0.7890050329074719,
      "grad_norm": 10.633072853088379,
      "learning_rate": 7.89005032907472e-06,
      "loss": 1.4803,
      "step": 2038
    },
    {
      "epoch": 0.7893921796360821,
      "grad_norm": 18.350297927856445,
      "learning_rate": 7.893921796360822e-06,
      "loss": 1.8844,
      "step": 2039
    },
    {
      "epoch": 0.7897793263646922,
      "grad_norm": 12.915491104125977,
      "learning_rate": 7.897793263646923e-06,
      "loss": 1.4727,
      "step": 2040
    },
    {
      "epoch": 0.7901664730933023,
      "grad_norm": 12.982916831970215,
      "learning_rate": 7.901664730933024e-06,
      "loss": 1.404,
      "step": 2041
    },
    {
      "epoch": 0.7905536198219125,
      "grad_norm": 9.356689453125,
      "learning_rate": 7.905536198219126e-06,
      "loss": 1.5784,
      "step": 2042
    },
    {
      "epoch": 0.7909407665505227,
      "grad_norm": 14.527385711669922,
      "learning_rate": 7.909407665505228e-06,
      "loss": 2.1243,
      "step": 2043
    },
    {
      "epoch": 0.7913279132791328,
      "grad_norm": 8.472180366516113,
      "learning_rate": 7.913279132791329e-06,
      "loss": 1.5957,
      "step": 2044
    },
    {
      "epoch": 0.7917150600077429,
      "grad_norm": 16.374982833862305,
      "learning_rate": 7.91715060007743e-06,
      "loss": 1.7892,
      "step": 2045
    },
    {
      "epoch": 0.7921022067363531,
      "grad_norm": 9.015387535095215,
      "learning_rate": 7.921022067363532e-06,
      "loss": 1.3636,
      "step": 2046
    },
    {
      "epoch": 0.7924893534649632,
      "grad_norm": 10.786067962646484,
      "learning_rate": 7.924893534649633e-06,
      "loss": 2.2642,
      "step": 2047
    },
    {
      "epoch": 0.7928765001935734,
      "grad_norm": 26.038461685180664,
      "learning_rate": 7.928765001935735e-06,
      "loss": 1.1573,
      "step": 2048
    },
    {
      "epoch": 0.7932636469221835,
      "grad_norm": 25.198867797851562,
      "learning_rate": 7.932636469221836e-06,
      "loss": 2.2964,
      "step": 2049
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 13.941287994384766,
      "learning_rate": 7.936507936507936e-06,
      "loss": 1.297,
      "step": 2050
    },
    {
      "epoch": 0.7940379403794038,
      "grad_norm": 9.05423641204834,
      "learning_rate": 7.940379403794039e-06,
      "loss": 1.4229,
      "step": 2051
    },
    {
      "epoch": 0.794425087108014,
      "grad_norm": 10.740323066711426,
      "learning_rate": 7.94425087108014e-06,
      "loss": 1.5449,
      "step": 2052
    },
    {
      "epoch": 0.794812233836624,
      "grad_norm": 10.183048248291016,
      "learning_rate": 7.948122338366241e-06,
      "loss": 1.5528,
      "step": 2053
    },
    {
      "epoch": 0.7951993805652342,
      "grad_norm": 15.2091703414917,
      "learning_rate": 7.951993805652342e-06,
      "loss": 1.3319,
      "step": 2054
    },
    {
      "epoch": 0.7955865272938444,
      "grad_norm": 28.51121711730957,
      "learning_rate": 7.955865272938444e-06,
      "loss": 1.2127,
      "step": 2055
    },
    {
      "epoch": 0.7959736740224546,
      "grad_norm": 10.28612232208252,
      "learning_rate": 7.959736740224547e-06,
      "loss": 1.5281,
      "step": 2056
    },
    {
      "epoch": 0.7963608207510646,
      "grad_norm": 27.027082443237305,
      "learning_rate": 7.963608207510647e-06,
      "loss": 1.8897,
      "step": 2057
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 11.64136028289795,
      "learning_rate": 7.967479674796748e-06,
      "loss": 1.6135,
      "step": 2058
    },
    {
      "epoch": 0.797135114208285,
      "grad_norm": 12.317622184753418,
      "learning_rate": 7.97135114208285e-06,
      "loss": 1.137,
      "step": 2059
    },
    {
      "epoch": 0.797522260936895,
      "grad_norm": 18.41493797302246,
      "learning_rate": 7.975222609368951e-06,
      "loss": 1.428,
      "step": 2060
    },
    {
      "epoch": 0.7979094076655052,
      "grad_norm": 9.622678756713867,
      "learning_rate": 7.979094076655053e-06,
      "loss": 0.9097,
      "step": 2061
    },
    {
      "epoch": 0.7982965543941154,
      "grad_norm": 17.047924041748047,
      "learning_rate": 7.982965543941154e-06,
      "loss": 2.5142,
      "step": 2062
    },
    {
      "epoch": 0.7986837011227255,
      "grad_norm": 18.034547805786133,
      "learning_rate": 7.986837011227255e-06,
      "loss": 1.6548,
      "step": 2063
    },
    {
      "epoch": 0.7990708478513356,
      "grad_norm": 11.33230972290039,
      "learning_rate": 7.990708478513357e-06,
      "loss": 1.1807,
      "step": 2064
    },
    {
      "epoch": 0.7994579945799458,
      "grad_norm": 14.44831371307373,
      "learning_rate": 7.99457994579946e-06,
      "loss": 1.574,
      "step": 2065
    },
    {
      "epoch": 0.799845141308556,
      "grad_norm": 23.73099708557129,
      "learning_rate": 7.99845141308556e-06,
      "loss": 1.827,
      "step": 2066
    },
    {
      "epoch": 0.8002322880371661,
      "grad_norm": 10.230483055114746,
      "learning_rate": 8.00232288037166e-06,
      "loss": 1.4317,
      "step": 2067
    },
    {
      "epoch": 0.8006194347657762,
      "grad_norm": 17.13132095336914,
      "learning_rate": 8.006194347657763e-06,
      "loss": 2.127,
      "step": 2068
    },
    {
      "epoch": 0.8010065814943864,
      "grad_norm": 14.252950668334961,
      "learning_rate": 8.010065814943865e-06,
      "loss": 1.7097,
      "step": 2069
    },
    {
      "epoch": 0.8013937282229965,
      "grad_norm": 10.91816234588623,
      "learning_rate": 8.013937282229966e-06,
      "loss": 1.5602,
      "step": 2070
    },
    {
      "epoch": 0.8017808749516067,
      "grad_norm": 17.23995018005371,
      "learning_rate": 8.017808749516067e-06,
      "loss": 1.8387,
      "step": 2071
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 10.96123218536377,
      "learning_rate": 8.021680216802169e-06,
      "loss": 1.3474,
      "step": 2072
    },
    {
      "epoch": 0.8025551684088269,
      "grad_norm": 59.898433685302734,
      "learning_rate": 8.02555168408827e-06,
      "loss": 2.0066,
      "step": 2073
    },
    {
      "epoch": 0.8029423151374371,
      "grad_norm": 9.275467872619629,
      "learning_rate": 8.029423151374372e-06,
      "loss": 1.5191,
      "step": 2074
    },
    {
      "epoch": 0.8033294618660473,
      "grad_norm": 18.352914810180664,
      "learning_rate": 8.033294618660473e-06,
      "loss": 1.7544,
      "step": 2075
    },
    {
      "epoch": 0.8037166085946573,
      "grad_norm": 10.870415687561035,
      "learning_rate": 8.037166085946573e-06,
      "loss": 1.5455,
      "step": 2076
    },
    {
      "epoch": 0.8041037553232675,
      "grad_norm": 10.928840637207031,
      "learning_rate": 8.041037553232676e-06,
      "loss": 1.4908,
      "step": 2077
    },
    {
      "epoch": 0.8044909020518777,
      "grad_norm": 17.364770889282227,
      "learning_rate": 8.044909020518778e-06,
      "loss": 1.3619,
      "step": 2078
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 13.622734069824219,
      "learning_rate": 8.048780487804879e-06,
      "loss": 1.4025,
      "step": 2079
    },
    {
      "epoch": 0.8052651955090979,
      "grad_norm": 23.949111938476562,
      "learning_rate": 8.05265195509098e-06,
      "loss": 1.2608,
      "step": 2080
    },
    {
      "epoch": 0.8056523422377081,
      "grad_norm": 14.556358337402344,
      "learning_rate": 8.056523422377082e-06,
      "loss": 1.7842,
      "step": 2081
    },
    {
      "epoch": 0.8060394889663183,
      "grad_norm": 20.568449020385742,
      "learning_rate": 8.060394889663184e-06,
      "loss": 1.3438,
      "step": 2082
    },
    {
      "epoch": 0.8064266356949283,
      "grad_norm": 16.196861267089844,
      "learning_rate": 8.064266356949285e-06,
      "loss": 1.5254,
      "step": 2083
    },
    {
      "epoch": 0.8068137824235385,
      "grad_norm": 28.71220588684082,
      "learning_rate": 8.068137824235385e-06,
      "loss": 2.3215,
      "step": 2084
    },
    {
      "epoch": 0.8072009291521487,
      "grad_norm": 15.975860595703125,
      "learning_rate": 8.072009291521488e-06,
      "loss": 1.6948,
      "step": 2085
    },
    {
      "epoch": 0.8075880758807588,
      "grad_norm": 14.02998161315918,
      "learning_rate": 8.075880758807588e-06,
      "loss": 1.717,
      "step": 2086
    },
    {
      "epoch": 0.8079752226093689,
      "grad_norm": 41.80805969238281,
      "learning_rate": 8.07975222609369e-06,
      "loss": 1.6576,
      "step": 2087
    },
    {
      "epoch": 0.8083623693379791,
      "grad_norm": 21.00052261352539,
      "learning_rate": 8.083623693379791e-06,
      "loss": 2.0447,
      "step": 2088
    },
    {
      "epoch": 0.8087495160665893,
      "grad_norm": 44.310699462890625,
      "learning_rate": 8.087495160665894e-06,
      "loss": 1.9461,
      "step": 2089
    },
    {
      "epoch": 0.8091366627951994,
      "grad_norm": 10.55066204071045,
      "learning_rate": 8.091366627951994e-06,
      "loss": 0.9652,
      "step": 2090
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 11.417497634887695,
      "learning_rate": 8.095238095238097e-06,
      "loss": 1.5785,
      "step": 2091
    },
    {
      "epoch": 0.8099109562524197,
      "grad_norm": 11.924347877502441,
      "learning_rate": 8.099109562524197e-06,
      "loss": 1.4805,
      "step": 2092
    },
    {
      "epoch": 0.8102981029810298,
      "grad_norm": 18.078807830810547,
      "learning_rate": 8.102981029810298e-06,
      "loss": 1.6493,
      "step": 2093
    },
    {
      "epoch": 0.81068524970964,
      "grad_norm": 13.243597984313965,
      "learning_rate": 8.1068524970964e-06,
      "loss": 1.5893,
      "step": 2094
    },
    {
      "epoch": 0.8110723964382501,
      "grad_norm": 11.638444900512695,
      "learning_rate": 8.110723964382503e-06,
      "loss": 1.4378,
      "step": 2095
    },
    {
      "epoch": 0.8114595431668602,
      "grad_norm": 19.294845581054688,
      "learning_rate": 8.114595431668603e-06,
      "loss": 1.7136,
      "step": 2096
    },
    {
      "epoch": 0.8118466898954704,
      "grad_norm": 13.344961166381836,
      "learning_rate": 8.118466898954704e-06,
      "loss": 1.2045,
      "step": 2097
    },
    {
      "epoch": 0.8122338366240806,
      "grad_norm": 16.976343154907227,
      "learning_rate": 8.122338366240806e-06,
      "loss": 1.8163,
      "step": 2098
    },
    {
      "epoch": 0.8126209833526906,
      "grad_norm": 14.334086418151855,
      "learning_rate": 8.126209833526907e-06,
      "loss": 1.2376,
      "step": 2099
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 10.489884376525879,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.2154,
      "step": 2100
    },
    {
      "epoch": 0.813395276809911,
      "grad_norm": 24.210803985595703,
      "learning_rate": 8.13395276809911e-06,
      "loss": 2.2648,
      "step": 2101
    },
    {
      "epoch": 0.8137824235385211,
      "grad_norm": 15.344009399414062,
      "learning_rate": 8.137824235385212e-06,
      "loss": 1.7779,
      "step": 2102
    },
    {
      "epoch": 0.8141695702671312,
      "grad_norm": 10.385869026184082,
      "learning_rate": 8.141695702671313e-06,
      "loss": 1.7158,
      "step": 2103
    },
    {
      "epoch": 0.8145567169957414,
      "grad_norm": 16.728370666503906,
      "learning_rate": 8.145567169957415e-06,
      "loss": 2.1645,
      "step": 2104
    },
    {
      "epoch": 0.8149438637243516,
      "grad_norm": 11.507981300354004,
      "learning_rate": 8.149438637243516e-06,
      "loss": 1.6753,
      "step": 2105
    },
    {
      "epoch": 0.8153310104529616,
      "grad_norm": 25.624345779418945,
      "learning_rate": 8.153310104529616e-06,
      "loss": 2.1438,
      "step": 2106
    },
    {
      "epoch": 0.8157181571815718,
      "grad_norm": 16.021713256835938,
      "learning_rate": 8.157181571815719e-06,
      "loss": 1.8879,
      "step": 2107
    },
    {
      "epoch": 0.816105303910182,
      "grad_norm": 14.28944206237793,
      "learning_rate": 8.161053039101821e-06,
      "loss": 1.7357,
      "step": 2108
    },
    {
      "epoch": 0.8164924506387921,
      "grad_norm": 13.740686416625977,
      "learning_rate": 8.164924506387922e-06,
      "loss": 1.1798,
      "step": 2109
    },
    {
      "epoch": 0.8168795973674022,
      "grad_norm": 8.679463386535645,
      "learning_rate": 8.168795973674022e-06,
      "loss": 0.998,
      "step": 2110
    },
    {
      "epoch": 0.8172667440960124,
      "grad_norm": 14.888792037963867,
      "learning_rate": 8.172667440960125e-06,
      "loss": 1.8089,
      "step": 2111
    },
    {
      "epoch": 0.8176538908246226,
      "grad_norm": 12.143692970275879,
      "learning_rate": 8.176538908246227e-06,
      "loss": 1.5368,
      "step": 2112
    },
    {
      "epoch": 0.8180410375532327,
      "grad_norm": 25.47806167602539,
      "learning_rate": 8.180410375532328e-06,
      "loss": 1.699,
      "step": 2113
    },
    {
      "epoch": 0.8184281842818428,
      "grad_norm": 14.56418228149414,
      "learning_rate": 8.184281842818428e-06,
      "loss": 1.8969,
      "step": 2114
    },
    {
      "epoch": 0.818815331010453,
      "grad_norm": 11.470760345458984,
      "learning_rate": 8.18815331010453e-06,
      "loss": 1.2484,
      "step": 2115
    },
    {
      "epoch": 0.8192024777390631,
      "grad_norm": 15.581243515014648,
      "learning_rate": 8.192024777390631e-06,
      "loss": 1.5506,
      "step": 2116
    },
    {
      "epoch": 0.8195896244676733,
      "grad_norm": 17.52492904663086,
      "learning_rate": 8.195896244676734e-06,
      "loss": 1.4061,
      "step": 2117
    },
    {
      "epoch": 0.8199767711962834,
      "grad_norm": 12.266321182250977,
      "learning_rate": 8.199767711962834e-06,
      "loss": 1.6297,
      "step": 2118
    },
    {
      "epoch": 0.8203639179248935,
      "grad_norm": 13.054709434509277,
      "learning_rate": 8.203639179248935e-06,
      "loss": 1.6799,
      "step": 2119
    },
    {
      "epoch": 0.8207510646535037,
      "grad_norm": 10.41582202911377,
      "learning_rate": 8.207510646535037e-06,
      "loss": 1.771,
      "step": 2120
    },
    {
      "epoch": 0.8211382113821138,
      "grad_norm": 11.430129051208496,
      "learning_rate": 8.21138211382114e-06,
      "loss": 1.6314,
      "step": 2121
    },
    {
      "epoch": 0.8215253581107239,
      "grad_norm": 11.911107063293457,
      "learning_rate": 8.21525358110724e-06,
      "loss": 1.3811,
      "step": 2122
    },
    {
      "epoch": 0.8219125048393341,
      "grad_norm": 19.053007125854492,
      "learning_rate": 8.219125048393341e-06,
      "loss": 1.6573,
      "step": 2123
    },
    {
      "epoch": 0.8222996515679443,
      "grad_norm": 14.288761138916016,
      "learning_rate": 8.222996515679443e-06,
      "loss": 1.6902,
      "step": 2124
    },
    {
      "epoch": 0.8226867982965544,
      "grad_norm": 20.283615112304688,
      "learning_rate": 8.226867982965546e-06,
      "loss": 1.5305,
      "step": 2125
    },
    {
      "epoch": 0.8230739450251645,
      "grad_norm": 15.44629192352295,
      "learning_rate": 8.230739450251646e-06,
      "loss": 1.5435,
      "step": 2126
    },
    {
      "epoch": 0.8234610917537747,
      "grad_norm": 13.141427040100098,
      "learning_rate": 8.234610917537747e-06,
      "loss": 1.5959,
      "step": 2127
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 11.349234580993652,
      "learning_rate": 8.23848238482385e-06,
      "loss": 2.1449,
      "step": 2128
    },
    {
      "epoch": 0.8242353852109949,
      "grad_norm": 20.24184226989746,
      "learning_rate": 8.24235385210995e-06,
      "loss": 1.7835,
      "step": 2129
    },
    {
      "epoch": 0.8246225319396051,
      "grad_norm": 7.774609088897705,
      "learning_rate": 8.246225319396052e-06,
      "loss": 1.7144,
      "step": 2130
    },
    {
      "epoch": 0.8250096786682153,
      "grad_norm": 10.912466049194336,
      "learning_rate": 8.250096786682153e-06,
      "loss": 1.7148,
      "step": 2131
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 11.331450462341309,
      "learning_rate": 8.253968253968254e-06,
      "loss": 1.6081,
      "step": 2132
    },
    {
      "epoch": 0.8257839721254355,
      "grad_norm": 10.165979385375977,
      "learning_rate": 8.257839721254356e-06,
      "loss": 1.5764,
      "step": 2133
    },
    {
      "epoch": 0.8261711188540457,
      "grad_norm": 11.920113563537598,
      "learning_rate": 8.261711188540458e-06,
      "loss": 1.6907,
      "step": 2134
    },
    {
      "epoch": 0.8265582655826558,
      "grad_norm": 13.684804916381836,
      "learning_rate": 8.265582655826559e-06,
      "loss": 1.5503,
      "step": 2135
    },
    {
      "epoch": 0.826945412311266,
      "grad_norm": 14.786256790161133,
      "learning_rate": 8.26945412311266e-06,
      "loss": 1.6055,
      "step": 2136
    },
    {
      "epoch": 0.8273325590398761,
      "grad_norm": 19.786827087402344,
      "learning_rate": 8.273325590398762e-06,
      "loss": 2.1582,
      "step": 2137
    },
    {
      "epoch": 0.8277197057684863,
      "grad_norm": 17.46035385131836,
      "learning_rate": 8.277197057684864e-06,
      "loss": 1.3962,
      "step": 2138
    },
    {
      "epoch": 0.8281068524970964,
      "grad_norm": 15.019089698791504,
      "learning_rate": 8.281068524970965e-06,
      "loss": 1.8448,
      "step": 2139
    },
    {
      "epoch": 0.8284939992257065,
      "grad_norm": 13.277210235595703,
      "learning_rate": 8.284939992257065e-06,
      "loss": 1.7468,
      "step": 2140
    },
    {
      "epoch": 0.8288811459543167,
      "grad_norm": 13.310478210449219,
      "learning_rate": 8.288811459543168e-06,
      "loss": 1.6477,
      "step": 2141
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 15.190135955810547,
      "learning_rate": 8.292682926829268e-06,
      "loss": 1.6926,
      "step": 2142
    },
    {
      "epoch": 0.829655439411537,
      "grad_norm": 53.67072296142578,
      "learning_rate": 8.29655439411537e-06,
      "loss": 1.4221,
      "step": 2143
    },
    {
      "epoch": 0.8300425861401471,
      "grad_norm": 13.836723327636719,
      "learning_rate": 8.300425861401471e-06,
      "loss": 1.1746,
      "step": 2144
    },
    {
      "epoch": 0.8304297328687572,
      "grad_norm": 9.867587089538574,
      "learning_rate": 8.304297328687572e-06,
      "loss": 1.1726,
      "step": 2145
    },
    {
      "epoch": 0.8308168795973674,
      "grad_norm": 26.833621978759766,
      "learning_rate": 8.308168795973674e-06,
      "loss": 2.722,
      "step": 2146
    },
    {
      "epoch": 0.8312040263259776,
      "grad_norm": 12.140603065490723,
      "learning_rate": 8.312040263259777e-06,
      "loss": 1.0798,
      "step": 2147
    },
    {
      "epoch": 0.8315911730545877,
      "grad_norm": 10.524012565612793,
      "learning_rate": 8.315911730545877e-06,
      "loss": 1.1014,
      "step": 2148
    },
    {
      "epoch": 0.8319783197831978,
      "grad_norm": 14.24618911743164,
      "learning_rate": 8.319783197831978e-06,
      "loss": 1.598,
      "step": 2149
    },
    {
      "epoch": 0.832365466511808,
      "grad_norm": 10.414746284484863,
      "learning_rate": 8.32365466511808e-06,
      "loss": 1.4065,
      "step": 2150
    },
    {
      "epoch": 0.8327526132404182,
      "grad_norm": 23.890275955200195,
      "learning_rate": 8.327526132404183e-06,
      "loss": 1.6112,
      "step": 2151
    },
    {
      "epoch": 0.8331397599690282,
      "grad_norm": 14.585893630981445,
      "learning_rate": 8.331397599690283e-06,
      "loss": 1.189,
      "step": 2152
    },
    {
      "epoch": 0.8335269066976384,
      "grad_norm": 17.004106521606445,
      "learning_rate": 8.335269066976384e-06,
      "loss": 1.6558,
      "step": 2153
    },
    {
      "epoch": 0.8339140534262486,
      "grad_norm": 18.965782165527344,
      "learning_rate": 8.339140534262486e-06,
      "loss": 1.5459,
      "step": 2154
    },
    {
      "epoch": 0.8343012001548586,
      "grad_norm": 31.639196395874023,
      "learning_rate": 8.343012001548587e-06,
      "loss": 1.8591,
      "step": 2155
    },
    {
      "epoch": 0.8346883468834688,
      "grad_norm": 26.23279571533203,
      "learning_rate": 8.34688346883469e-06,
      "loss": 1.4141,
      "step": 2156
    },
    {
      "epoch": 0.835075493612079,
      "grad_norm": 8.058737754821777,
      "learning_rate": 8.35075493612079e-06,
      "loss": 1.4096,
      "step": 2157
    },
    {
      "epoch": 0.8354626403406891,
      "grad_norm": 19.554719924926758,
      "learning_rate": 8.35462640340689e-06,
      "loss": 1.8641,
      "step": 2158
    },
    {
      "epoch": 0.8358497870692992,
      "grad_norm": 18.35394287109375,
      "learning_rate": 8.358497870692993e-06,
      "loss": 1.591,
      "step": 2159
    },
    {
      "epoch": 0.8362369337979094,
      "grad_norm": 9.319986343383789,
      "learning_rate": 8.362369337979095e-06,
      "loss": 0.9274,
      "step": 2160
    },
    {
      "epoch": 0.8366240805265196,
      "grad_norm": 19.622718811035156,
      "learning_rate": 8.366240805265196e-06,
      "loss": 1.7086,
      "step": 2161
    },
    {
      "epoch": 0.8370112272551297,
      "grad_norm": 15.330463409423828,
      "learning_rate": 8.370112272551297e-06,
      "loss": 2.1132,
      "step": 2162
    },
    {
      "epoch": 0.8373983739837398,
      "grad_norm": 14.79594612121582,
      "learning_rate": 8.373983739837399e-06,
      "loss": 2.1258,
      "step": 2163
    },
    {
      "epoch": 0.83778552071235,
      "grad_norm": 9.78537368774414,
      "learning_rate": 8.377855207123501e-06,
      "loss": 1.1433,
      "step": 2164
    },
    {
      "epoch": 0.8381726674409601,
      "grad_norm": 8.926616668701172,
      "learning_rate": 8.381726674409602e-06,
      "loss": 1.5136,
      "step": 2165
    },
    {
      "epoch": 0.8385598141695703,
      "grad_norm": 15.533157348632812,
      "learning_rate": 8.385598141695703e-06,
      "loss": 1.629,
      "step": 2166
    },
    {
      "epoch": 0.8389469608981804,
      "grad_norm": 17.258729934692383,
      "learning_rate": 8.389469608981805e-06,
      "loss": 2.4779,
      "step": 2167
    },
    {
      "epoch": 0.8393341076267905,
      "grad_norm": 20.144193649291992,
      "learning_rate": 8.393341076267906e-06,
      "loss": 1.7957,
      "step": 2168
    },
    {
      "epoch": 0.8397212543554007,
      "grad_norm": 12.98810863494873,
      "learning_rate": 8.397212543554008e-06,
      "loss": 1.2313,
      "step": 2169
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 54.01066970825195,
      "learning_rate": 8.401084010840109e-06,
      "loss": 1.0123,
      "step": 2170
    },
    {
      "epoch": 0.840495547812621,
      "grad_norm": 14.167572021484375,
      "learning_rate": 8.404955478126211e-06,
      "loss": 0.6615,
      "step": 2171
    },
    {
      "epoch": 0.8408826945412311,
      "grad_norm": 9.89950942993164,
      "learning_rate": 8.408826945412312e-06,
      "loss": 1.4337,
      "step": 2172
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 11.962043762207031,
      "learning_rate": 8.412698412698414e-06,
      "loss": 1.7105,
      "step": 2173
    },
    {
      "epoch": 0.8416569879984515,
      "grad_norm": 13.910822868347168,
      "learning_rate": 8.416569879984515e-06,
      "loss": 1.96,
      "step": 2174
    },
    {
      "epoch": 0.8420441347270615,
      "grad_norm": 19.704757690429688,
      "learning_rate": 8.420441347270615e-06,
      "loss": 3.1724,
      "step": 2175
    },
    {
      "epoch": 0.8424312814556717,
      "grad_norm": 13.390996932983398,
      "learning_rate": 8.424312814556718e-06,
      "loss": 1.7029,
      "step": 2176
    },
    {
      "epoch": 0.8428184281842819,
      "grad_norm": 11.462701797485352,
      "learning_rate": 8.42818428184282e-06,
      "loss": 1.1609,
      "step": 2177
    },
    {
      "epoch": 0.8432055749128919,
      "grad_norm": 20.789403915405273,
      "learning_rate": 8.43205574912892e-06,
      "loss": 2.0266,
      "step": 2178
    },
    {
      "epoch": 0.8435927216415021,
      "grad_norm": 10.42020320892334,
      "learning_rate": 8.435927216415021e-06,
      "loss": 1.6771,
      "step": 2179
    },
    {
      "epoch": 0.8439798683701123,
      "grad_norm": 18.41660499572754,
      "learning_rate": 8.439798683701124e-06,
      "loss": 1.6697,
      "step": 2180
    },
    {
      "epoch": 0.8443670150987224,
      "grad_norm": 14.073445320129395,
      "learning_rate": 8.443670150987224e-06,
      "loss": 1.1859,
      "step": 2181
    },
    {
      "epoch": 0.8447541618273325,
      "grad_norm": 15.706205368041992,
      "learning_rate": 8.447541618273327e-06,
      "loss": 1.4689,
      "step": 2182
    },
    {
      "epoch": 0.8451413085559427,
      "grad_norm": 8.167473793029785,
      "learning_rate": 8.451413085559427e-06,
      "loss": 1.3869,
      "step": 2183
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 12.855470657348633,
      "learning_rate": 8.45528455284553e-06,
      "loss": 1.4704,
      "step": 2184
    },
    {
      "epoch": 0.845915602013163,
      "grad_norm": 37.359683990478516,
      "learning_rate": 8.45915602013163e-06,
      "loss": 2.1863,
      "step": 2185
    },
    {
      "epoch": 0.8463027487417731,
      "grad_norm": 16.507675170898438,
      "learning_rate": 8.463027487417732e-06,
      "loss": 1.2882,
      "step": 2186
    },
    {
      "epoch": 0.8466898954703833,
      "grad_norm": 13.112751007080078,
      "learning_rate": 8.466898954703833e-06,
      "loss": 1.6452,
      "step": 2187
    },
    {
      "epoch": 0.8470770421989934,
      "grad_norm": 18.63865852355957,
      "learning_rate": 8.470770421989934e-06,
      "loss": 2.1938,
      "step": 2188
    },
    {
      "epoch": 0.8474641889276036,
      "grad_norm": 19.7222900390625,
      "learning_rate": 8.474641889276036e-06,
      "loss": 2.2125,
      "step": 2189
    },
    {
      "epoch": 0.8478513356562137,
      "grad_norm": 23.135313034057617,
      "learning_rate": 8.478513356562138e-06,
      "loss": 1.8557,
      "step": 2190
    },
    {
      "epoch": 0.8482384823848238,
      "grad_norm": 9.902103424072266,
      "learning_rate": 8.482384823848239e-06,
      "loss": 1.1417,
      "step": 2191
    },
    {
      "epoch": 0.848625629113434,
      "grad_norm": 11.430632591247559,
      "learning_rate": 8.48625629113434e-06,
      "loss": 1.2018,
      "step": 2192
    },
    {
      "epoch": 0.8490127758420442,
      "grad_norm": 24.98682975769043,
      "learning_rate": 8.490127758420442e-06,
      "loss": 1.4995,
      "step": 2193
    },
    {
      "epoch": 0.8493999225706543,
      "grad_norm": 35.070133209228516,
      "learning_rate": 8.493999225706544e-06,
      "loss": 1.4302,
      "step": 2194
    },
    {
      "epoch": 0.8497870692992644,
      "grad_norm": 29.556665420532227,
      "learning_rate": 8.497870692992645e-06,
      "loss": 1.2417,
      "step": 2195
    },
    {
      "epoch": 0.8501742160278746,
      "grad_norm": 14.488005638122559,
      "learning_rate": 8.501742160278746e-06,
      "loss": 1.3895,
      "step": 2196
    },
    {
      "epoch": 0.8505613627564848,
      "grad_norm": 9.084390640258789,
      "learning_rate": 8.505613627564848e-06,
      "loss": 1.4808,
      "step": 2197
    },
    {
      "epoch": 0.8509485094850948,
      "grad_norm": 9.788613319396973,
      "learning_rate": 8.509485094850949e-06,
      "loss": 1.5267,
      "step": 2198
    },
    {
      "epoch": 0.851335656213705,
      "grad_norm": 11.75755786895752,
      "learning_rate": 8.513356562137051e-06,
      "loss": 1.6979,
      "step": 2199
    },
    {
      "epoch": 0.8517228029423152,
      "grad_norm": 13.124393463134766,
      "learning_rate": 8.517228029423152e-06,
      "loss": 1.6381,
      "step": 2200
    },
    {
      "epoch": 0.8521099496709252,
      "grad_norm": 11.733028411865234,
      "learning_rate": 8.521099496709252e-06,
      "loss": 1.6207,
      "step": 2201
    },
    {
      "epoch": 0.8524970963995354,
      "grad_norm": 19.760021209716797,
      "learning_rate": 8.524970963995355e-06,
      "loss": 1.5988,
      "step": 2202
    },
    {
      "epoch": 0.8528842431281456,
      "grad_norm": 15.45322036743164,
      "learning_rate": 8.528842431281457e-06,
      "loss": 1.612,
      "step": 2203
    },
    {
      "epoch": 0.8532713898567557,
      "grad_norm": 25.96436309814453,
      "learning_rate": 8.532713898567558e-06,
      "loss": 2.0123,
      "step": 2204
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 18.135671615600586,
      "learning_rate": 8.536585365853658e-06,
      "loss": 1.4325,
      "step": 2205
    },
    {
      "epoch": 0.854045683313976,
      "grad_norm": 9.199226379394531,
      "learning_rate": 8.54045683313976e-06,
      "loss": 0.9241,
      "step": 2206
    },
    {
      "epoch": 0.8544328300425862,
      "grad_norm": 31.8306941986084,
      "learning_rate": 8.544328300425863e-06,
      "loss": 1.6128,
      "step": 2207
    },
    {
      "epoch": 0.8548199767711963,
      "grad_norm": 10.949952125549316,
      "learning_rate": 8.548199767711964e-06,
      "loss": 1.2165,
      "step": 2208
    },
    {
      "epoch": 0.8552071234998064,
      "grad_norm": 17.183326721191406,
      "learning_rate": 8.552071234998064e-06,
      "loss": 1.9418,
      "step": 2209
    },
    {
      "epoch": 0.8555942702284166,
      "grad_norm": 17.843435287475586,
      "learning_rate": 8.555942702284167e-06,
      "loss": 1.6941,
      "step": 2210
    },
    {
      "epoch": 0.8559814169570267,
      "grad_norm": 17.529129028320312,
      "learning_rate": 8.559814169570267e-06,
      "loss": 1.7653,
      "step": 2211
    },
    {
      "epoch": 0.8563685636856369,
      "grad_norm": 15.642940521240234,
      "learning_rate": 8.56368563685637e-06,
      "loss": 1.5423,
      "step": 2212
    },
    {
      "epoch": 0.856755710414247,
      "grad_norm": 15.752030372619629,
      "learning_rate": 8.56755710414247e-06,
      "loss": 1.7215,
      "step": 2213
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 23.798702239990234,
      "learning_rate": 8.571428571428571e-06,
      "loss": 1.5234,
      "step": 2214
    },
    {
      "epoch": 0.8575300038714673,
      "grad_norm": 24.3959903717041,
      "learning_rate": 8.575300038714673e-06,
      "loss": 1.217,
      "step": 2215
    },
    {
      "epoch": 0.8579171506000774,
      "grad_norm": 9.18332576751709,
      "learning_rate": 8.579171506000776e-06,
      "loss": 1.6293,
      "step": 2216
    },
    {
      "epoch": 0.8583042973286876,
      "grad_norm": 11.722647666931152,
      "learning_rate": 8.583042973286876e-06,
      "loss": 1.6938,
      "step": 2217
    },
    {
      "epoch": 0.8586914440572977,
      "grad_norm": 14.896014213562012,
      "learning_rate": 8.586914440572977e-06,
      "loss": 2.1324,
      "step": 2218
    },
    {
      "epoch": 0.8590785907859079,
      "grad_norm": 15.223676681518555,
      "learning_rate": 8.59078590785908e-06,
      "loss": 1.2524,
      "step": 2219
    },
    {
      "epoch": 0.859465737514518,
      "grad_norm": 12.06539535522461,
      "learning_rate": 8.594657375145182e-06,
      "loss": 1.5714,
      "step": 2220
    },
    {
      "epoch": 0.8598528842431281,
      "grad_norm": 16.286516189575195,
      "learning_rate": 8.598528842431282e-06,
      "loss": 2.1073,
      "step": 2221
    },
    {
      "epoch": 0.8602400309717383,
      "grad_norm": 13.37855339050293,
      "learning_rate": 8.602400309717383e-06,
      "loss": 1.2769,
      "step": 2222
    },
    {
      "epoch": 0.8606271777003485,
      "grad_norm": 14.750328063964844,
      "learning_rate": 8.606271777003485e-06,
      "loss": 1.1355,
      "step": 2223
    },
    {
      "epoch": 0.8610143244289585,
      "grad_norm": 12.071784973144531,
      "learning_rate": 8.610143244289586e-06,
      "loss": 1.6066,
      "step": 2224
    },
    {
      "epoch": 0.8614014711575687,
      "grad_norm": 16.02212905883789,
      "learning_rate": 8.614014711575688e-06,
      "loss": 1.8746,
      "step": 2225
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 10.740756034851074,
      "learning_rate": 8.617886178861789e-06,
      "loss": 1.1276,
      "step": 2226
    },
    {
      "epoch": 0.862175764614789,
      "grad_norm": 14.076102256774902,
      "learning_rate": 8.62175764614789e-06,
      "loss": 1.3813,
      "step": 2227
    },
    {
      "epoch": 0.8625629113433991,
      "grad_norm": 11.554852485656738,
      "learning_rate": 8.625629113433992e-06,
      "loss": 1.6069,
      "step": 2228
    },
    {
      "epoch": 0.8629500580720093,
      "grad_norm": 13.958612442016602,
      "learning_rate": 8.629500580720094e-06,
      "loss": 1.5389,
      "step": 2229
    },
    {
      "epoch": 0.8633372048006195,
      "grad_norm": 14.800100326538086,
      "learning_rate": 8.633372048006195e-06,
      "loss": 1.1257,
      "step": 2230
    },
    {
      "epoch": 0.8637243515292296,
      "grad_norm": 12.484302520751953,
      "learning_rate": 8.637243515292295e-06,
      "loss": 2.019,
      "step": 2231
    },
    {
      "epoch": 0.8641114982578397,
      "grad_norm": 12.822870254516602,
      "learning_rate": 8.641114982578398e-06,
      "loss": 1.6302,
      "step": 2232
    },
    {
      "epoch": 0.8644986449864499,
      "grad_norm": 16.397939682006836,
      "learning_rate": 8.6449864498645e-06,
      "loss": 1.9165,
      "step": 2233
    },
    {
      "epoch": 0.86488579171506,
      "grad_norm": 17.83855628967285,
      "learning_rate": 8.6488579171506e-06,
      "loss": 1.4446,
      "step": 2234
    },
    {
      "epoch": 0.8652729384436701,
      "grad_norm": 18.972755432128906,
      "learning_rate": 8.652729384436701e-06,
      "loss": 1.8924,
      "step": 2235
    },
    {
      "epoch": 0.8656600851722803,
      "grad_norm": 21.031139373779297,
      "learning_rate": 8.656600851722804e-06,
      "loss": 2.4871,
      "step": 2236
    },
    {
      "epoch": 0.8660472319008904,
      "grad_norm": 17.726720809936523,
      "learning_rate": 8.660472319008904e-06,
      "loss": 2.0166,
      "step": 2237
    },
    {
      "epoch": 0.8664343786295006,
      "grad_norm": 14.419464111328125,
      "learning_rate": 8.664343786295007e-06,
      "loss": 2.0454,
      "step": 2238
    },
    {
      "epoch": 0.8668215253581107,
      "grad_norm": 20.74118995666504,
      "learning_rate": 8.668215253581107e-06,
      "loss": 1.7399,
      "step": 2239
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 20.333431243896484,
      "learning_rate": 8.67208672086721e-06,
      "loss": 1.3718,
      "step": 2240
    },
    {
      "epoch": 0.867595818815331,
      "grad_norm": 20.801889419555664,
      "learning_rate": 8.67595818815331e-06,
      "loss": 1.4769,
      "step": 2241
    },
    {
      "epoch": 0.8679829655439412,
      "grad_norm": 15.907133102416992,
      "learning_rate": 8.679829655439413e-06,
      "loss": 1.4795,
      "step": 2242
    },
    {
      "epoch": 0.8683701122725513,
      "grad_norm": 12.965600967407227,
      "learning_rate": 8.683701122725513e-06,
      "loss": 0.904,
      "step": 2243
    },
    {
      "epoch": 0.8687572590011614,
      "grad_norm": 12.582987785339355,
      "learning_rate": 8.687572590011614e-06,
      "loss": 1.1121,
      "step": 2244
    },
    {
      "epoch": 0.8691444057297716,
      "grad_norm": 23.34198570251465,
      "learning_rate": 8.691444057297716e-06,
      "loss": 1.1769,
      "step": 2245
    },
    {
      "epoch": 0.8695315524583818,
      "grad_norm": 12.891820907592773,
      "learning_rate": 8.695315524583819e-06,
      "loss": 0.7337,
      "step": 2246
    },
    {
      "epoch": 0.8699186991869918,
      "grad_norm": 24.68622398376465,
      "learning_rate": 8.69918699186992e-06,
      "loss": 2.071,
      "step": 2247
    },
    {
      "epoch": 0.870305845915602,
      "grad_norm": 14.691848754882812,
      "learning_rate": 8.70305845915602e-06,
      "loss": 2.1575,
      "step": 2248
    },
    {
      "epoch": 0.8706929926442122,
      "grad_norm": 20.639516830444336,
      "learning_rate": 8.706929926442122e-06,
      "loss": 2.0549,
      "step": 2249
    },
    {
      "epoch": 0.8710801393728222,
      "grad_norm": 12.297811508178711,
      "learning_rate": 8.710801393728223e-06,
      "loss": 1.4962,
      "step": 2250
    },
    {
      "epoch": 0.8714672861014324,
      "grad_norm": 17.77300262451172,
      "learning_rate": 8.714672861014325e-06,
      "loss": 2.23,
      "step": 2251
    },
    {
      "epoch": 0.8718544328300426,
      "grad_norm": 11.60481071472168,
      "learning_rate": 8.718544328300426e-06,
      "loss": 1.6528,
      "step": 2252
    },
    {
      "epoch": 0.8722415795586528,
      "grad_norm": 15.060504913330078,
      "learning_rate": 8.722415795586528e-06,
      "loss": 1.5089,
      "step": 2253
    },
    {
      "epoch": 0.8726287262872628,
      "grad_norm": 14.47146224975586,
      "learning_rate": 8.726287262872629e-06,
      "loss": 1.4909,
      "step": 2254
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 11.070438385009766,
      "learning_rate": 8.730158730158731e-06,
      "loss": 1.4419,
      "step": 2255
    },
    {
      "epoch": 0.8734030197444832,
      "grad_norm": 12.088664054870605,
      "learning_rate": 8.734030197444832e-06,
      "loss": 1.1327,
      "step": 2256
    },
    {
      "epoch": 0.8737901664730933,
      "grad_norm": 17.66997528076172,
      "learning_rate": 8.737901664730933e-06,
      "loss": 1.4289,
      "step": 2257
    },
    {
      "epoch": 0.8741773132017034,
      "grad_norm": 14.155426025390625,
      "learning_rate": 8.741773132017035e-06,
      "loss": 1.535,
      "step": 2258
    },
    {
      "epoch": 0.8745644599303136,
      "grad_norm": 12.567061424255371,
      "learning_rate": 8.745644599303137e-06,
      "loss": 1.4027,
      "step": 2259
    },
    {
      "epoch": 0.8749516066589237,
      "grad_norm": 15.7269926071167,
      "learning_rate": 8.749516066589238e-06,
      "loss": 1.4648,
      "step": 2260
    },
    {
      "epoch": 0.8753387533875339,
      "grad_norm": 20.81868553161621,
      "learning_rate": 8.753387533875339e-06,
      "loss": 1.9668,
      "step": 2261
    },
    {
      "epoch": 0.875725900116144,
      "grad_norm": 12.825963973999023,
      "learning_rate": 8.757259001161441e-06,
      "loss": 1.662,
      "step": 2262
    },
    {
      "epoch": 0.8761130468447541,
      "grad_norm": 13.30033016204834,
      "learning_rate": 8.761130468447542e-06,
      "loss": 1.7352,
      "step": 2263
    },
    {
      "epoch": 0.8765001935733643,
      "grad_norm": 16.659343719482422,
      "learning_rate": 8.765001935733644e-06,
      "loss": 1.3876,
      "step": 2264
    },
    {
      "epoch": 0.8768873403019745,
      "grad_norm": 16.27847671508789,
      "learning_rate": 8.768873403019745e-06,
      "loss": 1.5372,
      "step": 2265
    },
    {
      "epoch": 0.8772744870305846,
      "grad_norm": 19.99696922302246,
      "learning_rate": 8.772744870305847e-06,
      "loss": 0.9447,
      "step": 2266
    },
    {
      "epoch": 0.8776616337591947,
      "grad_norm": 15.020037651062012,
      "learning_rate": 8.776616337591948e-06,
      "loss": 1.401,
      "step": 2267
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 13.22045612335205,
      "learning_rate": 8.78048780487805e-06,
      "loss": 1.5589,
      "step": 2268
    },
    {
      "epoch": 0.8784359272164151,
      "grad_norm": 28.87611198425293,
      "learning_rate": 8.78435927216415e-06,
      "loss": 1.5489,
      "step": 2269
    },
    {
      "epoch": 0.8788230739450251,
      "grad_norm": 14.399734497070312,
      "learning_rate": 8.788230739450251e-06,
      "loss": 1.6196,
      "step": 2270
    },
    {
      "epoch": 0.8792102206736353,
      "grad_norm": 18.10629653930664,
      "learning_rate": 8.792102206736354e-06,
      "loss": 1.4175,
      "step": 2271
    },
    {
      "epoch": 0.8795973674022455,
      "grad_norm": 11.851116180419922,
      "learning_rate": 8.795973674022456e-06,
      "loss": 1.1562,
      "step": 2272
    },
    {
      "epoch": 0.8799845141308555,
      "grad_norm": 16.055912017822266,
      "learning_rate": 8.799845141308556e-06,
      "loss": 1.5998,
      "step": 2273
    },
    {
      "epoch": 0.8803716608594657,
      "grad_norm": 32.25862503051758,
      "learning_rate": 8.803716608594657e-06,
      "loss": 1.5831,
      "step": 2274
    },
    {
      "epoch": 0.8807588075880759,
      "grad_norm": 15.607915878295898,
      "learning_rate": 8.80758807588076e-06,
      "loss": 1.5792,
      "step": 2275
    },
    {
      "epoch": 0.8811459543166861,
      "grad_norm": 18.523290634155273,
      "learning_rate": 8.811459543166862e-06,
      "loss": 1.6403,
      "step": 2276
    },
    {
      "epoch": 0.8815331010452961,
      "grad_norm": 23.760475158691406,
      "learning_rate": 8.815331010452962e-06,
      "loss": 1.4693,
      "step": 2277
    },
    {
      "epoch": 0.8819202477739063,
      "grad_norm": 17.157516479492188,
      "learning_rate": 8.819202477739063e-06,
      "loss": 1.7005,
      "step": 2278
    },
    {
      "epoch": 0.8823073945025165,
      "grad_norm": 17.168176651000977,
      "learning_rate": 8.823073945025165e-06,
      "loss": 1.5906,
      "step": 2279
    },
    {
      "epoch": 0.8826945412311266,
      "grad_norm": 16.34307861328125,
      "learning_rate": 8.826945412311266e-06,
      "loss": 1.4369,
      "step": 2280
    },
    {
      "epoch": 0.8830816879597367,
      "grad_norm": 14.74492073059082,
      "learning_rate": 8.830816879597368e-06,
      "loss": 1.2281,
      "step": 2281
    },
    {
      "epoch": 0.8834688346883469,
      "grad_norm": 11.97536563873291,
      "learning_rate": 8.834688346883469e-06,
      "loss": 1.3218,
      "step": 2282
    },
    {
      "epoch": 0.883855981416957,
      "grad_norm": 24.429309844970703,
      "learning_rate": 8.83855981416957e-06,
      "loss": 1.4704,
      "step": 2283
    },
    {
      "epoch": 0.8842431281455672,
      "grad_norm": 12.320389747619629,
      "learning_rate": 8.842431281455672e-06,
      "loss": 1.1616,
      "step": 2284
    },
    {
      "epoch": 0.8846302748741773,
      "grad_norm": 14.957619667053223,
      "learning_rate": 8.846302748741774e-06,
      "loss": 1.5386,
      "step": 2285
    },
    {
      "epoch": 0.8850174216027874,
      "grad_norm": 12.924386978149414,
      "learning_rate": 8.850174216027875e-06,
      "loss": 1.4798,
      "step": 2286
    },
    {
      "epoch": 0.8854045683313976,
      "grad_norm": 16.73602867126465,
      "learning_rate": 8.854045683313976e-06,
      "loss": 1.3266,
      "step": 2287
    },
    {
      "epoch": 0.8857917150600078,
      "grad_norm": 11.90524959564209,
      "learning_rate": 8.857917150600078e-06,
      "loss": 1.1304,
      "step": 2288
    },
    {
      "epoch": 0.8861788617886179,
      "grad_norm": 13.111261367797852,
      "learning_rate": 8.86178861788618e-06,
      "loss": 1.612,
      "step": 2289
    },
    {
      "epoch": 0.886566008517228,
      "grad_norm": 10.81860065460205,
      "learning_rate": 8.865660085172281e-06,
      "loss": 1.0047,
      "step": 2290
    },
    {
      "epoch": 0.8869531552458382,
      "grad_norm": 11.84177303314209,
      "learning_rate": 8.869531552458382e-06,
      "loss": 0.9812,
      "step": 2291
    },
    {
      "epoch": 0.8873403019744484,
      "grad_norm": 20.840534210205078,
      "learning_rate": 8.873403019744484e-06,
      "loss": 1.7371,
      "step": 2292
    },
    {
      "epoch": 0.8877274487030584,
      "grad_norm": 22.989032745361328,
      "learning_rate": 8.877274487030585e-06,
      "loss": 1.6599,
      "step": 2293
    },
    {
      "epoch": 0.8881145954316686,
      "grad_norm": 12.005914688110352,
      "learning_rate": 8.881145954316687e-06,
      "loss": 1.1745,
      "step": 2294
    },
    {
      "epoch": 0.8885017421602788,
      "grad_norm": 13.745367050170898,
      "learning_rate": 8.885017421602788e-06,
      "loss": 1.7005,
      "step": 2295
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 13.400556564331055,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.4364,
      "step": 2296
    },
    {
      "epoch": 0.889276035617499,
      "grad_norm": 15.58880615234375,
      "learning_rate": 8.89276035617499e-06,
      "loss": 1.2581,
      "step": 2297
    },
    {
      "epoch": 0.8896631823461092,
      "grad_norm": 15.218204498291016,
      "learning_rate": 8.896631823461093e-06,
      "loss": 1.6279,
      "step": 2298
    },
    {
      "epoch": 0.8900503290747194,
      "grad_norm": 10.251004219055176,
      "learning_rate": 8.900503290747194e-06,
      "loss": 1.5468,
      "step": 2299
    },
    {
      "epoch": 0.8904374758033294,
      "grad_norm": 11.572959899902344,
      "learning_rate": 8.904374758033294e-06,
      "loss": 1.119,
      "step": 2300
    },
    {
      "epoch": 0.8908246225319396,
      "grad_norm": 13.885424613952637,
      "learning_rate": 8.908246225319397e-06,
      "loss": 2.0078,
      "step": 2301
    },
    {
      "epoch": 0.8912117692605498,
      "grad_norm": 8.547845840454102,
      "learning_rate": 8.912117692605499e-06,
      "loss": 1.403,
      "step": 2302
    },
    {
      "epoch": 0.8915989159891599,
      "grad_norm": 19.928274154663086,
      "learning_rate": 8.9159891598916e-06,
      "loss": 2.2332,
      "step": 2303
    },
    {
      "epoch": 0.89198606271777,
      "grad_norm": 10.80008602142334,
      "learning_rate": 8.9198606271777e-06,
      "loss": 1.5958,
      "step": 2304
    },
    {
      "epoch": 0.8923732094463802,
      "grad_norm": 10.781627655029297,
      "learning_rate": 8.923732094463803e-06,
      "loss": 1.1257,
      "step": 2305
    },
    {
      "epoch": 0.8927603561749903,
      "grad_norm": 18.424612045288086,
      "learning_rate": 8.927603561749903e-06,
      "loss": 1.6353,
      "step": 2306
    },
    {
      "epoch": 0.8931475029036005,
      "grad_norm": 8.803183555603027,
      "learning_rate": 8.931475029036006e-06,
      "loss": 1.4344,
      "step": 2307
    },
    {
      "epoch": 0.8935346496322106,
      "grad_norm": 10.109408378601074,
      "learning_rate": 8.935346496322106e-06,
      "loss": 1.4473,
      "step": 2308
    },
    {
      "epoch": 0.8939217963608207,
      "grad_norm": 10.411210060119629,
      "learning_rate": 8.939217963608207e-06,
      "loss": 1.6122,
      "step": 2309
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 12.85700798034668,
      "learning_rate": 8.94308943089431e-06,
      "loss": 1.2788,
      "step": 2310
    },
    {
      "epoch": 0.894696089818041,
      "grad_norm": 23.017993927001953,
      "learning_rate": 8.946960898180412e-06,
      "loss": 1.8537,
      "step": 2311
    },
    {
      "epoch": 0.8950832365466512,
      "grad_norm": 23.13499641418457,
      "learning_rate": 8.950832365466512e-06,
      "loss": 3.1293,
      "step": 2312
    },
    {
      "epoch": 0.8954703832752613,
      "grad_norm": 18.346111297607422,
      "learning_rate": 8.954703832752613e-06,
      "loss": 2.1261,
      "step": 2313
    },
    {
      "epoch": 0.8958575300038715,
      "grad_norm": 16.246475219726562,
      "learning_rate": 8.958575300038715e-06,
      "loss": 1.6187,
      "step": 2314
    },
    {
      "epoch": 0.8962446767324816,
      "grad_norm": 14.044034004211426,
      "learning_rate": 8.962446767324818e-06,
      "loss": 2.0409,
      "step": 2315
    },
    {
      "epoch": 0.8966318234610917,
      "grad_norm": 9.911795616149902,
      "learning_rate": 8.966318234610918e-06,
      "loss": 1.4129,
      "step": 2316
    },
    {
      "epoch": 0.8970189701897019,
      "grad_norm": 22.82571029663086,
      "learning_rate": 8.970189701897019e-06,
      "loss": 1.9633,
      "step": 2317
    },
    {
      "epoch": 0.8974061169183121,
      "grad_norm": 12.343034744262695,
      "learning_rate": 8.974061169183121e-06,
      "loss": 1.1637,
      "step": 2318
    },
    {
      "epoch": 0.8977932636469221,
      "grad_norm": 10.994402885437012,
      "learning_rate": 8.977932636469222e-06,
      "loss": 1.592,
      "step": 2319
    },
    {
      "epoch": 0.8981804103755323,
      "grad_norm": 23.1755313873291,
      "learning_rate": 8.981804103755324e-06,
      "loss": 1.6409,
      "step": 2320
    },
    {
      "epoch": 0.8985675571041425,
      "grad_norm": 11.940260887145996,
      "learning_rate": 8.985675571041425e-06,
      "loss": 1.6626,
      "step": 2321
    },
    {
      "epoch": 0.8989547038327527,
      "grad_norm": 19.94672203063965,
      "learning_rate": 8.989547038327527e-06,
      "loss": 1.5697,
      "step": 2322
    },
    {
      "epoch": 0.8993418505613627,
      "grad_norm": 17.600189208984375,
      "learning_rate": 8.993418505613628e-06,
      "loss": 1.1746,
      "step": 2323
    },
    {
      "epoch": 0.8997289972899729,
      "grad_norm": 12.956334114074707,
      "learning_rate": 8.99728997289973e-06,
      "loss": 1.6042,
      "step": 2324
    },
    {
      "epoch": 0.9001161440185831,
      "grad_norm": 23.419052124023438,
      "learning_rate": 9.00116144018583e-06,
      "loss": 1.4831,
      "step": 2325
    },
    {
      "epoch": 0.9005032907471932,
      "grad_norm": 16.746448516845703,
      "learning_rate": 9.005032907471933e-06,
      "loss": 1.9065,
      "step": 2326
    },
    {
      "epoch": 0.9008904374758033,
      "grad_norm": 13.007083892822266,
      "learning_rate": 9.008904374758034e-06,
      "loss": 0.6093,
      "step": 2327
    },
    {
      "epoch": 0.9012775842044135,
      "grad_norm": 12.367864608764648,
      "learning_rate": 9.012775842044136e-06,
      "loss": 1.4398,
      "step": 2328
    },
    {
      "epoch": 0.9016647309330236,
      "grad_norm": 12.033641815185547,
      "learning_rate": 9.016647309330237e-06,
      "loss": 1.1713,
      "step": 2329
    },
    {
      "epoch": 0.9020518776616337,
      "grad_norm": 14.2890043258667,
      "learning_rate": 9.020518776616339e-06,
      "loss": 1.5398,
      "step": 2330
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 13.913548469543457,
      "learning_rate": 9.02439024390244e-06,
      "loss": 1.4976,
      "step": 2331
    },
    {
      "epoch": 0.902826171118854,
      "grad_norm": 15.085039138793945,
      "learning_rate": 9.02826171118854e-06,
      "loss": 1.6067,
      "step": 2332
    },
    {
      "epoch": 0.9032133178474642,
      "grad_norm": 17.061906814575195,
      "learning_rate": 9.032133178474643e-06,
      "loss": 1.9206,
      "step": 2333
    },
    {
      "epoch": 0.9036004645760743,
      "grad_norm": 23.209362030029297,
      "learning_rate": 9.036004645760745e-06,
      "loss": 1.588,
      "step": 2334
    },
    {
      "epoch": 0.9039876113046845,
      "grad_norm": 26.369630813598633,
      "learning_rate": 9.039876113046846e-06,
      "loss": 2.1319,
      "step": 2335
    },
    {
      "epoch": 0.9043747580332946,
      "grad_norm": 13.020764350891113,
      "learning_rate": 9.043747580332946e-06,
      "loss": 1.4821,
      "step": 2336
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 15.711220741271973,
      "learning_rate": 9.047619047619049e-06,
      "loss": 1.5899,
      "step": 2337
    },
    {
      "epoch": 0.9051490514905149,
      "grad_norm": 18.32108497619629,
      "learning_rate": 9.051490514905151e-06,
      "loss": 1.6117,
      "step": 2338
    },
    {
      "epoch": 0.905536198219125,
      "grad_norm": 13.442981719970703,
      "learning_rate": 9.055361982191252e-06,
      "loss": 1.5049,
      "step": 2339
    },
    {
      "epoch": 0.9059233449477352,
      "grad_norm": 12.958024024963379,
      "learning_rate": 9.059233449477352e-06,
      "loss": 1.5989,
      "step": 2340
    },
    {
      "epoch": 0.9063104916763454,
      "grad_norm": 23.134050369262695,
      "learning_rate": 9.063104916763455e-06,
      "loss": 1.7602,
      "step": 2341
    },
    {
      "epoch": 0.9066976384049554,
      "grad_norm": 34.762451171875,
      "learning_rate": 9.066976384049555e-06,
      "loss": 1.477,
      "step": 2342
    },
    {
      "epoch": 0.9070847851335656,
      "grad_norm": 14.378808975219727,
      "learning_rate": 9.070847851335658e-06,
      "loss": 1.0421,
      "step": 2343
    },
    {
      "epoch": 0.9074719318621758,
      "grad_norm": 20.486675262451172,
      "learning_rate": 9.074719318621758e-06,
      "loss": 1.9366,
      "step": 2344
    },
    {
      "epoch": 0.907859078590786,
      "grad_norm": 13.179032325744629,
      "learning_rate": 9.07859078590786e-06,
      "loss": 1.5512,
      "step": 2345
    },
    {
      "epoch": 0.908246225319396,
      "grad_norm": 31.507125854492188,
      "learning_rate": 9.082462253193961e-06,
      "loss": 1.4161,
      "step": 2346
    },
    {
      "epoch": 0.9086333720480062,
      "grad_norm": 20.894813537597656,
      "learning_rate": 9.086333720480064e-06,
      "loss": 1.5576,
      "step": 2347
    },
    {
      "epoch": 0.9090205187766164,
      "grad_norm": 11.242952346801758,
      "learning_rate": 9.090205187766164e-06,
      "loss": 1.4449,
      "step": 2348
    },
    {
      "epoch": 0.9094076655052264,
      "grad_norm": 15.805705070495605,
      "learning_rate": 9.094076655052265e-06,
      "loss": 1.2177,
      "step": 2349
    },
    {
      "epoch": 0.9097948122338366,
      "grad_norm": 23.7022705078125,
      "learning_rate": 9.097948122338367e-06,
      "loss": 1.3178,
      "step": 2350
    },
    {
      "epoch": 0.9101819589624468,
      "grad_norm": 11.574383735656738,
      "learning_rate": 9.10181958962447e-06,
      "loss": 1.6262,
      "step": 2351
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 16.93019676208496,
      "learning_rate": 9.10569105691057e-06,
      "loss": 1.8163,
      "step": 2352
    },
    {
      "epoch": 0.910956252419667,
      "grad_norm": 17.140581130981445,
      "learning_rate": 9.109562524196671e-06,
      "loss": 2.0023,
      "step": 2353
    },
    {
      "epoch": 0.9113433991482772,
      "grad_norm": 19.254098892211914,
      "learning_rate": 9.113433991482773e-06,
      "loss": 1.5814,
      "step": 2354
    },
    {
      "epoch": 0.9117305458768873,
      "grad_norm": 13.242351531982422,
      "learning_rate": 9.117305458768874e-06,
      "loss": 1.7686,
      "step": 2355
    },
    {
      "epoch": 0.9121176926054975,
      "grad_norm": 18.668685913085938,
      "learning_rate": 9.121176926054976e-06,
      "loss": 1.6695,
      "step": 2356
    },
    {
      "epoch": 0.9125048393341076,
      "grad_norm": 13.012821197509766,
      "learning_rate": 9.125048393341077e-06,
      "loss": 0.6488,
      "step": 2357
    },
    {
      "epoch": 0.9128919860627178,
      "grad_norm": 10.107338905334473,
      "learning_rate": 9.12891986062718e-06,
      "loss": 1.4543,
      "step": 2358
    },
    {
      "epoch": 0.9132791327913279,
      "grad_norm": 19.161630630493164,
      "learning_rate": 9.13279132791328e-06,
      "loss": 1.5762,
      "step": 2359
    },
    {
      "epoch": 0.9136662795199381,
      "grad_norm": 22.690185546875,
      "learning_rate": 9.136662795199382e-06,
      "loss": 2.2651,
      "step": 2360
    },
    {
      "epoch": 0.9140534262485482,
      "grad_norm": 11.013131141662598,
      "learning_rate": 9.140534262485483e-06,
      "loss": 1.7743,
      "step": 2361
    },
    {
      "epoch": 0.9144405729771583,
      "grad_norm": 30.30308723449707,
      "learning_rate": 9.144405729771583e-06,
      "loss": 1.5577,
      "step": 2362
    },
    {
      "epoch": 0.9148277197057685,
      "grad_norm": 16.0367374420166,
      "learning_rate": 9.148277197057686e-06,
      "loss": 1.589,
      "step": 2363
    },
    {
      "epoch": 0.9152148664343787,
      "grad_norm": 18.354352951049805,
      "learning_rate": 9.152148664343788e-06,
      "loss": 1.9855,
      "step": 2364
    },
    {
      "epoch": 0.9156020131629887,
      "grad_norm": 12.190423965454102,
      "learning_rate": 9.156020131629889e-06,
      "loss": 1.6008,
      "step": 2365
    },
    {
      "epoch": 0.9159891598915989,
      "grad_norm": 20.238805770874023,
      "learning_rate": 9.15989159891599e-06,
      "loss": 1.4064,
      "step": 2366
    },
    {
      "epoch": 0.9163763066202091,
      "grad_norm": 20.29241943359375,
      "learning_rate": 9.163763066202092e-06,
      "loss": 1.4082,
      "step": 2367
    },
    {
      "epoch": 0.9167634533488193,
      "grad_norm": 13.115478515625,
      "learning_rate": 9.167634533488194e-06,
      "loss": 1.7288,
      "step": 2368
    },
    {
      "epoch": 0.9171506000774293,
      "grad_norm": 13.040194511413574,
      "learning_rate": 9.171506000774295e-06,
      "loss": 1.1587,
      "step": 2369
    },
    {
      "epoch": 0.9175377468060395,
      "grad_norm": 23.43735694885254,
      "learning_rate": 9.175377468060395e-06,
      "loss": 1.3228,
      "step": 2370
    },
    {
      "epoch": 0.9179248935346497,
      "grad_norm": 16.442445755004883,
      "learning_rate": 9.179248935346498e-06,
      "loss": 1.5597,
      "step": 2371
    },
    {
      "epoch": 0.9183120402632597,
      "grad_norm": 13.542684555053711,
      "learning_rate": 9.183120402632598e-06,
      "loss": 0.6425,
      "step": 2372
    },
    {
      "epoch": 0.9186991869918699,
      "grad_norm": 22.42603302001953,
      "learning_rate": 9.1869918699187e-06,
      "loss": 1.9084,
      "step": 2373
    },
    {
      "epoch": 0.9190863337204801,
      "grad_norm": 12.764554023742676,
      "learning_rate": 9.190863337204801e-06,
      "loss": 1.6014,
      "step": 2374
    },
    {
      "epoch": 0.9194734804490902,
      "grad_norm": 15.377429008483887,
      "learning_rate": 9.194734804490902e-06,
      "loss": 1.5523,
      "step": 2375
    },
    {
      "epoch": 0.9198606271777003,
      "grad_norm": 16.361726760864258,
      "learning_rate": 9.198606271777004e-06,
      "loss": 1.6989,
      "step": 2376
    },
    {
      "epoch": 0.9202477739063105,
      "grad_norm": 17.958755493164062,
      "learning_rate": 9.202477739063107e-06,
      "loss": 2.0709,
      "step": 2377
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 13.025574684143066,
      "learning_rate": 9.206349206349207e-06,
      "loss": 1.5105,
      "step": 2378
    },
    {
      "epoch": 0.9210220673635308,
      "grad_norm": 11.80859661102295,
      "learning_rate": 9.210220673635308e-06,
      "loss": 1.0666,
      "step": 2379
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 13.937124252319336,
      "learning_rate": 9.21409214092141e-06,
      "loss": 1.5833,
      "step": 2380
    },
    {
      "epoch": 0.9217963608207511,
      "grad_norm": 16.18181610107422,
      "learning_rate": 9.217963608207513e-06,
      "loss": 1.9268,
      "step": 2381
    },
    {
      "epoch": 0.9221835075493612,
      "grad_norm": 18.005353927612305,
      "learning_rate": 9.221835075493613e-06,
      "loss": 1.9648,
      "step": 2382
    },
    {
      "epoch": 0.9225706542779714,
      "grad_norm": 16.683561325073242,
      "learning_rate": 9.225706542779714e-06,
      "loss": 1.5371,
      "step": 2383
    },
    {
      "epoch": 0.9229578010065815,
      "grad_norm": 19.468503952026367,
      "learning_rate": 9.229578010065816e-06,
      "loss": 2.0823,
      "step": 2384
    },
    {
      "epoch": 0.9233449477351916,
      "grad_norm": 40.522422790527344,
      "learning_rate": 9.233449477351917e-06,
      "loss": 1.791,
      "step": 2385
    },
    {
      "epoch": 0.9237320944638018,
      "grad_norm": 17.683931350708008,
      "learning_rate": 9.23732094463802e-06,
      "loss": 1.5831,
      "step": 2386
    },
    {
      "epoch": 0.924119241192412,
      "grad_norm": 21.762582778930664,
      "learning_rate": 9.24119241192412e-06,
      "loss": 1.8657,
      "step": 2387
    },
    {
      "epoch": 0.924506387921022,
      "grad_norm": 16.8353271484375,
      "learning_rate": 9.24506387921022e-06,
      "loss": 2.2314,
      "step": 2388
    },
    {
      "epoch": 0.9248935346496322,
      "grad_norm": 13.438691139221191,
      "learning_rate": 9.248935346496323e-06,
      "loss": 2.0699,
      "step": 2389
    },
    {
      "epoch": 0.9252806813782424,
      "grad_norm": 9.743907928466797,
      "learning_rate": 9.252806813782425e-06,
      "loss": 1.3659,
      "step": 2390
    },
    {
      "epoch": 0.9256678281068524,
      "grad_norm": 16.128942489624023,
      "learning_rate": 9.256678281068526e-06,
      "loss": 1.666,
      "step": 2391
    },
    {
      "epoch": 0.9260549748354626,
      "grad_norm": 21.764951705932617,
      "learning_rate": 9.260549748354627e-06,
      "loss": 1.3506,
      "step": 2392
    },
    {
      "epoch": 0.9264421215640728,
      "grad_norm": 14.598368644714355,
      "learning_rate": 9.264421215640729e-06,
      "loss": 1.2683,
      "step": 2393
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 19.210384368896484,
      "learning_rate": 9.268292682926831e-06,
      "loss": 1.8057,
      "step": 2394
    },
    {
      "epoch": 0.927216415021293,
      "grad_norm": 16.101091384887695,
      "learning_rate": 9.272164150212932e-06,
      "loss": 1.1264,
      "step": 2395
    },
    {
      "epoch": 0.9276035617499032,
      "grad_norm": 12.29494857788086,
      "learning_rate": 9.276035617499033e-06,
      "loss": 1.5805,
      "step": 2396
    },
    {
      "epoch": 0.9279907084785134,
      "grad_norm": 9.9263334274292,
      "learning_rate": 9.279907084785135e-06,
      "loss": 1.3774,
      "step": 2397
    },
    {
      "epoch": 0.9283778552071235,
      "grad_norm": 13.792848587036133,
      "learning_rate": 9.283778552071236e-06,
      "loss": 1.2696,
      "step": 2398
    },
    {
      "epoch": 0.9287650019357336,
      "grad_norm": 20.09490203857422,
      "learning_rate": 9.287650019357338e-06,
      "loss": 1.885,
      "step": 2399
    },
    {
      "epoch": 0.9291521486643438,
      "grad_norm": 11.56367015838623,
      "learning_rate": 9.291521486643439e-06,
      "loss": 1.0915,
      "step": 2400
    },
    {
      "epoch": 0.9295392953929539,
      "grad_norm": 13.34237289428711,
      "learning_rate": 9.29539295392954e-06,
      "loss": 1.1436,
      "step": 2401
    },
    {
      "epoch": 0.9299264421215641,
      "grad_norm": 11.43200397491455,
      "learning_rate": 9.299264421215642e-06,
      "loss": 1.4022,
      "step": 2402
    },
    {
      "epoch": 0.9303135888501742,
      "grad_norm": 20.76986312866211,
      "learning_rate": 9.303135888501744e-06,
      "loss": 1.4049,
      "step": 2403
    },
    {
      "epoch": 0.9307007355787844,
      "grad_norm": 14.892484664916992,
      "learning_rate": 9.307007355787845e-06,
      "loss": 1.477,
      "step": 2404
    },
    {
      "epoch": 0.9310878823073945,
      "grad_norm": 25.995710372924805,
      "learning_rate": 9.310878823073945e-06,
      "loss": 1.9349,
      "step": 2405
    },
    {
      "epoch": 0.9314750290360047,
      "grad_norm": 27.663976669311523,
      "learning_rate": 9.314750290360047e-06,
      "loss": 1.6966,
      "step": 2406
    },
    {
      "epoch": 0.9318621757646148,
      "grad_norm": 16.943809509277344,
      "learning_rate": 9.31862175764615e-06,
      "loss": 1.4204,
      "step": 2407
    },
    {
      "epoch": 0.9322493224932249,
      "grad_norm": 14.838353157043457,
      "learning_rate": 9.32249322493225e-06,
      "loss": 1.4813,
      "step": 2408
    },
    {
      "epoch": 0.9326364692218351,
      "grad_norm": 10.75448989868164,
      "learning_rate": 9.326364692218351e-06,
      "loss": 1.4801,
      "step": 2409
    },
    {
      "epoch": 0.9330236159504453,
      "grad_norm": 16.601646423339844,
      "learning_rate": 9.330236159504453e-06,
      "loss": 0.7622,
      "step": 2410
    },
    {
      "epoch": 0.9334107626790553,
      "grad_norm": 9.349617958068848,
      "learning_rate": 9.334107626790554e-06,
      "loss": 1.4584,
      "step": 2411
    },
    {
      "epoch": 0.9337979094076655,
      "grad_norm": 31.27602195739746,
      "learning_rate": 9.337979094076656e-06,
      "loss": 1.4303,
      "step": 2412
    },
    {
      "epoch": 0.9341850561362757,
      "grad_norm": 17.315326690673828,
      "learning_rate": 9.341850561362757e-06,
      "loss": 1.7213,
      "step": 2413
    },
    {
      "epoch": 0.9345722028648857,
      "grad_norm": 20.14692497253418,
      "learning_rate": 9.345722028648858e-06,
      "loss": 1.5622,
      "step": 2414
    },
    {
      "epoch": 0.9349593495934959,
      "grad_norm": 12.344671249389648,
      "learning_rate": 9.34959349593496e-06,
      "loss": 1.2256,
      "step": 2415
    },
    {
      "epoch": 0.9353464963221061,
      "grad_norm": 18.859813690185547,
      "learning_rate": 9.353464963221062e-06,
      "loss": 1.0116,
      "step": 2416
    },
    {
      "epoch": 0.9357336430507163,
      "grad_norm": 25.69373893737793,
      "learning_rate": 9.357336430507163e-06,
      "loss": 2.3313,
      "step": 2417
    },
    {
      "epoch": 0.9361207897793263,
      "grad_norm": 10.889843940734863,
      "learning_rate": 9.361207897793264e-06,
      "loss": 1.3481,
      "step": 2418
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 16.764297485351562,
      "learning_rate": 9.365079365079366e-06,
      "loss": 1.8266,
      "step": 2419
    },
    {
      "epoch": 0.9368950832365467,
      "grad_norm": 17.193193435668945,
      "learning_rate": 9.368950832365468e-06,
      "loss": 2.1259,
      "step": 2420
    },
    {
      "epoch": 0.9372822299651568,
      "grad_norm": 14.0260009765625,
      "learning_rate": 9.372822299651569e-06,
      "loss": 1.6465,
      "step": 2421
    },
    {
      "epoch": 0.9376693766937669,
      "grad_norm": 15.246230125427246,
      "learning_rate": 9.37669376693767e-06,
      "loss": 1.2291,
      "step": 2422
    },
    {
      "epoch": 0.9380565234223771,
      "grad_norm": 13.033675193786621,
      "learning_rate": 9.380565234223772e-06,
      "loss": 1.3934,
      "step": 2423
    },
    {
      "epoch": 0.9384436701509872,
      "grad_norm": 21.061824798583984,
      "learning_rate": 9.384436701509873e-06,
      "loss": 1.4074,
      "step": 2424
    },
    {
      "epoch": 0.9388308168795974,
      "grad_norm": 14.433723449707031,
      "learning_rate": 9.388308168795975e-06,
      "loss": 1.3989,
      "step": 2425
    },
    {
      "epoch": 0.9392179636082075,
      "grad_norm": 14.267729759216309,
      "learning_rate": 9.392179636082076e-06,
      "loss": 1.0865,
      "step": 2426
    },
    {
      "epoch": 0.9396051103368177,
      "grad_norm": 24.80900764465332,
      "learning_rate": 9.396051103368178e-06,
      "loss": 2.0322,
      "step": 2427
    },
    {
      "epoch": 0.9399922570654278,
      "grad_norm": 9.134562492370605,
      "learning_rate": 9.399922570654279e-06,
      "loss": 0.9409,
      "step": 2428
    },
    {
      "epoch": 0.940379403794038,
      "grad_norm": 37.82712936401367,
      "learning_rate": 9.403794037940381e-06,
      "loss": 1.7059,
      "step": 2429
    },
    {
      "epoch": 0.9407665505226481,
      "grad_norm": 22.591991424560547,
      "learning_rate": 9.407665505226482e-06,
      "loss": 1.5855,
      "step": 2430
    },
    {
      "epoch": 0.9411536972512582,
      "grad_norm": 13.572936058044434,
      "learning_rate": 9.411536972512582e-06,
      "loss": 1.4127,
      "step": 2431
    },
    {
      "epoch": 0.9415408439798684,
      "grad_norm": 18.473112106323242,
      "learning_rate": 9.415408439798685e-06,
      "loss": 2.1885,
      "step": 2432
    },
    {
      "epoch": 0.9419279907084785,
      "grad_norm": 14.272032737731934,
      "learning_rate": 9.419279907084787e-06,
      "loss": 1.419,
      "step": 2433
    },
    {
      "epoch": 0.9423151374370886,
      "grad_norm": 24.690614700317383,
      "learning_rate": 9.423151374370888e-06,
      "loss": 1.3399,
      "step": 2434
    },
    {
      "epoch": 0.9427022841656988,
      "grad_norm": 7.785721302032471,
      "learning_rate": 9.427022841656988e-06,
      "loss": 1.7068,
      "step": 2435
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 17.01764488220215,
      "learning_rate": 9.43089430894309e-06,
      "loss": 1.4329,
      "step": 2436
    },
    {
      "epoch": 0.943476577622919,
      "grad_norm": 25.782939910888672,
      "learning_rate": 9.434765776229191e-06,
      "loss": 1.5691,
      "step": 2437
    },
    {
      "epoch": 0.9438637243515292,
      "grad_norm": 15.145503044128418,
      "learning_rate": 9.438637243515294e-06,
      "loss": 1.7079,
      "step": 2438
    },
    {
      "epoch": 0.9442508710801394,
      "grad_norm": 15.970056533813477,
      "learning_rate": 9.442508710801394e-06,
      "loss": 1.0651,
      "step": 2439
    },
    {
      "epoch": 0.9446380178087496,
      "grad_norm": 9.232255935668945,
      "learning_rate": 9.446380178087497e-06,
      "loss": 1.3785,
      "step": 2440
    },
    {
      "epoch": 0.9450251645373596,
      "grad_norm": 15.347969055175781,
      "learning_rate": 9.450251645373597e-06,
      "loss": 1.9582,
      "step": 2441
    },
    {
      "epoch": 0.9454123112659698,
      "grad_norm": 13.718271255493164,
      "learning_rate": 9.4541231126597e-06,
      "loss": 1.6878,
      "step": 2442
    },
    {
      "epoch": 0.94579945799458,
      "grad_norm": 12.794452667236328,
      "learning_rate": 9.4579945799458e-06,
      "loss": 1.6623,
      "step": 2443
    },
    {
      "epoch": 0.94618660472319,
      "grad_norm": 12.400466918945312,
      "learning_rate": 9.461866047231901e-06,
      "loss": 1.0793,
      "step": 2444
    },
    {
      "epoch": 0.9465737514518002,
      "grad_norm": 18.931352615356445,
      "learning_rate": 9.465737514518003e-06,
      "loss": 1.6084,
      "step": 2445
    },
    {
      "epoch": 0.9469608981804104,
      "grad_norm": 38.066829681396484,
      "learning_rate": 9.469608981804106e-06,
      "loss": 2.7413,
      "step": 2446
    },
    {
      "epoch": 0.9473480449090205,
      "grad_norm": 25.894744873046875,
      "learning_rate": 9.473480449090206e-06,
      "loss": 1.7575,
      "step": 2447
    },
    {
      "epoch": 0.9477351916376306,
      "grad_norm": 42.95613098144531,
      "learning_rate": 9.477351916376307e-06,
      "loss": 1.134,
      "step": 2448
    },
    {
      "epoch": 0.9481223383662408,
      "grad_norm": 13.600801467895508,
      "learning_rate": 9.48122338366241e-06,
      "loss": 1.3171,
      "step": 2449
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 11.727331161499023,
      "learning_rate": 9.485094850948512e-06,
      "loss": 1.7117,
      "step": 2450
    },
    {
      "epoch": 0.9488966318234611,
      "grad_norm": 15.058943748474121,
      "learning_rate": 9.488966318234612e-06,
      "loss": 1.73,
      "step": 2451
    },
    {
      "epoch": 0.9492837785520712,
      "grad_norm": 19.389455795288086,
      "learning_rate": 9.492837785520713e-06,
      "loss": 1.7607,
      "step": 2452
    },
    {
      "epoch": 0.9496709252806814,
      "grad_norm": 12.034249305725098,
      "learning_rate": 9.496709252806815e-06,
      "loss": 1.7567,
      "step": 2453
    },
    {
      "epoch": 0.9500580720092915,
      "grad_norm": 30.04859161376953,
      "learning_rate": 9.500580720092916e-06,
      "loss": 1.8477,
      "step": 2454
    },
    {
      "epoch": 0.9504452187379017,
      "grad_norm": 17.20099449157715,
      "learning_rate": 9.504452187379018e-06,
      "loss": 1.4302,
      "step": 2455
    },
    {
      "epoch": 0.9508323654665118,
      "grad_norm": 8.845955848693848,
      "learning_rate": 9.508323654665119e-06,
      "loss": 1.3515,
      "step": 2456
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 21.175443649291992,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.3648,
      "step": 2457
    },
    {
      "epoch": 0.9516066589237321,
      "grad_norm": 22.235788345336914,
      "learning_rate": 9.516066589237322e-06,
      "loss": 1.6389,
      "step": 2458
    },
    {
      "epoch": 0.9519938056523423,
      "grad_norm": 16.619800567626953,
      "learning_rate": 9.519938056523424e-06,
      "loss": 2.0895,
      "step": 2459
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 13.149585723876953,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.1648,
      "step": 2460
    },
    {
      "epoch": 0.9527680991095625,
      "grad_norm": 23.319137573242188,
      "learning_rate": 9.527680991095625e-06,
      "loss": 2.8377,
      "step": 2461
    },
    {
      "epoch": 0.9531552458381727,
      "grad_norm": 14.723240852355957,
      "learning_rate": 9.531552458381728e-06,
      "loss": 1.5215,
      "step": 2462
    },
    {
      "epoch": 0.9535423925667829,
      "grad_norm": 19.70477867126465,
      "learning_rate": 9.53542392566783e-06,
      "loss": 1.5722,
      "step": 2463
    },
    {
      "epoch": 0.9539295392953929,
      "grad_norm": 27.534887313842773,
      "learning_rate": 9.53929539295393e-06,
      "loss": 1.6698,
      "step": 2464
    },
    {
      "epoch": 0.9543166860240031,
      "grad_norm": 20.85808753967285,
      "learning_rate": 9.543166860240031e-06,
      "loss": 1.5283,
      "step": 2465
    },
    {
      "epoch": 0.9547038327526133,
      "grad_norm": 19.407377243041992,
      "learning_rate": 9.547038327526134e-06,
      "loss": 1.8617,
      "step": 2466
    },
    {
      "epoch": 0.9550909794812233,
      "grad_norm": 47.65606689453125,
      "learning_rate": 9.550909794812234e-06,
      "loss": 2.9037,
      "step": 2467
    },
    {
      "epoch": 0.9554781262098335,
      "grad_norm": 16.20706558227539,
      "learning_rate": 9.554781262098337e-06,
      "loss": 1.7534,
      "step": 2468
    },
    {
      "epoch": 0.9558652729384437,
      "grad_norm": 12.767754554748535,
      "learning_rate": 9.558652729384437e-06,
      "loss": 1.4429,
      "step": 2469
    },
    {
      "epoch": 0.9562524196670538,
      "grad_norm": 20.498306274414062,
      "learning_rate": 9.562524196670538e-06,
      "loss": 1.3395,
      "step": 2470
    },
    {
      "epoch": 0.9566395663956639,
      "grad_norm": 15.765213012695312,
      "learning_rate": 9.56639566395664e-06,
      "loss": 1.4481,
      "step": 2471
    },
    {
      "epoch": 0.9570267131242741,
      "grad_norm": 15.332927703857422,
      "learning_rate": 9.570267131242743e-06,
      "loss": 1.6043,
      "step": 2472
    },
    {
      "epoch": 0.9574138598528843,
      "grad_norm": 29.2025203704834,
      "learning_rate": 9.574138598528843e-06,
      "loss": 1.1671,
      "step": 2473
    },
    {
      "epoch": 0.9578010065814944,
      "grad_norm": 26.906808853149414,
      "learning_rate": 9.578010065814944e-06,
      "loss": 1.8244,
      "step": 2474
    },
    {
      "epoch": 0.9581881533101045,
      "grad_norm": 22.60091781616211,
      "learning_rate": 9.581881533101046e-06,
      "loss": 1.788,
      "step": 2475
    },
    {
      "epoch": 0.9585753000387147,
      "grad_norm": 15.026811599731445,
      "learning_rate": 9.585753000387149e-06,
      "loss": 1.4295,
      "step": 2476
    },
    {
      "epoch": 0.9589624467673248,
      "grad_norm": 16.645891189575195,
      "learning_rate": 9.58962446767325e-06,
      "loss": 1.5895,
      "step": 2477
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 9.505697250366211,
      "learning_rate": 9.59349593495935e-06,
      "loss": 1.5302,
      "step": 2478
    },
    {
      "epoch": 0.9597367402245451,
      "grad_norm": 14.520769119262695,
      "learning_rate": 9.597367402245452e-06,
      "loss": 1.7381,
      "step": 2479
    },
    {
      "epoch": 0.9601238869531552,
      "grad_norm": 24.15897560119629,
      "learning_rate": 9.601238869531553e-06,
      "loss": 2.2318,
      "step": 2480
    },
    {
      "epoch": 0.9605110336817654,
      "grad_norm": 14.324383735656738,
      "learning_rate": 9.605110336817655e-06,
      "loss": 1.243,
      "step": 2481
    },
    {
      "epoch": 0.9608981804103756,
      "grad_norm": 17.041664123535156,
      "learning_rate": 9.608981804103756e-06,
      "loss": 1.8038,
      "step": 2482
    },
    {
      "epoch": 0.9612853271389856,
      "grad_norm": 13.885501861572266,
      "learning_rate": 9.612853271389857e-06,
      "loss": 1.3225,
      "step": 2483
    },
    {
      "epoch": 0.9616724738675958,
      "grad_norm": 29.35101318359375,
      "learning_rate": 9.616724738675959e-06,
      "loss": 1.497,
      "step": 2484
    },
    {
      "epoch": 0.962059620596206,
      "grad_norm": 10.921612739562988,
      "learning_rate": 9.620596205962061e-06,
      "loss": 1.6965,
      "step": 2485
    },
    {
      "epoch": 0.9624467673248162,
      "grad_norm": 16.019756317138672,
      "learning_rate": 9.624467673248162e-06,
      "loss": 1.3167,
      "step": 2486
    },
    {
      "epoch": 0.9628339140534262,
      "grad_norm": 12.54230785369873,
      "learning_rate": 9.628339140534263e-06,
      "loss": 1.1435,
      "step": 2487
    },
    {
      "epoch": 0.9632210607820364,
      "grad_norm": 9.355463027954102,
      "learning_rate": 9.632210607820365e-06,
      "loss": 1.4365,
      "step": 2488
    },
    {
      "epoch": 0.9636082075106466,
      "grad_norm": 10.55498218536377,
      "learning_rate": 9.636082075106467e-06,
      "loss": 1.6198,
      "step": 2489
    },
    {
      "epoch": 0.9639953542392566,
      "grad_norm": 13.701261520385742,
      "learning_rate": 9.639953542392568e-06,
      "loss": 2.2337,
      "step": 2490
    },
    {
      "epoch": 0.9643825009678668,
      "grad_norm": 40.77080535888672,
      "learning_rate": 9.643825009678669e-06,
      "loss": 1.3366,
      "step": 2491
    },
    {
      "epoch": 0.964769647696477,
      "grad_norm": 21.886022567749023,
      "learning_rate": 9.64769647696477e-06,
      "loss": 1.4343,
      "step": 2492
    },
    {
      "epoch": 0.9651567944250871,
      "grad_norm": 10.636088371276855,
      "learning_rate": 9.651567944250871e-06,
      "loss": 1.1051,
      "step": 2493
    },
    {
      "epoch": 0.9655439411536972,
      "grad_norm": 17.435609817504883,
      "learning_rate": 9.655439411536974e-06,
      "loss": 1.1707,
      "step": 2494
    },
    {
      "epoch": 0.9659310878823074,
      "grad_norm": 9.50867748260498,
      "learning_rate": 9.659310878823074e-06,
      "loss": 1.4348,
      "step": 2495
    },
    {
      "epoch": 0.9663182346109176,
      "grad_norm": 17.800506591796875,
      "learning_rate": 9.663182346109177e-06,
      "loss": 1.6305,
      "step": 2496
    },
    {
      "epoch": 0.9667053813395277,
      "grad_norm": 11.626612663269043,
      "learning_rate": 9.667053813395277e-06,
      "loss": 1.6394,
      "step": 2497
    },
    {
      "epoch": 0.9670925280681378,
      "grad_norm": 14.243045806884766,
      "learning_rate": 9.67092528068138e-06,
      "loss": 1.4224,
      "step": 2498
    },
    {
      "epoch": 0.967479674796748,
      "grad_norm": 13.251462936401367,
      "learning_rate": 9.67479674796748e-06,
      "loss": 1.3466,
      "step": 2499
    },
    {
      "epoch": 0.9678668215253581,
      "grad_norm": 8.359162330627441,
      "learning_rate": 9.678668215253581e-06,
      "loss": 1.0328,
      "step": 2500
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 17.624109268188477,
      "learning_rate": 9.682539682539683e-06,
      "loss": 1.6187,
      "step": 2501
    },
    {
      "epoch": 0.9686411149825784,
      "grad_norm": 15.328519821166992,
      "learning_rate": 9.686411149825786e-06,
      "loss": 1.8545,
      "step": 2502
    },
    {
      "epoch": 0.9690282617111885,
      "grad_norm": 13.258936882019043,
      "learning_rate": 9.690282617111886e-06,
      "loss": 2.1023,
      "step": 2503
    },
    {
      "epoch": 0.9694154084397987,
      "grad_norm": 13.75233268737793,
      "learning_rate": 9.694154084397987e-06,
      "loss": 1.3671,
      "step": 2504
    },
    {
      "epoch": 0.9698025551684089,
      "grad_norm": 9.982054710388184,
      "learning_rate": 9.69802555168409e-06,
      "loss": 0.9629,
      "step": 2505
    },
    {
      "epoch": 0.9701897018970189,
      "grad_norm": 11.263108253479004,
      "learning_rate": 9.70189701897019e-06,
      "loss": 1.6017,
      "step": 2506
    },
    {
      "epoch": 0.9705768486256291,
      "grad_norm": 16.134258270263672,
      "learning_rate": 9.705768486256292e-06,
      "loss": 1.5041,
      "step": 2507
    },
    {
      "epoch": 0.9709639953542393,
      "grad_norm": 14.935892105102539,
      "learning_rate": 9.709639953542393e-06,
      "loss": 1.4109,
      "step": 2508
    },
    {
      "epoch": 0.9713511420828495,
      "grad_norm": 9.89331340789795,
      "learning_rate": 9.713511420828495e-06,
      "loss": 1.601,
      "step": 2509
    },
    {
      "epoch": 0.9717382888114595,
      "grad_norm": 30.166852951049805,
      "learning_rate": 9.717382888114596e-06,
      "loss": 1.6253,
      "step": 2510
    },
    {
      "epoch": 0.9721254355400697,
      "grad_norm": 15.278709411621094,
      "learning_rate": 9.721254355400698e-06,
      "loss": 1.2424,
      "step": 2511
    },
    {
      "epoch": 0.9725125822686799,
      "grad_norm": 16.50385856628418,
      "learning_rate": 9.725125822686799e-06,
      "loss": 1.8546,
      "step": 2512
    },
    {
      "epoch": 0.9728997289972899,
      "grad_norm": 19.038211822509766,
      "learning_rate": 9.7289972899729e-06,
      "loss": 1.5566,
      "step": 2513
    },
    {
      "epoch": 0.9732868757259001,
      "grad_norm": 19.280607223510742,
      "learning_rate": 9.732868757259002e-06,
      "loss": 1.9039,
      "step": 2514
    },
    {
      "epoch": 0.9736740224545103,
      "grad_norm": 27.72847557067871,
      "learning_rate": 9.736740224545104e-06,
      "loss": 1.0504,
      "step": 2515
    },
    {
      "epoch": 0.9740611691831204,
      "grad_norm": 15.415870666503906,
      "learning_rate": 9.740611691831205e-06,
      "loss": 1.4687,
      "step": 2516
    },
    {
      "epoch": 0.9744483159117305,
      "grad_norm": 10.676066398620605,
      "learning_rate": 9.744483159117306e-06,
      "loss": 1.4078,
      "step": 2517
    },
    {
      "epoch": 0.9748354626403407,
      "grad_norm": 13.590201377868652,
      "learning_rate": 9.748354626403408e-06,
      "loss": 1.1165,
      "step": 2518
    },
    {
      "epoch": 0.9752226093689508,
      "grad_norm": 12.787718772888184,
      "learning_rate": 9.752226093689509e-06,
      "loss": 1.5834,
      "step": 2519
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 34.39543533325195,
      "learning_rate": 9.756097560975611e-06,
      "loss": 1.6226,
      "step": 2520
    },
    {
      "epoch": 0.9759969028261711,
      "grad_norm": 22.205814361572266,
      "learning_rate": 9.759969028261712e-06,
      "loss": 1.8868,
      "step": 2521
    },
    {
      "epoch": 0.9763840495547813,
      "grad_norm": 14.953932762145996,
      "learning_rate": 9.763840495547814e-06,
      "loss": 1.5732,
      "step": 2522
    },
    {
      "epoch": 0.9767711962833914,
      "grad_norm": 24.26693344116211,
      "learning_rate": 9.767711962833915e-06,
      "loss": 1.6035,
      "step": 2523
    },
    {
      "epoch": 0.9771583430120016,
      "grad_norm": 25.152528762817383,
      "learning_rate": 9.771583430120017e-06,
      "loss": 1.9118,
      "step": 2524
    },
    {
      "epoch": 0.9775454897406117,
      "grad_norm": 12.62132740020752,
      "learning_rate": 9.775454897406118e-06,
      "loss": 1.565,
      "step": 2525
    },
    {
      "epoch": 0.9779326364692218,
      "grad_norm": 14.87847900390625,
      "learning_rate": 9.779326364692218e-06,
      "loss": 1.5118,
      "step": 2526
    },
    {
      "epoch": 0.978319783197832,
      "grad_norm": 15.64487075805664,
      "learning_rate": 9.78319783197832e-06,
      "loss": 1.3423,
      "step": 2527
    },
    {
      "epoch": 0.9787069299264421,
      "grad_norm": 13.70876407623291,
      "learning_rate": 9.787069299264423e-06,
      "loss": 1.4081,
      "step": 2528
    },
    {
      "epoch": 0.9790940766550522,
      "grad_norm": 15.113752365112305,
      "learning_rate": 9.790940766550524e-06,
      "loss": 1.6401,
      "step": 2529
    },
    {
      "epoch": 0.9794812233836624,
      "grad_norm": 14.120649337768555,
      "learning_rate": 9.794812233836624e-06,
      "loss": 1.1461,
      "step": 2530
    },
    {
      "epoch": 0.9798683701122726,
      "grad_norm": 27.340917587280273,
      "learning_rate": 9.798683701122727e-06,
      "loss": 2.1994,
      "step": 2531
    },
    {
      "epoch": 0.9802555168408827,
      "grad_norm": 17.262170791625977,
      "learning_rate": 9.802555168408829e-06,
      "loss": 1.5305,
      "step": 2532
    },
    {
      "epoch": 0.9806426635694928,
      "grad_norm": 13.281227111816406,
      "learning_rate": 9.80642663569493e-06,
      "loss": 0.9674,
      "step": 2533
    },
    {
      "epoch": 0.981029810298103,
      "grad_norm": 20.432065963745117,
      "learning_rate": 9.81029810298103e-06,
      "loss": 1.5451,
      "step": 2534
    },
    {
      "epoch": 0.9814169570267132,
      "grad_norm": 14.50888729095459,
      "learning_rate": 9.814169570267133e-06,
      "loss": 2.1751,
      "step": 2535
    },
    {
      "epoch": 0.9818041037553232,
      "grad_norm": 7.63320779800415,
      "learning_rate": 9.818041037553233e-06,
      "loss": 0.9587,
      "step": 2536
    },
    {
      "epoch": 0.9821912504839334,
      "grad_norm": 13.159502983093262,
      "learning_rate": 9.821912504839336e-06,
      "loss": 1.6428,
      "step": 2537
    },
    {
      "epoch": 0.9825783972125436,
      "grad_norm": 20.119869232177734,
      "learning_rate": 9.825783972125436e-06,
      "loss": 1.5929,
      "step": 2538
    },
    {
      "epoch": 0.9829655439411537,
      "grad_norm": 8.515982627868652,
      "learning_rate": 9.829655439411537e-06,
      "loss": 0.9254,
      "step": 2539
    },
    {
      "epoch": 0.9833526906697638,
      "grad_norm": 12.554961204528809,
      "learning_rate": 9.833526906697639e-06,
      "loss": 1.1249,
      "step": 2540
    },
    {
      "epoch": 0.983739837398374,
      "grad_norm": 13.83731460571289,
      "learning_rate": 9.837398373983741e-06,
      "loss": 1.428,
      "step": 2541
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 15.132648468017578,
      "learning_rate": 9.841269841269842e-06,
      "loss": 2.0606,
      "step": 2542
    },
    {
      "epoch": 0.9845141308555942,
      "grad_norm": 13.795101165771484,
      "learning_rate": 9.845141308555943e-06,
      "loss": 1.1191,
      "step": 2543
    },
    {
      "epoch": 0.9849012775842044,
      "grad_norm": 32.88582992553711,
      "learning_rate": 9.849012775842045e-06,
      "loss": 2.1854,
      "step": 2544
    },
    {
      "epoch": 0.9852884243128146,
      "grad_norm": 17.08940887451172,
      "learning_rate": 9.852884243128147e-06,
      "loss": 1.5562,
      "step": 2545
    },
    {
      "epoch": 0.9856755710414247,
      "grad_norm": 12.026966094970703,
      "learning_rate": 9.856755710414248e-06,
      "loss": 1.5602,
      "step": 2546
    },
    {
      "epoch": 0.9860627177700348,
      "grad_norm": 19.02543067932129,
      "learning_rate": 9.860627177700349e-06,
      "loss": 2.2267,
      "step": 2547
    },
    {
      "epoch": 0.986449864498645,
      "grad_norm": 16.640535354614258,
      "learning_rate": 9.864498644986451e-06,
      "loss": 1.4911,
      "step": 2548
    },
    {
      "epoch": 0.9868370112272551,
      "grad_norm": 19.505102157592773,
      "learning_rate": 9.868370112272552e-06,
      "loss": 1.8137,
      "step": 2549
    },
    {
      "epoch": 0.9872241579558653,
      "grad_norm": 10.739411354064941,
      "learning_rate": 9.872241579558654e-06,
      "loss": 1.5136,
      "step": 2550
    },
    {
      "epoch": 0.9876113046844754,
      "grad_norm": 13.492147445678711,
      "learning_rate": 9.876113046844755e-06,
      "loss": 1.5452,
      "step": 2551
    },
    {
      "epoch": 0.9879984514130855,
      "grad_norm": 18.027143478393555,
      "learning_rate": 9.879984514130855e-06,
      "loss": 1.6421,
      "step": 2552
    },
    {
      "epoch": 0.9883855981416957,
      "grad_norm": 14.806660652160645,
      "learning_rate": 9.883855981416958e-06,
      "loss": 1.4613,
      "step": 2553
    },
    {
      "epoch": 0.9887727448703059,
      "grad_norm": 10.595080375671387,
      "learning_rate": 9.88772744870306e-06,
      "loss": 1.3687,
      "step": 2554
    },
    {
      "epoch": 0.989159891598916,
      "grad_norm": 16.189510345458984,
      "learning_rate": 9.89159891598916e-06,
      "loss": 1.6571,
      "step": 2555
    },
    {
      "epoch": 0.9895470383275261,
      "grad_norm": 21.16738510131836,
      "learning_rate": 9.895470383275261e-06,
      "loss": 2.5705,
      "step": 2556
    },
    {
      "epoch": 0.9899341850561363,
      "grad_norm": 18.801536560058594,
      "learning_rate": 9.899341850561364e-06,
      "loss": 1.7222,
      "step": 2557
    },
    {
      "epoch": 0.9903213317847465,
      "grad_norm": 14.925885200500488,
      "learning_rate": 9.903213317847466e-06,
      "loss": 1.5635,
      "step": 2558
    },
    {
      "epoch": 0.9907084785133565,
      "grad_norm": 19.014347076416016,
      "learning_rate": 9.907084785133567e-06,
      "loss": 1.6099,
      "step": 2559
    },
    {
      "epoch": 0.9910956252419667,
      "grad_norm": 23.29602813720703,
      "learning_rate": 9.910956252419667e-06,
      "loss": 1.86,
      "step": 2560
    },
    {
      "epoch": 0.9914827719705769,
      "grad_norm": 14.084688186645508,
      "learning_rate": 9.91482771970577e-06,
      "loss": 1.6384,
      "step": 2561
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 14.673308372497559,
      "learning_rate": 9.91869918699187e-06,
      "loss": 2.1152,
      "step": 2562
    },
    {
      "epoch": 0.9922570654277971,
      "grad_norm": 13.005427360534668,
      "learning_rate": 9.922570654277973e-06,
      "loss": 1.5282,
      "step": 2563
    },
    {
      "epoch": 0.9926442121564073,
      "grad_norm": 16.916603088378906,
      "learning_rate": 9.926442121564073e-06,
      "loss": 1.6518,
      "step": 2564
    },
    {
      "epoch": 0.9930313588850174,
      "grad_norm": 17.782567977905273,
      "learning_rate": 9.930313588850174e-06,
      "loss": 1.1994,
      "step": 2565
    },
    {
      "epoch": 0.9934185056136275,
      "grad_norm": 21.232084274291992,
      "learning_rate": 9.934185056136276e-06,
      "loss": 1.4839,
      "step": 2566
    },
    {
      "epoch": 0.9938056523422377,
      "grad_norm": 24.868227005004883,
      "learning_rate": 9.938056523422379e-06,
      "loss": 1.9744,
      "step": 2567
    },
    {
      "epoch": 0.9941927990708479,
      "grad_norm": 17.786909103393555,
      "learning_rate": 9.94192799070848e-06,
      "loss": 2.0374,
      "step": 2568
    },
    {
      "epoch": 0.994579945799458,
      "grad_norm": 15.702722549438477,
      "learning_rate": 9.94579945799458e-06,
      "loss": 1.3156,
      "step": 2569
    },
    {
      "epoch": 0.9949670925280681,
      "grad_norm": 17.65581703186035,
      "learning_rate": 9.949670925280682e-06,
      "loss": 1.6388,
      "step": 2570
    },
    {
      "epoch": 0.9953542392566783,
      "grad_norm": 21.25937843322754,
      "learning_rate": 9.953542392566785e-06,
      "loss": 1.8243,
      "step": 2571
    },
    {
      "epoch": 0.9957413859852884,
      "grad_norm": 16.396568298339844,
      "learning_rate": 9.957413859852885e-06,
      "loss": 1.3511,
      "step": 2572
    },
    {
      "epoch": 0.9961285327138986,
      "grad_norm": 22.33350372314453,
      "learning_rate": 9.961285327138986e-06,
      "loss": 1.5456,
      "step": 2573
    },
    {
      "epoch": 0.9965156794425087,
      "grad_norm": 23.696367263793945,
      "learning_rate": 9.965156794425088e-06,
      "loss": 2.4264,
      "step": 2574
    },
    {
      "epoch": 0.9969028261711188,
      "grad_norm": 19.24443244934082,
      "learning_rate": 9.969028261711189e-06,
      "loss": 1.7842,
      "step": 2575
    },
    {
      "epoch": 0.997289972899729,
      "grad_norm": 17.70773696899414,
      "learning_rate": 9.972899728997291e-06,
      "loss": 1.8099,
      "step": 2576
    },
    {
      "epoch": 0.9976771196283392,
      "grad_norm": 14.81180191040039,
      "learning_rate": 9.976771196283392e-06,
      "loss": 1.4985,
      "step": 2577
    },
    {
      "epoch": 0.9980642663569493,
      "grad_norm": 23.634035110473633,
      "learning_rate": 9.980642663569494e-06,
      "loss": 1.5511,
      "step": 2578
    },
    {
      "epoch": 0.9984514130855594,
      "grad_norm": 14.349963188171387,
      "learning_rate": 9.984514130855595e-06,
      "loss": 1.4931,
      "step": 2579
    },
    {
      "epoch": 0.9988385598141696,
      "grad_norm": 16.540782928466797,
      "learning_rate": 9.988385598141697e-06,
      "loss": 1.6154,
      "step": 2580
    },
    {
      "epoch": 0.9992257065427798,
      "grad_norm": 21.973140716552734,
      "learning_rate": 9.992257065427798e-06,
      "loss": 1.5554,
      "step": 2581
    },
    {
      "epoch": 0.9996128532713898,
      "grad_norm": 14.765721321105957,
      "learning_rate": 9.996128532713898e-06,
      "loss": 1.2196,
      "step": 2582
    },
    {
      "epoch": 1.0,
      "grad_norm": 23.19845962524414,
      "learning_rate": 1e-05,
      "loss": 1.4839,
      "step": 2583
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.30566037735849055,
      "eval_f1": 0.24708313558012496,
      "eval_loss": 1.6493834257125854,
      "eval_runtime": 421.8005,
      "eval_samples_per_second": 2.513,
      "eval_steps_per_second": 1.257,
      "step": 2583
    },
    {
      "epoch": 1.00038714672861,
      "grad_norm": 14.09315299987793,
      "learning_rate": 9.999569836968212e-06,
      "loss": 1.4162,
      "step": 2584
    },
    {
      "epoch": 1.0007742934572204,
      "grad_norm": 16.09914779663086,
      "learning_rate": 9.999139673936424e-06,
      "loss": 1.3967,
      "step": 2585
    },
    {
      "epoch": 1.0011614401858304,
      "grad_norm": 17.114521026611328,
      "learning_rate": 9.998709510904633e-06,
      "loss": 1.6082,
      "step": 2586
    },
    {
      "epoch": 1.0015485869144405,
      "grad_norm": 14.84126091003418,
      "learning_rate": 9.998279347872845e-06,
      "loss": 1.3602,
      "step": 2587
    },
    {
      "epoch": 1.0019357336430508,
      "grad_norm": 13.723974227905273,
      "learning_rate": 9.997849184841055e-06,
      "loss": 1.1899,
      "step": 2588
    },
    {
      "epoch": 1.0023228803716608,
      "grad_norm": 19.255876541137695,
      "learning_rate": 9.997419021809268e-06,
      "loss": 1.9304,
      "step": 2589
    },
    {
      "epoch": 1.002710027100271,
      "grad_norm": 14.713759422302246,
      "learning_rate": 9.996988858777477e-06,
      "loss": 0.9794,
      "step": 2590
    },
    {
      "epoch": 1.0030971738288812,
      "grad_norm": 9.271132469177246,
      "learning_rate": 9.996558695745689e-06,
      "loss": 1.5957,
      "step": 2591
    },
    {
      "epoch": 1.0034843205574913,
      "grad_norm": 12.925171852111816,
      "learning_rate": 9.996128532713898e-06,
      "loss": 1.5091,
      "step": 2592
    },
    {
      "epoch": 1.0038714672861013,
      "grad_norm": 21.85167694091797,
      "learning_rate": 9.995698369682112e-06,
      "loss": 1.46,
      "step": 2593
    },
    {
      "epoch": 1.0042586140147116,
      "grad_norm": 15.142290115356445,
      "learning_rate": 9.995268206650321e-06,
      "loss": 1.852,
      "step": 2594
    },
    {
      "epoch": 1.0046457607433217,
      "grad_norm": 12.539502143859863,
      "learning_rate": 9.994838043618533e-06,
      "loss": 1.468,
      "step": 2595
    },
    {
      "epoch": 1.005032907471932,
      "grad_norm": 17.726245880126953,
      "learning_rate": 9.994407880586742e-06,
      "loss": 1.3229,
      "step": 2596
    },
    {
      "epoch": 1.005420054200542,
      "grad_norm": 14.512673377990723,
      "learning_rate": 9.993977717554954e-06,
      "loss": 1.1577,
      "step": 2597
    },
    {
      "epoch": 1.005807200929152,
      "grad_norm": 12.546249389648438,
      "learning_rate": 9.993547554523165e-06,
      "loss": 1.4362,
      "step": 2598
    },
    {
      "epoch": 1.0061943476577624,
      "grad_norm": 11.196110725402832,
      "learning_rate": 9.993117391491377e-06,
      "loss": 1.5978,
      "step": 2599
    },
    {
      "epoch": 1.0065814943863725,
      "grad_norm": 14.639206886291504,
      "learning_rate": 9.992687228459586e-06,
      "loss": 1.0792,
      "step": 2600
    },
    {
      "epoch": 1.0069686411149825,
      "grad_norm": 14.33542251586914,
      "learning_rate": 9.992257065427798e-06,
      "loss": 1.054,
      "step": 2601
    },
    {
      "epoch": 1.0073557878435928,
      "grad_norm": 23.3632755279541,
      "learning_rate": 9.99182690239601e-06,
      "loss": 1.6689,
      "step": 2602
    },
    {
      "epoch": 1.0077429345722029,
      "grad_norm": 24.14690589904785,
      "learning_rate": 9.991396739364219e-06,
      "loss": 1.9858,
      "step": 2603
    },
    {
      "epoch": 1.008130081300813,
      "grad_norm": 18.774250030517578,
      "learning_rate": 9.99096657633243e-06,
      "loss": 1.6269,
      "step": 2604
    },
    {
      "epoch": 1.0085172280294232,
      "grad_norm": 11.012090682983398,
      "learning_rate": 9.990536413300642e-06,
      "loss": 0.8856,
      "step": 2605
    },
    {
      "epoch": 1.0089043747580333,
      "grad_norm": 39.710941314697266,
      "learning_rate": 9.990106250268853e-06,
      "loss": 1.1031,
      "step": 2606
    },
    {
      "epoch": 1.0092915214866434,
      "grad_norm": 9.749580383300781,
      "learning_rate": 9.989676087237063e-06,
      "loss": 1.3253,
      "step": 2607
    },
    {
      "epoch": 1.0096786682152536,
      "grad_norm": 23.097379684448242,
      "learning_rate": 9.989245924205274e-06,
      "loss": 1.763,
      "step": 2608
    },
    {
      "epoch": 1.0100658149438637,
      "grad_norm": 13.3756685256958,
      "learning_rate": 9.988815761173486e-06,
      "loss": 1.6811,
      "step": 2609
    },
    {
      "epoch": 1.0104529616724738,
      "grad_norm": 12.830037117004395,
      "learning_rate": 9.988385598141697e-06,
      "loss": 0.9569,
      "step": 2610
    },
    {
      "epoch": 1.010840108401084,
      "grad_norm": 18.245988845825195,
      "learning_rate": 9.987955435109907e-06,
      "loss": 1.2864,
      "step": 2611
    },
    {
      "epoch": 1.0112272551296941,
      "grad_norm": 56.722293853759766,
      "learning_rate": 9.987525272078118e-06,
      "loss": 2.1246,
      "step": 2612
    },
    {
      "epoch": 1.0116144018583042,
      "grad_norm": 23.643814086914062,
      "learning_rate": 9.98709510904633e-06,
      "loss": 2.1043,
      "step": 2613
    },
    {
      "epoch": 1.0120015485869145,
      "grad_norm": 13.166501998901367,
      "learning_rate": 9.986664946014541e-06,
      "loss": 1.647,
      "step": 2614
    },
    {
      "epoch": 1.0123886953155246,
      "grad_norm": 9.885017395019531,
      "learning_rate": 9.986234782982751e-06,
      "loss": 1.3324,
      "step": 2615
    },
    {
      "epoch": 1.0127758420441346,
      "grad_norm": 9.467904090881348,
      "learning_rate": 9.985804619950962e-06,
      "loss": 1.301,
      "step": 2616
    },
    {
      "epoch": 1.013162988772745,
      "grad_norm": 19.85218048095703,
      "learning_rate": 9.985374456919174e-06,
      "loss": 1.3001,
      "step": 2617
    },
    {
      "epoch": 1.013550135501355,
      "grad_norm": 7.8051934242248535,
      "learning_rate": 9.984944293887383e-06,
      "loss": 1.3422,
      "step": 2618
    },
    {
      "epoch": 1.0139372822299653,
      "grad_norm": 10.278396606445312,
      "learning_rate": 9.984514130855595e-06,
      "loss": 1.5395,
      "step": 2619
    },
    {
      "epoch": 1.0143244289585753,
      "grad_norm": 21.740522384643555,
      "learning_rate": 9.984083967823806e-06,
      "loss": 1.4721,
      "step": 2620
    },
    {
      "epoch": 1.0147115756871854,
      "grad_norm": 25.700231552124023,
      "learning_rate": 9.983653804792018e-06,
      "loss": 1.658,
      "step": 2621
    },
    {
      "epoch": 1.0150987224157957,
      "grad_norm": 13.382265090942383,
      "learning_rate": 9.983223641760227e-06,
      "loss": 2.305,
      "step": 2622
    },
    {
      "epoch": 1.0154858691444058,
      "grad_norm": 25.143522262573242,
      "learning_rate": 9.982793478728439e-06,
      "loss": 2.4553,
      "step": 2623
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 14.751433372497559,
      "learning_rate": 9.982363315696649e-06,
      "loss": 1.3549,
      "step": 2624
    },
    {
      "epoch": 1.016260162601626,
      "grad_norm": 16.997722625732422,
      "learning_rate": 9.981933152664862e-06,
      "loss": 1.7402,
      "step": 2625
    },
    {
      "epoch": 1.0166473093302362,
      "grad_norm": 12.751065254211426,
      "learning_rate": 9.981502989633071e-06,
      "loss": 1.1985,
      "step": 2626
    },
    {
      "epoch": 1.0170344560588462,
      "grad_norm": 19.965747833251953,
      "learning_rate": 9.981072826601283e-06,
      "loss": 1.1355,
      "step": 2627
    },
    {
      "epoch": 1.0174216027874565,
      "grad_norm": 9.752058029174805,
      "learning_rate": 9.980642663569494e-06,
      "loss": 1.371,
      "step": 2628
    },
    {
      "epoch": 1.0178087495160666,
      "grad_norm": 9.810931205749512,
      "learning_rate": 9.980212500537706e-06,
      "loss": 0.8008,
      "step": 2629
    },
    {
      "epoch": 1.0181958962446767,
      "grad_norm": 8.262858390808105,
      "learning_rate": 9.979782337505915e-06,
      "loss": 1.3404,
      "step": 2630
    },
    {
      "epoch": 1.018583042973287,
      "grad_norm": 12.261771202087402,
      "learning_rate": 9.979352174474127e-06,
      "loss": 0.9901,
      "step": 2631
    },
    {
      "epoch": 1.018970189701897,
      "grad_norm": 10.840831756591797,
      "learning_rate": 9.978922011442338e-06,
      "loss": 1.2846,
      "step": 2632
    },
    {
      "epoch": 1.019357336430507,
      "grad_norm": 19.183612823486328,
      "learning_rate": 9.978491848410548e-06,
      "loss": 1.2528,
      "step": 2633
    },
    {
      "epoch": 1.0197444831591174,
      "grad_norm": 10.848145484924316,
      "learning_rate": 9.97806168537876e-06,
      "loss": 1.1285,
      "step": 2634
    },
    {
      "epoch": 1.0201316298877274,
      "grad_norm": 13.361830711364746,
      "learning_rate": 9.97763152234697e-06,
      "loss": 1.6871,
      "step": 2635
    },
    {
      "epoch": 1.0205187766163375,
      "grad_norm": 30.11357879638672,
      "learning_rate": 9.977201359315182e-06,
      "loss": 1.86,
      "step": 2636
    },
    {
      "epoch": 1.0209059233449478,
      "grad_norm": 16.69076156616211,
      "learning_rate": 9.976771196283392e-06,
      "loss": 1.0964,
      "step": 2637
    },
    {
      "epoch": 1.0212930700735579,
      "grad_norm": 16.316829681396484,
      "learning_rate": 9.976341033251603e-06,
      "loss": 1.39,
      "step": 2638
    },
    {
      "epoch": 1.021680216802168,
      "grad_norm": 8.249836921691895,
      "learning_rate": 9.975910870219813e-06,
      "loss": 1.0287,
      "step": 2639
    },
    {
      "epoch": 1.0220673635307782,
      "grad_norm": 16.128551483154297,
      "learning_rate": 9.975480707188026e-06,
      "loss": 1.0403,
      "step": 2640
    },
    {
      "epoch": 1.0224545102593883,
      "grad_norm": 13.948752403259277,
      "learning_rate": 9.975050544156236e-06,
      "loss": 1.3028,
      "step": 2641
    },
    {
      "epoch": 1.0228416569879986,
      "grad_norm": 16.271055221557617,
      "learning_rate": 9.974620381124447e-06,
      "loss": 1.4251,
      "step": 2642
    },
    {
      "epoch": 1.0232288037166086,
      "grad_norm": 18.572494506835938,
      "learning_rate": 9.974190218092657e-06,
      "loss": 2.0752,
      "step": 2643
    },
    {
      "epoch": 1.0236159504452187,
      "grad_norm": 10.610494613647461,
      "learning_rate": 9.97376005506087e-06,
      "loss": 0.7889,
      "step": 2644
    },
    {
      "epoch": 1.024003097173829,
      "grad_norm": 12.43874454498291,
      "learning_rate": 9.97332989202908e-06,
      "loss": 1.5173,
      "step": 2645
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 12.149894714355469,
      "learning_rate": 9.972899728997291e-06,
      "loss": 1.6833,
      "step": 2646
    },
    {
      "epoch": 1.0247773906310491,
      "grad_norm": 22.250722885131836,
      "learning_rate": 9.972469565965501e-06,
      "loss": 2.1066,
      "step": 2647
    },
    {
      "epoch": 1.0251645373596594,
      "grad_norm": 31.769309997558594,
      "learning_rate": 9.972039402933712e-06,
      "loss": 1.2411,
      "step": 2648
    },
    {
      "epoch": 1.0255516840882695,
      "grad_norm": 23.790058135986328,
      "learning_rate": 9.971609239901924e-06,
      "loss": 1.7298,
      "step": 2649
    },
    {
      "epoch": 1.0259388308168795,
      "grad_norm": 10.287089347839355,
      "learning_rate": 9.971179076870135e-06,
      "loss": 1.2483,
      "step": 2650
    },
    {
      "epoch": 1.0263259775454898,
      "grad_norm": 8.567887306213379,
      "learning_rate": 9.970748913838345e-06,
      "loss": 1.117,
      "step": 2651
    },
    {
      "epoch": 1.0267131242740999,
      "grad_norm": 13.543364524841309,
      "learning_rate": 9.970318750806556e-06,
      "loss": 2.0638,
      "step": 2652
    },
    {
      "epoch": 1.02710027100271,
      "grad_norm": 19.337852478027344,
      "learning_rate": 9.969888587774768e-06,
      "loss": 1.9797,
      "step": 2653
    },
    {
      "epoch": 1.0274874177313202,
      "grad_norm": 19.458457946777344,
      "learning_rate": 9.969458424742977e-06,
      "loss": 2.2493,
      "step": 2654
    },
    {
      "epoch": 1.0278745644599303,
      "grad_norm": 14.663086891174316,
      "learning_rate": 9.969028261711189e-06,
      "loss": 1.8724,
      "step": 2655
    },
    {
      "epoch": 1.0282617111885404,
      "grad_norm": 12.829110145568848,
      "learning_rate": 9.9685980986794e-06,
      "loss": 1.5622,
      "step": 2656
    },
    {
      "epoch": 1.0286488579171507,
      "grad_norm": 26.229982376098633,
      "learning_rate": 9.968167935647612e-06,
      "loss": 1.5588,
      "step": 2657
    },
    {
      "epoch": 1.0290360046457607,
      "grad_norm": 15.808579444885254,
      "learning_rate": 9.967737772615821e-06,
      "loss": 1.4224,
      "step": 2658
    },
    {
      "epoch": 1.0294231513743708,
      "grad_norm": 13.52181339263916,
      "learning_rate": 9.967307609584033e-06,
      "loss": 1.7146,
      "step": 2659
    },
    {
      "epoch": 1.029810298102981,
      "grad_norm": 15.731485366821289,
      "learning_rate": 9.966877446552244e-06,
      "loss": 2.1965,
      "step": 2660
    },
    {
      "epoch": 1.0301974448315911,
      "grad_norm": 25.084585189819336,
      "learning_rate": 9.966447283520456e-06,
      "loss": 1.4525,
      "step": 2661
    },
    {
      "epoch": 1.0305845915602012,
      "grad_norm": 21.00584602355957,
      "learning_rate": 9.966017120488665e-06,
      "loss": 1.7112,
      "step": 2662
    },
    {
      "epoch": 1.0309717382888115,
      "grad_norm": 10.195930480957031,
      "learning_rate": 9.965586957456877e-06,
      "loss": 0.7986,
      "step": 2663
    },
    {
      "epoch": 1.0313588850174216,
      "grad_norm": 11.363364219665527,
      "learning_rate": 9.965156794425088e-06,
      "loss": 1.4594,
      "step": 2664
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 17.544635772705078,
      "learning_rate": 9.9647266313933e-06,
      "loss": 1.8068,
      "step": 2665
    },
    {
      "epoch": 1.032133178474642,
      "grad_norm": 14.959081649780273,
      "learning_rate": 9.96429646836151e-06,
      "loss": 1.3973,
      "step": 2666
    },
    {
      "epoch": 1.032520325203252,
      "grad_norm": 19.163761138916016,
      "learning_rate": 9.96386630532972e-06,
      "loss": 2.2167,
      "step": 2667
    },
    {
      "epoch": 1.0329074719318623,
      "grad_norm": 12.247987747192383,
      "learning_rate": 9.963436142297932e-06,
      "loss": 1.188,
      "step": 2668
    },
    {
      "epoch": 1.0332946186604723,
      "grad_norm": 13.103798866271973,
      "learning_rate": 9.963005979266142e-06,
      "loss": 2.1821,
      "step": 2669
    },
    {
      "epoch": 1.0336817653890824,
      "grad_norm": 26.705350875854492,
      "learning_rate": 9.962575816234353e-06,
      "loss": 3.5965,
      "step": 2670
    },
    {
      "epoch": 1.0340689121176927,
      "grad_norm": 12.05018424987793,
      "learning_rate": 9.962145653202565e-06,
      "loss": 1.0646,
      "step": 2671
    },
    {
      "epoch": 1.0344560588463028,
      "grad_norm": 14.519885063171387,
      "learning_rate": 9.961715490170776e-06,
      "loss": 1.3766,
      "step": 2672
    },
    {
      "epoch": 1.0348432055749128,
      "grad_norm": 27.926054000854492,
      "learning_rate": 9.961285327138986e-06,
      "loss": 1.4593,
      "step": 2673
    },
    {
      "epoch": 1.0352303523035231,
      "grad_norm": 19.111059188842773,
      "learning_rate": 9.960855164107197e-06,
      "loss": 1.7912,
      "step": 2674
    },
    {
      "epoch": 1.0356174990321332,
      "grad_norm": 13.485430717468262,
      "learning_rate": 9.960425001075409e-06,
      "loss": 1.1393,
      "step": 2675
    },
    {
      "epoch": 1.0360046457607432,
      "grad_norm": 16.21665382385254,
      "learning_rate": 9.95999483804362e-06,
      "loss": 1.1484,
      "step": 2676
    },
    {
      "epoch": 1.0363917924893535,
      "grad_norm": 11.625178337097168,
      "learning_rate": 9.95956467501183e-06,
      "loss": 1.4928,
      "step": 2677
    },
    {
      "epoch": 1.0367789392179636,
      "grad_norm": 27.97455596923828,
      "learning_rate": 9.959134511980041e-06,
      "loss": 1.6877,
      "step": 2678
    },
    {
      "epoch": 1.0371660859465737,
      "grad_norm": 13.501774787902832,
      "learning_rate": 9.958704348948253e-06,
      "loss": 1.5828,
      "step": 2679
    },
    {
      "epoch": 1.037553232675184,
      "grad_norm": 14.371302604675293,
      "learning_rate": 9.958274185916464e-06,
      "loss": 1.6827,
      "step": 2680
    },
    {
      "epoch": 1.037940379403794,
      "grad_norm": 29.210491180419922,
      "learning_rate": 9.957844022884674e-06,
      "loss": 2.4055,
      "step": 2681
    },
    {
      "epoch": 1.038327526132404,
      "grad_norm": 13.52111530303955,
      "learning_rate": 9.957413859852885e-06,
      "loss": 1.6646,
      "step": 2682
    },
    {
      "epoch": 1.0387146728610144,
      "grad_norm": 28.332359313964844,
      "learning_rate": 9.956983696821097e-06,
      "loss": 1.5333,
      "step": 2683
    },
    {
      "epoch": 1.0391018195896244,
      "grad_norm": 14.842509269714355,
      "learning_rate": 9.956553533789306e-06,
      "loss": 1.5887,
      "step": 2684
    },
    {
      "epoch": 1.0394889663182345,
      "grad_norm": 29.07134437561035,
      "learning_rate": 9.956123370757518e-06,
      "loss": 1.8383,
      "step": 2685
    },
    {
      "epoch": 1.0398761130468448,
      "grad_norm": 16.8466854095459,
      "learning_rate": 9.95569320772573e-06,
      "loss": 1.5168,
      "step": 2686
    },
    {
      "epoch": 1.0402632597754549,
      "grad_norm": 11.186052322387695,
      "learning_rate": 9.95526304469394e-06,
      "loss": 1.4999,
      "step": 2687
    },
    {
      "epoch": 1.040650406504065,
      "grad_norm": 19.452342987060547,
      "learning_rate": 9.95483288166215e-06,
      "loss": 1.1015,
      "step": 2688
    },
    {
      "epoch": 1.0410375532326752,
      "grad_norm": 18.598880767822266,
      "learning_rate": 9.954402718630362e-06,
      "loss": 1.4323,
      "step": 2689
    },
    {
      "epoch": 1.0414246999612853,
      "grad_norm": 25.457122802734375,
      "learning_rate": 9.953972555598571e-06,
      "loss": 1.6193,
      "step": 2690
    },
    {
      "epoch": 1.0418118466898956,
      "grad_norm": 22.03586196899414,
      "learning_rate": 9.953542392566785e-06,
      "loss": 2.5221,
      "step": 2691
    },
    {
      "epoch": 1.0421989934185056,
      "grad_norm": 14.365385055541992,
      "learning_rate": 9.953112229534994e-06,
      "loss": 1.7201,
      "step": 2692
    },
    {
      "epoch": 1.0425861401471157,
      "grad_norm": 19.65247344970703,
      "learning_rate": 9.952682066503206e-06,
      "loss": 1.9715,
      "step": 2693
    },
    {
      "epoch": 1.042973286875726,
      "grad_norm": 21.079561233520508,
      "learning_rate": 9.952251903471415e-06,
      "loss": 1.2685,
      "step": 2694
    },
    {
      "epoch": 1.043360433604336,
      "grad_norm": 15.313301086425781,
      "learning_rate": 9.951821740439629e-06,
      "loss": 1.5385,
      "step": 2695
    },
    {
      "epoch": 1.0437475803329461,
      "grad_norm": 19.117565155029297,
      "learning_rate": 9.951391577407838e-06,
      "loss": 2.2661,
      "step": 2696
    },
    {
      "epoch": 1.0441347270615564,
      "grad_norm": 12.05198860168457,
      "learning_rate": 9.95096141437605e-06,
      "loss": 1.597,
      "step": 2697
    },
    {
      "epoch": 1.0445218737901665,
      "grad_norm": 16.273447036743164,
      "learning_rate": 9.95053125134426e-06,
      "loss": 1.2871,
      "step": 2698
    },
    {
      "epoch": 1.0449090205187765,
      "grad_norm": 21.990150451660156,
      "learning_rate": 9.95010108831247e-06,
      "loss": 1.7014,
      "step": 2699
    },
    {
      "epoch": 1.0452961672473868,
      "grad_norm": 13.947320938110352,
      "learning_rate": 9.949670925280682e-06,
      "loss": 1.0555,
      "step": 2700
    },
    {
      "epoch": 1.045683313975997,
      "grad_norm": 13.0911865234375,
      "learning_rate": 9.949240762248894e-06,
      "loss": 1.3109,
      "step": 2701
    },
    {
      "epoch": 1.046070460704607,
      "grad_norm": 13.747376441955566,
      "learning_rate": 9.948810599217103e-06,
      "loss": 1.6242,
      "step": 2702
    },
    {
      "epoch": 1.0464576074332173,
      "grad_norm": 11.979917526245117,
      "learning_rate": 9.948380436185315e-06,
      "loss": 1.3778,
      "step": 2703
    },
    {
      "epoch": 1.0468447541618273,
      "grad_norm": 13.424851417541504,
      "learning_rate": 9.947950273153526e-06,
      "loss": 1.4891,
      "step": 2704
    },
    {
      "epoch": 1.0472319008904374,
      "grad_norm": 13.677470207214355,
      "learning_rate": 9.947520110121736e-06,
      "loss": 1.1094,
      "step": 2705
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 15.99482250213623,
      "learning_rate": 9.947089947089947e-06,
      "loss": 1.3605,
      "step": 2706
    },
    {
      "epoch": 1.0480061943476577,
      "grad_norm": 18.929494857788086,
      "learning_rate": 9.946659784058159e-06,
      "loss": 1.7863,
      "step": 2707
    },
    {
      "epoch": 1.0483933410762678,
      "grad_norm": 20.880464553833008,
      "learning_rate": 9.94622962102637e-06,
      "loss": 1.3812,
      "step": 2708
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 11.52778434753418,
      "learning_rate": 9.94579945799458e-06,
      "loss": 1.3256,
      "step": 2709
    },
    {
      "epoch": 1.0491676345334882,
      "grad_norm": 23.00777244567871,
      "learning_rate": 9.945369294962793e-06,
      "loss": 1.9646,
      "step": 2710
    },
    {
      "epoch": 1.0495547812620982,
      "grad_norm": 14.476585388183594,
      "learning_rate": 9.944939131931003e-06,
      "loss": 1.5711,
      "step": 2711
    },
    {
      "epoch": 1.0499419279907085,
      "grad_norm": 14.349151611328125,
      "learning_rate": 9.944508968899214e-06,
      "loss": 1.4947,
      "step": 2712
    },
    {
      "epoch": 1.0503290747193186,
      "grad_norm": 42.917572021484375,
      "learning_rate": 9.944078805867424e-06,
      "loss": 2.1038,
      "step": 2713
    },
    {
      "epoch": 1.0507162214479289,
      "grad_norm": 20.197017669677734,
      "learning_rate": 9.943648642835635e-06,
      "loss": 1.2373,
      "step": 2714
    },
    {
      "epoch": 1.051103368176539,
      "grad_norm": 12.746315956115723,
      "learning_rate": 9.943218479803847e-06,
      "loss": 1.2247,
      "step": 2715
    },
    {
      "epoch": 1.051490514905149,
      "grad_norm": 23.372669219970703,
      "learning_rate": 9.942788316772058e-06,
      "loss": 1.6807,
      "step": 2716
    },
    {
      "epoch": 1.0518776616337593,
      "grad_norm": 21.663854598999023,
      "learning_rate": 9.942358153740268e-06,
      "loss": 2.7335,
      "step": 2717
    },
    {
      "epoch": 1.0522648083623694,
      "grad_norm": 11.27500057220459,
      "learning_rate": 9.94192799070848e-06,
      "loss": 1.4169,
      "step": 2718
    },
    {
      "epoch": 1.0526519550909794,
      "grad_norm": 12.647601127624512,
      "learning_rate": 9.94149782767669e-06,
      "loss": 1.6562,
      "step": 2719
    },
    {
      "epoch": 1.0530391018195897,
      "grad_norm": 13.9559965133667,
      "learning_rate": 9.9410676646449e-06,
      "loss": 1.0289,
      "step": 2720
    },
    {
      "epoch": 1.0534262485481998,
      "grad_norm": 23.833908081054688,
      "learning_rate": 9.940637501613112e-06,
      "loss": 1.9903,
      "step": 2721
    },
    {
      "epoch": 1.0538133952768098,
      "grad_norm": 21.276700973510742,
      "learning_rate": 9.940207338581323e-06,
      "loss": 1.625,
      "step": 2722
    },
    {
      "epoch": 1.0542005420054201,
      "grad_norm": 15.311570167541504,
      "learning_rate": 9.939777175549535e-06,
      "loss": 1.5141,
      "step": 2723
    },
    {
      "epoch": 1.0545876887340302,
      "grad_norm": 16.3049259185791,
      "learning_rate": 9.939347012517744e-06,
      "loss": 1.0908,
      "step": 2724
    },
    {
      "epoch": 1.0549748354626403,
      "grad_norm": 9.163262367248535,
      "learning_rate": 9.938916849485956e-06,
      "loss": 1.4294,
      "step": 2725
    },
    {
      "epoch": 1.0553619821912505,
      "grad_norm": 16.678686141967773,
      "learning_rate": 9.938486686454167e-06,
      "loss": 1.7523,
      "step": 2726
    },
    {
      "epoch": 1.0557491289198606,
      "grad_norm": 34.530879974365234,
      "learning_rate": 9.938056523422379e-06,
      "loss": 1.4687,
      "step": 2727
    },
    {
      "epoch": 1.0561362756484707,
      "grad_norm": 28.6888370513916,
      "learning_rate": 9.937626360390588e-06,
      "loss": 1.2836,
      "step": 2728
    },
    {
      "epoch": 1.056523422377081,
      "grad_norm": 10.500811576843262,
      "learning_rate": 9.9371961973588e-06,
      "loss": 1.2797,
      "step": 2729
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 20.976951599121094,
      "learning_rate": 9.936766034327011e-06,
      "loss": 0.8104,
      "step": 2730
    },
    {
      "epoch": 1.057297715834301,
      "grad_norm": 16.457035064697266,
      "learning_rate": 9.936335871295223e-06,
      "loss": 1.4265,
      "step": 2731
    },
    {
      "epoch": 1.0576848625629114,
      "grad_norm": 16.970029830932617,
      "learning_rate": 9.935905708263432e-06,
      "loss": 2.3172,
      "step": 2732
    },
    {
      "epoch": 1.0580720092915215,
      "grad_norm": 20.946565628051758,
      "learning_rate": 9.935475545231644e-06,
      "loss": 1.1788,
      "step": 2733
    },
    {
      "epoch": 1.0584591560201315,
      "grad_norm": 15.498973846435547,
      "learning_rate": 9.935045382199855e-06,
      "loss": 0.9804,
      "step": 2734
    },
    {
      "epoch": 1.0588463027487418,
      "grad_norm": 19.490032196044922,
      "learning_rate": 9.934615219168065e-06,
      "loss": 1.7958,
      "step": 2735
    },
    {
      "epoch": 1.0592334494773519,
      "grad_norm": 11.397605895996094,
      "learning_rate": 9.934185056136276e-06,
      "loss": 1.2566,
      "step": 2736
    },
    {
      "epoch": 1.0596205962059622,
      "grad_norm": 21.040130615234375,
      "learning_rate": 9.933754893104488e-06,
      "loss": 2.3323,
      "step": 2737
    },
    {
      "epoch": 1.0600077429345722,
      "grad_norm": 12.039347648620605,
      "learning_rate": 9.933324730072699e-06,
      "loss": 1.0577,
      "step": 2738
    },
    {
      "epoch": 1.0603948896631823,
      "grad_norm": 14.440267562866211,
      "learning_rate": 9.932894567040909e-06,
      "loss": 1.1544,
      "step": 2739
    },
    {
      "epoch": 1.0607820363917926,
      "grad_norm": 18.006820678710938,
      "learning_rate": 9.93246440400912e-06,
      "loss": 1.2512,
      "step": 2740
    },
    {
      "epoch": 1.0611691831204026,
      "grad_norm": 22.797887802124023,
      "learning_rate": 9.93203424097733e-06,
      "loss": 1.4821,
      "step": 2741
    },
    {
      "epoch": 1.0615563298490127,
      "grad_norm": 21.490034103393555,
      "learning_rate": 9.931604077945543e-06,
      "loss": 1.4541,
      "step": 2742
    },
    {
      "epoch": 1.061943476577623,
      "grad_norm": 7.195273399353027,
      "learning_rate": 9.931173914913753e-06,
      "loss": 1.0029,
      "step": 2743
    },
    {
      "epoch": 1.062330623306233,
      "grad_norm": 11.953699111938477,
      "learning_rate": 9.930743751881964e-06,
      "loss": 1.3832,
      "step": 2744
    },
    {
      "epoch": 1.0627177700348431,
      "grad_norm": 9.280082702636719,
      "learning_rate": 9.930313588850174e-06,
      "loss": 1.26,
      "step": 2745
    },
    {
      "epoch": 1.0631049167634534,
      "grad_norm": 19.15138053894043,
      "learning_rate": 9.929883425818387e-06,
      "loss": 1.3036,
      "step": 2746
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 12.706775665283203,
      "learning_rate": 9.929453262786597e-06,
      "loss": 1.3731,
      "step": 2747
    },
    {
      "epoch": 1.0638792102206736,
      "grad_norm": 14.212179183959961,
      "learning_rate": 9.929023099754808e-06,
      "loss": 1.4801,
      "step": 2748
    },
    {
      "epoch": 1.0642663569492838,
      "grad_norm": 14.667960166931152,
      "learning_rate": 9.92859293672302e-06,
      "loss": 1.1137,
      "step": 2749
    },
    {
      "epoch": 1.064653503677894,
      "grad_norm": 12.149188995361328,
      "learning_rate": 9.92816277369123e-06,
      "loss": 1.673,
      "step": 2750
    },
    {
      "epoch": 1.065040650406504,
      "grad_norm": 14.388509750366211,
      "learning_rate": 9.92773261065944e-06,
      "loss": 1.4741,
      "step": 2751
    },
    {
      "epoch": 1.0654277971351143,
      "grad_norm": 13.235852241516113,
      "learning_rate": 9.927302447627652e-06,
      "loss": 1.1686,
      "step": 2752
    },
    {
      "epoch": 1.0658149438637243,
      "grad_norm": 13.542532920837402,
      "learning_rate": 9.926872284595864e-06,
      "loss": 1.0848,
      "step": 2753
    },
    {
      "epoch": 1.0662020905923344,
      "grad_norm": 16.028987884521484,
      "learning_rate": 9.926442121564073e-06,
      "loss": 1.5912,
      "step": 2754
    },
    {
      "epoch": 1.0665892373209447,
      "grad_norm": 14.137190818786621,
      "learning_rate": 9.926011958532285e-06,
      "loss": 1.8032,
      "step": 2755
    },
    {
      "epoch": 1.0669763840495547,
      "grad_norm": 26.410377502441406,
      "learning_rate": 9.925581795500494e-06,
      "loss": 1.8921,
      "step": 2756
    },
    {
      "epoch": 1.0673635307781648,
      "grad_norm": 26.358259201049805,
      "learning_rate": 9.925151632468708e-06,
      "loss": 1.6838,
      "step": 2757
    },
    {
      "epoch": 1.067750677506775,
      "grad_norm": 43.467864990234375,
      "learning_rate": 9.924721469436917e-06,
      "loss": 1.6382,
      "step": 2758
    },
    {
      "epoch": 1.0681378242353852,
      "grad_norm": 16.535184860229492,
      "learning_rate": 9.924291306405129e-06,
      "loss": 2.8522,
      "step": 2759
    },
    {
      "epoch": 1.0685249709639955,
      "grad_norm": 16.37344741821289,
      "learning_rate": 9.923861143373338e-06,
      "loss": 1.4407,
      "step": 2760
    },
    {
      "epoch": 1.0689121176926055,
      "grad_norm": 16.149799346923828,
      "learning_rate": 9.923430980341552e-06,
      "loss": 1.4488,
      "step": 2761
    },
    {
      "epoch": 1.0692992644212156,
      "grad_norm": 39.77202606201172,
      "learning_rate": 9.923000817309761e-06,
      "loss": 2.5566,
      "step": 2762
    },
    {
      "epoch": 1.0696864111498259,
      "grad_norm": 13.254947662353516,
      "learning_rate": 9.922570654277973e-06,
      "loss": 1.0573,
      "step": 2763
    },
    {
      "epoch": 1.070073557878436,
      "grad_norm": 21.62392807006836,
      "learning_rate": 9.922140491246182e-06,
      "loss": 2.2617,
      "step": 2764
    },
    {
      "epoch": 1.070460704607046,
      "grad_norm": 16.465484619140625,
      "learning_rate": 9.921710328214394e-06,
      "loss": 1.5905,
      "step": 2765
    },
    {
      "epoch": 1.0708478513356563,
      "grad_norm": 21.469348907470703,
      "learning_rate": 9.921280165182605e-06,
      "loss": 1.5324,
      "step": 2766
    },
    {
      "epoch": 1.0712349980642664,
      "grad_norm": 17.836688995361328,
      "learning_rate": 9.920850002150817e-06,
      "loss": 1.3134,
      "step": 2767
    },
    {
      "epoch": 1.0716221447928764,
      "grad_norm": 14.737174987792969,
      "learning_rate": 9.920419839119026e-06,
      "loss": 1.2941,
      "step": 2768
    },
    {
      "epoch": 1.0720092915214867,
      "grad_norm": 19.31356430053711,
      "learning_rate": 9.919989676087238e-06,
      "loss": 1.1312,
      "step": 2769
    },
    {
      "epoch": 1.0723964382500968,
      "grad_norm": 17.751359939575195,
      "learning_rate": 9.91955951305545e-06,
      "loss": 1.6935,
      "step": 2770
    },
    {
      "epoch": 1.0727835849787069,
      "grad_norm": 11.188058853149414,
      "learning_rate": 9.919129350023659e-06,
      "loss": 1.4623,
      "step": 2771
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 17.781143188476562,
      "learning_rate": 9.91869918699187e-06,
      "loss": 2.043,
      "step": 2772
    },
    {
      "epoch": 1.0735578784359272,
      "grad_norm": 19.81410026550293,
      "learning_rate": 9.918269023960082e-06,
      "loss": 1.4629,
      "step": 2773
    },
    {
      "epoch": 1.0739450251645373,
      "grad_norm": 14.021395683288574,
      "learning_rate": 9.917838860928293e-06,
      "loss": 1.6348,
      "step": 2774
    },
    {
      "epoch": 1.0743321718931476,
      "grad_norm": 9.510770797729492,
      "learning_rate": 9.917408697896503e-06,
      "loss": 1.3489,
      "step": 2775
    },
    {
      "epoch": 1.0747193186217576,
      "grad_norm": 11.668146133422852,
      "learning_rate": 9.916978534864714e-06,
      "loss": 1.2344,
      "step": 2776
    },
    {
      "epoch": 1.0751064653503677,
      "grad_norm": 11.00218391418457,
      "learning_rate": 9.916548371832926e-06,
      "loss": 1.0664,
      "step": 2777
    },
    {
      "epoch": 1.075493612078978,
      "grad_norm": 12.528824806213379,
      "learning_rate": 9.916118208801137e-06,
      "loss": 1.238,
      "step": 2778
    },
    {
      "epoch": 1.075880758807588,
      "grad_norm": 16.28148078918457,
      "learning_rate": 9.915688045769347e-06,
      "loss": 1.4971,
      "step": 2779
    },
    {
      "epoch": 1.076267905536198,
      "grad_norm": 15.889019966125488,
      "learning_rate": 9.915257882737558e-06,
      "loss": 1.5115,
      "step": 2780
    },
    {
      "epoch": 1.0766550522648084,
      "grad_norm": 26.95136070251465,
      "learning_rate": 9.91482771970577e-06,
      "loss": 1.8876,
      "step": 2781
    },
    {
      "epoch": 1.0770421989934185,
      "grad_norm": 11.517533302307129,
      "learning_rate": 9.914397556673981e-06,
      "loss": 1.6444,
      "step": 2782
    },
    {
      "epoch": 1.0774293457220288,
      "grad_norm": 11.656947135925293,
      "learning_rate": 9.91396739364219e-06,
      "loss": 0.9656,
      "step": 2783
    },
    {
      "epoch": 1.0778164924506388,
      "grad_norm": 21.93158531188965,
      "learning_rate": 9.913537230610402e-06,
      "loss": 1.7222,
      "step": 2784
    },
    {
      "epoch": 1.0782036391792489,
      "grad_norm": 19.456613540649414,
      "learning_rate": 9.913107067578614e-06,
      "loss": 1.7828,
      "step": 2785
    },
    {
      "epoch": 1.0785907859078592,
      "grad_norm": 17.169706344604492,
      "learning_rate": 9.912676904546823e-06,
      "loss": 1.2231,
      "step": 2786
    },
    {
      "epoch": 1.0789779326364692,
      "grad_norm": 16.774999618530273,
      "learning_rate": 9.912246741515035e-06,
      "loss": 1.7352,
      "step": 2787
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 17.424175262451172,
      "learning_rate": 9.911816578483246e-06,
      "loss": 1.8645,
      "step": 2788
    },
    {
      "epoch": 1.0797522260936896,
      "grad_norm": 24.920774459838867,
      "learning_rate": 9.911386415451458e-06,
      "loss": 2.1198,
      "step": 2789
    },
    {
      "epoch": 1.0801393728222997,
      "grad_norm": 15.377825736999512,
      "learning_rate": 9.910956252419667e-06,
      "loss": 1.6927,
      "step": 2790
    },
    {
      "epoch": 1.0805265195509097,
      "grad_norm": 7.957217693328857,
      "learning_rate": 9.910526089387879e-06,
      "loss": 1.3961,
      "step": 2791
    },
    {
      "epoch": 1.08091366627952,
      "grad_norm": 19.455642700195312,
      "learning_rate": 9.91009592635609e-06,
      "loss": 1.1531,
      "step": 2792
    },
    {
      "epoch": 1.08130081300813,
      "grad_norm": 13.98209285736084,
      "learning_rate": 9.909665763324302e-06,
      "loss": 1.2244,
      "step": 2793
    },
    {
      "epoch": 1.0816879597367401,
      "grad_norm": 11.010237693786621,
      "learning_rate": 9.909235600292511e-06,
      "loss": 1.3215,
      "step": 2794
    },
    {
      "epoch": 1.0820751064653504,
      "grad_norm": 15.592874526977539,
      "learning_rate": 9.908805437260723e-06,
      "loss": 1.3656,
      "step": 2795
    },
    {
      "epoch": 1.0824622531939605,
      "grad_norm": 31.84793472290039,
      "learning_rate": 9.908375274228934e-06,
      "loss": 1.6123,
      "step": 2796
    },
    {
      "epoch": 1.0828493999225706,
      "grad_norm": 29.248933792114258,
      "learning_rate": 9.907945111197146e-06,
      "loss": 1.9832,
      "step": 2797
    },
    {
      "epoch": 1.0832365466511809,
      "grad_norm": 13.054574012756348,
      "learning_rate": 9.907514948165355e-06,
      "loss": 0.8483,
      "step": 2798
    },
    {
      "epoch": 1.083623693379791,
      "grad_norm": 11.118550300598145,
      "learning_rate": 9.907084785133567e-06,
      "loss": 1.2801,
      "step": 2799
    },
    {
      "epoch": 1.084010840108401,
      "grad_norm": 13.279389381408691,
      "learning_rate": 9.906654622101778e-06,
      "loss": 1.5434,
      "step": 2800
    },
    {
      "epoch": 1.0843979868370113,
      "grad_norm": 12.259542465209961,
      "learning_rate": 9.906224459069988e-06,
      "loss": 1.5604,
      "step": 2801
    },
    {
      "epoch": 1.0847851335656213,
      "grad_norm": 13.227477073669434,
      "learning_rate": 9.9057942960382e-06,
      "loss": 1.2603,
      "step": 2802
    },
    {
      "epoch": 1.0851722802942314,
      "grad_norm": 13.82474136352539,
      "learning_rate": 9.90536413300641e-06,
      "loss": 1.2329,
      "step": 2803
    },
    {
      "epoch": 1.0855594270228417,
      "grad_norm": 20.358135223388672,
      "learning_rate": 9.904933969974622e-06,
      "loss": 1.7101,
      "step": 2804
    },
    {
      "epoch": 1.0859465737514518,
      "grad_norm": 19.31425666809082,
      "learning_rate": 9.904503806942832e-06,
      "loss": 1.3342,
      "step": 2805
    },
    {
      "epoch": 1.086333720480062,
      "grad_norm": 14.565228462219238,
      "learning_rate": 9.904073643911043e-06,
      "loss": 1.0581,
      "step": 2806
    },
    {
      "epoch": 1.0867208672086721,
      "grad_norm": 27.635637283325195,
      "learning_rate": 9.903643480879253e-06,
      "loss": 1.1022,
      "step": 2807
    },
    {
      "epoch": 1.0871080139372822,
      "grad_norm": 15.063994407653809,
      "learning_rate": 9.903213317847466e-06,
      "loss": 1.5473,
      "step": 2808
    },
    {
      "epoch": 1.0874951606658925,
      "grad_norm": 16.790645599365234,
      "learning_rate": 9.902783154815676e-06,
      "loss": 1.6172,
      "step": 2809
    },
    {
      "epoch": 1.0878823073945025,
      "grad_norm": 22.252273559570312,
      "learning_rate": 9.902352991783887e-06,
      "loss": 2.1159,
      "step": 2810
    },
    {
      "epoch": 1.0882694541231126,
      "grad_norm": 44.53988265991211,
      "learning_rate": 9.901922828752097e-06,
      "loss": 1.696,
      "step": 2811
    },
    {
      "epoch": 1.0886566008517229,
      "grad_norm": 14.825989723205566,
      "learning_rate": 9.90149266572031e-06,
      "loss": 1.5576,
      "step": 2812
    },
    {
      "epoch": 1.089043747580333,
      "grad_norm": 19.902507781982422,
      "learning_rate": 9.90106250268852e-06,
      "loss": 1.6071,
      "step": 2813
    },
    {
      "epoch": 1.089430894308943,
      "grad_norm": 18.876441955566406,
      "learning_rate": 9.900632339656731e-06,
      "loss": 1.9451,
      "step": 2814
    },
    {
      "epoch": 1.0898180410375533,
      "grad_norm": 14.324376106262207,
      "learning_rate": 9.900202176624941e-06,
      "loss": 1.5577,
      "step": 2815
    },
    {
      "epoch": 1.0902051877661634,
      "grad_norm": 14.74123477935791,
      "learning_rate": 9.899772013593152e-06,
      "loss": 1.3675,
      "step": 2816
    },
    {
      "epoch": 1.0905923344947734,
      "grad_norm": 11.133639335632324,
      "learning_rate": 9.899341850561364e-06,
      "loss": 0.9762,
      "step": 2817
    },
    {
      "epoch": 1.0909794812233837,
      "grad_norm": 14.743924140930176,
      "learning_rate": 9.898911687529575e-06,
      "loss": 1.3684,
      "step": 2818
    },
    {
      "epoch": 1.0913666279519938,
      "grad_norm": 17.064224243164062,
      "learning_rate": 9.898481524497785e-06,
      "loss": 1.9085,
      "step": 2819
    },
    {
      "epoch": 1.0917537746806039,
      "grad_norm": 13.625917434692383,
      "learning_rate": 9.898051361465996e-06,
      "loss": 1.4753,
      "step": 2820
    },
    {
      "epoch": 1.0921409214092141,
      "grad_norm": 13.991575241088867,
      "learning_rate": 9.897621198434208e-06,
      "loss": 1.0837,
      "step": 2821
    },
    {
      "epoch": 1.0925280681378242,
      "grad_norm": 8.075464248657227,
      "learning_rate": 9.897191035402417e-06,
      "loss": 0.9777,
      "step": 2822
    },
    {
      "epoch": 1.0929152148664343,
      "grad_norm": 14.909154891967773,
      "learning_rate": 9.896760872370629e-06,
      "loss": 1.3451,
      "step": 2823
    },
    {
      "epoch": 1.0933023615950446,
      "grad_norm": 20.100353240966797,
      "learning_rate": 9.89633070933884e-06,
      "loss": 1.4664,
      "step": 2824
    },
    {
      "epoch": 1.0936895083236546,
      "grad_norm": 9.702215194702148,
      "learning_rate": 9.895900546307052e-06,
      "loss": 0.874,
      "step": 2825
    },
    {
      "epoch": 1.0940766550522647,
      "grad_norm": 12.105074882507324,
      "learning_rate": 9.895470383275261e-06,
      "loss": 1.1986,
      "step": 2826
    },
    {
      "epoch": 1.094463801780875,
      "grad_norm": 17.996173858642578,
      "learning_rate": 9.895040220243473e-06,
      "loss": 1.2679,
      "step": 2827
    },
    {
      "epoch": 1.094850948509485,
      "grad_norm": 17.211721420288086,
      "learning_rate": 9.894610057211684e-06,
      "loss": 1.3798,
      "step": 2828
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 18.759727478027344,
      "learning_rate": 9.894179894179896e-06,
      "loss": 2.0501,
      "step": 2829
    },
    {
      "epoch": 1.0956252419667054,
      "grad_norm": 14.94530963897705,
      "learning_rate": 9.893749731148105e-06,
      "loss": 1.109,
      "step": 2830
    },
    {
      "epoch": 1.0960123886953155,
      "grad_norm": 20.430320739746094,
      "learning_rate": 9.893319568116317e-06,
      "loss": 1.41,
      "step": 2831
    },
    {
      "epoch": 1.0963995354239258,
      "grad_norm": 13.926247596740723,
      "learning_rate": 9.892889405084528e-06,
      "loss": 1.0417,
      "step": 2832
    },
    {
      "epoch": 1.0967866821525358,
      "grad_norm": 11.009696960449219,
      "learning_rate": 9.89245924205274e-06,
      "loss": 0.8338,
      "step": 2833
    },
    {
      "epoch": 1.097173828881146,
      "grad_norm": 16.837055206298828,
      "learning_rate": 9.89202907902095e-06,
      "loss": 1.5148,
      "step": 2834
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 16.786354064941406,
      "learning_rate": 9.89159891598916e-06,
      "loss": 2.2301,
      "step": 2835
    },
    {
      "epoch": 1.0979481223383663,
      "grad_norm": 21.383834838867188,
      "learning_rate": 9.891168752957372e-06,
      "loss": 1.5593,
      "step": 2836
    },
    {
      "epoch": 1.0983352690669763,
      "grad_norm": 17.937488555908203,
      "learning_rate": 9.890738589925582e-06,
      "loss": 2.2074,
      "step": 2837
    },
    {
      "epoch": 1.0987224157955866,
      "grad_norm": 21.846654891967773,
      "learning_rate": 9.890308426893793e-06,
      "loss": 2.0217,
      "step": 2838
    },
    {
      "epoch": 1.0991095625241967,
      "grad_norm": 16.236703872680664,
      "learning_rate": 9.889878263862005e-06,
      "loss": 1.436,
      "step": 2839
    },
    {
      "epoch": 1.0994967092528067,
      "grad_norm": 19.087764739990234,
      "learning_rate": 9.889448100830216e-06,
      "loss": 2.0653,
      "step": 2840
    },
    {
      "epoch": 1.099883855981417,
      "grad_norm": 9.46346664428711,
      "learning_rate": 9.889017937798426e-06,
      "loss": 1.2812,
      "step": 2841
    },
    {
      "epoch": 1.100271002710027,
      "grad_norm": 16.659927368164062,
      "learning_rate": 9.888587774766637e-06,
      "loss": 1.5111,
      "step": 2842
    },
    {
      "epoch": 1.1006581494386372,
      "grad_norm": 13.559014320373535,
      "learning_rate": 9.888157611734849e-06,
      "loss": 1.2929,
      "step": 2843
    },
    {
      "epoch": 1.1010452961672474,
      "grad_norm": 35.87084197998047,
      "learning_rate": 9.88772744870306e-06,
      "loss": 2.0532,
      "step": 2844
    },
    {
      "epoch": 1.1014324428958575,
      "grad_norm": 22.946332931518555,
      "learning_rate": 9.88729728567127e-06,
      "loss": 1.7546,
      "step": 2845
    },
    {
      "epoch": 1.1018195896244676,
      "grad_norm": 28.69762420654297,
      "learning_rate": 9.886867122639481e-06,
      "loss": 1.5162,
      "step": 2846
    },
    {
      "epoch": 1.1022067363530779,
      "grad_norm": 16.257526397705078,
      "learning_rate": 9.886436959607693e-06,
      "loss": 1.4686,
      "step": 2847
    },
    {
      "epoch": 1.102593883081688,
      "grad_norm": 11.159704208374023,
      "learning_rate": 9.886006796575904e-06,
      "loss": 0.77,
      "step": 2848
    },
    {
      "epoch": 1.102981029810298,
      "grad_norm": 15.644683837890625,
      "learning_rate": 9.885576633544114e-06,
      "loss": 1.1403,
      "step": 2849
    },
    {
      "epoch": 1.1033681765389083,
      "grad_norm": 14.23741626739502,
      "learning_rate": 9.885146470512325e-06,
      "loss": 1.6198,
      "step": 2850
    },
    {
      "epoch": 1.1037553232675184,
      "grad_norm": 28.121519088745117,
      "learning_rate": 9.884716307480537e-06,
      "loss": 1.9775,
      "step": 2851
    },
    {
      "epoch": 1.1041424699961286,
      "grad_norm": 25.742584228515625,
      "learning_rate": 9.884286144448746e-06,
      "loss": 1.7004,
      "step": 2852
    },
    {
      "epoch": 1.1045296167247387,
      "grad_norm": 17.49236297607422,
      "learning_rate": 9.883855981416958e-06,
      "loss": 1.1611,
      "step": 2853
    },
    {
      "epoch": 1.1049167634533488,
      "grad_norm": 17.130149841308594,
      "learning_rate": 9.883425818385169e-06,
      "loss": 2.3317,
      "step": 2854
    },
    {
      "epoch": 1.105303910181959,
      "grad_norm": 13.101213455200195,
      "learning_rate": 9.88299565535338e-06,
      "loss": 1.4709,
      "step": 2855
    },
    {
      "epoch": 1.1056910569105691,
      "grad_norm": 19.066110610961914,
      "learning_rate": 9.88256549232159e-06,
      "loss": 1.4789,
      "step": 2856
    },
    {
      "epoch": 1.1060782036391792,
      "grad_norm": 15.437875747680664,
      "learning_rate": 9.882135329289802e-06,
      "loss": 1.57,
      "step": 2857
    },
    {
      "epoch": 1.1064653503677895,
      "grad_norm": 18.65119171142578,
      "learning_rate": 9.881705166258011e-06,
      "loss": 1.7021,
      "step": 2858
    },
    {
      "epoch": 1.1068524970963995,
      "grad_norm": 15.455779075622559,
      "learning_rate": 9.881275003226225e-06,
      "loss": 0.9609,
      "step": 2859
    },
    {
      "epoch": 1.1072396438250096,
      "grad_norm": 13.241989135742188,
      "learning_rate": 9.880844840194434e-06,
      "loss": 1.6783,
      "step": 2860
    },
    {
      "epoch": 1.10762679055362,
      "grad_norm": 15.756484985351562,
      "learning_rate": 9.880414677162646e-06,
      "loss": 1.8227,
      "step": 2861
    },
    {
      "epoch": 1.10801393728223,
      "grad_norm": 17.05937957763672,
      "learning_rate": 9.879984514130855e-06,
      "loss": 1.927,
      "step": 2862
    },
    {
      "epoch": 1.10840108401084,
      "grad_norm": 18.8395938873291,
      "learning_rate": 9.879554351099068e-06,
      "loss": 1.5739,
      "step": 2863
    },
    {
      "epoch": 1.1087882307394503,
      "grad_norm": 12.058917999267578,
      "learning_rate": 9.879124188067278e-06,
      "loss": 0.7897,
      "step": 2864
    },
    {
      "epoch": 1.1091753774680604,
      "grad_norm": 15.459847450256348,
      "learning_rate": 9.87869402503549e-06,
      "loss": 1.302,
      "step": 2865
    },
    {
      "epoch": 1.1095625241966705,
      "grad_norm": 14.86146068572998,
      "learning_rate": 9.8782638620037e-06,
      "loss": 1.6483,
      "step": 2866
    },
    {
      "epoch": 1.1099496709252807,
      "grad_norm": 15.276413917541504,
      "learning_rate": 9.87783369897191e-06,
      "loss": 1.627,
      "step": 2867
    },
    {
      "epoch": 1.1103368176538908,
      "grad_norm": 17.290611267089844,
      "learning_rate": 9.877403535940122e-06,
      "loss": 1.1946,
      "step": 2868
    },
    {
      "epoch": 1.1107239643825009,
      "grad_norm": 18.400821685791016,
      "learning_rate": 9.876973372908334e-06,
      "loss": 1.4861,
      "step": 2869
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 23.436397552490234,
      "learning_rate": 9.876543209876543e-06,
      "loss": 1.6987,
      "step": 2870
    },
    {
      "epoch": 1.1114982578397212,
      "grad_norm": 20.829509735107422,
      "learning_rate": 9.876113046844755e-06,
      "loss": 1.5905,
      "step": 2871
    },
    {
      "epoch": 1.1118854045683313,
      "grad_norm": 13.985857009887695,
      "learning_rate": 9.875682883812966e-06,
      "loss": 0.9942,
      "step": 2872
    },
    {
      "epoch": 1.1122725512969416,
      "grad_norm": 16.4593563079834,
      "learning_rate": 9.875252720781176e-06,
      "loss": 2.0655,
      "step": 2873
    },
    {
      "epoch": 1.1126596980255516,
      "grad_norm": 20.540531158447266,
      "learning_rate": 9.874822557749389e-06,
      "loss": 1.2762,
      "step": 2874
    },
    {
      "epoch": 1.113046844754162,
      "grad_norm": 12.928327560424805,
      "learning_rate": 9.874392394717599e-06,
      "loss": 1.6058,
      "step": 2875
    },
    {
      "epoch": 1.113433991482772,
      "grad_norm": 12.820450782775879,
      "learning_rate": 9.87396223168581e-06,
      "loss": 0.8833,
      "step": 2876
    },
    {
      "epoch": 1.113821138211382,
      "grad_norm": 16.101985931396484,
      "learning_rate": 9.87353206865402e-06,
      "loss": 1.5987,
      "step": 2877
    },
    {
      "epoch": 1.1142082849399924,
      "grad_norm": 13.91110897064209,
      "learning_rate": 9.873101905622233e-06,
      "loss": 1.6438,
      "step": 2878
    },
    {
      "epoch": 1.1145954316686024,
      "grad_norm": 21.589624404907227,
      "learning_rate": 9.872671742590443e-06,
      "loss": 2.0941,
      "step": 2879
    },
    {
      "epoch": 1.1149825783972125,
      "grad_norm": 14.72765064239502,
      "learning_rate": 9.872241579558654e-06,
      "loss": 1.1228,
      "step": 2880
    },
    {
      "epoch": 1.1153697251258228,
      "grad_norm": 15.649147987365723,
      "learning_rate": 9.871811416526864e-06,
      "loss": 1.1748,
      "step": 2881
    },
    {
      "epoch": 1.1157568718544328,
      "grad_norm": 34.76520538330078,
      "learning_rate": 9.871381253495075e-06,
      "loss": 2.3709,
      "step": 2882
    },
    {
      "epoch": 1.116144018583043,
      "grad_norm": 32.62091827392578,
      "learning_rate": 9.870951090463287e-06,
      "loss": 2.3695,
      "step": 2883
    },
    {
      "epoch": 1.1165311653116532,
      "grad_norm": 29.130706787109375,
      "learning_rate": 9.870520927431498e-06,
      "loss": 1.9355,
      "step": 2884
    },
    {
      "epoch": 1.1169183120402633,
      "grad_norm": 19.52931022644043,
      "learning_rate": 9.870090764399708e-06,
      "loss": 1.4003,
      "step": 2885
    },
    {
      "epoch": 1.1173054587688733,
      "grad_norm": 14.301163673400879,
      "learning_rate": 9.86966060136792e-06,
      "loss": 1.3021,
      "step": 2886
    },
    {
      "epoch": 1.1176926054974836,
      "grad_norm": 20.859174728393555,
      "learning_rate": 9.86923043833613e-06,
      "loss": 1.327,
      "step": 2887
    },
    {
      "epoch": 1.1180797522260937,
      "grad_norm": 22.406583786010742,
      "learning_rate": 9.86880027530434e-06,
      "loss": 2.6807,
      "step": 2888
    },
    {
      "epoch": 1.1184668989547037,
      "grad_norm": 14.193469047546387,
      "learning_rate": 9.868370112272552e-06,
      "loss": 1.1535,
      "step": 2889
    },
    {
      "epoch": 1.118854045683314,
      "grad_norm": 13.19684886932373,
      "learning_rate": 9.867939949240763e-06,
      "loss": 1.0448,
      "step": 2890
    },
    {
      "epoch": 1.119241192411924,
      "grad_norm": 12.06394100189209,
      "learning_rate": 9.867509786208975e-06,
      "loss": 0.476,
      "step": 2891
    },
    {
      "epoch": 1.1196283391405342,
      "grad_norm": 13.442334175109863,
      "learning_rate": 9.867079623177184e-06,
      "loss": 0.9449,
      "step": 2892
    },
    {
      "epoch": 1.1200154858691445,
      "grad_norm": 12.541753768920898,
      "learning_rate": 9.866649460145396e-06,
      "loss": 1.4878,
      "step": 2893
    },
    {
      "epoch": 1.1204026325977545,
      "grad_norm": 20.570653915405273,
      "learning_rate": 9.866219297113607e-06,
      "loss": 1.4762,
      "step": 2894
    },
    {
      "epoch": 1.1207897793263646,
      "grad_norm": 23.315093994140625,
      "learning_rate": 9.865789134081819e-06,
      "loss": 1.8936,
      "step": 2895
    },
    {
      "epoch": 1.1211769260549749,
      "grad_norm": 12.95217514038086,
      "learning_rate": 9.865358971050028e-06,
      "loss": 1.039,
      "step": 2896
    },
    {
      "epoch": 1.121564072783585,
      "grad_norm": 14.480637550354004,
      "learning_rate": 9.86492880801824e-06,
      "loss": 1.6756,
      "step": 2897
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 18.257648468017578,
      "learning_rate": 9.864498644986451e-06,
      "loss": 1.6468,
      "step": 2898
    },
    {
      "epoch": 1.1223383662408053,
      "grad_norm": 8.871774673461914,
      "learning_rate": 9.864068481954663e-06,
      "loss": 0.9127,
      "step": 2899
    },
    {
      "epoch": 1.1227255129694154,
      "grad_norm": 14.512118339538574,
      "learning_rate": 9.863638318922872e-06,
      "loss": 1.3284,
      "step": 2900
    },
    {
      "epoch": 1.1231126596980257,
      "grad_norm": 22.90131950378418,
      "learning_rate": 9.863208155891084e-06,
      "loss": 1.5087,
      "step": 2901
    },
    {
      "epoch": 1.1234998064266357,
      "grad_norm": 19.218748092651367,
      "learning_rate": 9.862777992859295e-06,
      "loss": 1.9366,
      "step": 2902
    },
    {
      "epoch": 1.1238869531552458,
      "grad_norm": 23.199901580810547,
      "learning_rate": 9.862347829827505e-06,
      "loss": 1.5777,
      "step": 2903
    },
    {
      "epoch": 1.124274099883856,
      "grad_norm": 21.8846378326416,
      "learning_rate": 9.861917666795716e-06,
      "loss": 1.6683,
      "step": 2904
    },
    {
      "epoch": 1.1246612466124661,
      "grad_norm": 31.64393424987793,
      "learning_rate": 9.861487503763928e-06,
      "loss": 1.9385,
      "step": 2905
    },
    {
      "epoch": 1.1250483933410762,
      "grad_norm": 13.708986282348633,
      "learning_rate": 9.861057340732139e-06,
      "loss": 1.0758,
      "step": 2906
    },
    {
      "epoch": 1.1254355400696865,
      "grad_norm": 26.54155731201172,
      "learning_rate": 9.860627177700349e-06,
      "loss": 1.6345,
      "step": 2907
    },
    {
      "epoch": 1.1258226867982966,
      "grad_norm": 20.284221649169922,
      "learning_rate": 9.86019701466856e-06,
      "loss": 1.5414,
      "step": 2908
    },
    {
      "epoch": 1.1262098335269066,
      "grad_norm": 15.819384574890137,
      "learning_rate": 9.85976685163677e-06,
      "loss": 1.5592,
      "step": 2909
    },
    {
      "epoch": 1.126596980255517,
      "grad_norm": 12.518463134765625,
      "learning_rate": 9.859336688604983e-06,
      "loss": 1.4985,
      "step": 2910
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 12.258554458618164,
      "learning_rate": 9.858906525573193e-06,
      "loss": 1.2325,
      "step": 2911
    },
    {
      "epoch": 1.127371273712737,
      "grad_norm": 19.8494815826416,
      "learning_rate": 9.858476362541404e-06,
      "loss": 1.5866,
      "step": 2912
    },
    {
      "epoch": 1.1277584204413473,
      "grad_norm": 19.8845272064209,
      "learning_rate": 9.858046199509616e-06,
      "loss": 1.5697,
      "step": 2913
    },
    {
      "epoch": 1.1281455671699574,
      "grad_norm": 20.78984260559082,
      "learning_rate": 9.857616036477827e-06,
      "loss": 1.0873,
      "step": 2914
    },
    {
      "epoch": 1.1285327138985675,
      "grad_norm": 17.576236724853516,
      "learning_rate": 9.857185873446037e-06,
      "loss": 2.1668,
      "step": 2915
    },
    {
      "epoch": 1.1289198606271778,
      "grad_norm": 39.09327697753906,
      "learning_rate": 9.856755710414248e-06,
      "loss": 2.1146,
      "step": 2916
    },
    {
      "epoch": 1.1293070073557878,
      "grad_norm": 26.03662109375,
      "learning_rate": 9.85632554738246e-06,
      "loss": 1.851,
      "step": 2917
    },
    {
      "epoch": 1.1296941540843979,
      "grad_norm": 22.948375701904297,
      "learning_rate": 9.85589538435067e-06,
      "loss": 1.8922,
      "step": 2918
    },
    {
      "epoch": 1.1300813008130082,
      "grad_norm": 40.99020767211914,
      "learning_rate": 9.85546522131888e-06,
      "loss": 2.5331,
      "step": 2919
    },
    {
      "epoch": 1.1304684475416182,
      "grad_norm": 22.032133102416992,
      "learning_rate": 9.855035058287092e-06,
      "loss": 2.031,
      "step": 2920
    },
    {
      "epoch": 1.1308555942702285,
      "grad_norm": 15.252873420715332,
      "learning_rate": 9.854604895255303e-06,
      "loss": 1.6853,
      "step": 2921
    },
    {
      "epoch": 1.1312427409988386,
      "grad_norm": 21.01805305480957,
      "learning_rate": 9.854174732223513e-06,
      "loss": 1.5046,
      "step": 2922
    },
    {
      "epoch": 1.1316298877274487,
      "grad_norm": 20.58396339416504,
      "learning_rate": 9.853744569191725e-06,
      "loss": 1.9132,
      "step": 2923
    },
    {
      "epoch": 1.1320170344560587,
      "grad_norm": 15.415303230285645,
      "learning_rate": 9.853314406159934e-06,
      "loss": 1.3863,
      "step": 2924
    },
    {
      "epoch": 1.132404181184669,
      "grad_norm": 30.044641494750977,
      "learning_rate": 9.852884243128147e-06,
      "loss": 3.0817,
      "step": 2925
    },
    {
      "epoch": 1.132791327913279,
      "grad_norm": 20.82019805908203,
      "learning_rate": 9.852454080096357e-06,
      "loss": 2.3817,
      "step": 2926
    },
    {
      "epoch": 1.1331784746418894,
      "grad_norm": 11.634734153747559,
      "learning_rate": 9.852023917064569e-06,
      "loss": 1.2372,
      "step": 2927
    },
    {
      "epoch": 1.1335656213704994,
      "grad_norm": 9.812459945678711,
      "learning_rate": 9.851593754032778e-06,
      "loss": 1.2807,
      "step": 2928
    },
    {
      "epoch": 1.1339527680991095,
      "grad_norm": 23.327823638916016,
      "learning_rate": 9.851163591000991e-06,
      "loss": 1.7764,
      "step": 2929
    },
    {
      "epoch": 1.1343399148277198,
      "grad_norm": 21.15706443786621,
      "learning_rate": 9.850733427969201e-06,
      "loss": 1.7085,
      "step": 2930
    },
    {
      "epoch": 1.1347270615563299,
      "grad_norm": 18.809093475341797,
      "learning_rate": 9.850303264937413e-06,
      "loss": 1.8063,
      "step": 2931
    },
    {
      "epoch": 1.13511420828494,
      "grad_norm": 13.963958740234375,
      "learning_rate": 9.849873101905622e-06,
      "loss": 1.4123,
      "step": 2932
    },
    {
      "epoch": 1.1355013550135502,
      "grad_norm": 14.73353385925293,
      "learning_rate": 9.849442938873834e-06,
      "loss": 1.4928,
      "step": 2933
    },
    {
      "epoch": 1.1358885017421603,
      "grad_norm": 11.870315551757812,
      "learning_rate": 9.849012775842045e-06,
      "loss": 1.2289,
      "step": 2934
    },
    {
      "epoch": 1.1362756484707703,
      "grad_norm": 15.009160995483398,
      "learning_rate": 9.848582612810257e-06,
      "loss": 1.5651,
      "step": 2935
    },
    {
      "epoch": 1.1366627951993806,
      "grad_norm": 20.653011322021484,
      "learning_rate": 9.848152449778466e-06,
      "loss": 1.5242,
      "step": 2936
    },
    {
      "epoch": 1.1370499419279907,
      "grad_norm": 18.36014747619629,
      "learning_rate": 9.847722286746678e-06,
      "loss": 1.5932,
      "step": 2937
    },
    {
      "epoch": 1.1374370886566008,
      "grad_norm": 13.839876174926758,
      "learning_rate": 9.847292123714889e-06,
      "loss": 1.4004,
      "step": 2938
    },
    {
      "epoch": 1.137824235385211,
      "grad_norm": 12.33191204071045,
      "learning_rate": 9.846861960683099e-06,
      "loss": 0.8605,
      "step": 2939
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 31.5849609375,
      "learning_rate": 9.84643179765131e-06,
      "loss": 1.5645,
      "step": 2940
    },
    {
      "epoch": 1.1385985288424312,
      "grad_norm": 19.39096450805664,
      "learning_rate": 9.846001634619522e-06,
      "loss": 1.6948,
      "step": 2941
    },
    {
      "epoch": 1.1389856755710415,
      "grad_norm": 13.62159252166748,
      "learning_rate": 9.845571471587733e-06,
      "loss": 1.5285,
      "step": 2942
    },
    {
      "epoch": 1.1393728222996515,
      "grad_norm": 29.118619918823242,
      "learning_rate": 9.845141308555943e-06,
      "loss": 1.8037,
      "step": 2943
    },
    {
      "epoch": 1.1397599690282618,
      "grad_norm": 12.638991355895996,
      "learning_rate": 9.844711145524154e-06,
      "loss": 1.008,
      "step": 2944
    },
    {
      "epoch": 1.1401471157568719,
      "grad_norm": 16.07928466796875,
      "learning_rate": 9.844280982492366e-06,
      "loss": 1.5854,
      "step": 2945
    },
    {
      "epoch": 1.140534262485482,
      "grad_norm": 14.37851333618164,
      "learning_rate": 9.843850819460577e-06,
      "loss": 1.1365,
      "step": 2946
    },
    {
      "epoch": 1.140921409214092,
      "grad_norm": 14.047250747680664,
      "learning_rate": 9.843420656428787e-06,
      "loss": 1.2828,
      "step": 2947
    },
    {
      "epoch": 1.1413085559427023,
      "grad_norm": 14.361780166625977,
      "learning_rate": 9.842990493396998e-06,
      "loss": 1.3068,
      "step": 2948
    },
    {
      "epoch": 1.1416957026713124,
      "grad_norm": 32.172325134277344,
      "learning_rate": 9.84256033036521e-06,
      "loss": 1.2087,
      "step": 2949
    },
    {
      "epoch": 1.1420828493999227,
      "grad_norm": 19.969881057739258,
      "learning_rate": 9.842130167333421e-06,
      "loss": 1.5188,
      "step": 2950
    },
    {
      "epoch": 1.1424699961285327,
      "grad_norm": 16.035797119140625,
      "learning_rate": 9.84170000430163e-06,
      "loss": 1.5234,
      "step": 2951
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 15.759203910827637,
      "learning_rate": 9.841269841269842e-06,
      "loss": 2.3967,
      "step": 2952
    },
    {
      "epoch": 1.143244289585753,
      "grad_norm": 12.339290618896484,
      "learning_rate": 9.840839678238054e-06,
      "loss": 1.2222,
      "step": 2953
    },
    {
      "epoch": 1.1436314363143631,
      "grad_norm": 18.471786499023438,
      "learning_rate": 9.840409515206263e-06,
      "loss": 1.1467,
      "step": 2954
    },
    {
      "epoch": 1.1440185830429732,
      "grad_norm": 32.72879409790039,
      "learning_rate": 9.839979352174475e-06,
      "loss": 1.8418,
      "step": 2955
    },
    {
      "epoch": 1.1444057297715835,
      "grad_norm": 16.96495246887207,
      "learning_rate": 9.839549189142686e-06,
      "loss": 0.9659,
      "step": 2956
    },
    {
      "epoch": 1.1447928765001936,
      "grad_norm": 13.96474838256836,
      "learning_rate": 9.839119026110898e-06,
      "loss": 1.2243,
      "step": 2957
    },
    {
      "epoch": 1.1451800232288036,
      "grad_norm": 14.887704849243164,
      "learning_rate": 9.838688863079107e-06,
      "loss": 1.5392,
      "step": 2958
    },
    {
      "epoch": 1.145567169957414,
      "grad_norm": 17.775535583496094,
      "learning_rate": 9.838258700047319e-06,
      "loss": 1.7224,
      "step": 2959
    },
    {
      "epoch": 1.145954316686024,
      "grad_norm": 9.922630310058594,
      "learning_rate": 9.83782853701553e-06,
      "loss": 0.4296,
      "step": 2960
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 22.245471954345703,
      "learning_rate": 9.837398373983741e-06,
      "loss": 1.1567,
      "step": 2961
    },
    {
      "epoch": 1.1467286101432443,
      "grad_norm": 17.460216522216797,
      "learning_rate": 9.836968210951951e-06,
      "loss": 1.5537,
      "step": 2962
    },
    {
      "epoch": 1.1471157568718544,
      "grad_norm": 13.60256290435791,
      "learning_rate": 9.836538047920163e-06,
      "loss": 0.9501,
      "step": 2963
    },
    {
      "epoch": 1.1475029036004645,
      "grad_norm": 11.705642700195312,
      "learning_rate": 9.836107884888374e-06,
      "loss": 1.4102,
      "step": 2964
    },
    {
      "epoch": 1.1478900503290748,
      "grad_norm": 24.933467864990234,
      "learning_rate": 9.835677721856585e-06,
      "loss": 1.2985,
      "step": 2965
    },
    {
      "epoch": 1.1482771970576848,
      "grad_norm": 9.95095157623291,
      "learning_rate": 9.835247558824795e-06,
      "loss": 0.7327,
      "step": 2966
    },
    {
      "epoch": 1.1486643437862951,
      "grad_norm": 20.407468795776367,
      "learning_rate": 9.834817395793007e-06,
      "loss": 2.2685,
      "step": 2967
    },
    {
      "epoch": 1.1490514905149052,
      "grad_norm": 11.88222885131836,
      "learning_rate": 9.834387232761218e-06,
      "loss": 1.055,
      "step": 2968
    },
    {
      "epoch": 1.1494386372435152,
      "grad_norm": 18.896774291992188,
      "learning_rate": 9.833957069729428e-06,
      "loss": 1.6306,
      "step": 2969
    },
    {
      "epoch": 1.1498257839721253,
      "grad_norm": 17.754701614379883,
      "learning_rate": 9.833526906697639e-06,
      "loss": 1.5751,
      "step": 2970
    },
    {
      "epoch": 1.1502129307007356,
      "grad_norm": 20.714019775390625,
      "learning_rate": 9.83309674366585e-06,
      "loss": 1.8457,
      "step": 2971
    },
    {
      "epoch": 1.1506000774293457,
      "grad_norm": 31.65999984741211,
      "learning_rate": 9.832666580634062e-06,
      "loss": 2.1101,
      "step": 2972
    },
    {
      "epoch": 1.150987224157956,
      "grad_norm": 35.955039978027344,
      "learning_rate": 9.832236417602272e-06,
      "loss": 1.5473,
      "step": 2973
    },
    {
      "epoch": 1.151374370886566,
      "grad_norm": 7.081954002380371,
      "learning_rate": 9.831806254570483e-06,
      "loss": 1.4192,
      "step": 2974
    },
    {
      "epoch": 1.151761517615176,
      "grad_norm": 17.49835205078125,
      "learning_rate": 9.831376091538693e-06,
      "loss": 1.2485,
      "step": 2975
    },
    {
      "epoch": 1.1521486643437864,
      "grad_norm": 28.85236930847168,
      "learning_rate": 9.830945928506906e-06,
      "loss": 1.2437,
      "step": 2976
    },
    {
      "epoch": 1.1525358110723964,
      "grad_norm": 20.845680236816406,
      "learning_rate": 9.830515765475116e-06,
      "loss": 2.4573,
      "step": 2977
    },
    {
      "epoch": 1.1529229578010065,
      "grad_norm": 18.903173446655273,
      "learning_rate": 9.830085602443327e-06,
      "loss": 2.115,
      "step": 2978
    },
    {
      "epoch": 1.1533101045296168,
      "grad_norm": 20.152050018310547,
      "learning_rate": 9.829655439411537e-06,
      "loss": 1.6075,
      "step": 2979
    },
    {
      "epoch": 1.1536972512582269,
      "grad_norm": 15.459424018859863,
      "learning_rate": 9.82922527637975e-06,
      "loss": 1.7338,
      "step": 2980
    },
    {
      "epoch": 1.154084397986837,
      "grad_norm": 25.91725730895996,
      "learning_rate": 9.82879511334796e-06,
      "loss": 2.3755,
      "step": 2981
    },
    {
      "epoch": 1.1544715447154472,
      "grad_norm": 26.847471237182617,
      "learning_rate": 9.828364950316171e-06,
      "loss": 1.3795,
      "step": 2982
    },
    {
      "epoch": 1.1548586914440573,
      "grad_norm": 16.094446182250977,
      "learning_rate": 9.82793478728438e-06,
      "loss": 0.9474,
      "step": 2983
    },
    {
      "epoch": 1.1552458381726673,
      "grad_norm": 15.619918823242188,
      "learning_rate": 9.827504624252592e-06,
      "loss": 1.5938,
      "step": 2984
    },
    {
      "epoch": 1.1556329849012776,
      "grad_norm": 41.57929611206055,
      "learning_rate": 9.827074461220804e-06,
      "loss": 1.8007,
      "step": 2985
    },
    {
      "epoch": 1.1560201316298877,
      "grad_norm": 36.08284378051758,
      "learning_rate": 9.826644298189015e-06,
      "loss": 2.4414,
      "step": 2986
    },
    {
      "epoch": 1.1564072783584978,
      "grad_norm": 21.90114402770996,
      "learning_rate": 9.826214135157225e-06,
      "loss": 1.5616,
      "step": 2987
    },
    {
      "epoch": 1.156794425087108,
      "grad_norm": 15.733972549438477,
      "learning_rate": 9.825783972125436e-06,
      "loss": 1.4788,
      "step": 2988
    },
    {
      "epoch": 1.1571815718157181,
      "grad_norm": 9.461965560913086,
      "learning_rate": 9.825353809093648e-06,
      "loss": 1.4567,
      "step": 2989
    },
    {
      "epoch": 1.1575687185443284,
      "grad_norm": 19.564434051513672,
      "learning_rate": 9.824923646061857e-06,
      "loss": 1.5664,
      "step": 2990
    },
    {
      "epoch": 1.1579558652729385,
      "grad_norm": 26.272539138793945,
      "learning_rate": 9.824493483030069e-06,
      "loss": 2.2861,
      "step": 2991
    },
    {
      "epoch": 1.1583430120015485,
      "grad_norm": 11.656249046325684,
      "learning_rate": 9.82406331999828e-06,
      "loss": 0.8916,
      "step": 2992
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 22.562274932861328,
      "learning_rate": 9.823633156966492e-06,
      "loss": 2.3353,
      "step": 2993
    },
    {
      "epoch": 1.159117305458769,
      "grad_norm": 13.80146598815918,
      "learning_rate": 9.823202993934701e-06,
      "loss": 0.9675,
      "step": 2994
    },
    {
      "epoch": 1.159504452187379,
      "grad_norm": 14.673208236694336,
      "learning_rate": 9.822772830902914e-06,
      "loss": 1.2516,
      "step": 2995
    },
    {
      "epoch": 1.1598915989159893,
      "grad_norm": 16.515405654907227,
      "learning_rate": 9.822342667871124e-06,
      "loss": 1.4274,
      "step": 2996
    },
    {
      "epoch": 1.1602787456445993,
      "grad_norm": 17.443923950195312,
      "learning_rate": 9.821912504839336e-06,
      "loss": 1.6368,
      "step": 2997
    },
    {
      "epoch": 1.1606658923732094,
      "grad_norm": 17.66025733947754,
      "learning_rate": 9.821482341807545e-06,
      "loss": 1.5627,
      "step": 2998
    },
    {
      "epoch": 1.1610530391018197,
      "grad_norm": 18.860763549804688,
      "learning_rate": 9.821052178775757e-06,
      "loss": 1.4924,
      "step": 2999
    },
    {
      "epoch": 1.1614401858304297,
      "grad_norm": 13.486059188842773,
      "learning_rate": 9.820622015743968e-06,
      "loss": 1.456,
      "step": 3000
    },
    {
      "epoch": 1.1618273325590398,
      "grad_norm": 22.92889404296875,
      "learning_rate": 9.82019185271218e-06,
      "loss": 1.445,
      "step": 3001
    },
    {
      "epoch": 1.16221447928765,
      "grad_norm": 16.136220932006836,
      "learning_rate": 9.81976168968039e-06,
      "loss": 1.5168,
      "step": 3002
    },
    {
      "epoch": 1.1626016260162602,
      "grad_norm": 20.222835540771484,
      "learning_rate": 9.8193315266486e-06,
      "loss": 2.1027,
      "step": 3003
    },
    {
      "epoch": 1.1629887727448702,
      "grad_norm": 15.182382583618164,
      "learning_rate": 9.818901363616812e-06,
      "loss": 1.3647,
      "step": 3004
    },
    {
      "epoch": 1.1633759194734805,
      "grad_norm": 11.764989852905273,
      "learning_rate": 9.818471200585022e-06,
      "loss": 0.8838,
      "step": 3005
    },
    {
      "epoch": 1.1637630662020906,
      "grad_norm": 13.789085388183594,
      "learning_rate": 9.818041037553233e-06,
      "loss": 1.0464,
      "step": 3006
    },
    {
      "epoch": 1.1641502129307006,
      "grad_norm": 12.53756046295166,
      "learning_rate": 9.817610874521445e-06,
      "loss": 1.1329,
      "step": 3007
    },
    {
      "epoch": 1.164537359659311,
      "grad_norm": 15.539376258850098,
      "learning_rate": 9.817180711489656e-06,
      "loss": 1.7286,
      "step": 3008
    },
    {
      "epoch": 1.164924506387921,
      "grad_norm": 17.020790100097656,
      "learning_rate": 9.816750548457866e-06,
      "loss": 1.7717,
      "step": 3009
    },
    {
      "epoch": 1.165311653116531,
      "grad_norm": 12.021709442138672,
      "learning_rate": 9.816320385426077e-06,
      "loss": 1.5294,
      "step": 3010
    },
    {
      "epoch": 1.1656987998451414,
      "grad_norm": 16.95293426513672,
      "learning_rate": 9.815890222394289e-06,
      "loss": 1.2216,
      "step": 3011
    },
    {
      "epoch": 1.1660859465737514,
      "grad_norm": 26.857521057128906,
      "learning_rate": 9.8154600593625e-06,
      "loss": 1.9899,
      "step": 3012
    },
    {
      "epoch": 1.1664730933023617,
      "grad_norm": 21.654739379882812,
      "learning_rate": 9.81502989633071e-06,
      "loss": 1.736,
      "step": 3013
    },
    {
      "epoch": 1.1668602400309718,
      "grad_norm": 19.27728271484375,
      "learning_rate": 9.814599733298921e-06,
      "loss": 1.5669,
      "step": 3014
    },
    {
      "epoch": 1.1672473867595818,
      "grad_norm": 21.01061248779297,
      "learning_rate": 9.814169570267133e-06,
      "loss": 1.8742,
      "step": 3015
    },
    {
      "epoch": 1.167634533488192,
      "grad_norm": 18.747554779052734,
      "learning_rate": 9.813739407235344e-06,
      "loss": 1.6276,
      "step": 3016
    },
    {
      "epoch": 1.1680216802168022,
      "grad_norm": 16.065994262695312,
      "learning_rate": 9.813309244203554e-06,
      "loss": 1.3208,
      "step": 3017
    },
    {
      "epoch": 1.1684088269454123,
      "grad_norm": 18.052812576293945,
      "learning_rate": 9.812879081171765e-06,
      "loss": 1.412,
      "step": 3018
    },
    {
      "epoch": 1.1687959736740225,
      "grad_norm": 20.589031219482422,
      "learning_rate": 9.812448918139976e-06,
      "loss": 2.1381,
      "step": 3019
    },
    {
      "epoch": 1.1691831204026326,
      "grad_norm": 18.82927703857422,
      "learning_rate": 9.812018755108186e-06,
      "loss": 2.015,
      "step": 3020
    },
    {
      "epoch": 1.1695702671312427,
      "grad_norm": 14.648150444030762,
      "learning_rate": 9.811588592076398e-06,
      "loss": 1.5797,
      "step": 3021
    },
    {
      "epoch": 1.169957413859853,
      "grad_norm": 16.551654815673828,
      "learning_rate": 9.811158429044609e-06,
      "loss": 1.4224,
      "step": 3022
    },
    {
      "epoch": 1.170344560588463,
      "grad_norm": 26.70111083984375,
      "learning_rate": 9.81072826601282e-06,
      "loss": 1.5828,
      "step": 3023
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 17.38921356201172,
      "learning_rate": 9.81029810298103e-06,
      "loss": 1.48,
      "step": 3024
    },
    {
      "epoch": 1.1711188540456834,
      "grad_norm": 21.90741539001465,
      "learning_rate": 9.809867939949242e-06,
      "loss": 1.6149,
      "step": 3025
    },
    {
      "epoch": 1.1715060007742935,
      "grad_norm": 16.900083541870117,
      "learning_rate": 9.809437776917451e-06,
      "loss": 1.2615,
      "step": 3026
    },
    {
      "epoch": 1.1718931475029035,
      "grad_norm": 17.020925521850586,
      "learning_rate": 9.809007613885664e-06,
      "loss": 2.5166,
      "step": 3027
    },
    {
      "epoch": 1.1722802942315138,
      "grad_norm": 12.226788520812988,
      "learning_rate": 9.808577450853874e-06,
      "loss": 0.4676,
      "step": 3028
    },
    {
      "epoch": 1.1726674409601239,
      "grad_norm": 17.987957000732422,
      "learning_rate": 9.808147287822086e-06,
      "loss": 1.5398,
      "step": 3029
    },
    {
      "epoch": 1.173054587688734,
      "grad_norm": 17.478696823120117,
      "learning_rate": 9.807717124790295e-06,
      "loss": 1.6298,
      "step": 3030
    },
    {
      "epoch": 1.1734417344173442,
      "grad_norm": 14.859460830688477,
      "learning_rate": 9.807286961758508e-06,
      "loss": 1.4204,
      "step": 3031
    },
    {
      "epoch": 1.1738288811459543,
      "grad_norm": 18.234813690185547,
      "learning_rate": 9.806856798726718e-06,
      "loss": 1.5239,
      "step": 3032
    },
    {
      "epoch": 1.1742160278745644,
      "grad_norm": 15.822694778442383,
      "learning_rate": 9.80642663569493e-06,
      "loss": 1.4184,
      "step": 3033
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 15.34094524383545,
      "learning_rate": 9.80599647266314e-06,
      "loss": 1.4459,
      "step": 3034
    },
    {
      "epoch": 1.1749903213317847,
      "grad_norm": 28.835205078125,
      "learning_rate": 9.80556630963135e-06,
      "loss": 2.0387,
      "step": 3035
    },
    {
      "epoch": 1.175377468060395,
      "grad_norm": 25.378496170043945,
      "learning_rate": 9.805136146599562e-06,
      "loss": 1.5501,
      "step": 3036
    },
    {
      "epoch": 1.175764614789005,
      "grad_norm": 15.982877731323242,
      "learning_rate": 9.804705983567774e-06,
      "loss": 0.8811,
      "step": 3037
    },
    {
      "epoch": 1.1761517615176151,
      "grad_norm": 15.605831146240234,
      "learning_rate": 9.804275820535985e-06,
      "loss": 1.3322,
      "step": 3038
    },
    {
      "epoch": 1.1765389082462252,
      "grad_norm": 18.751754760742188,
      "learning_rate": 9.803845657504195e-06,
      "loss": 1.6367,
      "step": 3039
    },
    {
      "epoch": 1.1769260549748355,
      "grad_norm": 22.39113998413086,
      "learning_rate": 9.803415494472406e-06,
      "loss": 1.3702,
      "step": 3040
    },
    {
      "epoch": 1.1773132017034456,
      "grad_norm": 13.966052055358887,
      "learning_rate": 9.802985331440616e-06,
      "loss": 1.3971,
      "step": 3041
    },
    {
      "epoch": 1.1777003484320558,
      "grad_norm": 15.1185302734375,
      "learning_rate": 9.802555168408829e-06,
      "loss": 1.6062,
      "step": 3042
    },
    {
      "epoch": 1.178087495160666,
      "grad_norm": 15.840719223022461,
      "learning_rate": 9.802125005377039e-06,
      "loss": 1.2415,
      "step": 3043
    },
    {
      "epoch": 1.178474641889276,
      "grad_norm": 19.78141975402832,
      "learning_rate": 9.80169484234525e-06,
      "loss": 1.5468,
      "step": 3044
    },
    {
      "epoch": 1.1788617886178863,
      "grad_norm": 20.989940643310547,
      "learning_rate": 9.80126467931346e-06,
      "loss": 1.15,
      "step": 3045
    },
    {
      "epoch": 1.1792489353464963,
      "grad_norm": 10.915839195251465,
      "learning_rate": 9.800834516281673e-06,
      "loss": 1.0367,
      "step": 3046
    },
    {
      "epoch": 1.1796360820751064,
      "grad_norm": 17.73381805419922,
      "learning_rate": 9.800404353249883e-06,
      "loss": 1.0686,
      "step": 3047
    },
    {
      "epoch": 1.1800232288037167,
      "grad_norm": 15.845843315124512,
      "learning_rate": 9.799974190218094e-06,
      "loss": 1.0286,
      "step": 3048
    },
    {
      "epoch": 1.1804103755323267,
      "grad_norm": 17.48198890686035,
      "learning_rate": 9.799544027186304e-06,
      "loss": 1.5204,
      "step": 3049
    },
    {
      "epoch": 1.1807975222609368,
      "grad_norm": 15.380476951599121,
      "learning_rate": 9.799113864154515e-06,
      "loss": 1.6431,
      "step": 3050
    },
    {
      "epoch": 1.181184668989547,
      "grad_norm": 16.611730575561523,
      "learning_rate": 9.798683701122727e-06,
      "loss": 1.0813,
      "step": 3051
    },
    {
      "epoch": 1.1815718157181572,
      "grad_norm": 21.591283798217773,
      "learning_rate": 9.798253538090938e-06,
      "loss": 1.7625,
      "step": 3052
    },
    {
      "epoch": 1.1819589624467672,
      "grad_norm": 8.745270729064941,
      "learning_rate": 9.797823375059148e-06,
      "loss": 0.9132,
      "step": 3053
    },
    {
      "epoch": 1.1823461091753775,
      "grad_norm": 22.69384765625,
      "learning_rate": 9.797393212027359e-06,
      "loss": 1.6537,
      "step": 3054
    },
    {
      "epoch": 1.1827332559039876,
      "grad_norm": 13.326053619384766,
      "learning_rate": 9.79696304899557e-06,
      "loss": 1.1552,
      "step": 3055
    },
    {
      "epoch": 1.1831204026325977,
      "grad_norm": 15.162103652954102,
      "learning_rate": 9.79653288596378e-06,
      "loss": 1.4753,
      "step": 3056
    },
    {
      "epoch": 1.183507549361208,
      "grad_norm": 14.489585876464844,
      "learning_rate": 9.796102722931992e-06,
      "loss": 1.3867,
      "step": 3057
    },
    {
      "epoch": 1.183894696089818,
      "grad_norm": 17.16240692138672,
      "learning_rate": 9.795672559900203e-06,
      "loss": 1.2336,
      "step": 3058
    },
    {
      "epoch": 1.1842818428184283,
      "grad_norm": 11.332317352294922,
      "learning_rate": 9.795242396868414e-06,
      "loss": 1.2895,
      "step": 3059
    },
    {
      "epoch": 1.1846689895470384,
      "grad_norm": 20.84572982788086,
      "learning_rate": 9.794812233836624e-06,
      "loss": 1.4105,
      "step": 3060
    },
    {
      "epoch": 1.1850561362756484,
      "grad_norm": 22.489238739013672,
      "learning_rate": 9.794382070804836e-06,
      "loss": 1.6686,
      "step": 3061
    },
    {
      "epoch": 1.1854432830042585,
      "grad_norm": 14.409152030944824,
      "learning_rate": 9.793951907773047e-06,
      "loss": 1.3506,
      "step": 3062
    },
    {
      "epoch": 1.1858304297328688,
      "grad_norm": 42.8580322265625,
      "learning_rate": 9.793521744741258e-06,
      "loss": 2.4383,
      "step": 3063
    },
    {
      "epoch": 1.1862175764614789,
      "grad_norm": 18.06700325012207,
      "learning_rate": 9.793091581709468e-06,
      "loss": 1.5512,
      "step": 3064
    },
    {
      "epoch": 1.1866047231900891,
      "grad_norm": 28.48997688293457,
      "learning_rate": 9.79266141867768e-06,
      "loss": 2.8746,
      "step": 3065
    },
    {
      "epoch": 1.1869918699186992,
      "grad_norm": 18.089374542236328,
      "learning_rate": 9.792231255645891e-06,
      "loss": 2.0267,
      "step": 3066
    },
    {
      "epoch": 1.1873790166473093,
      "grad_norm": 14.755876541137695,
      "learning_rate": 9.791801092614102e-06,
      "loss": 2.8311,
      "step": 3067
    },
    {
      "epoch": 1.1877661633759196,
      "grad_norm": 29.355398178100586,
      "learning_rate": 9.791370929582312e-06,
      "loss": 1.5332,
      "step": 3068
    },
    {
      "epoch": 1.1881533101045296,
      "grad_norm": 19.715835571289062,
      "learning_rate": 9.790940766550524e-06,
      "loss": 1.6341,
      "step": 3069
    },
    {
      "epoch": 1.1885404568331397,
      "grad_norm": 15.60018539428711,
      "learning_rate": 9.790510603518735e-06,
      "loss": 1.0439,
      "step": 3070
    },
    {
      "epoch": 1.18892760356175,
      "grad_norm": 17.047536849975586,
      "learning_rate": 9.790080440486945e-06,
      "loss": 1.4192,
      "step": 3071
    },
    {
      "epoch": 1.18931475029036,
      "grad_norm": 15.645800590515137,
      "learning_rate": 9.789650277455156e-06,
      "loss": 1.1543,
      "step": 3072
    },
    {
      "epoch": 1.1897018970189701,
      "grad_norm": 9.197578430175781,
      "learning_rate": 9.789220114423368e-06,
      "loss": 0.6776,
      "step": 3073
    },
    {
      "epoch": 1.1900890437475804,
      "grad_norm": 19.637731552124023,
      "learning_rate": 9.788789951391579e-06,
      "loss": 1.9816,
      "step": 3074
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 10.762657165527344,
      "learning_rate": 9.788359788359789e-06,
      "loss": 0.7388,
      "step": 3075
    },
    {
      "epoch": 1.1908633372048005,
      "grad_norm": 14.620525360107422,
      "learning_rate": 9.787929625328e-06,
      "loss": 1.5044,
      "step": 3076
    },
    {
      "epoch": 1.1912504839334108,
      "grad_norm": 17.763824462890625,
      "learning_rate": 9.787499462296211e-06,
      "loss": 1.5536,
      "step": 3077
    },
    {
      "epoch": 1.1916376306620209,
      "grad_norm": 23.60650062561035,
      "learning_rate": 9.787069299264423e-06,
      "loss": 1.9365,
      "step": 3078
    },
    {
      "epoch": 1.192024777390631,
      "grad_norm": 15.560032844543457,
      "learning_rate": 9.786639136232633e-06,
      "loss": 1.2124,
      "step": 3079
    },
    {
      "epoch": 1.1924119241192412,
      "grad_norm": 25.57758331298828,
      "learning_rate": 9.786208973200844e-06,
      "loss": 2.3982,
      "step": 3080
    },
    {
      "epoch": 1.1927990708478513,
      "grad_norm": 21.021339416503906,
      "learning_rate": 9.785778810169055e-06,
      "loss": 3.1495,
      "step": 3081
    },
    {
      "epoch": 1.1931862175764616,
      "grad_norm": 30.362653732299805,
      "learning_rate": 9.785348647137267e-06,
      "loss": 1.8394,
      "step": 3082
    },
    {
      "epoch": 1.1935733643050717,
      "grad_norm": 18.890501022338867,
      "learning_rate": 9.784918484105477e-06,
      "loss": 1.9142,
      "step": 3083
    },
    {
      "epoch": 1.1939605110336817,
      "grad_norm": 14.699313163757324,
      "learning_rate": 9.784488321073688e-06,
      "loss": 1.1503,
      "step": 3084
    },
    {
      "epoch": 1.1943476577622918,
      "grad_norm": 22.52219009399414,
      "learning_rate": 9.7840581580419e-06,
      "loss": 1.2827,
      "step": 3085
    },
    {
      "epoch": 1.194734804490902,
      "grad_norm": 13.142180442810059,
      "learning_rate": 9.78362799501011e-06,
      "loss": 1.1669,
      "step": 3086
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 15.19536018371582,
      "learning_rate": 9.78319783197832e-06,
      "loss": 1.293,
      "step": 3087
    },
    {
      "epoch": 1.1955090979481224,
      "grad_norm": 13.536229133605957,
      "learning_rate": 9.782767668946532e-06,
      "loss": 1.1526,
      "step": 3088
    },
    {
      "epoch": 1.1958962446767325,
      "grad_norm": 26.121841430664062,
      "learning_rate": 9.782337505914743e-06,
      "loss": 1.1007,
      "step": 3089
    },
    {
      "epoch": 1.1962833914053426,
      "grad_norm": 14.07019329071045,
      "learning_rate": 9.781907342882953e-06,
      "loss": 1.7054,
      "step": 3090
    },
    {
      "epoch": 1.1966705381339529,
      "grad_norm": 15.080976486206055,
      "learning_rate": 9.781477179851165e-06,
      "loss": 1.3445,
      "step": 3091
    },
    {
      "epoch": 1.197057684862563,
      "grad_norm": 16.302383422851562,
      "learning_rate": 9.781047016819374e-06,
      "loss": 1.5789,
      "step": 3092
    },
    {
      "epoch": 1.197444831591173,
      "grad_norm": 20.442184448242188,
      "learning_rate": 9.780616853787587e-06,
      "loss": 1.6982,
      "step": 3093
    },
    {
      "epoch": 1.1978319783197833,
      "grad_norm": 25.04652976989746,
      "learning_rate": 9.780186690755797e-06,
      "loss": 1.4145,
      "step": 3094
    },
    {
      "epoch": 1.1982191250483933,
      "grad_norm": 13.898717880249023,
      "learning_rate": 9.779756527724009e-06,
      "loss": 0.8982,
      "step": 3095
    },
    {
      "epoch": 1.1986062717770034,
      "grad_norm": 16.84522247314453,
      "learning_rate": 9.779326364692218e-06,
      "loss": 1.8327,
      "step": 3096
    },
    {
      "epoch": 1.1989934185056137,
      "grad_norm": 25.888500213623047,
      "learning_rate": 9.778896201660431e-06,
      "loss": 1.4141,
      "step": 3097
    },
    {
      "epoch": 1.1993805652342238,
      "grad_norm": 16.591463088989258,
      "learning_rate": 9.778466038628641e-06,
      "loss": 1.1514,
      "step": 3098
    },
    {
      "epoch": 1.1997677119628338,
      "grad_norm": 13.106358528137207,
      "learning_rate": 9.778035875596852e-06,
      "loss": 1.1569,
      "step": 3099
    },
    {
      "epoch": 1.2001548586914441,
      "grad_norm": 13.718827247619629,
      "learning_rate": 9.777605712565062e-06,
      "loss": 1.3266,
      "step": 3100
    },
    {
      "epoch": 1.2005420054200542,
      "grad_norm": 15.039712905883789,
      "learning_rate": 9.777175549533274e-06,
      "loss": 1.1704,
      "step": 3101
    },
    {
      "epoch": 1.2009291521486642,
      "grad_norm": 16.75265121459961,
      "learning_rate": 9.776745386501485e-06,
      "loss": 1.131,
      "step": 3102
    },
    {
      "epoch": 1.2013162988772745,
      "grad_norm": 14.807233810424805,
      "learning_rate": 9.776315223469696e-06,
      "loss": 1.1885,
      "step": 3103
    },
    {
      "epoch": 1.2017034456058846,
      "grad_norm": 17.932580947875977,
      "learning_rate": 9.775885060437906e-06,
      "loss": 1.4412,
      "step": 3104
    },
    {
      "epoch": 1.202090592334495,
      "grad_norm": 13.495857238769531,
      "learning_rate": 9.775454897406118e-06,
      "loss": 1.5403,
      "step": 3105
    },
    {
      "epoch": 1.202477739063105,
      "grad_norm": 15.055131912231445,
      "learning_rate": 9.775024734374329e-06,
      "loss": 1.7669,
      "step": 3106
    },
    {
      "epoch": 1.202864885791715,
      "grad_norm": 17.160120010375977,
      "learning_rate": 9.774594571342539e-06,
      "loss": 2.3819,
      "step": 3107
    },
    {
      "epoch": 1.203252032520325,
      "grad_norm": 13.365689277648926,
      "learning_rate": 9.77416440831075e-06,
      "loss": 1.4016,
      "step": 3108
    },
    {
      "epoch": 1.2036391792489354,
      "grad_norm": 19.044315338134766,
      "learning_rate": 9.773734245278962e-06,
      "loss": 1.8014,
      "step": 3109
    },
    {
      "epoch": 1.2040263259775454,
      "grad_norm": 9.955920219421387,
      "learning_rate": 9.773304082247173e-06,
      "loss": 1.2552,
      "step": 3110
    },
    {
      "epoch": 1.2044134727061557,
      "grad_norm": 30.21571159362793,
      "learning_rate": 9.772873919215383e-06,
      "loss": 1.7888,
      "step": 3111
    },
    {
      "epoch": 1.2048006194347658,
      "grad_norm": 18.008140563964844,
      "learning_rate": 9.772443756183594e-06,
      "loss": 2.645,
      "step": 3112
    },
    {
      "epoch": 1.2051877661633759,
      "grad_norm": 20.486852645874023,
      "learning_rate": 9.772013593151806e-06,
      "loss": 1.7594,
      "step": 3113
    },
    {
      "epoch": 1.2055749128919862,
      "grad_norm": 64.62298583984375,
      "learning_rate": 9.771583430120017e-06,
      "loss": 1.0514,
      "step": 3114
    },
    {
      "epoch": 1.2059620596205962,
      "grad_norm": 13.12026596069336,
      "learning_rate": 9.771153267088227e-06,
      "loss": 0.8795,
      "step": 3115
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 14.375173568725586,
      "learning_rate": 9.770723104056438e-06,
      "loss": 1.394,
      "step": 3116
    },
    {
      "epoch": 1.2067363530778166,
      "grad_norm": 16.634183883666992,
      "learning_rate": 9.77029294102465e-06,
      "loss": 1.5412,
      "step": 3117
    },
    {
      "epoch": 1.2071234998064266,
      "grad_norm": 18.613420486450195,
      "learning_rate": 9.769862777992861e-06,
      "loss": 1.5349,
      "step": 3118
    },
    {
      "epoch": 1.2075106465350367,
      "grad_norm": 12.587372779846191,
      "learning_rate": 9.76943261496107e-06,
      "loss": 1.4336,
      "step": 3119
    },
    {
      "epoch": 1.207897793263647,
      "grad_norm": 20.389205932617188,
      "learning_rate": 9.769002451929282e-06,
      "loss": 1.4968,
      "step": 3120
    },
    {
      "epoch": 1.208284939992257,
      "grad_norm": 22.871414184570312,
      "learning_rate": 9.768572288897493e-06,
      "loss": 2.0417,
      "step": 3121
    },
    {
      "epoch": 1.2086720867208671,
      "grad_norm": 23.616647720336914,
      "learning_rate": 9.768142125865703e-06,
      "loss": 1.4864,
      "step": 3122
    },
    {
      "epoch": 1.2090592334494774,
      "grad_norm": 28.835098266601562,
      "learning_rate": 9.767711962833915e-06,
      "loss": 1.532,
      "step": 3123
    },
    {
      "epoch": 1.2094463801780875,
      "grad_norm": 15.325265884399414,
      "learning_rate": 9.767281799802126e-06,
      "loss": 1.4074,
      "step": 3124
    },
    {
      "epoch": 1.2098335269066975,
      "grad_norm": 19.85487174987793,
      "learning_rate": 9.766851636770337e-06,
      "loss": 2.9544,
      "step": 3125
    },
    {
      "epoch": 1.2102206736353078,
      "grad_norm": 17.612648010253906,
      "learning_rate": 9.766421473738547e-06,
      "loss": 1.2416,
      "step": 3126
    },
    {
      "epoch": 1.210607820363918,
      "grad_norm": 13.179670333862305,
      "learning_rate": 9.765991310706759e-06,
      "loss": 1.1451,
      "step": 3127
    },
    {
      "epoch": 1.2109949670925282,
      "grad_norm": 15.012333869934082,
      "learning_rate": 9.76556114767497e-06,
      "loss": 1.8342,
      "step": 3128
    },
    {
      "epoch": 1.2113821138211383,
      "grad_norm": 23.382232666015625,
      "learning_rate": 9.765130984643181e-06,
      "loss": 2.1935,
      "step": 3129
    },
    {
      "epoch": 1.2117692605497483,
      "grad_norm": 15.78349494934082,
      "learning_rate": 9.764700821611391e-06,
      "loss": 1.0928,
      "step": 3130
    },
    {
      "epoch": 1.2121564072783584,
      "grad_norm": 13.81524658203125,
      "learning_rate": 9.764270658579603e-06,
      "loss": 1.0702,
      "step": 3131
    },
    {
      "epoch": 1.2125435540069687,
      "grad_norm": 19.994224548339844,
      "learning_rate": 9.763840495547814e-06,
      "loss": 2.1858,
      "step": 3132
    },
    {
      "epoch": 1.2129307007355787,
      "grad_norm": 27.11276626586914,
      "learning_rate": 9.763410332516025e-06,
      "loss": 2.5132,
      "step": 3133
    },
    {
      "epoch": 1.213317847464189,
      "grad_norm": 13.535355567932129,
      "learning_rate": 9.762980169484235e-06,
      "loss": 1.5923,
      "step": 3134
    },
    {
      "epoch": 1.213704994192799,
      "grad_norm": 17.355627059936523,
      "learning_rate": 9.762550006452447e-06,
      "loss": 1.7406,
      "step": 3135
    },
    {
      "epoch": 1.2140921409214092,
      "grad_norm": 10.758639335632324,
      "learning_rate": 9.762119843420658e-06,
      "loss": 1.2799,
      "step": 3136
    },
    {
      "epoch": 1.2144792876500194,
      "grad_norm": 23.31247329711914,
      "learning_rate": 9.761689680388868e-06,
      "loss": 1.2103,
      "step": 3137
    },
    {
      "epoch": 1.2148664343786295,
      "grad_norm": 12.592398643493652,
      "learning_rate": 9.761259517357079e-06,
      "loss": 1.0454,
      "step": 3138
    },
    {
      "epoch": 1.2152535811072396,
      "grad_norm": 12.90467643737793,
      "learning_rate": 9.76082935432529e-06,
      "loss": 1.1648,
      "step": 3139
    },
    {
      "epoch": 1.2156407278358499,
      "grad_norm": 9.507275581359863,
      "learning_rate": 9.760399191293502e-06,
      "loss": 1.279,
      "step": 3140
    },
    {
      "epoch": 1.21602787456446,
      "grad_norm": 16.21649932861328,
      "learning_rate": 9.759969028261712e-06,
      "loss": 2.342,
      "step": 3141
    },
    {
      "epoch": 1.21641502129307,
      "grad_norm": 16.131473541259766,
      "learning_rate": 9.759538865229923e-06,
      "loss": 1.339,
      "step": 3142
    },
    {
      "epoch": 1.2168021680216803,
      "grad_norm": 21.813587188720703,
      "learning_rate": 9.759108702198133e-06,
      "loss": 1.7441,
      "step": 3143
    },
    {
      "epoch": 1.2171893147502904,
      "grad_norm": 10.322762489318848,
      "learning_rate": 9.758678539166346e-06,
      "loss": 0.4111,
      "step": 3144
    },
    {
      "epoch": 1.2175764614789004,
      "grad_norm": 13.516968727111816,
      "learning_rate": 9.758248376134556e-06,
      "loss": 0.9542,
      "step": 3145
    },
    {
      "epoch": 1.2179636082075107,
      "grad_norm": 13.72641658782959,
      "learning_rate": 9.757818213102767e-06,
      "loss": 1.1,
      "step": 3146
    },
    {
      "epoch": 1.2183507549361208,
      "grad_norm": 24.433515548706055,
      "learning_rate": 9.757388050070977e-06,
      "loss": 1.3665,
      "step": 3147
    },
    {
      "epoch": 1.2187379016647308,
      "grad_norm": 11.27368450164795,
      "learning_rate": 9.75695788703919e-06,
      "loss": 1.4145,
      "step": 3148
    },
    {
      "epoch": 1.2191250483933411,
      "grad_norm": 17.937541961669922,
      "learning_rate": 9.7565277240074e-06,
      "loss": 1.225,
      "step": 3149
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 22.43718147277832,
      "learning_rate": 9.756097560975611e-06,
      "loss": 1.7906,
      "step": 3150
    },
    {
      "epoch": 1.2198993418505615,
      "grad_norm": 17.17709732055664,
      "learning_rate": 9.75566739794382e-06,
      "loss": 1.3701,
      "step": 3151
    },
    {
      "epoch": 1.2202864885791715,
      "grad_norm": 22.362865447998047,
      "learning_rate": 9.755237234912032e-06,
      "loss": 1.0098,
      "step": 3152
    },
    {
      "epoch": 1.2206736353077816,
      "grad_norm": 33.693851470947266,
      "learning_rate": 9.754807071880244e-06,
      "loss": 1.5259,
      "step": 3153
    },
    {
      "epoch": 1.2210607820363917,
      "grad_norm": 10.855972290039062,
      "learning_rate": 9.754376908848455e-06,
      "loss": 1.2092,
      "step": 3154
    },
    {
      "epoch": 1.221447928765002,
      "grad_norm": 21.034648895263672,
      "learning_rate": 9.753946745816665e-06,
      "loss": 1.6799,
      "step": 3155
    },
    {
      "epoch": 1.221835075493612,
      "grad_norm": 14.020866394042969,
      "learning_rate": 9.753516582784876e-06,
      "loss": 1.2996,
      "step": 3156
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 32.28239440917969,
      "learning_rate": 9.753086419753087e-06,
      "loss": 2.352,
      "step": 3157
    },
    {
      "epoch": 1.2226093689508324,
      "grad_norm": 16.011947631835938,
      "learning_rate": 9.752656256721297e-06,
      "loss": 1.7998,
      "step": 3158
    },
    {
      "epoch": 1.2229965156794425,
      "grad_norm": 17.443132400512695,
      "learning_rate": 9.752226093689509e-06,
      "loss": 1.0641,
      "step": 3159
    },
    {
      "epoch": 1.2233836624080527,
      "grad_norm": 19.176340103149414,
      "learning_rate": 9.75179593065772e-06,
      "loss": 2.1969,
      "step": 3160
    },
    {
      "epoch": 1.2237708091366628,
      "grad_norm": 18.671064376831055,
      "learning_rate": 9.751365767625931e-06,
      "loss": 1.6104,
      "step": 3161
    },
    {
      "epoch": 1.2241579558652729,
      "grad_norm": 9.247101783752441,
      "learning_rate": 9.750935604594141e-06,
      "loss": 0.6934,
      "step": 3162
    },
    {
      "epoch": 1.2245451025938832,
      "grad_norm": 15.999277114868164,
      "learning_rate": 9.750505441562354e-06,
      "loss": 1.4947,
      "step": 3163
    },
    {
      "epoch": 1.2249322493224932,
      "grad_norm": 18.861522674560547,
      "learning_rate": 9.750075278530564e-06,
      "loss": 1.8541,
      "step": 3164
    },
    {
      "epoch": 1.2253193960511033,
      "grad_norm": 38.95787811279297,
      "learning_rate": 9.749645115498775e-06,
      "loss": 2.2634,
      "step": 3165
    },
    {
      "epoch": 1.2257065427797136,
      "grad_norm": 17.901823043823242,
      "learning_rate": 9.749214952466985e-06,
      "loss": 2.1485,
      "step": 3166
    },
    {
      "epoch": 1.2260936895083236,
      "grad_norm": 12.456291198730469,
      "learning_rate": 9.748784789435197e-06,
      "loss": 0.8904,
      "step": 3167
    },
    {
      "epoch": 1.2264808362369337,
      "grad_norm": 14.512628555297852,
      "learning_rate": 9.748354626403408e-06,
      "loss": 1.9656,
      "step": 3168
    },
    {
      "epoch": 1.226867982965544,
      "grad_norm": 12.287969589233398,
      "learning_rate": 9.74792446337162e-06,
      "loss": 1.0424,
      "step": 3169
    },
    {
      "epoch": 1.227255129694154,
      "grad_norm": 9.78237247467041,
      "learning_rate": 9.747494300339829e-06,
      "loss": 1.3905,
      "step": 3170
    },
    {
      "epoch": 1.2276422764227641,
      "grad_norm": 14.63348388671875,
      "learning_rate": 9.74706413730804e-06,
      "loss": 1.0032,
      "step": 3171
    },
    {
      "epoch": 1.2280294231513744,
      "grad_norm": 14.406023979187012,
      "learning_rate": 9.746633974276252e-06,
      "loss": 1.7232,
      "step": 3172
    },
    {
      "epoch": 1.2284165698799845,
      "grad_norm": 7.43907356262207,
      "learning_rate": 9.746203811244462e-06,
      "loss": 0.9673,
      "step": 3173
    },
    {
      "epoch": 1.2288037166085948,
      "grad_norm": 19.971786499023438,
      "learning_rate": 9.745773648212673e-06,
      "loss": 2.1597,
      "step": 3174
    },
    {
      "epoch": 1.2291908633372048,
      "grad_norm": 15.919750213623047,
      "learning_rate": 9.745343485180885e-06,
      "loss": 1.2564,
      "step": 3175
    },
    {
      "epoch": 1.229578010065815,
      "grad_norm": 11.826698303222656,
      "learning_rate": 9.744913322149096e-06,
      "loss": 1.054,
      "step": 3176
    },
    {
      "epoch": 1.229965156794425,
      "grad_norm": 20.63727378845215,
      "learning_rate": 9.744483159117306e-06,
      "loss": 1.687,
      "step": 3177
    },
    {
      "epoch": 1.2303523035230353,
      "grad_norm": 28.185518264770508,
      "learning_rate": 9.744052996085517e-06,
      "loss": 1.2939,
      "step": 3178
    },
    {
      "epoch": 1.2307394502516453,
      "grad_norm": 18.30876350402832,
      "learning_rate": 9.743622833053728e-06,
      "loss": 1.7053,
      "step": 3179
    },
    {
      "epoch": 1.2311265969802556,
      "grad_norm": 33.37335205078125,
      "learning_rate": 9.74319267002194e-06,
      "loss": 1.5716,
      "step": 3180
    },
    {
      "epoch": 1.2315137437088657,
      "grad_norm": 19.982162475585938,
      "learning_rate": 9.74276250699015e-06,
      "loss": 1.6535,
      "step": 3181
    },
    {
      "epoch": 1.2319008904374757,
      "grad_norm": 8.646696090698242,
      "learning_rate": 9.742332343958361e-06,
      "loss": 0.6393,
      "step": 3182
    },
    {
      "epoch": 1.232288037166086,
      "grad_norm": 21.951969146728516,
      "learning_rate": 9.741902180926572e-06,
      "loss": 3.5211,
      "step": 3183
    },
    {
      "epoch": 1.232675183894696,
      "grad_norm": 9.936111450195312,
      "learning_rate": 9.741472017894784e-06,
      "loss": 1.2912,
      "step": 3184
    },
    {
      "epoch": 1.2330623306233062,
      "grad_norm": 17.73468589782715,
      "learning_rate": 9.741041854862994e-06,
      "loss": 0.7507,
      "step": 3185
    },
    {
      "epoch": 1.2334494773519165,
      "grad_norm": 33.8300895690918,
      "learning_rate": 9.740611691831205e-06,
      "loss": 1.083,
      "step": 3186
    },
    {
      "epoch": 1.2338366240805265,
      "grad_norm": 33.93206787109375,
      "learning_rate": 9.740181528799416e-06,
      "loss": 2.5306,
      "step": 3187
    },
    {
      "epoch": 1.2342237708091366,
      "grad_norm": 37.65287780761719,
      "learning_rate": 9.739751365767626e-06,
      "loss": 1.921,
      "step": 3188
    },
    {
      "epoch": 1.2346109175377469,
      "grad_norm": 18.55474090576172,
      "learning_rate": 9.739321202735838e-06,
      "loss": 1.6562,
      "step": 3189
    },
    {
      "epoch": 1.234998064266357,
      "grad_norm": 15.199946403503418,
      "learning_rate": 9.738891039704049e-06,
      "loss": 1.636,
      "step": 3190
    },
    {
      "epoch": 1.235385210994967,
      "grad_norm": 9.118399620056152,
      "learning_rate": 9.73846087667226e-06,
      "loss": 1.0603,
      "step": 3191
    },
    {
      "epoch": 1.2357723577235773,
      "grad_norm": 15.418420791625977,
      "learning_rate": 9.73803071364047e-06,
      "loss": 1.4044,
      "step": 3192
    },
    {
      "epoch": 1.2361595044521874,
      "grad_norm": 22.149795532226562,
      "learning_rate": 9.737600550608682e-06,
      "loss": 1.6233,
      "step": 3193
    },
    {
      "epoch": 1.2365466511807974,
      "grad_norm": 9.965652465820312,
      "learning_rate": 9.737170387576891e-06,
      "loss": 0.8635,
      "step": 3194
    },
    {
      "epoch": 1.2369337979094077,
      "grad_norm": 12.179009437561035,
      "learning_rate": 9.736740224545104e-06,
      "loss": 1.4446,
      "step": 3195
    },
    {
      "epoch": 1.2373209446380178,
      "grad_norm": 29.49188232421875,
      "learning_rate": 9.736310061513314e-06,
      "loss": 3.0161,
      "step": 3196
    },
    {
      "epoch": 1.237708091366628,
      "grad_norm": 41.29104995727539,
      "learning_rate": 9.735879898481525e-06,
      "loss": 2.6921,
      "step": 3197
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 11.022710800170898,
      "learning_rate": 9.735449735449735e-06,
      "loss": 1.074,
      "step": 3198
    },
    {
      "epoch": 1.2384823848238482,
      "grad_norm": 13.800790786743164,
      "learning_rate": 9.735019572417948e-06,
      "loss": 0.9662,
      "step": 3199
    },
    {
      "epoch": 1.2388695315524583,
      "grad_norm": 37.71929931640625,
      "learning_rate": 9.734589409386158e-06,
      "loss": 1.7464,
      "step": 3200
    },
    {
      "epoch": 1.2392566782810686,
      "grad_norm": 16.926761627197266,
      "learning_rate": 9.73415924635437e-06,
      "loss": 1.3916,
      "step": 3201
    },
    {
      "epoch": 1.2396438250096786,
      "grad_norm": 15.729730606079102,
      "learning_rate": 9.733729083322581e-06,
      "loss": 1.6168,
      "step": 3202
    },
    {
      "epoch": 1.240030971738289,
      "grad_norm": 18.318832397460938,
      "learning_rate": 9.73329892029079e-06,
      "loss": 1.5338,
      "step": 3203
    },
    {
      "epoch": 1.240418118466899,
      "grad_norm": 31.654428482055664,
      "learning_rate": 9.732868757259002e-06,
      "loss": 1.7008,
      "step": 3204
    },
    {
      "epoch": 1.240805265195509,
      "grad_norm": 26.801971435546875,
      "learning_rate": 9.732438594227213e-06,
      "loss": 1.6171,
      "step": 3205
    },
    {
      "epoch": 1.2411924119241193,
      "grad_norm": 15.905384063720703,
      "learning_rate": 9.732008431195425e-06,
      "loss": 1.0967,
      "step": 3206
    },
    {
      "epoch": 1.2415795586527294,
      "grad_norm": 46.61048126220703,
      "learning_rate": 9.731578268163635e-06,
      "loss": 2.5442,
      "step": 3207
    },
    {
      "epoch": 1.2419667053813395,
      "grad_norm": 13.990106582641602,
      "learning_rate": 9.731148105131846e-06,
      "loss": 1.4651,
      "step": 3208
    },
    {
      "epoch": 1.2423538521099498,
      "grad_norm": 20.290258407592773,
      "learning_rate": 9.730717942100056e-06,
      "loss": 2.2636,
      "step": 3209
    },
    {
      "epoch": 1.2427409988385598,
      "grad_norm": 14.235318183898926,
      "learning_rate": 9.730287779068269e-06,
      "loss": 1.4295,
      "step": 3210
    },
    {
      "epoch": 1.2431281455671699,
      "grad_norm": 22.32913589477539,
      "learning_rate": 9.729857616036479e-06,
      "loss": 1.0644,
      "step": 3211
    },
    {
      "epoch": 1.2435152922957802,
      "grad_norm": 21.833669662475586,
      "learning_rate": 9.72942745300469e-06,
      "loss": 1.3755,
      "step": 3212
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 14.035148620605469,
      "learning_rate": 9.7289972899729e-06,
      "loss": 1.8859,
      "step": 3213
    },
    {
      "epoch": 1.2442895857530003,
      "grad_norm": 18.820079803466797,
      "learning_rate": 9.728567126941113e-06,
      "loss": 2.1499,
      "step": 3214
    },
    {
      "epoch": 1.2446767324816106,
      "grad_norm": 25.59492301940918,
      "learning_rate": 9.728136963909322e-06,
      "loss": 1.5084,
      "step": 3215
    },
    {
      "epoch": 1.2450638792102207,
      "grad_norm": 13.886759757995605,
      "learning_rate": 9.727706800877534e-06,
      "loss": 1.5441,
      "step": 3216
    },
    {
      "epoch": 1.2454510259388307,
      "grad_norm": 15.113125801086426,
      "learning_rate": 9.727276637845744e-06,
      "loss": 1.5593,
      "step": 3217
    },
    {
      "epoch": 1.245838172667441,
      "grad_norm": 12.288847923278809,
      "learning_rate": 9.726846474813955e-06,
      "loss": 0.9764,
      "step": 3218
    },
    {
      "epoch": 1.246225319396051,
      "grad_norm": 11.05102252960205,
      "learning_rate": 9.726416311782166e-06,
      "loss": 0.7221,
      "step": 3219
    },
    {
      "epoch": 1.2466124661246614,
      "grad_norm": 20.791175842285156,
      "learning_rate": 9.725986148750378e-06,
      "loss": 2.447,
      "step": 3220
    },
    {
      "epoch": 1.2469996128532714,
      "grad_norm": 11.105088233947754,
      "learning_rate": 9.725555985718588e-06,
      "loss": 1.4407,
      "step": 3221
    },
    {
      "epoch": 1.2473867595818815,
      "grad_norm": 24.097166061401367,
      "learning_rate": 9.725125822686799e-06,
      "loss": 1.8421,
      "step": 3222
    },
    {
      "epoch": 1.2477739063104916,
      "grad_norm": 9.503959655761719,
      "learning_rate": 9.72469565965501e-06,
      "loss": 0.7583,
      "step": 3223
    },
    {
      "epoch": 1.2481610530391019,
      "grad_norm": 20.85822296142578,
      "learning_rate": 9.72426549662322e-06,
      "loss": 1.6602,
      "step": 3224
    },
    {
      "epoch": 1.248548199767712,
      "grad_norm": 18.58648681640625,
      "learning_rate": 9.723835333591432e-06,
      "loss": 2.0328,
      "step": 3225
    },
    {
      "epoch": 1.2489353464963222,
      "grad_norm": 14.822598457336426,
      "learning_rate": 9.723405170559643e-06,
      "loss": 1.6914,
      "step": 3226
    },
    {
      "epoch": 1.2493224932249323,
      "grad_norm": 21.907865524291992,
      "learning_rate": 9.722975007527854e-06,
      "loss": 1.6368,
      "step": 3227
    },
    {
      "epoch": 1.2497096399535423,
      "grad_norm": 32.83900451660156,
      "learning_rate": 9.722544844496064e-06,
      "loss": 2.0329,
      "step": 3228
    },
    {
      "epoch": 1.2500967866821524,
      "grad_norm": 25.500627517700195,
      "learning_rate": 9.722114681464276e-06,
      "loss": 2.4779,
      "step": 3229
    },
    {
      "epoch": 1.2504839334107627,
      "grad_norm": 21.167142868041992,
      "learning_rate": 9.721684518432487e-06,
      "loss": 2.3112,
      "step": 3230
    },
    {
      "epoch": 1.2508710801393728,
      "grad_norm": 15.050511360168457,
      "learning_rate": 9.721254355400698e-06,
      "loss": 1.507,
      "step": 3231
    },
    {
      "epoch": 1.251258226867983,
      "grad_norm": 14.130298614501953,
      "learning_rate": 9.720824192368908e-06,
      "loss": 1.4606,
      "step": 3232
    },
    {
      "epoch": 1.2516453735965931,
      "grad_norm": 21.182647705078125,
      "learning_rate": 9.72039402933712e-06,
      "loss": 0.9019,
      "step": 3233
    },
    {
      "epoch": 1.2520325203252032,
      "grad_norm": 10.922479629516602,
      "learning_rate": 9.719963866305331e-06,
      "loss": 0.9021,
      "step": 3234
    },
    {
      "epoch": 1.2524196670538135,
      "grad_norm": 14.72940731048584,
      "learning_rate": 9.719533703273542e-06,
      "loss": 1.6678,
      "step": 3235
    },
    {
      "epoch": 1.2528068137824235,
      "grad_norm": 26.77144432067871,
      "learning_rate": 9.719103540241752e-06,
      "loss": 1.5726,
      "step": 3236
    },
    {
      "epoch": 1.2531939605110336,
      "grad_norm": 16.161155700683594,
      "learning_rate": 9.718673377209963e-06,
      "loss": 1.3943,
      "step": 3237
    },
    {
      "epoch": 1.2535811072396439,
      "grad_norm": 14.914190292358398,
      "learning_rate": 9.718243214178175e-06,
      "loss": 1.5508,
      "step": 3238
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 23.380462646484375,
      "learning_rate": 9.717813051146385e-06,
      "loss": 1.0377,
      "step": 3239
    },
    {
      "epoch": 1.254355400696864,
      "grad_norm": 19.437379837036133,
      "learning_rate": 9.717382888114596e-06,
      "loss": 1.7992,
      "step": 3240
    },
    {
      "epoch": 1.2547425474254743,
      "grad_norm": 18.056739807128906,
      "learning_rate": 9.716952725082807e-06,
      "loss": 1.9771,
      "step": 3241
    },
    {
      "epoch": 1.2551296941540844,
      "grad_norm": 21.15443229675293,
      "learning_rate": 9.716522562051019e-06,
      "loss": 1.6361,
      "step": 3242
    },
    {
      "epoch": 1.2555168408826947,
      "grad_norm": 24.650775909423828,
      "learning_rate": 9.716092399019229e-06,
      "loss": 1.871,
      "step": 3243
    },
    {
      "epoch": 1.2559039876113047,
      "grad_norm": 14.716514587402344,
      "learning_rate": 9.71566223598744e-06,
      "loss": 1.0469,
      "step": 3244
    },
    {
      "epoch": 1.2562911343399148,
      "grad_norm": 15.825467109680176,
      "learning_rate": 9.715232072955651e-06,
      "loss": 1.3215,
      "step": 3245
    },
    {
      "epoch": 1.2566782810685249,
      "grad_norm": 19.16295623779297,
      "learning_rate": 9.714801909923863e-06,
      "loss": 1.9752,
      "step": 3246
    },
    {
      "epoch": 1.2570654277971351,
      "grad_norm": 21.728979110717773,
      "learning_rate": 9.714371746892073e-06,
      "loss": 1.5984,
      "step": 3247
    },
    {
      "epoch": 1.2574525745257452,
      "grad_norm": 15.6837797164917,
      "learning_rate": 9.713941583860284e-06,
      "loss": 1.4993,
      "step": 3248
    },
    {
      "epoch": 1.2578397212543555,
      "grad_norm": 34.62144088745117,
      "learning_rate": 9.713511420828495e-06,
      "loss": 1.4777,
      "step": 3249
    },
    {
      "epoch": 1.2582268679829656,
      "grad_norm": 22.691104888916016,
      "learning_rate": 9.713081257796707e-06,
      "loss": 1.8855,
      "step": 3250
    },
    {
      "epoch": 1.2586140147115756,
      "grad_norm": 13.58126163482666,
      "learning_rate": 9.712651094764917e-06,
      "loss": 1.399,
      "step": 3251
    },
    {
      "epoch": 1.2590011614401857,
      "grad_norm": 9.981210708618164,
      "learning_rate": 9.712220931733128e-06,
      "loss": 0.7136,
      "step": 3252
    },
    {
      "epoch": 1.259388308168796,
      "grad_norm": 19.724308013916016,
      "learning_rate": 9.71179076870134e-06,
      "loss": 1.844,
      "step": 3253
    },
    {
      "epoch": 1.259775454897406,
      "grad_norm": 15.86931324005127,
      "learning_rate": 9.711360605669549e-06,
      "loss": 1.7368,
      "step": 3254
    },
    {
      "epoch": 1.2601626016260163,
      "grad_norm": 20.19759178161621,
      "learning_rate": 9.71093044263776e-06,
      "loss": 1.627,
      "step": 3255
    },
    {
      "epoch": 1.2605497483546264,
      "grad_norm": 16.1435546875,
      "learning_rate": 9.710500279605972e-06,
      "loss": 1.4883,
      "step": 3256
    },
    {
      "epoch": 1.2609368950832365,
      "grad_norm": 13.317451477050781,
      "learning_rate": 9.710070116574183e-06,
      "loss": 1.0772,
      "step": 3257
    },
    {
      "epoch": 1.2613240418118468,
      "grad_norm": 11.130797386169434,
      "learning_rate": 9.709639953542393e-06,
      "loss": 1.1782,
      "step": 3258
    },
    {
      "epoch": 1.2617111885404568,
      "grad_norm": 9.878373146057129,
      "learning_rate": 9.709209790510604e-06,
      "loss": 1.3747,
      "step": 3259
    },
    {
      "epoch": 1.262098335269067,
      "grad_norm": 25.332271575927734,
      "learning_rate": 9.708779627478814e-06,
      "loss": 2.3747,
      "step": 3260
    },
    {
      "epoch": 1.2624854819976772,
      "grad_norm": 12.971724510192871,
      "learning_rate": 9.708349464447027e-06,
      "loss": 1.1049,
      "step": 3261
    },
    {
      "epoch": 1.2628726287262872,
      "grad_norm": 13.382844924926758,
      "learning_rate": 9.707919301415237e-06,
      "loss": 0.8284,
      "step": 3262
    },
    {
      "epoch": 1.2632597754548973,
      "grad_norm": 21.925065994262695,
      "learning_rate": 9.707489138383448e-06,
      "loss": 1.5777,
      "step": 3263
    },
    {
      "epoch": 1.2636469221835076,
      "grad_norm": 12.820685386657715,
      "learning_rate": 9.707058975351658e-06,
      "loss": 1.0806,
      "step": 3264
    },
    {
      "epoch": 1.2640340689121177,
      "grad_norm": 15.547905921936035,
      "learning_rate": 9.706628812319871e-06,
      "loss": 1.8276,
      "step": 3265
    },
    {
      "epoch": 1.264421215640728,
      "grad_norm": 41.567779541015625,
      "learning_rate": 9.706198649288081e-06,
      "loss": 1.8534,
      "step": 3266
    },
    {
      "epoch": 1.264808362369338,
      "grad_norm": 23.72753143310547,
      "learning_rate": 9.705768486256292e-06,
      "loss": 1.5138,
      "step": 3267
    },
    {
      "epoch": 1.265195509097948,
      "grad_norm": 13.613243103027344,
      "learning_rate": 9.705338323224502e-06,
      "loss": 1.2042,
      "step": 3268
    },
    {
      "epoch": 1.2655826558265582,
      "grad_norm": 17.425737380981445,
      "learning_rate": 9.704908160192714e-06,
      "loss": 1.6399,
      "step": 3269
    },
    {
      "epoch": 1.2659698025551684,
      "grad_norm": 12.85645866394043,
      "learning_rate": 9.704477997160925e-06,
      "loss": 1.0971,
      "step": 3270
    },
    {
      "epoch": 1.2663569492837785,
      "grad_norm": 9.166070938110352,
      "learning_rate": 9.704047834129136e-06,
      "loss": 1.4291,
      "step": 3271
    },
    {
      "epoch": 1.2667440960123888,
      "grad_norm": 17.07532501220703,
      "learning_rate": 9.703617671097346e-06,
      "loss": 1.2797,
      "step": 3272
    },
    {
      "epoch": 1.2671312427409989,
      "grad_norm": 17.060678482055664,
      "learning_rate": 9.703187508065558e-06,
      "loss": 1.449,
      "step": 3273
    },
    {
      "epoch": 1.267518389469609,
      "grad_norm": 16.083446502685547,
      "learning_rate": 9.702757345033769e-06,
      "loss": 1.0208,
      "step": 3274
    },
    {
      "epoch": 1.267905536198219,
      "grad_norm": 14.649917602539062,
      "learning_rate": 9.702327182001979e-06,
      "loss": 1.0791,
      "step": 3275
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 13.60891342163086,
      "learning_rate": 9.70189701897019e-06,
      "loss": 1.0106,
      "step": 3276
    },
    {
      "epoch": 1.2686798296554394,
      "grad_norm": 15.313760757446289,
      "learning_rate": 9.701466855938401e-06,
      "loss": 0.794,
      "step": 3277
    },
    {
      "epoch": 1.2690669763840496,
      "grad_norm": 20.49737548828125,
      "learning_rate": 9.701036692906613e-06,
      "loss": 1.8289,
      "step": 3278
    },
    {
      "epoch": 1.2694541231126597,
      "grad_norm": 14.213780403137207,
      "learning_rate": 9.700606529874823e-06,
      "loss": 0.9897,
      "step": 3279
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 25.622934341430664,
      "learning_rate": 9.700176366843034e-06,
      "loss": 1.2772,
      "step": 3280
    },
    {
      "epoch": 1.27022841656988,
      "grad_norm": 20.725723266601562,
      "learning_rate": 9.699746203811245e-06,
      "loss": 2.1614,
      "step": 3281
    },
    {
      "epoch": 1.2706155632984901,
      "grad_norm": 21.326505661010742,
      "learning_rate": 9.699316040779457e-06,
      "loss": 2.2819,
      "step": 3282
    },
    {
      "epoch": 1.2710027100271002,
      "grad_norm": 18.862451553344727,
      "learning_rate": 9.698885877747667e-06,
      "loss": 1.1173,
      "step": 3283
    },
    {
      "epoch": 1.2713898567557105,
      "grad_norm": 31.599777221679688,
      "learning_rate": 9.698455714715878e-06,
      "loss": 3.0348,
      "step": 3284
    },
    {
      "epoch": 1.2717770034843205,
      "grad_norm": 14.505105972290039,
      "learning_rate": 9.69802555168409e-06,
      "loss": 0.5428,
      "step": 3285
    },
    {
      "epoch": 1.2721641502129306,
      "grad_norm": 27.11631965637207,
      "learning_rate": 9.6975953886523e-06,
      "loss": 1.707,
      "step": 3286
    },
    {
      "epoch": 1.272551296941541,
      "grad_norm": 20.468534469604492,
      "learning_rate": 9.69716522562051e-06,
      "loss": 1.9114,
      "step": 3287
    },
    {
      "epoch": 1.272938443670151,
      "grad_norm": 15.965800285339355,
      "learning_rate": 9.696735062588722e-06,
      "loss": 1.5302,
      "step": 3288
    },
    {
      "epoch": 1.2733255903987613,
      "grad_norm": 16.15327262878418,
      "learning_rate": 9.696304899556933e-06,
      "loss": 1.5292,
      "step": 3289
    },
    {
      "epoch": 1.2737127371273713,
      "grad_norm": 58.397972106933594,
      "learning_rate": 9.695874736525143e-06,
      "loss": 3.1395,
      "step": 3290
    },
    {
      "epoch": 1.2740998838559814,
      "grad_norm": 18.919288635253906,
      "learning_rate": 9.695444573493355e-06,
      "loss": 2.255,
      "step": 3291
    },
    {
      "epoch": 1.2744870305845915,
      "grad_norm": 15.345280647277832,
      "learning_rate": 9.695014410461566e-06,
      "loss": 1.5958,
      "step": 3292
    },
    {
      "epoch": 1.2748741773132017,
      "grad_norm": 23.239707946777344,
      "learning_rate": 9.694584247429777e-06,
      "loss": 1.4844,
      "step": 3293
    },
    {
      "epoch": 1.2752613240418118,
      "grad_norm": 33.525611877441406,
      "learning_rate": 9.694154084397987e-06,
      "loss": 2.03,
      "step": 3294
    },
    {
      "epoch": 1.275648470770422,
      "grad_norm": 13.756267547607422,
      "learning_rate": 9.693723921366198e-06,
      "loss": 1.7245,
      "step": 3295
    },
    {
      "epoch": 1.2760356174990322,
      "grad_norm": 40.17377471923828,
      "learning_rate": 9.69329375833441e-06,
      "loss": 1.6383,
      "step": 3296
    },
    {
      "epoch": 1.2764227642276422,
      "grad_norm": 23.345556259155273,
      "learning_rate": 9.692863595302621e-06,
      "loss": 1.2773,
      "step": 3297
    },
    {
      "epoch": 1.2768099109562523,
      "grad_norm": 10.175095558166504,
      "learning_rate": 9.692433432270831e-06,
      "loss": 1.1747,
      "step": 3298
    },
    {
      "epoch": 1.2771970576848626,
      "grad_norm": 13.016672134399414,
      "learning_rate": 9.692003269239042e-06,
      "loss": 0.9885,
      "step": 3299
    },
    {
      "epoch": 1.2775842044134726,
      "grad_norm": 29.733572006225586,
      "learning_rate": 9.691573106207254e-06,
      "loss": 1.9277,
      "step": 3300
    },
    {
      "epoch": 1.277971351142083,
      "grad_norm": 21.714582443237305,
      "learning_rate": 9.691142943175465e-06,
      "loss": 1.6587,
      "step": 3301
    },
    {
      "epoch": 1.278358497870693,
      "grad_norm": 16.177486419677734,
      "learning_rate": 9.690712780143675e-06,
      "loss": 1.8059,
      "step": 3302
    },
    {
      "epoch": 1.278745644599303,
      "grad_norm": 9.931166648864746,
      "learning_rate": 9.690282617111886e-06,
      "loss": 1.0671,
      "step": 3303
    },
    {
      "epoch": 1.2791327913279134,
      "grad_norm": 17.09491729736328,
      "learning_rate": 9.689852454080098e-06,
      "loss": 1.5693,
      "step": 3304
    },
    {
      "epoch": 1.2795199380565234,
      "grad_norm": 24.932231903076172,
      "learning_rate": 9.689422291048308e-06,
      "loss": 1.9352,
      "step": 3305
    },
    {
      "epoch": 1.2799070847851335,
      "grad_norm": 21.566740036010742,
      "learning_rate": 9.688992128016519e-06,
      "loss": 2.1048,
      "step": 3306
    },
    {
      "epoch": 1.2802942315137438,
      "grad_norm": 68.03015899658203,
      "learning_rate": 9.68856196498473e-06,
      "loss": 1.7339,
      "step": 3307
    },
    {
      "epoch": 1.2806813782423538,
      "grad_norm": 11.51016616821289,
      "learning_rate": 9.688131801952942e-06,
      "loss": 1.4832,
      "step": 3308
    },
    {
      "epoch": 1.281068524970964,
      "grad_norm": 15.27468204498291,
      "learning_rate": 9.687701638921152e-06,
      "loss": 0.8499,
      "step": 3309
    },
    {
      "epoch": 1.2814556716995742,
      "grad_norm": 12.367563247680664,
      "learning_rate": 9.687271475889363e-06,
      "loss": 0.9416,
      "step": 3310
    },
    {
      "epoch": 1.2818428184281843,
      "grad_norm": 9.575948715209961,
      "learning_rate": 9.686841312857573e-06,
      "loss": 1.3857,
      "step": 3311
    },
    {
      "epoch": 1.2822299651567945,
      "grad_norm": 14.732059478759766,
      "learning_rate": 9.686411149825786e-06,
      "loss": 0.7887,
      "step": 3312
    },
    {
      "epoch": 1.2826171118854046,
      "grad_norm": 14.909027099609375,
      "learning_rate": 9.685980986793996e-06,
      "loss": 1.0884,
      "step": 3313
    },
    {
      "epoch": 1.2830042586140147,
      "grad_norm": 25.21554183959961,
      "learning_rate": 9.685550823762207e-06,
      "loss": 0.8714,
      "step": 3314
    },
    {
      "epoch": 1.2833914053426247,
      "grad_norm": 17.186092376708984,
      "learning_rate": 9.685120660730417e-06,
      "loss": 0.9391,
      "step": 3315
    },
    {
      "epoch": 1.283778552071235,
      "grad_norm": 15.65897274017334,
      "learning_rate": 9.684690497698628e-06,
      "loss": 1.7216,
      "step": 3316
    },
    {
      "epoch": 1.284165698799845,
      "grad_norm": 21.36167335510254,
      "learning_rate": 9.68426033466684e-06,
      "loss": 1.6416,
      "step": 3317
    },
    {
      "epoch": 1.2845528455284554,
      "grad_norm": 20.911548614501953,
      "learning_rate": 9.683830171635051e-06,
      "loss": 1.3365,
      "step": 3318
    },
    {
      "epoch": 1.2849399922570655,
      "grad_norm": 21.69590187072754,
      "learning_rate": 9.68340000860326e-06,
      "loss": 1.751,
      "step": 3319
    },
    {
      "epoch": 1.2853271389856755,
      "grad_norm": 19.207963943481445,
      "learning_rate": 9.682969845571472e-06,
      "loss": 1.5993,
      "step": 3320
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 13.618947982788086,
      "learning_rate": 9.682539682539683e-06,
      "loss": 1.5634,
      "step": 3321
    },
    {
      "epoch": 1.2861014324428959,
      "grad_norm": 12.111778259277344,
      "learning_rate": 9.682109519507895e-06,
      "loss": 0.9602,
      "step": 3322
    },
    {
      "epoch": 1.286488579171506,
      "grad_norm": 29.309297561645508,
      "learning_rate": 9.681679356476105e-06,
      "loss": 2.0014,
      "step": 3323
    },
    {
      "epoch": 1.2868757259001162,
      "grad_norm": 11.898804664611816,
      "learning_rate": 9.681249193444316e-06,
      "loss": 1.4363,
      "step": 3324
    },
    {
      "epoch": 1.2872628726287263,
      "grad_norm": 22.153390884399414,
      "learning_rate": 9.680819030412527e-06,
      "loss": 1.6389,
      "step": 3325
    },
    {
      "epoch": 1.2876500193573364,
      "grad_norm": 10.679762840270996,
      "learning_rate": 9.680388867380737e-06,
      "loss": 1.0232,
      "step": 3326
    },
    {
      "epoch": 1.2880371660859466,
      "grad_norm": 22.199230194091797,
      "learning_rate": 9.67995870434895e-06,
      "loss": 2.006,
      "step": 3327
    },
    {
      "epoch": 1.2884243128145567,
      "grad_norm": 32.10906982421875,
      "learning_rate": 9.67952854131716e-06,
      "loss": 1.7624,
      "step": 3328
    },
    {
      "epoch": 1.2888114595431668,
      "grad_norm": 17.99242401123047,
      "learning_rate": 9.679098378285371e-06,
      "loss": 0.965,
      "step": 3329
    },
    {
      "epoch": 1.289198606271777,
      "grad_norm": 9.240885734558105,
      "learning_rate": 9.678668215253581e-06,
      "loss": 1.0597,
      "step": 3330
    },
    {
      "epoch": 1.2895857530003871,
      "grad_norm": 9.513111114501953,
      "learning_rate": 9.678238052221793e-06,
      "loss": 1.0424,
      "step": 3331
    },
    {
      "epoch": 1.2899728997289972,
      "grad_norm": 24.48053741455078,
      "learning_rate": 9.677807889190004e-06,
      "loss": 1.6013,
      "step": 3332
    },
    {
      "epoch": 1.2903600464576075,
      "grad_norm": 9.635660171508789,
      "learning_rate": 9.677377726158215e-06,
      "loss": 1.2474,
      "step": 3333
    },
    {
      "epoch": 1.2907471931862176,
      "grad_norm": 25.403959274291992,
      "learning_rate": 9.676947563126425e-06,
      "loss": 3.0401,
      "step": 3334
    },
    {
      "epoch": 1.2911343399148278,
      "grad_norm": 9.964703559875488,
      "learning_rate": 9.676517400094636e-06,
      "loss": 0.3981,
      "step": 3335
    },
    {
      "epoch": 1.291521486643438,
      "grad_norm": 22.03668212890625,
      "learning_rate": 9.676087237062848e-06,
      "loss": 1.4553,
      "step": 3336
    },
    {
      "epoch": 1.291908633372048,
      "grad_norm": 18.518678665161133,
      "learning_rate": 9.67565707403106e-06,
      "loss": 1.1309,
      "step": 3337
    },
    {
      "epoch": 1.292295780100658,
      "grad_norm": 12.104294776916504,
      "learning_rate": 9.675226910999269e-06,
      "loss": 1.2565,
      "step": 3338
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 15.823452949523926,
      "learning_rate": 9.67479674796748e-06,
      "loss": 1.8209,
      "step": 3339
    },
    {
      "epoch": 1.2930700735578784,
      "grad_norm": 12.968160629272461,
      "learning_rate": 9.674366584935692e-06,
      "loss": 1.0514,
      "step": 3340
    },
    {
      "epoch": 1.2934572202864887,
      "grad_norm": 19.80754280090332,
      "learning_rate": 9.673936421903902e-06,
      "loss": 1.8392,
      "step": 3341
    },
    {
      "epoch": 1.2938443670150988,
      "grad_norm": 17.071598052978516,
      "learning_rate": 9.673506258872113e-06,
      "loss": 1.2478,
      "step": 3342
    },
    {
      "epoch": 1.2942315137437088,
      "grad_norm": 19.01749038696289,
      "learning_rate": 9.673076095840324e-06,
      "loss": 1.6027,
      "step": 3343
    },
    {
      "epoch": 1.2946186604723189,
      "grad_norm": 16.308300018310547,
      "learning_rate": 9.672645932808536e-06,
      "loss": 1.495,
      "step": 3344
    },
    {
      "epoch": 1.2950058072009292,
      "grad_norm": 21.174715042114258,
      "learning_rate": 9.672215769776746e-06,
      "loss": 1.5615,
      "step": 3345
    },
    {
      "epoch": 1.2953929539295392,
      "grad_norm": 18.816925048828125,
      "learning_rate": 9.671785606744957e-06,
      "loss": 1.7012,
      "step": 3346
    },
    {
      "epoch": 1.2957801006581495,
      "grad_norm": 17.377412796020508,
      "learning_rate": 9.671355443713168e-06,
      "loss": 1.6281,
      "step": 3347
    },
    {
      "epoch": 1.2961672473867596,
      "grad_norm": 15.943391799926758,
      "learning_rate": 9.67092528068138e-06,
      "loss": 1.6755,
      "step": 3348
    },
    {
      "epoch": 1.2965543941153697,
      "grad_norm": 26.40155601501465,
      "learning_rate": 9.67049511764959e-06,
      "loss": 2.3617,
      "step": 3349
    },
    {
      "epoch": 1.29694154084398,
      "grad_norm": 13.924914360046387,
      "learning_rate": 9.670064954617801e-06,
      "loss": 1.1688,
      "step": 3350
    },
    {
      "epoch": 1.29732868757259,
      "grad_norm": 14.837048530578613,
      "learning_rate": 9.669634791586012e-06,
      "loss": 0.9855,
      "step": 3351
    },
    {
      "epoch": 1.2977158343012,
      "grad_norm": 24.940237045288086,
      "learning_rate": 9.669204628554222e-06,
      "loss": 1.2227,
      "step": 3352
    },
    {
      "epoch": 1.2981029810298104,
      "grad_norm": 13.079712867736816,
      "learning_rate": 9.668774465522433e-06,
      "loss": 1.2094,
      "step": 3353
    },
    {
      "epoch": 1.2984901277584204,
      "grad_norm": 12.964866638183594,
      "learning_rate": 9.668344302490645e-06,
      "loss": 1.4801,
      "step": 3354
    },
    {
      "epoch": 1.2988772744870305,
      "grad_norm": 15.798587799072266,
      "learning_rate": 9.667914139458856e-06,
      "loss": 1.4917,
      "step": 3355
    },
    {
      "epoch": 1.2992644212156408,
      "grad_norm": 19.26820182800293,
      "learning_rate": 9.667483976427066e-06,
      "loss": 1.0846,
      "step": 3356
    },
    {
      "epoch": 1.2996515679442509,
      "grad_norm": 13.980013847351074,
      "learning_rate": 9.667053813395277e-06,
      "loss": 0.9377,
      "step": 3357
    },
    {
      "epoch": 1.3000387146728611,
      "grad_norm": 22.0952091217041,
      "learning_rate": 9.666623650363489e-06,
      "loss": 1.9607,
      "step": 3358
    },
    {
      "epoch": 1.3004258614014712,
      "grad_norm": 22.01922607421875,
      "learning_rate": 9.6661934873317e-06,
      "loss": 2.4066,
      "step": 3359
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 20.779891967773438,
      "learning_rate": 9.66576332429991e-06,
      "loss": 1.1087,
      "step": 3360
    },
    {
      "epoch": 1.3012001548586913,
      "grad_norm": 24.43221664428711,
      "learning_rate": 9.665333161268121e-06,
      "loss": 1.3221,
      "step": 3361
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 17.033061981201172,
      "learning_rate": 9.664902998236331e-06,
      "loss": 1.5171,
      "step": 3362
    },
    {
      "epoch": 1.3019744483159117,
      "grad_norm": 36.26148986816406,
      "learning_rate": 9.664472835204544e-06,
      "loss": 1.6572,
      "step": 3363
    },
    {
      "epoch": 1.302361595044522,
      "grad_norm": 22.503494262695312,
      "learning_rate": 9.664042672172754e-06,
      "loss": 1.5498,
      "step": 3364
    },
    {
      "epoch": 1.302748741773132,
      "grad_norm": 20.758296966552734,
      "learning_rate": 9.663612509140965e-06,
      "loss": 1.6391,
      "step": 3365
    },
    {
      "epoch": 1.3031358885017421,
      "grad_norm": 14.692475318908691,
      "learning_rate": 9.663182346109177e-06,
      "loss": 1.1978,
      "step": 3366
    },
    {
      "epoch": 1.3035230352303522,
      "grad_norm": 17.801555633544922,
      "learning_rate": 9.662752183077387e-06,
      "loss": 1.3728,
      "step": 3367
    },
    {
      "epoch": 1.3039101819589625,
      "grad_norm": 16.596481323242188,
      "learning_rate": 9.662322020045598e-06,
      "loss": 1.6341,
      "step": 3368
    },
    {
      "epoch": 1.3042973286875725,
      "grad_norm": 18.138591766357422,
      "learning_rate": 9.66189185701381e-06,
      "loss": 1.6289,
      "step": 3369
    },
    {
      "epoch": 1.3046844754161828,
      "grad_norm": 16.189727783203125,
      "learning_rate": 9.66146169398202e-06,
      "loss": 1.0237,
      "step": 3370
    },
    {
      "epoch": 1.3050716221447929,
      "grad_norm": 18.230623245239258,
      "learning_rate": 9.66103153095023e-06,
      "loss": 1.7535,
      "step": 3371
    },
    {
      "epoch": 1.305458768873403,
      "grad_norm": 15.612677574157715,
      "learning_rate": 9.660601367918442e-06,
      "loss": 1.4639,
      "step": 3372
    },
    {
      "epoch": 1.3058459156020132,
      "grad_norm": 13.41771125793457,
      "learning_rate": 9.660171204886653e-06,
      "loss": 1.3558,
      "step": 3373
    },
    {
      "epoch": 1.3062330623306233,
      "grad_norm": 24.50428009033203,
      "learning_rate": 9.659741041854865e-06,
      "loss": 1.8957,
      "step": 3374
    },
    {
      "epoch": 1.3066202090592334,
      "grad_norm": 33.353355407714844,
      "learning_rate": 9.659310878823074e-06,
      "loss": 2.5199,
      "step": 3375
    },
    {
      "epoch": 1.3070073557878437,
      "grad_norm": 16.305633544921875,
      "learning_rate": 9.658880715791286e-06,
      "loss": 1.1079,
      "step": 3376
    },
    {
      "epoch": 1.3073945025164537,
      "grad_norm": 24.859350204467773,
      "learning_rate": 9.658450552759496e-06,
      "loss": 1.7337,
      "step": 3377
    },
    {
      "epoch": 1.3077816492450638,
      "grad_norm": 26.84339714050293,
      "learning_rate": 9.658020389727709e-06,
      "loss": 1.5897,
      "step": 3378
    },
    {
      "epoch": 1.308168795973674,
      "grad_norm": 12.134149551391602,
      "learning_rate": 9.657590226695918e-06,
      "loss": 1.2963,
      "step": 3379
    },
    {
      "epoch": 1.3085559427022841,
      "grad_norm": 22.818891525268555,
      "learning_rate": 9.65716006366413e-06,
      "loss": 1.5598,
      "step": 3380
    },
    {
      "epoch": 1.3089430894308944,
      "grad_norm": 18.331636428833008,
      "learning_rate": 9.65672990063234e-06,
      "loss": 1.8373,
      "step": 3381
    },
    {
      "epoch": 1.3093302361595045,
      "grad_norm": 42.82891082763672,
      "learning_rate": 9.656299737600551e-06,
      "loss": 1.292,
      "step": 3382
    },
    {
      "epoch": 1.3097173828881146,
      "grad_norm": 12.89648151397705,
      "learning_rate": 9.655869574568762e-06,
      "loss": 1.2975,
      "step": 3383
    },
    {
      "epoch": 1.3101045296167246,
      "grad_norm": 17.787561416625977,
      "learning_rate": 9.655439411536974e-06,
      "loss": 1.2746,
      "step": 3384
    },
    {
      "epoch": 1.310491676345335,
      "grad_norm": 20.891450881958008,
      "learning_rate": 9.655009248505184e-06,
      "loss": 1.8093,
      "step": 3385
    },
    {
      "epoch": 1.310878823073945,
      "grad_norm": 12.46840763092041,
      "learning_rate": 9.654579085473395e-06,
      "loss": 1.0917,
      "step": 3386
    },
    {
      "epoch": 1.3112659698025553,
      "grad_norm": 14.112421035766602,
      "learning_rate": 9.654148922441606e-06,
      "loss": 1.5805,
      "step": 3387
    },
    {
      "epoch": 1.3116531165311653,
      "grad_norm": 17.769887924194336,
      "learning_rate": 9.653718759409816e-06,
      "loss": 1.6792,
      "step": 3388
    },
    {
      "epoch": 1.3120402632597754,
      "grad_norm": 23.842226028442383,
      "learning_rate": 9.653288596378028e-06,
      "loss": 1.4303,
      "step": 3389
    },
    {
      "epoch": 1.3124274099883855,
      "grad_norm": 9.300182342529297,
      "learning_rate": 9.652858433346239e-06,
      "loss": 0.705,
      "step": 3390
    },
    {
      "epoch": 1.3128145567169958,
      "grad_norm": 18.489015579223633,
      "learning_rate": 9.65242827031445e-06,
      "loss": 1.3403,
      "step": 3391
    },
    {
      "epoch": 1.3132017034456058,
      "grad_norm": 15.720929145812988,
      "learning_rate": 9.65199810728266e-06,
      "loss": 1.6585,
      "step": 3392
    },
    {
      "epoch": 1.3135888501742161,
      "grad_norm": 14.094223022460938,
      "learning_rate": 9.651567944250871e-06,
      "loss": 1.3564,
      "step": 3393
    },
    {
      "epoch": 1.3139759969028262,
      "grad_norm": 28.164201736450195,
      "learning_rate": 9.651137781219083e-06,
      "loss": 1.1983,
      "step": 3394
    },
    {
      "epoch": 1.3143631436314362,
      "grad_norm": 17.560741424560547,
      "learning_rate": 9.650707618187294e-06,
      "loss": 1.3248,
      "step": 3395
    },
    {
      "epoch": 1.3147502903600465,
      "grad_norm": 32.0415153503418,
      "learning_rate": 9.650277455155504e-06,
      "loss": 2.9825,
      "step": 3396
    },
    {
      "epoch": 1.3151374370886566,
      "grad_norm": 20.53676986694336,
      "learning_rate": 9.649847292123715e-06,
      "loss": 0.926,
      "step": 3397
    },
    {
      "epoch": 1.3155245838172667,
      "grad_norm": 15.176612854003906,
      "learning_rate": 9.649417129091927e-06,
      "loss": 1.6402,
      "step": 3398
    },
    {
      "epoch": 1.315911730545877,
      "grad_norm": 16.3530216217041,
      "learning_rate": 9.648986966060138e-06,
      "loss": 1.5216,
      "step": 3399
    },
    {
      "epoch": 1.316298877274487,
      "grad_norm": 32.54514694213867,
      "learning_rate": 9.648556803028348e-06,
      "loss": 2.742,
      "step": 3400
    },
    {
      "epoch": 1.316686024003097,
      "grad_norm": 19.002429962158203,
      "learning_rate": 9.64812663999656e-06,
      "loss": 1.8131,
      "step": 3401
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 16.531545639038086,
      "learning_rate": 9.64769647696477e-06,
      "loss": 1.6578,
      "step": 3402
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 15.888009071350098,
      "learning_rate": 9.64726631393298e-06,
      "loss": 1.6679,
      "step": 3403
    },
    {
      "epoch": 1.3178474641889277,
      "grad_norm": 27.521785736083984,
      "learning_rate": 9.646836150901192e-06,
      "loss": 1.2181,
      "step": 3404
    },
    {
      "epoch": 1.3182346109175378,
      "grad_norm": 18.376510620117188,
      "learning_rate": 9.646405987869403e-06,
      "loss": 1.8687,
      "step": 3405
    },
    {
      "epoch": 1.3186217576461479,
      "grad_norm": 22.532203674316406,
      "learning_rate": 9.645975824837615e-06,
      "loss": 1.6539,
      "step": 3406
    },
    {
      "epoch": 1.319008904374758,
      "grad_norm": 15.198843002319336,
      "learning_rate": 9.645545661805825e-06,
      "loss": 1.1417,
      "step": 3407
    },
    {
      "epoch": 1.3193960511033682,
      "grad_norm": 43.06319046020508,
      "learning_rate": 9.645115498774036e-06,
      "loss": 2.1019,
      "step": 3408
    },
    {
      "epoch": 1.3197831978319783,
      "grad_norm": 24.51716423034668,
      "learning_rate": 9.644685335742247e-06,
      "loss": 1.4524,
      "step": 3409
    },
    {
      "epoch": 1.3201703445605886,
      "grad_norm": 14.62345027923584,
      "learning_rate": 9.644255172710459e-06,
      "loss": 0.5857,
      "step": 3410
    },
    {
      "epoch": 1.3205574912891986,
      "grad_norm": 17.768856048583984,
      "learning_rate": 9.643825009678669e-06,
      "loss": 1.1161,
      "step": 3411
    },
    {
      "epoch": 1.3209446380178087,
      "grad_norm": 14.153352737426758,
      "learning_rate": 9.64339484664688e-06,
      "loss": 1.4665,
      "step": 3412
    },
    {
      "epoch": 1.3213317847464188,
      "grad_norm": 31.423934936523438,
      "learning_rate": 9.642964683615091e-06,
      "loss": 1.8024,
      "step": 3413
    },
    {
      "epoch": 1.321718931475029,
      "grad_norm": 20.573917388916016,
      "learning_rate": 9.642534520583303e-06,
      "loss": 2.0796,
      "step": 3414
    },
    {
      "epoch": 1.3221060782036391,
      "grad_norm": 22.55805015563965,
      "learning_rate": 9.642104357551512e-06,
      "loss": 1.7507,
      "step": 3415
    },
    {
      "epoch": 1.3224932249322494,
      "grad_norm": 15.18310546875,
      "learning_rate": 9.641674194519724e-06,
      "loss": 1.6338,
      "step": 3416
    },
    {
      "epoch": 1.3228803716608595,
      "grad_norm": 12.264091491699219,
      "learning_rate": 9.641244031487935e-06,
      "loss": 1.1504,
      "step": 3417
    },
    {
      "epoch": 1.3232675183894695,
      "grad_norm": 20.935256958007812,
      "learning_rate": 9.640813868456145e-06,
      "loss": 2.467,
      "step": 3418
    },
    {
      "epoch": 1.3236546651180798,
      "grad_norm": 13.690155029296875,
      "learning_rate": 9.640383705424356e-06,
      "loss": 1.4612,
      "step": 3419
    },
    {
      "epoch": 1.32404181184669,
      "grad_norm": 17.28893280029297,
      "learning_rate": 9.639953542392568e-06,
      "loss": 1.8173,
      "step": 3420
    },
    {
      "epoch": 1.3244289585753,
      "grad_norm": 16.320608139038086,
      "learning_rate": 9.63952337936078e-06,
      "loss": 1.6497,
      "step": 3421
    },
    {
      "epoch": 1.3248161053039103,
      "grad_norm": 19.458900451660156,
      "learning_rate": 9.639093216328989e-06,
      "loss": 1.6852,
      "step": 3422
    },
    {
      "epoch": 1.3252032520325203,
      "grad_norm": 14.68132495880127,
      "learning_rate": 9.6386630532972e-06,
      "loss": 1.0371,
      "step": 3423
    },
    {
      "epoch": 1.3255903987611304,
      "grad_norm": 12.477901458740234,
      "learning_rate": 9.63823289026541e-06,
      "loss": 1.4236,
      "step": 3424
    },
    {
      "epoch": 1.3259775454897407,
      "grad_norm": 13.613871574401855,
      "learning_rate": 9.637802727233623e-06,
      "loss": 1.4593,
      "step": 3425
    },
    {
      "epoch": 1.3263646922183507,
      "grad_norm": 12.49155330657959,
      "learning_rate": 9.637372564201833e-06,
      "loss": 1.4276,
      "step": 3426
    },
    {
      "epoch": 1.326751838946961,
      "grad_norm": 13.62575912475586,
      "learning_rate": 9.636942401170044e-06,
      "loss": 1.5996,
      "step": 3427
    },
    {
      "epoch": 1.327138985675571,
      "grad_norm": 11.197802543640137,
      "learning_rate": 9.636512238138254e-06,
      "loss": 0.7781,
      "step": 3428
    },
    {
      "epoch": 1.3275261324041812,
      "grad_norm": 20.936513900756836,
      "learning_rate": 9.636082075106467e-06,
      "loss": 1.1645,
      "step": 3429
    },
    {
      "epoch": 1.3279132791327912,
      "grad_norm": 14.205543518066406,
      "learning_rate": 9.635651912074677e-06,
      "loss": 1.4589,
      "step": 3430
    },
    {
      "epoch": 1.3283004258614015,
      "grad_norm": 32.11210250854492,
      "learning_rate": 9.635221749042888e-06,
      "loss": 1.6508,
      "step": 3431
    },
    {
      "epoch": 1.3286875725900116,
      "grad_norm": 15.868603706359863,
      "learning_rate": 9.634791586011098e-06,
      "loss": 1.5584,
      "step": 3432
    },
    {
      "epoch": 1.3290747193186219,
      "grad_norm": 19.77918243408203,
      "learning_rate": 9.63436142297931e-06,
      "loss": 1.5816,
      "step": 3433
    },
    {
      "epoch": 1.329461866047232,
      "grad_norm": 27.626691818237305,
      "learning_rate": 9.633931259947521e-06,
      "loss": 1.8961,
      "step": 3434
    },
    {
      "epoch": 1.329849012775842,
      "grad_norm": 10.127907752990723,
      "learning_rate": 9.633501096915732e-06,
      "loss": 1.8504,
      "step": 3435
    },
    {
      "epoch": 1.330236159504452,
      "grad_norm": 14.391097068786621,
      "learning_rate": 9.633070933883942e-06,
      "loss": 1.2051,
      "step": 3436
    },
    {
      "epoch": 1.3306233062330624,
      "grad_norm": 25.888399124145508,
      "learning_rate": 9.632640770852153e-06,
      "loss": 1.6564,
      "step": 3437
    },
    {
      "epoch": 1.3310104529616724,
      "grad_norm": 13.01993465423584,
      "learning_rate": 9.632210607820365e-06,
      "loss": 1.3869,
      "step": 3438
    },
    {
      "epoch": 1.3313975996902827,
      "grad_norm": 21.876121520996094,
      "learning_rate": 9.631780444788575e-06,
      "loss": 1.7757,
      "step": 3439
    },
    {
      "epoch": 1.3317847464188928,
      "grad_norm": 14.903050422668457,
      "learning_rate": 9.631350281756786e-06,
      "loss": 0.8845,
      "step": 3440
    },
    {
      "epoch": 1.3321718931475028,
      "grad_norm": 14.575642585754395,
      "learning_rate": 9.630920118724997e-06,
      "loss": 1.5567,
      "step": 3441
    },
    {
      "epoch": 1.3325590398761131,
      "grad_norm": 22.90084457397461,
      "learning_rate": 9.630489955693209e-06,
      "loss": 1.6448,
      "step": 3442
    },
    {
      "epoch": 1.3329461866047232,
      "grad_norm": 17.03199577331543,
      "learning_rate": 9.630059792661419e-06,
      "loss": 1.8449,
      "step": 3443
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 13.455605506896973,
      "learning_rate": 9.62962962962963e-06,
      "loss": 1.5526,
      "step": 3444
    },
    {
      "epoch": 1.3337204800619435,
      "grad_norm": 11.355820655822754,
      "learning_rate": 9.629199466597841e-06,
      "loss": 1.229,
      "step": 3445
    },
    {
      "epoch": 1.3341076267905536,
      "grad_norm": 15.893288612365723,
      "learning_rate": 9.628769303566053e-06,
      "loss": 1.5392,
      "step": 3446
    },
    {
      "epoch": 1.3344947735191637,
      "grad_norm": 20.88265037536621,
      "learning_rate": 9.628339140534263e-06,
      "loss": 1.0838,
      "step": 3447
    },
    {
      "epoch": 1.334881920247774,
      "grad_norm": 25.060264587402344,
      "learning_rate": 9.627908977502474e-06,
      "loss": 1.6364,
      "step": 3448
    },
    {
      "epoch": 1.335269066976384,
      "grad_norm": 20.531225204467773,
      "learning_rate": 9.627478814470685e-06,
      "loss": 2.4238,
      "step": 3449
    },
    {
      "epoch": 1.3356562137049943,
      "grad_norm": 13.812579154968262,
      "learning_rate": 9.627048651438897e-06,
      "loss": 0.9264,
      "step": 3450
    },
    {
      "epoch": 1.3360433604336044,
      "grad_norm": 24.500900268554688,
      "learning_rate": 9.626618488407107e-06,
      "loss": 1.4781,
      "step": 3451
    },
    {
      "epoch": 1.3364305071622145,
      "grad_norm": 15.895552635192871,
      "learning_rate": 9.626188325375318e-06,
      "loss": 1.5544,
      "step": 3452
    },
    {
      "epoch": 1.3368176538908245,
      "grad_norm": 22.024145126342773,
      "learning_rate": 9.62575816234353e-06,
      "loss": 1.8333,
      "step": 3453
    },
    {
      "epoch": 1.3372048006194348,
      "grad_norm": 16.74407196044922,
      "learning_rate": 9.625327999311739e-06,
      "loss": 1.3914,
      "step": 3454
    },
    {
      "epoch": 1.3375919473480449,
      "grad_norm": 23.65851593017578,
      "learning_rate": 9.62489783627995e-06,
      "loss": 1.9918,
      "step": 3455
    },
    {
      "epoch": 1.3379790940766552,
      "grad_norm": 18.055118560791016,
      "learning_rate": 9.624467673248162e-06,
      "loss": 1.7498,
      "step": 3456
    },
    {
      "epoch": 1.3383662408052652,
      "grad_norm": 12.637370109558105,
      "learning_rate": 9.624037510216373e-06,
      "loss": 0.7492,
      "step": 3457
    },
    {
      "epoch": 1.3387533875338753,
      "grad_norm": 19.001380920410156,
      "learning_rate": 9.623607347184583e-06,
      "loss": 2.248,
      "step": 3458
    },
    {
      "epoch": 1.3391405342624854,
      "grad_norm": 14.410921096801758,
      "learning_rate": 9.623177184152794e-06,
      "loss": 1.5533,
      "step": 3459
    },
    {
      "epoch": 1.3395276809910956,
      "grad_norm": 15.341729164123535,
      "learning_rate": 9.622747021121006e-06,
      "loss": 1.5207,
      "step": 3460
    },
    {
      "epoch": 1.3399148277197057,
      "grad_norm": 25.953672409057617,
      "learning_rate": 9.622316858089217e-06,
      "loss": 1.3231,
      "step": 3461
    },
    {
      "epoch": 1.340301974448316,
      "grad_norm": 18.3841552734375,
      "learning_rate": 9.621886695057427e-06,
      "loss": 1.3641,
      "step": 3462
    },
    {
      "epoch": 1.340689121176926,
      "grad_norm": 25.626413345336914,
      "learning_rate": 9.621456532025638e-06,
      "loss": 1.6771,
      "step": 3463
    },
    {
      "epoch": 1.3410762679055361,
      "grad_norm": 13.292607307434082,
      "learning_rate": 9.62102636899385e-06,
      "loss": 0.9426,
      "step": 3464
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 33.015682220458984,
      "learning_rate": 9.620596205962061e-06,
      "loss": 1.689,
      "step": 3465
    },
    {
      "epoch": 1.3418505613627565,
      "grad_norm": 26.144439697265625,
      "learning_rate": 9.620166042930271e-06,
      "loss": 2.0046,
      "step": 3466
    },
    {
      "epoch": 1.3422377080913666,
      "grad_norm": 22.526676177978516,
      "learning_rate": 9.619735879898482e-06,
      "loss": 1.8549,
      "step": 3467
    },
    {
      "epoch": 1.3426248548199768,
      "grad_norm": 17.302200317382812,
      "learning_rate": 9.619305716866694e-06,
      "loss": 1.282,
      "step": 3468
    },
    {
      "epoch": 1.343012001548587,
      "grad_norm": 25.541383743286133,
      "learning_rate": 9.618875553834904e-06,
      "loss": 1.6948,
      "step": 3469
    },
    {
      "epoch": 1.343399148277197,
      "grad_norm": 17.177343368530273,
      "learning_rate": 9.618445390803115e-06,
      "loss": 1.8015,
      "step": 3470
    },
    {
      "epoch": 1.3437862950058073,
      "grad_norm": 16.522708892822266,
      "learning_rate": 9.618015227771326e-06,
      "loss": 1.5567,
      "step": 3471
    },
    {
      "epoch": 1.3441734417344173,
      "grad_norm": 16.339515686035156,
      "learning_rate": 9.617585064739538e-06,
      "loss": 1.6642,
      "step": 3472
    },
    {
      "epoch": 1.3445605884630276,
      "grad_norm": 26.31557846069336,
      "learning_rate": 9.617154901707747e-06,
      "loss": 1.1847,
      "step": 3473
    },
    {
      "epoch": 1.3449477351916377,
      "grad_norm": 25.60638427734375,
      "learning_rate": 9.616724738675959e-06,
      "loss": 2.31,
      "step": 3474
    },
    {
      "epoch": 1.3453348819202477,
      "grad_norm": 18.261266708374023,
      "learning_rate": 9.616294575644169e-06,
      "loss": 1.7167,
      "step": 3475
    },
    {
      "epoch": 1.3457220286488578,
      "grad_norm": 16.537546157836914,
      "learning_rate": 9.615864412612382e-06,
      "loss": 2.445,
      "step": 3476
    },
    {
      "epoch": 1.346109175377468,
      "grad_norm": 15.526817321777344,
      "learning_rate": 9.615434249580591e-06,
      "loss": 1.7366,
      "step": 3477
    },
    {
      "epoch": 1.3464963221060782,
      "grad_norm": 22.298677444458008,
      "learning_rate": 9.615004086548803e-06,
      "loss": 0.9798,
      "step": 3478
    },
    {
      "epoch": 1.3468834688346885,
      "grad_norm": 21.987302780151367,
      "learning_rate": 9.614573923517013e-06,
      "loss": 1.5398,
      "step": 3479
    },
    {
      "epoch": 1.3472706155632985,
      "grad_norm": 16.146724700927734,
      "learning_rate": 9.614143760485226e-06,
      "loss": 1.284,
      "step": 3480
    },
    {
      "epoch": 1.3476577622919086,
      "grad_norm": 42.82249069213867,
      "learning_rate": 9.613713597453435e-06,
      "loss": 2.0976,
      "step": 3481
    },
    {
      "epoch": 1.3480449090205187,
      "grad_norm": 12.40505599975586,
      "learning_rate": 9.613283434421647e-06,
      "loss": 1.557,
      "step": 3482
    },
    {
      "epoch": 1.348432055749129,
      "grad_norm": 19.735118865966797,
      "learning_rate": 9.612853271389857e-06,
      "loss": 1.9154,
      "step": 3483
    },
    {
      "epoch": 1.348819202477739,
      "grad_norm": 9.333877563476562,
      "learning_rate": 9.612423108358068e-06,
      "loss": 0.6606,
      "step": 3484
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 19.917957305908203,
      "learning_rate": 9.61199294532628e-06,
      "loss": 2.1717,
      "step": 3485
    },
    {
      "epoch": 1.3495934959349594,
      "grad_norm": 9.478782653808594,
      "learning_rate": 9.61156278229449e-06,
      "loss": 1.2281,
      "step": 3486
    },
    {
      "epoch": 1.3499806426635694,
      "grad_norm": 16.020320892333984,
      "learning_rate": 9.6111326192627e-06,
      "loss": 1.032,
      "step": 3487
    },
    {
      "epoch": 1.3503677893921797,
      "grad_norm": 23.84636688232422,
      "learning_rate": 9.610702456230912e-06,
      "loss": 2.0645,
      "step": 3488
    },
    {
      "epoch": 1.3507549361207898,
      "grad_norm": 19.698400497436523,
      "learning_rate": 9.610272293199123e-06,
      "loss": 2.2971,
      "step": 3489
    },
    {
      "epoch": 1.3511420828493999,
      "grad_norm": 23.008508682250977,
      "learning_rate": 9.609842130167333e-06,
      "loss": 1.5395,
      "step": 3490
    },
    {
      "epoch": 1.3515292295780101,
      "grad_norm": 32.634765625,
      "learning_rate": 9.609411967135546e-06,
      "loss": 1.4945,
      "step": 3491
    },
    {
      "epoch": 1.3519163763066202,
      "grad_norm": 11.456148147583008,
      "learning_rate": 9.608981804103756e-06,
      "loss": 1.3683,
      "step": 3492
    },
    {
      "epoch": 1.3523035230352303,
      "grad_norm": 25.16816520690918,
      "learning_rate": 9.608551641071967e-06,
      "loss": 2.4135,
      "step": 3493
    },
    {
      "epoch": 1.3526906697638406,
      "grad_norm": 24.68096923828125,
      "learning_rate": 9.608121478040177e-06,
      "loss": 1.7738,
      "step": 3494
    },
    {
      "epoch": 1.3530778164924506,
      "grad_norm": 19.967809677124023,
      "learning_rate": 9.60769131500839e-06,
      "loss": 1.4758,
      "step": 3495
    },
    {
      "epoch": 1.353464963221061,
      "grad_norm": 26.237321853637695,
      "learning_rate": 9.6072611519766e-06,
      "loss": 1.4945,
      "step": 3496
    },
    {
      "epoch": 1.353852109949671,
      "grad_norm": 15.790268898010254,
      "learning_rate": 9.606830988944811e-06,
      "loss": 2.4626,
      "step": 3497
    },
    {
      "epoch": 1.354239256678281,
      "grad_norm": 23.68973731994629,
      "learning_rate": 9.606400825913021e-06,
      "loss": 1.4758,
      "step": 3498
    },
    {
      "epoch": 1.354626403406891,
      "grad_norm": 22.662799835205078,
      "learning_rate": 9.605970662881232e-06,
      "loss": 1.6033,
      "step": 3499
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 14.864972114562988,
      "learning_rate": 9.605540499849444e-06,
      "loss": 1.4721,
      "step": 3500
    },
    {
      "epoch": 1.3554006968641115,
      "grad_norm": 17.347036361694336,
      "learning_rate": 9.605110336817655e-06,
      "loss": 1.0213,
      "step": 3501
    },
    {
      "epoch": 1.3557878435927218,
      "grad_norm": 22.828685760498047,
      "learning_rate": 9.604680173785865e-06,
      "loss": 1.4313,
      "step": 3502
    },
    {
      "epoch": 1.3561749903213318,
      "grad_norm": 13.30090618133545,
      "learning_rate": 9.604250010754076e-06,
      "loss": 1.1683,
      "step": 3503
    },
    {
      "epoch": 1.3565621370499419,
      "grad_norm": 22.427967071533203,
      "learning_rate": 9.603819847722288e-06,
      "loss": 2.3406,
      "step": 3504
    },
    {
      "epoch": 1.356949283778552,
      "grad_norm": 15.385917663574219,
      "learning_rate": 9.603389684690498e-06,
      "loss": 1.59,
      "step": 3505
    },
    {
      "epoch": 1.3573364305071622,
      "grad_norm": 16.466846466064453,
      "learning_rate": 9.602959521658709e-06,
      "loss": 1.0232,
      "step": 3506
    },
    {
      "epoch": 1.3577235772357723,
      "grad_norm": 17.26390266418457,
      "learning_rate": 9.60252935862692e-06,
      "loss": 1.5017,
      "step": 3507
    },
    {
      "epoch": 1.3581107239643826,
      "grad_norm": 12.371686935424805,
      "learning_rate": 9.602099195595132e-06,
      "loss": 1.5026,
      "step": 3508
    },
    {
      "epoch": 1.3584978706929927,
      "grad_norm": 18.153127670288086,
      "learning_rate": 9.601669032563342e-06,
      "loss": 1.5795,
      "step": 3509
    },
    {
      "epoch": 1.3588850174216027,
      "grad_norm": 21.939407348632812,
      "learning_rate": 9.601238869531553e-06,
      "loss": 0.9562,
      "step": 3510
    },
    {
      "epoch": 1.359272164150213,
      "grad_norm": 11.34705924987793,
      "learning_rate": 9.600808706499764e-06,
      "loss": 1.3961,
      "step": 3511
    },
    {
      "epoch": 1.359659310878823,
      "grad_norm": 8.758858680725098,
      "learning_rate": 9.600378543467976e-06,
      "loss": 0.7065,
      "step": 3512
    },
    {
      "epoch": 1.3600464576074331,
      "grad_norm": 34.111167907714844,
      "learning_rate": 9.599948380436185e-06,
      "loss": 1.6705,
      "step": 3513
    },
    {
      "epoch": 1.3604336043360434,
      "grad_norm": 19.796598434448242,
      "learning_rate": 9.599518217404397e-06,
      "loss": 1.9703,
      "step": 3514
    },
    {
      "epoch": 1.3608207510646535,
      "grad_norm": 24.188997268676758,
      "learning_rate": 9.599088054372608e-06,
      "loss": 1.6476,
      "step": 3515
    },
    {
      "epoch": 1.3612078977932636,
      "grad_norm": 16.172489166259766,
      "learning_rate": 9.59865789134082e-06,
      "loss": 1.3066,
      "step": 3516
    },
    {
      "epoch": 1.3615950445218739,
      "grad_norm": 15.83199691772461,
      "learning_rate": 9.59822772830903e-06,
      "loss": 1.6864,
      "step": 3517
    },
    {
      "epoch": 1.361982191250484,
      "grad_norm": 23.04738998413086,
      "learning_rate": 9.597797565277241e-06,
      "loss": 2.1675,
      "step": 3518
    },
    {
      "epoch": 1.3623693379790942,
      "grad_norm": 14.54413890838623,
      "learning_rate": 9.597367402245452e-06,
      "loss": 1.34,
      "step": 3519
    },
    {
      "epoch": 1.3627564847077043,
      "grad_norm": 13.993019104003906,
      "learning_rate": 9.596937239213662e-06,
      "loss": 1.0836,
      "step": 3520
    },
    {
      "epoch": 1.3631436314363143,
      "grad_norm": 15.482339859008789,
      "learning_rate": 9.596507076181873e-06,
      "loss": 1.1896,
      "step": 3521
    },
    {
      "epoch": 1.3635307781649244,
      "grad_norm": 16.267318725585938,
      "learning_rate": 9.596076913150085e-06,
      "loss": 1.7389,
      "step": 3522
    },
    {
      "epoch": 1.3639179248935347,
      "grad_norm": 20.676830291748047,
      "learning_rate": 9.595646750118296e-06,
      "loss": 0.9689,
      "step": 3523
    },
    {
      "epoch": 1.3643050716221448,
      "grad_norm": 14.435134887695312,
      "learning_rate": 9.595216587086506e-06,
      "loss": 1.3037,
      "step": 3524
    },
    {
      "epoch": 1.364692218350755,
      "grad_norm": 10.160842895507812,
      "learning_rate": 9.594786424054717e-06,
      "loss": 1.3814,
      "step": 3525
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 19.350954055786133,
      "learning_rate": 9.594356261022927e-06,
      "loss": 1.3492,
      "step": 3526
    },
    {
      "epoch": 1.3654665118079752,
      "grad_norm": 14.83062744140625,
      "learning_rate": 9.59392609799114e-06,
      "loss": 1.3542,
      "step": 3527
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 26.41290283203125,
      "learning_rate": 9.59349593495935e-06,
      "loss": 2.2811,
      "step": 3528
    },
    {
      "epoch": 1.3662408052651955,
      "grad_norm": 20.683237075805664,
      "learning_rate": 9.593065771927561e-06,
      "loss": 1.0309,
      "step": 3529
    },
    {
      "epoch": 1.3666279519938056,
      "grad_norm": 23.57701873779297,
      "learning_rate": 9.592635608895773e-06,
      "loss": 1.7172,
      "step": 3530
    },
    {
      "epoch": 1.3670150987224159,
      "grad_norm": 23.646635055541992,
      "learning_rate": 9.592205445863984e-06,
      "loss": 1.5853,
      "step": 3531
    },
    {
      "epoch": 1.367402245451026,
      "grad_norm": 24.84259033203125,
      "learning_rate": 9.591775282832194e-06,
      "loss": 0.9029,
      "step": 3532
    },
    {
      "epoch": 1.367789392179636,
      "grad_norm": 20.724918365478516,
      "learning_rate": 9.591345119800405e-06,
      "loss": 1.2161,
      "step": 3533
    },
    {
      "epoch": 1.368176538908246,
      "grad_norm": 22.292753219604492,
      "learning_rate": 9.590914956768617e-06,
      "loss": 1.3782,
      "step": 3534
    },
    {
      "epoch": 1.3685636856368564,
      "grad_norm": 17.008453369140625,
      "learning_rate": 9.590484793736826e-06,
      "loss": 1.5696,
      "step": 3535
    },
    {
      "epoch": 1.3689508323654664,
      "grad_norm": 8.011855125427246,
      "learning_rate": 9.590054630705038e-06,
      "loss": 1.3458,
      "step": 3536
    },
    {
      "epoch": 1.3693379790940767,
      "grad_norm": 7.891359329223633,
      "learning_rate": 9.58962446767325e-06,
      "loss": 1.0885,
      "step": 3537
    },
    {
      "epoch": 1.3697251258226868,
      "grad_norm": 11.614701271057129,
      "learning_rate": 9.58919430464146e-06,
      "loss": 0.4847,
      "step": 3538
    },
    {
      "epoch": 1.3701122725512969,
      "grad_norm": 14.757722854614258,
      "learning_rate": 9.58876414160967e-06,
      "loss": 0.984,
      "step": 3539
    },
    {
      "epoch": 1.3704994192799071,
      "grad_norm": 15.103273391723633,
      "learning_rate": 9.588333978577882e-06,
      "loss": 1.4263,
      "step": 3540
    },
    {
      "epoch": 1.3708865660085172,
      "grad_norm": 16.59102439880371,
      "learning_rate": 9.587903815546092e-06,
      "loss": 1.4788,
      "step": 3541
    },
    {
      "epoch": 1.3712737127371275,
      "grad_norm": 18.490917205810547,
      "learning_rate": 9.587473652514305e-06,
      "loss": 2.8409,
      "step": 3542
    },
    {
      "epoch": 1.3716608594657376,
      "grad_norm": 12.286789894104004,
      "learning_rate": 9.587043489482514e-06,
      "loss": 0.7132,
      "step": 3543
    },
    {
      "epoch": 1.3720480061943476,
      "grad_norm": 8.39908218383789,
      "learning_rate": 9.586613326450726e-06,
      "loss": 0.6447,
      "step": 3544
    },
    {
      "epoch": 1.3724351529229577,
      "grad_norm": 24.145570755004883,
      "learning_rate": 9.586183163418936e-06,
      "loss": 2.0586,
      "step": 3545
    },
    {
      "epoch": 1.372822299651568,
      "grad_norm": 27.597412109375,
      "learning_rate": 9.585753000387149e-06,
      "loss": 1.3586,
      "step": 3546
    },
    {
      "epoch": 1.373209446380178,
      "grad_norm": 29.442134857177734,
      "learning_rate": 9.585322837355358e-06,
      "loss": 2.2221,
      "step": 3547
    },
    {
      "epoch": 1.3735965931087883,
      "grad_norm": 11.36711597442627,
      "learning_rate": 9.58489267432357e-06,
      "loss": 0.9323,
      "step": 3548
    },
    {
      "epoch": 1.3739837398373984,
      "grad_norm": 38.84115219116211,
      "learning_rate": 9.58446251129178e-06,
      "loss": 2.0521,
      "step": 3549
    },
    {
      "epoch": 1.3743708865660085,
      "grad_norm": 17.980995178222656,
      "learning_rate": 9.584032348259991e-06,
      "loss": 1.1918,
      "step": 3550
    },
    {
      "epoch": 1.3747580332946185,
      "grad_norm": 14.392972946166992,
      "learning_rate": 9.583602185228202e-06,
      "loss": 1.5151,
      "step": 3551
    },
    {
      "epoch": 1.3751451800232288,
      "grad_norm": 7.78441047668457,
      "learning_rate": 9.583172022196414e-06,
      "loss": 0.562,
      "step": 3552
    },
    {
      "epoch": 1.375532326751839,
      "grad_norm": 9.173548698425293,
      "learning_rate": 9.582741859164623e-06,
      "loss": 1.3207,
      "step": 3553
    },
    {
      "epoch": 1.3759194734804492,
      "grad_norm": 11.747632026672363,
      "learning_rate": 9.582311696132835e-06,
      "loss": 0.7385,
      "step": 3554
    },
    {
      "epoch": 1.3763066202090593,
      "grad_norm": 14.926589012145996,
      "learning_rate": 9.581881533101046e-06,
      "loss": 1.2158,
      "step": 3555
    },
    {
      "epoch": 1.3766937669376693,
      "grad_norm": 19.055112838745117,
      "learning_rate": 9.581451370069256e-06,
      "loss": 1.7706,
      "step": 3556
    },
    {
      "epoch": 1.3770809136662794,
      "grad_norm": 13.38760757446289,
      "learning_rate": 9.581021207037467e-06,
      "loss": 1.119,
      "step": 3557
    },
    {
      "epoch": 1.3774680603948897,
      "grad_norm": 15.868931770324707,
      "learning_rate": 9.580591044005679e-06,
      "loss": 1.5502,
      "step": 3558
    },
    {
      "epoch": 1.3778552071234997,
      "grad_norm": 12.771061897277832,
      "learning_rate": 9.58016088097389e-06,
      "loss": 1.1551,
      "step": 3559
    },
    {
      "epoch": 1.37824235385211,
      "grad_norm": 21.564416885375977,
      "learning_rate": 9.5797307179421e-06,
      "loss": 1.6548,
      "step": 3560
    },
    {
      "epoch": 1.37862950058072,
      "grad_norm": 21.748104095458984,
      "learning_rate": 9.579300554910311e-06,
      "loss": 1.7672,
      "step": 3561
    },
    {
      "epoch": 1.3790166473093302,
      "grad_norm": 41.533451080322266,
      "learning_rate": 9.578870391878523e-06,
      "loss": 1.2596,
      "step": 3562
    },
    {
      "epoch": 1.3794037940379404,
      "grad_norm": 19.553369522094727,
      "learning_rate": 9.578440228846734e-06,
      "loss": 1.5767,
      "step": 3563
    },
    {
      "epoch": 1.3797909407665505,
      "grad_norm": 47.86676025390625,
      "learning_rate": 9.578010065814944e-06,
      "loss": 1.3266,
      "step": 3564
    },
    {
      "epoch": 1.3801780874951608,
      "grad_norm": 15.96479320526123,
      "learning_rate": 9.577579902783155e-06,
      "loss": 2.3863,
      "step": 3565
    },
    {
      "epoch": 1.3805652342237709,
      "grad_norm": 14.267049789428711,
      "learning_rate": 9.577149739751367e-06,
      "loss": 0.782,
      "step": 3566
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 50.56477355957031,
      "learning_rate": 9.576719576719578e-06,
      "loss": 1.792,
      "step": 3567
    },
    {
      "epoch": 1.381339527680991,
      "grad_norm": 16.52740478515625,
      "learning_rate": 9.576289413687788e-06,
      "loss": 1.4485,
      "step": 3568
    },
    {
      "epoch": 1.3817266744096013,
      "grad_norm": 21.923978805541992,
      "learning_rate": 9.575859250656e-06,
      "loss": 1.1944,
      "step": 3569
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 24.03523063659668,
      "learning_rate": 9.57542908762421e-06,
      "loss": 2.477,
      "step": 3570
    },
    {
      "epoch": 1.3825009678668216,
      "grad_norm": 22.33869743347168,
      "learning_rate": 9.57499892459242e-06,
      "loss": 1.552,
      "step": 3571
    },
    {
      "epoch": 1.3828881145954317,
      "grad_norm": 31.526147842407227,
      "learning_rate": 9.574568761560632e-06,
      "loss": 2.897,
      "step": 3572
    },
    {
      "epoch": 1.3832752613240418,
      "grad_norm": 17.543306350708008,
      "learning_rate": 9.574138598528843e-06,
      "loss": 1.8544,
      "step": 3573
    },
    {
      "epoch": 1.3836624080526518,
      "grad_norm": 37.566139221191406,
      "learning_rate": 9.573708435497055e-06,
      "loss": 1.7974,
      "step": 3574
    },
    {
      "epoch": 1.3840495547812621,
      "grad_norm": 33.36931228637695,
      "learning_rate": 9.573278272465264e-06,
      "loss": 1.715,
      "step": 3575
    },
    {
      "epoch": 1.3844367015098722,
      "grad_norm": 13.667819023132324,
      "learning_rate": 9.572848109433476e-06,
      "loss": 0.9941,
      "step": 3576
    },
    {
      "epoch": 1.3848238482384825,
      "grad_norm": 16.70513916015625,
      "learning_rate": 9.572417946401687e-06,
      "loss": 1.4886,
      "step": 3577
    },
    {
      "epoch": 1.3852109949670925,
      "grad_norm": 16.013887405395508,
      "learning_rate": 9.571987783369899e-06,
      "loss": 1.2787,
      "step": 3578
    },
    {
      "epoch": 1.3855981416957026,
      "grad_norm": 17.46553611755371,
      "learning_rate": 9.571557620338108e-06,
      "loss": 1.7699,
      "step": 3579
    },
    {
      "epoch": 1.3859852884243127,
      "grad_norm": 20.88226318359375,
      "learning_rate": 9.57112745730632e-06,
      "loss": 2.0133,
      "step": 3580
    },
    {
      "epoch": 1.386372435152923,
      "grad_norm": 21.302406311035156,
      "learning_rate": 9.570697294274531e-06,
      "loss": 1.1988,
      "step": 3581
    },
    {
      "epoch": 1.386759581881533,
      "grad_norm": 11.546379089355469,
      "learning_rate": 9.570267131242743e-06,
      "loss": 1.3616,
      "step": 3582
    },
    {
      "epoch": 1.3871467286101433,
      "grad_norm": 42.6988410949707,
      "learning_rate": 9.569836968210952e-06,
      "loss": 1.7959,
      "step": 3583
    },
    {
      "epoch": 1.3875338753387534,
      "grad_norm": 14.121162414550781,
      "learning_rate": 9.569406805179164e-06,
      "loss": 1.0947,
      "step": 3584
    },
    {
      "epoch": 1.3879210220673635,
      "grad_norm": 20.13431167602539,
      "learning_rate": 9.568976642147375e-06,
      "loss": 1.1171,
      "step": 3585
    },
    {
      "epoch": 1.3883081687959737,
      "grad_norm": 19.878847122192383,
      "learning_rate": 9.568546479115585e-06,
      "loss": 1.5404,
      "step": 3586
    },
    {
      "epoch": 1.3886953155245838,
      "grad_norm": 45.44544982910156,
      "learning_rate": 9.568116316083796e-06,
      "loss": 2.3652,
      "step": 3587
    },
    {
      "epoch": 1.389082462253194,
      "grad_norm": 19.73773956298828,
      "learning_rate": 9.567686153052008e-06,
      "loss": 1.3241,
      "step": 3588
    },
    {
      "epoch": 1.3894696089818042,
      "grad_norm": 16.222614288330078,
      "learning_rate": 9.56725599002022e-06,
      "loss": 1.7011,
      "step": 3589
    },
    {
      "epoch": 1.3898567557104142,
      "grad_norm": 12.824857711791992,
      "learning_rate": 9.566825826988429e-06,
      "loss": 1.3135,
      "step": 3590
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 14.66210651397705,
      "learning_rate": 9.56639566395664e-06,
      "loss": 1.4737,
      "step": 3591
    },
    {
      "epoch": 1.3906310491676346,
      "grad_norm": 18.79764747619629,
      "learning_rate": 9.56596550092485e-06,
      "loss": 1.6242,
      "step": 3592
    },
    {
      "epoch": 1.3910181958962446,
      "grad_norm": 10.779136657714844,
      "learning_rate": 9.565535337893063e-06,
      "loss": 0.7418,
      "step": 3593
    },
    {
      "epoch": 1.391405342624855,
      "grad_norm": 13.835148811340332,
      "learning_rate": 9.565105174861273e-06,
      "loss": 0.9928,
      "step": 3594
    },
    {
      "epoch": 1.391792489353465,
      "grad_norm": 23.564531326293945,
      "learning_rate": 9.564675011829484e-06,
      "loss": 1.1466,
      "step": 3595
    },
    {
      "epoch": 1.392179636082075,
      "grad_norm": 14.840373039245605,
      "learning_rate": 9.564244848797694e-06,
      "loss": 1.4599,
      "step": 3596
    },
    {
      "epoch": 1.3925667828106851,
      "grad_norm": 18.769840240478516,
      "learning_rate": 9.563814685765907e-06,
      "loss": 1.6917,
      "step": 3597
    },
    {
      "epoch": 1.3929539295392954,
      "grad_norm": 19.322507858276367,
      "learning_rate": 9.563384522734117e-06,
      "loss": 1.4439,
      "step": 3598
    },
    {
      "epoch": 1.3933410762679055,
      "grad_norm": 10.227241516113281,
      "learning_rate": 9.562954359702328e-06,
      "loss": 0.6598,
      "step": 3599
    },
    {
      "epoch": 1.3937282229965158,
      "grad_norm": 17.111080169677734,
      "learning_rate": 9.562524196670538e-06,
      "loss": 1.5706,
      "step": 3600
    },
    {
      "epoch": 1.3941153697251258,
      "grad_norm": 24.19242286682129,
      "learning_rate": 9.56209403363875e-06,
      "loss": 1.4269,
      "step": 3601
    },
    {
      "epoch": 1.394502516453736,
      "grad_norm": 27.989076614379883,
      "learning_rate": 9.56166387060696e-06,
      "loss": 2.1827,
      "step": 3602
    },
    {
      "epoch": 1.394889663182346,
      "grad_norm": 23.679946899414062,
      "learning_rate": 9.561233707575172e-06,
      "loss": 1.7967,
      "step": 3603
    },
    {
      "epoch": 1.3952768099109563,
      "grad_norm": 17.50737953186035,
      "learning_rate": 9.560803544543382e-06,
      "loss": 1.2811,
      "step": 3604
    },
    {
      "epoch": 1.3956639566395663,
      "grad_norm": 15.845333099365234,
      "learning_rate": 9.560373381511593e-06,
      "loss": 1.6614,
      "step": 3605
    },
    {
      "epoch": 1.3960511033681766,
      "grad_norm": 28.210250854492188,
      "learning_rate": 9.559943218479805e-06,
      "loss": 1.2948,
      "step": 3606
    },
    {
      "epoch": 1.3964382500967867,
      "grad_norm": 25.887392044067383,
      "learning_rate": 9.559513055448015e-06,
      "loss": 1.8591,
      "step": 3607
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 27.38494300842285,
      "learning_rate": 9.559082892416226e-06,
      "loss": 1.7259,
      "step": 3608
    },
    {
      "epoch": 1.397212543554007,
      "grad_norm": 16.731182098388672,
      "learning_rate": 9.558652729384437e-06,
      "loss": 1.7935,
      "step": 3609
    },
    {
      "epoch": 1.397599690282617,
      "grad_norm": 19.483205795288086,
      "learning_rate": 9.558222566352649e-06,
      "loss": 1.0147,
      "step": 3610
    },
    {
      "epoch": 1.3979868370112274,
      "grad_norm": 22.293371200561523,
      "learning_rate": 9.557792403320858e-06,
      "loss": 1.4724,
      "step": 3611
    },
    {
      "epoch": 1.3983739837398375,
      "grad_norm": 21.144023895263672,
      "learning_rate": 9.557362240289072e-06,
      "loss": 1.6779,
      "step": 3612
    },
    {
      "epoch": 1.3987611304684475,
      "grad_norm": 15.23646354675293,
      "learning_rate": 9.556932077257281e-06,
      "loss": 1.1107,
      "step": 3613
    },
    {
      "epoch": 1.3991482771970576,
      "grad_norm": 21.523151397705078,
      "learning_rate": 9.556501914225493e-06,
      "loss": 1.6168,
      "step": 3614
    },
    {
      "epoch": 1.3995354239256679,
      "grad_norm": 14.18027114868164,
      "learning_rate": 9.556071751193702e-06,
      "loss": 0.7514,
      "step": 3615
    },
    {
      "epoch": 1.399922570654278,
      "grad_norm": 33.01518249511719,
      "learning_rate": 9.555641588161914e-06,
      "loss": 1.5116,
      "step": 3616
    },
    {
      "epoch": 1.4003097173828882,
      "grad_norm": 13.545287132263184,
      "learning_rate": 9.555211425130125e-06,
      "loss": 1.1868,
      "step": 3617
    },
    {
      "epoch": 1.4006968641114983,
      "grad_norm": 15.072162628173828,
      "learning_rate": 9.554781262098337e-06,
      "loss": 0.7824,
      "step": 3618
    },
    {
      "epoch": 1.4010840108401084,
      "grad_norm": 21.856178283691406,
      "learning_rate": 9.554351099066546e-06,
      "loss": 2.3681,
      "step": 3619
    },
    {
      "epoch": 1.4014711575687184,
      "grad_norm": 13.244826316833496,
      "learning_rate": 9.553920936034758e-06,
      "loss": 1.206,
      "step": 3620
    },
    {
      "epoch": 1.4018583042973287,
      "grad_norm": 10.352575302124023,
      "learning_rate": 9.55349077300297e-06,
      "loss": 1.3016,
      "step": 3621
    },
    {
      "epoch": 1.4022454510259388,
      "grad_norm": 16.94053840637207,
      "learning_rate": 9.553060609971179e-06,
      "loss": 1.8456,
      "step": 3622
    },
    {
      "epoch": 1.402632597754549,
      "grad_norm": 9.614395141601562,
      "learning_rate": 9.55263044693939e-06,
      "loss": 1.295,
      "step": 3623
    },
    {
      "epoch": 1.4030197444831591,
      "grad_norm": 19.890670776367188,
      "learning_rate": 9.552200283907602e-06,
      "loss": 2.4076,
      "step": 3624
    },
    {
      "epoch": 1.4034068912117692,
      "grad_norm": 15.93909740447998,
      "learning_rate": 9.551770120875813e-06,
      "loss": 1.1148,
      "step": 3625
    },
    {
      "epoch": 1.4037940379403793,
      "grad_norm": 21.39826202392578,
      "learning_rate": 9.551339957844023e-06,
      "loss": 2.1967,
      "step": 3626
    },
    {
      "epoch": 1.4041811846689896,
      "grad_norm": 12.765335083007812,
      "learning_rate": 9.550909794812234e-06,
      "loss": 1.4088,
      "step": 3627
    },
    {
      "epoch": 1.4045683313975996,
      "grad_norm": 47.098960876464844,
      "learning_rate": 9.550479631780446e-06,
      "loss": 3.004,
      "step": 3628
    },
    {
      "epoch": 1.40495547812621,
      "grad_norm": 25.874513626098633,
      "learning_rate": 9.550049468748657e-06,
      "loss": 1.6365,
      "step": 3629
    },
    {
      "epoch": 1.40534262485482,
      "grad_norm": 12.27521800994873,
      "learning_rate": 9.549619305716867e-06,
      "loss": 1.203,
      "step": 3630
    },
    {
      "epoch": 1.40572977158343,
      "grad_norm": 14.119560241699219,
      "learning_rate": 9.549189142685078e-06,
      "loss": 1.5502,
      "step": 3631
    },
    {
      "epoch": 1.4061169183120403,
      "grad_norm": 11.076017379760742,
      "learning_rate": 9.54875897965329e-06,
      "loss": 0.4331,
      "step": 3632
    },
    {
      "epoch": 1.4065040650406504,
      "grad_norm": 15.187954902648926,
      "learning_rate": 9.548328816621501e-06,
      "loss": 1.2142,
      "step": 3633
    },
    {
      "epoch": 1.4068912117692607,
      "grad_norm": 17.457950592041016,
      "learning_rate": 9.547898653589711e-06,
      "loss": 1.4189,
      "step": 3634
    },
    {
      "epoch": 1.4072783584978708,
      "grad_norm": 14.904985427856445,
      "learning_rate": 9.547468490557922e-06,
      "loss": 1.675,
      "step": 3635
    },
    {
      "epoch": 1.4076655052264808,
      "grad_norm": 9.059663772583008,
      "learning_rate": 9.547038327526134e-06,
      "loss": 0.6198,
      "step": 3636
    },
    {
      "epoch": 1.4080526519550909,
      "grad_norm": 18.54106903076172,
      "learning_rate": 9.546608164494343e-06,
      "loss": 1.1544,
      "step": 3637
    },
    {
      "epoch": 1.4084397986837012,
      "grad_norm": 16.67202377319336,
      "learning_rate": 9.546178001462555e-06,
      "loss": 1.8065,
      "step": 3638
    },
    {
      "epoch": 1.4088269454123112,
      "grad_norm": 15.585121154785156,
      "learning_rate": 9.545747838430766e-06,
      "loss": 1.1459,
      "step": 3639
    },
    {
      "epoch": 1.4092140921409215,
      "grad_norm": 13.68992805480957,
      "learning_rate": 9.545317675398978e-06,
      "loss": 0.9093,
      "step": 3640
    },
    {
      "epoch": 1.4096012388695316,
      "grad_norm": 12.294504165649414,
      "learning_rate": 9.544887512367187e-06,
      "loss": 1.047,
      "step": 3641
    },
    {
      "epoch": 1.4099883855981417,
      "grad_norm": 17.756256103515625,
      "learning_rate": 9.544457349335399e-06,
      "loss": 0.8328,
      "step": 3642
    },
    {
      "epoch": 1.4103755323267517,
      "grad_norm": 30.489517211914062,
      "learning_rate": 9.544027186303609e-06,
      "loss": 2.352,
      "step": 3643
    },
    {
      "epoch": 1.410762679055362,
      "grad_norm": 17.20305633544922,
      "learning_rate": 9.543597023271822e-06,
      "loss": 0.9017,
      "step": 3644
    },
    {
      "epoch": 1.411149825783972,
      "grad_norm": 17.982446670532227,
      "learning_rate": 9.543166860240031e-06,
      "loss": 1.3991,
      "step": 3645
    },
    {
      "epoch": 1.4115369725125824,
      "grad_norm": 18.239267349243164,
      "learning_rate": 9.542736697208243e-06,
      "loss": 1.6473,
      "step": 3646
    },
    {
      "epoch": 1.4119241192411924,
      "grad_norm": 12.946782112121582,
      "learning_rate": 9.542306534176453e-06,
      "loss": 0.4725,
      "step": 3647
    },
    {
      "epoch": 1.4123112659698025,
      "grad_norm": 20.65831184387207,
      "learning_rate": 9.541876371144666e-06,
      "loss": 1.1565,
      "step": 3648
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 10.483732223510742,
      "learning_rate": 9.541446208112875e-06,
      "loss": 1.2742,
      "step": 3649
    },
    {
      "epoch": 1.4130855594270229,
      "grad_norm": 31.258920669555664,
      "learning_rate": 9.541016045081087e-06,
      "loss": 2.7283,
      "step": 3650
    },
    {
      "epoch": 1.413472706155633,
      "grad_norm": 12.4324312210083,
      "learning_rate": 9.540585882049296e-06,
      "loss": 1.1082,
      "step": 3651
    },
    {
      "epoch": 1.4138598528842432,
      "grad_norm": 16.60248374938965,
      "learning_rate": 9.540155719017508e-06,
      "loss": 0.9952,
      "step": 3652
    },
    {
      "epoch": 1.4142469996128533,
      "grad_norm": 10.963432312011719,
      "learning_rate": 9.53972555598572e-06,
      "loss": 1.4476,
      "step": 3653
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 9.094389915466309,
      "learning_rate": 9.53929539295393e-06,
      "loss": 1.2848,
      "step": 3654
    },
    {
      "epoch": 1.4150212930700736,
      "grad_norm": 39.477359771728516,
      "learning_rate": 9.538865229922142e-06,
      "loss": 1.294,
      "step": 3655
    },
    {
      "epoch": 1.4154084397986837,
      "grad_norm": 19.827434539794922,
      "learning_rate": 9.538435066890352e-06,
      "loss": 1.6109,
      "step": 3656
    },
    {
      "epoch": 1.415795586527294,
      "grad_norm": 12.565638542175293,
      "learning_rate": 9.538004903858563e-06,
      "loss": 1.1223,
      "step": 3657
    },
    {
      "epoch": 1.416182733255904,
      "grad_norm": 10.880878448486328,
      "learning_rate": 9.537574740826773e-06,
      "loss": 1.0312,
      "step": 3658
    },
    {
      "epoch": 1.4165698799845141,
      "grad_norm": 22.723514556884766,
      "learning_rate": 9.537144577794986e-06,
      "loss": 1.9056,
      "step": 3659
    },
    {
      "epoch": 1.4169570267131242,
      "grad_norm": 23.09491539001465,
      "learning_rate": 9.536714414763196e-06,
      "loss": 1.5379,
      "step": 3660
    },
    {
      "epoch": 1.4173441734417345,
      "grad_norm": 15.963936805725098,
      "learning_rate": 9.536284251731407e-06,
      "loss": 1.5559,
      "step": 3661
    },
    {
      "epoch": 1.4177313201703445,
      "grad_norm": 19.110506057739258,
      "learning_rate": 9.535854088699617e-06,
      "loss": 1.9317,
      "step": 3662
    },
    {
      "epoch": 1.4181184668989548,
      "grad_norm": 39.30975341796875,
      "learning_rate": 9.53542392566783e-06,
      "loss": 1.6755,
      "step": 3663
    },
    {
      "epoch": 1.4185056136275649,
      "grad_norm": 21.616533279418945,
      "learning_rate": 9.53499376263604e-06,
      "loss": 2.074,
      "step": 3664
    },
    {
      "epoch": 1.418892760356175,
      "grad_norm": 16.180273056030273,
      "learning_rate": 9.534563599604251e-06,
      "loss": 1.8374,
      "step": 3665
    },
    {
      "epoch": 1.419279907084785,
      "grad_norm": 32.42030715942383,
      "learning_rate": 9.534133436572461e-06,
      "loss": 1.9887,
      "step": 3666
    },
    {
      "epoch": 1.4196670538133953,
      "grad_norm": 21.925540924072266,
      "learning_rate": 9.533703273540672e-06,
      "loss": 1.6288,
      "step": 3667
    },
    {
      "epoch": 1.4200542005420054,
      "grad_norm": 21.330652236938477,
      "learning_rate": 9.533273110508884e-06,
      "loss": 1.628,
      "step": 3668
    },
    {
      "epoch": 1.4204413472706157,
      "grad_norm": 28.615720748901367,
      "learning_rate": 9.532842947477095e-06,
      "loss": 2.5625,
      "step": 3669
    },
    {
      "epoch": 1.4208284939992257,
      "grad_norm": 26.141355514526367,
      "learning_rate": 9.532412784445305e-06,
      "loss": 1.3038,
      "step": 3670
    },
    {
      "epoch": 1.4212156407278358,
      "grad_norm": 14.833569526672363,
      "learning_rate": 9.531982621413516e-06,
      "loss": 0.9084,
      "step": 3671
    },
    {
      "epoch": 1.4216027874564459,
      "grad_norm": 15.940132141113281,
      "learning_rate": 9.531552458381728e-06,
      "loss": 1.5376,
      "step": 3672
    },
    {
      "epoch": 1.4219899341850561,
      "grad_norm": 12.231499671936035,
      "learning_rate": 9.531122295349937e-06,
      "loss": 0.7937,
      "step": 3673
    },
    {
      "epoch": 1.4223770809136662,
      "grad_norm": 16.32278060913086,
      "learning_rate": 9.530692132318149e-06,
      "loss": 1.4239,
      "step": 3674
    },
    {
      "epoch": 1.4227642276422765,
      "grad_norm": 16.540245056152344,
      "learning_rate": 9.53026196928636e-06,
      "loss": 1.1907,
      "step": 3675
    },
    {
      "epoch": 1.4231513743708866,
      "grad_norm": 16.743633270263672,
      "learning_rate": 9.529831806254572e-06,
      "loss": 1.6008,
      "step": 3676
    },
    {
      "epoch": 1.4235385210994966,
      "grad_norm": 20.99725914001465,
      "learning_rate": 9.529401643222781e-06,
      "loss": 2.3794,
      "step": 3677
    },
    {
      "epoch": 1.423925667828107,
      "grad_norm": 16.698938369750977,
      "learning_rate": 9.528971480190993e-06,
      "loss": 1.3365,
      "step": 3678
    },
    {
      "epoch": 1.424312814556717,
      "grad_norm": 24.623876571655273,
      "learning_rate": 9.528541317159204e-06,
      "loss": 1.316,
      "step": 3679
    },
    {
      "epoch": 1.4246999612853273,
      "grad_norm": 33.833465576171875,
      "learning_rate": 9.528111154127416e-06,
      "loss": 2.2556,
      "step": 3680
    },
    {
      "epoch": 1.4250871080139373,
      "grad_norm": 14.53125286102295,
      "learning_rate": 9.527680991095625e-06,
      "loss": 0.9216,
      "step": 3681
    },
    {
      "epoch": 1.4254742547425474,
      "grad_norm": 12.110595703125,
      "learning_rate": 9.527250828063837e-06,
      "loss": 0.8871,
      "step": 3682
    },
    {
      "epoch": 1.4258614014711575,
      "grad_norm": 21.05938720703125,
      "learning_rate": 9.526820665032048e-06,
      "loss": 1.9293,
      "step": 3683
    },
    {
      "epoch": 1.4262485481997678,
      "grad_norm": 34.108642578125,
      "learning_rate": 9.52639050200026e-06,
      "loss": 1.6278,
      "step": 3684
    },
    {
      "epoch": 1.4266356949283778,
      "grad_norm": 18.04875946044922,
      "learning_rate": 9.52596033896847e-06,
      "loss": 1.5399,
      "step": 3685
    },
    {
      "epoch": 1.4270228416569881,
      "grad_norm": 19.8041934967041,
      "learning_rate": 9.52553017593668e-06,
      "loss": 1.0782,
      "step": 3686
    },
    {
      "epoch": 1.4274099883855982,
      "grad_norm": 20.535764694213867,
      "learning_rate": 9.525100012904892e-06,
      "loss": 1.1176,
      "step": 3687
    },
    {
      "epoch": 1.4277971351142082,
      "grad_norm": 23.34959602355957,
      "learning_rate": 9.524669849873102e-06,
      "loss": 1.1379,
      "step": 3688
    },
    {
      "epoch": 1.4281842818428183,
      "grad_norm": 26.55074691772461,
      "learning_rate": 9.524239686841313e-06,
      "loss": 2.3257,
      "step": 3689
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 21.377174377441406,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.9511,
      "step": 3690
    },
    {
      "epoch": 1.4289585753000387,
      "grad_norm": 24.351844787597656,
      "learning_rate": 9.523379360777736e-06,
      "loss": 1.3327,
      "step": 3691
    },
    {
      "epoch": 1.429345722028649,
      "grad_norm": 18.027124404907227,
      "learning_rate": 9.522949197745946e-06,
      "loss": 1.452,
      "step": 3692
    },
    {
      "epoch": 1.429732868757259,
      "grad_norm": 22.35357093811035,
      "learning_rate": 9.522519034714157e-06,
      "loss": 1.2743,
      "step": 3693
    },
    {
      "epoch": 1.430120015485869,
      "grad_norm": 14.408860206604004,
      "learning_rate": 9.522088871682369e-06,
      "loss": 1.0611,
      "step": 3694
    },
    {
      "epoch": 1.4305071622144792,
      "grad_norm": 25.08940887451172,
      "learning_rate": 9.52165870865058e-06,
      "loss": 1.6741,
      "step": 3695
    },
    {
      "epoch": 1.4308943089430894,
      "grad_norm": 17.658222198486328,
      "learning_rate": 9.52122854561879e-06,
      "loss": 1.5993,
      "step": 3696
    },
    {
      "epoch": 1.4312814556716995,
      "grad_norm": 14.764775276184082,
      "learning_rate": 9.520798382587001e-06,
      "loss": 0.9409,
      "step": 3697
    },
    {
      "epoch": 1.4316686024003098,
      "grad_norm": 17.272924423217773,
      "learning_rate": 9.520368219555213e-06,
      "loss": 1.8102,
      "step": 3698
    },
    {
      "epoch": 1.4320557491289199,
      "grad_norm": 18.6186466217041,
      "learning_rate": 9.519938056523424e-06,
      "loss": 1.9165,
      "step": 3699
    },
    {
      "epoch": 1.43244289585753,
      "grad_norm": 26.37054443359375,
      "learning_rate": 9.519507893491634e-06,
      "loss": 1.4913,
      "step": 3700
    },
    {
      "epoch": 1.4328300425861402,
      "grad_norm": 17.893463134765625,
      "learning_rate": 9.519077730459845e-06,
      "loss": 0.7429,
      "step": 3701
    },
    {
      "epoch": 1.4332171893147503,
      "grad_norm": 47.69562530517578,
      "learning_rate": 9.518647567428057e-06,
      "loss": 1.311,
      "step": 3702
    },
    {
      "epoch": 1.4336043360433606,
      "grad_norm": 17.906286239624023,
      "learning_rate": 9.518217404396266e-06,
      "loss": 2.0256,
      "step": 3703
    },
    {
      "epoch": 1.4339914827719706,
      "grad_norm": 25.152368545532227,
      "learning_rate": 9.517787241364478e-06,
      "loss": 1.5511,
      "step": 3704
    },
    {
      "epoch": 1.4343786295005807,
      "grad_norm": 14.436120986938477,
      "learning_rate": 9.51735707833269e-06,
      "loss": 1.0532,
      "step": 3705
    },
    {
      "epoch": 1.4347657762291908,
      "grad_norm": 27.525239944458008,
      "learning_rate": 9.5169269153009e-06,
      "loss": 1.1882,
      "step": 3706
    },
    {
      "epoch": 1.435152922957801,
      "grad_norm": 22.73465347290039,
      "learning_rate": 9.51649675226911e-06,
      "loss": 1.0503,
      "step": 3707
    },
    {
      "epoch": 1.4355400696864111,
      "grad_norm": 14.905771255493164,
      "learning_rate": 9.516066589237322e-06,
      "loss": 1.4635,
      "step": 3708
    },
    {
      "epoch": 1.4359272164150214,
      "grad_norm": 15.758349418640137,
      "learning_rate": 9.515636426205531e-06,
      "loss": 1.4036,
      "step": 3709
    },
    {
      "epoch": 1.4363143631436315,
      "grad_norm": 25.784732818603516,
      "learning_rate": 9.515206263173745e-06,
      "loss": 1.8916,
      "step": 3710
    },
    {
      "epoch": 1.4367015098722415,
      "grad_norm": 16.767595291137695,
      "learning_rate": 9.514776100141954e-06,
      "loss": 0.7201,
      "step": 3711
    },
    {
      "epoch": 1.4370886566008516,
      "grad_norm": 19.392375946044922,
      "learning_rate": 9.514345937110166e-06,
      "loss": 0.7126,
      "step": 3712
    },
    {
      "epoch": 1.437475803329462,
      "grad_norm": 20.6655330657959,
      "learning_rate": 9.513915774078375e-06,
      "loss": 1.9479,
      "step": 3713
    },
    {
      "epoch": 1.437862950058072,
      "grad_norm": 16.872249603271484,
      "learning_rate": 9.513485611046589e-06,
      "loss": 1.4229,
      "step": 3714
    },
    {
      "epoch": 1.4382500967866823,
      "grad_norm": 12.847192764282227,
      "learning_rate": 9.513055448014798e-06,
      "loss": 1.0242,
      "step": 3715
    },
    {
      "epoch": 1.4386372435152923,
      "grad_norm": 20.74510955810547,
      "learning_rate": 9.51262528498301e-06,
      "loss": 1.0771,
      "step": 3716
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 14.493424415588379,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.0278,
      "step": 3717
    },
    {
      "epoch": 1.4394115369725125,
      "grad_norm": 21.907644271850586,
      "learning_rate": 9.51176495891943e-06,
      "loss": 2.3762,
      "step": 3718
    },
    {
      "epoch": 1.4397986837011227,
      "grad_norm": 7.012363433837891,
      "learning_rate": 9.511334795887642e-06,
      "loss": 0.4538,
      "step": 3719
    },
    {
      "epoch": 1.4401858304297328,
      "grad_norm": 21.908979415893555,
      "learning_rate": 9.510904632855854e-06,
      "loss": 1.2769,
      "step": 3720
    },
    {
      "epoch": 1.440572977158343,
      "grad_norm": 12.86209487915039,
      "learning_rate": 9.510474469824063e-06,
      "loss": 1.1657,
      "step": 3721
    },
    {
      "epoch": 1.4409601238869532,
      "grad_norm": 20.734710693359375,
      "learning_rate": 9.510044306792275e-06,
      "loss": 1.6851,
      "step": 3722
    },
    {
      "epoch": 1.4413472706155632,
      "grad_norm": 13.306396484375,
      "learning_rate": 9.509614143760486e-06,
      "loss": 1.2233,
      "step": 3723
    },
    {
      "epoch": 1.4417344173441735,
      "grad_norm": 20.721832275390625,
      "learning_rate": 9.509183980728696e-06,
      "loss": 1.0985,
      "step": 3724
    },
    {
      "epoch": 1.4421215640727836,
      "grad_norm": 15.491926193237305,
      "learning_rate": 9.508753817696907e-06,
      "loss": 1.3153,
      "step": 3725
    },
    {
      "epoch": 1.4425087108013936,
      "grad_norm": 14.215641975402832,
      "learning_rate": 9.508323654665119e-06,
      "loss": 1.1314,
      "step": 3726
    },
    {
      "epoch": 1.442895857530004,
      "grad_norm": 12.457674026489258,
      "learning_rate": 9.50789349163333e-06,
      "loss": 2.1562,
      "step": 3727
    },
    {
      "epoch": 1.443283004258614,
      "grad_norm": 13.595939636230469,
      "learning_rate": 9.50746332860154e-06,
      "loss": 1.7776,
      "step": 3728
    },
    {
      "epoch": 1.443670150987224,
      "grad_norm": 18.86102294921875,
      "learning_rate": 9.507033165569751e-06,
      "loss": 1.2841,
      "step": 3729
    },
    {
      "epoch": 1.4440572977158344,
      "grad_norm": 22.3024959564209,
      "learning_rate": 9.506603002537963e-06,
      "loss": 1.6669,
      "step": 3730
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 14.186941146850586,
      "learning_rate": 9.506172839506174e-06,
      "loss": 1.0717,
      "step": 3731
    },
    {
      "epoch": 1.4448315911730547,
      "grad_norm": 17.74237823486328,
      "learning_rate": 9.505742676474384e-06,
      "loss": 0.582,
      "step": 3732
    },
    {
      "epoch": 1.4452187379016648,
      "grad_norm": 19.653759002685547,
      "learning_rate": 9.505312513442595e-06,
      "loss": 1.6184,
      "step": 3733
    },
    {
      "epoch": 1.4456058846302748,
      "grad_norm": 17.72967529296875,
      "learning_rate": 9.504882350410807e-06,
      "loss": 1.7947,
      "step": 3734
    },
    {
      "epoch": 1.445993031358885,
      "grad_norm": 9.57983684539795,
      "learning_rate": 9.504452187379018e-06,
      "loss": 1.3496,
      "step": 3735
    },
    {
      "epoch": 1.4463801780874952,
      "grad_norm": 25.59796905517578,
      "learning_rate": 9.504022024347228e-06,
      "loss": 2.5934,
      "step": 3736
    },
    {
      "epoch": 1.4467673248161053,
      "grad_norm": 18.657522201538086,
      "learning_rate": 9.50359186131544e-06,
      "loss": 1.7724,
      "step": 3737
    },
    {
      "epoch": 1.4471544715447155,
      "grad_norm": 11.905975341796875,
      "learning_rate": 9.50316169828365e-06,
      "loss": 0.8264,
      "step": 3738
    },
    {
      "epoch": 1.4475416182733256,
      "grad_norm": 15.931320190429688,
      "learning_rate": 9.50273153525186e-06,
      "loss": 1.762,
      "step": 3739
    },
    {
      "epoch": 1.4479287650019357,
      "grad_norm": 16.423171997070312,
      "learning_rate": 9.502301372220072e-06,
      "loss": 1.8067,
      "step": 3740
    },
    {
      "epoch": 1.4483159117305457,
      "grad_norm": 19.281112670898438,
      "learning_rate": 9.501871209188283e-06,
      "loss": 1.1367,
      "step": 3741
    },
    {
      "epoch": 1.448703058459156,
      "grad_norm": 20.58602523803711,
      "learning_rate": 9.501441046156495e-06,
      "loss": 1.7134,
      "step": 3742
    },
    {
      "epoch": 1.449090205187766,
      "grad_norm": 13.015419960021973,
      "learning_rate": 9.501010883124704e-06,
      "loss": 1.4343,
      "step": 3743
    },
    {
      "epoch": 1.4494773519163764,
      "grad_norm": 23.519901275634766,
      "learning_rate": 9.500580720092916e-06,
      "loss": 1.346,
      "step": 3744
    },
    {
      "epoch": 1.4498644986449865,
      "grad_norm": 15.124411582946777,
      "learning_rate": 9.500150557061127e-06,
      "loss": 1.0134,
      "step": 3745
    },
    {
      "epoch": 1.4502516453735965,
      "grad_norm": 16.432985305786133,
      "learning_rate": 9.499720394029339e-06,
      "loss": 1.7082,
      "step": 3746
    },
    {
      "epoch": 1.4506387921022068,
      "grad_norm": 18.928054809570312,
      "learning_rate": 9.499290230997548e-06,
      "loss": 2.0152,
      "step": 3747
    },
    {
      "epoch": 1.4510259388308169,
      "grad_norm": 18.145671844482422,
      "learning_rate": 9.49886006796576e-06,
      "loss": 1.3595,
      "step": 3748
    },
    {
      "epoch": 1.451413085559427,
      "grad_norm": 34.345829010009766,
      "learning_rate": 9.498429904933971e-06,
      "loss": 1.9037,
      "step": 3749
    },
    {
      "epoch": 1.4518002322880372,
      "grad_norm": 33.20063018798828,
      "learning_rate": 9.497999741902183e-06,
      "loss": 2.0492,
      "step": 3750
    },
    {
      "epoch": 1.4521873790166473,
      "grad_norm": 11.700613021850586,
      "learning_rate": 9.497569578870392e-06,
      "loss": 1.0773,
      "step": 3751
    },
    {
      "epoch": 1.4525745257452574,
      "grad_norm": 13.939396858215332,
      "learning_rate": 9.497139415838604e-06,
      "loss": 1.0199,
      "step": 3752
    },
    {
      "epoch": 1.4529616724738676,
      "grad_norm": 18.99810791015625,
      "learning_rate": 9.496709252806815e-06,
      "loss": 1.8536,
      "step": 3753
    },
    {
      "epoch": 1.4533488192024777,
      "grad_norm": 12.221540451049805,
      "learning_rate": 9.496279089775025e-06,
      "loss": 1.9668,
      "step": 3754
    },
    {
      "epoch": 1.453735965931088,
      "grad_norm": 19.287878036499023,
      "learning_rate": 9.495848926743236e-06,
      "loss": 1.7545,
      "step": 3755
    },
    {
      "epoch": 1.454123112659698,
      "grad_norm": 41.00162887573242,
      "learning_rate": 9.495418763711448e-06,
      "loss": 1.5359,
      "step": 3756
    },
    {
      "epoch": 1.4545102593883081,
      "grad_norm": 17.30933952331543,
      "learning_rate": 9.494988600679659e-06,
      "loss": 1.513,
      "step": 3757
    },
    {
      "epoch": 1.4548974061169182,
      "grad_norm": 16.779428482055664,
      "learning_rate": 9.494558437647869e-06,
      "loss": 1.5719,
      "step": 3758
    },
    {
      "epoch": 1.4552845528455285,
      "grad_norm": 15.078276634216309,
      "learning_rate": 9.49412827461608e-06,
      "loss": 1.6094,
      "step": 3759
    },
    {
      "epoch": 1.4556716995741386,
      "grad_norm": 15.486088752746582,
      "learning_rate": 9.49369811158429e-06,
      "loss": 1.1272,
      "step": 3760
    },
    {
      "epoch": 1.4560588463027488,
      "grad_norm": 13.495430946350098,
      "learning_rate": 9.493267948552503e-06,
      "loss": 1.4662,
      "step": 3761
    },
    {
      "epoch": 1.456445993031359,
      "grad_norm": 17.047365188598633,
      "learning_rate": 9.492837785520713e-06,
      "loss": 1.7809,
      "step": 3762
    },
    {
      "epoch": 1.456833139759969,
      "grad_norm": 13.938576698303223,
      "learning_rate": 9.492407622488924e-06,
      "loss": 1.1026,
      "step": 3763
    },
    {
      "epoch": 1.457220286488579,
      "grad_norm": 21.14166259765625,
      "learning_rate": 9.491977459457134e-06,
      "loss": 0.995,
      "step": 3764
    },
    {
      "epoch": 1.4576074332171893,
      "grad_norm": 23.95529556274414,
      "learning_rate": 9.491547296425347e-06,
      "loss": 1.5835,
      "step": 3765
    },
    {
      "epoch": 1.4579945799457994,
      "grad_norm": 15.530014991760254,
      "learning_rate": 9.491117133393557e-06,
      "loss": 1.0761,
      "step": 3766
    },
    {
      "epoch": 1.4583817266744097,
      "grad_norm": 12.042799949645996,
      "learning_rate": 9.490686970361768e-06,
      "loss": 0.9698,
      "step": 3767
    },
    {
      "epoch": 1.4587688734030198,
      "grad_norm": 13.809616088867188,
      "learning_rate": 9.490256807329978e-06,
      "loss": 1.8818,
      "step": 3768
    },
    {
      "epoch": 1.4591560201316298,
      "grad_norm": 25.756975173950195,
      "learning_rate": 9.48982664429819e-06,
      "loss": 1.9839,
      "step": 3769
    },
    {
      "epoch": 1.45954316686024,
      "grad_norm": 19.921998977661133,
      "learning_rate": 9.4893964812664e-06,
      "loss": 1.9301,
      "step": 3770
    },
    {
      "epoch": 1.4599303135888502,
      "grad_norm": 32.842315673828125,
      "learning_rate": 9.488966318234612e-06,
      "loss": 1.5553,
      "step": 3771
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 8.364005088806152,
      "learning_rate": 9.488536155202822e-06,
      "loss": 0.3347,
      "step": 3772
    },
    {
      "epoch": 1.4607046070460705,
      "grad_norm": 14.907505989074707,
      "learning_rate": 9.488105992171033e-06,
      "loss": 1.1152,
      "step": 3773
    },
    {
      "epoch": 1.4610917537746806,
      "grad_norm": 13.670364379882812,
      "learning_rate": 9.487675829139245e-06,
      "loss": 1.4592,
      "step": 3774
    },
    {
      "epoch": 1.4614789005032907,
      "grad_norm": 20.31901741027832,
      "learning_rate": 9.487245666107454e-06,
      "loss": 1.7564,
      "step": 3775
    },
    {
      "epoch": 1.461866047231901,
      "grad_norm": 26.41939353942871,
      "learning_rate": 9.486815503075666e-06,
      "loss": 1.0709,
      "step": 3776
    },
    {
      "epoch": 1.462253193960511,
      "grad_norm": 17.201210021972656,
      "learning_rate": 9.486385340043877e-06,
      "loss": 2.4491,
      "step": 3777
    },
    {
      "epoch": 1.4626403406891213,
      "grad_norm": 24.953027725219727,
      "learning_rate": 9.485955177012089e-06,
      "loss": 1.5614,
      "step": 3778
    },
    {
      "epoch": 1.4630274874177314,
      "grad_norm": 17.116172790527344,
      "learning_rate": 9.485525013980298e-06,
      "loss": 1.5657,
      "step": 3779
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 21.199731826782227,
      "learning_rate": 9.485094850948512e-06,
      "loss": 1.2007,
      "step": 3780
    },
    {
      "epoch": 1.4638017808749515,
      "grad_norm": 22.08456802368164,
      "learning_rate": 9.484664687916721e-06,
      "loss": 1.8944,
      "step": 3781
    },
    {
      "epoch": 1.4641889276035618,
      "grad_norm": 22.918039321899414,
      "learning_rate": 9.484234524884933e-06,
      "loss": 1.3168,
      "step": 3782
    },
    {
      "epoch": 1.4645760743321719,
      "grad_norm": 22.70132827758789,
      "learning_rate": 9.483804361853142e-06,
      "loss": 1.965,
      "step": 3783
    },
    {
      "epoch": 1.4649632210607821,
      "grad_norm": 18.425886154174805,
      "learning_rate": 9.483374198821354e-06,
      "loss": 1.7478,
      "step": 3784
    },
    {
      "epoch": 1.4653503677893922,
      "grad_norm": 14.878325462341309,
      "learning_rate": 9.482944035789565e-06,
      "loss": 1.3124,
      "step": 3785
    },
    {
      "epoch": 1.4657375145180023,
      "grad_norm": 17.699195861816406,
      "learning_rate": 9.482513872757777e-06,
      "loss": 1.5911,
      "step": 3786
    },
    {
      "epoch": 1.4661246612466123,
      "grad_norm": 14.30493450164795,
      "learning_rate": 9.482083709725986e-06,
      "loss": 1.0362,
      "step": 3787
    },
    {
      "epoch": 1.4665118079752226,
      "grad_norm": 32.10088348388672,
      "learning_rate": 9.481653546694198e-06,
      "loss": 1.5444,
      "step": 3788
    },
    {
      "epoch": 1.4668989547038327,
      "grad_norm": 14.254013061523438,
      "learning_rate": 9.48122338366241e-06,
      "loss": 1.0433,
      "step": 3789
    },
    {
      "epoch": 1.467286101432443,
      "grad_norm": 13.330840110778809,
      "learning_rate": 9.480793220630619e-06,
      "loss": 1.4255,
      "step": 3790
    },
    {
      "epoch": 1.467673248161053,
      "grad_norm": 12.750984191894531,
      "learning_rate": 9.48036305759883e-06,
      "loss": 0.9021,
      "step": 3791
    },
    {
      "epoch": 1.4680603948896631,
      "grad_norm": 13.2025785446167,
      "learning_rate": 9.479932894567042e-06,
      "loss": 1.4881,
      "step": 3792
    },
    {
      "epoch": 1.4684475416182734,
      "grad_norm": 29.27882957458496,
      "learning_rate": 9.479502731535253e-06,
      "loss": 1.4746,
      "step": 3793
    },
    {
      "epoch": 1.4688346883468835,
      "grad_norm": 23.47199249267578,
      "learning_rate": 9.479072568503463e-06,
      "loss": 1.3723,
      "step": 3794
    },
    {
      "epoch": 1.4692218350754935,
      "grad_norm": 15.17221736907959,
      "learning_rate": 9.478642405471674e-06,
      "loss": 1.0551,
      "step": 3795
    },
    {
      "epoch": 1.4696089818041038,
      "grad_norm": 26.623550415039062,
      "learning_rate": 9.478212242439886e-06,
      "loss": 2.2621,
      "step": 3796
    },
    {
      "epoch": 1.4699961285327139,
      "grad_norm": 12.838220596313477,
      "learning_rate": 9.477782079408097e-06,
      "loss": 1.3606,
      "step": 3797
    },
    {
      "epoch": 1.470383275261324,
      "grad_norm": 20.185503005981445,
      "learning_rate": 9.477351916376307e-06,
      "loss": 2.2873,
      "step": 3798
    },
    {
      "epoch": 1.4707704219899342,
      "grad_norm": 20.242918014526367,
      "learning_rate": 9.476921753344518e-06,
      "loss": 1.7351,
      "step": 3799
    },
    {
      "epoch": 1.4711575687185443,
      "grad_norm": 15.935962677001953,
      "learning_rate": 9.47649159031273e-06,
      "loss": 1.4324,
      "step": 3800
    },
    {
      "epoch": 1.4715447154471546,
      "grad_norm": 13.154728889465332,
      "learning_rate": 9.476061427280941e-06,
      "loss": 0.9234,
      "step": 3801
    },
    {
      "epoch": 1.4719318621757647,
      "grad_norm": 19.310834884643555,
      "learning_rate": 9.47563126424915e-06,
      "loss": 1.9493,
      "step": 3802
    },
    {
      "epoch": 1.4723190089043747,
      "grad_norm": 15.071966171264648,
      "learning_rate": 9.475201101217362e-06,
      "loss": 1.6068,
      "step": 3803
    },
    {
      "epoch": 1.4727061556329848,
      "grad_norm": 19.867849349975586,
      "learning_rate": 9.474770938185574e-06,
      "loss": 1.7278,
      "step": 3804
    },
    {
      "epoch": 1.473093302361595,
      "grad_norm": 18.536296844482422,
      "learning_rate": 9.474340775153783e-06,
      "loss": 1.6153,
      "step": 3805
    },
    {
      "epoch": 1.4734804490902051,
      "grad_norm": 14.365636825561523,
      "learning_rate": 9.473910612121995e-06,
      "loss": 0.594,
      "step": 3806
    },
    {
      "epoch": 1.4738675958188154,
      "grad_norm": 12.324121475219727,
      "learning_rate": 9.473480449090206e-06,
      "loss": 0.7482,
      "step": 3807
    },
    {
      "epoch": 1.4742547425474255,
      "grad_norm": 17.62993812561035,
      "learning_rate": 9.473050286058418e-06,
      "loss": 1.3446,
      "step": 3808
    },
    {
      "epoch": 1.4746418892760356,
      "grad_norm": 13.91637134552002,
      "learning_rate": 9.472620123026627e-06,
      "loss": 1.1237,
      "step": 3809
    },
    {
      "epoch": 1.4750290360046456,
      "grad_norm": 23.11749267578125,
      "learning_rate": 9.472189959994839e-06,
      "loss": 1.644,
      "step": 3810
    },
    {
      "epoch": 1.475416182733256,
      "grad_norm": 15.406393051147461,
      "learning_rate": 9.471759796963048e-06,
      "loss": 1.3487,
      "step": 3811
    },
    {
      "epoch": 1.475803329461866,
      "grad_norm": 22.918996810913086,
      "learning_rate": 9.471329633931262e-06,
      "loss": 2.5641,
      "step": 3812
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 19.608362197875977,
      "learning_rate": 9.470899470899471e-06,
      "loss": 1.9891,
      "step": 3813
    },
    {
      "epoch": 1.4765776229190863,
      "grad_norm": 22.134769439697266,
      "learning_rate": 9.470469307867683e-06,
      "loss": 1.7572,
      "step": 3814
    },
    {
      "epoch": 1.4769647696476964,
      "grad_norm": 20.426788330078125,
      "learning_rate": 9.470039144835892e-06,
      "loss": 1.3754,
      "step": 3815
    },
    {
      "epoch": 1.4773519163763067,
      "grad_norm": 13.061929702758789,
      "learning_rate": 9.469608981804106e-06,
      "loss": 1.0661,
      "step": 3816
    },
    {
      "epoch": 1.4777390631049168,
      "grad_norm": 11.091650009155273,
      "learning_rate": 9.469178818772315e-06,
      "loss": 1.3372,
      "step": 3817
    },
    {
      "epoch": 1.4781262098335268,
      "grad_norm": 13.021749496459961,
      "learning_rate": 9.468748655740527e-06,
      "loss": 1.2059,
      "step": 3818
    },
    {
      "epoch": 1.4785133565621371,
      "grad_norm": 18.970430374145508,
      "learning_rate": 9.468318492708738e-06,
      "loss": 1.0455,
      "step": 3819
    },
    {
      "epoch": 1.4789005032907472,
      "grad_norm": 27.089801788330078,
      "learning_rate": 9.467888329676948e-06,
      "loss": 2.2514,
      "step": 3820
    },
    {
      "epoch": 1.4792876500193572,
      "grad_norm": 18.355806350708008,
      "learning_rate": 9.46745816664516e-06,
      "loss": 2.3771,
      "step": 3821
    },
    {
      "epoch": 1.4796747967479675,
      "grad_norm": 20.29918670654297,
      "learning_rate": 9.46702800361337e-06,
      "loss": 2.3622,
      "step": 3822
    },
    {
      "epoch": 1.4800619434765776,
      "grad_norm": 19.230262756347656,
      "learning_rate": 9.466597840581582e-06,
      "loss": 2.1505,
      "step": 3823
    },
    {
      "epoch": 1.480449090205188,
      "grad_norm": 19.211597442626953,
      "learning_rate": 9.466167677549792e-06,
      "loss": 1.9099,
      "step": 3824
    },
    {
      "epoch": 1.480836236933798,
      "grad_norm": 20.236154556274414,
      "learning_rate": 9.465737514518003e-06,
      "loss": 2.2555,
      "step": 3825
    },
    {
      "epoch": 1.481223383662408,
      "grad_norm": 14.391087532043457,
      "learning_rate": 9.465307351486213e-06,
      "loss": 1.8834,
      "step": 3826
    },
    {
      "epoch": 1.481610530391018,
      "grad_norm": 19.4555721282959,
      "learning_rate": 9.464877188454426e-06,
      "loss": 1.4196,
      "step": 3827
    },
    {
      "epoch": 1.4819976771196284,
      "grad_norm": 25.94155502319336,
      "learning_rate": 9.464447025422636e-06,
      "loss": 2.3253,
      "step": 3828
    },
    {
      "epoch": 1.4823848238482384,
      "grad_norm": 12.648688316345215,
      "learning_rate": 9.464016862390847e-06,
      "loss": 0.88,
      "step": 3829
    },
    {
      "epoch": 1.4827719705768487,
      "grad_norm": 14.392012596130371,
      "learning_rate": 9.463586699359057e-06,
      "loss": 1.4664,
      "step": 3830
    },
    {
      "epoch": 1.4831591173054588,
      "grad_norm": 26.383689880371094,
      "learning_rate": 9.46315653632727e-06,
      "loss": 1.5308,
      "step": 3831
    },
    {
      "epoch": 1.4835462640340689,
      "grad_norm": 17.51293182373047,
      "learning_rate": 9.46272637329548e-06,
      "loss": 2.0361,
      "step": 3832
    },
    {
      "epoch": 1.483933410762679,
      "grad_norm": 11.99791145324707,
      "learning_rate": 9.462296210263691e-06,
      "loss": 1.4206,
      "step": 3833
    },
    {
      "epoch": 1.4843205574912892,
      "grad_norm": 12.287755966186523,
      "learning_rate": 9.461866047231901e-06,
      "loss": 1.0482,
      "step": 3834
    },
    {
      "epoch": 1.4847077042198993,
      "grad_norm": 37.76327896118164,
      "learning_rate": 9.461435884200112e-06,
      "loss": 1.4854,
      "step": 3835
    },
    {
      "epoch": 1.4850948509485096,
      "grad_norm": 14.521543502807617,
      "learning_rate": 9.461005721168324e-06,
      "loss": 1.5964,
      "step": 3836
    },
    {
      "epoch": 1.4854819976771196,
      "grad_norm": 11.423442840576172,
      "learning_rate": 9.460575558136535e-06,
      "loss": 0.967,
      "step": 3837
    },
    {
      "epoch": 1.4858691444057297,
      "grad_norm": 11.223771095275879,
      "learning_rate": 9.460145395104745e-06,
      "loss": 1.3517,
      "step": 3838
    },
    {
      "epoch": 1.48625629113434,
      "grad_norm": 33.31816864013672,
      "learning_rate": 9.459715232072956e-06,
      "loss": 1.5945,
      "step": 3839
    },
    {
      "epoch": 1.48664343786295,
      "grad_norm": 12.590106964111328,
      "learning_rate": 9.459285069041168e-06,
      "loss": 0.8432,
      "step": 3840
    },
    {
      "epoch": 1.4870305845915601,
      "grad_norm": 12.973419189453125,
      "learning_rate": 9.458854906009377e-06,
      "loss": 0.9188,
      "step": 3841
    },
    {
      "epoch": 1.4874177313201704,
      "grad_norm": 14.774730682373047,
      "learning_rate": 9.458424742977589e-06,
      "loss": 0.9469,
      "step": 3842
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 14.100372314453125,
      "learning_rate": 9.4579945799458e-06,
      "loss": 1.6444,
      "step": 3843
    },
    {
      "epoch": 1.4881920247773905,
      "grad_norm": 18.767030715942383,
      "learning_rate": 9.457564416914012e-06,
      "loss": 1.8925,
      "step": 3844
    },
    {
      "epoch": 1.4885791715060008,
      "grad_norm": 21.492691040039062,
      "learning_rate": 9.457134253882221e-06,
      "loss": 2.5096,
      "step": 3845
    },
    {
      "epoch": 1.488966318234611,
      "grad_norm": 20.60612678527832,
      "learning_rate": 9.456704090850433e-06,
      "loss": 1.5089,
      "step": 3846
    },
    {
      "epoch": 1.4893534649632212,
      "grad_norm": 15.58771800994873,
      "learning_rate": 9.456273927818644e-06,
      "loss": 1.6018,
      "step": 3847
    },
    {
      "epoch": 1.4897406116918313,
      "grad_norm": 14.651637077331543,
      "learning_rate": 9.455843764786856e-06,
      "loss": 1.7536,
      "step": 3848
    },
    {
      "epoch": 1.4901277584204413,
      "grad_norm": 14.798362731933594,
      "learning_rate": 9.455413601755065e-06,
      "loss": 1.5822,
      "step": 3849
    },
    {
      "epoch": 1.4905149051490514,
      "grad_norm": 23.820688247680664,
      "learning_rate": 9.454983438723277e-06,
      "loss": 1.3638,
      "step": 3850
    },
    {
      "epoch": 1.4909020518776617,
      "grad_norm": 28.43178939819336,
      "learning_rate": 9.454553275691488e-06,
      "loss": 1.5871,
      "step": 3851
    },
    {
      "epoch": 1.4912891986062717,
      "grad_norm": 13.702770233154297,
      "learning_rate": 9.4541231126597e-06,
      "loss": 1.3954,
      "step": 3852
    },
    {
      "epoch": 1.491676345334882,
      "grad_norm": 12.753701210021973,
      "learning_rate": 9.45369294962791e-06,
      "loss": 1.0322,
      "step": 3853
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 15.69642448425293,
      "learning_rate": 9.45326278659612e-06,
      "loss": 1.5612,
      "step": 3854
    },
    {
      "epoch": 1.4924506387921022,
      "grad_norm": 20.41327667236328,
      "learning_rate": 9.452832623564332e-06,
      "loss": 1.7093,
      "step": 3855
    },
    {
      "epoch": 1.4928377855207122,
      "grad_norm": 16.76844024658203,
      "learning_rate": 9.452402460532542e-06,
      "loss": 1.075,
      "step": 3856
    },
    {
      "epoch": 1.4932249322493225,
      "grad_norm": 22.291200637817383,
      "learning_rate": 9.451972297500753e-06,
      "loss": 1.436,
      "step": 3857
    },
    {
      "epoch": 1.4936120789779326,
      "grad_norm": 12.689984321594238,
      "learning_rate": 9.451542134468965e-06,
      "loss": 1.0705,
      "step": 3858
    },
    {
      "epoch": 1.4939992257065429,
      "grad_norm": 16.753707885742188,
      "learning_rate": 9.451111971437176e-06,
      "loss": 1.1939,
      "step": 3859
    },
    {
      "epoch": 1.494386372435153,
      "grad_norm": 15.32621955871582,
      "learning_rate": 9.450681808405386e-06,
      "loss": 1.0978,
      "step": 3860
    },
    {
      "epoch": 1.494773519163763,
      "grad_norm": 24.219873428344727,
      "learning_rate": 9.450251645373597e-06,
      "loss": 1.3453,
      "step": 3861
    },
    {
      "epoch": 1.4951606658923733,
      "grad_norm": 22.278968811035156,
      "learning_rate": 9.449821482341809e-06,
      "loss": 1.5217,
      "step": 3862
    },
    {
      "epoch": 1.4955478126209834,
      "grad_norm": 16.611326217651367,
      "learning_rate": 9.44939131931002e-06,
      "loss": 1.2218,
      "step": 3863
    },
    {
      "epoch": 1.4959349593495934,
      "grad_norm": 16.864238739013672,
      "learning_rate": 9.44896115627823e-06,
      "loss": 0.9536,
      "step": 3864
    },
    {
      "epoch": 1.4963221060782037,
      "grad_norm": 17.514108657836914,
      "learning_rate": 9.448530993246441e-06,
      "loss": 1.7641,
      "step": 3865
    },
    {
      "epoch": 1.4967092528068138,
      "grad_norm": 22.80377197265625,
      "learning_rate": 9.448100830214653e-06,
      "loss": 2.2043,
      "step": 3866
    },
    {
      "epoch": 1.4970963995354238,
      "grad_norm": 15.426070213317871,
      "learning_rate": 9.447670667182864e-06,
      "loss": 1.5306,
      "step": 3867
    },
    {
      "epoch": 1.4974835462640341,
      "grad_norm": 29.72636604309082,
      "learning_rate": 9.447240504151074e-06,
      "loss": 1.5082,
      "step": 3868
    },
    {
      "epoch": 1.4978706929926442,
      "grad_norm": 32.02777099609375,
      "learning_rate": 9.446810341119285e-06,
      "loss": 2.0202,
      "step": 3869
    },
    {
      "epoch": 1.4982578397212545,
      "grad_norm": 15.311335563659668,
      "learning_rate": 9.446380178087497e-06,
      "loss": 1.5723,
      "step": 3870
    },
    {
      "epoch": 1.4986449864498645,
      "grad_norm": 26.624263763427734,
      "learning_rate": 9.445950015055706e-06,
      "loss": 1.9573,
      "step": 3871
    },
    {
      "epoch": 1.4990321331784746,
      "grad_norm": 15.075349807739258,
      "learning_rate": 9.445519852023918e-06,
      "loss": 1.5592,
      "step": 3872
    },
    {
      "epoch": 1.4994192799070847,
      "grad_norm": 12.935165405273438,
      "learning_rate": 9.445089688992129e-06,
      "loss": 1.0046,
      "step": 3873
    },
    {
      "epoch": 1.499806426635695,
      "grad_norm": 38.7092399597168,
      "learning_rate": 9.44465952596034e-06,
      "loss": 2.7376,
      "step": 3874
    },
    {
      "epoch": 1.500193573364305,
      "grad_norm": 19.47774887084961,
      "learning_rate": 9.44422936292855e-06,
      "loss": 2.084,
      "step": 3875
    },
    {
      "epoch": 1.5005807200929153,
      "grad_norm": 14.522294044494629,
      "learning_rate": 9.443799199896762e-06,
      "loss": 1.5467,
      "step": 3876
    },
    {
      "epoch": 1.5009678668215254,
      "grad_norm": 17.018680572509766,
      "learning_rate": 9.443369036864971e-06,
      "loss": 1.6893,
      "step": 3877
    },
    {
      "epoch": 1.5013550135501355,
      "grad_norm": 15.505231857299805,
      "learning_rate": 9.442938873833185e-06,
      "loss": 1.4417,
      "step": 3878
    },
    {
      "epoch": 1.5017421602787455,
      "grad_norm": 36.97850799560547,
      "learning_rate": 9.442508710801394e-06,
      "loss": 1.802,
      "step": 3879
    },
    {
      "epoch": 1.5021293070073558,
      "grad_norm": 24.041025161743164,
      "learning_rate": 9.442078547769606e-06,
      "loss": 1.8935,
      "step": 3880
    },
    {
      "epoch": 1.502516453735966,
      "grad_norm": 22.990543365478516,
      "learning_rate": 9.441648384737815e-06,
      "loss": 2.2231,
      "step": 3881
    },
    {
      "epoch": 1.5029036004645762,
      "grad_norm": 23.483055114746094,
      "learning_rate": 9.441218221706028e-06,
      "loss": 1.3042,
      "step": 3882
    },
    {
      "epoch": 1.5032907471931862,
      "grad_norm": 23.369346618652344,
      "learning_rate": 9.440788058674238e-06,
      "loss": 2.0418,
      "step": 3883
    },
    {
      "epoch": 1.5036778939217963,
      "grad_norm": 23.211944580078125,
      "learning_rate": 9.44035789564245e-06,
      "loss": 1.6773,
      "step": 3884
    },
    {
      "epoch": 1.5040650406504064,
      "grad_norm": 20.211036682128906,
      "learning_rate": 9.43992773261066e-06,
      "loss": 1.3008,
      "step": 3885
    },
    {
      "epoch": 1.5044521873790166,
      "grad_norm": 19.26239013671875,
      "learning_rate": 9.43949756957887e-06,
      "loss": 2.2753,
      "step": 3886
    },
    {
      "epoch": 1.504839334107627,
      "grad_norm": 19.2429141998291,
      "learning_rate": 9.439067406547082e-06,
      "loss": 1.2503,
      "step": 3887
    },
    {
      "epoch": 1.505226480836237,
      "grad_norm": 21.462247848510742,
      "learning_rate": 9.438637243515294e-06,
      "loss": 2.2055,
      "step": 3888
    },
    {
      "epoch": 1.505613627564847,
      "grad_norm": 13.353675842285156,
      "learning_rate": 9.438207080483503e-06,
      "loss": 0.9216,
      "step": 3889
    },
    {
      "epoch": 1.5060007742934571,
      "grad_norm": 11.640013694763184,
      "learning_rate": 9.437776917451715e-06,
      "loss": 1.3952,
      "step": 3890
    },
    {
      "epoch": 1.5063879210220672,
      "grad_norm": 18.978132247924805,
      "learning_rate": 9.437346754419926e-06,
      "loss": 1.1646,
      "step": 3891
    },
    {
      "epoch": 1.5067750677506775,
      "grad_norm": 14.71306324005127,
      "learning_rate": 9.436916591388136e-06,
      "loss": 0.6364,
      "step": 3892
    },
    {
      "epoch": 1.5071622144792878,
      "grad_norm": 23.786497116088867,
      "learning_rate": 9.436486428356347e-06,
      "loss": 1.8545,
      "step": 3893
    },
    {
      "epoch": 1.5075493612078978,
      "grad_norm": 20.55413818359375,
      "learning_rate": 9.436056265324559e-06,
      "loss": 1.7539,
      "step": 3894
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 10.08866024017334,
      "learning_rate": 9.43562610229277e-06,
      "loss": 1.1969,
      "step": 3895
    },
    {
      "epoch": 1.508323654665118,
      "grad_norm": 33.55976104736328,
      "learning_rate": 9.43519593926098e-06,
      "loss": 1.1679,
      "step": 3896
    },
    {
      "epoch": 1.5087108013937283,
      "grad_norm": 26.86595344543457,
      "learning_rate": 9.434765776229191e-06,
      "loss": 0.9961,
      "step": 3897
    },
    {
      "epoch": 1.5090979481223383,
      "grad_norm": 13.73779010772705,
      "learning_rate": 9.434335613197403e-06,
      "loss": 1.0918,
      "step": 3898
    },
    {
      "epoch": 1.5094850948509486,
      "grad_norm": 12.445355415344238,
      "learning_rate": 9.433905450165614e-06,
      "loss": 0.6774,
      "step": 3899
    },
    {
      "epoch": 1.5098722415795587,
      "grad_norm": 13.097793579101562,
      "learning_rate": 9.433475287133824e-06,
      "loss": 1.0742,
      "step": 3900
    },
    {
      "epoch": 1.5102593883081687,
      "grad_norm": 27.882577896118164,
      "learning_rate": 9.433045124102035e-06,
      "loss": 1.7419,
      "step": 3901
    },
    {
      "epoch": 1.5106465350367788,
      "grad_norm": 11.622425079345703,
      "learning_rate": 9.432614961070247e-06,
      "loss": 0.8681,
      "step": 3902
    },
    {
      "epoch": 1.511033681765389,
      "grad_norm": 31.147403717041016,
      "learning_rate": 9.432184798038458e-06,
      "loss": 1.384,
      "step": 3903
    },
    {
      "epoch": 1.5114208284939994,
      "grad_norm": 37.17719268798828,
      "learning_rate": 9.431754635006668e-06,
      "loss": 2.1155,
      "step": 3904
    },
    {
      "epoch": 1.5118079752226095,
      "grad_norm": 16.17116355895996,
      "learning_rate": 9.43132447197488e-06,
      "loss": 1.6359,
      "step": 3905
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 12.613065719604492,
      "learning_rate": 9.43089430894309e-06,
      "loss": 2.1252,
      "step": 3906
    },
    {
      "epoch": 1.5125822686798296,
      "grad_norm": 16.115129470825195,
      "learning_rate": 9.4304641459113e-06,
      "loss": 1.0522,
      "step": 3907
    },
    {
      "epoch": 1.5129694154084397,
      "grad_norm": 13.471986770629883,
      "learning_rate": 9.430033982879512e-06,
      "loss": 1.1583,
      "step": 3908
    },
    {
      "epoch": 1.51335656213705,
      "grad_norm": 17.46193504333496,
      "learning_rate": 9.429603819847723e-06,
      "loss": 1.6651,
      "step": 3909
    },
    {
      "epoch": 1.5137437088656602,
      "grad_norm": 20.066329956054688,
      "learning_rate": 9.429173656815935e-06,
      "loss": 1.4204,
      "step": 3910
    },
    {
      "epoch": 1.5141308555942703,
      "grad_norm": 15.097694396972656,
      "learning_rate": 9.428743493784144e-06,
      "loss": 1.1588,
      "step": 3911
    },
    {
      "epoch": 1.5145180023228804,
      "grad_norm": 23.959217071533203,
      "learning_rate": 9.428313330752356e-06,
      "loss": 2.1686,
      "step": 3912
    },
    {
      "epoch": 1.5149051490514904,
      "grad_norm": 26.256126403808594,
      "learning_rate": 9.427883167720567e-06,
      "loss": 2.4759,
      "step": 3913
    },
    {
      "epoch": 1.5152922957801005,
      "grad_norm": 12.944364547729492,
      "learning_rate": 9.427453004688779e-06,
      "loss": 1.4366,
      "step": 3914
    },
    {
      "epoch": 1.5156794425087108,
      "grad_norm": 86.43086242675781,
      "learning_rate": 9.427022841656988e-06,
      "loss": 1.9852,
      "step": 3915
    },
    {
      "epoch": 1.516066589237321,
      "grad_norm": 13.451542854309082,
      "learning_rate": 9.4265926786252e-06,
      "loss": 1.6871,
      "step": 3916
    },
    {
      "epoch": 1.5164537359659311,
      "grad_norm": 22.11095428466797,
      "learning_rate": 9.426162515593411e-06,
      "loss": 2.0388,
      "step": 3917
    },
    {
      "epoch": 1.5168408826945412,
      "grad_norm": 6.704661846160889,
      "learning_rate": 9.425732352561623e-06,
      "loss": 0.4669,
      "step": 3918
    },
    {
      "epoch": 1.5172280294231513,
      "grad_norm": 16.158058166503906,
      "learning_rate": 9.425302189529832e-06,
      "loss": 1.184,
      "step": 3919
    },
    {
      "epoch": 1.5176151761517616,
      "grad_norm": 21.858524322509766,
      "learning_rate": 9.424872026498044e-06,
      "loss": 1.6067,
      "step": 3920
    },
    {
      "epoch": 1.5180023228803716,
      "grad_norm": 21.612056732177734,
      "learning_rate": 9.424441863466255e-06,
      "loss": 1.4601,
      "step": 3921
    },
    {
      "epoch": 1.518389469608982,
      "grad_norm": 23.2360897064209,
      "learning_rate": 9.424011700434465e-06,
      "loss": 1.561,
      "step": 3922
    },
    {
      "epoch": 1.518776616337592,
      "grad_norm": 16.678388595581055,
      "learning_rate": 9.423581537402676e-06,
      "loss": 1.7277,
      "step": 3923
    },
    {
      "epoch": 1.519163763066202,
      "grad_norm": 13.514790534973145,
      "learning_rate": 9.423151374370888e-06,
      "loss": 1.0079,
      "step": 3924
    },
    {
      "epoch": 1.519550909794812,
      "grad_norm": 11.047492027282715,
      "learning_rate": 9.422721211339099e-06,
      "loss": 1.172,
      "step": 3925
    },
    {
      "epoch": 1.5199380565234224,
      "grad_norm": 21.837682723999023,
      "learning_rate": 9.422291048307309e-06,
      "loss": 2.0483,
      "step": 3926
    },
    {
      "epoch": 1.5203252032520327,
      "grad_norm": 15.495177268981934,
      "learning_rate": 9.42186088527552e-06,
      "loss": 0.9523,
      "step": 3927
    },
    {
      "epoch": 1.5207123499806428,
      "grad_norm": 17.577999114990234,
      "learning_rate": 9.42143072224373e-06,
      "loss": 1.2081,
      "step": 3928
    },
    {
      "epoch": 1.5210994967092528,
      "grad_norm": 14.941057205200195,
      "learning_rate": 9.421000559211943e-06,
      "loss": 1.344,
      "step": 3929
    },
    {
      "epoch": 1.5214866434378629,
      "grad_norm": 13.183587074279785,
      "learning_rate": 9.420570396180153e-06,
      "loss": 0.8979,
      "step": 3930
    },
    {
      "epoch": 1.521873790166473,
      "grad_norm": 15.241125106811523,
      "learning_rate": 9.420140233148364e-06,
      "loss": 1.7059,
      "step": 3931
    },
    {
      "epoch": 1.5222609368950832,
      "grad_norm": 13.749785423278809,
      "learning_rate": 9.419710070116574e-06,
      "loss": 1.3549,
      "step": 3932
    },
    {
      "epoch": 1.5226480836236935,
      "grad_norm": 17.42852210998535,
      "learning_rate": 9.419279907084787e-06,
      "loss": 1.6334,
      "step": 3933
    },
    {
      "epoch": 1.5230352303523036,
      "grad_norm": 13.074681282043457,
      "learning_rate": 9.418849744052997e-06,
      "loss": 0.8865,
      "step": 3934
    },
    {
      "epoch": 1.5234223770809137,
      "grad_norm": 22.834720611572266,
      "learning_rate": 9.418419581021208e-06,
      "loss": 1.7812,
      "step": 3935
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 15.491644859313965,
      "learning_rate": 9.417989417989418e-06,
      "loss": 0.8299,
      "step": 3936
    },
    {
      "epoch": 1.5241966705381338,
      "grad_norm": 14.28906536102295,
      "learning_rate": 9.41755925495763e-06,
      "loss": 0.5199,
      "step": 3937
    },
    {
      "epoch": 1.524583817266744,
      "grad_norm": 16.186254501342773,
      "learning_rate": 9.41712909192584e-06,
      "loss": 1.1357,
      "step": 3938
    },
    {
      "epoch": 1.5249709639953544,
      "grad_norm": 19.905899047851562,
      "learning_rate": 9.416698928894052e-06,
      "loss": 2.288,
      "step": 3939
    },
    {
      "epoch": 1.5253581107239644,
      "grad_norm": 18.550222396850586,
      "learning_rate": 9.416268765862262e-06,
      "loss": 1.8623,
      "step": 3940
    },
    {
      "epoch": 1.5257452574525745,
      "grad_norm": 25.147581100463867,
      "learning_rate": 9.415838602830473e-06,
      "loss": 1.6018,
      "step": 3941
    },
    {
      "epoch": 1.5261324041811846,
      "grad_norm": 22.822059631347656,
      "learning_rate": 9.415408439798685e-06,
      "loss": 2.5317,
      "step": 3942
    },
    {
      "epoch": 1.5265195509097949,
      "grad_norm": 17.51810073852539,
      "learning_rate": 9.414978276766894e-06,
      "loss": 1.5012,
      "step": 3943
    },
    {
      "epoch": 1.526906697638405,
      "grad_norm": 15.397089958190918,
      "learning_rate": 9.414548113735107e-06,
      "loss": 1.6265,
      "step": 3944
    },
    {
      "epoch": 1.5272938443670152,
      "grad_norm": 19.187231063842773,
      "learning_rate": 9.414117950703317e-06,
      "loss": 1.6169,
      "step": 3945
    },
    {
      "epoch": 1.5276809910956253,
      "grad_norm": 16.254732131958008,
      "learning_rate": 9.413687787671529e-06,
      "loss": 1.1416,
      "step": 3946
    },
    {
      "epoch": 1.5280681378242353,
      "grad_norm": 13.0378999710083,
      "learning_rate": 9.413257624639738e-06,
      "loss": 0.8379,
      "step": 3947
    },
    {
      "epoch": 1.5284552845528454,
      "grad_norm": 12.8118257522583,
      "learning_rate": 9.412827461607951e-06,
      "loss": 1.2129,
      "step": 3948
    },
    {
      "epoch": 1.5288424312814557,
      "grad_norm": 14.828121185302734,
      "learning_rate": 9.412397298576161e-06,
      "loss": 0.438,
      "step": 3949
    },
    {
      "epoch": 1.5292295780100658,
      "grad_norm": 11.931756019592285,
      "learning_rate": 9.411967135544373e-06,
      "loss": 0.8421,
      "step": 3950
    },
    {
      "epoch": 1.529616724738676,
      "grad_norm": 14.859469413757324,
      "learning_rate": 9.411536972512582e-06,
      "loss": 1.5207,
      "step": 3951
    },
    {
      "epoch": 1.5300038714672861,
      "grad_norm": 12.964058876037598,
      "learning_rate": 9.411106809480794e-06,
      "loss": 0.8251,
      "step": 3952
    },
    {
      "epoch": 1.5303910181958962,
      "grad_norm": 23.955738067626953,
      "learning_rate": 9.410676646449005e-06,
      "loss": 1.3702,
      "step": 3953
    },
    {
      "epoch": 1.5307781649245062,
      "grad_norm": 14.399362564086914,
      "learning_rate": 9.410246483417217e-06,
      "loss": 1.5559,
      "step": 3954
    },
    {
      "epoch": 1.5311653116531165,
      "grad_norm": 20.977088928222656,
      "learning_rate": 9.409816320385426e-06,
      "loss": 0.9933,
      "step": 3955
    },
    {
      "epoch": 1.5315524583817268,
      "grad_norm": 15.495823860168457,
      "learning_rate": 9.409386157353638e-06,
      "loss": 1.096,
      "step": 3956
    },
    {
      "epoch": 1.5319396051103369,
      "grad_norm": 14.730125427246094,
      "learning_rate": 9.408955994321849e-06,
      "loss": 1.5583,
      "step": 3957
    },
    {
      "epoch": 1.532326751838947,
      "grad_norm": 15.700297355651855,
      "learning_rate": 9.408525831290059e-06,
      "loss": 1.2222,
      "step": 3958
    },
    {
      "epoch": 1.532713898567557,
      "grad_norm": 21.24092674255371,
      "learning_rate": 9.40809566825827e-06,
      "loss": 1.3141,
      "step": 3959
    },
    {
      "epoch": 1.533101045296167,
      "grad_norm": 20.40489387512207,
      "learning_rate": 9.407665505226482e-06,
      "loss": 1.8391,
      "step": 3960
    },
    {
      "epoch": 1.5334881920247774,
      "grad_norm": 16.544797897338867,
      "learning_rate": 9.407235342194693e-06,
      "loss": 1.3968,
      "step": 3961
    },
    {
      "epoch": 1.5338753387533877,
      "grad_norm": 14.599631309509277,
      "learning_rate": 9.406805179162903e-06,
      "loss": 0.9567,
      "step": 3962
    },
    {
      "epoch": 1.5342624854819977,
      "grad_norm": 8.073845863342285,
      "learning_rate": 9.406375016131114e-06,
      "loss": 0.4393,
      "step": 3963
    },
    {
      "epoch": 1.5346496322106078,
      "grad_norm": 29.411725997924805,
      "learning_rate": 9.405944853099326e-06,
      "loss": 2.4947,
      "step": 3964
    },
    {
      "epoch": 1.5350367789392179,
      "grad_norm": 17.32366943359375,
      "learning_rate": 9.405514690067537e-06,
      "loss": 0.788,
      "step": 3965
    },
    {
      "epoch": 1.5354239256678281,
      "grad_norm": 14.597768783569336,
      "learning_rate": 9.405084527035747e-06,
      "loss": 1.4308,
      "step": 3966
    },
    {
      "epoch": 1.5358110723964382,
      "grad_norm": 31.09840202331543,
      "learning_rate": 9.404654364003958e-06,
      "loss": 1.6618,
      "step": 3967
    },
    {
      "epoch": 1.5361982191250485,
      "grad_norm": 18.076019287109375,
      "learning_rate": 9.40422420097217e-06,
      "loss": 1.0749,
      "step": 3968
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 10.876916885375977,
      "learning_rate": 9.403794037940381e-06,
      "loss": 1.1488,
      "step": 3969
    },
    {
      "epoch": 1.5369725125822686,
      "grad_norm": 36.41008758544922,
      "learning_rate": 9.40336387490859e-06,
      "loss": 1.11,
      "step": 3970
    },
    {
      "epoch": 1.5373596593108787,
      "grad_norm": 20.07271385192871,
      "learning_rate": 9.402933711876802e-06,
      "loss": 2.0825,
      "step": 3971
    },
    {
      "epoch": 1.537746806039489,
      "grad_norm": 29.33096694946289,
      "learning_rate": 9.402503548845014e-06,
      "loss": 2.0868,
      "step": 3972
    },
    {
      "epoch": 1.538133952768099,
      "grad_norm": 18.996341705322266,
      "learning_rate": 9.402073385813223e-06,
      "loss": 1.3752,
      "step": 3973
    },
    {
      "epoch": 1.5385210994967093,
      "grad_norm": 23.78791618347168,
      "learning_rate": 9.401643222781435e-06,
      "loss": 0.8033,
      "step": 3974
    },
    {
      "epoch": 1.5389082462253194,
      "grad_norm": 12.820208549499512,
      "learning_rate": 9.401213059749646e-06,
      "loss": 1.1945,
      "step": 3975
    },
    {
      "epoch": 1.5392953929539295,
      "grad_norm": 14.268813133239746,
      "learning_rate": 9.400782896717858e-06,
      "loss": 1.2348,
      "step": 3976
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 8.762625694274902,
      "learning_rate": 9.400352733686067e-06,
      "loss": 1.5827,
      "step": 3977
    },
    {
      "epoch": 1.5400696864111498,
      "grad_norm": 18.558727264404297,
      "learning_rate": 9.399922570654279e-06,
      "loss": 1.199,
      "step": 3978
    },
    {
      "epoch": 1.5404568331397601,
      "grad_norm": 17.79087257385254,
      "learning_rate": 9.399492407622488e-06,
      "loss": 1.8586,
      "step": 3979
    },
    {
      "epoch": 1.5408439798683702,
      "grad_norm": 16.22873878479004,
      "learning_rate": 9.399062244590701e-06,
      "loss": 0.9386,
      "step": 3980
    },
    {
      "epoch": 1.5412311265969802,
      "grad_norm": 15.530988693237305,
      "learning_rate": 9.398632081558911e-06,
      "loss": 1.0475,
      "step": 3981
    },
    {
      "epoch": 1.5416182733255903,
      "grad_norm": 27.44196128845215,
      "learning_rate": 9.398201918527123e-06,
      "loss": 1.2622,
      "step": 3982
    },
    {
      "epoch": 1.5420054200542004,
      "grad_norm": 12.762165069580078,
      "learning_rate": 9.397771755495334e-06,
      "loss": 1.2465,
      "step": 3983
    },
    {
      "epoch": 1.5423925667828107,
      "grad_norm": 33.533870697021484,
      "learning_rate": 9.397341592463545e-06,
      "loss": 1.8584,
      "step": 3984
    },
    {
      "epoch": 1.542779713511421,
      "grad_norm": 14.593361854553223,
      "learning_rate": 9.396911429431755e-06,
      "loss": 1.3712,
      "step": 3985
    },
    {
      "epoch": 1.543166860240031,
      "grad_norm": 21.035036087036133,
      "learning_rate": 9.396481266399967e-06,
      "loss": 1.3071,
      "step": 3986
    },
    {
      "epoch": 1.543554006968641,
      "grad_norm": 25.665224075317383,
      "learning_rate": 9.396051103368178e-06,
      "loss": 1.4736,
      "step": 3987
    },
    {
      "epoch": 1.5439411536972512,
      "grad_norm": 20.98574447631836,
      "learning_rate": 9.395620940336388e-06,
      "loss": 1.1754,
      "step": 3988
    },
    {
      "epoch": 1.5443283004258614,
      "grad_norm": 15.763877868652344,
      "learning_rate": 9.395190777304599e-06,
      "loss": 1.4581,
      "step": 3989
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 25.372386932373047,
      "learning_rate": 9.39476061427281e-06,
      "loss": 1.334,
      "step": 3990
    },
    {
      "epoch": 1.5451025938830818,
      "grad_norm": 12.397263526916504,
      "learning_rate": 9.394330451241022e-06,
      "loss": 0.7921,
      "step": 3991
    },
    {
      "epoch": 1.5454897406116919,
      "grad_norm": 15.087065696716309,
      "learning_rate": 9.393900288209232e-06,
      "loss": 1.2949,
      "step": 3992
    },
    {
      "epoch": 1.545876887340302,
      "grad_norm": 31.227380752563477,
      "learning_rate": 9.393470125177443e-06,
      "loss": 1.5497,
      "step": 3993
    },
    {
      "epoch": 1.546264034068912,
      "grad_norm": 15.098467826843262,
      "learning_rate": 9.393039962145653e-06,
      "loss": 1.4166,
      "step": 3994
    },
    {
      "epoch": 1.5466511807975223,
      "grad_norm": 17.18636703491211,
      "learning_rate": 9.392609799113866e-06,
      "loss": 1.4586,
      "step": 3995
    },
    {
      "epoch": 1.5470383275261324,
      "grad_norm": 23.4821834564209,
      "learning_rate": 9.392179636082076e-06,
      "loss": 2.4765,
      "step": 3996
    },
    {
      "epoch": 1.5474254742547426,
      "grad_norm": 25.781484603881836,
      "learning_rate": 9.391749473050287e-06,
      "loss": 1.4599,
      "step": 3997
    },
    {
      "epoch": 1.5478126209833527,
      "grad_norm": 6.632574081420898,
      "learning_rate": 9.391319310018497e-06,
      "loss": 0.402,
      "step": 3998
    },
    {
      "epoch": 1.5481997677119628,
      "grad_norm": 14.807177543640137,
      "learning_rate": 9.39088914698671e-06,
      "loss": 1.2666,
      "step": 3999
    },
    {
      "epoch": 1.5485869144405728,
      "grad_norm": 19.153217315673828,
      "learning_rate": 9.39045898395492e-06,
      "loss": 1.168,
      "step": 4000
    },
    {
      "epoch": 1.5489740611691831,
      "grad_norm": 8.536092758178711,
      "learning_rate": 9.390028820923131e-06,
      "loss": 0.6696,
      "step": 4001
    },
    {
      "epoch": 1.5493612078977934,
      "grad_norm": 18.136287689208984,
      "learning_rate": 9.38959865789134e-06,
      "loss": 1.079,
      "step": 4002
    },
    {
      "epoch": 1.5497483546264035,
      "grad_norm": 19.77773666381836,
      "learning_rate": 9.389168494859552e-06,
      "loss": 2.4854,
      "step": 4003
    },
    {
      "epoch": 1.5501355013550135,
      "grad_norm": 21.517131805419922,
      "learning_rate": 9.388738331827764e-06,
      "loss": 1.5085,
      "step": 4004
    },
    {
      "epoch": 1.5505226480836236,
      "grad_norm": 25.72987937927246,
      "learning_rate": 9.388308168795975e-06,
      "loss": 1.5498,
      "step": 4005
    },
    {
      "epoch": 1.5509097948122337,
      "grad_norm": 17.059377670288086,
      "learning_rate": 9.387878005764185e-06,
      "loss": 1.7216,
      "step": 4006
    },
    {
      "epoch": 1.551296941540844,
      "grad_norm": 19.799463272094727,
      "learning_rate": 9.387447842732396e-06,
      "loss": 2.0447,
      "step": 4007
    },
    {
      "epoch": 1.5516840882694543,
      "grad_norm": 25.783370971679688,
      "learning_rate": 9.387017679700608e-06,
      "loss": 1.771,
      "step": 4008
    },
    {
      "epoch": 1.5520712349980643,
      "grad_norm": 14.400238037109375,
      "learning_rate": 9.386587516668817e-06,
      "loss": 1.0304,
      "step": 4009
    },
    {
      "epoch": 1.5524583817266744,
      "grad_norm": 16.98451805114746,
      "learning_rate": 9.386157353637029e-06,
      "loss": 0.6912,
      "step": 4010
    },
    {
      "epoch": 1.5528455284552845,
      "grad_norm": 19.803504943847656,
      "learning_rate": 9.38572719060524e-06,
      "loss": 0.9974,
      "step": 4011
    },
    {
      "epoch": 1.5532326751838947,
      "grad_norm": 32.38468933105469,
      "learning_rate": 9.385297027573452e-06,
      "loss": 0.8829,
      "step": 4012
    },
    {
      "epoch": 1.5536198219125048,
      "grad_norm": 20.700450897216797,
      "learning_rate": 9.384866864541661e-06,
      "loss": 1.6671,
      "step": 4013
    },
    {
      "epoch": 1.554006968641115,
      "grad_norm": 17.59571075439453,
      "learning_rate": 9.384436701509873e-06,
      "loss": 1.6044,
      "step": 4014
    },
    {
      "epoch": 1.5543941153697252,
      "grad_norm": 18.17901039123535,
      "learning_rate": 9.384006538478084e-06,
      "loss": 1.2898,
      "step": 4015
    },
    {
      "epoch": 1.5547812620983352,
      "grad_norm": 17.310306549072266,
      "learning_rate": 9.383576375446296e-06,
      "loss": 1.4833,
      "step": 4016
    },
    {
      "epoch": 1.5551684088269453,
      "grad_norm": 22.343280792236328,
      "learning_rate": 9.383146212414505e-06,
      "loss": 1.0518,
      "step": 4017
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 16.205488204956055,
      "learning_rate": 9.382716049382717e-06,
      "loss": 1.0548,
      "step": 4018
    },
    {
      "epoch": 1.5559427022841656,
      "grad_norm": 26.666162490844727,
      "learning_rate": 9.382285886350928e-06,
      "loss": 1.9067,
      "step": 4019
    },
    {
      "epoch": 1.556329849012776,
      "grad_norm": 40.99513626098633,
      "learning_rate": 9.38185572331914e-06,
      "loss": 1.4198,
      "step": 4020
    },
    {
      "epoch": 1.556716995741386,
      "grad_norm": 13.414250373840332,
      "learning_rate": 9.38142556028735e-06,
      "loss": 1.2698,
      "step": 4021
    },
    {
      "epoch": 1.557104142469996,
      "grad_norm": 20.083120346069336,
      "learning_rate": 9.38099539725556e-06,
      "loss": 1.3597,
      "step": 4022
    },
    {
      "epoch": 1.5574912891986061,
      "grad_norm": 7.951657295227051,
      "learning_rate": 9.380565234223772e-06,
      "loss": 1.2255,
      "step": 4023
    },
    {
      "epoch": 1.5578784359272164,
      "grad_norm": 32.37173843383789,
      "learning_rate": 9.380135071191982e-06,
      "loss": 1.4278,
      "step": 4024
    },
    {
      "epoch": 1.5582655826558267,
      "grad_norm": 20.108516693115234,
      "learning_rate": 9.379704908160193e-06,
      "loss": 2.0294,
      "step": 4025
    },
    {
      "epoch": 1.5586527293844368,
      "grad_norm": 25.311010360717773,
      "learning_rate": 9.379274745128405e-06,
      "loss": 1.7543,
      "step": 4026
    },
    {
      "epoch": 1.5590398761130468,
      "grad_norm": 15.930974006652832,
      "learning_rate": 9.378844582096616e-06,
      "loss": 1.1232,
      "step": 4027
    },
    {
      "epoch": 1.559427022841657,
      "grad_norm": 9.953201293945312,
      "learning_rate": 9.378414419064826e-06,
      "loss": 1.3656,
      "step": 4028
    },
    {
      "epoch": 1.559814169570267,
      "grad_norm": 15.369306564331055,
      "learning_rate": 9.377984256033037e-06,
      "loss": 0.8565,
      "step": 4029
    },
    {
      "epoch": 1.5602013162988773,
      "grad_norm": 15.487290382385254,
      "learning_rate": 9.377554093001249e-06,
      "loss": 1.3265,
      "step": 4030
    },
    {
      "epoch": 1.5605884630274875,
      "grad_norm": 21.08793067932129,
      "learning_rate": 9.37712392996946e-06,
      "loss": 2.1202,
      "step": 4031
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 14.423012733459473,
      "learning_rate": 9.37669376693767e-06,
      "loss": 1.3909,
      "step": 4032
    },
    {
      "epoch": 1.5613627564847077,
      "grad_norm": 7.974408149719238,
      "learning_rate": 9.376263603905881e-06,
      "loss": 0.4902,
      "step": 4033
    },
    {
      "epoch": 1.5617499032133177,
      "grad_norm": 47.39072036743164,
      "learning_rate": 9.375833440874093e-06,
      "loss": 1.0332,
      "step": 4034
    },
    {
      "epoch": 1.562137049941928,
      "grad_norm": 18.833789825439453,
      "learning_rate": 9.375403277842304e-06,
      "loss": 1.1982,
      "step": 4035
    },
    {
      "epoch": 1.562524196670538,
      "grad_norm": 16.765975952148438,
      "learning_rate": 9.374973114810514e-06,
      "loss": 1.0216,
      "step": 4036
    },
    {
      "epoch": 1.5629113433991484,
      "grad_norm": 20.335853576660156,
      "learning_rate": 9.374542951778725e-06,
      "loss": 1.1081,
      "step": 4037
    },
    {
      "epoch": 1.5632984901277585,
      "grad_norm": 14.207318305969238,
      "learning_rate": 9.374112788746936e-06,
      "loss": 1.04,
      "step": 4038
    },
    {
      "epoch": 1.5636856368563685,
      "grad_norm": 12.773507118225098,
      "learning_rate": 9.373682625715146e-06,
      "loss": 0.8423,
      "step": 4039
    },
    {
      "epoch": 1.5640727835849786,
      "grad_norm": 7.1028337478637695,
      "learning_rate": 9.373252462683358e-06,
      "loss": 0.473,
      "step": 4040
    },
    {
      "epoch": 1.5644599303135889,
      "grad_norm": 13.864154815673828,
      "learning_rate": 9.372822299651569e-06,
      "loss": 1.2928,
      "step": 4041
    },
    {
      "epoch": 1.564847077042199,
      "grad_norm": 14.700790405273438,
      "learning_rate": 9.37239213661978e-06,
      "loss": 1.6209,
      "step": 4042
    },
    {
      "epoch": 1.5652342237708092,
      "grad_norm": 31.01461410522461,
      "learning_rate": 9.37196197358799e-06,
      "loss": 2.3524,
      "step": 4043
    },
    {
      "epoch": 1.5656213704994193,
      "grad_norm": 16.942543029785156,
      "learning_rate": 9.371531810556202e-06,
      "loss": 1.4835,
      "step": 4044
    },
    {
      "epoch": 1.5660085172280294,
      "grad_norm": 25.15765953063965,
      "learning_rate": 9.371101647524411e-06,
      "loss": 1.8851,
      "step": 4045
    },
    {
      "epoch": 1.5663956639566394,
      "grad_norm": 43.97419357299805,
      "learning_rate": 9.370671484492624e-06,
      "loss": 3.2379,
      "step": 4046
    },
    {
      "epoch": 1.5667828106852497,
      "grad_norm": 24.769939422607422,
      "learning_rate": 9.370241321460834e-06,
      "loss": 1.5716,
      "step": 4047
    },
    {
      "epoch": 1.56716995741386,
      "grad_norm": 22.813940048217773,
      "learning_rate": 9.369811158429046e-06,
      "loss": 1.9538,
      "step": 4048
    },
    {
      "epoch": 1.56755710414247,
      "grad_norm": 13.951101303100586,
      "learning_rate": 9.369380995397255e-06,
      "loss": 1.3656,
      "step": 4049
    },
    {
      "epoch": 1.5679442508710801,
      "grad_norm": 24.223766326904297,
      "learning_rate": 9.368950832365468e-06,
      "loss": 1.3299,
      "step": 4050
    },
    {
      "epoch": 1.5683313975996902,
      "grad_norm": 15.811868667602539,
      "learning_rate": 9.368520669333678e-06,
      "loss": 1.332,
      "step": 4051
    },
    {
      "epoch": 1.5687185443283003,
      "grad_norm": 24.982419967651367,
      "learning_rate": 9.36809050630189e-06,
      "loss": 1.9767,
      "step": 4052
    },
    {
      "epoch": 1.5691056910569106,
      "grad_norm": 21.359968185424805,
      "learning_rate": 9.3676603432701e-06,
      "loss": 1.6121,
      "step": 4053
    },
    {
      "epoch": 1.5694928377855208,
      "grad_norm": 25.785993576049805,
      "learning_rate": 9.36723018023831e-06,
      "loss": 1.6679,
      "step": 4054
    },
    {
      "epoch": 1.569879984514131,
      "grad_norm": 10.838200569152832,
      "learning_rate": 9.366800017206522e-06,
      "loss": 0.6416,
      "step": 4055
    },
    {
      "epoch": 1.570267131242741,
      "grad_norm": 19.75639533996582,
      "learning_rate": 9.366369854174734e-06,
      "loss": 1.5779,
      "step": 4056
    },
    {
      "epoch": 1.570654277971351,
      "grad_norm": 19.546476364135742,
      "learning_rate": 9.365939691142943e-06,
      "loss": 1.288,
      "step": 4057
    },
    {
      "epoch": 1.5710414246999613,
      "grad_norm": 17.13753890991211,
      "learning_rate": 9.365509528111155e-06,
      "loss": 1.71,
      "step": 4058
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 17.607160568237305,
      "learning_rate": 9.365079365079366e-06,
      "loss": 1.4302,
      "step": 4059
    },
    {
      "epoch": 1.5718157181571817,
      "grad_norm": 35.427886962890625,
      "learning_rate": 9.364649202047576e-06,
      "loss": 0.8009,
      "step": 4060
    },
    {
      "epoch": 1.5722028648857918,
      "grad_norm": 15.329849243164062,
      "learning_rate": 9.364219039015787e-06,
      "loss": 1.3925,
      "step": 4061
    },
    {
      "epoch": 1.5725900116144018,
      "grad_norm": 28.808996200561523,
      "learning_rate": 9.363788875983999e-06,
      "loss": 2.4942,
      "step": 4062
    },
    {
      "epoch": 1.5729771583430119,
      "grad_norm": 26.759241104125977,
      "learning_rate": 9.36335871295221e-06,
      "loss": 0.596,
      "step": 4063
    },
    {
      "epoch": 1.5733643050716222,
      "grad_norm": 10.877384185791016,
      "learning_rate": 9.36292854992042e-06,
      "loss": 0.6658,
      "step": 4064
    },
    {
      "epoch": 1.5737514518002322,
      "grad_norm": 23.334280014038086,
      "learning_rate": 9.362498386888633e-06,
      "loss": 2.2794,
      "step": 4065
    },
    {
      "epoch": 1.5741385985288425,
      "grad_norm": 25.14316177368164,
      "learning_rate": 9.362068223856843e-06,
      "loss": 2.0647,
      "step": 4066
    },
    {
      "epoch": 1.5745257452574526,
      "grad_norm": 16.03374481201172,
      "learning_rate": 9.361638060825054e-06,
      "loss": 1.5873,
      "step": 4067
    },
    {
      "epoch": 1.5749128919860627,
      "grad_norm": 27.689512252807617,
      "learning_rate": 9.361207897793264e-06,
      "loss": 1.8737,
      "step": 4068
    },
    {
      "epoch": 1.5753000387146727,
      "grad_norm": 25.662033081054688,
      "learning_rate": 9.360777734761475e-06,
      "loss": 1.3288,
      "step": 4069
    },
    {
      "epoch": 1.575687185443283,
      "grad_norm": 19.914907455444336,
      "learning_rate": 9.360347571729687e-06,
      "loss": 1.8805,
      "step": 4070
    },
    {
      "epoch": 1.5760743321718933,
      "grad_norm": 17.58182716369629,
      "learning_rate": 9.359917408697898e-06,
      "loss": 1.4115,
      "step": 4071
    },
    {
      "epoch": 1.5764614789005034,
      "grad_norm": 15.244479179382324,
      "learning_rate": 9.359487245666108e-06,
      "loss": 0.6416,
      "step": 4072
    },
    {
      "epoch": 1.5768486256291134,
      "grad_norm": 20.227317810058594,
      "learning_rate": 9.359057082634319e-06,
      "loss": 1.6463,
      "step": 4073
    },
    {
      "epoch": 1.5772357723577235,
      "grad_norm": 20.605655670166016,
      "learning_rate": 9.35862691960253e-06,
      "loss": 1.9468,
      "step": 4074
    },
    {
      "epoch": 1.5776229190863336,
      "grad_norm": 30.989871978759766,
      "learning_rate": 9.35819675657074e-06,
      "loss": 1.5456,
      "step": 4075
    },
    {
      "epoch": 1.5780100658149439,
      "grad_norm": 17.801454544067383,
      "learning_rate": 9.357766593538952e-06,
      "loss": 1.7664,
      "step": 4076
    },
    {
      "epoch": 1.5783972125435541,
      "grad_norm": 21.277868270874023,
      "learning_rate": 9.357336430507163e-06,
      "loss": 0.7292,
      "step": 4077
    },
    {
      "epoch": 1.5787843592721642,
      "grad_norm": 45.3848762512207,
      "learning_rate": 9.356906267475374e-06,
      "loss": 1.8764,
      "step": 4078
    },
    {
      "epoch": 1.5791715060007743,
      "grad_norm": 21.616926193237305,
      "learning_rate": 9.356476104443584e-06,
      "loss": 0.6266,
      "step": 4079
    },
    {
      "epoch": 1.5795586527293843,
      "grad_norm": 23.366418838500977,
      "learning_rate": 9.356045941411796e-06,
      "loss": 1.7143,
      "step": 4080
    },
    {
      "epoch": 1.5799457994579946,
      "grad_norm": 22.609373092651367,
      "learning_rate": 9.355615778380007e-06,
      "loss": 1.2512,
      "step": 4081
    },
    {
      "epoch": 1.5803329461866047,
      "grad_norm": 14.313847541809082,
      "learning_rate": 9.355185615348218e-06,
      "loss": 1.0119,
      "step": 4082
    },
    {
      "epoch": 1.580720092915215,
      "grad_norm": 23.20594024658203,
      "learning_rate": 9.354755452316428e-06,
      "loss": 1.4022,
      "step": 4083
    },
    {
      "epoch": 1.581107239643825,
      "grad_norm": 19.019058227539062,
      "learning_rate": 9.35432528928464e-06,
      "loss": 1.7179,
      "step": 4084
    },
    {
      "epoch": 1.5814943863724351,
      "grad_norm": 22.71950340270996,
      "learning_rate": 9.353895126252851e-06,
      "loss": 1.3279,
      "step": 4085
    },
    {
      "epoch": 1.5818815331010452,
      "grad_norm": 16.664169311523438,
      "learning_rate": 9.353464963221062e-06,
      "loss": 1.5676,
      "step": 4086
    },
    {
      "epoch": 1.5822686798296555,
      "grad_norm": 18.03811264038086,
      "learning_rate": 9.353034800189272e-06,
      "loss": 1.2436,
      "step": 4087
    },
    {
      "epoch": 1.5826558265582655,
      "grad_norm": 23.17261505126953,
      "learning_rate": 9.352604637157484e-06,
      "loss": 1.579,
      "step": 4088
    },
    {
      "epoch": 1.5830429732868758,
      "grad_norm": 35.13852310180664,
      "learning_rate": 9.352174474125695e-06,
      "loss": 1.0351,
      "step": 4089
    },
    {
      "epoch": 1.5834301200154859,
      "grad_norm": 12.432600021362305,
      "learning_rate": 9.351744311093905e-06,
      "loss": 2.7188,
      "step": 4090
    },
    {
      "epoch": 1.583817266744096,
      "grad_norm": 30.338642120361328,
      "learning_rate": 9.351314148062116e-06,
      "loss": 1.9378,
      "step": 4091
    },
    {
      "epoch": 1.584204413472706,
      "grad_norm": 37.60462951660156,
      "learning_rate": 9.350883985030328e-06,
      "loss": 2.216,
      "step": 4092
    },
    {
      "epoch": 1.5845915602013163,
      "grad_norm": 16.26364517211914,
      "learning_rate": 9.350453821998539e-06,
      "loss": 1.5507,
      "step": 4093
    },
    {
      "epoch": 1.5849787069299266,
      "grad_norm": 11.724665641784668,
      "learning_rate": 9.350023658966749e-06,
      "loss": 0.769,
      "step": 4094
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 15.551807403564453,
      "learning_rate": 9.34959349593496e-06,
      "loss": 1.7552,
      "step": 4095
    },
    {
      "epoch": 1.5857530003871467,
      "grad_norm": 22.834827423095703,
      "learning_rate": 9.34916333290317e-06,
      "loss": 1.9,
      "step": 4096
    },
    {
      "epoch": 1.5861401471157568,
      "grad_norm": 31.48217010498047,
      "learning_rate": 9.348733169871383e-06,
      "loss": 1.6551,
      "step": 4097
    },
    {
      "epoch": 1.5865272938443669,
      "grad_norm": 22.74690818786621,
      "learning_rate": 9.348303006839593e-06,
      "loss": 1.5176,
      "step": 4098
    },
    {
      "epoch": 1.5869144405729771,
      "grad_norm": 15.243289947509766,
      "learning_rate": 9.347872843807804e-06,
      "loss": 1.1578,
      "step": 4099
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 14.489785194396973,
      "learning_rate": 9.347442680776014e-06,
      "loss": 1.1397,
      "step": 4100
    },
    {
      "epoch": 1.5876887340301975,
      "grad_norm": 24.281557083129883,
      "learning_rate": 9.347012517744227e-06,
      "loss": 2.2119,
      "step": 4101
    },
    {
      "epoch": 1.5880758807588076,
      "grad_norm": 11.3148832321167,
      "learning_rate": 9.346582354712437e-06,
      "loss": 2.0416,
      "step": 4102
    },
    {
      "epoch": 1.5884630274874176,
      "grad_norm": 39.257720947265625,
      "learning_rate": 9.346152191680648e-06,
      "loss": 1.4669,
      "step": 4103
    },
    {
      "epoch": 1.588850174216028,
      "grad_norm": 15.148138999938965,
      "learning_rate": 9.345722028648858e-06,
      "loss": 1.3049,
      "step": 4104
    },
    {
      "epoch": 1.589237320944638,
      "grad_norm": 17.409446716308594,
      "learning_rate": 9.34529186561707e-06,
      "loss": 1.3957,
      "step": 4105
    },
    {
      "epoch": 1.5896244676732483,
      "grad_norm": 16.51162338256836,
      "learning_rate": 9.34486170258528e-06,
      "loss": 1.9267,
      "step": 4106
    },
    {
      "epoch": 1.5900116144018583,
      "grad_norm": 14.548907279968262,
      "learning_rate": 9.344431539553492e-06,
      "loss": 1.014,
      "step": 4107
    },
    {
      "epoch": 1.5903987611304684,
      "grad_norm": 17.87949562072754,
      "learning_rate": 9.344001376521703e-06,
      "loss": 1.3039,
      "step": 4108
    },
    {
      "epoch": 1.5907859078590785,
      "grad_norm": 12.701443672180176,
      "learning_rate": 9.343571213489913e-06,
      "loss": 0.8302,
      "step": 4109
    },
    {
      "epoch": 1.5911730545876888,
      "grad_norm": 24.646343231201172,
      "learning_rate": 9.343141050458125e-06,
      "loss": 2.0979,
      "step": 4110
    },
    {
      "epoch": 1.5915602013162988,
      "grad_norm": 16.470911026000977,
      "learning_rate": 9.342710887426334e-06,
      "loss": 0.8659,
      "step": 4111
    },
    {
      "epoch": 1.5919473480449091,
      "grad_norm": 19.824012756347656,
      "learning_rate": 9.342280724394547e-06,
      "loss": 0.9924,
      "step": 4112
    },
    {
      "epoch": 1.5923344947735192,
      "grad_norm": 30.213884353637695,
      "learning_rate": 9.341850561362757e-06,
      "loss": 2.0693,
      "step": 4113
    },
    {
      "epoch": 1.5927216415021292,
      "grad_norm": 45.39655303955078,
      "learning_rate": 9.341420398330969e-06,
      "loss": 1.9287,
      "step": 4114
    },
    {
      "epoch": 1.5931087882307393,
      "grad_norm": 31.958852767944336,
      "learning_rate": 9.340990235299178e-06,
      "loss": 1.4746,
      "step": 4115
    },
    {
      "epoch": 1.5934959349593496,
      "grad_norm": 6.947291374206543,
      "learning_rate": 9.340560072267391e-06,
      "loss": 0.4581,
      "step": 4116
    },
    {
      "epoch": 1.59388308168796,
      "grad_norm": 21.103078842163086,
      "learning_rate": 9.340129909235601e-06,
      "loss": 1.9159,
      "step": 4117
    },
    {
      "epoch": 1.59427022841657,
      "grad_norm": 18.70794677734375,
      "learning_rate": 9.339699746203812e-06,
      "loss": 1.2624,
      "step": 4118
    },
    {
      "epoch": 1.59465737514518,
      "grad_norm": 13.5807466506958,
      "learning_rate": 9.339269583172022e-06,
      "loss": 0.8633,
      "step": 4119
    },
    {
      "epoch": 1.59504452187379,
      "grad_norm": 22.455869674682617,
      "learning_rate": 9.338839420140234e-06,
      "loss": 2.0457,
      "step": 4120
    },
    {
      "epoch": 1.5954316686024002,
      "grad_norm": 16.224042892456055,
      "learning_rate": 9.338409257108445e-06,
      "loss": 1.3245,
      "step": 4121
    },
    {
      "epoch": 1.5958188153310104,
      "grad_norm": 31.20893669128418,
      "learning_rate": 9.337979094076656e-06,
      "loss": 1.655,
      "step": 4122
    },
    {
      "epoch": 1.5962059620596207,
      "grad_norm": 19.57966423034668,
      "learning_rate": 9.337548931044866e-06,
      "loss": 1.9226,
      "step": 4123
    },
    {
      "epoch": 1.5965931087882308,
      "grad_norm": 22.400249481201172,
      "learning_rate": 9.337118768013078e-06,
      "loss": 1.3418,
      "step": 4124
    },
    {
      "epoch": 1.5969802555168409,
      "grad_norm": 12.911104202270508,
      "learning_rate": 9.336688604981289e-06,
      "loss": 0.7448,
      "step": 4125
    },
    {
      "epoch": 1.597367402245451,
      "grad_norm": 28.309226989746094,
      "learning_rate": 9.336258441949499e-06,
      "loss": 1.4788,
      "step": 4126
    },
    {
      "epoch": 1.5977545489740612,
      "grad_norm": 16.60659408569336,
      "learning_rate": 9.33582827891771e-06,
      "loss": 1.2268,
      "step": 4127
    },
    {
      "epoch": 1.5981416957026713,
      "grad_norm": 22.43778419494629,
      "learning_rate": 9.335398115885922e-06,
      "loss": 1.2828,
      "step": 4128
    },
    {
      "epoch": 1.5985288424312816,
      "grad_norm": 17.824411392211914,
      "learning_rate": 9.334967952854133e-06,
      "loss": 1.2235,
      "step": 4129
    },
    {
      "epoch": 1.5989159891598916,
      "grad_norm": 23.8215274810791,
      "learning_rate": 9.334537789822343e-06,
      "loss": 1.3491,
      "step": 4130
    },
    {
      "epoch": 1.5993031358885017,
      "grad_norm": 11.722540855407715,
      "learning_rate": 9.334107626790554e-06,
      "loss": 0.6537,
      "step": 4131
    },
    {
      "epoch": 1.5996902826171118,
      "grad_norm": 24.932300567626953,
      "learning_rate": 9.333677463758766e-06,
      "loss": 1.3896,
      "step": 4132
    },
    {
      "epoch": 1.600077429345722,
      "grad_norm": 15.977005004882812,
      "learning_rate": 9.333247300726977e-06,
      "loss": 1.1009,
      "step": 4133
    },
    {
      "epoch": 1.6004645760743321,
      "grad_norm": 32.38204574584961,
      "learning_rate": 9.332817137695187e-06,
      "loss": 2.547,
      "step": 4134
    },
    {
      "epoch": 1.6008517228029424,
      "grad_norm": 22.532379150390625,
      "learning_rate": 9.332386974663398e-06,
      "loss": 1.7098,
      "step": 4135
    },
    {
      "epoch": 1.6012388695315525,
      "grad_norm": 23.96293830871582,
      "learning_rate": 9.33195681163161e-06,
      "loss": 2.0297,
      "step": 4136
    },
    {
      "epoch": 1.6016260162601625,
      "grad_norm": 28.883560180664062,
      "learning_rate": 9.331526648599821e-06,
      "loss": 1.0756,
      "step": 4137
    },
    {
      "epoch": 1.6020131629887726,
      "grad_norm": 28.403955459594727,
      "learning_rate": 9.33109648556803e-06,
      "loss": 2.1112,
      "step": 4138
    },
    {
      "epoch": 1.602400309717383,
      "grad_norm": 26.25531768798828,
      "learning_rate": 9.330666322536242e-06,
      "loss": 1.7434,
      "step": 4139
    },
    {
      "epoch": 1.6027874564459932,
      "grad_norm": 23.409690856933594,
      "learning_rate": 9.330236159504453e-06,
      "loss": 1.592,
      "step": 4140
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 37.810890197753906,
      "learning_rate": 9.329805996472663e-06,
      "loss": 1.0607,
      "step": 4141
    },
    {
      "epoch": 1.6035617499032133,
      "grad_norm": 13.868578910827637,
      "learning_rate": 9.329375833440875e-06,
      "loss": 1.3903,
      "step": 4142
    },
    {
      "epoch": 1.6039488966318234,
      "grad_norm": 26.629364013671875,
      "learning_rate": 9.328945670409086e-06,
      "loss": 2.5481,
      "step": 4143
    },
    {
      "epoch": 1.6043360433604335,
      "grad_norm": 16.671491622924805,
      "learning_rate": 9.328515507377297e-06,
      "loss": 1.5453,
      "step": 4144
    },
    {
      "epoch": 1.6047231900890437,
      "grad_norm": 54.28145217895508,
      "learning_rate": 9.328085344345507e-06,
      "loss": 2.6773,
      "step": 4145
    },
    {
      "epoch": 1.605110336817654,
      "grad_norm": 18.708803176879883,
      "learning_rate": 9.327655181313719e-06,
      "loss": 1.0173,
      "step": 4146
    },
    {
      "epoch": 1.605497483546264,
      "grad_norm": 19.974239349365234,
      "learning_rate": 9.32722501828193e-06,
      "loss": 1.6406,
      "step": 4147
    },
    {
      "epoch": 1.6058846302748742,
      "grad_norm": 18.127216339111328,
      "learning_rate": 9.326794855250141e-06,
      "loss": 1.4369,
      "step": 4148
    },
    {
      "epoch": 1.6062717770034842,
      "grad_norm": 33.32419204711914,
      "learning_rate": 9.326364692218351e-06,
      "loss": 2.1967,
      "step": 4149
    },
    {
      "epoch": 1.6066589237320945,
      "grad_norm": 16.124752044677734,
      "learning_rate": 9.325934529186563e-06,
      "loss": 1.654,
      "step": 4150
    },
    {
      "epoch": 1.6070460704607046,
      "grad_norm": 8.692889213562012,
      "learning_rate": 9.325504366154774e-06,
      "loss": 0.3177,
      "step": 4151
    },
    {
      "epoch": 1.6074332171893149,
      "grad_norm": 16.90325355529785,
      "learning_rate": 9.325074203122985e-06,
      "loss": 1.2529,
      "step": 4152
    },
    {
      "epoch": 1.607820363917925,
      "grad_norm": 30.52513885498047,
      "learning_rate": 9.324644040091195e-06,
      "loss": 2.2081,
      "step": 4153
    },
    {
      "epoch": 1.608207510646535,
      "grad_norm": 15.308099746704102,
      "learning_rate": 9.324213877059407e-06,
      "loss": 1.3101,
      "step": 4154
    },
    {
      "epoch": 1.608594657375145,
      "grad_norm": 21.37240219116211,
      "learning_rate": 9.323783714027618e-06,
      "loss": 1.5438,
      "step": 4155
    },
    {
      "epoch": 1.6089818041037554,
      "grad_norm": 17.684429168701172,
      "learning_rate": 9.323353550995828e-06,
      "loss": 1.4766,
      "step": 4156
    },
    {
      "epoch": 1.6093689508323654,
      "grad_norm": 34.76742935180664,
      "learning_rate": 9.322923387964039e-06,
      "loss": 2.5863,
      "step": 4157
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 14.996479988098145,
      "learning_rate": 9.32249322493225e-06,
      "loss": 1.3988,
      "step": 4158
    },
    {
      "epoch": 1.6101432442895858,
      "grad_norm": 13.54751205444336,
      "learning_rate": 9.322063061900462e-06,
      "loss": 1.3273,
      "step": 4159
    },
    {
      "epoch": 1.6105303910181958,
      "grad_norm": 24.23432731628418,
      "learning_rate": 9.321632898868672e-06,
      "loss": 2.0658,
      "step": 4160
    },
    {
      "epoch": 1.610917537746806,
      "grad_norm": 13.856364250183105,
      "learning_rate": 9.321202735836883e-06,
      "loss": 1.2677,
      "step": 4161
    },
    {
      "epoch": 1.6113046844754162,
      "grad_norm": 16.99272346496582,
      "learning_rate": 9.320772572805093e-06,
      "loss": 1.1977,
      "step": 4162
    },
    {
      "epoch": 1.6116918312040265,
      "grad_norm": 15.436083793640137,
      "learning_rate": 9.320342409773306e-06,
      "loss": 1.2923,
      "step": 4163
    },
    {
      "epoch": 1.6120789779326365,
      "grad_norm": 17.554893493652344,
      "learning_rate": 9.319912246741516e-06,
      "loss": 1.8017,
      "step": 4164
    },
    {
      "epoch": 1.6124661246612466,
      "grad_norm": 14.737011909484863,
      "learning_rate": 9.319482083709727e-06,
      "loss": 1.489,
      "step": 4165
    },
    {
      "epoch": 1.6128532713898567,
      "grad_norm": 25.779897689819336,
      "learning_rate": 9.319051920677937e-06,
      "loss": 1.5019,
      "step": 4166
    },
    {
      "epoch": 1.6132404181184667,
      "grad_norm": 15.315616607666016,
      "learning_rate": 9.31862175764615e-06,
      "loss": 1.5956,
      "step": 4167
    },
    {
      "epoch": 1.613627564847077,
      "grad_norm": 21.50710105895996,
      "learning_rate": 9.31819159461436e-06,
      "loss": 1.4919,
      "step": 4168
    },
    {
      "epoch": 1.6140147115756873,
      "grad_norm": 19.70393943786621,
      "learning_rate": 9.317761431582571e-06,
      "loss": 1.7546,
      "step": 4169
    },
    {
      "epoch": 1.6144018583042974,
      "grad_norm": 14.826733589172363,
      "learning_rate": 9.31733126855078e-06,
      "loss": 1.4941,
      "step": 4170
    },
    {
      "epoch": 1.6147890050329075,
      "grad_norm": 23.495529174804688,
      "learning_rate": 9.316901105518992e-06,
      "loss": 1.5315,
      "step": 4171
    },
    {
      "epoch": 1.6151761517615175,
      "grad_norm": 15.056807518005371,
      "learning_rate": 9.316470942487204e-06,
      "loss": 1.3017,
      "step": 4172
    },
    {
      "epoch": 1.6155632984901278,
      "grad_norm": 13.43264102935791,
      "learning_rate": 9.316040779455415e-06,
      "loss": 1.1169,
      "step": 4173
    },
    {
      "epoch": 1.6159504452187379,
      "grad_norm": 25.123432159423828,
      "learning_rate": 9.315610616423625e-06,
      "loss": 2.1752,
      "step": 4174
    },
    {
      "epoch": 1.6163375919473482,
      "grad_norm": 17.843761444091797,
      "learning_rate": 9.315180453391836e-06,
      "loss": 1.7174,
      "step": 4175
    },
    {
      "epoch": 1.6167247386759582,
      "grad_norm": 24.96756362915039,
      "learning_rate": 9.314750290360047e-06,
      "loss": 1.1467,
      "step": 4176
    },
    {
      "epoch": 1.6171118854045683,
      "grad_norm": 13.098641395568848,
      "learning_rate": 9.314320127328257e-06,
      "loss": 0.8414,
      "step": 4177
    },
    {
      "epoch": 1.6174990321331784,
      "grad_norm": 11.352010726928711,
      "learning_rate": 9.313889964296469e-06,
      "loss": 1.2574,
      "step": 4178
    },
    {
      "epoch": 1.6178861788617886,
      "grad_norm": 27.216270446777344,
      "learning_rate": 9.31345980126468e-06,
      "loss": 2.0836,
      "step": 4179
    },
    {
      "epoch": 1.6182733255903987,
      "grad_norm": 25.475690841674805,
      "learning_rate": 9.313029638232891e-06,
      "loss": 1.4347,
      "step": 4180
    },
    {
      "epoch": 1.618660472319009,
      "grad_norm": 12.843114852905273,
      "learning_rate": 9.312599475201101e-06,
      "loss": 1.5469,
      "step": 4181
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 14.590983390808105,
      "learning_rate": 9.312169312169313e-06,
      "loss": 0.8702,
      "step": 4182
    },
    {
      "epoch": 1.6194347657762291,
      "grad_norm": 20.735177993774414,
      "learning_rate": 9.311739149137524e-06,
      "loss": 3.3929,
      "step": 4183
    },
    {
      "epoch": 1.6198219125048392,
      "grad_norm": 15.68274974822998,
      "learning_rate": 9.311308986105735e-06,
      "loss": 1.4237,
      "step": 4184
    },
    {
      "epoch": 1.6202090592334495,
      "grad_norm": 24.041292190551758,
      "learning_rate": 9.310878823073945e-06,
      "loss": 1.3113,
      "step": 4185
    },
    {
      "epoch": 1.6205962059620598,
      "grad_norm": 13.684220314025879,
      "learning_rate": 9.310448660042157e-06,
      "loss": 1.3894,
      "step": 4186
    },
    {
      "epoch": 1.6209833526906698,
      "grad_norm": 20.61174774169922,
      "learning_rate": 9.310018497010368e-06,
      "loss": 3.274,
      "step": 4187
    },
    {
      "epoch": 1.62137049941928,
      "grad_norm": 22.825021743774414,
      "learning_rate": 9.30958833397858e-06,
      "loss": 2.1251,
      "step": 4188
    },
    {
      "epoch": 1.62175764614789,
      "grad_norm": 9.812503814697266,
      "learning_rate": 9.309158170946789e-06,
      "loss": 1.1048,
      "step": 4189
    },
    {
      "epoch": 1.6221447928765,
      "grad_norm": 26.1920223236084,
      "learning_rate": 9.308728007915e-06,
      "loss": 1.5608,
      "step": 4190
    },
    {
      "epoch": 1.6225319396051103,
      "grad_norm": 31.18404197692871,
      "learning_rate": 9.308297844883212e-06,
      "loss": 2.1664,
      "step": 4191
    },
    {
      "epoch": 1.6229190863337206,
      "grad_norm": 14.65050220489502,
      "learning_rate": 9.307867681851422e-06,
      "loss": 0.996,
      "step": 4192
    },
    {
      "epoch": 1.6233062330623307,
      "grad_norm": 21.569944381713867,
      "learning_rate": 9.307437518819633e-06,
      "loss": 0.8163,
      "step": 4193
    },
    {
      "epoch": 1.6236933797909407,
      "grad_norm": 17.927507400512695,
      "learning_rate": 9.307007355787845e-06,
      "loss": 1.0823,
      "step": 4194
    },
    {
      "epoch": 1.6240805265195508,
      "grad_norm": 15.77232837677002,
      "learning_rate": 9.306577192756056e-06,
      "loss": 1.3474,
      "step": 4195
    },
    {
      "epoch": 1.624467673248161,
      "grad_norm": 23.1616268157959,
      "learning_rate": 9.306147029724266e-06,
      "loss": 1.7759,
      "step": 4196
    },
    {
      "epoch": 1.6248548199767712,
      "grad_norm": 14.678008079528809,
      "learning_rate": 9.305716866692477e-06,
      "loss": 1.4977,
      "step": 4197
    },
    {
      "epoch": 1.6252419667053815,
      "grad_norm": 19.876976013183594,
      "learning_rate": 9.305286703660688e-06,
      "loss": 1.3032,
      "step": 4198
    },
    {
      "epoch": 1.6256291134339915,
      "grad_norm": 13.204312324523926,
      "learning_rate": 9.3048565406289e-06,
      "loss": 1.5177,
      "step": 4199
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 12.811399459838867,
      "learning_rate": 9.30442637759711e-06,
      "loss": 0.9981,
      "step": 4200
    },
    {
      "epoch": 1.6264034068912117,
      "grad_norm": 17.186368942260742,
      "learning_rate": 9.303996214565321e-06,
      "loss": 1.5928,
      "step": 4201
    },
    {
      "epoch": 1.626790553619822,
      "grad_norm": 23.72660255432129,
      "learning_rate": 9.303566051533532e-06,
      "loss": 1.422,
      "step": 4202
    },
    {
      "epoch": 1.627177700348432,
      "grad_norm": 25.532670974731445,
      "learning_rate": 9.303135888501744e-06,
      "loss": 1.614,
      "step": 4203
    },
    {
      "epoch": 1.6275648470770423,
      "grad_norm": 19.226802825927734,
      "learning_rate": 9.302705725469954e-06,
      "loss": 1.4732,
      "step": 4204
    },
    {
      "epoch": 1.6279519938056524,
      "grad_norm": 30.27008819580078,
      "learning_rate": 9.302275562438165e-06,
      "loss": 3.1302,
      "step": 4205
    },
    {
      "epoch": 1.6283391405342624,
      "grad_norm": 27.660324096679688,
      "learning_rate": 9.301845399406376e-06,
      "loss": 1.8533,
      "step": 4206
    },
    {
      "epoch": 1.6287262872628725,
      "grad_norm": 9.290343284606934,
      "learning_rate": 9.301415236374586e-06,
      "loss": 0.9351,
      "step": 4207
    },
    {
      "epoch": 1.6291134339914828,
      "grad_norm": 14.27415657043457,
      "learning_rate": 9.300985073342798e-06,
      "loss": 1.0368,
      "step": 4208
    },
    {
      "epoch": 1.629500580720093,
      "grad_norm": 26.43857192993164,
      "learning_rate": 9.300554910311009e-06,
      "loss": 1.8459,
      "step": 4209
    },
    {
      "epoch": 1.6298877274487031,
      "grad_norm": 7.016826152801514,
      "learning_rate": 9.30012474727922e-06,
      "loss": 0.4597,
      "step": 4210
    },
    {
      "epoch": 1.6302748741773132,
      "grad_norm": 12.5070161819458,
      "learning_rate": 9.29969458424743e-06,
      "loss": 0.8543,
      "step": 4211
    },
    {
      "epoch": 1.6306620209059233,
      "grad_norm": 10.646514892578125,
      "learning_rate": 9.299264421215642e-06,
      "loss": 1.369,
      "step": 4212
    },
    {
      "epoch": 1.6310491676345333,
      "grad_norm": 42.99148178100586,
      "learning_rate": 9.298834258183851e-06,
      "loss": 2.0581,
      "step": 4213
    },
    {
      "epoch": 1.6314363143631436,
      "grad_norm": 19.09624671936035,
      "learning_rate": 9.298404095152064e-06,
      "loss": 1.6861,
      "step": 4214
    },
    {
      "epoch": 1.631823461091754,
      "grad_norm": 10.415050506591797,
      "learning_rate": 9.297973932120274e-06,
      "loss": 0.655,
      "step": 4215
    },
    {
      "epoch": 1.632210607820364,
      "grad_norm": 35.44053268432617,
      "learning_rate": 9.297543769088485e-06,
      "loss": 2.2995,
      "step": 4216
    },
    {
      "epoch": 1.632597754548974,
      "grad_norm": 16.418033599853516,
      "learning_rate": 9.297113606056695e-06,
      "loss": 1.4826,
      "step": 4217
    },
    {
      "epoch": 1.6329849012775841,
      "grad_norm": 22.52870750427246,
      "learning_rate": 9.296683443024908e-06,
      "loss": 1.4953,
      "step": 4218
    },
    {
      "epoch": 1.6333720480061944,
      "grad_norm": 14.037686347961426,
      "learning_rate": 9.296253279993118e-06,
      "loss": 1.4054,
      "step": 4219
    },
    {
      "epoch": 1.6337591947348045,
      "grad_norm": 24.507959365844727,
      "learning_rate": 9.29582311696133e-06,
      "loss": 1.8903,
      "step": 4220
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 17.28823471069336,
      "learning_rate": 9.29539295392954e-06,
      "loss": 1.5439,
      "step": 4221
    },
    {
      "epoch": 1.6345334881920248,
      "grad_norm": 14.46258544921875,
      "learning_rate": 9.29496279089775e-06,
      "loss": 1.507,
      "step": 4222
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 13.656926155090332,
      "learning_rate": 9.294532627865962e-06,
      "loss": 1.3435,
      "step": 4223
    },
    {
      "epoch": 1.635307781649245,
      "grad_norm": 13.297588348388672,
      "learning_rate": 9.294102464834173e-06,
      "loss": 1.0581,
      "step": 4224
    },
    {
      "epoch": 1.6356949283778552,
      "grad_norm": 20.246889114379883,
      "learning_rate": 9.293672301802383e-06,
      "loss": 1.5925,
      "step": 4225
    },
    {
      "epoch": 1.6360820751064653,
      "grad_norm": 10.820215225219727,
      "learning_rate": 9.293242138770595e-06,
      "loss": 0.6689,
      "step": 4226
    },
    {
      "epoch": 1.6364692218350756,
      "grad_norm": 22.302051544189453,
      "learning_rate": 9.292811975738806e-06,
      "loss": 1.9669,
      "step": 4227
    },
    {
      "epoch": 1.6368563685636857,
      "grad_norm": 27.759904861450195,
      "learning_rate": 9.292381812707016e-06,
      "loss": 1.5314,
      "step": 4228
    },
    {
      "epoch": 1.6372435152922957,
      "grad_norm": 12.62214469909668,
      "learning_rate": 9.291951649675229e-06,
      "loss": 1.1228,
      "step": 4229
    },
    {
      "epoch": 1.6376306620209058,
      "grad_norm": 13.731943130493164,
      "learning_rate": 9.291521486643439e-06,
      "loss": 0.8792,
      "step": 4230
    },
    {
      "epoch": 1.638017808749516,
      "grad_norm": 16.60959243774414,
      "learning_rate": 9.29109132361165e-06,
      "loss": 1.7162,
      "step": 4231
    },
    {
      "epoch": 1.6384049554781264,
      "grad_norm": 14.50012493133545,
      "learning_rate": 9.29066116057986e-06,
      "loss": 1.1921,
      "step": 4232
    },
    {
      "epoch": 1.6387921022067364,
      "grad_norm": 16.69760513305664,
      "learning_rate": 9.290230997548073e-06,
      "loss": 1.6685,
      "step": 4233
    },
    {
      "epoch": 1.6391792489353465,
      "grad_norm": 22.938236236572266,
      "learning_rate": 9.289800834516283e-06,
      "loss": 1.5133,
      "step": 4234
    },
    {
      "epoch": 1.6395663956639566,
      "grad_norm": 24.44687271118164,
      "learning_rate": 9.289370671484494e-06,
      "loss": 1.0277,
      "step": 4235
    },
    {
      "epoch": 1.6399535423925666,
      "grad_norm": 14.63229751586914,
      "learning_rate": 9.288940508452704e-06,
      "loss": 1.0233,
      "step": 4236
    },
    {
      "epoch": 1.640340689121177,
      "grad_norm": 16.095504760742188,
      "learning_rate": 9.288510345420915e-06,
      "loss": 1.7066,
      "step": 4237
    },
    {
      "epoch": 1.6407278358497872,
      "grad_norm": 15.68777847290039,
      "learning_rate": 9.288080182389126e-06,
      "loss": 1.568,
      "step": 4238
    },
    {
      "epoch": 1.6411149825783973,
      "grad_norm": 22.765361785888672,
      "learning_rate": 9.287650019357338e-06,
      "loss": 1.3658,
      "step": 4239
    },
    {
      "epoch": 1.6415021293070073,
      "grad_norm": 11.377098083496094,
      "learning_rate": 9.287219856325548e-06,
      "loss": 0.8741,
      "step": 4240
    },
    {
      "epoch": 1.6418892760356174,
      "grad_norm": 20.876005172729492,
      "learning_rate": 9.286789693293759e-06,
      "loss": 1.3681,
      "step": 4241
    },
    {
      "epoch": 1.6422764227642277,
      "grad_norm": 17.71363639831543,
      "learning_rate": 9.28635953026197e-06,
      "loss": 1.8792,
      "step": 4242
    },
    {
      "epoch": 1.6426635694928378,
      "grad_norm": 23.507291793823242,
      "learning_rate": 9.28592936723018e-06,
      "loss": 1.5045,
      "step": 4243
    },
    {
      "epoch": 1.643050716221448,
      "grad_norm": 15.871716499328613,
      "learning_rate": 9.285499204198392e-06,
      "loss": 1.4958,
      "step": 4244
    },
    {
      "epoch": 1.6434378629500581,
      "grad_norm": 18.63460922241211,
      "learning_rate": 9.285069041166603e-06,
      "loss": 2.016,
      "step": 4245
    },
    {
      "epoch": 1.6438250096786682,
      "grad_norm": 16.705488204956055,
      "learning_rate": 9.284638878134814e-06,
      "loss": 1.6284,
      "step": 4246
    },
    {
      "epoch": 1.6442121564072782,
      "grad_norm": 18.565675735473633,
      "learning_rate": 9.284208715103024e-06,
      "loss": 1.2204,
      "step": 4247
    },
    {
      "epoch": 1.6445993031358885,
      "grad_norm": 24.591215133666992,
      "learning_rate": 9.283778552071236e-06,
      "loss": 1.6367,
      "step": 4248
    },
    {
      "epoch": 1.6449864498644986,
      "grad_norm": 14.470715522766113,
      "learning_rate": 9.283348389039447e-06,
      "loss": 1.267,
      "step": 4249
    },
    {
      "epoch": 1.645373596593109,
      "grad_norm": 15.463166236877441,
      "learning_rate": 9.282918226007658e-06,
      "loss": 1.5126,
      "step": 4250
    },
    {
      "epoch": 1.645760743321719,
      "grad_norm": 15.275872230529785,
      "learning_rate": 9.282488062975868e-06,
      "loss": 1.6545,
      "step": 4251
    },
    {
      "epoch": 1.646147890050329,
      "grad_norm": 13.41774845123291,
      "learning_rate": 9.28205789994408e-06,
      "loss": 1.4796,
      "step": 4252
    },
    {
      "epoch": 1.646535036778939,
      "grad_norm": 33.589622497558594,
      "learning_rate": 9.281627736912291e-06,
      "loss": 2.7132,
      "step": 4253
    },
    {
      "epoch": 1.6469221835075494,
      "grad_norm": 17.634363174438477,
      "learning_rate": 9.281197573880502e-06,
      "loss": 1.6941,
      "step": 4254
    },
    {
      "epoch": 1.6473093302361597,
      "grad_norm": 32.556678771972656,
      "learning_rate": 9.280767410848712e-06,
      "loss": 1.1174,
      "step": 4255
    },
    {
      "epoch": 1.6476964769647697,
      "grad_norm": 17.827165603637695,
      "learning_rate": 9.280337247816923e-06,
      "loss": 0.3475,
      "step": 4256
    },
    {
      "epoch": 1.6480836236933798,
      "grad_norm": 26.73976707458496,
      "learning_rate": 9.279907084785135e-06,
      "loss": 1.3809,
      "step": 4257
    },
    {
      "epoch": 1.6484707704219899,
      "grad_norm": 26.423274993896484,
      "learning_rate": 9.279476921753345e-06,
      "loss": 1.3127,
      "step": 4258
    },
    {
      "epoch": 1.6488579171506,
      "grad_norm": 20.93657875061035,
      "learning_rate": 9.279046758721556e-06,
      "loss": 1.7048,
      "step": 4259
    },
    {
      "epoch": 1.6492450638792102,
      "grad_norm": 17.898195266723633,
      "learning_rate": 9.278616595689767e-06,
      "loss": 1.2714,
      "step": 4260
    },
    {
      "epoch": 1.6496322106078205,
      "grad_norm": 18.79642677307129,
      "learning_rate": 9.278186432657979e-06,
      "loss": 1.6333,
      "step": 4261
    },
    {
      "epoch": 1.6500193573364306,
      "grad_norm": 18.392478942871094,
      "learning_rate": 9.277756269626189e-06,
      "loss": 1.3411,
      "step": 4262
    },
    {
      "epoch": 1.6504065040650406,
      "grad_norm": 15.21732234954834,
      "learning_rate": 9.2773261065944e-06,
      "loss": 1.0661,
      "step": 4263
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 17.833663940429688,
      "learning_rate": 9.27689594356261e-06,
      "loss": 1.5846,
      "step": 4264
    },
    {
      "epoch": 1.651180797522261,
      "grad_norm": 11.36895751953125,
      "learning_rate": 9.276465780530823e-06,
      "loss": 0.6142,
      "step": 4265
    },
    {
      "epoch": 1.651567944250871,
      "grad_norm": 24.013355255126953,
      "learning_rate": 9.276035617499033e-06,
      "loss": 1.7517,
      "step": 4266
    },
    {
      "epoch": 1.6519550909794813,
      "grad_norm": 14.679115295410156,
      "learning_rate": 9.275605454467244e-06,
      "loss": 1.1594,
      "step": 4267
    },
    {
      "epoch": 1.6523422377080914,
      "grad_norm": 26.492172241210938,
      "learning_rate": 9.275175291435454e-06,
      "loss": 1.1012,
      "step": 4268
    },
    {
      "epoch": 1.6527293844367015,
      "grad_norm": 28.05732536315918,
      "learning_rate": 9.274745128403667e-06,
      "loss": 1.7222,
      "step": 4269
    },
    {
      "epoch": 1.6531165311653115,
      "grad_norm": 12.832670211791992,
      "learning_rate": 9.274314965371877e-06,
      "loss": 0.7658,
      "step": 4270
    },
    {
      "epoch": 1.6535036778939218,
      "grad_norm": 31.60272979736328,
      "learning_rate": 9.273884802340088e-06,
      "loss": 2.6542,
      "step": 4271
    },
    {
      "epoch": 1.653890824622532,
      "grad_norm": 29.59255599975586,
      "learning_rate": 9.2734546393083e-06,
      "loss": 1.4137,
      "step": 4272
    },
    {
      "epoch": 1.6542779713511422,
      "grad_norm": 25.302927017211914,
      "learning_rate": 9.273024476276509e-06,
      "loss": 2.4924,
      "step": 4273
    },
    {
      "epoch": 1.6546651180797523,
      "grad_norm": 15.19959831237793,
      "learning_rate": 9.27259431324472e-06,
      "loss": 1.2752,
      "step": 4274
    },
    {
      "epoch": 1.6550522648083623,
      "grad_norm": 25.1585750579834,
      "learning_rate": 9.272164150212932e-06,
      "loss": 3.1266,
      "step": 4275
    },
    {
      "epoch": 1.6554394115369724,
      "grad_norm": 31.60601043701172,
      "learning_rate": 9.271733987181143e-06,
      "loss": 2.4035,
      "step": 4276
    },
    {
      "epoch": 1.6558265582655827,
      "grad_norm": 13.022612571716309,
      "learning_rate": 9.271303824149353e-06,
      "loss": 0.9478,
      "step": 4277
    },
    {
      "epoch": 1.656213704994193,
      "grad_norm": 14.370631217956543,
      "learning_rate": 9.270873661117564e-06,
      "loss": 0.7121,
      "step": 4278
    },
    {
      "epoch": 1.656600851722803,
      "grad_norm": 33.447235107421875,
      "learning_rate": 9.270443498085774e-06,
      "loss": 1.609,
      "step": 4279
    },
    {
      "epoch": 1.656987998451413,
      "grad_norm": 18.562095642089844,
      "learning_rate": 9.270013335053987e-06,
      "loss": 1.874,
      "step": 4280
    },
    {
      "epoch": 1.6573751451800232,
      "grad_norm": 16.100461959838867,
      "learning_rate": 9.269583172022197e-06,
      "loss": 0.8413,
      "step": 4281
    },
    {
      "epoch": 1.6577622919086332,
      "grad_norm": 17.66569709777832,
      "learning_rate": 9.269153008990408e-06,
      "loss": 1.983,
      "step": 4282
    },
    {
      "epoch": 1.6581494386372435,
      "grad_norm": 27.425697326660156,
      "learning_rate": 9.268722845958618e-06,
      "loss": 1.4489,
      "step": 4283
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 22.65583038330078,
      "learning_rate": 9.268292682926831e-06,
      "loss": 1.4231,
      "step": 4284
    },
    {
      "epoch": 1.6589237320944639,
      "grad_norm": 20.165077209472656,
      "learning_rate": 9.267862519895041e-06,
      "loss": 1.0635,
      "step": 4285
    },
    {
      "epoch": 1.659310878823074,
      "grad_norm": 14.02633285522461,
      "learning_rate": 9.267432356863252e-06,
      "loss": 1.8176,
      "step": 4286
    },
    {
      "epoch": 1.659698025551684,
      "grad_norm": 14.355517387390137,
      "learning_rate": 9.267002193831462e-06,
      "loss": 1.4333,
      "step": 4287
    },
    {
      "epoch": 1.6600851722802943,
      "grad_norm": 36.96015548706055,
      "learning_rate": 9.266572030799674e-06,
      "loss": 1.6492,
      "step": 4288
    },
    {
      "epoch": 1.6604723190089044,
      "grad_norm": 25.71731948852539,
      "learning_rate": 9.266141867767885e-06,
      "loss": 1.6743,
      "step": 4289
    },
    {
      "epoch": 1.6608594657375146,
      "grad_norm": 30.2637882232666,
      "learning_rate": 9.265711704736096e-06,
      "loss": 2.5192,
      "step": 4290
    },
    {
      "epoch": 1.6612466124661247,
      "grad_norm": 40.778900146484375,
      "learning_rate": 9.265281541704306e-06,
      "loss": 2.7306,
      "step": 4291
    },
    {
      "epoch": 1.6616337591947348,
      "grad_norm": 15.002294540405273,
      "learning_rate": 9.264851378672518e-06,
      "loss": 1.8274,
      "step": 4292
    },
    {
      "epoch": 1.6620209059233448,
      "grad_norm": 44.533206939697266,
      "learning_rate": 9.264421215640729e-06,
      "loss": 1.7553,
      "step": 4293
    },
    {
      "epoch": 1.6624080526519551,
      "grad_norm": 5.980274200439453,
      "learning_rate": 9.263991052608939e-06,
      "loss": 0.1978,
      "step": 4294
    },
    {
      "epoch": 1.6627951993805652,
      "grad_norm": 22.488052368164062,
      "learning_rate": 9.26356088957715e-06,
      "loss": 1.5714,
      "step": 4295
    },
    {
      "epoch": 1.6631823461091755,
      "grad_norm": 35.71044158935547,
      "learning_rate": 9.263130726545361e-06,
      "loss": 1.4679,
      "step": 4296
    },
    {
      "epoch": 1.6635694928377855,
      "grad_norm": 16.018213272094727,
      "learning_rate": 9.262700563513573e-06,
      "loss": 1.6096,
      "step": 4297
    },
    {
      "epoch": 1.6639566395663956,
      "grad_norm": 32.71603775024414,
      "learning_rate": 9.262270400481783e-06,
      "loss": 1.745,
      "step": 4298
    },
    {
      "epoch": 1.6643437862950057,
      "grad_norm": 36.99655532836914,
      "learning_rate": 9.261840237449994e-06,
      "loss": 2.0758,
      "step": 4299
    },
    {
      "epoch": 1.664730933023616,
      "grad_norm": 33.20533752441406,
      "learning_rate": 9.261410074418205e-06,
      "loss": 2.9374,
      "step": 4300
    },
    {
      "epoch": 1.6651180797522263,
      "grad_norm": 30.99820899963379,
      "learning_rate": 9.260979911386417e-06,
      "loss": 2.0709,
      "step": 4301
    },
    {
      "epoch": 1.6655052264808363,
      "grad_norm": 29.09837532043457,
      "learning_rate": 9.260549748354627e-06,
      "loss": 0.6687,
      "step": 4302
    },
    {
      "epoch": 1.6658923732094464,
      "grad_norm": 13.331209182739258,
      "learning_rate": 9.260119585322838e-06,
      "loss": 0.9565,
      "step": 4303
    },
    {
      "epoch": 1.6662795199380565,
      "grad_norm": 26.241634368896484,
      "learning_rate": 9.25968942229105e-06,
      "loss": 2.8861,
      "step": 4304
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 35.369903564453125,
      "learning_rate": 9.25925925925926e-06,
      "loss": 2.2062,
      "step": 4305
    },
    {
      "epoch": 1.6670538133952768,
      "grad_norm": 21.87674903869629,
      "learning_rate": 9.25882909622747e-06,
      "loss": 1.0504,
      "step": 4306
    },
    {
      "epoch": 1.667440960123887,
      "grad_norm": 15.63996696472168,
      "learning_rate": 9.258398933195682e-06,
      "loss": 0.9692,
      "step": 4307
    },
    {
      "epoch": 1.6678281068524972,
      "grad_norm": 14.117318153381348,
      "learning_rate": 9.257968770163893e-06,
      "loss": 0.8856,
      "step": 4308
    },
    {
      "epoch": 1.6682152535811072,
      "grad_norm": 22.95919418334961,
      "learning_rate": 9.257538607132103e-06,
      "loss": 1.7984,
      "step": 4309
    },
    {
      "epoch": 1.6686024003097173,
      "grad_norm": 23.24422264099121,
      "learning_rate": 9.257108444100315e-06,
      "loss": 2.2067,
      "step": 4310
    },
    {
      "epoch": 1.6689895470383276,
      "grad_norm": 15.208248138427734,
      "learning_rate": 9.256678281068526e-06,
      "loss": 0.9882,
      "step": 4311
    },
    {
      "epoch": 1.6693766937669376,
      "grad_norm": 13.944034576416016,
      "learning_rate": 9.256248118036737e-06,
      "loss": 1.1578,
      "step": 4312
    },
    {
      "epoch": 1.669763840495548,
      "grad_norm": 10.636375427246094,
      "learning_rate": 9.255817955004947e-06,
      "loss": 0.6338,
      "step": 4313
    },
    {
      "epoch": 1.670150987224158,
      "grad_norm": 22.313589096069336,
      "learning_rate": 9.255387791973158e-06,
      "loss": 1.598,
      "step": 4314
    },
    {
      "epoch": 1.670538133952768,
      "grad_norm": 16.271928787231445,
      "learning_rate": 9.25495762894137e-06,
      "loss": 1.2873,
      "step": 4315
    },
    {
      "epoch": 1.6709252806813781,
      "grad_norm": 8.821406364440918,
      "learning_rate": 9.254527465909581e-06,
      "loss": 0.5773,
      "step": 4316
    },
    {
      "epoch": 1.6713124274099884,
      "grad_norm": 24.63607406616211,
      "learning_rate": 9.254097302877791e-06,
      "loss": 1.6379,
      "step": 4317
    },
    {
      "epoch": 1.6716995741385985,
      "grad_norm": 21.456645965576172,
      "learning_rate": 9.253667139846002e-06,
      "loss": 2.003,
      "step": 4318
    },
    {
      "epoch": 1.6720867208672088,
      "grad_norm": 25.302913665771484,
      "learning_rate": 9.253236976814214e-06,
      "loss": 1.9521,
      "step": 4319
    },
    {
      "epoch": 1.6724738675958188,
      "grad_norm": 18.597942352294922,
      "learning_rate": 9.252806813782425e-06,
      "loss": 1.6401,
      "step": 4320
    },
    {
      "epoch": 1.672861014324429,
      "grad_norm": 22.29169273376465,
      "learning_rate": 9.252376650750635e-06,
      "loss": 2.1138,
      "step": 4321
    },
    {
      "epoch": 1.673248161053039,
      "grad_norm": 19.93821907043457,
      "learning_rate": 9.251946487718846e-06,
      "loss": 1.3708,
      "step": 4322
    },
    {
      "epoch": 1.6736353077816493,
      "grad_norm": 11.243326187133789,
      "learning_rate": 9.251516324687058e-06,
      "loss": 0.9328,
      "step": 4323
    },
    {
      "epoch": 1.6740224545102595,
      "grad_norm": 17.13962745666504,
      "learning_rate": 9.251086161655268e-06,
      "loss": 1.6368,
      "step": 4324
    },
    {
      "epoch": 1.6744096012388696,
      "grad_norm": 11.87857723236084,
      "learning_rate": 9.250655998623479e-06,
      "loss": 1.0555,
      "step": 4325
    },
    {
      "epoch": 1.6747967479674797,
      "grad_norm": 20.99456214904785,
      "learning_rate": 9.25022583559169e-06,
      "loss": 1.7774,
      "step": 4326
    },
    {
      "epoch": 1.6751838946960897,
      "grad_norm": 15.499956130981445,
      "learning_rate": 9.249795672559902e-06,
      "loss": 1.2343,
      "step": 4327
    },
    {
      "epoch": 1.6755710414246998,
      "grad_norm": 18.337026596069336,
      "learning_rate": 9.249365509528112e-06,
      "loss": 1.3866,
      "step": 4328
    },
    {
      "epoch": 1.67595818815331,
      "grad_norm": 13.371996879577637,
      "learning_rate": 9.248935346496323e-06,
      "loss": 0.9432,
      "step": 4329
    },
    {
      "epoch": 1.6763453348819204,
      "grad_norm": 13.966998100280762,
      "learning_rate": 9.248505183464533e-06,
      "loss": 0.8803,
      "step": 4330
    },
    {
      "epoch": 1.6767324816105305,
      "grad_norm": 16.525474548339844,
      "learning_rate": 9.248075020432746e-06,
      "loss": 1.5461,
      "step": 4331
    },
    {
      "epoch": 1.6771196283391405,
      "grad_norm": 26.456859588623047,
      "learning_rate": 9.247644857400956e-06,
      "loss": 1.5127,
      "step": 4332
    },
    {
      "epoch": 1.6775067750677506,
      "grad_norm": 21.527803421020508,
      "learning_rate": 9.247214694369167e-06,
      "loss": 1.6696,
      "step": 4333
    },
    {
      "epoch": 1.6778939217963607,
      "grad_norm": 17.677209854125977,
      "learning_rate": 9.246784531337377e-06,
      "loss": 1.2548,
      "step": 4334
    },
    {
      "epoch": 1.678281068524971,
      "grad_norm": 22.845773696899414,
      "learning_rate": 9.24635436830559e-06,
      "loss": 1.5632,
      "step": 4335
    },
    {
      "epoch": 1.6786682152535812,
      "grad_norm": 25.04315757751465,
      "learning_rate": 9.2459242052738e-06,
      "loss": 1.5574,
      "step": 4336
    },
    {
      "epoch": 1.6790553619821913,
      "grad_norm": 19.744699478149414,
      "learning_rate": 9.245494042242011e-06,
      "loss": 1.7126,
      "step": 4337
    },
    {
      "epoch": 1.6794425087108014,
      "grad_norm": 17.08208656311035,
      "learning_rate": 9.24506387921022e-06,
      "loss": 1.65,
      "step": 4338
    },
    {
      "epoch": 1.6798296554394114,
      "grad_norm": 28.14984703063965,
      "learning_rate": 9.244633716178432e-06,
      "loss": 1.8355,
      "step": 4339
    },
    {
      "epoch": 1.6802168021680217,
      "grad_norm": 13.34072208404541,
      "learning_rate": 9.244203553146643e-06,
      "loss": 1.3777,
      "step": 4340
    },
    {
      "epoch": 1.6806039488966318,
      "grad_norm": 18.479145050048828,
      "learning_rate": 9.243773390114855e-06,
      "loss": 1.3024,
      "step": 4341
    },
    {
      "epoch": 1.680991095625242,
      "grad_norm": 19.02326202392578,
      "learning_rate": 9.243343227083065e-06,
      "loss": 1.6691,
      "step": 4342
    },
    {
      "epoch": 1.6813782423538521,
      "grad_norm": 25.373098373413086,
      "learning_rate": 9.242913064051276e-06,
      "loss": 1.8547,
      "step": 4343
    },
    {
      "epoch": 1.6817653890824622,
      "grad_norm": 15.198352813720703,
      "learning_rate": 9.242482901019487e-06,
      "loss": 1.4839,
      "step": 4344
    },
    {
      "epoch": 1.6821525358110723,
      "grad_norm": 24.25505256652832,
      "learning_rate": 9.242052737987697e-06,
      "loss": 1.0619,
      "step": 4345
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 14.855430603027344,
      "learning_rate": 9.241622574955909e-06,
      "loss": 0.7748,
      "step": 4346
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 20.986637115478516,
      "learning_rate": 9.24119241192412e-06,
      "loss": 1.7112,
      "step": 4347
    },
    {
      "epoch": 1.683313975996903,
      "grad_norm": 19.854455947875977,
      "learning_rate": 9.240762248892331e-06,
      "loss": 1.3627,
      "step": 4348
    },
    {
      "epoch": 1.683701122725513,
      "grad_norm": 25.990665435791016,
      "learning_rate": 9.240332085860541e-06,
      "loss": 1.7093,
      "step": 4349
    },
    {
      "epoch": 1.684088269454123,
      "grad_norm": 20.63603973388672,
      "learning_rate": 9.239901922828753e-06,
      "loss": 1.28,
      "step": 4350
    },
    {
      "epoch": 1.684475416182733,
      "grad_norm": 26.045167922973633,
      "learning_rate": 9.239471759796964e-06,
      "loss": 1.408,
      "step": 4351
    },
    {
      "epoch": 1.6848625629113434,
      "grad_norm": 17.984678268432617,
      "learning_rate": 9.239041596765175e-06,
      "loss": 1.4778,
      "step": 4352
    },
    {
      "epoch": 1.6852497096399537,
      "grad_norm": 16.21076774597168,
      "learning_rate": 9.238611433733385e-06,
      "loss": 1.5372,
      "step": 4353
    },
    {
      "epoch": 1.6856368563685638,
      "grad_norm": 8.907779693603516,
      "learning_rate": 9.238181270701596e-06,
      "loss": 1.1863,
      "step": 4354
    },
    {
      "epoch": 1.6860240030971738,
      "grad_norm": 25.727447509765625,
      "learning_rate": 9.237751107669808e-06,
      "loss": 1.6503,
      "step": 4355
    },
    {
      "epoch": 1.6864111498257839,
      "grad_norm": 21.82853889465332,
      "learning_rate": 9.23732094463802e-06,
      "loss": 1.2793,
      "step": 4356
    },
    {
      "epoch": 1.686798296554394,
      "grad_norm": 21.169342041015625,
      "learning_rate": 9.236890781606229e-06,
      "loss": 0.9887,
      "step": 4357
    },
    {
      "epoch": 1.6871854432830042,
      "grad_norm": 27.636934280395508,
      "learning_rate": 9.23646061857444e-06,
      "loss": 1.6634,
      "step": 4358
    },
    {
      "epoch": 1.6875725900116145,
      "grad_norm": 17.6590576171875,
      "learning_rate": 9.236030455542652e-06,
      "loss": 1.8629,
      "step": 4359
    },
    {
      "epoch": 1.6879597367402246,
      "grad_norm": 15.729812622070312,
      "learning_rate": 9.235600292510862e-06,
      "loss": 1.4779,
      "step": 4360
    },
    {
      "epoch": 1.6883468834688347,
      "grad_norm": 18.04213523864746,
      "learning_rate": 9.235170129479073e-06,
      "loss": 1.2494,
      "step": 4361
    },
    {
      "epoch": 1.6887340301974447,
      "grad_norm": 20.036334991455078,
      "learning_rate": 9.234739966447284e-06,
      "loss": 0.9702,
      "step": 4362
    },
    {
      "epoch": 1.689121176926055,
      "grad_norm": 17.286176681518555,
      "learning_rate": 9.234309803415496e-06,
      "loss": 1.5683,
      "step": 4363
    },
    {
      "epoch": 1.689508323654665,
      "grad_norm": 15.921704292297363,
      "learning_rate": 9.233879640383706e-06,
      "loss": 1.3013,
      "step": 4364
    },
    {
      "epoch": 1.6898954703832754,
      "grad_norm": 21.105113983154297,
      "learning_rate": 9.233449477351917e-06,
      "loss": 2.3469,
      "step": 4365
    },
    {
      "epoch": 1.6902826171118854,
      "grad_norm": 9.725896835327148,
      "learning_rate": 9.233019314320128e-06,
      "loss": 0.6969,
      "step": 4366
    },
    {
      "epoch": 1.6906697638404955,
      "grad_norm": 22.006921768188477,
      "learning_rate": 9.23258915128834e-06,
      "loss": 1.5145,
      "step": 4367
    },
    {
      "epoch": 1.6910569105691056,
      "grad_norm": 27.907228469848633,
      "learning_rate": 9.23215898825655e-06,
      "loss": 2.157,
      "step": 4368
    },
    {
      "epoch": 1.6914440572977159,
      "grad_norm": 14.99786376953125,
      "learning_rate": 9.231728825224761e-06,
      "loss": 1.5696,
      "step": 4369
    },
    {
      "epoch": 1.6918312040263261,
      "grad_norm": 24.94982147216797,
      "learning_rate": 9.231298662192972e-06,
      "loss": 1.8884,
      "step": 4370
    },
    {
      "epoch": 1.6922183507549362,
      "grad_norm": 17.390066146850586,
      "learning_rate": 9.230868499161184e-06,
      "loss": 1.5323,
      "step": 4371
    },
    {
      "epoch": 1.6926054974835463,
      "grad_norm": 11.922277450561523,
      "learning_rate": 9.230438336129394e-06,
      "loss": 1.4112,
      "step": 4372
    },
    {
      "epoch": 1.6929926442121563,
      "grad_norm": 34.425254821777344,
      "learning_rate": 9.230008173097605e-06,
      "loss": 1.8581,
      "step": 4373
    },
    {
      "epoch": 1.6933797909407664,
      "grad_norm": 28.555896759033203,
      "learning_rate": 9.229578010065816e-06,
      "loss": 0.847,
      "step": 4374
    },
    {
      "epoch": 1.6937669376693767,
      "grad_norm": 40.06651306152344,
      "learning_rate": 9.229147847034026e-06,
      "loss": 1.6084,
      "step": 4375
    },
    {
      "epoch": 1.694154084397987,
      "grad_norm": 21.804182052612305,
      "learning_rate": 9.228717684002237e-06,
      "loss": 1.7286,
      "step": 4376
    },
    {
      "epoch": 1.694541231126597,
      "grad_norm": 10.066481590270996,
      "learning_rate": 9.228287520970449e-06,
      "loss": 0.6464,
      "step": 4377
    },
    {
      "epoch": 1.6949283778552071,
      "grad_norm": 29.237218856811523,
      "learning_rate": 9.22785735793866e-06,
      "loss": 2.1054,
      "step": 4378
    },
    {
      "epoch": 1.6953155245838172,
      "grad_norm": 17.246931076049805,
      "learning_rate": 9.22742719490687e-06,
      "loss": 1.5911,
      "step": 4379
    },
    {
      "epoch": 1.6957026713124272,
      "grad_norm": 13.901362419128418,
      "learning_rate": 9.226997031875081e-06,
      "loss": 1.3007,
      "step": 4380
    },
    {
      "epoch": 1.6960898180410375,
      "grad_norm": 27.20305633544922,
      "learning_rate": 9.226566868843291e-06,
      "loss": 1.9295,
      "step": 4381
    },
    {
      "epoch": 1.6964769647696478,
      "grad_norm": 13.889681816101074,
      "learning_rate": 9.226136705811504e-06,
      "loss": 1.4145,
      "step": 4382
    },
    {
      "epoch": 1.6968641114982579,
      "grad_norm": 20.579198837280273,
      "learning_rate": 9.225706542779714e-06,
      "loss": 1.6057,
      "step": 4383
    },
    {
      "epoch": 1.697251258226868,
      "grad_norm": 23.714479446411133,
      "learning_rate": 9.225276379747925e-06,
      "loss": 1.3813,
      "step": 4384
    },
    {
      "epoch": 1.697638404955478,
      "grad_norm": 14.421454429626465,
      "learning_rate": 9.224846216716135e-06,
      "loss": 1.4211,
      "step": 4385
    },
    {
      "epoch": 1.6980255516840883,
      "grad_norm": 21.036401748657227,
      "learning_rate": 9.224416053684348e-06,
      "loss": 1.7569,
      "step": 4386
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 25.38158416748047,
      "learning_rate": 9.223985890652558e-06,
      "loss": 2.5442,
      "step": 4387
    },
    {
      "epoch": 1.6987998451413087,
      "grad_norm": 12.267827987670898,
      "learning_rate": 9.22355572762077e-06,
      "loss": 0.8069,
      "step": 4388
    },
    {
      "epoch": 1.6991869918699187,
      "grad_norm": 29.39716911315918,
      "learning_rate": 9.223125564588979e-06,
      "loss": 2.5682,
      "step": 4389
    },
    {
      "epoch": 1.6995741385985288,
      "grad_norm": 18.356081008911133,
      "learning_rate": 9.22269540155719e-06,
      "loss": 1.2736,
      "step": 4390
    },
    {
      "epoch": 1.6999612853271389,
      "grad_norm": 14.93539047241211,
      "learning_rate": 9.222265238525402e-06,
      "loss": 1.7794,
      "step": 4391
    },
    {
      "epoch": 1.7003484320557491,
      "grad_norm": 24.712745666503906,
      "learning_rate": 9.221835075493613e-06,
      "loss": 2.1763,
      "step": 4392
    },
    {
      "epoch": 1.7007355787843594,
      "grad_norm": 17.965181350708008,
      "learning_rate": 9.221404912461823e-06,
      "loss": 1.3611,
      "step": 4393
    },
    {
      "epoch": 1.7011227255129695,
      "grad_norm": 27.850086212158203,
      "learning_rate": 9.220974749430034e-06,
      "loss": 1.4486,
      "step": 4394
    },
    {
      "epoch": 1.7015098722415796,
      "grad_norm": 16.120466232299805,
      "learning_rate": 9.220544586398246e-06,
      "loss": 1.6277,
      "step": 4395
    },
    {
      "epoch": 1.7018970189701896,
      "grad_norm": 10.582768440246582,
      "learning_rate": 9.220114423366456e-06,
      "loss": 1.2914,
      "step": 4396
    },
    {
      "epoch": 1.7022841656987997,
      "grad_norm": 31.721302032470703,
      "learning_rate": 9.219684260334669e-06,
      "loss": 1.7744,
      "step": 4397
    },
    {
      "epoch": 1.70267131242741,
      "grad_norm": 23.28312110900879,
      "learning_rate": 9.219254097302878e-06,
      "loss": 2.0182,
      "step": 4398
    },
    {
      "epoch": 1.7030584591560203,
      "grad_norm": 27.66520118713379,
      "learning_rate": 9.21882393427109e-06,
      "loss": 1.8441,
      "step": 4399
    },
    {
      "epoch": 1.7034456058846303,
      "grad_norm": 29.0831298828125,
      "learning_rate": 9.2183937712393e-06,
      "loss": 1.4614,
      "step": 4400
    },
    {
      "epoch": 1.7038327526132404,
      "grad_norm": 22.761245727539062,
      "learning_rate": 9.217963608207513e-06,
      "loss": 0.9167,
      "step": 4401
    },
    {
      "epoch": 1.7042198993418505,
      "grad_norm": 23.648914337158203,
      "learning_rate": 9.217533445175722e-06,
      "loss": 1.5211,
      "step": 4402
    },
    {
      "epoch": 1.7046070460704605,
      "grad_norm": 40.65217971801758,
      "learning_rate": 9.217103282143934e-06,
      "loss": 0.5159,
      "step": 4403
    },
    {
      "epoch": 1.7049941927990708,
      "grad_norm": 18.970834732055664,
      "learning_rate": 9.216673119112144e-06,
      "loss": 1.3421,
      "step": 4404
    },
    {
      "epoch": 1.7053813395276811,
      "grad_norm": 16.80160903930664,
      "learning_rate": 9.216242956080355e-06,
      "loss": 0.9544,
      "step": 4405
    },
    {
      "epoch": 1.7057684862562912,
      "grad_norm": 17.951404571533203,
      "learning_rate": 9.215812793048566e-06,
      "loss": 1.4003,
      "step": 4406
    },
    {
      "epoch": 1.7061556329849012,
      "grad_norm": 27.85550308227539,
      "learning_rate": 9.215382630016778e-06,
      "loss": 1.585,
      "step": 4407
    },
    {
      "epoch": 1.7065427797135113,
      "grad_norm": 13.530662536621094,
      "learning_rate": 9.214952466984988e-06,
      "loss": 1.7994,
      "step": 4408
    },
    {
      "epoch": 1.7069299264421216,
      "grad_norm": 16.028663635253906,
      "learning_rate": 9.214522303953199e-06,
      "loss": 0.9455,
      "step": 4409
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 18.46578025817871,
      "learning_rate": 9.21409214092141e-06,
      "loss": 1.6046,
      "step": 4410
    },
    {
      "epoch": 1.707704219899342,
      "grad_norm": 21.805103302001953,
      "learning_rate": 9.21366197788962e-06,
      "loss": 1.0571,
      "step": 4411
    },
    {
      "epoch": 1.708091366627952,
      "grad_norm": 14.879240989685059,
      "learning_rate": 9.213231814857831e-06,
      "loss": 1.5315,
      "step": 4412
    },
    {
      "epoch": 1.708478513356562,
      "grad_norm": 15.903364181518555,
      "learning_rate": 9.212801651826043e-06,
      "loss": 1.5105,
      "step": 4413
    },
    {
      "epoch": 1.7088656600851722,
      "grad_norm": 20.67316436767578,
      "learning_rate": 9.212371488794254e-06,
      "loss": 1.7013,
      "step": 4414
    },
    {
      "epoch": 1.7092528068137824,
      "grad_norm": 30.050981521606445,
      "learning_rate": 9.211941325762464e-06,
      "loss": 1.9887,
      "step": 4415
    },
    {
      "epoch": 1.7096399535423927,
      "grad_norm": 18.067928314208984,
      "learning_rate": 9.211511162730675e-06,
      "loss": 1.7537,
      "step": 4416
    },
    {
      "epoch": 1.7100271002710028,
      "grad_norm": 24.930084228515625,
      "learning_rate": 9.211080999698887e-06,
      "loss": 1.8374,
      "step": 4417
    },
    {
      "epoch": 1.7104142469996129,
      "grad_norm": 22.5344181060791,
      "learning_rate": 9.210650836667098e-06,
      "loss": 1.6306,
      "step": 4418
    },
    {
      "epoch": 1.710801393728223,
      "grad_norm": 21.78977394104004,
      "learning_rate": 9.210220673635308e-06,
      "loss": 1.3708,
      "step": 4419
    },
    {
      "epoch": 1.711188540456833,
      "grad_norm": 24.17893409729004,
      "learning_rate": 9.20979051060352e-06,
      "loss": 1.0595,
      "step": 4420
    },
    {
      "epoch": 1.7115756871854433,
      "grad_norm": 31.7674560546875,
      "learning_rate": 9.209360347571731e-06,
      "loss": 1.3054,
      "step": 4421
    },
    {
      "epoch": 1.7119628339140536,
      "grad_norm": 24.29538345336914,
      "learning_rate": 9.208930184539942e-06,
      "loss": 1.8919,
      "step": 4422
    },
    {
      "epoch": 1.7123499806426636,
      "grad_norm": 10.830338478088379,
      "learning_rate": 9.208500021508152e-06,
      "loss": 1.1054,
      "step": 4423
    },
    {
      "epoch": 1.7127371273712737,
      "grad_norm": 27.32147789001465,
      "learning_rate": 9.208069858476363e-06,
      "loss": 1.2957,
      "step": 4424
    },
    {
      "epoch": 1.7131242740998838,
      "grad_norm": 7.731200218200684,
      "learning_rate": 9.207639695444575e-06,
      "loss": 0.5072,
      "step": 4425
    },
    {
      "epoch": 1.7135114208284938,
      "grad_norm": 22.019245147705078,
      "learning_rate": 9.207209532412785e-06,
      "loss": 1.4947,
      "step": 4426
    },
    {
      "epoch": 1.7138985675571041,
      "grad_norm": 12.086294174194336,
      "learning_rate": 9.206779369380996e-06,
      "loss": 0.9641,
      "step": 4427
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 20.408462524414062,
      "learning_rate": 9.206349206349207e-06,
      "loss": 1.8556,
      "step": 4428
    },
    {
      "epoch": 1.7146728610143245,
      "grad_norm": 17.958539962768555,
      "learning_rate": 9.205919043317419e-06,
      "loss": 0.9744,
      "step": 4429
    },
    {
      "epoch": 1.7150600077429345,
      "grad_norm": 23.242630004882812,
      "learning_rate": 9.205488880285629e-06,
      "loss": 1.4977,
      "step": 4430
    },
    {
      "epoch": 1.7154471544715446,
      "grad_norm": 16.577308654785156,
      "learning_rate": 9.20505871725384e-06,
      "loss": 1.6963,
      "step": 4431
    },
    {
      "epoch": 1.715834301200155,
      "grad_norm": 10.314142227172852,
      "learning_rate": 9.20462855422205e-06,
      "loss": 1.081,
      "step": 4432
    },
    {
      "epoch": 1.716221447928765,
      "grad_norm": 21.2368106842041,
      "learning_rate": 9.204198391190263e-06,
      "loss": 1.5796,
      "step": 4433
    },
    {
      "epoch": 1.7166085946573753,
      "grad_norm": 22.866003036499023,
      "learning_rate": 9.203768228158472e-06,
      "loss": 1.2523,
      "step": 4434
    },
    {
      "epoch": 1.7169957413859853,
      "grad_norm": 17.150279998779297,
      "learning_rate": 9.203338065126684e-06,
      "loss": 2.4068,
      "step": 4435
    },
    {
      "epoch": 1.7173828881145954,
      "grad_norm": 7.560318946838379,
      "learning_rate": 9.202907902094895e-06,
      "loss": 0.4673,
      "step": 4436
    },
    {
      "epoch": 1.7177700348432055,
      "grad_norm": 15.207324028015137,
      "learning_rate": 9.202477739063107e-06,
      "loss": 1.5275,
      "step": 4437
    },
    {
      "epoch": 1.7181571815718157,
      "grad_norm": 29.267520904541016,
      "learning_rate": 9.202047576031316e-06,
      "loss": 1.9138,
      "step": 4438
    },
    {
      "epoch": 1.718544328300426,
      "grad_norm": 24.074359893798828,
      "learning_rate": 9.201617412999528e-06,
      "loss": 0.8051,
      "step": 4439
    },
    {
      "epoch": 1.718931475029036,
      "grad_norm": 24.182941436767578,
      "learning_rate": 9.20118724996774e-06,
      "loss": 1.1398,
      "step": 4440
    },
    {
      "epoch": 1.7193186217576462,
      "grad_norm": 21.37498664855957,
      "learning_rate": 9.200757086935949e-06,
      "loss": 0.9702,
      "step": 4441
    },
    {
      "epoch": 1.7197057684862562,
      "grad_norm": 35.72555923461914,
      "learning_rate": 9.20032692390416e-06,
      "loss": 1.9244,
      "step": 4442
    },
    {
      "epoch": 1.7200929152148663,
      "grad_norm": 18.983535766601562,
      "learning_rate": 9.199896760872372e-06,
      "loss": 1.7488,
      "step": 4443
    },
    {
      "epoch": 1.7204800619434766,
      "grad_norm": 28.08824348449707,
      "learning_rate": 9.199466597840583e-06,
      "loss": 1.0929,
      "step": 4444
    },
    {
      "epoch": 1.7208672086720869,
      "grad_norm": 33.339229583740234,
      "learning_rate": 9.199036434808793e-06,
      "loss": 1.3901,
      "step": 4445
    },
    {
      "epoch": 1.721254355400697,
      "grad_norm": 26.58721351623535,
      "learning_rate": 9.198606271777004e-06,
      "loss": 2.0234,
      "step": 4446
    },
    {
      "epoch": 1.721641502129307,
      "grad_norm": 18.25115394592285,
      "learning_rate": 9.198176108745214e-06,
      "loss": 0.8991,
      "step": 4447
    },
    {
      "epoch": 1.722028648857917,
      "grad_norm": 16.675817489624023,
      "learning_rate": 9.197745945713427e-06,
      "loss": 1.2924,
      "step": 4448
    },
    {
      "epoch": 1.7224157955865271,
      "grad_norm": 7.746568202972412,
      "learning_rate": 9.197315782681637e-06,
      "loss": 0.4053,
      "step": 4449
    },
    {
      "epoch": 1.7228029423151374,
      "grad_norm": 18.114225387573242,
      "learning_rate": 9.196885619649848e-06,
      "loss": 1.0872,
      "step": 4450
    },
    {
      "epoch": 1.7231900890437477,
      "grad_norm": 24.844017028808594,
      "learning_rate": 9.196455456618058e-06,
      "loss": 0.5864,
      "step": 4451
    },
    {
      "epoch": 1.7235772357723578,
      "grad_norm": 10.88758373260498,
      "learning_rate": 9.196025293586271e-06,
      "loss": 0.6769,
      "step": 4452
    },
    {
      "epoch": 1.7239643825009678,
      "grad_norm": 15.334258079528809,
      "learning_rate": 9.195595130554481e-06,
      "loss": 0.6711,
      "step": 4453
    },
    {
      "epoch": 1.724351529229578,
      "grad_norm": 13.855875015258789,
      "learning_rate": 9.195164967522692e-06,
      "loss": 0.7326,
      "step": 4454
    },
    {
      "epoch": 1.7247386759581882,
      "grad_norm": 17.104171752929688,
      "learning_rate": 9.194734804490902e-06,
      "loss": 1.6569,
      "step": 4455
    },
    {
      "epoch": 1.7251258226867983,
      "grad_norm": 12.459426879882812,
      "learning_rate": 9.194304641459113e-06,
      "loss": 0.8141,
      "step": 4456
    },
    {
      "epoch": 1.7255129694154085,
      "grad_norm": 5.991300582885742,
      "learning_rate": 9.193874478427325e-06,
      "loss": 0.3507,
      "step": 4457
    },
    {
      "epoch": 1.7259001161440186,
      "grad_norm": 20.055688858032227,
      "learning_rate": 9.193444315395536e-06,
      "loss": 1.7215,
      "step": 4458
    },
    {
      "epoch": 1.7262872628726287,
      "grad_norm": 16.294553756713867,
      "learning_rate": 9.193014152363746e-06,
      "loss": 0.9782,
      "step": 4459
    },
    {
      "epoch": 1.7266744096012387,
      "grad_norm": 35.79800796508789,
      "learning_rate": 9.192583989331957e-06,
      "loss": 1.8681,
      "step": 4460
    },
    {
      "epoch": 1.727061556329849,
      "grad_norm": 24.18794059753418,
      "learning_rate": 9.192153826300169e-06,
      "loss": 1.8541,
      "step": 4461
    },
    {
      "epoch": 1.727448703058459,
      "grad_norm": 16.499357223510742,
      "learning_rate": 9.191723663268379e-06,
      "loss": 0.9207,
      "step": 4462
    },
    {
      "epoch": 1.7278358497870694,
      "grad_norm": 19.669864654541016,
      "learning_rate": 9.19129350023659e-06,
      "loss": 1.9462,
      "step": 4463
    },
    {
      "epoch": 1.7282229965156795,
      "grad_norm": 24.523653030395508,
      "learning_rate": 9.190863337204801e-06,
      "loss": 1.5549,
      "step": 4464
    },
    {
      "epoch": 1.7286101432442895,
      "grad_norm": 17.59524917602539,
      "learning_rate": 9.190433174173013e-06,
      "loss": 1.5567,
      "step": 4465
    },
    {
      "epoch": 1.7289972899728996,
      "grad_norm": 14.219677925109863,
      "learning_rate": 9.190003011141223e-06,
      "loss": 1.4983,
      "step": 4466
    },
    {
      "epoch": 1.7293844367015099,
      "grad_norm": 13.626916885375977,
      "learning_rate": 9.189572848109434e-06,
      "loss": 0.7972,
      "step": 4467
    },
    {
      "epoch": 1.7297715834301202,
      "grad_norm": 21.099573135375977,
      "learning_rate": 9.189142685077645e-06,
      "loss": 2.0181,
      "step": 4468
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 13.053382873535156,
      "learning_rate": 9.188712522045857e-06,
      "loss": 0.897,
      "step": 4469
    },
    {
      "epoch": 1.7305458768873403,
      "grad_norm": 19.516963958740234,
      "learning_rate": 9.188282359014067e-06,
      "loss": 1.812,
      "step": 4470
    },
    {
      "epoch": 1.7309330236159504,
      "grad_norm": 22.92887306213379,
      "learning_rate": 9.187852195982278e-06,
      "loss": 1.5623,
      "step": 4471
    },
    {
      "epoch": 1.7313201703445604,
      "grad_norm": 36.629295349121094,
      "learning_rate": 9.18742203295049e-06,
      "loss": 2.3212,
      "step": 4472
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 12.380600929260254,
      "learning_rate": 9.1869918699187e-06,
      "loss": 0.7925,
      "step": 4473
    },
    {
      "epoch": 1.732094463801781,
      "grad_norm": 19.85953712463379,
      "learning_rate": 9.18656170688691e-06,
      "loss": 1.2847,
      "step": 4474
    },
    {
      "epoch": 1.732481610530391,
      "grad_norm": 19.276193618774414,
      "learning_rate": 9.186131543855122e-06,
      "loss": 1.2809,
      "step": 4475
    },
    {
      "epoch": 1.7328687572590011,
      "grad_norm": 14.649266242980957,
      "learning_rate": 9.185701380823333e-06,
      "loss": 1.0727,
      "step": 4476
    },
    {
      "epoch": 1.7332559039876112,
      "grad_norm": 18.073593139648438,
      "learning_rate": 9.185271217791543e-06,
      "loss": 1.9435,
      "step": 4477
    },
    {
      "epoch": 1.7336430507162215,
      "grad_norm": 28.675411224365234,
      "learning_rate": 9.184841054759754e-06,
      "loss": 2.5937,
      "step": 4478
    },
    {
      "epoch": 1.7340301974448316,
      "grad_norm": 14.290305137634277,
      "learning_rate": 9.184410891727966e-06,
      "loss": 1.0701,
      "step": 4479
    },
    {
      "epoch": 1.7344173441734418,
      "grad_norm": 10.853109359741211,
      "learning_rate": 9.183980728696177e-06,
      "loss": 1.1927,
      "step": 4480
    },
    {
      "epoch": 1.734804490902052,
      "grad_norm": 38.29320526123047,
      "learning_rate": 9.183550565664387e-06,
      "loss": 2.1443,
      "step": 4481
    },
    {
      "epoch": 1.735191637630662,
      "grad_norm": 14.139076232910156,
      "learning_rate": 9.183120402632598e-06,
      "loss": 1.3476,
      "step": 4482
    },
    {
      "epoch": 1.735578784359272,
      "grad_norm": 15.987998962402344,
      "learning_rate": 9.18269023960081e-06,
      "loss": 1.5097,
      "step": 4483
    },
    {
      "epoch": 1.7359659310878823,
      "grad_norm": 26.095218658447266,
      "learning_rate": 9.182260076569021e-06,
      "loss": 1.5168,
      "step": 4484
    },
    {
      "epoch": 1.7363530778164924,
      "grad_norm": 16.624298095703125,
      "learning_rate": 9.181829913537231e-06,
      "loss": 1.5727,
      "step": 4485
    },
    {
      "epoch": 1.7367402245451027,
      "grad_norm": 10.554410934448242,
      "learning_rate": 9.181399750505442e-06,
      "loss": 0.7677,
      "step": 4486
    },
    {
      "epoch": 1.7371273712737128,
      "grad_norm": 23.77164077758789,
      "learning_rate": 9.180969587473654e-06,
      "loss": 1.1236,
      "step": 4487
    },
    {
      "epoch": 1.7375145180023228,
      "grad_norm": 26.081701278686523,
      "learning_rate": 9.180539424441865e-06,
      "loss": 2.4391,
      "step": 4488
    },
    {
      "epoch": 1.7379016647309329,
      "grad_norm": 22.718042373657227,
      "learning_rate": 9.180109261410075e-06,
      "loss": 1.2705,
      "step": 4489
    },
    {
      "epoch": 1.7382888114595432,
      "grad_norm": 17.286602020263672,
      "learning_rate": 9.179679098378286e-06,
      "loss": 1.6031,
      "step": 4490
    },
    {
      "epoch": 1.7386759581881535,
      "grad_norm": 24.36741828918457,
      "learning_rate": 9.179248935346498e-06,
      "loss": 1.9885,
      "step": 4491
    },
    {
      "epoch": 1.7390631049167635,
      "grad_norm": 16.65433692932129,
      "learning_rate": 9.178818772314707e-06,
      "loss": 1.3173,
      "step": 4492
    },
    {
      "epoch": 1.7394502516453736,
      "grad_norm": 27.231891632080078,
      "learning_rate": 9.178388609282919e-06,
      "loss": 1.1933,
      "step": 4493
    },
    {
      "epoch": 1.7398373983739837,
      "grad_norm": 14.514915466308594,
      "learning_rate": 9.17795844625113e-06,
      "loss": 1.5053,
      "step": 4494
    },
    {
      "epoch": 1.7402245451025937,
      "grad_norm": 16.62972068786621,
      "learning_rate": 9.177528283219342e-06,
      "loss": 1.6251,
      "step": 4495
    },
    {
      "epoch": 1.740611691831204,
      "grad_norm": 23.241897583007812,
      "learning_rate": 9.177098120187551e-06,
      "loss": 1.8409,
      "step": 4496
    },
    {
      "epoch": 1.7409988385598143,
      "grad_norm": 16.291322708129883,
      "learning_rate": 9.176667957155763e-06,
      "loss": 1.5371,
      "step": 4497
    },
    {
      "epoch": 1.7413859852884244,
      "grad_norm": 18.720542907714844,
      "learning_rate": 9.176237794123973e-06,
      "loss": 1.7311,
      "step": 4498
    },
    {
      "epoch": 1.7417731320170344,
      "grad_norm": 26.483949661254883,
      "learning_rate": 9.175807631092186e-06,
      "loss": 1.8123,
      "step": 4499
    },
    {
      "epoch": 1.7421602787456445,
      "grad_norm": 18.75701904296875,
      "learning_rate": 9.175377468060395e-06,
      "loss": 1.5508,
      "step": 4500
    },
    {
      "epoch": 1.7425474254742548,
      "grad_norm": 18.85723304748535,
      "learning_rate": 9.174947305028607e-06,
      "loss": 1.4105,
      "step": 4501
    },
    {
      "epoch": 1.7429345722028649,
      "grad_norm": 24.17290687561035,
      "learning_rate": 9.174517141996817e-06,
      "loss": 1.1334,
      "step": 4502
    },
    {
      "epoch": 1.7433217189314751,
      "grad_norm": 15.149174690246582,
      "learning_rate": 9.17408697896503e-06,
      "loss": 1.5256,
      "step": 4503
    },
    {
      "epoch": 1.7437088656600852,
      "grad_norm": 16.89396095275879,
      "learning_rate": 9.17365681593324e-06,
      "loss": 1.597,
      "step": 4504
    },
    {
      "epoch": 1.7440960123886953,
      "grad_norm": 23.736783981323242,
      "learning_rate": 9.17322665290145e-06,
      "loss": 1.627,
      "step": 4505
    },
    {
      "epoch": 1.7444831591173053,
      "grad_norm": 20.196788787841797,
      "learning_rate": 9.17279648986966e-06,
      "loss": 1.1293,
      "step": 4506
    },
    {
      "epoch": 1.7448703058459156,
      "grad_norm": 21.0212345123291,
      "learning_rate": 9.172366326837872e-06,
      "loss": 1.736,
      "step": 4507
    },
    {
      "epoch": 1.7452574525745257,
      "grad_norm": 20.108478546142578,
      "learning_rate": 9.171936163806083e-06,
      "loss": 1.2976,
      "step": 4508
    },
    {
      "epoch": 1.745644599303136,
      "grad_norm": 18.758146286010742,
      "learning_rate": 9.171506000774295e-06,
      "loss": 1.8374,
      "step": 4509
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 19.61980628967285,
      "learning_rate": 9.171075837742504e-06,
      "loss": 1.7882,
      "step": 4510
    },
    {
      "epoch": 1.7464188927603561,
      "grad_norm": 17.120094299316406,
      "learning_rate": 9.170645674710716e-06,
      "loss": 1.5471,
      "step": 4511
    },
    {
      "epoch": 1.7468060394889662,
      "grad_norm": 9.545855522155762,
      "learning_rate": 9.170215511678927e-06,
      "loss": 1.2965,
      "step": 4512
    },
    {
      "epoch": 1.7471931862175765,
      "grad_norm": 19.835731506347656,
      "learning_rate": 9.169785348647137e-06,
      "loss": 1.5122,
      "step": 4513
    },
    {
      "epoch": 1.7475803329461868,
      "grad_norm": 23.32377052307129,
      "learning_rate": 9.169355185615348e-06,
      "loss": 1.5253,
      "step": 4514
    },
    {
      "epoch": 1.7479674796747968,
      "grad_norm": 23.381855010986328,
      "learning_rate": 9.16892502258356e-06,
      "loss": 1.4891,
      "step": 4515
    },
    {
      "epoch": 1.7483546264034069,
      "grad_norm": 23.537670135498047,
      "learning_rate": 9.168494859551771e-06,
      "loss": 1.1614,
      "step": 4516
    },
    {
      "epoch": 1.748741773132017,
      "grad_norm": 19.368249893188477,
      "learning_rate": 9.168064696519981e-06,
      "loss": 1.1947,
      "step": 4517
    },
    {
      "epoch": 1.749128919860627,
      "grad_norm": 27.85027313232422,
      "learning_rate": 9.167634533488194e-06,
      "loss": 2.1036,
      "step": 4518
    },
    {
      "epoch": 1.7495160665892373,
      "grad_norm": 21.802690505981445,
      "learning_rate": 9.167204370456404e-06,
      "loss": 1.5972,
      "step": 4519
    },
    {
      "epoch": 1.7499032133178476,
      "grad_norm": 14.391538619995117,
      "learning_rate": 9.166774207424615e-06,
      "loss": 1.465,
      "step": 4520
    },
    {
      "epoch": 1.7502903600464577,
      "grad_norm": 29.930076599121094,
      "learning_rate": 9.166344044392825e-06,
      "loss": 1.6491,
      "step": 4521
    },
    {
      "epoch": 1.7506775067750677,
      "grad_norm": 24.224016189575195,
      "learning_rate": 9.165913881361036e-06,
      "loss": 1.2869,
      "step": 4522
    },
    {
      "epoch": 1.7510646535036778,
      "grad_norm": 16.734277725219727,
      "learning_rate": 9.165483718329248e-06,
      "loss": 1.0763,
      "step": 4523
    },
    {
      "epoch": 1.751451800232288,
      "grad_norm": 15.640958786010742,
      "learning_rate": 9.16505355529746e-06,
      "loss": 1.2961,
      "step": 4524
    },
    {
      "epoch": 1.7518389469608981,
      "grad_norm": 16.36305046081543,
      "learning_rate": 9.164623392265669e-06,
      "loss": 1.4876,
      "step": 4525
    },
    {
      "epoch": 1.7522260936895084,
      "grad_norm": 15.450028419494629,
      "learning_rate": 9.16419322923388e-06,
      "loss": 1.5788,
      "step": 4526
    },
    {
      "epoch": 1.7526132404181185,
      "grad_norm": 24.73768424987793,
      "learning_rate": 9.163763066202092e-06,
      "loss": 1.526,
      "step": 4527
    },
    {
      "epoch": 1.7530003871467286,
      "grad_norm": 19.16252326965332,
      "learning_rate": 9.163332903170302e-06,
      "loss": 2.082,
      "step": 4528
    },
    {
      "epoch": 1.7533875338753386,
      "grad_norm": 27.403255462646484,
      "learning_rate": 9.162902740138513e-06,
      "loss": 1.1262,
      "step": 4529
    },
    {
      "epoch": 1.753774680603949,
      "grad_norm": 14.941604614257812,
      "learning_rate": 9.162472577106724e-06,
      "loss": 0.3092,
      "step": 4530
    },
    {
      "epoch": 1.754161827332559,
      "grad_norm": 10.688907623291016,
      "learning_rate": 9.162042414074936e-06,
      "loss": 0.4922,
      "step": 4531
    },
    {
      "epoch": 1.7545489740611693,
      "grad_norm": 14.584424018859863,
      "learning_rate": 9.161612251043145e-06,
      "loss": 1.4353,
      "step": 4532
    },
    {
      "epoch": 1.7549361207897793,
      "grad_norm": 14.548165321350098,
      "learning_rate": 9.161182088011357e-06,
      "loss": 1.404,
      "step": 4533
    },
    {
      "epoch": 1.7553232675183894,
      "grad_norm": 14.812557220458984,
      "learning_rate": 9.160751924979568e-06,
      "loss": 1.4873,
      "step": 4534
    },
    {
      "epoch": 1.7557104142469995,
      "grad_norm": 26.2131404876709,
      "learning_rate": 9.16032176194778e-06,
      "loss": 3.0054,
      "step": 4535
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 12.603919982910156,
      "learning_rate": 9.15989159891599e-06,
      "loss": 0.851,
      "step": 4536
    },
    {
      "epoch": 1.75648470770422,
      "grad_norm": 21.5288028717041,
      "learning_rate": 9.159461435884201e-06,
      "loss": 1.4929,
      "step": 4537
    },
    {
      "epoch": 1.7568718544328301,
      "grad_norm": 12.659451484680176,
      "learning_rate": 9.159031272852412e-06,
      "loss": 0.7728,
      "step": 4538
    },
    {
      "epoch": 1.7572590011614402,
      "grad_norm": 25.973934173583984,
      "learning_rate": 9.158601109820624e-06,
      "loss": 2.0874,
      "step": 4539
    },
    {
      "epoch": 1.7576461478900502,
      "grad_norm": 20.82561683654785,
      "learning_rate": 9.158170946788833e-06,
      "loss": 2.8401,
      "step": 4540
    },
    {
      "epoch": 1.7580332946186603,
      "grad_norm": 19.50092315673828,
      "learning_rate": 9.157740783757045e-06,
      "loss": 1.767,
      "step": 4541
    },
    {
      "epoch": 1.7584204413472706,
      "grad_norm": 33.2757682800293,
      "learning_rate": 9.157310620725256e-06,
      "loss": 1.3564,
      "step": 4542
    },
    {
      "epoch": 1.758807588075881,
      "grad_norm": 16.137157440185547,
      "learning_rate": 9.156880457693466e-06,
      "loss": 1.4755,
      "step": 4543
    },
    {
      "epoch": 1.759194734804491,
      "grad_norm": 7.719583034515381,
      "learning_rate": 9.156450294661677e-06,
      "loss": 0.2483,
      "step": 4544
    },
    {
      "epoch": 1.759581881533101,
      "grad_norm": 29.75459098815918,
      "learning_rate": 9.156020131629889e-06,
      "loss": 1.4113,
      "step": 4545
    },
    {
      "epoch": 1.759969028261711,
      "grad_norm": 22.617103576660156,
      "learning_rate": 9.1555899685981e-06,
      "loss": 1.4787,
      "step": 4546
    },
    {
      "epoch": 1.7603561749903214,
      "grad_norm": 14.117708206176758,
      "learning_rate": 9.15515980556631e-06,
      "loss": 0.8108,
      "step": 4547
    },
    {
      "epoch": 1.7607433217189314,
      "grad_norm": 24.438486099243164,
      "learning_rate": 9.154729642534521e-06,
      "loss": 0.8271,
      "step": 4548
    },
    {
      "epoch": 1.7611304684475417,
      "grad_norm": 28.06772232055664,
      "learning_rate": 9.154299479502731e-06,
      "loss": 1.6372,
      "step": 4549
    },
    {
      "epoch": 1.7615176151761518,
      "grad_norm": 14.844985961914062,
      "learning_rate": 9.153869316470944e-06,
      "loss": 0.9577,
      "step": 4550
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 22.551952362060547,
      "learning_rate": 9.153439153439154e-06,
      "loss": 2.1998,
      "step": 4551
    },
    {
      "epoch": 1.762291908633372,
      "grad_norm": 15.018440246582031,
      "learning_rate": 9.153008990407365e-06,
      "loss": 0.5866,
      "step": 4552
    },
    {
      "epoch": 1.7626790553619822,
      "grad_norm": 29.674833297729492,
      "learning_rate": 9.152578827375575e-06,
      "loss": 1.359,
      "step": 4553
    },
    {
      "epoch": 1.7630662020905923,
      "grad_norm": 22.86784553527832,
      "learning_rate": 9.152148664343788e-06,
      "loss": 1.8715,
      "step": 4554
    },
    {
      "epoch": 1.7634533488192026,
      "grad_norm": 14.819486618041992,
      "learning_rate": 9.151718501311998e-06,
      "loss": 0.8739,
      "step": 4555
    },
    {
      "epoch": 1.7638404955478126,
      "grad_norm": 17.8555850982666,
      "learning_rate": 9.15128833828021e-06,
      "loss": 1.6129,
      "step": 4556
    },
    {
      "epoch": 1.7642276422764227,
      "grad_norm": 25.12284278869629,
      "learning_rate": 9.150858175248419e-06,
      "loss": 0.8943,
      "step": 4557
    },
    {
      "epoch": 1.7646147890050328,
      "grad_norm": 24.32223892211914,
      "learning_rate": 9.15042801221663e-06,
      "loss": 2.0972,
      "step": 4558
    },
    {
      "epoch": 1.765001935733643,
      "grad_norm": 15.381014823913574,
      "learning_rate": 9.149997849184842e-06,
      "loss": 1.2772,
      "step": 4559
    },
    {
      "epoch": 1.7653890824622533,
      "grad_norm": 14.32927417755127,
      "learning_rate": 9.149567686153053e-06,
      "loss": 1.4038,
      "step": 4560
    },
    {
      "epoch": 1.7657762291908634,
      "grad_norm": 20.363313674926758,
      "learning_rate": 9.149137523121265e-06,
      "loss": 1.8724,
      "step": 4561
    },
    {
      "epoch": 1.7661633759194735,
      "grad_norm": 67.725830078125,
      "learning_rate": 9.148707360089474e-06,
      "loss": 2.3878,
      "step": 4562
    },
    {
      "epoch": 1.7665505226480835,
      "grad_norm": 12.874127388000488,
      "learning_rate": 9.148277197057686e-06,
      "loss": 1.011,
      "step": 4563
    },
    {
      "epoch": 1.7669376693766936,
      "grad_norm": 17.408344268798828,
      "learning_rate": 9.147847034025896e-06,
      "loss": 1.3006,
      "step": 4564
    },
    {
      "epoch": 1.767324816105304,
      "grad_norm": 19.56664276123047,
      "learning_rate": 9.147416870994109e-06,
      "loss": 1.95,
      "step": 4565
    },
    {
      "epoch": 1.7677119628339142,
      "grad_norm": 15.216754913330078,
      "learning_rate": 9.146986707962318e-06,
      "loss": 1.232,
      "step": 4566
    },
    {
      "epoch": 1.7680991095625243,
      "grad_norm": 22.809499740600586,
      "learning_rate": 9.14655654493053e-06,
      "loss": 1.0398,
      "step": 4567
    },
    {
      "epoch": 1.7684862562911343,
      "grad_norm": 27.263811111450195,
      "learning_rate": 9.14612638189874e-06,
      "loss": 1.2856,
      "step": 4568
    },
    {
      "epoch": 1.7688734030197444,
      "grad_norm": 19.445053100585938,
      "learning_rate": 9.145696218866953e-06,
      "loss": 1.6648,
      "step": 4569
    },
    {
      "epoch": 1.7692605497483547,
      "grad_norm": 20.91402816772461,
      "learning_rate": 9.145266055835162e-06,
      "loss": 1.3524,
      "step": 4570
    },
    {
      "epoch": 1.7696476964769647,
      "grad_norm": 14.350419998168945,
      "learning_rate": 9.144835892803374e-06,
      "loss": 1.6282,
      "step": 4571
    },
    {
      "epoch": 1.770034843205575,
      "grad_norm": 23.761516571044922,
      "learning_rate": 9.144405729771583e-06,
      "loss": 2.592,
      "step": 4572
    },
    {
      "epoch": 1.770421989934185,
      "grad_norm": 14.84110164642334,
      "learning_rate": 9.143975566739795e-06,
      "loss": 1.6213,
      "step": 4573
    },
    {
      "epoch": 1.7708091366627952,
      "grad_norm": 25.282779693603516,
      "learning_rate": 9.143545403708006e-06,
      "loss": 2.494,
      "step": 4574
    },
    {
      "epoch": 1.7711962833914052,
      "grad_norm": 23.86191177368164,
      "learning_rate": 9.143115240676218e-06,
      "loss": 1.4155,
      "step": 4575
    },
    {
      "epoch": 1.7715834301200155,
      "grad_norm": 10.375974655151367,
      "learning_rate": 9.142685077644427e-06,
      "loss": 1.3078,
      "step": 4576
    },
    {
      "epoch": 1.7719705768486256,
      "grad_norm": 11.747454643249512,
      "learning_rate": 9.142254914612639e-06,
      "loss": 1.0467,
      "step": 4577
    },
    {
      "epoch": 1.7723577235772359,
      "grad_norm": 20.231735229492188,
      "learning_rate": 9.14182475158085e-06,
      "loss": 1.8555,
      "step": 4578
    },
    {
      "epoch": 1.772744870305846,
      "grad_norm": 16.557392120361328,
      "learning_rate": 9.14139458854906e-06,
      "loss": 1.0535,
      "step": 4579
    },
    {
      "epoch": 1.773132017034456,
      "grad_norm": 20.999420166015625,
      "learning_rate": 9.140964425517271e-06,
      "loss": 0.9774,
      "step": 4580
    },
    {
      "epoch": 1.773519163763066,
      "grad_norm": 14.986236572265625,
      "learning_rate": 9.140534262485483e-06,
      "loss": 0.9069,
      "step": 4581
    },
    {
      "epoch": 1.7739063104916764,
      "grad_norm": 17.554094314575195,
      "learning_rate": 9.140104099453694e-06,
      "loss": 1.2965,
      "step": 4582
    },
    {
      "epoch": 1.7742934572202866,
      "grad_norm": 50.11005401611328,
      "learning_rate": 9.139673936421904e-06,
      "loss": 1.885,
      "step": 4583
    },
    {
      "epoch": 1.7746806039488967,
      "grad_norm": 16.936494827270508,
      "learning_rate": 9.139243773390115e-06,
      "loss": 1.18,
      "step": 4584
    },
    {
      "epoch": 1.7750677506775068,
      "grad_norm": 15.116061210632324,
      "learning_rate": 9.138813610358327e-06,
      "loss": 1.0944,
      "step": 4585
    },
    {
      "epoch": 1.7754548974061168,
      "grad_norm": 22.059614181518555,
      "learning_rate": 9.138383447326538e-06,
      "loss": 0.9324,
      "step": 4586
    },
    {
      "epoch": 1.775842044134727,
      "grad_norm": 24.247215270996094,
      "learning_rate": 9.137953284294748e-06,
      "loss": 1.4891,
      "step": 4587
    },
    {
      "epoch": 1.7762291908633372,
      "grad_norm": 26.66714859008789,
      "learning_rate": 9.13752312126296e-06,
      "loss": 1.9139,
      "step": 4588
    },
    {
      "epoch": 1.7766163375919475,
      "grad_norm": 26.74095916748047,
      "learning_rate": 9.13709295823117e-06,
      "loss": 1.5049,
      "step": 4589
    },
    {
      "epoch": 1.7770034843205575,
      "grad_norm": 17.864242553710938,
      "learning_rate": 9.136662795199382e-06,
      "loss": 1.2338,
      "step": 4590
    },
    {
      "epoch": 1.7773906310491676,
      "grad_norm": 17.266746520996094,
      "learning_rate": 9.136232632167592e-06,
      "loss": 1.0891,
      "step": 4591
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 24.731613159179688,
      "learning_rate": 9.135802469135803e-06,
      "loss": 1.4694,
      "step": 4592
    },
    {
      "epoch": 1.778164924506388,
      "grad_norm": 14.820184707641602,
      "learning_rate": 9.135372306104015e-06,
      "loss": 1.4947,
      "step": 4593
    },
    {
      "epoch": 1.778552071234998,
      "grad_norm": 17.972148895263672,
      "learning_rate": 9.134942143072224e-06,
      "loss": 2.9015,
      "step": 4594
    },
    {
      "epoch": 1.7789392179636083,
      "grad_norm": 25.1130313873291,
      "learning_rate": 9.134511980040436e-06,
      "loss": 1.8965,
      "step": 4595
    },
    {
      "epoch": 1.7793263646922184,
      "grad_norm": 17.45899200439453,
      "learning_rate": 9.134081817008647e-06,
      "loss": 1.1631,
      "step": 4596
    },
    {
      "epoch": 1.7797135114208285,
      "grad_norm": 13.505768775939941,
      "learning_rate": 9.133651653976859e-06,
      "loss": 2.2468,
      "step": 4597
    },
    {
      "epoch": 1.7801006581494385,
      "grad_norm": 21.386165618896484,
      "learning_rate": 9.133221490945068e-06,
      "loss": 1.9022,
      "step": 4598
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 13.605504989624023,
      "learning_rate": 9.13279132791328e-06,
      "loss": 0.9549,
      "step": 4599
    },
    {
      "epoch": 1.7808749516066589,
      "grad_norm": 10.877789497375488,
      "learning_rate": 9.132361164881491e-06,
      "loss": 0.4905,
      "step": 4600
    },
    {
      "epoch": 1.7812620983352692,
      "grad_norm": 16.274320602416992,
      "learning_rate": 9.131931001849703e-06,
      "loss": 1.4912,
      "step": 4601
    },
    {
      "epoch": 1.7816492450638792,
      "grad_norm": 27.972822189331055,
      "learning_rate": 9.131500838817912e-06,
      "loss": 1.5586,
      "step": 4602
    },
    {
      "epoch": 1.7820363917924893,
      "grad_norm": 18.038673400878906,
      "learning_rate": 9.131070675786124e-06,
      "loss": 1.5495,
      "step": 4603
    },
    {
      "epoch": 1.7824235385210994,
      "grad_norm": 23.25387191772461,
      "learning_rate": 9.130640512754335e-06,
      "loss": 1.5715,
      "step": 4604
    },
    {
      "epoch": 1.7828106852497096,
      "grad_norm": 14.387360572814941,
      "learning_rate": 9.130210349722547e-06,
      "loss": 0.9389,
      "step": 4605
    },
    {
      "epoch": 1.78319783197832,
      "grad_norm": 25.21636199951172,
      "learning_rate": 9.129780186690756e-06,
      "loss": 2.0225,
      "step": 4606
    },
    {
      "epoch": 1.78358497870693,
      "grad_norm": 33.97183609008789,
      "learning_rate": 9.129350023658968e-06,
      "loss": 1.9928,
      "step": 4607
    },
    {
      "epoch": 1.78397212543554,
      "grad_norm": 19.0562801361084,
      "learning_rate": 9.12891986062718e-06,
      "loss": 2.1351,
      "step": 4608
    },
    {
      "epoch": 1.7843592721641501,
      "grad_norm": 13.281758308410645,
      "learning_rate": 9.128489697595389e-06,
      "loss": 0.7078,
      "step": 4609
    },
    {
      "epoch": 1.7847464188927602,
      "grad_norm": 19.826133728027344,
      "learning_rate": 9.1280595345636e-06,
      "loss": 0.8356,
      "step": 4610
    },
    {
      "epoch": 1.7851335656213705,
      "grad_norm": 13.684333801269531,
      "learning_rate": 9.127629371531812e-06,
      "loss": 1.3464,
      "step": 4611
    },
    {
      "epoch": 1.7855207123499808,
      "grad_norm": 17.480772018432617,
      "learning_rate": 9.127199208500023e-06,
      "loss": 1.4344,
      "step": 4612
    },
    {
      "epoch": 1.7859078590785908,
      "grad_norm": 17.252161026000977,
      "learning_rate": 9.126769045468233e-06,
      "loss": 1.6929,
      "step": 4613
    },
    {
      "epoch": 1.786295005807201,
      "grad_norm": 13.966492652893066,
      "learning_rate": 9.126338882436444e-06,
      "loss": 1.1063,
      "step": 4614
    },
    {
      "epoch": 1.786682152535811,
      "grad_norm": 15.08607006072998,
      "learning_rate": 9.125908719404654e-06,
      "loss": 1.3971,
      "step": 4615
    },
    {
      "epoch": 1.7870692992644213,
      "grad_norm": 24.393068313598633,
      "learning_rate": 9.125478556372867e-06,
      "loss": 1.9061,
      "step": 4616
    },
    {
      "epoch": 1.7874564459930313,
      "grad_norm": 22.888050079345703,
      "learning_rate": 9.125048393341077e-06,
      "loss": 1.5517,
      "step": 4617
    },
    {
      "epoch": 1.7878435927216416,
      "grad_norm": 12.790576934814453,
      "learning_rate": 9.124618230309288e-06,
      "loss": 1.2876,
      "step": 4618
    },
    {
      "epoch": 1.7882307394502517,
      "grad_norm": 14.523802757263184,
      "learning_rate": 9.124188067277498e-06,
      "loss": 0.9253,
      "step": 4619
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 15.605945587158203,
      "learning_rate": 9.123757904245711e-06,
      "loss": 1.1902,
      "step": 4620
    },
    {
      "epoch": 1.7890050329074718,
      "grad_norm": 18.38454246520996,
      "learning_rate": 9.12332774121392e-06,
      "loss": 1.7644,
      "step": 4621
    },
    {
      "epoch": 1.789392179636082,
      "grad_norm": 12.19017219543457,
      "learning_rate": 9.122897578182132e-06,
      "loss": 1.492,
      "step": 4622
    },
    {
      "epoch": 1.7897793263646922,
      "grad_norm": 20.162109375,
      "learning_rate": 9.122467415150342e-06,
      "loss": 1.2208,
      "step": 4623
    },
    {
      "epoch": 1.7901664730933025,
      "grad_norm": 26.259050369262695,
      "learning_rate": 9.122037252118553e-06,
      "loss": 1.9364,
      "step": 4624
    },
    {
      "epoch": 1.7905536198219125,
      "grad_norm": 22.133100509643555,
      "learning_rate": 9.121607089086765e-06,
      "loss": 1.0572,
      "step": 4625
    },
    {
      "epoch": 1.7909407665505226,
      "grad_norm": 25.303892135620117,
      "learning_rate": 9.121176926054976e-06,
      "loss": 1.266,
      "step": 4626
    },
    {
      "epoch": 1.7913279132791327,
      "grad_norm": 15.066391944885254,
      "learning_rate": 9.120746763023186e-06,
      "loss": 1.415,
      "step": 4627
    },
    {
      "epoch": 1.791715060007743,
      "grad_norm": 32.96600341796875,
      "learning_rate": 9.120316599991397e-06,
      "loss": 2.675,
      "step": 4628
    },
    {
      "epoch": 1.7921022067363532,
      "grad_norm": 26.473388671875,
      "learning_rate": 9.119886436959609e-06,
      "loss": 1.9169,
      "step": 4629
    },
    {
      "epoch": 1.7924893534649633,
      "grad_norm": 16.76715660095215,
      "learning_rate": 9.119456273927818e-06,
      "loss": 1.0854,
      "step": 4630
    },
    {
      "epoch": 1.7928765001935734,
      "grad_norm": 20.812271118164062,
      "learning_rate": 9.11902611089603e-06,
      "loss": 0.7364,
      "step": 4631
    },
    {
      "epoch": 1.7932636469221834,
      "grad_norm": 19.766273498535156,
      "learning_rate": 9.118595947864241e-06,
      "loss": 1.2462,
      "step": 4632
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 13.518383979797363,
      "learning_rate": 9.118165784832453e-06,
      "loss": 0.575,
      "step": 4633
    },
    {
      "epoch": 1.7940379403794038,
      "grad_norm": 13.823429107666016,
      "learning_rate": 9.117735621800662e-06,
      "loss": 0.9903,
      "step": 4634
    },
    {
      "epoch": 1.794425087108014,
      "grad_norm": 19.44105339050293,
      "learning_rate": 9.117305458768874e-06,
      "loss": 1.4057,
      "step": 4635
    },
    {
      "epoch": 1.7948122338366241,
      "grad_norm": 33.58798599243164,
      "learning_rate": 9.116875295737085e-06,
      "loss": 1.8894,
      "step": 4636
    },
    {
      "epoch": 1.7951993805652342,
      "grad_norm": 18.675382614135742,
      "learning_rate": 9.116445132705297e-06,
      "loss": 1.0055,
      "step": 4637
    },
    {
      "epoch": 1.7955865272938443,
      "grad_norm": 16.190196990966797,
      "learning_rate": 9.116014969673506e-06,
      "loss": 1.2409,
      "step": 4638
    },
    {
      "epoch": 1.7959736740224546,
      "grad_norm": 17.17102813720703,
      "learning_rate": 9.115584806641718e-06,
      "loss": 1.2354,
      "step": 4639
    },
    {
      "epoch": 1.7963608207510646,
      "grad_norm": 23.497678756713867,
      "learning_rate": 9.11515464360993e-06,
      "loss": 1.3753,
      "step": 4640
    },
    {
      "epoch": 1.796747967479675,
      "grad_norm": 19.736345291137695,
      "learning_rate": 9.11472448057814e-06,
      "loss": 1.6109,
      "step": 4641
    },
    {
      "epoch": 1.797135114208285,
      "grad_norm": 10.819583892822266,
      "learning_rate": 9.11429431754635e-06,
      "loss": 0.6906,
      "step": 4642
    },
    {
      "epoch": 1.797522260936895,
      "grad_norm": 26.23871421813965,
      "learning_rate": 9.113864154514562e-06,
      "loss": 1.5394,
      "step": 4643
    },
    {
      "epoch": 1.797909407665505,
      "grad_norm": 24.167470932006836,
      "learning_rate": 9.113433991482773e-06,
      "loss": 2.5027,
      "step": 4644
    },
    {
      "epoch": 1.7982965543941154,
      "grad_norm": 30.81083106994629,
      "learning_rate": 9.113003828450983e-06,
      "loss": 3.0649,
      "step": 4645
    },
    {
      "epoch": 1.7986837011227255,
      "grad_norm": 16.394790649414062,
      "learning_rate": 9.112573665419194e-06,
      "loss": 1.0691,
      "step": 4646
    },
    {
      "epoch": 1.7990708478513358,
      "grad_norm": 23.087684631347656,
      "learning_rate": 9.112143502387406e-06,
      "loss": 1.6729,
      "step": 4647
    },
    {
      "epoch": 1.7994579945799458,
      "grad_norm": 23.86063575744629,
      "learning_rate": 9.111713339355617e-06,
      "loss": 1.4825,
      "step": 4648
    },
    {
      "epoch": 1.7998451413085559,
      "grad_norm": 24.01270294189453,
      "learning_rate": 9.111283176323827e-06,
      "loss": 1.2829,
      "step": 4649
    },
    {
      "epoch": 1.800232288037166,
      "grad_norm": 46.29359817504883,
      "learning_rate": 9.110853013292038e-06,
      "loss": 2.5051,
      "step": 4650
    },
    {
      "epoch": 1.8006194347657762,
      "grad_norm": 7.216480731964111,
      "learning_rate": 9.11042285026025e-06,
      "loss": 0.2282,
      "step": 4651
    },
    {
      "epoch": 1.8010065814943865,
      "grad_norm": 14.147415161132812,
      "learning_rate": 9.109992687228461e-06,
      "loss": 0.883,
      "step": 4652
    },
    {
      "epoch": 1.8013937282229966,
      "grad_norm": 14.966934204101562,
      "learning_rate": 9.109562524196671e-06,
      "loss": 0.8051,
      "step": 4653
    },
    {
      "epoch": 1.8017808749516067,
      "grad_norm": 21.678268432617188,
      "learning_rate": 9.109132361164882e-06,
      "loss": 1.6858,
      "step": 4654
    },
    {
      "epoch": 1.8021680216802167,
      "grad_norm": 14.016328811645508,
      "learning_rate": 9.108702198133094e-06,
      "loss": 1.3387,
      "step": 4655
    },
    {
      "epoch": 1.8025551684088268,
      "grad_norm": 16.02761459350586,
      "learning_rate": 9.108272035101305e-06,
      "loss": 1.3923,
      "step": 4656
    },
    {
      "epoch": 1.802942315137437,
      "grad_norm": 28.98533058166504,
      "learning_rate": 9.107841872069515e-06,
      "loss": 1.9284,
      "step": 4657
    },
    {
      "epoch": 1.8033294618660474,
      "grad_norm": 18.691036224365234,
      "learning_rate": 9.107411709037726e-06,
      "loss": 1.5941,
      "step": 4658
    },
    {
      "epoch": 1.8037166085946574,
      "grad_norm": 22.56295394897461,
      "learning_rate": 9.106981546005938e-06,
      "loss": 1.7079,
      "step": 4659
    },
    {
      "epoch": 1.8041037553232675,
      "grad_norm": 13.460077285766602,
      "learning_rate": 9.106551382974147e-06,
      "loss": 1.0661,
      "step": 4660
    },
    {
      "epoch": 1.8044909020518776,
      "grad_norm": 13.567485809326172,
      "learning_rate": 9.106121219942359e-06,
      "loss": 0.8054,
      "step": 4661
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 13.677257537841797,
      "learning_rate": 9.10569105691057e-06,
      "loss": 0.8951,
      "step": 4662
    },
    {
      "epoch": 1.805265195509098,
      "grad_norm": 14.394182205200195,
      "learning_rate": 9.105260893878782e-06,
      "loss": 0.987,
      "step": 4663
    },
    {
      "epoch": 1.8056523422377082,
      "grad_norm": 30.73436737060547,
      "learning_rate": 9.104830730846991e-06,
      "loss": 1.0659,
      "step": 4664
    },
    {
      "epoch": 1.8060394889663183,
      "grad_norm": 25.233705520629883,
      "learning_rate": 9.104400567815203e-06,
      "loss": 1.4349,
      "step": 4665
    },
    {
      "epoch": 1.8064266356949283,
      "grad_norm": 34.19230270385742,
      "learning_rate": 9.103970404783413e-06,
      "loss": 2.4309,
      "step": 4666
    },
    {
      "epoch": 1.8068137824235384,
      "grad_norm": 28.033540725708008,
      "learning_rate": 9.103540241751626e-06,
      "loss": 1.1249,
      "step": 4667
    },
    {
      "epoch": 1.8072009291521487,
      "grad_norm": 17.124540328979492,
      "learning_rate": 9.103110078719835e-06,
      "loss": 1.2176,
      "step": 4668
    },
    {
      "epoch": 1.8075880758807588,
      "grad_norm": 20.082792282104492,
      "learning_rate": 9.102679915688047e-06,
      "loss": 0.8024,
      "step": 4669
    },
    {
      "epoch": 1.807975222609369,
      "grad_norm": 16.428234100341797,
      "learning_rate": 9.102249752656256e-06,
      "loss": 1.0127,
      "step": 4670
    },
    {
      "epoch": 1.8083623693379791,
      "grad_norm": 14.07869815826416,
      "learning_rate": 9.10181958962447e-06,
      "loss": 0.977,
      "step": 4671
    },
    {
      "epoch": 1.8087495160665892,
      "grad_norm": 18.936771392822266,
      "learning_rate": 9.10138942659268e-06,
      "loss": 1.6585,
      "step": 4672
    },
    {
      "epoch": 1.8091366627951992,
      "grad_norm": 16.570817947387695,
      "learning_rate": 9.10095926356089e-06,
      "loss": 1.4638,
      "step": 4673
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 14.783575057983398,
      "learning_rate": 9.1005291005291e-06,
      "loss": 1.6066,
      "step": 4674
    },
    {
      "epoch": 1.8099109562524198,
      "grad_norm": 19.027999877929688,
      "learning_rate": 9.100098937497312e-06,
      "loss": 0.741,
      "step": 4675
    },
    {
      "epoch": 1.8102981029810299,
      "grad_norm": 13.174925804138184,
      "learning_rate": 9.099668774465523e-06,
      "loss": 0.7509,
      "step": 4676
    },
    {
      "epoch": 1.81068524970964,
      "grad_norm": 20.378664016723633,
      "learning_rate": 9.099238611433735e-06,
      "loss": 1.8055,
      "step": 4677
    },
    {
      "epoch": 1.81107239643825,
      "grad_norm": 18.584985733032227,
      "learning_rate": 9.098808448401944e-06,
      "loss": 1.238,
      "step": 4678
    },
    {
      "epoch": 1.81145954316686,
      "grad_norm": 17.527578353881836,
      "learning_rate": 9.098378285370156e-06,
      "loss": 1.5626,
      "step": 4679
    },
    {
      "epoch": 1.8118466898954704,
      "grad_norm": 24.397504806518555,
      "learning_rate": 9.097948122338367e-06,
      "loss": 2.124,
      "step": 4680
    },
    {
      "epoch": 1.8122338366240807,
      "grad_norm": 20.804956436157227,
      "learning_rate": 9.097517959306577e-06,
      "loss": 1.5479,
      "step": 4681
    },
    {
      "epoch": 1.8126209833526907,
      "grad_norm": 15.115038871765137,
      "learning_rate": 9.09708779627479e-06,
      "loss": 1.4153,
      "step": 4682
    },
    {
      "epoch": 1.8130081300813008,
      "grad_norm": 21.61750602722168,
      "learning_rate": 9.096657633243e-06,
      "loss": 1.7184,
      "step": 4683
    },
    {
      "epoch": 1.8133952768099109,
      "grad_norm": 21.904491424560547,
      "learning_rate": 9.096227470211211e-06,
      "loss": 1.6684,
      "step": 4684
    },
    {
      "epoch": 1.8137824235385211,
      "grad_norm": 28.801706314086914,
      "learning_rate": 9.095797307179421e-06,
      "loss": 2.2498,
      "step": 4685
    },
    {
      "epoch": 1.8141695702671312,
      "grad_norm": 25.440006256103516,
      "learning_rate": 9.095367144147634e-06,
      "loss": 1.305,
      "step": 4686
    },
    {
      "epoch": 1.8145567169957415,
      "grad_norm": 8.483223915100098,
      "learning_rate": 9.094936981115844e-06,
      "loss": 0.3485,
      "step": 4687
    },
    {
      "epoch": 1.8149438637243516,
      "grad_norm": 39.17142105102539,
      "learning_rate": 9.094506818084055e-06,
      "loss": 2.3069,
      "step": 4688
    },
    {
      "epoch": 1.8153310104529616,
      "grad_norm": 20.911758422851562,
      "learning_rate": 9.094076655052265e-06,
      "loss": 1.7043,
      "step": 4689
    },
    {
      "epoch": 1.8157181571815717,
      "grad_norm": 19.37892723083496,
      "learning_rate": 9.093646492020476e-06,
      "loss": 1.6873,
      "step": 4690
    },
    {
      "epoch": 1.816105303910182,
      "grad_norm": 26.744487762451172,
      "learning_rate": 9.093216328988688e-06,
      "loss": 3.8207,
      "step": 4691
    },
    {
      "epoch": 1.816492450638792,
      "grad_norm": 19.85626220703125,
      "learning_rate": 9.0927861659569e-06,
      "loss": 2.3377,
      "step": 4692
    },
    {
      "epoch": 1.8168795973674023,
      "grad_norm": 28.156740188598633,
      "learning_rate": 9.092356002925109e-06,
      "loss": 2.5679,
      "step": 4693
    },
    {
      "epoch": 1.8172667440960124,
      "grad_norm": 18.01519203186035,
      "learning_rate": 9.09192583989332e-06,
      "loss": 1.5446,
      "step": 4694
    },
    {
      "epoch": 1.8176538908246225,
      "grad_norm": 12.032381057739258,
      "learning_rate": 9.091495676861532e-06,
      "loss": 0.5468,
      "step": 4695
    },
    {
      "epoch": 1.8180410375532325,
      "grad_norm": 18.528240203857422,
      "learning_rate": 9.091065513829741e-06,
      "loss": 1.3596,
      "step": 4696
    },
    {
      "epoch": 1.8184281842818428,
      "grad_norm": 15.683764457702637,
      "learning_rate": 9.090635350797953e-06,
      "loss": 1.0041,
      "step": 4697
    },
    {
      "epoch": 1.8188153310104531,
      "grad_norm": 32.717288970947266,
      "learning_rate": 9.090205187766164e-06,
      "loss": 1.1922,
      "step": 4698
    },
    {
      "epoch": 1.8192024777390632,
      "grad_norm": 14.342623710632324,
      "learning_rate": 9.089775024734376e-06,
      "loss": 1.4727,
      "step": 4699
    },
    {
      "epoch": 1.8195896244676733,
      "grad_norm": 20.59796905517578,
      "learning_rate": 9.089344861702585e-06,
      "loss": 0.9084,
      "step": 4700
    },
    {
      "epoch": 1.8199767711962833,
      "grad_norm": 25.421886444091797,
      "learning_rate": 9.088914698670797e-06,
      "loss": 1.5512,
      "step": 4701
    },
    {
      "epoch": 1.8203639179248934,
      "grad_norm": 30.12930679321289,
      "learning_rate": 9.088484535639008e-06,
      "loss": 2.489,
      "step": 4702
    },
    {
      "epoch": 1.8207510646535037,
      "grad_norm": 13.32548999786377,
      "learning_rate": 9.08805437260722e-06,
      "loss": 0.8011,
      "step": 4703
    },
    {
      "epoch": 1.821138211382114,
      "grad_norm": 40.96614074707031,
      "learning_rate": 9.08762420957543e-06,
      "loss": 1.9293,
      "step": 4704
    },
    {
      "epoch": 1.821525358110724,
      "grad_norm": 17.10525131225586,
      "learning_rate": 9.08719404654364e-06,
      "loss": 1.622,
      "step": 4705
    },
    {
      "epoch": 1.821912504839334,
      "grad_norm": 13.107076644897461,
      "learning_rate": 9.086763883511852e-06,
      "loss": 0.8672,
      "step": 4706
    },
    {
      "epoch": 1.8222996515679442,
      "grad_norm": 18.5621280670166,
      "learning_rate": 9.086333720480064e-06,
      "loss": 1.1973,
      "step": 4707
    },
    {
      "epoch": 1.8226867982965544,
      "grad_norm": 16.534631729125977,
      "learning_rate": 9.085903557448273e-06,
      "loss": 0.9444,
      "step": 4708
    },
    {
      "epoch": 1.8230739450251645,
      "grad_norm": 16.50160789489746,
      "learning_rate": 9.085473394416485e-06,
      "loss": 0.6599,
      "step": 4709
    },
    {
      "epoch": 1.8234610917537748,
      "grad_norm": 15.50728702545166,
      "learning_rate": 9.085043231384696e-06,
      "loss": 0.7606,
      "step": 4710
    },
    {
      "epoch": 1.8238482384823849,
      "grad_norm": 21.26910972595215,
      "learning_rate": 9.084613068352906e-06,
      "loss": 1.7214,
      "step": 4711
    },
    {
      "epoch": 1.824235385210995,
      "grad_norm": 10.359936714172363,
      "learning_rate": 9.084182905321117e-06,
      "loss": 1.1222,
      "step": 4712
    },
    {
      "epoch": 1.824622531939605,
      "grad_norm": 54.040748596191406,
      "learning_rate": 9.083752742289329e-06,
      "loss": 1.7876,
      "step": 4713
    },
    {
      "epoch": 1.8250096786682153,
      "grad_norm": 16.002439498901367,
      "learning_rate": 9.08332257925754e-06,
      "loss": 0.9005,
      "step": 4714
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 16.573368072509766,
      "learning_rate": 9.08289241622575e-06,
      "loss": 1.0502,
      "step": 4715
    },
    {
      "epoch": 1.8257839721254356,
      "grad_norm": 12.731379508972168,
      "learning_rate": 9.082462253193961e-06,
      "loss": 0.7754,
      "step": 4716
    },
    {
      "epoch": 1.8261711188540457,
      "grad_norm": 16.716434478759766,
      "learning_rate": 9.082032090162171e-06,
      "loss": 1.509,
      "step": 4717
    },
    {
      "epoch": 1.8265582655826558,
      "grad_norm": 17.859046936035156,
      "learning_rate": 9.081601927130384e-06,
      "loss": 1.2797,
      "step": 4718
    },
    {
      "epoch": 1.8269454123112658,
      "grad_norm": 13.62917423248291,
      "learning_rate": 9.081171764098594e-06,
      "loss": 0.6727,
      "step": 4719
    },
    {
      "epoch": 1.8273325590398761,
      "grad_norm": 21.967517852783203,
      "learning_rate": 9.080741601066805e-06,
      "loss": 1.102,
      "step": 4720
    },
    {
      "epoch": 1.8277197057684864,
      "grad_norm": 17.707082748413086,
      "learning_rate": 9.080311438035015e-06,
      "loss": 1.3329,
      "step": 4721
    },
    {
      "epoch": 1.8281068524970965,
      "grad_norm": 59.55329895019531,
      "learning_rate": 9.079881275003228e-06,
      "loss": 3.4179,
      "step": 4722
    },
    {
      "epoch": 1.8284939992257065,
      "grad_norm": 57.547576904296875,
      "learning_rate": 9.079451111971438e-06,
      "loss": 3.6522,
      "step": 4723
    },
    {
      "epoch": 1.8288811459543166,
      "grad_norm": 16.085952758789062,
      "learning_rate": 9.07902094893965e-06,
      "loss": 1.0819,
      "step": 4724
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 12.748781204223633,
      "learning_rate": 9.07859078590786e-06,
      "loss": 0.9518,
      "step": 4725
    },
    {
      "epoch": 1.829655439411537,
      "grad_norm": 21.87891960144043,
      "learning_rate": 9.07816062287607e-06,
      "loss": 2.1397,
      "step": 4726
    },
    {
      "epoch": 1.8300425861401473,
      "grad_norm": 12.068488121032715,
      "learning_rate": 9.077730459844282e-06,
      "loss": 1.0799,
      "step": 4727
    },
    {
      "epoch": 1.8304297328687573,
      "grad_norm": 19.47056770324707,
      "learning_rate": 9.077300296812493e-06,
      "loss": 1.988,
      "step": 4728
    },
    {
      "epoch": 1.8308168795973674,
      "grad_norm": 21.73441505432129,
      "learning_rate": 9.076870133780705e-06,
      "loss": 2.6527,
      "step": 4729
    },
    {
      "epoch": 1.8312040263259775,
      "grad_norm": 26.396364212036133,
      "learning_rate": 9.076439970748914e-06,
      "loss": 2.7494,
      "step": 4730
    },
    {
      "epoch": 1.8315911730545877,
      "grad_norm": 18.084714889526367,
      "learning_rate": 9.076009807717126e-06,
      "loss": 1.7473,
      "step": 4731
    },
    {
      "epoch": 1.8319783197831978,
      "grad_norm": 9.633057594299316,
      "learning_rate": 9.075579644685335e-06,
      "loss": 0.6152,
      "step": 4732
    },
    {
      "epoch": 1.832365466511808,
      "grad_norm": 15.700702667236328,
      "learning_rate": 9.075149481653549e-06,
      "loss": 1.0031,
      "step": 4733
    },
    {
      "epoch": 1.8327526132404182,
      "grad_norm": 19.861698150634766,
      "learning_rate": 9.074719318621758e-06,
      "loss": 1.3151,
      "step": 4734
    },
    {
      "epoch": 1.8331397599690282,
      "grad_norm": 25.351274490356445,
      "learning_rate": 9.07428915558997e-06,
      "loss": 1.7763,
      "step": 4735
    },
    {
      "epoch": 1.8335269066976383,
      "grad_norm": 45.254878997802734,
      "learning_rate": 9.07385899255818e-06,
      "loss": 1.2898,
      "step": 4736
    },
    {
      "epoch": 1.8339140534262486,
      "grad_norm": 15.296209335327148,
      "learning_rate": 9.073428829526393e-06,
      "loss": 1.0225,
      "step": 4737
    },
    {
      "epoch": 1.8343012001548586,
      "grad_norm": 23.304718017578125,
      "learning_rate": 9.072998666494602e-06,
      "loss": 1.4655,
      "step": 4738
    },
    {
      "epoch": 1.834688346883469,
      "grad_norm": 10.28002643585205,
      "learning_rate": 9.072568503462814e-06,
      "loss": 1.2175,
      "step": 4739
    },
    {
      "epoch": 1.835075493612079,
      "grad_norm": 11.539523124694824,
      "learning_rate": 9.072138340431023e-06,
      "loss": 1.1621,
      "step": 4740
    },
    {
      "epoch": 1.835462640340689,
      "grad_norm": 20.335124969482422,
      "learning_rate": 9.071708177399235e-06,
      "loss": 1.3476,
      "step": 4741
    },
    {
      "epoch": 1.8358497870692991,
      "grad_norm": 45.76581573486328,
      "learning_rate": 9.071278014367446e-06,
      "loss": 1.6607,
      "step": 4742
    },
    {
      "epoch": 1.8362369337979094,
      "grad_norm": 23.19999885559082,
      "learning_rate": 9.070847851335658e-06,
      "loss": 0.8655,
      "step": 4743
    },
    {
      "epoch": 1.8366240805265197,
      "grad_norm": 13.703385353088379,
      "learning_rate": 9.070417688303867e-06,
      "loss": 0.826,
      "step": 4744
    },
    {
      "epoch": 1.8370112272551298,
      "grad_norm": 21.16792869567871,
      "learning_rate": 9.069987525272079e-06,
      "loss": 2.0475,
      "step": 4745
    },
    {
      "epoch": 1.8373983739837398,
      "grad_norm": 9.595331192016602,
      "learning_rate": 9.06955736224029e-06,
      "loss": 0.6149,
      "step": 4746
    },
    {
      "epoch": 1.83778552071235,
      "grad_norm": 22.84583854675293,
      "learning_rate": 9.0691271992085e-06,
      "loss": 1.6638,
      "step": 4747
    },
    {
      "epoch": 1.83817266744096,
      "grad_norm": 23.357730865478516,
      "learning_rate": 9.068697036176711e-06,
      "loss": 2.4136,
      "step": 4748
    },
    {
      "epoch": 1.8385598141695703,
      "grad_norm": 18.276216506958008,
      "learning_rate": 9.068266873144923e-06,
      "loss": 1.3755,
      "step": 4749
    },
    {
      "epoch": 1.8389469608981805,
      "grad_norm": 23.345197677612305,
      "learning_rate": 9.067836710113134e-06,
      "loss": 2.4195,
      "step": 4750
    },
    {
      "epoch": 1.8393341076267906,
      "grad_norm": 14.439151763916016,
      "learning_rate": 9.067406547081344e-06,
      "loss": 0.7689,
      "step": 4751
    },
    {
      "epoch": 1.8397212543554007,
      "grad_norm": 23.39655303955078,
      "learning_rate": 9.066976384049555e-06,
      "loss": 1.6226,
      "step": 4752
    },
    {
      "epoch": 1.8401084010840107,
      "grad_norm": 15.727763175964355,
      "learning_rate": 9.066546221017767e-06,
      "loss": 0.9439,
      "step": 4753
    },
    {
      "epoch": 1.840495547812621,
      "grad_norm": 16.315696716308594,
      "learning_rate": 9.066116057985978e-06,
      "loss": 1.2588,
      "step": 4754
    },
    {
      "epoch": 1.840882694541231,
      "grad_norm": 13.965880393981934,
      "learning_rate": 9.065685894954188e-06,
      "loss": 0.86,
      "step": 4755
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 13.084175109863281,
      "learning_rate": 9.0652557319224e-06,
      "loss": 0.8393,
      "step": 4756
    },
    {
      "epoch": 1.8416569879984515,
      "grad_norm": 34.75734329223633,
      "learning_rate": 9.06482556889061e-06,
      "loss": 2.5995,
      "step": 4757
    },
    {
      "epoch": 1.8420441347270615,
      "grad_norm": 29.392772674560547,
      "learning_rate": 9.064395405858822e-06,
      "loss": 1.3374,
      "step": 4758
    },
    {
      "epoch": 1.8424312814556716,
      "grad_norm": 19.343809127807617,
      "learning_rate": 9.063965242827032e-06,
      "loss": 1.1225,
      "step": 4759
    },
    {
      "epoch": 1.8428184281842819,
      "grad_norm": 20.661174774169922,
      "learning_rate": 9.063535079795243e-06,
      "loss": 1.2181,
      "step": 4760
    },
    {
      "epoch": 1.843205574912892,
      "grad_norm": 28.99399185180664,
      "learning_rate": 9.063104916763455e-06,
      "loss": 1.5197,
      "step": 4761
    },
    {
      "epoch": 1.8435927216415022,
      "grad_norm": 15.362297058105469,
      "learning_rate": 9.062674753731664e-06,
      "loss": 1.3532,
      "step": 4762
    },
    {
      "epoch": 1.8439798683701123,
      "grad_norm": 29.799381256103516,
      "learning_rate": 9.062244590699876e-06,
      "loss": 1.6654,
      "step": 4763
    },
    {
      "epoch": 1.8443670150987224,
      "grad_norm": 19.358633041381836,
      "learning_rate": 9.061814427668087e-06,
      "loss": 1.745,
      "step": 4764
    },
    {
      "epoch": 1.8447541618273324,
      "grad_norm": 29.13005256652832,
      "learning_rate": 9.061384264636299e-06,
      "loss": 1.4424,
      "step": 4765
    },
    {
      "epoch": 1.8451413085559427,
      "grad_norm": 16.269596099853516,
      "learning_rate": 9.060954101604508e-06,
      "loss": 0.9686,
      "step": 4766
    },
    {
      "epoch": 1.845528455284553,
      "grad_norm": 17.469667434692383,
      "learning_rate": 9.06052393857272e-06,
      "loss": 1.0925,
      "step": 4767
    },
    {
      "epoch": 1.845915602013163,
      "grad_norm": 30.074661254882812,
      "learning_rate": 9.060093775540931e-06,
      "loss": 1.0673,
      "step": 4768
    },
    {
      "epoch": 1.8463027487417731,
      "grad_norm": 16.127859115600586,
      "learning_rate": 9.059663612509143e-06,
      "loss": 1.2864,
      "step": 4769
    },
    {
      "epoch": 1.8466898954703832,
      "grad_norm": 23.439823150634766,
      "learning_rate": 9.059233449477352e-06,
      "loss": 2.0354,
      "step": 4770
    },
    {
      "epoch": 1.8470770421989933,
      "grad_norm": 18.812597274780273,
      "learning_rate": 9.058803286445564e-06,
      "loss": 1.957,
      "step": 4771
    },
    {
      "epoch": 1.8474641889276036,
      "grad_norm": 26.657691955566406,
      "learning_rate": 9.058373123413775e-06,
      "loss": 1.4634,
      "step": 4772
    },
    {
      "epoch": 1.8478513356562138,
      "grad_norm": 14.918055534362793,
      "learning_rate": 9.057942960381987e-06,
      "loss": 1.0426,
      "step": 4773
    },
    {
      "epoch": 1.848238482384824,
      "grad_norm": 14.6629056930542,
      "learning_rate": 9.057512797350196e-06,
      "loss": 1.4661,
      "step": 4774
    },
    {
      "epoch": 1.848625629113434,
      "grad_norm": 27.451555252075195,
      "learning_rate": 9.057082634318408e-06,
      "loss": 1.7831,
      "step": 4775
    },
    {
      "epoch": 1.849012775842044,
      "grad_norm": 13.879433631896973,
      "learning_rate": 9.056652471286619e-06,
      "loss": 1.0763,
      "step": 4776
    },
    {
      "epoch": 1.8493999225706543,
      "grad_norm": 25.5767765045166,
      "learning_rate": 9.056222308254829e-06,
      "loss": 1.4026,
      "step": 4777
    },
    {
      "epoch": 1.8497870692992644,
      "grad_norm": 15.759086608886719,
      "learning_rate": 9.05579214522304e-06,
      "loss": 1.5206,
      "step": 4778
    },
    {
      "epoch": 1.8501742160278747,
      "grad_norm": 28.017648696899414,
      "learning_rate": 9.055361982191252e-06,
      "loss": 1.4793,
      "step": 4779
    },
    {
      "epoch": 1.8505613627564848,
      "grad_norm": 14.912091255187988,
      "learning_rate": 9.054931819159463e-06,
      "loss": 1.4594,
      "step": 4780
    },
    {
      "epoch": 1.8509485094850948,
      "grad_norm": 29.08673858642578,
      "learning_rate": 9.054501656127673e-06,
      "loss": 3.808,
      "step": 4781
    },
    {
      "epoch": 1.8513356562137049,
      "grad_norm": 15.783758163452148,
      "learning_rate": 9.054071493095884e-06,
      "loss": 1.5341,
      "step": 4782
    },
    {
      "epoch": 1.8517228029423152,
      "grad_norm": 19.635221481323242,
      "learning_rate": 9.053641330064094e-06,
      "loss": 3.5291,
      "step": 4783
    },
    {
      "epoch": 1.8521099496709252,
      "grad_norm": 16.68414878845215,
      "learning_rate": 9.053211167032307e-06,
      "loss": 1.2438,
      "step": 4784
    },
    {
      "epoch": 1.8524970963995355,
      "grad_norm": 13.648909568786621,
      "learning_rate": 9.052781004000517e-06,
      "loss": 1.0456,
      "step": 4785
    },
    {
      "epoch": 1.8528842431281456,
      "grad_norm": 26.786710739135742,
      "learning_rate": 9.052350840968728e-06,
      "loss": 0.9611,
      "step": 4786
    },
    {
      "epoch": 1.8532713898567557,
      "grad_norm": 16.374773025512695,
      "learning_rate": 9.051920677936938e-06,
      "loss": 1.4488,
      "step": 4787
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 18.653093338012695,
      "learning_rate": 9.051490514905151e-06,
      "loss": 1.4016,
      "step": 4788
    },
    {
      "epoch": 1.854045683313976,
      "grad_norm": 24.571386337280273,
      "learning_rate": 9.05106035187336e-06,
      "loss": 2.7764,
      "step": 4789
    },
    {
      "epoch": 1.8544328300425863,
      "grad_norm": 15.28890323638916,
      "learning_rate": 9.050630188841572e-06,
      "loss": 1.1779,
      "step": 4790
    },
    {
      "epoch": 1.8548199767711964,
      "grad_norm": 15.232799530029297,
      "learning_rate": 9.050200025809782e-06,
      "loss": 1.43,
      "step": 4791
    },
    {
      "epoch": 1.8552071234998064,
      "grad_norm": 14.443206787109375,
      "learning_rate": 9.049769862777993e-06,
      "loss": 1.3688,
      "step": 4792
    },
    {
      "epoch": 1.8555942702284165,
      "grad_norm": 19.68345069885254,
      "learning_rate": 9.049339699746205e-06,
      "loss": 1.0042,
      "step": 4793
    },
    {
      "epoch": 1.8559814169570266,
      "grad_norm": 17.868690490722656,
      "learning_rate": 9.048909536714416e-06,
      "loss": 1.3129,
      "step": 4794
    },
    {
      "epoch": 1.8563685636856369,
      "grad_norm": 30.245113372802734,
      "learning_rate": 9.048479373682626e-06,
      "loss": 1.7796,
      "step": 4795
    },
    {
      "epoch": 1.8567557104142471,
      "grad_norm": 28.681182861328125,
      "learning_rate": 9.048049210650837e-06,
      "loss": 2.0294,
      "step": 4796
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 22.450902938842773,
      "learning_rate": 9.047619047619049e-06,
      "loss": 1.5833,
      "step": 4797
    },
    {
      "epoch": 1.8575300038714673,
      "grad_norm": 14.8160982131958,
      "learning_rate": 9.047188884587258e-06,
      "loss": 1.421,
      "step": 4798
    },
    {
      "epoch": 1.8579171506000773,
      "grad_norm": 25.503637313842773,
      "learning_rate": 9.04675872155547e-06,
      "loss": 1.7958,
      "step": 4799
    },
    {
      "epoch": 1.8583042973286876,
      "grad_norm": 42.9366340637207,
      "learning_rate": 9.046328558523681e-06,
      "loss": 2.3292,
      "step": 4800
    },
    {
      "epoch": 1.8586914440572977,
      "grad_norm": 20.017213821411133,
      "learning_rate": 9.045898395491893e-06,
      "loss": 1.6969,
      "step": 4801
    },
    {
      "epoch": 1.859078590785908,
      "grad_norm": 20.79746437072754,
      "learning_rate": 9.045468232460102e-06,
      "loss": 0.9016,
      "step": 4802
    },
    {
      "epoch": 1.859465737514518,
      "grad_norm": 14.973353385925293,
      "learning_rate": 9.045038069428314e-06,
      "loss": 1.4254,
      "step": 4803
    },
    {
      "epoch": 1.8598528842431281,
      "grad_norm": 25.202787399291992,
      "learning_rate": 9.044607906396525e-06,
      "loss": 3.1384,
      "step": 4804
    },
    {
      "epoch": 1.8602400309717382,
      "grad_norm": 28.751760482788086,
      "learning_rate": 9.044177743364737e-06,
      "loss": 2.2753,
      "step": 4805
    },
    {
      "epoch": 1.8606271777003485,
      "grad_norm": 15.216693878173828,
      "learning_rate": 9.043747580332946e-06,
      "loss": 1.5844,
      "step": 4806
    },
    {
      "epoch": 1.8610143244289585,
      "grad_norm": 20.611061096191406,
      "learning_rate": 9.043317417301158e-06,
      "loss": 1.1166,
      "step": 4807
    },
    {
      "epoch": 1.8614014711575688,
      "grad_norm": 12.601858139038086,
      "learning_rate": 9.04288725426937e-06,
      "loss": 0.8852,
      "step": 4808
    },
    {
      "epoch": 1.8617886178861789,
      "grad_norm": 22.922582626342773,
      "learning_rate": 9.04245709123758e-06,
      "loss": 1.0052,
      "step": 4809
    },
    {
      "epoch": 1.862175764614789,
      "grad_norm": 37.34313201904297,
      "learning_rate": 9.04202692820579e-06,
      "loss": 1.4511,
      "step": 4810
    },
    {
      "epoch": 1.862562911343399,
      "grad_norm": 28.52491569519043,
      "learning_rate": 9.041596765174002e-06,
      "loss": 1.8636,
      "step": 4811
    },
    {
      "epoch": 1.8629500580720093,
      "grad_norm": 27.182514190673828,
      "learning_rate": 9.041166602142213e-06,
      "loss": 1.4325,
      "step": 4812
    },
    {
      "epoch": 1.8633372048006196,
      "grad_norm": 20.66800308227539,
      "learning_rate": 9.040736439110423e-06,
      "loss": 1.4575,
      "step": 4813
    },
    {
      "epoch": 1.8637243515292297,
      "grad_norm": 30.405241012573242,
      "learning_rate": 9.040306276078634e-06,
      "loss": 1.8148,
      "step": 4814
    },
    {
      "epoch": 1.8641114982578397,
      "grad_norm": 17.600982666015625,
      "learning_rate": 9.039876113046846e-06,
      "loss": 1.7735,
      "step": 4815
    },
    {
      "epoch": 1.8644986449864498,
      "grad_norm": 11.933355331420898,
      "learning_rate": 9.039445950015057e-06,
      "loss": 0.7676,
      "step": 4816
    },
    {
      "epoch": 1.8648857917150599,
      "grad_norm": 17.385284423828125,
      "learning_rate": 9.039015786983267e-06,
      "loss": 1.5362,
      "step": 4817
    },
    {
      "epoch": 1.8652729384436701,
      "grad_norm": 18.979774475097656,
      "learning_rate": 9.038585623951478e-06,
      "loss": 1.9994,
      "step": 4818
    },
    {
      "epoch": 1.8656600851722804,
      "grad_norm": 20.254302978515625,
      "learning_rate": 9.03815546091969e-06,
      "loss": 1.748,
      "step": 4819
    },
    {
      "epoch": 1.8660472319008905,
      "grad_norm": 13.354011535644531,
      "learning_rate": 9.037725297887901e-06,
      "loss": 0.5541,
      "step": 4820
    },
    {
      "epoch": 1.8664343786295006,
      "grad_norm": 18.440994262695312,
      "learning_rate": 9.03729513485611e-06,
      "loss": 1.4478,
      "step": 4821
    },
    {
      "epoch": 1.8668215253581106,
      "grad_norm": 13.879532814025879,
      "learning_rate": 9.036864971824322e-06,
      "loss": 0.8082,
      "step": 4822
    },
    {
      "epoch": 1.867208672086721,
      "grad_norm": 26.9272403717041,
      "learning_rate": 9.036434808792534e-06,
      "loss": 0.7744,
      "step": 4823
    },
    {
      "epoch": 1.867595818815331,
      "grad_norm": 36.60193634033203,
      "learning_rate": 9.036004645760745e-06,
      "loss": 2.0296,
      "step": 4824
    },
    {
      "epoch": 1.8679829655439413,
      "grad_norm": 30.688915252685547,
      "learning_rate": 9.035574482728955e-06,
      "loss": 1.2468,
      "step": 4825
    },
    {
      "epoch": 1.8683701122725513,
      "grad_norm": 24.419816970825195,
      "learning_rate": 9.035144319697166e-06,
      "loss": 1.6243,
      "step": 4826
    },
    {
      "epoch": 1.8687572590011614,
      "grad_norm": 28.06755828857422,
      "learning_rate": 9.034714156665378e-06,
      "loss": 1.3556,
      "step": 4827
    },
    {
      "epoch": 1.8691444057297715,
      "grad_norm": 22.00957489013672,
      "learning_rate": 9.034283993633587e-06,
      "loss": 1.81,
      "step": 4828
    },
    {
      "epoch": 1.8695315524583818,
      "grad_norm": 21.631187438964844,
      "learning_rate": 9.033853830601799e-06,
      "loss": 2.4254,
      "step": 4829
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 17.648027420043945,
      "learning_rate": 9.03342366757001e-06,
      "loss": 1.5423,
      "step": 4830
    },
    {
      "epoch": 1.8703058459156021,
      "grad_norm": 15.049750328063965,
      "learning_rate": 9.032993504538222e-06,
      "loss": 0.9388,
      "step": 4831
    },
    {
      "epoch": 1.8706929926442122,
      "grad_norm": 10.187567710876465,
      "learning_rate": 9.032563341506431e-06,
      "loss": 1.1964,
      "step": 4832
    },
    {
      "epoch": 1.8710801393728222,
      "grad_norm": 24.172895431518555,
      "learning_rate": 9.032133178474643e-06,
      "loss": 2.0852,
      "step": 4833
    },
    {
      "epoch": 1.8714672861014323,
      "grad_norm": 19.82834815979004,
      "learning_rate": 9.031703015442852e-06,
      "loss": 2.305,
      "step": 4834
    },
    {
      "epoch": 1.8718544328300426,
      "grad_norm": 18.459020614624023,
      "learning_rate": 9.031272852411066e-06,
      "loss": 1.5589,
      "step": 4835
    },
    {
      "epoch": 1.872241579558653,
      "grad_norm": 25.391210556030273,
      "learning_rate": 9.030842689379275e-06,
      "loss": 2.1315,
      "step": 4836
    },
    {
      "epoch": 1.872628726287263,
      "grad_norm": 32.55442428588867,
      "learning_rate": 9.030412526347487e-06,
      "loss": 1.6763,
      "step": 4837
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 12.192415237426758,
      "learning_rate": 9.029982363315696e-06,
      "loss": 0.6951,
      "step": 4838
    },
    {
      "epoch": 1.873403019744483,
      "grad_norm": 27.946279525756836,
      "learning_rate": 9.02955220028391e-06,
      "loss": 1.0936,
      "step": 4839
    },
    {
      "epoch": 1.8737901664730932,
      "grad_norm": 26.880645751953125,
      "learning_rate": 9.02912203725212e-06,
      "loss": 0.5588,
      "step": 4840
    },
    {
      "epoch": 1.8741773132017034,
      "grad_norm": 14.52866268157959,
      "learning_rate": 9.02869187422033e-06,
      "loss": 1.5185,
      "step": 4841
    },
    {
      "epoch": 1.8745644599303137,
      "grad_norm": 14.86014175415039,
      "learning_rate": 9.02826171118854e-06,
      "loss": 0.9726,
      "step": 4842
    },
    {
      "epoch": 1.8749516066589238,
      "grad_norm": 17.194509506225586,
      "learning_rate": 9.027831548156752e-06,
      "loss": 1.0442,
      "step": 4843
    },
    {
      "epoch": 1.8753387533875339,
      "grad_norm": 13.533390045166016,
      "learning_rate": 9.027401385124963e-06,
      "loss": 1.2204,
      "step": 4844
    },
    {
      "epoch": 1.875725900116144,
      "grad_norm": 17.0097599029541,
      "learning_rate": 9.026971222093175e-06,
      "loss": 1.5597,
      "step": 4845
    },
    {
      "epoch": 1.876113046844754,
      "grad_norm": 16.152118682861328,
      "learning_rate": 9.026541059061386e-06,
      "loss": 1.5325,
      "step": 4846
    },
    {
      "epoch": 1.8765001935733643,
      "grad_norm": 16.320127487182617,
      "learning_rate": 9.026110896029596e-06,
      "loss": 0.9221,
      "step": 4847
    },
    {
      "epoch": 1.8768873403019746,
      "grad_norm": 19.385662078857422,
      "learning_rate": 9.025680732997807e-06,
      "loss": 1.2747,
      "step": 4848
    },
    {
      "epoch": 1.8772744870305846,
      "grad_norm": 15.370759010314941,
      "learning_rate": 9.025250569966017e-06,
      "loss": 0.8985,
      "step": 4849
    },
    {
      "epoch": 1.8776616337591947,
      "grad_norm": 12.312460899353027,
      "learning_rate": 9.02482040693423e-06,
      "loss": 1.3343,
      "step": 4850
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 22.102134704589844,
      "learning_rate": 9.02439024390244e-06,
      "loss": 1.4608,
      "step": 4851
    },
    {
      "epoch": 1.878435927216415,
      "grad_norm": 21.798173904418945,
      "learning_rate": 9.023960080870651e-06,
      "loss": 1.8657,
      "step": 4852
    },
    {
      "epoch": 1.8788230739450251,
      "grad_norm": 20.847139358520508,
      "learning_rate": 9.023529917838861e-06,
      "loss": 0.985,
      "step": 4853
    },
    {
      "epoch": 1.8792102206736354,
      "grad_norm": 17.23723030090332,
      "learning_rate": 9.023099754807072e-06,
      "loss": 1.1416,
      "step": 4854
    },
    {
      "epoch": 1.8795973674022455,
      "grad_norm": 24.79956817626953,
      "learning_rate": 9.022669591775284e-06,
      "loss": 1.544,
      "step": 4855
    },
    {
      "epoch": 1.8799845141308555,
      "grad_norm": 22.3746337890625,
      "learning_rate": 9.022239428743495e-06,
      "loss": 1.7372,
      "step": 4856
    },
    {
      "epoch": 1.8803716608594656,
      "grad_norm": 14.972613334655762,
      "learning_rate": 9.021809265711705e-06,
      "loss": 1.3692,
      "step": 4857
    },
    {
      "epoch": 1.880758807588076,
      "grad_norm": 18.260515213012695,
      "learning_rate": 9.021379102679916e-06,
      "loss": 1.3446,
      "step": 4858
    },
    {
      "epoch": 1.8811459543166862,
      "grad_norm": 20.09796714782715,
      "learning_rate": 9.020948939648128e-06,
      "loss": 1.3448,
      "step": 4859
    },
    {
      "epoch": 1.8815331010452963,
      "grad_norm": 25.705270767211914,
      "learning_rate": 9.020518776616339e-06,
      "loss": 1.3962,
      "step": 4860
    },
    {
      "epoch": 1.8819202477739063,
      "grad_norm": 20.59067153930664,
      "learning_rate": 9.020088613584549e-06,
      "loss": 1.3067,
      "step": 4861
    },
    {
      "epoch": 1.8823073945025164,
      "grad_norm": 31.531557083129883,
      "learning_rate": 9.01965845055276e-06,
      "loss": 1.6964,
      "step": 4862
    },
    {
      "epoch": 1.8826945412311265,
      "grad_norm": 30.551698684692383,
      "learning_rate": 9.019228287520972e-06,
      "loss": 1.5251,
      "step": 4863
    },
    {
      "epoch": 1.8830816879597367,
      "grad_norm": 34.48265075683594,
      "learning_rate": 9.018798124489181e-06,
      "loss": 3.0648,
      "step": 4864
    },
    {
      "epoch": 1.883468834688347,
      "grad_norm": 20.82428741455078,
      "learning_rate": 9.018367961457393e-06,
      "loss": 1.81,
      "step": 4865
    },
    {
      "epoch": 1.883855981416957,
      "grad_norm": 36.51818084716797,
      "learning_rate": 9.017937798425604e-06,
      "loss": 2.2805,
      "step": 4866
    },
    {
      "epoch": 1.8842431281455672,
      "grad_norm": 22.640655517578125,
      "learning_rate": 9.017507635393816e-06,
      "loss": 1.2713,
      "step": 4867
    },
    {
      "epoch": 1.8846302748741772,
      "grad_norm": 6.875522136688232,
      "learning_rate": 9.017077472362025e-06,
      "loss": 0.2192,
      "step": 4868
    },
    {
      "epoch": 1.8850174216027873,
      "grad_norm": 9.207972526550293,
      "learning_rate": 9.016647309330237e-06,
      "loss": 0.3702,
      "step": 4869
    },
    {
      "epoch": 1.8854045683313976,
      "grad_norm": 27.489046096801758,
      "learning_rate": 9.016217146298448e-06,
      "loss": 1.3441,
      "step": 4870
    },
    {
      "epoch": 1.8857917150600079,
      "grad_norm": 23.544109344482422,
      "learning_rate": 9.01578698326666e-06,
      "loss": 1.6055,
      "step": 4871
    },
    {
      "epoch": 1.886178861788618,
      "grad_norm": 8.501269340515137,
      "learning_rate": 9.01535682023487e-06,
      "loss": 0.281,
      "step": 4872
    },
    {
      "epoch": 1.886566008517228,
      "grad_norm": 24.326276779174805,
      "learning_rate": 9.01492665720308e-06,
      "loss": 2.0524,
      "step": 4873
    },
    {
      "epoch": 1.886953155245838,
      "grad_norm": 45.6463737487793,
      "learning_rate": 9.014496494171292e-06,
      "loss": 2.2004,
      "step": 4874
    },
    {
      "epoch": 1.8873403019744484,
      "grad_norm": 12.111969947814941,
      "learning_rate": 9.014066331139504e-06,
      "loss": 1.1056,
      "step": 4875
    },
    {
      "epoch": 1.8877274487030584,
      "grad_norm": 15.28996467590332,
      "learning_rate": 9.013636168107713e-06,
      "loss": 1.395,
      "step": 4876
    },
    {
      "epoch": 1.8881145954316687,
      "grad_norm": 31.05906105041504,
      "learning_rate": 9.013206005075925e-06,
      "loss": 1.6388,
      "step": 4877
    },
    {
      "epoch": 1.8885017421602788,
      "grad_norm": 21.70121192932129,
      "learning_rate": 9.012775842044136e-06,
      "loss": 1.9754,
      "step": 4878
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 18.480268478393555,
      "learning_rate": 9.012345679012346e-06,
      "loss": 1.5846,
      "step": 4879
    },
    {
      "epoch": 1.889276035617499,
      "grad_norm": 23.643251419067383,
      "learning_rate": 9.011915515980557e-06,
      "loss": 2.0424,
      "step": 4880
    },
    {
      "epoch": 1.8896631823461092,
      "grad_norm": 15.464435577392578,
      "learning_rate": 9.011485352948769e-06,
      "loss": 1.2022,
      "step": 4881
    },
    {
      "epoch": 1.8900503290747195,
      "grad_norm": 13.01025390625,
      "learning_rate": 9.01105518991698e-06,
      "loss": 1.395,
      "step": 4882
    },
    {
      "epoch": 1.8904374758033295,
      "grad_norm": 16.55594253540039,
      "learning_rate": 9.01062502688519e-06,
      "loss": 1.2876,
      "step": 4883
    },
    {
      "epoch": 1.8908246225319396,
      "grad_norm": 22.38551139831543,
      "learning_rate": 9.010194863853401e-06,
      "loss": 2.256,
      "step": 4884
    },
    {
      "epoch": 1.8912117692605497,
      "grad_norm": 26.88243293762207,
      "learning_rate": 9.009764700821611e-06,
      "loss": 1.7546,
      "step": 4885
    },
    {
      "epoch": 1.8915989159891597,
      "grad_norm": 18.60291290283203,
      "learning_rate": 9.009334537789824e-06,
      "loss": 1.403,
      "step": 4886
    },
    {
      "epoch": 1.89198606271777,
      "grad_norm": 19.555078506469727,
      "learning_rate": 9.008904374758034e-06,
      "loss": 1.8596,
      "step": 4887
    },
    {
      "epoch": 1.8923732094463803,
      "grad_norm": 12.822184562683105,
      "learning_rate": 9.008474211726245e-06,
      "loss": 0.9218,
      "step": 4888
    },
    {
      "epoch": 1.8927603561749904,
      "grad_norm": 18.987356185913086,
      "learning_rate": 9.008044048694457e-06,
      "loss": 1.9458,
      "step": 4889
    },
    {
      "epoch": 1.8931475029036005,
      "grad_norm": 20.090641021728516,
      "learning_rate": 9.007613885662666e-06,
      "loss": 1.5383,
      "step": 4890
    },
    {
      "epoch": 1.8935346496322105,
      "grad_norm": 21.058300018310547,
      "learning_rate": 9.007183722630878e-06,
      "loss": 2.4167,
      "step": 4891
    },
    {
      "epoch": 1.8939217963608206,
      "grad_norm": 9.63124942779541,
      "learning_rate": 9.006753559599089e-06,
      "loss": 1.288,
      "step": 4892
    },
    {
      "epoch": 1.8943089430894309,
      "grad_norm": 13.493571281433105,
      "learning_rate": 9.0063233965673e-06,
      "loss": 0.8912,
      "step": 4893
    },
    {
      "epoch": 1.8946960898180412,
      "grad_norm": 17.705041885375977,
      "learning_rate": 9.00589323353551e-06,
      "loss": 0.9878,
      "step": 4894
    },
    {
      "epoch": 1.8950832365466512,
      "grad_norm": 33.49905776977539,
      "learning_rate": 9.005463070503722e-06,
      "loss": 1.6185,
      "step": 4895
    },
    {
      "epoch": 1.8954703832752613,
      "grad_norm": 15.033263206481934,
      "learning_rate": 9.005032907471933e-06,
      "loss": 0.9303,
      "step": 4896
    },
    {
      "epoch": 1.8958575300038714,
      "grad_norm": 29.672603607177734,
      "learning_rate": 9.004602744440145e-06,
      "loss": 1.5941,
      "step": 4897
    },
    {
      "epoch": 1.8962446767324816,
      "grad_norm": 39.99323654174805,
      "learning_rate": 9.004172581408354e-06,
      "loss": 2.2276,
      "step": 4898
    },
    {
      "epoch": 1.8966318234610917,
      "grad_norm": 19.313974380493164,
      "learning_rate": 9.003742418376566e-06,
      "loss": 1.535,
      "step": 4899
    },
    {
      "epoch": 1.897018970189702,
      "grad_norm": 29.926616668701172,
      "learning_rate": 9.003312255344775e-06,
      "loss": 1.3861,
      "step": 4900
    },
    {
      "epoch": 1.897406116918312,
      "grad_norm": 17.096752166748047,
      "learning_rate": 9.002882092312988e-06,
      "loss": 0.3354,
      "step": 4901
    },
    {
      "epoch": 1.8977932636469221,
      "grad_norm": 16.81753921508789,
      "learning_rate": 9.002451929281198e-06,
      "loss": 1.0258,
      "step": 4902
    },
    {
      "epoch": 1.8981804103755322,
      "grad_norm": 22.825397491455078,
      "learning_rate": 9.00202176624941e-06,
      "loss": 1.9347,
      "step": 4903
    },
    {
      "epoch": 1.8985675571041425,
      "grad_norm": 9.373381614685059,
      "learning_rate": 9.00159160321762e-06,
      "loss": 1.2859,
      "step": 4904
    },
    {
      "epoch": 1.8989547038327528,
      "grad_norm": 22.79969596862793,
      "learning_rate": 9.00116144018583e-06,
      "loss": 1.552,
      "step": 4905
    },
    {
      "epoch": 1.8993418505613628,
      "grad_norm": 26.879638671875,
      "learning_rate": 9.000731277154042e-06,
      "loss": 1.0207,
      "step": 4906
    },
    {
      "epoch": 1.899728997289973,
      "grad_norm": 25.43794822692871,
      "learning_rate": 9.000301114122254e-06,
      "loss": 1.1575,
      "step": 4907
    },
    {
      "epoch": 1.900116144018583,
      "grad_norm": 25.885757446289062,
      "learning_rate": 8.999870951090463e-06,
      "loss": 1.4017,
      "step": 4908
    },
    {
      "epoch": 1.900503290747193,
      "grad_norm": 35.24931716918945,
      "learning_rate": 8.999440788058675e-06,
      "loss": 1.0081,
      "step": 4909
    },
    {
      "epoch": 1.9008904374758033,
      "grad_norm": 29.761072158813477,
      "learning_rate": 8.999010625026886e-06,
      "loss": 1.2149,
      "step": 4910
    },
    {
      "epoch": 1.9012775842044136,
      "grad_norm": 10.461771011352539,
      "learning_rate": 8.998580461995098e-06,
      "loss": 1.3643,
      "step": 4911
    },
    {
      "epoch": 1.9016647309330237,
      "grad_norm": 24.999794006347656,
      "learning_rate": 8.998150298963307e-06,
      "loss": 2.8978,
      "step": 4912
    },
    {
      "epoch": 1.9020518776616337,
      "grad_norm": 27.203588485717773,
      "learning_rate": 8.997720135931519e-06,
      "loss": 1.577,
      "step": 4913
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 24.79545783996582,
      "learning_rate": 8.99728997289973e-06,
      "loss": 1.2629,
      "step": 4914
    },
    {
      "epoch": 1.9028261711188539,
      "grad_norm": 20.289138793945312,
      "learning_rate": 8.99685980986794e-06,
      "loss": 0.9668,
      "step": 4915
    },
    {
      "epoch": 1.9032133178474642,
      "grad_norm": 25.089807510375977,
      "learning_rate": 8.996429646836151e-06,
      "loss": 1.4409,
      "step": 4916
    },
    {
      "epoch": 1.9036004645760745,
      "grad_norm": 51.35795593261719,
      "learning_rate": 8.995999483804363e-06,
      "loss": 2.2146,
      "step": 4917
    },
    {
      "epoch": 1.9039876113046845,
      "grad_norm": 17.856700897216797,
      "learning_rate": 8.995569320772574e-06,
      "loss": 1.3768,
      "step": 4918
    },
    {
      "epoch": 1.9043747580332946,
      "grad_norm": 12.666621208190918,
      "learning_rate": 8.995139157740784e-06,
      "loss": 1.1887,
      "step": 4919
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 18.414051055908203,
      "learning_rate": 8.994708994708995e-06,
      "loss": 1.4409,
      "step": 4920
    },
    {
      "epoch": 1.905149051490515,
      "grad_norm": 21.924274444580078,
      "learning_rate": 8.994278831677207e-06,
      "loss": 1.6524,
      "step": 4921
    },
    {
      "epoch": 1.905536198219125,
      "grad_norm": 20.565805435180664,
      "learning_rate": 8.993848668645418e-06,
      "loss": 1.9846,
      "step": 4922
    },
    {
      "epoch": 1.9059233449477353,
      "grad_norm": 18.570789337158203,
      "learning_rate": 8.993418505613628e-06,
      "loss": 1.539,
      "step": 4923
    },
    {
      "epoch": 1.9063104916763454,
      "grad_norm": 16.439891815185547,
      "learning_rate": 8.99298834258184e-06,
      "loss": 1.1402,
      "step": 4924
    },
    {
      "epoch": 1.9066976384049554,
      "grad_norm": 21.317459106445312,
      "learning_rate": 8.99255817955005e-06,
      "loss": 1.8219,
      "step": 4925
    },
    {
      "epoch": 1.9070847851335655,
      "grad_norm": 24.63262176513672,
      "learning_rate": 8.99212801651826e-06,
      "loss": 1.0184,
      "step": 4926
    },
    {
      "epoch": 1.9074719318621758,
      "grad_norm": 16.02054214477539,
      "learning_rate": 8.991697853486472e-06,
      "loss": 1.1517,
      "step": 4927
    },
    {
      "epoch": 1.907859078590786,
      "grad_norm": 18.70572280883789,
      "learning_rate": 8.991267690454683e-06,
      "loss": 1.1682,
      "step": 4928
    },
    {
      "epoch": 1.9082462253193961,
      "grad_norm": 25.162269592285156,
      "learning_rate": 8.990837527422895e-06,
      "loss": 1.5876,
      "step": 4929
    },
    {
      "epoch": 1.9086333720480062,
      "grad_norm": 16.48763656616211,
      "learning_rate": 8.990407364391104e-06,
      "loss": 1.9778,
      "step": 4930
    },
    {
      "epoch": 1.9090205187766163,
      "grad_norm": 18.969533920288086,
      "learning_rate": 8.989977201359316e-06,
      "loss": 1.2704,
      "step": 4931
    },
    {
      "epoch": 1.9094076655052263,
      "grad_norm": 41.66298294067383,
      "learning_rate": 8.989547038327527e-06,
      "loss": 1.0822,
      "step": 4932
    },
    {
      "epoch": 1.9097948122338366,
      "grad_norm": 26.027294158935547,
      "learning_rate": 8.989116875295739e-06,
      "loss": 1.5257,
      "step": 4933
    },
    {
      "epoch": 1.910181958962447,
      "grad_norm": 24.02448844909668,
      "learning_rate": 8.988686712263948e-06,
      "loss": 1.8519,
      "step": 4934
    },
    {
      "epoch": 1.910569105691057,
      "grad_norm": 21.22686004638672,
      "learning_rate": 8.98825654923216e-06,
      "loss": 2.1691,
      "step": 4935
    },
    {
      "epoch": 1.910956252419667,
      "grad_norm": 22.871536254882812,
      "learning_rate": 8.987826386200371e-06,
      "loss": 1.3318,
      "step": 4936
    },
    {
      "epoch": 1.9113433991482771,
      "grad_norm": 13.411397933959961,
      "learning_rate": 8.987396223168583e-06,
      "loss": 0.827,
      "step": 4937
    },
    {
      "epoch": 1.9117305458768872,
      "grad_norm": 19.786052703857422,
      "learning_rate": 8.986966060136792e-06,
      "loss": 1.2776,
      "step": 4938
    },
    {
      "epoch": 1.9121176926054975,
      "grad_norm": 14.237548828125,
      "learning_rate": 8.986535897105004e-06,
      "loss": 0.8481,
      "step": 4939
    },
    {
      "epoch": 1.9125048393341078,
      "grad_norm": 25.06519889831543,
      "learning_rate": 8.986105734073215e-06,
      "loss": 1.6934,
      "step": 4940
    },
    {
      "epoch": 1.9128919860627178,
      "grad_norm": 21.115345001220703,
      "learning_rate": 8.985675571041425e-06,
      "loss": 1.3234,
      "step": 4941
    },
    {
      "epoch": 1.9132791327913279,
      "grad_norm": 20.585487365722656,
      "learning_rate": 8.985245408009636e-06,
      "loss": 1.6723,
      "step": 4942
    },
    {
      "epoch": 1.913666279519938,
      "grad_norm": 13.805294036865234,
      "learning_rate": 8.984815244977848e-06,
      "loss": 1.414,
      "step": 4943
    },
    {
      "epoch": 1.9140534262485482,
      "grad_norm": 26.34968376159668,
      "learning_rate": 8.984385081946059e-06,
      "loss": 1.4427,
      "step": 4944
    },
    {
      "epoch": 1.9144405729771583,
      "grad_norm": 27.496482849121094,
      "learning_rate": 8.983954918914269e-06,
      "loss": 1.2428,
      "step": 4945
    },
    {
      "epoch": 1.9148277197057686,
      "grad_norm": 27.63329315185547,
      "learning_rate": 8.98352475588248e-06,
      "loss": 1.4792,
      "step": 4946
    },
    {
      "epoch": 1.9152148664343787,
      "grad_norm": 17.35972785949707,
      "learning_rate": 8.983094592850692e-06,
      "loss": 0.82,
      "step": 4947
    },
    {
      "epoch": 1.9156020131629887,
      "grad_norm": 23.872060775756836,
      "learning_rate": 8.982664429818903e-06,
      "loss": 1.7518,
      "step": 4948
    },
    {
      "epoch": 1.9159891598915988,
      "grad_norm": 32.231502532958984,
      "learning_rate": 8.982234266787113e-06,
      "loss": 2.7638,
      "step": 4949
    },
    {
      "epoch": 1.916376306620209,
      "grad_norm": 14.199078559875488,
      "learning_rate": 8.981804103755324e-06,
      "loss": 0.8954,
      "step": 4950
    },
    {
      "epoch": 1.9167634533488194,
      "grad_norm": 24.528013229370117,
      "learning_rate": 8.981373940723534e-06,
      "loss": 1.3217,
      "step": 4951
    },
    {
      "epoch": 1.9171506000774294,
      "grad_norm": 26.732643127441406,
      "learning_rate": 8.980943777691747e-06,
      "loss": 1.9691,
      "step": 4952
    },
    {
      "epoch": 1.9175377468060395,
      "grad_norm": 15.953629493713379,
      "learning_rate": 8.980513614659957e-06,
      "loss": 1.3936,
      "step": 4953
    },
    {
      "epoch": 1.9179248935346496,
      "grad_norm": 8.628576278686523,
      "learning_rate": 8.980083451628168e-06,
      "loss": 0.4089,
      "step": 4954
    },
    {
      "epoch": 1.9183120402632596,
      "grad_norm": 26.416601181030273,
      "learning_rate": 8.979653288596378e-06,
      "loss": 1.6096,
      "step": 4955
    },
    {
      "epoch": 1.91869918699187,
      "grad_norm": 21.26247787475586,
      "learning_rate": 8.97922312556459e-06,
      "loss": 2.5864,
      "step": 4956
    },
    {
      "epoch": 1.9190863337204802,
      "grad_norm": 23.64114761352539,
      "learning_rate": 8.9787929625328e-06,
      "loss": 1.4178,
      "step": 4957
    },
    {
      "epoch": 1.9194734804490903,
      "grad_norm": 16.936702728271484,
      "learning_rate": 8.978362799501012e-06,
      "loss": 1.7217,
      "step": 4958
    },
    {
      "epoch": 1.9198606271777003,
      "grad_norm": 25.5001163482666,
      "learning_rate": 8.977932636469222e-06,
      "loss": 1.9446,
      "step": 4959
    },
    {
      "epoch": 1.9202477739063104,
      "grad_norm": 29.822956085205078,
      "learning_rate": 8.977502473437433e-06,
      "loss": 1.6047,
      "step": 4960
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 34.72706985473633,
      "learning_rate": 8.977072310405645e-06,
      "loss": 1.7574,
      "step": 4961
    },
    {
      "epoch": 1.9210220673635308,
      "grad_norm": 8.17906379699707,
      "learning_rate": 8.976642147373854e-06,
      "loss": 0.5461,
      "step": 4962
    },
    {
      "epoch": 1.921409214092141,
      "grad_norm": 11.645334243774414,
      "learning_rate": 8.976211984342066e-06,
      "loss": 0.5216,
      "step": 4963
    },
    {
      "epoch": 1.9217963608207511,
      "grad_norm": 41.04625701904297,
      "learning_rate": 8.975781821310277e-06,
      "loss": 2.5799,
      "step": 4964
    },
    {
      "epoch": 1.9221835075493612,
      "grad_norm": 18.667362213134766,
      "learning_rate": 8.975351658278489e-06,
      "loss": 1.3428,
      "step": 4965
    },
    {
      "epoch": 1.9225706542779712,
      "grad_norm": 14.303143501281738,
      "learning_rate": 8.974921495246698e-06,
      "loss": 0.9794,
      "step": 4966
    },
    {
      "epoch": 1.9229578010065815,
      "grad_norm": 12.849405288696289,
      "learning_rate": 8.97449133221491e-06,
      "loss": 0.8764,
      "step": 4967
    },
    {
      "epoch": 1.9233449477351916,
      "grad_norm": 22.610260009765625,
      "learning_rate": 8.974061169183121e-06,
      "loss": 1.1984,
      "step": 4968
    },
    {
      "epoch": 1.923732094463802,
      "grad_norm": 23.663394927978516,
      "learning_rate": 8.973631006151333e-06,
      "loss": 2.002,
      "step": 4969
    },
    {
      "epoch": 1.924119241192412,
      "grad_norm": 15.697454452514648,
      "learning_rate": 8.973200843119542e-06,
      "loss": 0.896,
      "step": 4970
    },
    {
      "epoch": 1.924506387921022,
      "grad_norm": 14.352622032165527,
      "learning_rate": 8.972770680087754e-06,
      "loss": 1.3712,
      "step": 4971
    },
    {
      "epoch": 1.924893534649632,
      "grad_norm": 20.42909812927246,
      "learning_rate": 8.972340517055965e-06,
      "loss": 1.3822,
      "step": 4972
    },
    {
      "epoch": 1.9252806813782424,
      "grad_norm": 17.873180389404297,
      "learning_rate": 8.971910354024177e-06,
      "loss": 1.7086,
      "step": 4973
    },
    {
      "epoch": 1.9256678281068524,
      "grad_norm": 23.521024703979492,
      "learning_rate": 8.971480190992386e-06,
      "loss": 1.163,
      "step": 4974
    },
    {
      "epoch": 1.9260549748354627,
      "grad_norm": 17.26421356201172,
      "learning_rate": 8.971050027960598e-06,
      "loss": 1.5236,
      "step": 4975
    },
    {
      "epoch": 1.9264421215640728,
      "grad_norm": 14.830324172973633,
      "learning_rate": 8.970619864928809e-06,
      "loss": 1.1913,
      "step": 4976
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 21.70372200012207,
      "learning_rate": 8.970189701897019e-06,
      "loss": 1.5759,
      "step": 4977
    },
    {
      "epoch": 1.927216415021293,
      "grad_norm": 22.018672943115234,
      "learning_rate": 8.96975953886523e-06,
      "loss": 1.7433,
      "step": 4978
    },
    {
      "epoch": 1.9276035617499032,
      "grad_norm": 42.068206787109375,
      "learning_rate": 8.969329375833442e-06,
      "loss": 1.4337,
      "step": 4979
    },
    {
      "epoch": 1.9279907084785135,
      "grad_norm": 19.72142791748047,
      "learning_rate": 8.968899212801653e-06,
      "loss": 1.624,
      "step": 4980
    },
    {
      "epoch": 1.9283778552071236,
      "grad_norm": 25.839208602905273,
      "learning_rate": 8.968469049769863e-06,
      "loss": 1.4811,
      "step": 4981
    },
    {
      "epoch": 1.9287650019357336,
      "grad_norm": 14.108383178710938,
      "learning_rate": 8.968038886738074e-06,
      "loss": 1.0157,
      "step": 4982
    },
    {
      "epoch": 1.9291521486643437,
      "grad_norm": 15.001276969909668,
      "learning_rate": 8.967608723706286e-06,
      "loss": 0.5326,
      "step": 4983
    },
    {
      "epoch": 1.9295392953929538,
      "grad_norm": 30.3634033203125,
      "learning_rate": 8.967178560674497e-06,
      "loss": 2.4995,
      "step": 4984
    },
    {
      "epoch": 1.929926442121564,
      "grad_norm": 20.692209243774414,
      "learning_rate": 8.966748397642707e-06,
      "loss": 1.7759,
      "step": 4985
    },
    {
      "epoch": 1.9303135888501743,
      "grad_norm": 16.235151290893555,
      "learning_rate": 8.966318234610918e-06,
      "loss": 1.3394,
      "step": 4986
    },
    {
      "epoch": 1.9307007355787844,
      "grad_norm": 20.598243713378906,
      "learning_rate": 8.96588807157913e-06,
      "loss": 1.7926,
      "step": 4987
    },
    {
      "epoch": 1.9310878823073945,
      "grad_norm": 23.412961959838867,
      "learning_rate": 8.965457908547341e-06,
      "loss": 1.8497,
      "step": 4988
    },
    {
      "epoch": 1.9314750290360045,
      "grad_norm": 29.564186096191406,
      "learning_rate": 8.96502774551555e-06,
      "loss": 1.5428,
      "step": 4989
    },
    {
      "epoch": 1.9318621757646148,
      "grad_norm": 21.28982925415039,
      "learning_rate": 8.964597582483762e-06,
      "loss": 1.3082,
      "step": 4990
    },
    {
      "epoch": 1.932249322493225,
      "grad_norm": 24.595672607421875,
      "learning_rate": 8.964167419451974e-06,
      "loss": 1.7253,
      "step": 4991
    },
    {
      "epoch": 1.9326364692218352,
      "grad_norm": 14.839834213256836,
      "learning_rate": 8.963737256420183e-06,
      "loss": 0.9543,
      "step": 4992
    },
    {
      "epoch": 1.9330236159504453,
      "grad_norm": 22.162818908691406,
      "learning_rate": 8.963307093388395e-06,
      "loss": 1.7232,
      "step": 4993
    },
    {
      "epoch": 1.9334107626790553,
      "grad_norm": 19.117101669311523,
      "learning_rate": 8.962876930356606e-06,
      "loss": 1.8885,
      "step": 4994
    },
    {
      "epoch": 1.9337979094076654,
      "grad_norm": 19.931550979614258,
      "learning_rate": 8.962446767324818e-06,
      "loss": 1.6844,
      "step": 4995
    },
    {
      "epoch": 1.9341850561362757,
      "grad_norm": 29.34485626220703,
      "learning_rate": 8.962016604293027e-06,
      "loss": 1.6281,
      "step": 4996
    },
    {
      "epoch": 1.9345722028648857,
      "grad_norm": 18.886865615844727,
      "learning_rate": 8.961586441261239e-06,
      "loss": 1.3513,
      "step": 4997
    },
    {
      "epoch": 1.934959349593496,
      "grad_norm": 16.671855926513672,
      "learning_rate": 8.961156278229448e-06,
      "loss": 1.3599,
      "step": 4998
    },
    {
      "epoch": 1.935346496322106,
      "grad_norm": 13.790915489196777,
      "learning_rate": 8.960726115197661e-06,
      "loss": 0.7882,
      "step": 4999
    },
    {
      "epoch": 1.9357336430507162,
      "grad_norm": 18.460559844970703,
      "learning_rate": 8.960295952165871e-06,
      "loss": 1.6672,
      "step": 5000
    },
    {
      "epoch": 1.9361207897793262,
      "grad_norm": 23.79583168029785,
      "learning_rate": 8.959865789134083e-06,
      "loss": 1.7994,
      "step": 5001
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 18.61696434020996,
      "learning_rate": 8.959435626102292e-06,
      "loss": 0.9771,
      "step": 5002
    },
    {
      "epoch": 1.9368950832365468,
      "grad_norm": 13.473126411437988,
      "learning_rate": 8.959005463070505e-06,
      "loss": 0.5516,
      "step": 5003
    },
    {
      "epoch": 1.9372822299651569,
      "grad_norm": 20.256925582885742,
      "learning_rate": 8.958575300038715e-06,
      "loss": 1.5957,
      "step": 5004
    },
    {
      "epoch": 1.937669376693767,
      "grad_norm": 18.566518783569336,
      "learning_rate": 8.958145137006927e-06,
      "loss": 1.5907,
      "step": 5005
    },
    {
      "epoch": 1.938056523422377,
      "grad_norm": 22.403587341308594,
      "learning_rate": 8.957714973975136e-06,
      "loss": 1.6719,
      "step": 5006
    },
    {
      "epoch": 1.938443670150987,
      "grad_norm": 18.056713104248047,
      "learning_rate": 8.957284810943348e-06,
      "loss": 1.3833,
      "step": 5007
    },
    {
      "epoch": 1.9388308168795974,
      "grad_norm": 28.40604019165039,
      "learning_rate": 8.956854647911559e-06,
      "loss": 1.6835,
      "step": 5008
    },
    {
      "epoch": 1.9392179636082076,
      "grad_norm": 17.384031295776367,
      "learning_rate": 8.95642448487977e-06,
      "loss": 1.355,
      "step": 5009
    },
    {
      "epoch": 1.9396051103368177,
      "grad_norm": 25.678709030151367,
      "learning_rate": 8.95599432184798e-06,
      "loss": 0.9626,
      "step": 5010
    },
    {
      "epoch": 1.9399922570654278,
      "grad_norm": 28.737470626831055,
      "learning_rate": 8.955564158816192e-06,
      "loss": 1.5817,
      "step": 5011
    },
    {
      "epoch": 1.9403794037940378,
      "grad_norm": 13.74497127532959,
      "learning_rate": 8.955133995784403e-06,
      "loss": 1.0506,
      "step": 5012
    },
    {
      "epoch": 1.9407665505226481,
      "grad_norm": 15.772296905517578,
      "learning_rate": 8.954703832752613e-06,
      "loss": 1.0893,
      "step": 5013
    },
    {
      "epoch": 1.9411536972512582,
      "grad_norm": 27.078197479248047,
      "learning_rate": 8.954273669720826e-06,
      "loss": 2.8366,
      "step": 5014
    },
    {
      "epoch": 1.9415408439798685,
      "grad_norm": 36.06413650512695,
      "learning_rate": 8.953843506689036e-06,
      "loss": 2.0108,
      "step": 5015
    },
    {
      "epoch": 1.9419279907084785,
      "grad_norm": 23.48630714416504,
      "learning_rate": 8.953413343657247e-06,
      "loss": 2.476,
      "step": 5016
    },
    {
      "epoch": 1.9423151374370886,
      "grad_norm": 18.036592483520508,
      "learning_rate": 8.952983180625457e-06,
      "loss": 1.3987,
      "step": 5017
    },
    {
      "epoch": 1.9427022841656987,
      "grad_norm": 34.800724029541016,
      "learning_rate": 8.95255301759367e-06,
      "loss": 1.3232,
      "step": 5018
    },
    {
      "epoch": 1.943089430894309,
      "grad_norm": 13.481511116027832,
      "learning_rate": 8.95212285456188e-06,
      "loss": 0.8536,
      "step": 5019
    },
    {
      "epoch": 1.943476577622919,
      "grad_norm": 16.697940826416016,
      "learning_rate": 8.951692691530091e-06,
      "loss": 1.3831,
      "step": 5020
    },
    {
      "epoch": 1.9438637243515293,
      "grad_norm": 12.888999938964844,
      "learning_rate": 8.9512625284983e-06,
      "loss": 1.0941,
      "step": 5021
    },
    {
      "epoch": 1.9442508710801394,
      "grad_norm": 16.493701934814453,
      "learning_rate": 8.950832365466512e-06,
      "loss": 1.6612,
      "step": 5022
    },
    {
      "epoch": 1.9446380178087495,
      "grad_norm": 25.98165512084961,
      "learning_rate": 8.950402202434724e-06,
      "loss": 2.028,
      "step": 5023
    },
    {
      "epoch": 1.9450251645373595,
      "grad_norm": 19.496999740600586,
      "learning_rate": 8.949972039402935e-06,
      "loss": 0.3908,
      "step": 5024
    },
    {
      "epoch": 1.9454123112659698,
      "grad_norm": 14.68396282196045,
      "learning_rate": 8.949541876371145e-06,
      "loss": 0.9793,
      "step": 5025
    },
    {
      "epoch": 1.94579945799458,
      "grad_norm": 39.87046813964844,
      "learning_rate": 8.949111713339356e-06,
      "loss": 2.8255,
      "step": 5026
    },
    {
      "epoch": 1.9461866047231902,
      "grad_norm": 30.765535354614258,
      "learning_rate": 8.948681550307568e-06,
      "loss": 1.0577,
      "step": 5027
    },
    {
      "epoch": 1.9465737514518002,
      "grad_norm": 39.40552520751953,
      "learning_rate": 8.948251387275777e-06,
      "loss": 1.4707,
      "step": 5028
    },
    {
      "epoch": 1.9469608981804103,
      "grad_norm": 15.692784309387207,
      "learning_rate": 8.947821224243989e-06,
      "loss": 1.4032,
      "step": 5029
    },
    {
      "epoch": 1.9473480449090204,
      "grad_norm": 47.64657974243164,
      "learning_rate": 8.9473910612122e-06,
      "loss": 1.3878,
      "step": 5030
    },
    {
      "epoch": 1.9477351916376306,
      "grad_norm": 14.196982383728027,
      "learning_rate": 8.946960898180412e-06,
      "loss": 1.3315,
      "step": 5031
    },
    {
      "epoch": 1.948122338366241,
      "grad_norm": 12.911508560180664,
      "learning_rate": 8.946530735148621e-06,
      "loss": 0.8248,
      "step": 5032
    },
    {
      "epoch": 1.948509485094851,
      "grad_norm": 9.948476791381836,
      "learning_rate": 8.946100572116833e-06,
      "loss": 0.4315,
      "step": 5033
    },
    {
      "epoch": 1.948896631823461,
      "grad_norm": 30.982824325561523,
      "learning_rate": 8.945670409085044e-06,
      "loss": 2.4275,
      "step": 5034
    },
    {
      "epoch": 1.9492837785520711,
      "grad_norm": 13.27056884765625,
      "learning_rate": 8.945240246053256e-06,
      "loss": 1.3572,
      "step": 5035
    },
    {
      "epoch": 1.9496709252806814,
      "grad_norm": 20.373085021972656,
      "learning_rate": 8.944810083021465e-06,
      "loss": 3.6855,
      "step": 5036
    },
    {
      "epoch": 1.9500580720092915,
      "grad_norm": 18.303537368774414,
      "learning_rate": 8.944379919989677e-06,
      "loss": 1.5146,
      "step": 5037
    },
    {
      "epoch": 1.9504452187379018,
      "grad_norm": 12.003211975097656,
      "learning_rate": 8.943949756957888e-06,
      "loss": 1.088,
      "step": 5038
    },
    {
      "epoch": 1.9508323654665118,
      "grad_norm": 28.192487716674805,
      "learning_rate": 8.9435195939261e-06,
      "loss": 2.0845,
      "step": 5039
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 30.38310432434082,
      "learning_rate": 8.94308943089431e-06,
      "loss": 1.9182,
      "step": 5040
    },
    {
      "epoch": 1.951606658923732,
      "grad_norm": 22.06005096435547,
      "learning_rate": 8.94265926786252e-06,
      "loss": 2.6774,
      "step": 5041
    },
    {
      "epoch": 1.9519938056523423,
      "grad_norm": 20.07892608642578,
      "learning_rate": 8.942229104830732e-06,
      "loss": 2.0473,
      "step": 5042
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 12.130998611450195,
      "learning_rate": 8.941798941798942e-06,
      "loss": 1.14,
      "step": 5043
    },
    {
      "epoch": 1.9527680991095626,
      "grad_norm": 34.77061462402344,
      "learning_rate": 8.941368778767153e-06,
      "loss": 1.8194,
      "step": 5044
    },
    {
      "epoch": 1.9531552458381727,
      "grad_norm": 43.321372985839844,
      "learning_rate": 8.940938615735365e-06,
      "loss": 1.2439,
      "step": 5045
    },
    {
      "epoch": 1.9535423925667827,
      "grad_norm": 30.865442276000977,
      "learning_rate": 8.940508452703576e-06,
      "loss": 1.3808,
      "step": 5046
    },
    {
      "epoch": 1.9539295392953928,
      "grad_norm": 40.06325149536133,
      "learning_rate": 8.940078289671786e-06,
      "loss": 2.1459,
      "step": 5047
    },
    {
      "epoch": 1.954316686024003,
      "grad_norm": 22.136259078979492,
      "learning_rate": 8.939648126639997e-06,
      "loss": 1.33,
      "step": 5048
    },
    {
      "epoch": 1.9547038327526134,
      "grad_norm": 16.23042869567871,
      "learning_rate": 8.939217963608207e-06,
      "loss": 1.6541,
      "step": 5049
    },
    {
      "epoch": 1.9550909794812235,
      "grad_norm": 16.286090850830078,
      "learning_rate": 8.93878780057642e-06,
      "loss": 1.5186,
      "step": 5050
    },
    {
      "epoch": 1.9554781262098335,
      "grad_norm": 36.57447052001953,
      "learning_rate": 8.93835763754463e-06,
      "loss": 1.5664,
      "step": 5051
    },
    {
      "epoch": 1.9558652729384436,
      "grad_norm": 32.15333557128906,
      "learning_rate": 8.937927474512841e-06,
      "loss": 4.6609,
      "step": 5052
    },
    {
      "epoch": 1.9562524196670537,
      "grad_norm": 16.023290634155273,
      "learning_rate": 8.937497311481053e-06,
      "loss": 1.5514,
      "step": 5053
    },
    {
      "epoch": 1.956639566395664,
      "grad_norm": 13.492148399353027,
      "learning_rate": 8.937067148449264e-06,
      "loss": 0.7571,
      "step": 5054
    },
    {
      "epoch": 1.9570267131242742,
      "grad_norm": 19.243133544921875,
      "learning_rate": 8.936636985417474e-06,
      "loss": 1.6833,
      "step": 5055
    },
    {
      "epoch": 1.9574138598528843,
      "grad_norm": 15.968663215637207,
      "learning_rate": 8.936206822385685e-06,
      "loss": 1.066,
      "step": 5056
    },
    {
      "epoch": 1.9578010065814944,
      "grad_norm": 11.537787437438965,
      "learning_rate": 8.935776659353896e-06,
      "loss": 0.6234,
      "step": 5057
    },
    {
      "epoch": 1.9581881533101044,
      "grad_norm": 58.885562896728516,
      "learning_rate": 8.935346496322106e-06,
      "loss": 1.5358,
      "step": 5058
    },
    {
      "epoch": 1.9585753000387147,
      "grad_norm": 19.7783145904541,
      "learning_rate": 8.934916333290318e-06,
      "loss": 2.4503,
      "step": 5059
    },
    {
      "epoch": 1.9589624467673248,
      "grad_norm": 16.37778091430664,
      "learning_rate": 8.934486170258529e-06,
      "loss": 1.5457,
      "step": 5060
    },
    {
      "epoch": 1.959349593495935,
      "grad_norm": 26.414962768554688,
      "learning_rate": 8.93405600722674e-06,
      "loss": 1.5584,
      "step": 5061
    },
    {
      "epoch": 1.9597367402245451,
      "grad_norm": 22.50078010559082,
      "learning_rate": 8.93362584419495e-06,
      "loss": 2.0213,
      "step": 5062
    },
    {
      "epoch": 1.9601238869531552,
      "grad_norm": 42.97148513793945,
      "learning_rate": 8.933195681163162e-06,
      "loss": 1.4458,
      "step": 5063
    },
    {
      "epoch": 1.9605110336817653,
      "grad_norm": 28.216140747070312,
      "learning_rate": 8.932765518131371e-06,
      "loss": 1.4513,
      "step": 5064
    },
    {
      "epoch": 1.9608981804103756,
      "grad_norm": 15.856345176696777,
      "learning_rate": 8.932335355099584e-06,
      "loss": 1.1619,
      "step": 5065
    },
    {
      "epoch": 1.9612853271389856,
      "grad_norm": 20.520109176635742,
      "learning_rate": 8.931905192067794e-06,
      "loss": 1.4508,
      "step": 5066
    },
    {
      "epoch": 1.961672473867596,
      "grad_norm": 10.177332878112793,
      "learning_rate": 8.931475029036006e-06,
      "loss": 1.1992,
      "step": 5067
    },
    {
      "epoch": 1.962059620596206,
      "grad_norm": 28.254623413085938,
      "learning_rate": 8.931044866004215e-06,
      "loss": 1.6819,
      "step": 5068
    },
    {
      "epoch": 1.962446767324816,
      "grad_norm": 30.59616470336914,
      "learning_rate": 8.930614702972428e-06,
      "loss": 1.7784,
      "step": 5069
    },
    {
      "epoch": 1.962833914053426,
      "grad_norm": 14.747424125671387,
      "learning_rate": 8.930184539940638e-06,
      "loss": 0.8766,
      "step": 5070
    },
    {
      "epoch": 1.9632210607820364,
      "grad_norm": 19.92981719970703,
      "learning_rate": 8.92975437690885e-06,
      "loss": 1.2762,
      "step": 5071
    },
    {
      "epoch": 1.9636082075106467,
      "grad_norm": 16.607160568237305,
      "learning_rate": 8.92932421387706e-06,
      "loss": 1.3156,
      "step": 5072
    },
    {
      "epoch": 1.9639953542392568,
      "grad_norm": 25.851394653320312,
      "learning_rate": 8.92889405084527e-06,
      "loss": 1.3573,
      "step": 5073
    },
    {
      "epoch": 1.9643825009678668,
      "grad_norm": 21.419614791870117,
      "learning_rate": 8.928463887813482e-06,
      "loss": 1.7765,
      "step": 5074
    },
    {
      "epoch": 1.9647696476964769,
      "grad_norm": 18.649211883544922,
      "learning_rate": 8.928033724781694e-06,
      "loss": 1.3704,
      "step": 5075
    },
    {
      "epoch": 1.965156794425087,
      "grad_norm": 26.410093307495117,
      "learning_rate": 8.927603561749903e-06,
      "loss": 1.4014,
      "step": 5076
    },
    {
      "epoch": 1.9655439411536972,
      "grad_norm": 66.94514465332031,
      "learning_rate": 8.927173398718115e-06,
      "loss": 2.7198,
      "step": 5077
    },
    {
      "epoch": 1.9659310878823075,
      "grad_norm": 16.62771224975586,
      "learning_rate": 8.926743235686326e-06,
      "loss": 1.4551,
      "step": 5078
    },
    {
      "epoch": 1.9663182346109176,
      "grad_norm": 33.97562789916992,
      "learning_rate": 8.926313072654536e-06,
      "loss": 1.5145,
      "step": 5079
    },
    {
      "epoch": 1.9667053813395277,
      "grad_norm": 20.423072814941406,
      "learning_rate": 8.925882909622747e-06,
      "loss": 1.6092,
      "step": 5080
    },
    {
      "epoch": 1.9670925280681377,
      "grad_norm": 16.590843200683594,
      "learning_rate": 8.925452746590959e-06,
      "loss": 1.6157,
      "step": 5081
    },
    {
      "epoch": 1.967479674796748,
      "grad_norm": 13.103160858154297,
      "learning_rate": 8.92502258355917e-06,
      "loss": 0.8734,
      "step": 5082
    },
    {
      "epoch": 1.967866821525358,
      "grad_norm": 30.16637420654297,
      "learning_rate": 8.92459242052738e-06,
      "loss": 2.2792,
      "step": 5083
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 29.49611473083496,
      "learning_rate": 8.924162257495591e-06,
      "loss": 1.5791,
      "step": 5084
    },
    {
      "epoch": 1.9686411149825784,
      "grad_norm": 28.307647705078125,
      "learning_rate": 8.923732094463803e-06,
      "loss": 2.0614,
      "step": 5085
    },
    {
      "epoch": 1.9690282617111885,
      "grad_norm": 19.164813995361328,
      "learning_rate": 8.923301931432014e-06,
      "loss": 1.3219,
      "step": 5086
    },
    {
      "epoch": 1.9694154084397986,
      "grad_norm": 12.012247085571289,
      "learning_rate": 8.922871768400224e-06,
      "loss": 0.6395,
      "step": 5087
    },
    {
      "epoch": 1.9698025551684089,
      "grad_norm": 21.6662540435791,
      "learning_rate": 8.922441605368435e-06,
      "loss": 1.4938,
      "step": 5088
    },
    {
      "epoch": 1.970189701897019,
      "grad_norm": 14.841596603393555,
      "learning_rate": 8.922011442336647e-06,
      "loss": 0.8594,
      "step": 5089
    },
    {
      "epoch": 1.9705768486256292,
      "grad_norm": 24.2488956451416,
      "learning_rate": 8.921581279304858e-06,
      "loss": 1.5893,
      "step": 5090
    },
    {
      "epoch": 1.9709639953542393,
      "grad_norm": 24.07193946838379,
      "learning_rate": 8.921151116273068e-06,
      "loss": 1.7391,
      "step": 5091
    },
    {
      "epoch": 1.9713511420828493,
      "grad_norm": 14.020094871520996,
      "learning_rate": 8.920720953241279e-06,
      "loss": 1.2958,
      "step": 5092
    },
    {
      "epoch": 1.9717382888114594,
      "grad_norm": 21.800079345703125,
      "learning_rate": 8.92029079020949e-06,
      "loss": 1.5865,
      "step": 5093
    },
    {
      "epoch": 1.9721254355400697,
      "grad_norm": 17.113872528076172,
      "learning_rate": 8.9198606271777e-06,
      "loss": 1.5815,
      "step": 5094
    },
    {
      "epoch": 1.97251258226868,
      "grad_norm": 32.22504806518555,
      "learning_rate": 8.919430464145912e-06,
      "loss": 1.7309,
      "step": 5095
    },
    {
      "epoch": 1.97289972899729,
      "grad_norm": 22.034461975097656,
      "learning_rate": 8.919000301114123e-06,
      "loss": 2.0597,
      "step": 5096
    },
    {
      "epoch": 1.9732868757259001,
      "grad_norm": 15.282878875732422,
      "learning_rate": 8.918570138082334e-06,
      "loss": 1.0442,
      "step": 5097
    },
    {
      "epoch": 1.9736740224545102,
      "grad_norm": 24.93488311767578,
      "learning_rate": 8.918139975050544e-06,
      "loss": 1.3731,
      "step": 5098
    },
    {
      "epoch": 1.9740611691831202,
      "grad_norm": 23.699443817138672,
      "learning_rate": 8.917709812018756e-06,
      "loss": 1.5669,
      "step": 5099
    },
    {
      "epoch": 1.9744483159117305,
      "grad_norm": 16.005857467651367,
      "learning_rate": 8.917279648986967e-06,
      "loss": 1.1406,
      "step": 5100
    },
    {
      "epoch": 1.9748354626403408,
      "grad_norm": 17.878875732421875,
      "learning_rate": 8.916849485955178e-06,
      "loss": 1.6122,
      "step": 5101
    },
    {
      "epoch": 1.9752226093689509,
      "grad_norm": 16.089630126953125,
      "learning_rate": 8.916419322923388e-06,
      "loss": 1.7255,
      "step": 5102
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 20.468366622924805,
      "learning_rate": 8.9159891598916e-06,
      "loss": 1.4316,
      "step": 5103
    },
    {
      "epoch": 1.975996902826171,
      "grad_norm": 17.24050521850586,
      "learning_rate": 8.915558996859811e-06,
      "loss": 1.0907,
      "step": 5104
    },
    {
      "epoch": 1.9763840495547813,
      "grad_norm": 25.300413131713867,
      "learning_rate": 8.915128833828022e-06,
      "loss": 1.7965,
      "step": 5105
    },
    {
      "epoch": 1.9767711962833914,
      "grad_norm": 20.702165603637695,
      "learning_rate": 8.914698670796232e-06,
      "loss": 0.8733,
      "step": 5106
    },
    {
      "epoch": 1.9771583430120017,
      "grad_norm": 20.634004592895508,
      "learning_rate": 8.914268507764444e-06,
      "loss": 1.8152,
      "step": 5107
    },
    {
      "epoch": 1.9775454897406117,
      "grad_norm": 22.72633934020996,
      "learning_rate": 8.913838344732655e-06,
      "loss": 1.6052,
      "step": 5108
    },
    {
      "epoch": 1.9779326364692218,
      "grad_norm": 26.17214584350586,
      "learning_rate": 8.913408181700865e-06,
      "loss": 1.4701,
      "step": 5109
    },
    {
      "epoch": 1.9783197831978319,
      "grad_norm": 20.926300048828125,
      "learning_rate": 8.912978018669076e-06,
      "loss": 1.139,
      "step": 5110
    },
    {
      "epoch": 1.9787069299264421,
      "grad_norm": 22.57765769958496,
      "learning_rate": 8.912547855637288e-06,
      "loss": 1.8059,
      "step": 5111
    },
    {
      "epoch": 1.9790940766550522,
      "grad_norm": 26.34497833251953,
      "learning_rate": 8.912117692605499e-06,
      "loss": 1.1736,
      "step": 5112
    },
    {
      "epoch": 1.9794812233836625,
      "grad_norm": 25.313993453979492,
      "learning_rate": 8.911687529573709e-06,
      "loss": 1.5926,
      "step": 5113
    },
    {
      "epoch": 1.9798683701122726,
      "grad_norm": 28.109262466430664,
      "learning_rate": 8.91125736654192e-06,
      "loss": 2.5271,
      "step": 5114
    },
    {
      "epoch": 1.9802555168408826,
      "grad_norm": 22.546096801757812,
      "learning_rate": 8.91082720351013e-06,
      "loss": 1.0622,
      "step": 5115
    },
    {
      "epoch": 1.9806426635694927,
      "grad_norm": 14.79718017578125,
      "learning_rate": 8.910397040478343e-06,
      "loss": 1.2346,
      "step": 5116
    },
    {
      "epoch": 1.981029810298103,
      "grad_norm": 16.067081451416016,
      "learning_rate": 8.909966877446553e-06,
      "loss": 1.3985,
      "step": 5117
    },
    {
      "epoch": 1.9814169570267133,
      "grad_norm": 23.512046813964844,
      "learning_rate": 8.909536714414764e-06,
      "loss": 1.4701,
      "step": 5118
    },
    {
      "epoch": 1.9818041037553233,
      "grad_norm": 24.276819229125977,
      "learning_rate": 8.909106551382974e-06,
      "loss": 1.7406,
      "step": 5119
    },
    {
      "epoch": 1.9821912504839334,
      "grad_norm": 20.780975341796875,
      "learning_rate": 8.908676388351187e-06,
      "loss": 1.8246,
      "step": 5120
    },
    {
      "epoch": 1.9825783972125435,
      "grad_norm": 15.267280578613281,
      "learning_rate": 8.908246225319397e-06,
      "loss": 0.9166,
      "step": 5121
    },
    {
      "epoch": 1.9829655439411535,
      "grad_norm": 11.298053741455078,
      "learning_rate": 8.907816062287608e-06,
      "loss": 0.9897,
      "step": 5122
    },
    {
      "epoch": 1.9833526906697638,
      "grad_norm": 16.452783584594727,
      "learning_rate": 8.907385899255818e-06,
      "loss": 1.2999,
      "step": 5123
    },
    {
      "epoch": 1.9837398373983741,
      "grad_norm": 13.438739776611328,
      "learning_rate": 8.90695573622403e-06,
      "loss": 0.7316,
      "step": 5124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 8.873785972595215,
      "learning_rate": 8.90652557319224e-06,
      "loss": 1.2314,
      "step": 5125
    },
    {
      "epoch": 1.9845141308555942,
      "grad_norm": 23.566085815429688,
      "learning_rate": 8.906095410160452e-06,
      "loss": 1.5802,
      "step": 5126
    },
    {
      "epoch": 1.9849012775842043,
      "grad_norm": 18.50239372253418,
      "learning_rate": 8.905665247128662e-06,
      "loss": 1.4084,
      "step": 5127
    },
    {
      "epoch": 1.9852884243128146,
      "grad_norm": 26.04203987121582,
      "learning_rate": 8.905235084096873e-06,
      "loss": 1.5448,
      "step": 5128
    },
    {
      "epoch": 1.9856755710414247,
      "grad_norm": 17.22743034362793,
      "learning_rate": 8.904804921065085e-06,
      "loss": 3.1625,
      "step": 5129
    },
    {
      "epoch": 1.986062717770035,
      "grad_norm": 15.887223243713379,
      "learning_rate": 8.904374758033294e-06,
      "loss": 1.3405,
      "step": 5130
    },
    {
      "epoch": 1.986449864498645,
      "grad_norm": 13.759778022766113,
      "learning_rate": 8.903944595001506e-06,
      "loss": 0.8945,
      "step": 5131
    },
    {
      "epoch": 1.986837011227255,
      "grad_norm": 11.975772857666016,
      "learning_rate": 8.903514431969717e-06,
      "loss": 2.1105,
      "step": 5132
    },
    {
      "epoch": 1.9872241579558652,
      "grad_norm": 12.834270477294922,
      "learning_rate": 8.903084268937929e-06,
      "loss": 0.7792,
      "step": 5133
    },
    {
      "epoch": 1.9876113046844754,
      "grad_norm": 26.31777000427246,
      "learning_rate": 8.902654105906138e-06,
      "loss": 0.896,
      "step": 5134
    },
    {
      "epoch": 1.9879984514130855,
      "grad_norm": 17.266155242919922,
      "learning_rate": 8.902223942874351e-06,
      "loss": 1.2382,
      "step": 5135
    },
    {
      "epoch": 1.9883855981416958,
      "grad_norm": 25.256160736083984,
      "learning_rate": 8.901793779842561e-06,
      "loss": 1.0057,
      "step": 5136
    },
    {
      "epoch": 1.9887727448703059,
      "grad_norm": 16.617509841918945,
      "learning_rate": 8.901363616810772e-06,
      "loss": 1.5818,
      "step": 5137
    },
    {
      "epoch": 1.989159891598916,
      "grad_norm": 25.78107452392578,
      "learning_rate": 8.900933453778982e-06,
      "loss": 1.4601,
      "step": 5138
    },
    {
      "epoch": 1.989547038327526,
      "grad_norm": 16.603744506835938,
      "learning_rate": 8.900503290747194e-06,
      "loss": 1.503,
      "step": 5139
    },
    {
      "epoch": 1.9899341850561363,
      "grad_norm": 20.366905212402344,
      "learning_rate": 8.900073127715405e-06,
      "loss": 1.5788,
      "step": 5140
    },
    {
      "epoch": 1.9903213317847466,
      "grad_norm": 37.17045593261719,
      "learning_rate": 8.899642964683616e-06,
      "loss": 2.9594,
      "step": 5141
    },
    {
      "epoch": 1.9907084785133566,
      "grad_norm": 35.04768371582031,
      "learning_rate": 8.899212801651826e-06,
      "loss": 1.9838,
      "step": 5142
    },
    {
      "epoch": 1.9910956252419667,
      "grad_norm": 22.63867950439453,
      "learning_rate": 8.898782638620038e-06,
      "loss": 1.6409,
      "step": 5143
    },
    {
      "epoch": 1.9914827719705768,
      "grad_norm": 26.467220306396484,
      "learning_rate": 8.898352475588249e-06,
      "loss": 2.0948,
      "step": 5144
    },
    {
      "epoch": 1.9918699186991868,
      "grad_norm": 15.964762687683105,
      "learning_rate": 8.897922312556459e-06,
      "loss": 1.5777,
      "step": 5145
    },
    {
      "epoch": 1.9922570654277971,
      "grad_norm": 6.411641597747803,
      "learning_rate": 8.89749214952467e-06,
      "loss": 0.4026,
      "step": 5146
    },
    {
      "epoch": 1.9926442121564074,
      "grad_norm": 11.948575973510742,
      "learning_rate": 8.897061986492882e-06,
      "loss": 1.0605,
      "step": 5147
    },
    {
      "epoch": 1.9930313588850175,
      "grad_norm": 16.324018478393555,
      "learning_rate": 8.896631823461093e-06,
      "loss": 1.6197,
      "step": 5148
    },
    {
      "epoch": 1.9934185056136275,
      "grad_norm": 34.89543533325195,
      "learning_rate": 8.896201660429303e-06,
      "loss": 1.1257,
      "step": 5149
    },
    {
      "epoch": 1.9938056523422376,
      "grad_norm": 13.395251274108887,
      "learning_rate": 8.895771497397514e-06,
      "loss": 1.5348,
      "step": 5150
    },
    {
      "epoch": 1.994192799070848,
      "grad_norm": 23.23766326904297,
      "learning_rate": 8.895341334365726e-06,
      "loss": 1.3587,
      "step": 5151
    },
    {
      "epoch": 1.994579945799458,
      "grad_norm": 22.807100296020508,
      "learning_rate": 8.894911171333937e-06,
      "loss": 1.5504,
      "step": 5152
    },
    {
      "epoch": 1.9949670925280683,
      "grad_norm": 18.683162689208984,
      "learning_rate": 8.894481008302147e-06,
      "loss": 1.7337,
      "step": 5153
    },
    {
      "epoch": 1.9953542392566783,
      "grad_norm": 15.663130760192871,
      "learning_rate": 8.894050845270358e-06,
      "loss": 1.2712,
      "step": 5154
    },
    {
      "epoch": 1.9957413859852884,
      "grad_norm": 12.65172004699707,
      "learning_rate": 8.89362068223857e-06,
      "loss": 0.8572,
      "step": 5155
    },
    {
      "epoch": 1.9961285327138985,
      "grad_norm": 22.445858001708984,
      "learning_rate": 8.893190519206781e-06,
      "loss": 1.5504,
      "step": 5156
    },
    {
      "epoch": 1.9965156794425087,
      "grad_norm": 18.06094741821289,
      "learning_rate": 8.89276035617499e-06,
      "loss": 1.2677,
      "step": 5157
    },
    {
      "epoch": 1.9969028261711188,
      "grad_norm": 12.256243705749512,
      "learning_rate": 8.892330193143202e-06,
      "loss": 0.6064,
      "step": 5158
    },
    {
      "epoch": 1.997289972899729,
      "grad_norm": 16.136953353881836,
      "learning_rate": 8.891900030111413e-06,
      "loss": 0.8408,
      "step": 5159
    },
    {
      "epoch": 1.9976771196283392,
      "grad_norm": 34.73657989501953,
      "learning_rate": 8.891469867079623e-06,
      "loss": 1.4119,
      "step": 5160
    },
    {
      "epoch": 1.9980642663569492,
      "grad_norm": 25.083860397338867,
      "learning_rate": 8.891039704047835e-06,
      "loss": 3.11,
      "step": 5161
    },
    {
      "epoch": 1.9984514130855593,
      "grad_norm": 12.184844017028809,
      "learning_rate": 8.890609541016046e-06,
      "loss": 0.7453,
      "step": 5162
    },
    {
      "epoch": 1.9988385598141696,
      "grad_norm": 38.69596481323242,
      "learning_rate": 8.890179377984257e-06,
      "loss": 1.9925,
      "step": 5163
    },
    {
      "epoch": 1.9992257065427799,
      "grad_norm": 7.004556179046631,
      "learning_rate": 8.889749214952467e-06,
      "loss": 0.4341,
      "step": 5164
    },
    {
      "epoch": 1.99961285327139,
      "grad_norm": 18.260684967041016,
      "learning_rate": 8.889319051920679e-06,
      "loss": 1.5575,
      "step": 5165
    },
    {
      "epoch": 2.0,
      "grad_norm": 37.05977249145508,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.3387,
      "step": 5166
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.4179245283018868,
      "eval_f1": 0.3349792039497048,
      "eval_loss": 1.5170574188232422,
      "eval_runtime": 384.1361,
      "eval_samples_per_second": 2.759,
      "eval_steps_per_second": 1.38,
      "step": 5166
    },
    {
      "epoch": 2.00038714672861,
      "grad_norm": 12.469918251037598,
      "learning_rate": 8.888458725857101e-06,
      "loss": 0.8011,
      "step": 5167
    },
    {
      "epoch": 2.00077429345722,
      "grad_norm": 15.270923614501953,
      "learning_rate": 8.888028562825311e-06,
      "loss": 1.6286,
      "step": 5168
    },
    {
      "epoch": 2.0011614401858306,
      "grad_norm": 12.142326354980469,
      "learning_rate": 8.887598399793523e-06,
      "loss": 0.9793,
      "step": 5169
    },
    {
      "epoch": 2.0015485869144407,
      "grad_norm": 23.50999641418457,
      "learning_rate": 8.887168236761732e-06,
      "loss": 1.6186,
      "step": 5170
    },
    {
      "epoch": 2.0019357336430508,
      "grad_norm": 40.09588623046875,
      "learning_rate": 8.886738073729945e-06,
      "loss": 1.1431,
      "step": 5171
    },
    {
      "epoch": 2.002322880371661,
      "grad_norm": 16.982465744018555,
      "learning_rate": 8.886307910698155e-06,
      "loss": 0.9652,
      "step": 5172
    },
    {
      "epoch": 2.002710027100271,
      "grad_norm": 23.12416648864746,
      "learning_rate": 8.885877747666367e-06,
      "loss": 1.3357,
      "step": 5173
    },
    {
      "epoch": 2.003097173828881,
      "grad_norm": 11.033178329467773,
      "learning_rate": 8.885447584634576e-06,
      "loss": 0.571,
      "step": 5174
    },
    {
      "epoch": 2.0034843205574915,
      "grad_norm": 25.256996154785156,
      "learning_rate": 8.885017421602788e-06,
      "loss": 1.1852,
      "step": 5175
    },
    {
      "epoch": 2.0038714672861015,
      "grad_norm": 15.734246253967285,
      "learning_rate": 8.884587258570999e-06,
      "loss": 0.5739,
      "step": 5176
    },
    {
      "epoch": 2.0042586140147116,
      "grad_norm": 20.253589630126953,
      "learning_rate": 8.88415709553921e-06,
      "loss": 1.7766,
      "step": 5177
    },
    {
      "epoch": 2.0046457607433217,
      "grad_norm": 13.455538749694824,
      "learning_rate": 8.883726932507422e-06,
      "loss": 0.8131,
      "step": 5178
    },
    {
      "epoch": 2.0050329074719317,
      "grad_norm": 13.075383186340332,
      "learning_rate": 8.883296769475632e-06,
      "loss": 0.8323,
      "step": 5179
    },
    {
      "epoch": 2.005420054200542,
      "grad_norm": 28.98955535888672,
      "learning_rate": 8.882866606443843e-06,
      "loss": 1.9334,
      "step": 5180
    },
    {
      "epoch": 2.0058072009291523,
      "grad_norm": 21.13695526123047,
      "learning_rate": 8.882436443412053e-06,
      "loss": 1.7823,
      "step": 5181
    },
    {
      "epoch": 2.0061943476577624,
      "grad_norm": 13.51959228515625,
      "learning_rate": 8.882006280380266e-06,
      "loss": 0.7897,
      "step": 5182
    },
    {
      "epoch": 2.0065814943863725,
      "grad_norm": 18.832542419433594,
      "learning_rate": 8.881576117348476e-06,
      "loss": 1.8319,
      "step": 5183
    },
    {
      "epoch": 2.0069686411149825,
      "grad_norm": 6.642711162567139,
      "learning_rate": 8.881145954316687e-06,
      "loss": 0.4273,
      "step": 5184
    },
    {
      "epoch": 2.0073557878435926,
      "grad_norm": 42.16477966308594,
      "learning_rate": 8.880715791284897e-06,
      "loss": 2.1973,
      "step": 5185
    },
    {
      "epoch": 2.0077429345722027,
      "grad_norm": 31.73929214477539,
      "learning_rate": 8.88028562825311e-06,
      "loss": 2.7788,
      "step": 5186
    },
    {
      "epoch": 2.008130081300813,
      "grad_norm": 16.267333984375,
      "learning_rate": 8.87985546522132e-06,
      "loss": 1.5236,
      "step": 5187
    },
    {
      "epoch": 2.0085172280294232,
      "grad_norm": 11.661026954650879,
      "learning_rate": 8.879425302189531e-06,
      "loss": 0.5422,
      "step": 5188
    },
    {
      "epoch": 2.0089043747580333,
      "grad_norm": 19.39784812927246,
      "learning_rate": 8.87899513915774e-06,
      "loss": 1.4066,
      "step": 5189
    },
    {
      "epoch": 2.0092915214866434,
      "grad_norm": 15.351884841918945,
      "learning_rate": 8.878564976125952e-06,
      "loss": 1.2843,
      "step": 5190
    },
    {
      "epoch": 2.0096786682152534,
      "grad_norm": 23.648836135864258,
      "learning_rate": 8.878134813094164e-06,
      "loss": 1.9406,
      "step": 5191
    },
    {
      "epoch": 2.010065814943864,
      "grad_norm": 29.18762969970703,
      "learning_rate": 8.877704650062375e-06,
      "loss": 1.7413,
      "step": 5192
    },
    {
      "epoch": 2.010452961672474,
      "grad_norm": 15.705753326416016,
      "learning_rate": 8.877274487030585e-06,
      "loss": 1.2069,
      "step": 5193
    },
    {
      "epoch": 2.010840108401084,
      "grad_norm": 23.700441360473633,
      "learning_rate": 8.876844323998796e-06,
      "loss": 1.7228,
      "step": 5194
    },
    {
      "epoch": 2.011227255129694,
      "grad_norm": 22.387317657470703,
      "learning_rate": 8.876414160967007e-06,
      "loss": 1.5486,
      "step": 5195
    },
    {
      "epoch": 2.011614401858304,
      "grad_norm": 28.559335708618164,
      "learning_rate": 8.875983997935217e-06,
      "loss": 1.1091,
      "step": 5196
    },
    {
      "epoch": 2.0120015485869143,
      "grad_norm": 34.89916229248047,
      "learning_rate": 8.875553834903429e-06,
      "loss": 2.1615,
      "step": 5197
    },
    {
      "epoch": 2.012388695315525,
      "grad_norm": 14.466558456420898,
      "learning_rate": 8.87512367187164e-06,
      "loss": 1.073,
      "step": 5198
    },
    {
      "epoch": 2.012775842044135,
      "grad_norm": 32.409915924072266,
      "learning_rate": 8.874693508839851e-06,
      "loss": 1.9611,
      "step": 5199
    },
    {
      "epoch": 2.013162988772745,
      "grad_norm": 17.05964469909668,
      "learning_rate": 8.874263345808061e-06,
      "loss": 1.5679,
      "step": 5200
    },
    {
      "epoch": 2.013550135501355,
      "grad_norm": 12.140432357788086,
      "learning_rate": 8.873833182776273e-06,
      "loss": 0.8733,
      "step": 5201
    },
    {
      "epoch": 2.013937282229965,
      "grad_norm": 7.121780872344971,
      "learning_rate": 8.873403019744484e-06,
      "loss": 0.3687,
      "step": 5202
    },
    {
      "epoch": 2.014324428958575,
      "grad_norm": 21.0531005859375,
      "learning_rate": 8.872972856712695e-06,
      "loss": 1.6618,
      "step": 5203
    },
    {
      "epoch": 2.0147115756871856,
      "grad_norm": 25.946449279785156,
      "learning_rate": 8.872542693680905e-06,
      "loss": 1.3983,
      "step": 5204
    },
    {
      "epoch": 2.0150987224157957,
      "grad_norm": 14.27526569366455,
      "learning_rate": 8.872112530649117e-06,
      "loss": 1.4902,
      "step": 5205
    },
    {
      "epoch": 2.0154858691444058,
      "grad_norm": 15.478309631347656,
      "learning_rate": 8.871682367617328e-06,
      "loss": 0.8597,
      "step": 5206
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 21.079429626464844,
      "learning_rate": 8.87125220458554e-06,
      "loss": 0.9209,
      "step": 5207
    },
    {
      "epoch": 2.016260162601626,
      "grad_norm": 40.21015167236328,
      "learning_rate": 8.870822041553749e-06,
      "loss": 0.9496,
      "step": 5208
    },
    {
      "epoch": 2.016647309330236,
      "grad_norm": 24.85200309753418,
      "learning_rate": 8.87039187852196e-06,
      "loss": 1.5203,
      "step": 5209
    },
    {
      "epoch": 2.0170344560588465,
      "grad_norm": 16.508127212524414,
      "learning_rate": 8.869961715490172e-06,
      "loss": 0.9379,
      "step": 5210
    },
    {
      "epoch": 2.0174216027874565,
      "grad_norm": 20.369144439697266,
      "learning_rate": 8.869531552458382e-06,
      "loss": 3.2077,
      "step": 5211
    },
    {
      "epoch": 2.0178087495160666,
      "grad_norm": 30.41213607788086,
      "learning_rate": 8.869101389426593e-06,
      "loss": 1.3691,
      "step": 5212
    },
    {
      "epoch": 2.0181958962446767,
      "grad_norm": 11.665181159973145,
      "learning_rate": 8.868671226394805e-06,
      "loss": 0.6226,
      "step": 5213
    },
    {
      "epoch": 2.0185830429732867,
      "grad_norm": 37.52671432495117,
      "learning_rate": 8.868241063363016e-06,
      "loss": 2.7684,
      "step": 5214
    },
    {
      "epoch": 2.0189701897018972,
      "grad_norm": 31.01368522644043,
      "learning_rate": 8.867810900331226e-06,
      "loss": 1.307,
      "step": 5215
    },
    {
      "epoch": 2.0193573364305073,
      "grad_norm": 27.4733943939209,
      "learning_rate": 8.867380737299437e-06,
      "loss": 1.6884,
      "step": 5216
    },
    {
      "epoch": 2.0197444831591174,
      "grad_norm": 16.41004753112793,
      "learning_rate": 8.866950574267648e-06,
      "loss": 1.7154,
      "step": 5217
    },
    {
      "epoch": 2.0201316298877274,
      "grad_norm": 28.92599105834961,
      "learning_rate": 8.86652041123586e-06,
      "loss": 1.398,
      "step": 5218
    },
    {
      "epoch": 2.0205187766163375,
      "grad_norm": 13.447602272033691,
      "learning_rate": 8.86609024820407e-06,
      "loss": 0.932,
      "step": 5219
    },
    {
      "epoch": 2.0209059233449476,
      "grad_norm": 23.14986801147461,
      "learning_rate": 8.865660085172281e-06,
      "loss": 2.5602,
      "step": 5220
    },
    {
      "epoch": 2.021293070073558,
      "grad_norm": 25.622880935668945,
      "learning_rate": 8.865229922140492e-06,
      "loss": 1.7698,
      "step": 5221
    },
    {
      "epoch": 2.021680216802168,
      "grad_norm": 9.672566413879395,
      "learning_rate": 8.864799759108704e-06,
      "loss": 0.5463,
      "step": 5222
    },
    {
      "epoch": 2.022067363530778,
      "grad_norm": 21.62165069580078,
      "learning_rate": 8.864369596076914e-06,
      "loss": 1.8564,
      "step": 5223
    },
    {
      "epoch": 2.0224545102593883,
      "grad_norm": 20.5743350982666,
      "learning_rate": 8.863939433045125e-06,
      "loss": 1.9382,
      "step": 5224
    },
    {
      "epoch": 2.0228416569879983,
      "grad_norm": 27.3149471282959,
      "learning_rate": 8.863509270013336e-06,
      "loss": 3.4029,
      "step": 5225
    },
    {
      "epoch": 2.0232288037166084,
      "grad_norm": 28.99734878540039,
      "learning_rate": 8.863079106981546e-06,
      "loss": 3.1968,
      "step": 5226
    },
    {
      "epoch": 2.023615950445219,
      "grad_norm": 19.040557861328125,
      "learning_rate": 8.862648943949758e-06,
      "loss": 1.1729,
      "step": 5227
    },
    {
      "epoch": 2.024003097173829,
      "grad_norm": 27.671955108642578,
      "learning_rate": 8.862218780917969e-06,
      "loss": 2.7004,
      "step": 5228
    },
    {
      "epoch": 2.024390243902439,
      "grad_norm": 28.52119255065918,
      "learning_rate": 8.86178861788618e-06,
      "loss": 1.0695,
      "step": 5229
    },
    {
      "epoch": 2.024777390631049,
      "grad_norm": 29.859792709350586,
      "learning_rate": 8.86135845485439e-06,
      "loss": 2.1697,
      "step": 5230
    },
    {
      "epoch": 2.025164537359659,
      "grad_norm": 25.07796859741211,
      "learning_rate": 8.860928291822602e-06,
      "loss": 1.0318,
      "step": 5231
    },
    {
      "epoch": 2.0255516840882692,
      "grad_norm": 28.785364151000977,
      "learning_rate": 8.860498128790811e-06,
      "loss": 2.7063,
      "step": 5232
    },
    {
      "epoch": 2.0259388308168798,
      "grad_norm": 18.434234619140625,
      "learning_rate": 8.860067965759024e-06,
      "loss": 1.5911,
      "step": 5233
    },
    {
      "epoch": 2.02632597754549,
      "grad_norm": 24.460533142089844,
      "learning_rate": 8.859637802727234e-06,
      "loss": 0.9435,
      "step": 5234
    },
    {
      "epoch": 2.0267131242741,
      "grad_norm": 15.837151527404785,
      "learning_rate": 8.859207639695445e-06,
      "loss": 1.1822,
      "step": 5235
    },
    {
      "epoch": 2.02710027100271,
      "grad_norm": 18.78538703918457,
      "learning_rate": 8.858777476663655e-06,
      "loss": 0.8724,
      "step": 5236
    },
    {
      "epoch": 2.02748741773132,
      "grad_norm": 16.222400665283203,
      "learning_rate": 8.858347313631868e-06,
      "loss": 1.4566,
      "step": 5237
    },
    {
      "epoch": 2.0278745644599305,
      "grad_norm": 17.231895446777344,
      "learning_rate": 8.857917150600078e-06,
      "loss": 1.4938,
      "step": 5238
    },
    {
      "epoch": 2.0282617111885406,
      "grad_norm": 16.9628849029541,
      "learning_rate": 8.85748698756829e-06,
      "loss": 1.593,
      "step": 5239
    },
    {
      "epoch": 2.0286488579171507,
      "grad_norm": 14.213948249816895,
      "learning_rate": 8.8570568245365e-06,
      "loss": 0.8176,
      "step": 5240
    },
    {
      "epoch": 2.0290360046457607,
      "grad_norm": 29.42449951171875,
      "learning_rate": 8.85662666150471e-06,
      "loss": 1.3464,
      "step": 5241
    },
    {
      "epoch": 2.029423151374371,
      "grad_norm": 28.838367462158203,
      "learning_rate": 8.856196498472922e-06,
      "loss": 1.5782,
      "step": 5242
    },
    {
      "epoch": 2.029810298102981,
      "grad_norm": 22.399494171142578,
      "learning_rate": 8.855766335441133e-06,
      "loss": 1.6843,
      "step": 5243
    },
    {
      "epoch": 2.0301974448315914,
      "grad_norm": 43.45531463623047,
      "learning_rate": 8.855336172409343e-06,
      "loss": 1.2747,
      "step": 5244
    },
    {
      "epoch": 2.0305845915602014,
      "grad_norm": 17.570837020874023,
      "learning_rate": 8.854906009377555e-06,
      "loss": 1.5309,
      "step": 5245
    },
    {
      "epoch": 2.0309717382888115,
      "grad_norm": 23.029436111450195,
      "learning_rate": 8.854475846345766e-06,
      "loss": 2.0388,
      "step": 5246
    },
    {
      "epoch": 2.0313588850174216,
      "grad_norm": 15.179142951965332,
      "learning_rate": 8.854045683313976e-06,
      "loss": 1.8044,
      "step": 5247
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 26.90972137451172,
      "learning_rate": 8.853615520282187e-06,
      "loss": 1.2685,
      "step": 5248
    },
    {
      "epoch": 2.0321331784746417,
      "grad_norm": 26.506975173950195,
      "learning_rate": 8.853185357250399e-06,
      "loss": 1.3366,
      "step": 5249
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 15.991565704345703,
      "learning_rate": 8.85275519421861e-06,
      "loss": 0.9906,
      "step": 5250
    },
    {
      "epoch": 2.0329074719318623,
      "grad_norm": 24.687143325805664,
      "learning_rate": 8.85232503118682e-06,
      "loss": 1.6006,
      "step": 5251
    },
    {
      "epoch": 2.0332946186604723,
      "grad_norm": 15.640265464782715,
      "learning_rate": 8.851894868155031e-06,
      "loss": 1.0542,
      "step": 5252
    },
    {
      "epoch": 2.0336817653890824,
      "grad_norm": 25.983394622802734,
      "learning_rate": 8.851464705123243e-06,
      "loss": 1.4572,
      "step": 5253
    },
    {
      "epoch": 2.0340689121176925,
      "grad_norm": 13.480133056640625,
      "learning_rate": 8.851034542091454e-06,
      "loss": 0.4417,
      "step": 5254
    },
    {
      "epoch": 2.0344560588463025,
      "grad_norm": 69.73641967773438,
      "learning_rate": 8.850604379059664e-06,
      "loss": 2.349,
      "step": 5255
    },
    {
      "epoch": 2.034843205574913,
      "grad_norm": 31.301340103149414,
      "learning_rate": 8.850174216027875e-06,
      "loss": 1.8991,
      "step": 5256
    },
    {
      "epoch": 2.035230352303523,
      "grad_norm": 35.097774505615234,
      "learning_rate": 8.849744052996086e-06,
      "loss": 1.7403,
      "step": 5257
    },
    {
      "epoch": 2.035617499032133,
      "grad_norm": 20.318090438842773,
      "learning_rate": 8.849313889964298e-06,
      "loss": 1.791,
      "step": 5258
    },
    {
      "epoch": 2.0360046457607432,
      "grad_norm": 23.41549301147461,
      "learning_rate": 8.848883726932508e-06,
      "loss": 1.1889,
      "step": 5259
    },
    {
      "epoch": 2.0363917924893533,
      "grad_norm": 17.34352684020996,
      "learning_rate": 8.848453563900719e-06,
      "loss": 1.4619,
      "step": 5260
    },
    {
      "epoch": 2.036778939217964,
      "grad_norm": 15.91549301147461,
      "learning_rate": 8.84802340086893e-06,
      "loss": 0.9968,
      "step": 5261
    },
    {
      "epoch": 2.037166085946574,
      "grad_norm": 9.992873191833496,
      "learning_rate": 8.84759323783714e-06,
      "loss": 1.3519,
      "step": 5262
    },
    {
      "epoch": 2.037553232675184,
      "grad_norm": 24.179229736328125,
      "learning_rate": 8.847163074805352e-06,
      "loss": 1.6138,
      "step": 5263
    },
    {
      "epoch": 2.037940379403794,
      "grad_norm": 16.556427001953125,
      "learning_rate": 8.846732911773563e-06,
      "loss": 1.6065,
      "step": 5264
    },
    {
      "epoch": 2.038327526132404,
      "grad_norm": 33.76530838012695,
      "learning_rate": 8.846302748741774e-06,
      "loss": 3.2262,
      "step": 5265
    },
    {
      "epoch": 2.038714672861014,
      "grad_norm": 24.635574340820312,
      "learning_rate": 8.845872585709984e-06,
      "loss": 1.7165,
      "step": 5266
    },
    {
      "epoch": 2.0391018195896247,
      "grad_norm": 21.842308044433594,
      "learning_rate": 8.845442422678196e-06,
      "loss": 2.0045,
      "step": 5267
    },
    {
      "epoch": 2.0394889663182347,
      "grad_norm": 14.315924644470215,
      "learning_rate": 8.845012259646407e-06,
      "loss": 1.1797,
      "step": 5268
    },
    {
      "epoch": 2.039876113046845,
      "grad_norm": 16.7281436920166,
      "learning_rate": 8.844582096614618e-06,
      "loss": 1.6775,
      "step": 5269
    },
    {
      "epoch": 2.040263259775455,
      "grad_norm": 24.599185943603516,
      "learning_rate": 8.844151933582828e-06,
      "loss": 2.3567,
      "step": 5270
    },
    {
      "epoch": 2.040650406504065,
      "grad_norm": 16.640377044677734,
      "learning_rate": 8.84372177055104e-06,
      "loss": 1.1211,
      "step": 5271
    },
    {
      "epoch": 2.041037553232675,
      "grad_norm": 15.511281967163086,
      "learning_rate": 8.843291607519251e-06,
      "loss": 0.6919,
      "step": 5272
    },
    {
      "epoch": 2.0414246999612855,
      "grad_norm": 13.122479438781738,
      "learning_rate": 8.842861444487462e-06,
      "loss": 1.0425,
      "step": 5273
    },
    {
      "epoch": 2.0418118466898956,
      "grad_norm": 15.480423927307129,
      "learning_rate": 8.842431281455672e-06,
      "loss": 1.1877,
      "step": 5274
    },
    {
      "epoch": 2.0421989934185056,
      "grad_norm": 12.7503080368042,
      "learning_rate": 8.842001118423883e-06,
      "loss": 1.6218,
      "step": 5275
    },
    {
      "epoch": 2.0425861401471157,
      "grad_norm": 15.252778053283691,
      "learning_rate": 8.841570955392095e-06,
      "loss": 1.5122,
      "step": 5276
    },
    {
      "epoch": 2.0429732868757258,
      "grad_norm": 19.316408157348633,
      "learning_rate": 8.841140792360305e-06,
      "loss": 0.8272,
      "step": 5277
    },
    {
      "epoch": 2.043360433604336,
      "grad_norm": 14.719864845275879,
      "learning_rate": 8.840710629328516e-06,
      "loss": 1.0139,
      "step": 5278
    },
    {
      "epoch": 2.0437475803329463,
      "grad_norm": 21.59795379638672,
      "learning_rate": 8.840280466296727e-06,
      "loss": 0.9344,
      "step": 5279
    },
    {
      "epoch": 2.0441347270615564,
      "grad_norm": 16.91336441040039,
      "learning_rate": 8.839850303264939e-06,
      "loss": 1.4633,
      "step": 5280
    },
    {
      "epoch": 2.0445218737901665,
      "grad_norm": 15.276266098022461,
      "learning_rate": 8.839420140233149e-06,
      "loss": 1.0767,
      "step": 5281
    },
    {
      "epoch": 2.0449090205187765,
      "grad_norm": 16.984580993652344,
      "learning_rate": 8.83898997720136e-06,
      "loss": 1.4646,
      "step": 5282
    },
    {
      "epoch": 2.0452961672473866,
      "grad_norm": 15.137110710144043,
      "learning_rate": 8.83855981416957e-06,
      "loss": 1.005,
      "step": 5283
    },
    {
      "epoch": 2.045683313975997,
      "grad_norm": 19.60561180114746,
      "learning_rate": 8.838129651137783e-06,
      "loss": 1.8432,
      "step": 5284
    },
    {
      "epoch": 2.046070460704607,
      "grad_norm": 12.362703323364258,
      "learning_rate": 8.837699488105993e-06,
      "loss": 0.6065,
      "step": 5285
    },
    {
      "epoch": 2.0464576074332173,
      "grad_norm": 15.948601722717285,
      "learning_rate": 8.837269325074204e-06,
      "loss": 0.9309,
      "step": 5286
    },
    {
      "epoch": 2.0468447541618273,
      "grad_norm": 13.10516357421875,
      "learning_rate": 8.836839162042414e-06,
      "loss": 1.2524,
      "step": 5287
    },
    {
      "epoch": 2.0472319008904374,
      "grad_norm": 27.152185440063477,
      "learning_rate": 8.836408999010627e-06,
      "loss": 1.706,
      "step": 5288
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 19.120315551757812,
      "learning_rate": 8.835978835978837e-06,
      "loss": 1.7283,
      "step": 5289
    },
    {
      "epoch": 2.048006194347658,
      "grad_norm": 7.99722957611084,
      "learning_rate": 8.835548672947048e-06,
      "loss": 0.5244,
      "step": 5290
    },
    {
      "epoch": 2.048393341076268,
      "grad_norm": 28.609943389892578,
      "learning_rate": 8.835118509915258e-06,
      "loss": 1.5281,
      "step": 5291
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 16.968786239624023,
      "learning_rate": 8.834688346883469e-06,
      "loss": 1.6958,
      "step": 5292
    },
    {
      "epoch": 2.049167634533488,
      "grad_norm": 35.781761169433594,
      "learning_rate": 8.83425818385168e-06,
      "loss": 1.5281,
      "step": 5293
    },
    {
      "epoch": 2.0495547812620982,
      "grad_norm": 12.7510347366333,
      "learning_rate": 8.833828020819892e-06,
      "loss": 0.8221,
      "step": 5294
    },
    {
      "epoch": 2.0499419279907083,
      "grad_norm": 17.562185287475586,
      "learning_rate": 8.833397857788102e-06,
      "loss": 1.1014,
      "step": 5295
    },
    {
      "epoch": 2.050329074719319,
      "grad_norm": 42.74247741699219,
      "learning_rate": 8.832967694756313e-06,
      "loss": 1.4601,
      "step": 5296
    },
    {
      "epoch": 2.050716221447929,
      "grad_norm": 13.476424217224121,
      "learning_rate": 8.832537531724524e-06,
      "loss": 0.8067,
      "step": 5297
    },
    {
      "epoch": 2.051103368176539,
      "grad_norm": 23.96453857421875,
      "learning_rate": 8.832107368692734e-06,
      "loss": 1.4537,
      "step": 5298
    },
    {
      "epoch": 2.051490514905149,
      "grad_norm": 11.443099975585938,
      "learning_rate": 8.831677205660947e-06,
      "loss": 0.3224,
      "step": 5299
    },
    {
      "epoch": 2.051877661633759,
      "grad_norm": 13.938823699951172,
      "learning_rate": 8.831247042629157e-06,
      "loss": 0.9775,
      "step": 5300
    },
    {
      "epoch": 2.052264808362369,
      "grad_norm": 15.540149688720703,
      "learning_rate": 8.830816879597368e-06,
      "loss": 1.7582,
      "step": 5301
    },
    {
      "epoch": 2.0526519550909796,
      "grad_norm": 16.50342559814453,
      "learning_rate": 8.830386716565578e-06,
      "loss": 1.508,
      "step": 5302
    },
    {
      "epoch": 2.0530391018195897,
      "grad_norm": 19.267459869384766,
      "learning_rate": 8.829956553533791e-06,
      "loss": 0.9366,
      "step": 5303
    },
    {
      "epoch": 2.0534262485481998,
      "grad_norm": 13.17273235321045,
      "learning_rate": 8.829526390502001e-06,
      "loss": 1.1847,
      "step": 5304
    },
    {
      "epoch": 2.05381339527681,
      "grad_norm": 12.085047721862793,
      "learning_rate": 8.829096227470212e-06,
      "loss": 0.9541,
      "step": 5305
    },
    {
      "epoch": 2.05420054200542,
      "grad_norm": 46.55402374267578,
      "learning_rate": 8.828666064438422e-06,
      "loss": 1.6834,
      "step": 5306
    },
    {
      "epoch": 2.0545876887340304,
      "grad_norm": 7.320636749267578,
      "learning_rate": 8.828235901406634e-06,
      "loss": 0.4044,
      "step": 5307
    },
    {
      "epoch": 2.0549748354626405,
      "grad_norm": 15.732946395874023,
      "learning_rate": 8.827805738374845e-06,
      "loss": 1.1791,
      "step": 5308
    },
    {
      "epoch": 2.0553619821912505,
      "grad_norm": 88.78375244140625,
      "learning_rate": 8.827375575343056e-06,
      "loss": 2.0438,
      "step": 5309
    },
    {
      "epoch": 2.0557491289198606,
      "grad_norm": 22.289180755615234,
      "learning_rate": 8.826945412311266e-06,
      "loss": 1.5233,
      "step": 5310
    },
    {
      "epoch": 2.0561362756484707,
      "grad_norm": 14.42411994934082,
      "learning_rate": 8.826515249279478e-06,
      "loss": 0.9013,
      "step": 5311
    },
    {
      "epoch": 2.0565234223770807,
      "grad_norm": 33.35084915161133,
      "learning_rate": 8.826085086247689e-06,
      "loss": 2.1586,
      "step": 5312
    },
    {
      "epoch": 2.0569105691056913,
      "grad_norm": 14.871529579162598,
      "learning_rate": 8.825654923215899e-06,
      "loss": 1.1713,
      "step": 5313
    },
    {
      "epoch": 2.0572977158343013,
      "grad_norm": 20.63605499267578,
      "learning_rate": 8.82522476018411e-06,
      "loss": 1.7607,
      "step": 5314
    },
    {
      "epoch": 2.0576848625629114,
      "grad_norm": 17.583789825439453,
      "learning_rate": 8.824794597152321e-06,
      "loss": 1.3471,
      "step": 5315
    },
    {
      "epoch": 2.0580720092915215,
      "grad_norm": 17.984874725341797,
      "learning_rate": 8.824364434120533e-06,
      "loss": 1.7815,
      "step": 5316
    },
    {
      "epoch": 2.0584591560201315,
      "grad_norm": 12.2960844039917,
      "learning_rate": 8.823934271088743e-06,
      "loss": 0.7254,
      "step": 5317
    },
    {
      "epoch": 2.0588463027487416,
      "grad_norm": 25.66693115234375,
      "learning_rate": 8.823504108056954e-06,
      "loss": 1.6096,
      "step": 5318
    },
    {
      "epoch": 2.059233449477352,
      "grad_norm": 24.645099639892578,
      "learning_rate": 8.823073945025165e-06,
      "loss": 1.5546,
      "step": 5319
    },
    {
      "epoch": 2.059620596205962,
      "grad_norm": 8.836212158203125,
      "learning_rate": 8.822643781993377e-06,
      "loss": 1.2162,
      "step": 5320
    },
    {
      "epoch": 2.0600077429345722,
      "grad_norm": 18.655427932739258,
      "learning_rate": 8.822213618961587e-06,
      "loss": 1.6357,
      "step": 5321
    },
    {
      "epoch": 2.0603948896631823,
      "grad_norm": 25.012483596801758,
      "learning_rate": 8.821783455929798e-06,
      "loss": 1.3131,
      "step": 5322
    },
    {
      "epoch": 2.0607820363917924,
      "grad_norm": 33.42828369140625,
      "learning_rate": 8.82135329289801e-06,
      "loss": 1.8525,
      "step": 5323
    },
    {
      "epoch": 2.0611691831204024,
      "grad_norm": 17.906614303588867,
      "learning_rate": 8.82092312986622e-06,
      "loss": 1.8514,
      "step": 5324
    },
    {
      "epoch": 2.061556329849013,
      "grad_norm": 27.293960571289062,
      "learning_rate": 8.82049296683443e-06,
      "loss": 1.1947,
      "step": 5325
    },
    {
      "epoch": 2.061943476577623,
      "grad_norm": 29.357967376708984,
      "learning_rate": 8.820062803802642e-06,
      "loss": 0.6829,
      "step": 5326
    },
    {
      "epoch": 2.062330623306233,
      "grad_norm": 28.224281311035156,
      "learning_rate": 8.819632640770853e-06,
      "loss": 2.4621,
      "step": 5327
    },
    {
      "epoch": 2.062717770034843,
      "grad_norm": 30.114131927490234,
      "learning_rate": 8.819202477739063e-06,
      "loss": 1.732,
      "step": 5328
    },
    {
      "epoch": 2.063104916763453,
      "grad_norm": 27.570446014404297,
      "learning_rate": 8.818772314707275e-06,
      "loss": 2.5895,
      "step": 5329
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 19.58695411682129,
      "learning_rate": 8.818342151675486e-06,
      "loss": 0.9856,
      "step": 5330
    },
    {
      "epoch": 2.0638792102206738,
      "grad_norm": 15.941386222839355,
      "learning_rate": 8.817911988643697e-06,
      "loss": 1.078,
      "step": 5331
    },
    {
      "epoch": 2.064266356949284,
      "grad_norm": 18.70539093017578,
      "learning_rate": 8.817481825611907e-06,
      "loss": 1.6823,
      "step": 5332
    },
    {
      "epoch": 2.064653503677894,
      "grad_norm": 21.345144271850586,
      "learning_rate": 8.817051662580118e-06,
      "loss": 1.6455,
      "step": 5333
    },
    {
      "epoch": 2.065040650406504,
      "grad_norm": 10.794562339782715,
      "learning_rate": 8.816621499548328e-06,
      "loss": 0.6206,
      "step": 5334
    },
    {
      "epoch": 2.065427797135114,
      "grad_norm": 22.119142532348633,
      "learning_rate": 8.816191336516541e-06,
      "loss": 2.1073,
      "step": 5335
    },
    {
      "epoch": 2.0658149438637246,
      "grad_norm": 15.799921989440918,
      "learning_rate": 8.815761173484751e-06,
      "loss": 0.8401,
      "step": 5336
    },
    {
      "epoch": 2.0662020905923346,
      "grad_norm": 22.102846145629883,
      "learning_rate": 8.815331010452962e-06,
      "loss": 1.935,
      "step": 5337
    },
    {
      "epoch": 2.0665892373209447,
      "grad_norm": 66.20753479003906,
      "learning_rate": 8.814900847421172e-06,
      "loss": 1.8996,
      "step": 5338
    },
    {
      "epoch": 2.0669763840495547,
      "grad_norm": 29.53036880493164,
      "learning_rate": 8.814470684389385e-06,
      "loss": 1.8235,
      "step": 5339
    },
    {
      "epoch": 2.067363530778165,
      "grad_norm": 12.932026863098145,
      "learning_rate": 8.814040521357595e-06,
      "loss": 0.7502,
      "step": 5340
    },
    {
      "epoch": 2.067750677506775,
      "grad_norm": 24.596826553344727,
      "learning_rate": 8.813610358325806e-06,
      "loss": 2.6912,
      "step": 5341
    },
    {
      "epoch": 2.0681378242353854,
      "grad_norm": 27.047353744506836,
      "learning_rate": 8.813180195294018e-06,
      "loss": 1.4698,
      "step": 5342
    },
    {
      "epoch": 2.0685249709639955,
      "grad_norm": 4.454951286315918,
      "learning_rate": 8.812750032262228e-06,
      "loss": 0.1496,
      "step": 5343
    },
    {
      "epoch": 2.0689121176926055,
      "grad_norm": 18.89039421081543,
      "learning_rate": 8.812319869230439e-06,
      "loss": 1.8978,
      "step": 5344
    },
    {
      "epoch": 2.0692992644212156,
      "grad_norm": 21.84629249572754,
      "learning_rate": 8.81188970619865e-06,
      "loss": 2.0591,
      "step": 5345
    },
    {
      "epoch": 2.0696864111498257,
      "grad_norm": 14.338268280029297,
      "learning_rate": 8.811459543166862e-06,
      "loss": 0.862,
      "step": 5346
    },
    {
      "epoch": 2.0700735578784357,
      "grad_norm": 15.987637519836426,
      "learning_rate": 8.811029380135072e-06,
      "loss": 1.4864,
      "step": 5347
    },
    {
      "epoch": 2.0704607046070462,
      "grad_norm": 27.519901275634766,
      "learning_rate": 8.810599217103283e-06,
      "loss": 1.6569,
      "step": 5348
    },
    {
      "epoch": 2.0708478513356563,
      "grad_norm": 14.722236633300781,
      "learning_rate": 8.810169054071493e-06,
      "loss": 1.2199,
      "step": 5349
    },
    {
      "epoch": 2.0712349980642664,
      "grad_norm": 8.375170707702637,
      "learning_rate": 8.809738891039706e-06,
      "loss": 0.4259,
      "step": 5350
    },
    {
      "epoch": 2.0716221447928764,
      "grad_norm": 20.073951721191406,
      "learning_rate": 8.809308728007916e-06,
      "loss": 2.1498,
      "step": 5351
    },
    {
      "epoch": 2.0720092915214865,
      "grad_norm": 14.446979522705078,
      "learning_rate": 8.808878564976127e-06,
      "loss": 0.8457,
      "step": 5352
    },
    {
      "epoch": 2.072396438250097,
      "grad_norm": 16.400062561035156,
      "learning_rate": 8.808448401944337e-06,
      "loss": 1.5696,
      "step": 5353
    },
    {
      "epoch": 2.072783584978707,
      "grad_norm": 15.826266288757324,
      "learning_rate": 8.80801823891255e-06,
      "loss": 1.661,
      "step": 5354
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 54.5934944152832,
      "learning_rate": 8.80758807588076e-06,
      "loss": 1.6087,
      "step": 5355
    },
    {
      "epoch": 2.073557878435927,
      "grad_norm": 20.22399139404297,
      "learning_rate": 8.807157912848971e-06,
      "loss": 2.4684,
      "step": 5356
    },
    {
      "epoch": 2.0739450251645373,
      "grad_norm": 22.395002365112305,
      "learning_rate": 8.80672774981718e-06,
      "loss": 2.3117,
      "step": 5357
    },
    {
      "epoch": 2.0743321718931473,
      "grad_norm": 13.073339462280273,
      "learning_rate": 8.806297586785392e-06,
      "loss": 0.7765,
      "step": 5358
    },
    {
      "epoch": 2.074719318621758,
      "grad_norm": 25.7995662689209,
      "learning_rate": 8.805867423753603e-06,
      "loss": 1.419,
      "step": 5359
    },
    {
      "epoch": 2.075106465350368,
      "grad_norm": 15.38979721069336,
      "learning_rate": 8.805437260721815e-06,
      "loss": 1.45,
      "step": 5360
    },
    {
      "epoch": 2.075493612078978,
      "grad_norm": 14.633212089538574,
      "learning_rate": 8.805007097690025e-06,
      "loss": 1.4658,
      "step": 5361
    },
    {
      "epoch": 2.075880758807588,
      "grad_norm": 14.661187171936035,
      "learning_rate": 8.804576934658236e-06,
      "loss": 2.95,
      "step": 5362
    },
    {
      "epoch": 2.076267905536198,
      "grad_norm": 15.903286933898926,
      "learning_rate": 8.804146771626447e-06,
      "loss": 0.9998,
      "step": 5363
    },
    {
      "epoch": 2.076655052264808,
      "grad_norm": 25.328216552734375,
      "learning_rate": 8.803716608594657e-06,
      "loss": 1.0178,
      "step": 5364
    },
    {
      "epoch": 2.0770421989934187,
      "grad_norm": 35.3859977722168,
      "learning_rate": 8.803286445562869e-06,
      "loss": 1.9338,
      "step": 5365
    },
    {
      "epoch": 2.0774293457220288,
      "grad_norm": 44.874420166015625,
      "learning_rate": 8.80285628253108e-06,
      "loss": 2.9736,
      "step": 5366
    },
    {
      "epoch": 2.077816492450639,
      "grad_norm": 18.643808364868164,
      "learning_rate": 8.802426119499291e-06,
      "loss": 1.3888,
      "step": 5367
    },
    {
      "epoch": 2.078203639179249,
      "grad_norm": 5.710572719573975,
      "learning_rate": 8.801995956467501e-06,
      "loss": 0.3334,
      "step": 5368
    },
    {
      "epoch": 2.078590785907859,
      "grad_norm": 16.524402618408203,
      "learning_rate": 8.801565793435713e-06,
      "loss": 0.6874,
      "step": 5369
    },
    {
      "epoch": 2.078977932636469,
      "grad_norm": 11.629517555236816,
      "learning_rate": 8.801135630403924e-06,
      "loss": 0.703,
      "step": 5370
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 15.618741989135742,
      "learning_rate": 8.800705467372135e-06,
      "loss": 1.3525,
      "step": 5371
    },
    {
      "epoch": 2.0797522260936896,
      "grad_norm": 34.45285415649414,
      "learning_rate": 8.800275304340345e-06,
      "loss": 1.8998,
      "step": 5372
    },
    {
      "epoch": 2.0801393728222997,
      "grad_norm": 14.727685928344727,
      "learning_rate": 8.799845141308556e-06,
      "loss": 0.8537,
      "step": 5373
    },
    {
      "epoch": 2.0805265195509097,
      "grad_norm": 44.79212188720703,
      "learning_rate": 8.799414978276768e-06,
      "loss": 0.7053,
      "step": 5374
    },
    {
      "epoch": 2.08091366627952,
      "grad_norm": 12.322593688964844,
      "learning_rate": 8.79898481524498e-06,
      "loss": 0.8106,
      "step": 5375
    },
    {
      "epoch": 2.08130081300813,
      "grad_norm": 18.62436294555664,
      "learning_rate": 8.798554652213189e-06,
      "loss": 2.6262,
      "step": 5376
    },
    {
      "epoch": 2.0816879597367404,
      "grad_norm": 17.907407760620117,
      "learning_rate": 8.7981244891814e-06,
      "loss": 1.2551,
      "step": 5377
    },
    {
      "epoch": 2.0820751064653504,
      "grad_norm": 24.051355361938477,
      "learning_rate": 8.797694326149612e-06,
      "loss": 3.3635,
      "step": 5378
    },
    {
      "epoch": 2.0824622531939605,
      "grad_norm": 21.283578872680664,
      "learning_rate": 8.797264163117822e-06,
      "loss": 2.3844,
      "step": 5379
    },
    {
      "epoch": 2.0828493999225706,
      "grad_norm": 18.678300857543945,
      "learning_rate": 8.796834000086033e-06,
      "loss": 2.202,
      "step": 5380
    },
    {
      "epoch": 2.0832365466511806,
      "grad_norm": 20.5657958984375,
      "learning_rate": 8.796403837054244e-06,
      "loss": 1.6187,
      "step": 5381
    },
    {
      "epoch": 2.083623693379791,
      "grad_norm": 73.9774169921875,
      "learning_rate": 8.795973674022456e-06,
      "loss": 3.226,
      "step": 5382
    },
    {
      "epoch": 2.084010840108401,
      "grad_norm": 14.40211296081543,
      "learning_rate": 8.795543510990666e-06,
      "loss": 0.8074,
      "step": 5383
    },
    {
      "epoch": 2.0843979868370113,
      "grad_norm": 15.099727630615234,
      "learning_rate": 8.795113347958877e-06,
      "loss": 0.6134,
      "step": 5384
    },
    {
      "epoch": 2.0847851335656213,
      "grad_norm": 16.526214599609375,
      "learning_rate": 8.794683184927088e-06,
      "loss": 0.8672,
      "step": 5385
    },
    {
      "epoch": 2.0851722802942314,
      "grad_norm": 13.781281471252441,
      "learning_rate": 8.7942530218953e-06,
      "loss": 0.6713,
      "step": 5386
    },
    {
      "epoch": 2.0855594270228415,
      "grad_norm": 19.45653533935547,
      "learning_rate": 8.79382285886351e-06,
      "loss": 0.8441,
      "step": 5387
    },
    {
      "epoch": 2.085946573751452,
      "grad_norm": 14.004570960998535,
      "learning_rate": 8.793392695831721e-06,
      "loss": 1.4227,
      "step": 5388
    },
    {
      "epoch": 2.086333720480062,
      "grad_norm": 21.63005828857422,
      "learning_rate": 8.792962532799932e-06,
      "loss": 1.9346,
      "step": 5389
    },
    {
      "epoch": 2.086720867208672,
      "grad_norm": 54.84817123413086,
      "learning_rate": 8.792532369768144e-06,
      "loss": 0.8205,
      "step": 5390
    },
    {
      "epoch": 2.087108013937282,
      "grad_norm": 17.265274047851562,
      "learning_rate": 8.792102206736354e-06,
      "loss": 1.5755,
      "step": 5391
    },
    {
      "epoch": 2.0874951606658922,
      "grad_norm": 13.342823028564453,
      "learning_rate": 8.791672043704565e-06,
      "loss": 1.0689,
      "step": 5392
    },
    {
      "epoch": 2.0878823073945023,
      "grad_norm": 15.423653602600098,
      "learning_rate": 8.791241880672776e-06,
      "loss": 1.3051,
      "step": 5393
    },
    {
      "epoch": 2.088269454123113,
      "grad_norm": 18.495588302612305,
      "learning_rate": 8.790811717640986e-06,
      "loss": 1.5523,
      "step": 5394
    },
    {
      "epoch": 2.088656600851723,
      "grad_norm": 21.20388412475586,
      "learning_rate": 8.790381554609197e-06,
      "loss": 1.4601,
      "step": 5395
    },
    {
      "epoch": 2.089043747580333,
      "grad_norm": 22.553314208984375,
      "learning_rate": 8.789951391577409e-06,
      "loss": 1.2859,
      "step": 5396
    },
    {
      "epoch": 2.089430894308943,
      "grad_norm": 25.889089584350586,
      "learning_rate": 8.78952122854562e-06,
      "loss": 2.007,
      "step": 5397
    },
    {
      "epoch": 2.089818041037553,
      "grad_norm": 33.90541458129883,
      "learning_rate": 8.78909106551383e-06,
      "loss": 2.275,
      "step": 5398
    },
    {
      "epoch": 2.0902051877661636,
      "grad_norm": 27.966970443725586,
      "learning_rate": 8.788660902482041e-06,
      "loss": 1.8818,
      "step": 5399
    },
    {
      "epoch": 2.0905923344947737,
      "grad_norm": 11.04706859588623,
      "learning_rate": 8.788230739450251e-06,
      "loss": 0.7209,
      "step": 5400
    },
    {
      "epoch": 2.0909794812233837,
      "grad_norm": 8.585047721862793,
      "learning_rate": 8.787800576418464e-06,
      "loss": 0.2584,
      "step": 5401
    },
    {
      "epoch": 2.091366627951994,
      "grad_norm": 19.9748592376709,
      "learning_rate": 8.787370413386674e-06,
      "loss": 1.6743,
      "step": 5402
    },
    {
      "epoch": 2.091753774680604,
      "grad_norm": 16.9166316986084,
      "learning_rate": 8.786940250354885e-06,
      "loss": 1.2344,
      "step": 5403
    },
    {
      "epoch": 2.092140921409214,
      "grad_norm": 20.011934280395508,
      "learning_rate": 8.786510087323095e-06,
      "loss": 0.678,
      "step": 5404
    },
    {
      "epoch": 2.0925280681378244,
      "grad_norm": 17.061737060546875,
      "learning_rate": 8.786079924291308e-06,
      "loss": 1.5077,
      "step": 5405
    },
    {
      "epoch": 2.0929152148664345,
      "grad_norm": 16.80703353881836,
      "learning_rate": 8.785649761259518e-06,
      "loss": 1.0763,
      "step": 5406
    },
    {
      "epoch": 2.0933023615950446,
      "grad_norm": 11.483606338500977,
      "learning_rate": 8.78521959822773e-06,
      "loss": 0.7078,
      "step": 5407
    },
    {
      "epoch": 2.0936895083236546,
      "grad_norm": 19.668214797973633,
      "learning_rate": 8.784789435195939e-06,
      "loss": 1.6308,
      "step": 5408
    },
    {
      "epoch": 2.0940766550522647,
      "grad_norm": 5.619824409484863,
      "learning_rate": 8.78435927216415e-06,
      "loss": 0.3868,
      "step": 5409
    },
    {
      "epoch": 2.0944638017808748,
      "grad_norm": 13.54746150970459,
      "learning_rate": 8.783929109132362e-06,
      "loss": 0.9687,
      "step": 5410
    },
    {
      "epoch": 2.0948509485094853,
      "grad_norm": 12.335101127624512,
      "learning_rate": 8.783498946100573e-06,
      "loss": 0.8519,
      "step": 5411
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 42.042633056640625,
      "learning_rate": 8.783068783068783e-06,
      "loss": 1.1356,
      "step": 5412
    },
    {
      "epoch": 2.0956252419667054,
      "grad_norm": 41.393798828125,
      "learning_rate": 8.782638620036994e-06,
      "loss": 1.7339,
      "step": 5413
    },
    {
      "epoch": 2.0960123886953155,
      "grad_norm": 51.41362380981445,
      "learning_rate": 8.782208457005206e-06,
      "loss": 3.375,
      "step": 5414
    },
    {
      "epoch": 2.0963995354239255,
      "grad_norm": 35.3565788269043,
      "learning_rate": 8.781778293973416e-06,
      "loss": 1.6827,
      "step": 5415
    },
    {
      "epoch": 2.0967866821525356,
      "grad_norm": 23.36135482788086,
      "learning_rate": 8.781348130941627e-06,
      "loss": 1.138,
      "step": 5416
    },
    {
      "epoch": 2.097173828881146,
      "grad_norm": 19.360137939453125,
      "learning_rate": 8.780917967909838e-06,
      "loss": 2.074,
      "step": 5417
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 16.060028076171875,
      "learning_rate": 8.78048780487805e-06,
      "loss": 1.0176,
      "step": 5418
    },
    {
      "epoch": 2.0979481223383663,
      "grad_norm": 15.319169044494629,
      "learning_rate": 8.78005764184626e-06,
      "loss": 1.3613,
      "step": 5419
    },
    {
      "epoch": 2.0983352690669763,
      "grad_norm": 13.840097427368164,
      "learning_rate": 8.779627478814471e-06,
      "loss": 1.0997,
      "step": 5420
    },
    {
      "epoch": 2.0987224157955864,
      "grad_norm": 8.894158363342285,
      "learning_rate": 8.779197315782682e-06,
      "loss": 0.4358,
      "step": 5421
    },
    {
      "epoch": 2.0991095625241964,
      "grad_norm": 17.758323669433594,
      "learning_rate": 8.778767152750894e-06,
      "loss": 0.8594,
      "step": 5422
    },
    {
      "epoch": 2.099496709252807,
      "grad_norm": 22.658916473388672,
      "learning_rate": 8.778336989719104e-06,
      "loss": 1.9844,
      "step": 5423
    },
    {
      "epoch": 2.099883855981417,
      "grad_norm": 17.941083908081055,
      "learning_rate": 8.777906826687315e-06,
      "loss": 2.1002,
      "step": 5424
    },
    {
      "epoch": 2.100271002710027,
      "grad_norm": 13.935158729553223,
      "learning_rate": 8.777476663655526e-06,
      "loss": 1.3904,
      "step": 5425
    },
    {
      "epoch": 2.100658149438637,
      "grad_norm": 48.92192459106445,
      "learning_rate": 8.777046500623738e-06,
      "loss": 1.8698,
      "step": 5426
    },
    {
      "epoch": 2.1010452961672472,
      "grad_norm": 22.83053207397461,
      "learning_rate": 8.776616337591948e-06,
      "loss": 1.4431,
      "step": 5427
    },
    {
      "epoch": 2.1014324428958577,
      "grad_norm": 13.815629959106445,
      "learning_rate": 8.776186174560159e-06,
      "loss": 0.8598,
      "step": 5428
    },
    {
      "epoch": 2.101819589624468,
      "grad_norm": 27.469825744628906,
      "learning_rate": 8.77575601152837e-06,
      "loss": 2.9488,
      "step": 5429
    },
    {
      "epoch": 2.102206736353078,
      "grad_norm": 17.938079833984375,
      "learning_rate": 8.77532584849658e-06,
      "loss": 1.0738,
      "step": 5430
    },
    {
      "epoch": 2.102593883081688,
      "grad_norm": 47.58565139770508,
      "learning_rate": 8.774895685464791e-06,
      "loss": 1.5236,
      "step": 5431
    },
    {
      "epoch": 2.102981029810298,
      "grad_norm": 59.06420135498047,
      "learning_rate": 8.774465522433003e-06,
      "loss": 1.8426,
      "step": 5432
    },
    {
      "epoch": 2.103368176538908,
      "grad_norm": 11.765705108642578,
      "learning_rate": 8.774035359401214e-06,
      "loss": 1.0819,
      "step": 5433
    },
    {
      "epoch": 2.1037553232675186,
      "grad_norm": 64.98958587646484,
      "learning_rate": 8.773605196369424e-06,
      "loss": 3.0766,
      "step": 5434
    },
    {
      "epoch": 2.1041424699961286,
      "grad_norm": 37.77132797241211,
      "learning_rate": 8.773175033337635e-06,
      "loss": 1.565,
      "step": 5435
    },
    {
      "epoch": 2.1045296167247387,
      "grad_norm": 22.61358070373535,
      "learning_rate": 8.772744870305847e-06,
      "loss": 1.4287,
      "step": 5436
    },
    {
      "epoch": 2.1049167634533488,
      "grad_norm": 29.527027130126953,
      "learning_rate": 8.772314707274058e-06,
      "loss": 3.3448,
      "step": 5437
    },
    {
      "epoch": 2.105303910181959,
      "grad_norm": 26.23744010925293,
      "learning_rate": 8.771884544242268e-06,
      "loss": 1.7201,
      "step": 5438
    },
    {
      "epoch": 2.105691056910569,
      "grad_norm": 13.761069297790527,
      "learning_rate": 8.77145438121048e-06,
      "loss": 0.4405,
      "step": 5439
    },
    {
      "epoch": 2.1060782036391794,
      "grad_norm": 14.765442848205566,
      "learning_rate": 8.771024218178691e-06,
      "loss": 0.8456,
      "step": 5440
    },
    {
      "epoch": 2.1064653503677895,
      "grad_norm": 36.09375,
      "learning_rate": 8.770594055146902e-06,
      "loss": 2.3753,
      "step": 5441
    },
    {
      "epoch": 2.1068524970963995,
      "grad_norm": 15.58292293548584,
      "learning_rate": 8.770163892115112e-06,
      "loss": 1.0133,
      "step": 5442
    },
    {
      "epoch": 2.1072396438250096,
      "grad_norm": 15.964179039001465,
      "learning_rate": 8.769733729083323e-06,
      "loss": 0.9041,
      "step": 5443
    },
    {
      "epoch": 2.1076267905536197,
      "grad_norm": 16.190465927124023,
      "learning_rate": 8.769303566051535e-06,
      "loss": 0.6601,
      "step": 5444
    },
    {
      "epoch": 2.10801393728223,
      "grad_norm": 24.068330764770508,
      "learning_rate": 8.768873403019745e-06,
      "loss": 1.7745,
      "step": 5445
    },
    {
      "epoch": 2.1084010840108403,
      "grad_norm": 21.25668716430664,
      "learning_rate": 8.768443239987956e-06,
      "loss": 2.5358,
      "step": 5446
    },
    {
      "epoch": 2.1087882307394503,
      "grad_norm": 15.601598739624023,
      "learning_rate": 8.768013076956167e-06,
      "loss": 0.9148,
      "step": 5447
    },
    {
      "epoch": 2.1091753774680604,
      "grad_norm": 20.018814086914062,
      "learning_rate": 8.767582913924379e-06,
      "loss": 1.128,
      "step": 5448
    },
    {
      "epoch": 2.1095625241966705,
      "grad_norm": 20.49239158630371,
      "learning_rate": 8.767152750892589e-06,
      "loss": 1.3102,
      "step": 5449
    },
    {
      "epoch": 2.1099496709252805,
      "grad_norm": 22.393020629882812,
      "learning_rate": 8.7667225878608e-06,
      "loss": 1.7171,
      "step": 5450
    },
    {
      "epoch": 2.110336817653891,
      "grad_norm": 11.805240631103516,
      "learning_rate": 8.76629242482901e-06,
      "loss": 0.6894,
      "step": 5451
    },
    {
      "epoch": 2.110723964382501,
      "grad_norm": 13.046709060668945,
      "learning_rate": 8.765862261797223e-06,
      "loss": 1.29,
      "step": 5452
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 28.36259651184082,
      "learning_rate": 8.765432098765432e-06,
      "loss": 1.8487,
      "step": 5453
    },
    {
      "epoch": 2.1114982578397212,
      "grad_norm": 24.32274627685547,
      "learning_rate": 8.765001935733644e-06,
      "loss": 1.5336,
      "step": 5454
    },
    {
      "epoch": 2.1118854045683313,
      "grad_norm": 19.071569442749023,
      "learning_rate": 8.764571772701854e-06,
      "loss": 2.4499,
      "step": 5455
    },
    {
      "epoch": 2.1122725512969414,
      "grad_norm": 31.19804573059082,
      "learning_rate": 8.764141609670067e-06,
      "loss": 1.5142,
      "step": 5456
    },
    {
      "epoch": 2.112659698025552,
      "grad_norm": 35.0242919921875,
      "learning_rate": 8.763711446638276e-06,
      "loss": 1.9195,
      "step": 5457
    },
    {
      "epoch": 2.113046844754162,
      "grad_norm": 12.120471000671387,
      "learning_rate": 8.763281283606488e-06,
      "loss": 0.9384,
      "step": 5458
    },
    {
      "epoch": 2.113433991482772,
      "grad_norm": 22.270343780517578,
      "learning_rate": 8.762851120574698e-06,
      "loss": 1.9563,
      "step": 5459
    },
    {
      "epoch": 2.113821138211382,
      "grad_norm": 14.313211441040039,
      "learning_rate": 8.762420957542909e-06,
      "loss": 1.1841,
      "step": 5460
    },
    {
      "epoch": 2.114208284939992,
      "grad_norm": 24.11211585998535,
      "learning_rate": 8.76199079451112e-06,
      "loss": 1.6221,
      "step": 5461
    },
    {
      "epoch": 2.114595431668602,
      "grad_norm": 50.964412689208984,
      "learning_rate": 8.761560631479332e-06,
      "loss": 1.7632,
      "step": 5462
    },
    {
      "epoch": 2.1149825783972127,
      "grad_norm": 13.555194854736328,
      "learning_rate": 8.761130468447542e-06,
      "loss": 1.4486,
      "step": 5463
    },
    {
      "epoch": 2.1153697251258228,
      "grad_norm": 15.29783821105957,
      "learning_rate": 8.760700305415753e-06,
      "loss": 1.459,
      "step": 5464
    },
    {
      "epoch": 2.115756871854433,
      "grad_norm": 13.238360404968262,
      "learning_rate": 8.760270142383964e-06,
      "loss": 0.7941,
      "step": 5465
    },
    {
      "epoch": 2.116144018583043,
      "grad_norm": 12.717109680175781,
      "learning_rate": 8.759839979352174e-06,
      "loss": 1.4103,
      "step": 5466
    },
    {
      "epoch": 2.116531165311653,
      "grad_norm": 22.96436309814453,
      "learning_rate": 8.759409816320387e-06,
      "loss": 2.3989,
      "step": 5467
    },
    {
      "epoch": 2.116918312040263,
      "grad_norm": 27.585935592651367,
      "learning_rate": 8.758979653288597e-06,
      "loss": 1.1663,
      "step": 5468
    },
    {
      "epoch": 2.1173054587688735,
      "grad_norm": 25.236948013305664,
      "learning_rate": 8.758549490256808e-06,
      "loss": 2.1201,
      "step": 5469
    },
    {
      "epoch": 2.1176926054974836,
      "grad_norm": 14.442621231079102,
      "learning_rate": 8.758119327225018e-06,
      "loss": 0.6983,
      "step": 5470
    },
    {
      "epoch": 2.1180797522260937,
      "grad_norm": 13.609702110290527,
      "learning_rate": 8.757689164193231e-06,
      "loss": 1.2189,
      "step": 5471
    },
    {
      "epoch": 2.1184668989547037,
      "grad_norm": 25.2722110748291,
      "learning_rate": 8.757259001161441e-06,
      "loss": 2.0583,
      "step": 5472
    },
    {
      "epoch": 2.118854045683314,
      "grad_norm": 18.350017547607422,
      "learning_rate": 8.756828838129652e-06,
      "loss": 0.8601,
      "step": 5473
    },
    {
      "epoch": 2.1192411924119243,
      "grad_norm": 41.23414611816406,
      "learning_rate": 8.756398675097862e-06,
      "loss": 1.1201,
      "step": 5474
    },
    {
      "epoch": 2.1196283391405344,
      "grad_norm": 14.632118225097656,
      "learning_rate": 8.755968512066073e-06,
      "loss": 1.2879,
      "step": 5475
    },
    {
      "epoch": 2.1200154858691445,
      "grad_norm": 29.743772506713867,
      "learning_rate": 8.755538349034285e-06,
      "loss": 2.0598,
      "step": 5476
    },
    {
      "epoch": 2.1204026325977545,
      "grad_norm": 30.44284439086914,
      "learning_rate": 8.755108186002496e-06,
      "loss": 1.682,
      "step": 5477
    },
    {
      "epoch": 2.1207897793263646,
      "grad_norm": 14.831629753112793,
      "learning_rate": 8.754678022970706e-06,
      "loss": 0.9774,
      "step": 5478
    },
    {
      "epoch": 2.1211769260549747,
      "grad_norm": 21.74472999572754,
      "learning_rate": 8.754247859938917e-06,
      "loss": 1.5338,
      "step": 5479
    },
    {
      "epoch": 2.121564072783585,
      "grad_norm": 20.183528900146484,
      "learning_rate": 8.753817696907129e-06,
      "loss": 1.7652,
      "step": 5480
    },
    {
      "epoch": 2.1219512195121952,
      "grad_norm": 25.81972312927246,
      "learning_rate": 8.753387533875339e-06,
      "loss": 2.6645,
      "step": 5481
    },
    {
      "epoch": 2.1223383662408053,
      "grad_norm": 24.860170364379883,
      "learning_rate": 8.75295737084355e-06,
      "loss": 2.381,
      "step": 5482
    },
    {
      "epoch": 2.1227255129694154,
      "grad_norm": 13.859825134277344,
      "learning_rate": 8.752527207811761e-06,
      "loss": 1.2708,
      "step": 5483
    },
    {
      "epoch": 2.1231126596980254,
      "grad_norm": 37.47441864013672,
      "learning_rate": 8.752097044779973e-06,
      "loss": 1.6675,
      "step": 5484
    },
    {
      "epoch": 2.1234998064266355,
      "grad_norm": 28.195186614990234,
      "learning_rate": 8.751666881748183e-06,
      "loss": 1.5722,
      "step": 5485
    },
    {
      "epoch": 2.123886953155246,
      "grad_norm": 23.275482177734375,
      "learning_rate": 8.751236718716394e-06,
      "loss": 1.2593,
      "step": 5486
    },
    {
      "epoch": 2.124274099883856,
      "grad_norm": 23.3780460357666,
      "learning_rate": 8.750806555684605e-06,
      "loss": 1.4952,
      "step": 5487
    },
    {
      "epoch": 2.124661246612466,
      "grad_norm": 20.013765335083008,
      "learning_rate": 8.750376392652817e-06,
      "loss": 1.6609,
      "step": 5488
    },
    {
      "epoch": 2.125048393341076,
      "grad_norm": 43.94383239746094,
      "learning_rate": 8.749946229621027e-06,
      "loss": 1.3972,
      "step": 5489
    },
    {
      "epoch": 2.1254355400696863,
      "grad_norm": 41.841487884521484,
      "learning_rate": 8.749516066589238e-06,
      "loss": 2.6057,
      "step": 5490
    },
    {
      "epoch": 2.125822686798297,
      "grad_norm": 36.100860595703125,
      "learning_rate": 8.74908590355745e-06,
      "loss": 2.0067,
      "step": 5491
    },
    {
      "epoch": 2.126209833526907,
      "grad_norm": 42.83831787109375,
      "learning_rate": 8.74865574052566e-06,
      "loss": 1.7716,
      "step": 5492
    },
    {
      "epoch": 2.126596980255517,
      "grad_norm": 12.032014846801758,
      "learning_rate": 8.74822557749387e-06,
      "loss": 0.8978,
      "step": 5493
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 13.873848915100098,
      "learning_rate": 8.747795414462082e-06,
      "loss": 1.3049,
      "step": 5494
    },
    {
      "epoch": 2.127371273712737,
      "grad_norm": 20.320743560791016,
      "learning_rate": 8.747365251430293e-06,
      "loss": 1.4803,
      "step": 5495
    },
    {
      "epoch": 2.127758420441347,
      "grad_norm": 15.638463020324707,
      "learning_rate": 8.746935088398503e-06,
      "loss": 1.3427,
      "step": 5496
    },
    {
      "epoch": 2.1281455671699576,
      "grad_norm": 30.578554153442383,
      "learning_rate": 8.746504925366714e-06,
      "loss": 1.9758,
      "step": 5497
    },
    {
      "epoch": 2.1285327138985677,
      "grad_norm": 20.487905502319336,
      "learning_rate": 8.746074762334926e-06,
      "loss": 1.7454,
      "step": 5498
    },
    {
      "epoch": 2.1289198606271778,
      "grad_norm": 13.150039672851562,
      "learning_rate": 8.745644599303137e-06,
      "loss": 1.0678,
      "step": 5499
    },
    {
      "epoch": 2.129307007355788,
      "grad_norm": 13.339587211608887,
      "learning_rate": 8.745214436271347e-06,
      "loss": 0.8516,
      "step": 5500
    },
    {
      "epoch": 2.129694154084398,
      "grad_norm": 13.905889511108398,
      "learning_rate": 8.744784273239558e-06,
      "loss": 1.3508,
      "step": 5501
    },
    {
      "epoch": 2.130081300813008,
      "grad_norm": 23.817304611206055,
      "learning_rate": 8.744354110207768e-06,
      "loss": 1.4052,
      "step": 5502
    },
    {
      "epoch": 2.1304684475416185,
      "grad_norm": 14.730607032775879,
      "learning_rate": 8.743923947175981e-06,
      "loss": 0.5375,
      "step": 5503
    },
    {
      "epoch": 2.1308555942702285,
      "grad_norm": 12.886999130249023,
      "learning_rate": 8.743493784144191e-06,
      "loss": 0.577,
      "step": 5504
    },
    {
      "epoch": 2.1312427409988386,
      "grad_norm": 12.96403694152832,
      "learning_rate": 8.743063621112402e-06,
      "loss": 0.9199,
      "step": 5505
    },
    {
      "epoch": 2.1316298877274487,
      "grad_norm": 29.194421768188477,
      "learning_rate": 8.742633458080614e-06,
      "loss": 2.0108,
      "step": 5506
    },
    {
      "epoch": 2.1320170344560587,
      "grad_norm": 13.44262409210205,
      "learning_rate": 8.742203295048825e-06,
      "loss": 1.4728,
      "step": 5507
    },
    {
      "epoch": 2.132404181184669,
      "grad_norm": 15.553369522094727,
      "learning_rate": 8.741773132017035e-06,
      "loss": 0.9904,
      "step": 5508
    },
    {
      "epoch": 2.1327913279132793,
      "grad_norm": 35.55663299560547,
      "learning_rate": 8.741342968985246e-06,
      "loss": 1.5654,
      "step": 5509
    },
    {
      "epoch": 2.1331784746418894,
      "grad_norm": 21.96167755126953,
      "learning_rate": 8.740912805953458e-06,
      "loss": 1.602,
      "step": 5510
    },
    {
      "epoch": 2.1335656213704994,
      "grad_norm": 26.394311904907227,
      "learning_rate": 8.740482642921667e-06,
      "loss": 1.5324,
      "step": 5511
    },
    {
      "epoch": 2.1339527680991095,
      "grad_norm": 27.62633514404297,
      "learning_rate": 8.740052479889879e-06,
      "loss": 1.1431,
      "step": 5512
    },
    {
      "epoch": 2.1343399148277196,
      "grad_norm": 13.809944152832031,
      "learning_rate": 8.73962231685809e-06,
      "loss": 0.9114,
      "step": 5513
    },
    {
      "epoch": 2.1347270615563296,
      "grad_norm": 17.73818588256836,
      "learning_rate": 8.739192153826302e-06,
      "loss": 1.5264,
      "step": 5514
    },
    {
      "epoch": 2.13511420828494,
      "grad_norm": 24.234107971191406,
      "learning_rate": 8.738761990794511e-06,
      "loss": 1.1306,
      "step": 5515
    },
    {
      "epoch": 2.13550135501355,
      "grad_norm": 19.172435760498047,
      "learning_rate": 8.738331827762723e-06,
      "loss": 1.1303,
      "step": 5516
    },
    {
      "epoch": 2.1358885017421603,
      "grad_norm": 16.59514617919922,
      "learning_rate": 8.737901664730933e-06,
      "loss": 1.476,
      "step": 5517
    },
    {
      "epoch": 2.1362756484707703,
      "grad_norm": 23.145198822021484,
      "learning_rate": 8.737471501699146e-06,
      "loss": 1.2811,
      "step": 5518
    },
    {
      "epoch": 2.1366627951993804,
      "grad_norm": 52.81734848022461,
      "learning_rate": 8.737041338667355e-06,
      "loss": 2.0819,
      "step": 5519
    },
    {
      "epoch": 2.137049941927991,
      "grad_norm": 10.682951927185059,
      "learning_rate": 8.736611175635567e-06,
      "loss": 1.0315,
      "step": 5520
    },
    {
      "epoch": 2.137437088656601,
      "grad_norm": 17.709924697875977,
      "learning_rate": 8.736181012603777e-06,
      "loss": 1.0862,
      "step": 5521
    },
    {
      "epoch": 2.137824235385211,
      "grad_norm": 19.34964942932129,
      "learning_rate": 8.73575084957199e-06,
      "loss": 1.8373,
      "step": 5522
    },
    {
      "epoch": 2.138211382113821,
      "grad_norm": 47.531272888183594,
      "learning_rate": 8.7353206865402e-06,
      "loss": 2.2186,
      "step": 5523
    },
    {
      "epoch": 2.138598528842431,
      "grad_norm": 17.673538208007812,
      "learning_rate": 8.73489052350841e-06,
      "loss": 1.0564,
      "step": 5524
    },
    {
      "epoch": 2.1389856755710412,
      "grad_norm": 12.939813613891602,
      "learning_rate": 8.73446036047662e-06,
      "loss": 0.909,
      "step": 5525
    },
    {
      "epoch": 2.1393728222996518,
      "grad_norm": 18.389057159423828,
      "learning_rate": 8.734030197444832e-06,
      "loss": 1.4286,
      "step": 5526
    },
    {
      "epoch": 2.139759969028262,
      "grad_norm": 6.385679244995117,
      "learning_rate": 8.733600034413043e-06,
      "loss": 0.3555,
      "step": 5527
    },
    {
      "epoch": 2.140147115756872,
      "grad_norm": 16.221738815307617,
      "learning_rate": 8.733169871381255e-06,
      "loss": 0.8675,
      "step": 5528
    },
    {
      "epoch": 2.140534262485482,
      "grad_norm": 20.534563064575195,
      "learning_rate": 8.732739708349465e-06,
      "loss": 1.4087,
      "step": 5529
    },
    {
      "epoch": 2.140921409214092,
      "grad_norm": 16.991741180419922,
      "learning_rate": 8.732309545317676e-06,
      "loss": 1.5793,
      "step": 5530
    },
    {
      "epoch": 2.141308555942702,
      "grad_norm": 29.558692932128906,
      "learning_rate": 8.731879382285887e-06,
      "loss": 2.1722,
      "step": 5531
    },
    {
      "epoch": 2.1416957026713126,
      "grad_norm": 20.335899353027344,
      "learning_rate": 8.731449219254097e-06,
      "loss": 1.6276,
      "step": 5532
    },
    {
      "epoch": 2.1420828493999227,
      "grad_norm": 19.3277530670166,
      "learning_rate": 8.731019056222308e-06,
      "loss": 2.1412,
      "step": 5533
    },
    {
      "epoch": 2.1424699961285327,
      "grad_norm": 19.572973251342773,
      "learning_rate": 8.73058889319052e-06,
      "loss": 1.5771,
      "step": 5534
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 18.54538345336914,
      "learning_rate": 8.730158730158731e-06,
      "loss": 1.095,
      "step": 5535
    },
    {
      "epoch": 2.143244289585753,
      "grad_norm": 37.16446304321289,
      "learning_rate": 8.729728567126941e-06,
      "loss": 1.6997,
      "step": 5536
    },
    {
      "epoch": 2.1436314363143634,
      "grad_norm": 14.078298568725586,
      "learning_rate": 8.729298404095152e-06,
      "loss": 1.3007,
      "step": 5537
    },
    {
      "epoch": 2.1440185830429734,
      "grad_norm": 15.868370056152344,
      "learning_rate": 8.728868241063364e-06,
      "loss": 1.6534,
      "step": 5538
    },
    {
      "epoch": 2.1444057297715835,
      "grad_norm": 10.215500831604004,
      "learning_rate": 8.728438078031575e-06,
      "loss": 0.9436,
      "step": 5539
    },
    {
      "epoch": 2.1447928765001936,
      "grad_norm": 25.79248046875,
      "learning_rate": 8.728007914999785e-06,
      "loss": 1.8894,
      "step": 5540
    },
    {
      "epoch": 2.1451800232288036,
      "grad_norm": 12.571565628051758,
      "learning_rate": 8.727577751967996e-06,
      "loss": 0.9179,
      "step": 5541
    },
    {
      "epoch": 2.1455671699574137,
      "grad_norm": 15.511063575744629,
      "learning_rate": 8.727147588936208e-06,
      "loss": 1.5313,
      "step": 5542
    },
    {
      "epoch": 2.145954316686024,
      "grad_norm": 13.670872688293457,
      "learning_rate": 8.72671742590442e-06,
      "loss": 0.9248,
      "step": 5543
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 19.23642921447754,
      "learning_rate": 8.726287262872629e-06,
      "loss": 1.6542,
      "step": 5544
    },
    {
      "epoch": 2.1467286101432443,
      "grad_norm": 13.490459442138672,
      "learning_rate": 8.72585709984084e-06,
      "loss": 0.6661,
      "step": 5545
    },
    {
      "epoch": 2.1471157568718544,
      "grad_norm": 23.750993728637695,
      "learning_rate": 8.725426936809052e-06,
      "loss": 1.3655,
      "step": 5546
    },
    {
      "epoch": 2.1475029036004645,
      "grad_norm": 16.953718185424805,
      "learning_rate": 8.724996773777262e-06,
      "loss": 1.5309,
      "step": 5547
    },
    {
      "epoch": 2.1478900503290745,
      "grad_norm": 13.342262268066406,
      "learning_rate": 8.724566610745473e-06,
      "loss": 0.9818,
      "step": 5548
    },
    {
      "epoch": 2.148277197057685,
      "grad_norm": 17.175289154052734,
      "learning_rate": 8.724136447713684e-06,
      "loss": 1.4248,
      "step": 5549
    },
    {
      "epoch": 2.148664343786295,
      "grad_norm": 12.379402160644531,
      "learning_rate": 8.723706284681896e-06,
      "loss": 0.9004,
      "step": 5550
    },
    {
      "epoch": 2.149051490514905,
      "grad_norm": 16.12058448791504,
      "learning_rate": 8.723276121650105e-06,
      "loss": 0.9872,
      "step": 5551
    },
    {
      "epoch": 2.1494386372435152,
      "grad_norm": 24.238033294677734,
      "learning_rate": 8.722845958618317e-06,
      "loss": 2.56,
      "step": 5552
    },
    {
      "epoch": 2.1498257839721253,
      "grad_norm": 15.820018768310547,
      "learning_rate": 8.722415795586528e-06,
      "loss": 1.4206,
      "step": 5553
    },
    {
      "epoch": 2.1502129307007354,
      "grad_norm": 20.745725631713867,
      "learning_rate": 8.72198563255474e-06,
      "loss": 1.6712,
      "step": 5554
    },
    {
      "epoch": 2.150600077429346,
      "grad_norm": 15.648917198181152,
      "learning_rate": 8.72155546952295e-06,
      "loss": 1.2458,
      "step": 5555
    },
    {
      "epoch": 2.150987224157956,
      "grad_norm": 30.16804313659668,
      "learning_rate": 8.721125306491161e-06,
      "loss": 1.5599,
      "step": 5556
    },
    {
      "epoch": 2.151374370886566,
      "grad_norm": 20.939022064208984,
      "learning_rate": 8.720695143459372e-06,
      "loss": 1.8141,
      "step": 5557
    },
    {
      "epoch": 2.151761517615176,
      "grad_norm": 29.85820198059082,
      "learning_rate": 8.720264980427584e-06,
      "loss": 1.7023,
      "step": 5558
    },
    {
      "epoch": 2.152148664343786,
      "grad_norm": 14.379895210266113,
      "learning_rate": 8.719834817395793e-06,
      "loss": 0.9776,
      "step": 5559
    },
    {
      "epoch": 2.152535811072396,
      "grad_norm": 20.128273010253906,
      "learning_rate": 8.719404654364005e-06,
      "loss": 1.0758,
      "step": 5560
    },
    {
      "epoch": 2.1529229578010067,
      "grad_norm": 15.519817352294922,
      "learning_rate": 8.718974491332216e-06,
      "loss": 0.8798,
      "step": 5561
    },
    {
      "epoch": 2.153310104529617,
      "grad_norm": 11.657852172851562,
      "learning_rate": 8.718544328300426e-06,
      "loss": 0.4112,
      "step": 5562
    },
    {
      "epoch": 2.153697251258227,
      "grad_norm": 73.40205383300781,
      "learning_rate": 8.718114165268637e-06,
      "loss": 1.7303,
      "step": 5563
    },
    {
      "epoch": 2.154084397986837,
      "grad_norm": 14.569931030273438,
      "learning_rate": 8.717684002236849e-06,
      "loss": 1.4153,
      "step": 5564
    },
    {
      "epoch": 2.154471544715447,
      "grad_norm": 27.54849624633789,
      "learning_rate": 8.71725383920506e-06,
      "loss": 3.6219,
      "step": 5565
    },
    {
      "epoch": 2.1548586914440575,
      "grad_norm": 12.5906982421875,
      "learning_rate": 8.71682367617327e-06,
      "loss": 0.7357,
      "step": 5566
    },
    {
      "epoch": 2.1552458381726676,
      "grad_norm": 24.073684692382812,
      "learning_rate": 8.716393513141481e-06,
      "loss": 2.0947,
      "step": 5567
    },
    {
      "epoch": 2.1556329849012776,
      "grad_norm": 67.25160217285156,
      "learning_rate": 8.715963350109691e-06,
      "loss": 1.0477,
      "step": 5568
    },
    {
      "epoch": 2.1560201316298877,
      "grad_norm": 27.85824966430664,
      "learning_rate": 8.715533187077904e-06,
      "loss": 1.8308,
      "step": 5569
    },
    {
      "epoch": 2.1564072783584978,
      "grad_norm": 18.09551239013672,
      "learning_rate": 8.715103024046114e-06,
      "loss": 1.7496,
      "step": 5570
    },
    {
      "epoch": 2.156794425087108,
      "grad_norm": 15.444225311279297,
      "learning_rate": 8.714672861014325e-06,
      "loss": 1.4756,
      "step": 5571
    },
    {
      "epoch": 2.1571815718157183,
      "grad_norm": 66.61254119873047,
      "learning_rate": 8.714242697982535e-06,
      "loss": 1.1629,
      "step": 5572
    },
    {
      "epoch": 2.1575687185443284,
      "grad_norm": 33.25580596923828,
      "learning_rate": 8.713812534950748e-06,
      "loss": 1.5504,
      "step": 5573
    },
    {
      "epoch": 2.1579558652729385,
      "grad_norm": 14.610329627990723,
      "learning_rate": 8.713382371918958e-06,
      "loss": 0.949,
      "step": 5574
    },
    {
      "epoch": 2.1583430120015485,
      "grad_norm": 33.33892822265625,
      "learning_rate": 8.71295220888717e-06,
      "loss": 1.1816,
      "step": 5575
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 15.902046203613281,
      "learning_rate": 8.712522045855379e-06,
      "loss": 1.3204,
      "step": 5576
    },
    {
      "epoch": 2.1591173054587687,
      "grad_norm": 30.252845764160156,
      "learning_rate": 8.71209188282359e-06,
      "loss": 1.7609,
      "step": 5577
    },
    {
      "epoch": 2.159504452187379,
      "grad_norm": 15.364675521850586,
      "learning_rate": 8.711661719791802e-06,
      "loss": 1.2388,
      "step": 5578
    },
    {
      "epoch": 2.1598915989159893,
      "grad_norm": 17.906232833862305,
      "learning_rate": 8.711231556760013e-06,
      "loss": 1.5829,
      "step": 5579
    },
    {
      "epoch": 2.1602787456445993,
      "grad_norm": 14.334857940673828,
      "learning_rate": 8.710801393728223e-06,
      "loss": 1.3896,
      "step": 5580
    },
    {
      "epoch": 2.1606658923732094,
      "grad_norm": 71.65946960449219,
      "learning_rate": 8.710371230696434e-06,
      "loss": 2.047,
      "step": 5581
    },
    {
      "epoch": 2.1610530391018195,
      "grad_norm": 13.809273719787598,
      "learning_rate": 8.709941067664646e-06,
      "loss": 0.423,
      "step": 5582
    },
    {
      "epoch": 2.16144018583043,
      "grad_norm": 39.18682861328125,
      "learning_rate": 8.709510904632856e-06,
      "loss": 2.2343,
      "step": 5583
    },
    {
      "epoch": 2.16182733255904,
      "grad_norm": 21.373743057250977,
      "learning_rate": 8.709080741601067e-06,
      "loss": 1.5061,
      "step": 5584
    },
    {
      "epoch": 2.16221447928765,
      "grad_norm": 24.526430130004883,
      "learning_rate": 8.708650578569278e-06,
      "loss": 1.4169,
      "step": 5585
    },
    {
      "epoch": 2.16260162601626,
      "grad_norm": 24.785858154296875,
      "learning_rate": 8.70822041553749e-06,
      "loss": 1.2561,
      "step": 5586
    },
    {
      "epoch": 2.1629887727448702,
      "grad_norm": 34.1836051940918,
      "learning_rate": 8.7077902525057e-06,
      "loss": 3.0306,
      "step": 5587
    },
    {
      "epoch": 2.1633759194734803,
      "grad_norm": 17.974227905273438,
      "learning_rate": 8.707360089473913e-06,
      "loss": 1.6093,
      "step": 5588
    },
    {
      "epoch": 2.1637630662020904,
      "grad_norm": 15.811424255371094,
      "learning_rate": 8.706929926442122e-06,
      "loss": 0.4131,
      "step": 5589
    },
    {
      "epoch": 2.164150212930701,
      "grad_norm": 39.51288986206055,
      "learning_rate": 8.706499763410334e-06,
      "loss": 1.5449,
      "step": 5590
    },
    {
      "epoch": 2.164537359659311,
      "grad_norm": 16.884105682373047,
      "learning_rate": 8.706069600378543e-06,
      "loss": 1.4809,
      "step": 5591
    },
    {
      "epoch": 2.164924506387921,
      "grad_norm": 6.713564395904541,
      "learning_rate": 8.705639437346755e-06,
      "loss": 0.4254,
      "step": 5592
    },
    {
      "epoch": 2.165311653116531,
      "grad_norm": 48.218536376953125,
      "learning_rate": 8.705209274314966e-06,
      "loss": 1.3379,
      "step": 5593
    },
    {
      "epoch": 2.165698799845141,
      "grad_norm": 17.687524795532227,
      "learning_rate": 8.704779111283178e-06,
      "loss": 1.3326,
      "step": 5594
    },
    {
      "epoch": 2.1660859465737516,
      "grad_norm": 16.637033462524414,
      "learning_rate": 8.704348948251387e-06,
      "loss": 0.9712,
      "step": 5595
    },
    {
      "epoch": 2.1664730933023617,
      "grad_norm": 56.31962966918945,
      "learning_rate": 8.703918785219599e-06,
      "loss": 0.9894,
      "step": 5596
    },
    {
      "epoch": 2.1668602400309718,
      "grad_norm": 15.924088478088379,
      "learning_rate": 8.70348862218781e-06,
      "loss": 1.1624,
      "step": 5597
    },
    {
      "epoch": 2.167247386759582,
      "grad_norm": 13.462034225463867,
      "learning_rate": 8.70305845915602e-06,
      "loss": 1.2339,
      "step": 5598
    },
    {
      "epoch": 2.167634533488192,
      "grad_norm": 54.61967849731445,
      "learning_rate": 8.702628296124231e-06,
      "loss": 2.8346,
      "step": 5599
    },
    {
      "epoch": 2.168021680216802,
      "grad_norm": 26.92546272277832,
      "learning_rate": 8.702198133092443e-06,
      "loss": 1.502,
      "step": 5600
    },
    {
      "epoch": 2.1684088269454125,
      "grad_norm": 11.193740844726562,
      "learning_rate": 8.701767970060654e-06,
      "loss": 0.6483,
      "step": 5601
    },
    {
      "epoch": 2.1687959736740225,
      "grad_norm": 21.617008209228516,
      "learning_rate": 8.701337807028864e-06,
      "loss": 1.8115,
      "step": 5602
    },
    {
      "epoch": 2.1691831204026326,
      "grad_norm": 16.287683486938477,
      "learning_rate": 8.700907643997075e-06,
      "loss": 1.4742,
      "step": 5603
    },
    {
      "epoch": 2.1695702671312427,
      "grad_norm": 24.744524002075195,
      "learning_rate": 8.700477480965287e-06,
      "loss": 0.602,
      "step": 5604
    },
    {
      "epoch": 2.1699574138598527,
      "grad_norm": 28.67487907409668,
      "learning_rate": 8.700047317933498e-06,
      "loss": 1.6241,
      "step": 5605
    },
    {
      "epoch": 2.170344560588463,
      "grad_norm": 32.49822235107422,
      "learning_rate": 8.699617154901708e-06,
      "loss": 1.7303,
      "step": 5606
    },
    {
      "epoch": 2.1707317073170733,
      "grad_norm": 19.537626266479492,
      "learning_rate": 8.69918699186992e-06,
      "loss": 1.6886,
      "step": 5607
    },
    {
      "epoch": 2.1711188540456834,
      "grad_norm": 28.749908447265625,
      "learning_rate": 8.69875682883813e-06,
      "loss": 1.1563,
      "step": 5608
    },
    {
      "epoch": 2.1715060007742935,
      "grad_norm": 10.682608604431152,
      "learning_rate": 8.698326665806342e-06,
      "loss": 0.7777,
      "step": 5609
    },
    {
      "epoch": 2.1718931475029035,
      "grad_norm": 50.361900329589844,
      "learning_rate": 8.697896502774552e-06,
      "loss": 1.3772,
      "step": 5610
    },
    {
      "epoch": 2.1722802942315136,
      "grad_norm": 19.933732986450195,
      "learning_rate": 8.697466339742763e-06,
      "loss": 0.9924,
      "step": 5611
    },
    {
      "epoch": 2.172667440960124,
      "grad_norm": 27.145950317382812,
      "learning_rate": 8.697036176710975e-06,
      "loss": 1.8916,
      "step": 5612
    },
    {
      "epoch": 2.173054587688734,
      "grad_norm": 22.96731185913086,
      "learning_rate": 8.696606013679184e-06,
      "loss": 1.3846,
      "step": 5613
    },
    {
      "epoch": 2.1734417344173442,
      "grad_norm": 25.20335578918457,
      "learning_rate": 8.696175850647396e-06,
      "loss": 2.5426,
      "step": 5614
    },
    {
      "epoch": 2.1738288811459543,
      "grad_norm": 23.64990234375,
      "learning_rate": 8.695745687615607e-06,
      "loss": 1.0473,
      "step": 5615
    },
    {
      "epoch": 2.1742160278745644,
      "grad_norm": 21.065013885498047,
      "learning_rate": 8.695315524583819e-06,
      "loss": 1.5633,
      "step": 5616
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 37.93104934692383,
      "learning_rate": 8.694885361552028e-06,
      "loss": 2.854,
      "step": 5617
    },
    {
      "epoch": 2.174990321331785,
      "grad_norm": 10.136884689331055,
      "learning_rate": 8.69445519852024e-06,
      "loss": 0.3699,
      "step": 5618
    },
    {
      "epoch": 2.175377468060395,
      "grad_norm": 16.31790542602539,
      "learning_rate": 8.69402503548845e-06,
      "loss": 1.6626,
      "step": 5619
    },
    {
      "epoch": 2.175764614789005,
      "grad_norm": 25.839956283569336,
      "learning_rate": 8.693594872456663e-06,
      "loss": 1.9587,
      "step": 5620
    },
    {
      "epoch": 2.176151761517615,
      "grad_norm": 32.861690521240234,
      "learning_rate": 8.693164709424872e-06,
      "loss": 2.0731,
      "step": 5621
    },
    {
      "epoch": 2.176538908246225,
      "grad_norm": 13.506763458251953,
      "learning_rate": 8.692734546393084e-06,
      "loss": 0.8152,
      "step": 5622
    },
    {
      "epoch": 2.1769260549748353,
      "grad_norm": 26.92336654663086,
      "learning_rate": 8.692304383361294e-06,
      "loss": 3.6823,
      "step": 5623
    },
    {
      "epoch": 2.1773132017034458,
      "grad_norm": 18.75119972229004,
      "learning_rate": 8.691874220329507e-06,
      "loss": 0.9899,
      "step": 5624
    },
    {
      "epoch": 2.177700348432056,
      "grad_norm": 27.13536834716797,
      "learning_rate": 8.691444057297716e-06,
      "loss": 0.9819,
      "step": 5625
    },
    {
      "epoch": 2.178087495160666,
      "grad_norm": 22.54461669921875,
      "learning_rate": 8.691013894265928e-06,
      "loss": 1.1496,
      "step": 5626
    },
    {
      "epoch": 2.178474641889276,
      "grad_norm": 12.797321319580078,
      "learning_rate": 8.690583731234138e-06,
      "loss": 1.3425,
      "step": 5627
    },
    {
      "epoch": 2.178861788617886,
      "grad_norm": 28.64940071105957,
      "learning_rate": 8.690153568202349e-06,
      "loss": 1.8189,
      "step": 5628
    },
    {
      "epoch": 2.1792489353464966,
      "grad_norm": 32.1628303527832,
      "learning_rate": 8.68972340517056e-06,
      "loss": 1.2129,
      "step": 5629
    },
    {
      "epoch": 2.1796360820751066,
      "grad_norm": 13.91480827331543,
      "learning_rate": 8.689293242138772e-06,
      "loss": 0.932,
      "step": 5630
    },
    {
      "epoch": 2.1800232288037167,
      "grad_norm": 25.756132125854492,
      "learning_rate": 8.688863079106983e-06,
      "loss": 3.0579,
      "step": 5631
    },
    {
      "epoch": 2.1804103755323267,
      "grad_norm": 22.27522850036621,
      "learning_rate": 8.688432916075193e-06,
      "loss": 1.8336,
      "step": 5632
    },
    {
      "epoch": 2.180797522260937,
      "grad_norm": 14.170891761779785,
      "learning_rate": 8.688002753043404e-06,
      "loss": 1.3633,
      "step": 5633
    },
    {
      "epoch": 2.181184668989547,
      "grad_norm": 5.16738224029541,
      "learning_rate": 8.687572590011614e-06,
      "loss": 0.3049,
      "step": 5634
    },
    {
      "epoch": 2.181571815718157,
      "grad_norm": 17.48908805847168,
      "learning_rate": 8.687142426979827e-06,
      "loss": 1.445,
      "step": 5635
    },
    {
      "epoch": 2.1819589624467675,
      "grad_norm": 11.5468111038208,
      "learning_rate": 8.686712263948037e-06,
      "loss": 1.2148,
      "step": 5636
    },
    {
      "epoch": 2.1823461091753775,
      "grad_norm": 23.58120346069336,
      "learning_rate": 8.686282100916248e-06,
      "loss": 1.3988,
      "step": 5637
    },
    {
      "epoch": 2.1827332559039876,
      "grad_norm": 11.596490859985352,
      "learning_rate": 8.685851937884458e-06,
      "loss": 0.7007,
      "step": 5638
    },
    {
      "epoch": 2.1831204026325977,
      "grad_norm": 30.496051788330078,
      "learning_rate": 8.685421774852671e-06,
      "loss": 2.2946,
      "step": 5639
    },
    {
      "epoch": 2.1835075493612077,
      "grad_norm": 13.67969036102295,
      "learning_rate": 8.68499161182088e-06,
      "loss": 0.7548,
      "step": 5640
    },
    {
      "epoch": 2.1838946960898182,
      "grad_norm": 45.68698501586914,
      "learning_rate": 8.684561448789092e-06,
      "loss": 1.2063,
      "step": 5641
    },
    {
      "epoch": 2.1842818428184283,
      "grad_norm": 29.896760940551758,
      "learning_rate": 8.684131285757302e-06,
      "loss": 1.9256,
      "step": 5642
    },
    {
      "epoch": 2.1846689895470384,
      "grad_norm": 12.412979125976562,
      "learning_rate": 8.683701122725513e-06,
      "loss": 0.5971,
      "step": 5643
    },
    {
      "epoch": 2.1850561362756484,
      "grad_norm": 11.985725402832031,
      "learning_rate": 8.683270959693725e-06,
      "loss": 0.7547,
      "step": 5644
    },
    {
      "epoch": 2.1854432830042585,
      "grad_norm": 16.291568756103516,
      "learning_rate": 8.682840796661936e-06,
      "loss": 1.4305,
      "step": 5645
    },
    {
      "epoch": 2.1858304297328686,
      "grad_norm": 25.673742294311523,
      "learning_rate": 8.682410633630146e-06,
      "loss": 2.6777,
      "step": 5646
    },
    {
      "epoch": 2.186217576461479,
      "grad_norm": 21.943466186523438,
      "learning_rate": 8.681980470598357e-06,
      "loss": 1.3586,
      "step": 5647
    },
    {
      "epoch": 2.186604723190089,
      "grad_norm": 9.901951789855957,
      "learning_rate": 8.681550307566569e-06,
      "loss": 0.5079,
      "step": 5648
    },
    {
      "epoch": 2.186991869918699,
      "grad_norm": 16.38618278503418,
      "learning_rate": 8.681120144534778e-06,
      "loss": 1.7735,
      "step": 5649
    },
    {
      "epoch": 2.1873790166473093,
      "grad_norm": 30.50965690612793,
      "learning_rate": 8.68068998150299e-06,
      "loss": 2.2423,
      "step": 5650
    },
    {
      "epoch": 2.1877661633759193,
      "grad_norm": 33.979129791259766,
      "learning_rate": 8.680259818471201e-06,
      "loss": 0.6657,
      "step": 5651
    },
    {
      "epoch": 2.1881533101045294,
      "grad_norm": 17.17148208618164,
      "learning_rate": 8.679829655439413e-06,
      "loss": 1.4833,
      "step": 5652
    },
    {
      "epoch": 2.18854045683314,
      "grad_norm": 43.750972747802734,
      "learning_rate": 8.679399492407622e-06,
      "loss": 1.5628,
      "step": 5653
    },
    {
      "epoch": 2.18892760356175,
      "grad_norm": 17.35064125061035,
      "learning_rate": 8.678969329375834e-06,
      "loss": 1.527,
      "step": 5654
    },
    {
      "epoch": 2.18931475029036,
      "grad_norm": 14.566039085388184,
      "learning_rate": 8.678539166344045e-06,
      "loss": 1.4068,
      "step": 5655
    },
    {
      "epoch": 2.18970189701897,
      "grad_norm": 31.55963897705078,
      "learning_rate": 8.678109003312257e-06,
      "loss": 1.9275,
      "step": 5656
    },
    {
      "epoch": 2.19008904374758,
      "grad_norm": 17.734642028808594,
      "learning_rate": 8.677678840280466e-06,
      "loss": 1.0969,
      "step": 5657
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 13.627419471740723,
      "learning_rate": 8.677248677248678e-06,
      "loss": 0.8354,
      "step": 5658
    },
    {
      "epoch": 2.1908633372048008,
      "grad_norm": 19.251916885375977,
      "learning_rate": 8.67681851421689e-06,
      "loss": 1.1582,
      "step": 5659
    },
    {
      "epoch": 2.191250483933411,
      "grad_norm": 27.29096221923828,
      "learning_rate": 8.6763883511851e-06,
      "loss": 1.5297,
      "step": 5660
    },
    {
      "epoch": 2.191637630662021,
      "grad_norm": 38.77918243408203,
      "learning_rate": 8.67595818815331e-06,
      "loss": 1.4638,
      "step": 5661
    },
    {
      "epoch": 2.192024777390631,
      "grad_norm": 17.602296829223633,
      "learning_rate": 8.675528025121522e-06,
      "loss": 0.9641,
      "step": 5662
    },
    {
      "epoch": 2.192411924119241,
      "grad_norm": 18.6450138092041,
      "learning_rate": 8.675097862089733e-06,
      "loss": 0.8111,
      "step": 5663
    },
    {
      "epoch": 2.1927990708478515,
      "grad_norm": 23.98210334777832,
      "learning_rate": 8.674667699057943e-06,
      "loss": 1.3939,
      "step": 5664
    },
    {
      "epoch": 2.1931862175764616,
      "grad_norm": 12.271434783935547,
      "learning_rate": 8.674237536026154e-06,
      "loss": 0.747,
      "step": 5665
    },
    {
      "epoch": 2.1935733643050717,
      "grad_norm": 18.278566360473633,
      "learning_rate": 8.673807372994366e-06,
      "loss": 1.1519,
      "step": 5666
    },
    {
      "epoch": 2.1939605110336817,
      "grad_norm": 33.8089485168457,
      "learning_rate": 8.673377209962577e-06,
      "loss": 1.8597,
      "step": 5667
    },
    {
      "epoch": 2.194347657762292,
      "grad_norm": 18.04059410095215,
      "learning_rate": 8.672947046930787e-06,
      "loss": 1.4154,
      "step": 5668
    },
    {
      "epoch": 2.194734804490902,
      "grad_norm": 33.06069564819336,
      "learning_rate": 8.672516883898998e-06,
      "loss": 1.0316,
      "step": 5669
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 13.443243980407715,
      "learning_rate": 8.67208672086721e-06,
      "loss": 0.7606,
      "step": 5670
    },
    {
      "epoch": 2.1955090979481224,
      "grad_norm": 15.432515144348145,
      "learning_rate": 8.671656557835421e-06,
      "loss": 1.1271,
      "step": 5671
    },
    {
      "epoch": 2.1958962446767325,
      "grad_norm": 26.598005294799805,
      "learning_rate": 8.671226394803631e-06,
      "loss": 2.7933,
      "step": 5672
    },
    {
      "epoch": 2.1962833914053426,
      "grad_norm": 17.571386337280273,
      "learning_rate": 8.670796231771842e-06,
      "loss": 1.4701,
      "step": 5673
    },
    {
      "epoch": 2.1966705381339526,
      "grad_norm": 16.04978370666504,
      "learning_rate": 8.670366068740054e-06,
      "loss": 1.9055,
      "step": 5674
    },
    {
      "epoch": 2.197057684862563,
      "grad_norm": 13.524290084838867,
      "learning_rate": 8.669935905708265e-06,
      "loss": 0.7808,
      "step": 5675
    },
    {
      "epoch": 2.197444831591173,
      "grad_norm": 4.841613292694092,
      "learning_rate": 8.669505742676475e-06,
      "loss": 0.2883,
      "step": 5676
    },
    {
      "epoch": 2.1978319783197833,
      "grad_norm": 26.897075653076172,
      "learning_rate": 8.669075579644686e-06,
      "loss": 1.4645,
      "step": 5677
    },
    {
      "epoch": 2.1982191250483933,
      "grad_norm": 15.567298889160156,
      "learning_rate": 8.668645416612898e-06,
      "loss": 1.1339,
      "step": 5678
    },
    {
      "epoch": 2.1986062717770034,
      "grad_norm": 12.071548461914062,
      "learning_rate": 8.668215253581107e-06,
      "loss": 0.763,
      "step": 5679
    },
    {
      "epoch": 2.1989934185056135,
      "grad_norm": 18.61468505859375,
      "learning_rate": 8.667785090549319e-06,
      "loss": 1.6675,
      "step": 5680
    },
    {
      "epoch": 2.1993805652342235,
      "grad_norm": 18.082298278808594,
      "learning_rate": 8.66735492751753e-06,
      "loss": 1.7247,
      "step": 5681
    },
    {
      "epoch": 2.199767711962834,
      "grad_norm": 31.678302764892578,
      "learning_rate": 8.666924764485742e-06,
      "loss": 1.9421,
      "step": 5682
    },
    {
      "epoch": 2.200154858691444,
      "grad_norm": 37.652366638183594,
      "learning_rate": 8.666494601453951e-06,
      "loss": 3.9971,
      "step": 5683
    },
    {
      "epoch": 2.200542005420054,
      "grad_norm": 28.612701416015625,
      "learning_rate": 8.666064438422163e-06,
      "loss": 0.7292,
      "step": 5684
    },
    {
      "epoch": 2.2009291521486642,
      "grad_norm": 11.818868637084961,
      "learning_rate": 8.665634275390373e-06,
      "loss": 0.772,
      "step": 5685
    },
    {
      "epoch": 2.2013162988772743,
      "grad_norm": 12.406148910522461,
      "learning_rate": 8.665204112358586e-06,
      "loss": 0.5393,
      "step": 5686
    },
    {
      "epoch": 2.201703445605885,
      "grad_norm": 19.71336555480957,
      "learning_rate": 8.664773949326795e-06,
      "loss": 1.4786,
      "step": 5687
    },
    {
      "epoch": 2.202090592334495,
      "grad_norm": 32.739566802978516,
      "learning_rate": 8.664343786295007e-06,
      "loss": 1.4328,
      "step": 5688
    },
    {
      "epoch": 2.202477739063105,
      "grad_norm": 9.46727466583252,
      "learning_rate": 8.663913623263216e-06,
      "loss": 0.45,
      "step": 5689
    },
    {
      "epoch": 2.202864885791715,
      "grad_norm": 3.808995246887207,
      "learning_rate": 8.66348346023143e-06,
      "loss": 0.1162,
      "step": 5690
    },
    {
      "epoch": 2.203252032520325,
      "grad_norm": 22.472515106201172,
      "learning_rate": 8.66305329719964e-06,
      "loss": 1.7183,
      "step": 5691
    },
    {
      "epoch": 2.203639179248935,
      "grad_norm": 19.749523162841797,
      "learning_rate": 8.66262313416785e-06,
      "loss": 0.6862,
      "step": 5692
    },
    {
      "epoch": 2.2040263259775457,
      "grad_norm": 23.209897994995117,
      "learning_rate": 8.66219297113606e-06,
      "loss": 1.6337,
      "step": 5693
    },
    {
      "epoch": 2.2044134727061557,
      "grad_norm": 24.343341827392578,
      "learning_rate": 8.661762808104272e-06,
      "loss": 2.9926,
      "step": 5694
    },
    {
      "epoch": 2.204800619434766,
      "grad_norm": 10.961103439331055,
      "learning_rate": 8.661332645072483e-06,
      "loss": 0.6693,
      "step": 5695
    },
    {
      "epoch": 2.205187766163376,
      "grad_norm": 25.91400909423828,
      "learning_rate": 8.660902482040695e-06,
      "loss": 1.357,
      "step": 5696
    },
    {
      "epoch": 2.205574912891986,
      "grad_norm": 32.4426383972168,
      "learning_rate": 8.660472319008904e-06,
      "loss": 0.8652,
      "step": 5697
    },
    {
      "epoch": 2.205962059620596,
      "grad_norm": 22.473031997680664,
      "learning_rate": 8.660042155977116e-06,
      "loss": 1.9666,
      "step": 5698
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 21.656789779663086,
      "learning_rate": 8.659611992945327e-06,
      "loss": 0.5056,
      "step": 5699
    },
    {
      "epoch": 2.2067363530778166,
      "grad_norm": 13.069352149963379,
      "learning_rate": 8.659181829913537e-06,
      "loss": 0.8431,
      "step": 5700
    },
    {
      "epoch": 2.2071234998064266,
      "grad_norm": 21.653369903564453,
      "learning_rate": 8.658751666881748e-06,
      "loss": 1.6978,
      "step": 5701
    },
    {
      "epoch": 2.2075106465350367,
      "grad_norm": 87.03356170654297,
      "learning_rate": 8.65832150384996e-06,
      "loss": 2.0948,
      "step": 5702
    },
    {
      "epoch": 2.2078977932636468,
      "grad_norm": 24.622404098510742,
      "learning_rate": 8.657891340818171e-06,
      "loss": 1.63,
      "step": 5703
    },
    {
      "epoch": 2.2082849399922573,
      "grad_norm": 29.88750648498535,
      "learning_rate": 8.657461177786381e-06,
      "loss": 1.9582,
      "step": 5704
    },
    {
      "epoch": 2.2086720867208673,
      "grad_norm": 36.71696472167969,
      "learning_rate": 8.657031014754592e-06,
      "loss": 1.3772,
      "step": 5705
    },
    {
      "epoch": 2.2090592334494774,
      "grad_norm": 41.802764892578125,
      "learning_rate": 8.656600851722804e-06,
      "loss": 2.7193,
      "step": 5706
    },
    {
      "epoch": 2.2094463801780875,
      "grad_norm": 19.114620208740234,
      "learning_rate": 8.656170688691015e-06,
      "loss": 2.1807,
      "step": 5707
    },
    {
      "epoch": 2.2098335269066975,
      "grad_norm": 18.313440322875977,
      "learning_rate": 8.655740525659225e-06,
      "loss": 0.9251,
      "step": 5708
    },
    {
      "epoch": 2.2102206736353076,
      "grad_norm": 19.105207443237305,
      "learning_rate": 8.655310362627436e-06,
      "loss": 0.9223,
      "step": 5709
    },
    {
      "epoch": 2.210607820363918,
      "grad_norm": 33.758575439453125,
      "learning_rate": 8.654880199595648e-06,
      "loss": 4.5376,
      "step": 5710
    },
    {
      "epoch": 2.210994967092528,
      "grad_norm": 24.329814910888672,
      "learning_rate": 8.65445003656386e-06,
      "loss": 1.5976,
      "step": 5711
    },
    {
      "epoch": 2.2113821138211383,
      "grad_norm": 14.477089881896973,
      "learning_rate": 8.654019873532069e-06,
      "loss": 0.9947,
      "step": 5712
    },
    {
      "epoch": 2.2117692605497483,
      "grad_norm": 25.732961654663086,
      "learning_rate": 8.65358971050028e-06,
      "loss": 2.5286,
      "step": 5713
    },
    {
      "epoch": 2.2121564072783584,
      "grad_norm": 24.749818801879883,
      "learning_rate": 8.653159547468492e-06,
      "loss": 2.2921,
      "step": 5714
    },
    {
      "epoch": 2.2125435540069684,
      "grad_norm": 18.872201919555664,
      "learning_rate": 8.652729384436701e-06,
      "loss": 1.6774,
      "step": 5715
    },
    {
      "epoch": 2.212930700735579,
      "grad_norm": 25.347068786621094,
      "learning_rate": 8.652299221404913e-06,
      "loss": 1.5672,
      "step": 5716
    },
    {
      "epoch": 2.213317847464189,
      "grad_norm": 19.266460418701172,
      "learning_rate": 8.651869058373124e-06,
      "loss": 0.8372,
      "step": 5717
    },
    {
      "epoch": 2.213704994192799,
      "grad_norm": 20.276844024658203,
      "learning_rate": 8.651438895341336e-06,
      "loss": 1.7973,
      "step": 5718
    },
    {
      "epoch": 2.214092140921409,
      "grad_norm": 13.142781257629395,
      "learning_rate": 8.651008732309545e-06,
      "loss": 1.1716,
      "step": 5719
    },
    {
      "epoch": 2.2144792876500192,
      "grad_norm": 59.192108154296875,
      "learning_rate": 8.650578569277757e-06,
      "loss": 1.5227,
      "step": 5720
    },
    {
      "epoch": 2.2148664343786297,
      "grad_norm": 13.16676139831543,
      "learning_rate": 8.650148406245968e-06,
      "loss": 0.8417,
      "step": 5721
    },
    {
      "epoch": 2.21525358110724,
      "grad_norm": 33.27337646484375,
      "learning_rate": 8.64971824321418e-06,
      "loss": 1.6483,
      "step": 5722
    },
    {
      "epoch": 2.21564072783585,
      "grad_norm": 20.159299850463867,
      "learning_rate": 8.64928808018239e-06,
      "loss": 1.5855,
      "step": 5723
    },
    {
      "epoch": 2.21602787456446,
      "grad_norm": 34.85837936401367,
      "learning_rate": 8.6488579171506e-06,
      "loss": 1.3898,
      "step": 5724
    },
    {
      "epoch": 2.21641502129307,
      "grad_norm": 20.54491424560547,
      "learning_rate": 8.648427754118812e-06,
      "loss": 1.5787,
      "step": 5725
    },
    {
      "epoch": 2.21680216802168,
      "grad_norm": 30.673852920532227,
      "learning_rate": 8.647997591087024e-06,
      "loss": 2.2045,
      "step": 5726
    },
    {
      "epoch": 2.21718931475029,
      "grad_norm": 9.560256958007812,
      "learning_rate": 8.647567428055233e-06,
      "loss": 1.283,
      "step": 5727
    },
    {
      "epoch": 2.2175764614789006,
      "grad_norm": 31.019668579101562,
      "learning_rate": 8.647137265023445e-06,
      "loss": 2.0186,
      "step": 5728
    },
    {
      "epoch": 2.2179636082075107,
      "grad_norm": 13.538093566894531,
      "learning_rate": 8.646707101991656e-06,
      "loss": 1.4496,
      "step": 5729
    },
    {
      "epoch": 2.2183507549361208,
      "grad_norm": 31.030742645263672,
      "learning_rate": 8.646276938959866e-06,
      "loss": 1.6326,
      "step": 5730
    },
    {
      "epoch": 2.218737901664731,
      "grad_norm": 52.9681396484375,
      "learning_rate": 8.645846775928077e-06,
      "loss": 3.1336,
      "step": 5731
    },
    {
      "epoch": 2.219125048393341,
      "grad_norm": 12.63992977142334,
      "learning_rate": 8.645416612896289e-06,
      "loss": 0.9014,
      "step": 5732
    },
    {
      "epoch": 2.2195121951219514,
      "grad_norm": 14.793625831604004,
      "learning_rate": 8.6449864498645e-06,
      "loss": 1.3943,
      "step": 5733
    },
    {
      "epoch": 2.2198993418505615,
      "grad_norm": 36.969085693359375,
      "learning_rate": 8.64455628683271e-06,
      "loss": 1.2107,
      "step": 5734
    },
    {
      "epoch": 2.2202864885791715,
      "grad_norm": 12.268299102783203,
      "learning_rate": 8.644126123800921e-06,
      "loss": 0.9307,
      "step": 5735
    },
    {
      "epoch": 2.2206736353077816,
      "grad_norm": 16.424142837524414,
      "learning_rate": 8.643695960769131e-06,
      "loss": 1.4059,
      "step": 5736
    },
    {
      "epoch": 2.2210607820363917,
      "grad_norm": 14.399559020996094,
      "learning_rate": 8.643265797737344e-06,
      "loss": 0.7709,
      "step": 5737
    },
    {
      "epoch": 2.2214479287650017,
      "grad_norm": 26.307626724243164,
      "learning_rate": 8.642835634705554e-06,
      "loss": 1.3982,
      "step": 5738
    },
    {
      "epoch": 2.2218350754936123,
      "grad_norm": 16.040672302246094,
      "learning_rate": 8.642405471673765e-06,
      "loss": 1.2607,
      "step": 5739
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 19.167251586914062,
      "learning_rate": 8.641975308641975e-06,
      "loss": 1.775,
      "step": 5740
    },
    {
      "epoch": 2.2226093689508324,
      "grad_norm": 14.994040489196777,
      "learning_rate": 8.641545145610188e-06,
      "loss": 0.93,
      "step": 5741
    },
    {
      "epoch": 2.2229965156794425,
      "grad_norm": 25.378543853759766,
      "learning_rate": 8.641114982578398e-06,
      "loss": 1.3416,
      "step": 5742
    },
    {
      "epoch": 2.2233836624080525,
      "grad_norm": 19.573631286621094,
      "learning_rate": 8.64068481954661e-06,
      "loss": 1.5759,
      "step": 5743
    },
    {
      "epoch": 2.2237708091366626,
      "grad_norm": 16.89473533630371,
      "learning_rate": 8.640254656514819e-06,
      "loss": 1.2447,
      "step": 5744
    },
    {
      "epoch": 2.224157955865273,
      "grad_norm": 17.17972755432129,
      "learning_rate": 8.63982449348303e-06,
      "loss": 1.7423,
      "step": 5745
    },
    {
      "epoch": 2.224545102593883,
      "grad_norm": 15.37057113647461,
      "learning_rate": 8.639394330451242e-06,
      "loss": 1.4475,
      "step": 5746
    },
    {
      "epoch": 2.2249322493224932,
      "grad_norm": 8.533224105834961,
      "learning_rate": 8.638964167419453e-06,
      "loss": 1.2683,
      "step": 5747
    },
    {
      "epoch": 2.2253193960511033,
      "grad_norm": 17.847293853759766,
      "learning_rate": 8.638534004387663e-06,
      "loss": 1.5367,
      "step": 5748
    },
    {
      "epoch": 2.2257065427797134,
      "grad_norm": 25.17490005493164,
      "learning_rate": 8.638103841355874e-06,
      "loss": 2.3021,
      "step": 5749
    },
    {
      "epoch": 2.226093689508324,
      "grad_norm": 13.160557746887207,
      "learning_rate": 8.637673678324086e-06,
      "loss": 0.8879,
      "step": 5750
    },
    {
      "epoch": 2.226480836236934,
      "grad_norm": 27.700340270996094,
      "learning_rate": 8.637243515292295e-06,
      "loss": 1.3721,
      "step": 5751
    },
    {
      "epoch": 2.226867982965544,
      "grad_norm": 19.532947540283203,
      "learning_rate": 8.636813352260509e-06,
      "loss": 1.5582,
      "step": 5752
    },
    {
      "epoch": 2.227255129694154,
      "grad_norm": 46.181339263916016,
      "learning_rate": 8.636383189228718e-06,
      "loss": 2.3688,
      "step": 5753
    },
    {
      "epoch": 2.227642276422764,
      "grad_norm": 22.301044464111328,
      "learning_rate": 8.63595302619693e-06,
      "loss": 1.9655,
      "step": 5754
    },
    {
      "epoch": 2.228029423151374,
      "grad_norm": 25.20313262939453,
      "learning_rate": 8.63552286316514e-06,
      "loss": 1.384,
      "step": 5755
    },
    {
      "epoch": 2.2284165698799847,
      "grad_norm": 22.572614669799805,
      "learning_rate": 8.635092700133353e-06,
      "loss": 1.5164,
      "step": 5756
    },
    {
      "epoch": 2.2288037166085948,
      "grad_norm": 34.91500473022461,
      "learning_rate": 8.634662537101562e-06,
      "loss": 1.3274,
      "step": 5757
    },
    {
      "epoch": 2.229190863337205,
      "grad_norm": 47.200199127197266,
      "learning_rate": 8.634232374069774e-06,
      "loss": 1.3352,
      "step": 5758
    },
    {
      "epoch": 2.229578010065815,
      "grad_norm": 26.68342399597168,
      "learning_rate": 8.633802211037983e-06,
      "loss": 1.6246,
      "step": 5759
    },
    {
      "epoch": 2.229965156794425,
      "grad_norm": 7.146963119506836,
      "learning_rate": 8.633372048006195e-06,
      "loss": 0.354,
      "step": 5760
    },
    {
      "epoch": 2.230352303523035,
      "grad_norm": 14.549541473388672,
      "learning_rate": 8.632941884974406e-06,
      "loss": 1.3679,
      "step": 5761
    },
    {
      "epoch": 2.2307394502516456,
      "grad_norm": 44.43584442138672,
      "learning_rate": 8.632511721942618e-06,
      "loss": 1.6165,
      "step": 5762
    },
    {
      "epoch": 2.2311265969802556,
      "grad_norm": 13.390754699707031,
      "learning_rate": 8.632081558910827e-06,
      "loss": 0.3281,
      "step": 5763
    },
    {
      "epoch": 2.2315137437088657,
      "grad_norm": 22.669113159179688,
      "learning_rate": 8.631651395879039e-06,
      "loss": 0.9952,
      "step": 5764
    },
    {
      "epoch": 2.2319008904374757,
      "grad_norm": 20.483312606811523,
      "learning_rate": 8.63122123284725e-06,
      "loss": 1.6449,
      "step": 5765
    },
    {
      "epoch": 2.232288037166086,
      "grad_norm": 25.19856834411621,
      "learning_rate": 8.63079106981546e-06,
      "loss": 1.2542,
      "step": 5766
    },
    {
      "epoch": 2.2326751838946963,
      "grad_norm": 14.552696228027344,
      "learning_rate": 8.630360906783671e-06,
      "loss": 1.1668,
      "step": 5767
    },
    {
      "epoch": 2.2330623306233064,
      "grad_norm": 15.986563682556152,
      "learning_rate": 8.629930743751883e-06,
      "loss": 1.379,
      "step": 5768
    },
    {
      "epoch": 2.2334494773519165,
      "grad_norm": 14.544148445129395,
      "learning_rate": 8.629500580720094e-06,
      "loss": 0.6936,
      "step": 5769
    },
    {
      "epoch": 2.2338366240805265,
      "grad_norm": 27.574018478393555,
      "learning_rate": 8.629070417688304e-06,
      "loss": 1.4759,
      "step": 5770
    },
    {
      "epoch": 2.2342237708091366,
      "grad_norm": 14.483154296875,
      "learning_rate": 8.628640254656515e-06,
      "loss": 1.1451,
      "step": 5771
    },
    {
      "epoch": 2.2346109175377467,
      "grad_norm": 15.774869918823242,
      "learning_rate": 8.628210091624727e-06,
      "loss": 0.8971,
      "step": 5772
    },
    {
      "epoch": 2.2349980642663567,
      "grad_norm": 27.20709800720215,
      "learning_rate": 8.627779928592938e-06,
      "loss": 1.5739,
      "step": 5773
    },
    {
      "epoch": 2.2353852109949672,
      "grad_norm": 12.519439697265625,
      "learning_rate": 8.627349765561148e-06,
      "loss": 0.9026,
      "step": 5774
    },
    {
      "epoch": 2.2357723577235773,
      "grad_norm": 13.538370132446289,
      "learning_rate": 8.62691960252936e-06,
      "loss": 1.3588,
      "step": 5775
    },
    {
      "epoch": 2.2361595044521874,
      "grad_norm": 19.77648162841797,
      "learning_rate": 8.62648943949757e-06,
      "loss": 1.2101,
      "step": 5776
    },
    {
      "epoch": 2.2365466511807974,
      "grad_norm": 32.62482452392578,
      "learning_rate": 8.626059276465782e-06,
      "loss": 1.1775,
      "step": 5777
    },
    {
      "epoch": 2.2369337979094075,
      "grad_norm": 17.96583366394043,
      "learning_rate": 8.625629113433992e-06,
      "loss": 0.8201,
      "step": 5778
    },
    {
      "epoch": 2.237320944638018,
      "grad_norm": 25.0937442779541,
      "learning_rate": 8.625198950402203e-06,
      "loss": 1.3689,
      "step": 5779
    },
    {
      "epoch": 2.237708091366628,
      "grad_norm": 26.901123046875,
      "learning_rate": 8.624768787370415e-06,
      "loss": 1.3778,
      "step": 5780
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 34.301361083984375,
      "learning_rate": 8.624338624338624e-06,
      "loss": 1.949,
      "step": 5781
    },
    {
      "epoch": 2.238482384823848,
      "grad_norm": 18.918291091918945,
      "learning_rate": 8.623908461306836e-06,
      "loss": 1.5365,
      "step": 5782
    },
    {
      "epoch": 2.2388695315524583,
      "grad_norm": 15.797140121459961,
      "learning_rate": 8.623478298275047e-06,
      "loss": 1.4596,
      "step": 5783
    },
    {
      "epoch": 2.2392566782810683,
      "grad_norm": 15.13597583770752,
      "learning_rate": 8.623048135243259e-06,
      "loss": 0.9174,
      "step": 5784
    },
    {
      "epoch": 2.239643825009679,
      "grad_norm": 22.818689346313477,
      "learning_rate": 8.622617972211468e-06,
      "loss": 2.9998,
      "step": 5785
    },
    {
      "epoch": 2.240030971738289,
      "grad_norm": 33.063865661621094,
      "learning_rate": 8.62218780917968e-06,
      "loss": 2.0253,
      "step": 5786
    },
    {
      "epoch": 2.240418118466899,
      "grad_norm": 24.026220321655273,
      "learning_rate": 8.62175764614789e-06,
      "loss": 1.0644,
      "step": 5787
    },
    {
      "epoch": 2.240805265195509,
      "grad_norm": 18.59556007385254,
      "learning_rate": 8.621327483116103e-06,
      "loss": 0.8578,
      "step": 5788
    },
    {
      "epoch": 2.241192411924119,
      "grad_norm": 10.73974323272705,
      "learning_rate": 8.620897320084312e-06,
      "loss": 0.5646,
      "step": 5789
    },
    {
      "epoch": 2.241579558652729,
      "grad_norm": 20.8892765045166,
      "learning_rate": 8.620467157052524e-06,
      "loss": 1.6831,
      "step": 5790
    },
    {
      "epoch": 2.2419667053813397,
      "grad_norm": 28.32622528076172,
      "learning_rate": 8.620036994020733e-06,
      "loss": 1.6389,
      "step": 5791
    },
    {
      "epoch": 2.2423538521099498,
      "grad_norm": 5.027558326721191,
      "learning_rate": 8.619606830988947e-06,
      "loss": 0.2931,
      "step": 5792
    },
    {
      "epoch": 2.24274099883856,
      "grad_norm": 20.259267807006836,
      "learning_rate": 8.619176667957156e-06,
      "loss": 1.2709,
      "step": 5793
    },
    {
      "epoch": 2.24312814556717,
      "grad_norm": 14.222272872924805,
      "learning_rate": 8.618746504925368e-06,
      "loss": 1.1416,
      "step": 5794
    },
    {
      "epoch": 2.24351529229578,
      "grad_norm": 18.70956802368164,
      "learning_rate": 8.618316341893579e-06,
      "loss": 1.6727,
      "step": 5795
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 23.130718231201172,
      "learning_rate": 8.617886178861789e-06,
      "loss": 1.8411,
      "step": 5796
    },
    {
      "epoch": 2.2442895857530005,
      "grad_norm": 40.3341178894043,
      "learning_rate": 8.61745601583e-06,
      "loss": 1.9894,
      "step": 5797
    },
    {
      "epoch": 2.2446767324816106,
      "grad_norm": 21.988264083862305,
      "learning_rate": 8.617025852798212e-06,
      "loss": 1.7375,
      "step": 5798
    },
    {
      "epoch": 2.2450638792102207,
      "grad_norm": 41.746559143066406,
      "learning_rate": 8.616595689766423e-06,
      "loss": 0.5981,
      "step": 5799
    },
    {
      "epoch": 2.2454510259388307,
      "grad_norm": 31.200807571411133,
      "learning_rate": 8.616165526734633e-06,
      "loss": 1.2145,
      "step": 5800
    },
    {
      "epoch": 2.245838172667441,
      "grad_norm": 27.233867645263672,
      "learning_rate": 8.615735363702844e-06,
      "loss": 3.7272,
      "step": 5801
    },
    {
      "epoch": 2.2462253193960513,
      "grad_norm": 16.573619842529297,
      "learning_rate": 8.615305200671054e-06,
      "loss": 1.5187,
      "step": 5802
    },
    {
      "epoch": 2.2466124661246614,
      "grad_norm": 14.343463897705078,
      "learning_rate": 8.614875037639267e-06,
      "loss": 0.9052,
      "step": 5803
    },
    {
      "epoch": 2.2469996128532714,
      "grad_norm": 22.169340133666992,
      "learning_rate": 8.614444874607477e-06,
      "loss": 1.8583,
      "step": 5804
    },
    {
      "epoch": 2.2473867595818815,
      "grad_norm": 16.833972930908203,
      "learning_rate": 8.614014711575688e-06,
      "loss": 1.0662,
      "step": 5805
    },
    {
      "epoch": 2.2477739063104916,
      "grad_norm": 17.556900024414062,
      "learning_rate": 8.613584548543898e-06,
      "loss": 1.7206,
      "step": 5806
    },
    {
      "epoch": 2.2481610530391016,
      "grad_norm": 15.90139389038086,
      "learning_rate": 8.613154385512111e-06,
      "loss": 1.0386,
      "step": 5807
    },
    {
      "epoch": 2.248548199767712,
      "grad_norm": 26.998811721801758,
      "learning_rate": 8.61272422248032e-06,
      "loss": 1.4785,
      "step": 5808
    },
    {
      "epoch": 2.248935346496322,
      "grad_norm": 30.742525100708008,
      "learning_rate": 8.612294059448532e-06,
      "loss": 1.3066,
      "step": 5809
    },
    {
      "epoch": 2.2493224932249323,
      "grad_norm": 22.543323516845703,
      "learning_rate": 8.611863896416742e-06,
      "loss": 1.9727,
      "step": 5810
    },
    {
      "epoch": 2.2497096399535423,
      "grad_norm": 21.355884552001953,
      "learning_rate": 8.611433733384953e-06,
      "loss": 1.4427,
      "step": 5811
    },
    {
      "epoch": 2.2500967866821524,
      "grad_norm": 24.761812210083008,
      "learning_rate": 8.611003570353165e-06,
      "loss": 1.3406,
      "step": 5812
    },
    {
      "epoch": 2.250483933410763,
      "grad_norm": 13.608951568603516,
      "learning_rate": 8.610573407321376e-06,
      "loss": 1.3094,
      "step": 5813
    },
    {
      "epoch": 2.250871080139373,
      "grad_norm": 28.321794509887695,
      "learning_rate": 8.610143244289586e-06,
      "loss": 1.443,
      "step": 5814
    },
    {
      "epoch": 2.251258226867983,
      "grad_norm": 18.438037872314453,
      "learning_rate": 8.609713081257797e-06,
      "loss": 0.9389,
      "step": 5815
    },
    {
      "epoch": 2.251645373596593,
      "grad_norm": 15.45367431640625,
      "learning_rate": 8.609282918226009e-06,
      "loss": 1.4604,
      "step": 5816
    },
    {
      "epoch": 2.252032520325203,
      "grad_norm": 29.697111129760742,
      "learning_rate": 8.608852755194218e-06,
      "loss": 1.6964,
      "step": 5817
    },
    {
      "epoch": 2.2524196670538132,
      "grad_norm": 22.036439895629883,
      "learning_rate": 8.60842259216243e-06,
      "loss": 2.3696,
      "step": 5818
    },
    {
      "epoch": 2.2528068137824233,
      "grad_norm": 14.359818458557129,
      "learning_rate": 8.607992429130641e-06,
      "loss": 1.3535,
      "step": 5819
    },
    {
      "epoch": 2.253193960511034,
      "grad_norm": 19.69683837890625,
      "learning_rate": 8.607562266098853e-06,
      "loss": 1.6818,
      "step": 5820
    },
    {
      "epoch": 2.253581107239644,
      "grad_norm": 16.906143188476562,
      "learning_rate": 8.607132103067062e-06,
      "loss": 1.5011,
      "step": 5821
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 15.37514591217041,
      "learning_rate": 8.606701940035274e-06,
      "loss": 1.0552,
      "step": 5822
    },
    {
      "epoch": 2.254355400696864,
      "grad_norm": 21.444181442260742,
      "learning_rate": 8.606271777003485e-06,
      "loss": 1.9527,
      "step": 5823
    },
    {
      "epoch": 2.254742547425474,
      "grad_norm": 15.414813995361328,
      "learning_rate": 8.605841613971697e-06,
      "loss": 1.2435,
      "step": 5824
    },
    {
      "epoch": 2.2551296941540846,
      "grad_norm": 17.8336238861084,
      "learning_rate": 8.605411450939906e-06,
      "loss": 1.2223,
      "step": 5825
    },
    {
      "epoch": 2.2555168408826947,
      "grad_norm": 24.911949157714844,
      "learning_rate": 8.604981287908118e-06,
      "loss": 1.5399,
      "step": 5826
    },
    {
      "epoch": 2.2559039876113047,
      "grad_norm": 21.87189483642578,
      "learning_rate": 8.60455112487633e-06,
      "loss": 1.9043,
      "step": 5827
    },
    {
      "epoch": 2.256291134339915,
      "grad_norm": 21.627487182617188,
      "learning_rate": 8.60412096184454e-06,
      "loss": 1.2587,
      "step": 5828
    },
    {
      "epoch": 2.256678281068525,
      "grad_norm": 25.9980411529541,
      "learning_rate": 8.60369079881275e-06,
      "loss": 0.8685,
      "step": 5829
    },
    {
      "epoch": 2.257065427797135,
      "grad_norm": 31.177148818969727,
      "learning_rate": 8.603260635780962e-06,
      "loss": 2.062,
      "step": 5830
    },
    {
      "epoch": 2.2574525745257454,
      "grad_norm": 21.72126579284668,
      "learning_rate": 8.602830472749173e-06,
      "loss": 2.0085,
      "step": 5831
    },
    {
      "epoch": 2.2578397212543555,
      "grad_norm": 14.251598358154297,
      "learning_rate": 8.602400309717383e-06,
      "loss": 1.165,
      "step": 5832
    },
    {
      "epoch": 2.2582268679829656,
      "grad_norm": 19.698396682739258,
      "learning_rate": 8.601970146685594e-06,
      "loss": 1.6336,
      "step": 5833
    },
    {
      "epoch": 2.2586140147115756,
      "grad_norm": 18.10692596435547,
      "learning_rate": 8.601539983653806e-06,
      "loss": 1.6023,
      "step": 5834
    },
    {
      "epoch": 2.2590011614401857,
      "grad_norm": 20.29986572265625,
      "learning_rate": 8.601109820622017e-06,
      "loss": 1.9524,
      "step": 5835
    },
    {
      "epoch": 2.2593883081687958,
      "grad_norm": 39.18864440917969,
      "learning_rate": 8.600679657590227e-06,
      "loss": 2.9172,
      "step": 5836
    },
    {
      "epoch": 2.2597754548974063,
      "grad_norm": 25.739938735961914,
      "learning_rate": 8.600249494558438e-06,
      "loss": 1.9673,
      "step": 5837
    },
    {
      "epoch": 2.2601626016260163,
      "grad_norm": 10.219795227050781,
      "learning_rate": 8.59981933152665e-06,
      "loss": 1.1709,
      "step": 5838
    },
    {
      "epoch": 2.2605497483546264,
      "grad_norm": 12.849098205566406,
      "learning_rate": 8.599389168494861e-06,
      "loss": 0.8068,
      "step": 5839
    },
    {
      "epoch": 2.2609368950832365,
      "grad_norm": 30.379554748535156,
      "learning_rate": 8.59895900546307e-06,
      "loss": 0.7817,
      "step": 5840
    },
    {
      "epoch": 2.2613240418118465,
      "grad_norm": 22.571916580200195,
      "learning_rate": 8.598528842431282e-06,
      "loss": 1.3225,
      "step": 5841
    },
    {
      "epoch": 2.261711188540457,
      "grad_norm": 85.0419692993164,
      "learning_rate": 8.598098679399494e-06,
      "loss": 0.7242,
      "step": 5842
    },
    {
      "epoch": 2.262098335269067,
      "grad_norm": 15.76247787475586,
      "learning_rate": 8.597668516367705e-06,
      "loss": 1.1727,
      "step": 5843
    },
    {
      "epoch": 2.262485481997677,
      "grad_norm": 45.483760833740234,
      "learning_rate": 8.597238353335915e-06,
      "loss": 2.7498,
      "step": 5844
    },
    {
      "epoch": 2.2628726287262872,
      "grad_norm": 21.30520248413086,
      "learning_rate": 8.596808190304126e-06,
      "loss": 1.9976,
      "step": 5845
    },
    {
      "epoch": 2.2632597754548973,
      "grad_norm": 22.859098434448242,
      "learning_rate": 8.596378027272338e-06,
      "loss": 0.877,
      "step": 5846
    },
    {
      "epoch": 2.2636469221835074,
      "grad_norm": 22.26677894592285,
      "learning_rate": 8.595947864240547e-06,
      "loss": 1.4041,
      "step": 5847
    },
    {
      "epoch": 2.2640340689121174,
      "grad_norm": 15.843491554260254,
      "learning_rate": 8.595517701208759e-06,
      "loss": 1.1794,
      "step": 5848
    },
    {
      "epoch": 2.264421215640728,
      "grad_norm": 36.819969177246094,
      "learning_rate": 8.59508753817697e-06,
      "loss": 1.5475,
      "step": 5849
    },
    {
      "epoch": 2.264808362369338,
      "grad_norm": 22.88143539428711,
      "learning_rate": 8.594657375145182e-06,
      "loss": 1.8102,
      "step": 5850
    },
    {
      "epoch": 2.265195509097948,
      "grad_norm": 31.58234405517578,
      "learning_rate": 8.594227212113391e-06,
      "loss": 2.2531,
      "step": 5851
    },
    {
      "epoch": 2.265582655826558,
      "grad_norm": 29.171945571899414,
      "learning_rate": 8.593797049081603e-06,
      "loss": 2.4836,
      "step": 5852
    },
    {
      "epoch": 2.2659698025551682,
      "grad_norm": 52.59333038330078,
      "learning_rate": 8.593366886049812e-06,
      "loss": 1.0588,
      "step": 5853
    },
    {
      "epoch": 2.2663569492837787,
      "grad_norm": 3.6480112075805664,
      "learning_rate": 8.592936723018026e-06,
      "loss": 0.1181,
      "step": 5854
    },
    {
      "epoch": 2.266744096012389,
      "grad_norm": 14.072564125061035,
      "learning_rate": 8.592506559986235e-06,
      "loss": 1.228,
      "step": 5855
    },
    {
      "epoch": 2.267131242740999,
      "grad_norm": 20.77191925048828,
      "learning_rate": 8.592076396954447e-06,
      "loss": 1.4808,
      "step": 5856
    },
    {
      "epoch": 2.267518389469609,
      "grad_norm": 25.5130672454834,
      "learning_rate": 8.591646233922656e-06,
      "loss": 1.6461,
      "step": 5857
    },
    {
      "epoch": 2.267905536198219,
      "grad_norm": 15.93921184539795,
      "learning_rate": 8.59121607089087e-06,
      "loss": 0.6608,
      "step": 5858
    },
    {
      "epoch": 2.2682926829268295,
      "grad_norm": 23.193702697753906,
      "learning_rate": 8.59078590785908e-06,
      "loss": 2.0176,
      "step": 5859
    },
    {
      "epoch": 2.2686798296554396,
      "grad_norm": 32.6268310546875,
      "learning_rate": 8.59035574482729e-06,
      "loss": 1.8986,
      "step": 5860
    },
    {
      "epoch": 2.2690669763840496,
      "grad_norm": 18.901098251342773,
      "learning_rate": 8.5899255817955e-06,
      "loss": 1.3976,
      "step": 5861
    },
    {
      "epoch": 2.2694541231126597,
      "grad_norm": 15.795952796936035,
      "learning_rate": 8.589495418763712e-06,
      "loss": 0.8434,
      "step": 5862
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 27.013643264770508,
      "learning_rate": 8.589065255731923e-06,
      "loss": 1.9279,
      "step": 5863
    },
    {
      "epoch": 2.27022841656988,
      "grad_norm": 35.958316802978516,
      "learning_rate": 8.588635092700135e-06,
      "loss": 2.0167,
      "step": 5864
    },
    {
      "epoch": 2.27061556329849,
      "grad_norm": 17.57177734375,
      "learning_rate": 8.588204929668344e-06,
      "loss": 1.6602,
      "step": 5865
    },
    {
      "epoch": 2.2710027100271004,
      "grad_norm": 21.85748863220215,
      "learning_rate": 8.587774766636556e-06,
      "loss": 2.6685,
      "step": 5866
    },
    {
      "epoch": 2.2713898567557105,
      "grad_norm": 14.71509075164795,
      "learning_rate": 8.587344603604767e-06,
      "loss": 0.4961,
      "step": 5867
    },
    {
      "epoch": 2.2717770034843205,
      "grad_norm": 39.267601013183594,
      "learning_rate": 8.586914440572977e-06,
      "loss": 2.3818,
      "step": 5868
    },
    {
      "epoch": 2.2721641502129306,
      "grad_norm": 13.936033248901367,
      "learning_rate": 8.586484277541188e-06,
      "loss": 1.288,
      "step": 5869
    },
    {
      "epoch": 2.2725512969415407,
      "grad_norm": 29.54446792602539,
      "learning_rate": 8.5860541145094e-06,
      "loss": 1.3296,
      "step": 5870
    },
    {
      "epoch": 2.272938443670151,
      "grad_norm": 12.819576263427734,
      "learning_rate": 8.585623951477611e-06,
      "loss": 1.4783,
      "step": 5871
    },
    {
      "epoch": 2.2733255903987613,
      "grad_norm": 32.493370056152344,
      "learning_rate": 8.585193788445821e-06,
      "loss": 2.2686,
      "step": 5872
    },
    {
      "epoch": 2.2737127371273713,
      "grad_norm": 25.947229385375977,
      "learning_rate": 8.584763625414032e-06,
      "loss": 1.5394,
      "step": 5873
    },
    {
      "epoch": 2.2740998838559814,
      "grad_norm": 14.014128684997559,
      "learning_rate": 8.584333462382244e-06,
      "loss": 1.0157,
      "step": 5874
    },
    {
      "epoch": 2.2744870305845915,
      "grad_norm": 16.816097259521484,
      "learning_rate": 8.583903299350455e-06,
      "loss": 1.6174,
      "step": 5875
    },
    {
      "epoch": 2.2748741773132015,
      "grad_norm": 23.577024459838867,
      "learning_rate": 8.583473136318665e-06,
      "loss": 1.4619,
      "step": 5876
    },
    {
      "epoch": 2.275261324041812,
      "grad_norm": 29.178550720214844,
      "learning_rate": 8.583042973286876e-06,
      "loss": 0.9721,
      "step": 5877
    },
    {
      "epoch": 2.275648470770422,
      "grad_norm": 4.658807277679443,
      "learning_rate": 8.582612810255088e-06,
      "loss": 0.14,
      "step": 5878
    },
    {
      "epoch": 2.276035617499032,
      "grad_norm": 34.85592269897461,
      "learning_rate": 8.582182647223299e-06,
      "loss": 1.5508,
      "step": 5879
    },
    {
      "epoch": 2.2764227642276422,
      "grad_norm": 17.244356155395508,
      "learning_rate": 8.581752484191509e-06,
      "loss": 1.1349,
      "step": 5880
    },
    {
      "epoch": 2.2768099109562523,
      "grad_norm": 17.90730094909668,
      "learning_rate": 8.58132232115972e-06,
      "loss": 1.0498,
      "step": 5881
    },
    {
      "epoch": 2.2771970576848624,
      "grad_norm": 7.900460243225098,
      "learning_rate": 8.580892158127932e-06,
      "loss": 0.4143,
      "step": 5882
    },
    {
      "epoch": 2.277584204413473,
      "grad_norm": 25.346221923828125,
      "learning_rate": 8.580461995096141e-06,
      "loss": 1.5393,
      "step": 5883
    },
    {
      "epoch": 2.277971351142083,
      "grad_norm": 24.37812042236328,
      "learning_rate": 8.580031832064353e-06,
      "loss": 0.7257,
      "step": 5884
    },
    {
      "epoch": 2.278358497870693,
      "grad_norm": 17.1657772064209,
      "learning_rate": 8.579601669032564e-06,
      "loss": 1.3481,
      "step": 5885
    },
    {
      "epoch": 2.278745644599303,
      "grad_norm": 30.94816780090332,
      "learning_rate": 8.579171506000776e-06,
      "loss": 1.4961,
      "step": 5886
    },
    {
      "epoch": 2.279132791327913,
      "grad_norm": 43.44512176513672,
      "learning_rate": 8.578741342968985e-06,
      "loss": 1.5301,
      "step": 5887
    },
    {
      "epoch": 2.2795199380565236,
      "grad_norm": 21.499988555908203,
      "learning_rate": 8.578311179937197e-06,
      "loss": 1.5503,
      "step": 5888
    },
    {
      "epoch": 2.2799070847851337,
      "grad_norm": 29.01264190673828,
      "learning_rate": 8.577881016905408e-06,
      "loss": 1.8049,
      "step": 5889
    },
    {
      "epoch": 2.2802942315137438,
      "grad_norm": 20.69333267211914,
      "learning_rate": 8.57745085387362e-06,
      "loss": 1.3214,
      "step": 5890
    },
    {
      "epoch": 2.280681378242354,
      "grad_norm": 29.592679977416992,
      "learning_rate": 8.57702069084183e-06,
      "loss": 1.682,
      "step": 5891
    },
    {
      "epoch": 2.281068524970964,
      "grad_norm": 17.429492950439453,
      "learning_rate": 8.57659052781004e-06,
      "loss": 1.0066,
      "step": 5892
    },
    {
      "epoch": 2.281455671699574,
      "grad_norm": 15.0574951171875,
      "learning_rate": 8.576160364778252e-06,
      "loss": 0.8904,
      "step": 5893
    },
    {
      "epoch": 2.281842818428184,
      "grad_norm": 10.715339660644531,
      "learning_rate": 8.575730201746464e-06,
      "loss": 0.5159,
      "step": 5894
    },
    {
      "epoch": 2.2822299651567945,
      "grad_norm": 26.99582862854004,
      "learning_rate": 8.575300038714673e-06,
      "loss": 1.5895,
      "step": 5895
    },
    {
      "epoch": 2.2826171118854046,
      "grad_norm": 29.703136444091797,
      "learning_rate": 8.574869875682885e-06,
      "loss": 1.409,
      "step": 5896
    },
    {
      "epoch": 2.2830042586140147,
      "grad_norm": 6.702640056610107,
      "learning_rate": 8.574439712651096e-06,
      "loss": 0.1911,
      "step": 5897
    },
    {
      "epoch": 2.2833914053426247,
      "grad_norm": 18.711711883544922,
      "learning_rate": 8.574009549619306e-06,
      "loss": 1.6347,
      "step": 5898
    },
    {
      "epoch": 2.283778552071235,
      "grad_norm": 43.24456787109375,
      "learning_rate": 8.573579386587517e-06,
      "loss": 1.4636,
      "step": 5899
    },
    {
      "epoch": 2.2841656987998453,
      "grad_norm": 28.43694496154785,
      "learning_rate": 8.573149223555729e-06,
      "loss": 1.2414,
      "step": 5900
    },
    {
      "epoch": 2.2845528455284554,
      "grad_norm": 17.347755432128906,
      "learning_rate": 8.57271906052394e-06,
      "loss": 1.4425,
      "step": 5901
    },
    {
      "epoch": 2.2849399922570655,
      "grad_norm": 17.794940948486328,
      "learning_rate": 8.57228889749215e-06,
      "loss": 1.1395,
      "step": 5902
    },
    {
      "epoch": 2.2853271389856755,
      "grad_norm": 24.806188583374023,
      "learning_rate": 8.571858734460361e-06,
      "loss": 2.0695,
      "step": 5903
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 14.560234069824219,
      "learning_rate": 8.571428571428571e-06,
      "loss": 0.789,
      "step": 5904
    },
    {
      "epoch": 2.286101432442896,
      "grad_norm": 25.6607666015625,
      "learning_rate": 8.570998408396784e-06,
      "loss": 2.6928,
      "step": 5905
    },
    {
      "epoch": 2.286488579171506,
      "grad_norm": 23.97498893737793,
      "learning_rate": 8.570568245364994e-06,
      "loss": 0.9866,
      "step": 5906
    },
    {
      "epoch": 2.2868757259001162,
      "grad_norm": 18.379764556884766,
      "learning_rate": 8.570138082333205e-06,
      "loss": 1.0514,
      "step": 5907
    },
    {
      "epoch": 2.2872628726287263,
      "grad_norm": 35.645015716552734,
      "learning_rate": 8.569707919301415e-06,
      "loss": 1.6429,
      "step": 5908
    },
    {
      "epoch": 2.2876500193573364,
      "grad_norm": 22.296401977539062,
      "learning_rate": 8.569277756269628e-06,
      "loss": 1.8444,
      "step": 5909
    },
    {
      "epoch": 2.2880371660859464,
      "grad_norm": 24.638612747192383,
      "learning_rate": 8.568847593237838e-06,
      "loss": 1.6795,
      "step": 5910
    },
    {
      "epoch": 2.2884243128145565,
      "grad_norm": 27.832910537719727,
      "learning_rate": 8.568417430206049e-06,
      "loss": 1.754,
      "step": 5911
    },
    {
      "epoch": 2.288811459543167,
      "grad_norm": 40.753639221191406,
      "learning_rate": 8.567987267174259e-06,
      "loss": 1.0499,
      "step": 5912
    },
    {
      "epoch": 2.289198606271777,
      "grad_norm": 18.270151138305664,
      "learning_rate": 8.56755710414247e-06,
      "loss": 1.3979,
      "step": 5913
    },
    {
      "epoch": 2.289585753000387,
      "grad_norm": 25.64008331298828,
      "learning_rate": 8.567126941110682e-06,
      "loss": 1.4793,
      "step": 5914
    },
    {
      "epoch": 2.289972899728997,
      "grad_norm": 22.057111740112305,
      "learning_rate": 8.566696778078893e-06,
      "loss": 1.7741,
      "step": 5915
    },
    {
      "epoch": 2.2903600464576073,
      "grad_norm": 27.668399810791016,
      "learning_rate": 8.566266615047105e-06,
      "loss": 1.4326,
      "step": 5916
    },
    {
      "epoch": 2.290747193186218,
      "grad_norm": 14.242630958557129,
      "learning_rate": 8.565836452015314e-06,
      "loss": 1.4488,
      "step": 5917
    },
    {
      "epoch": 2.291134339914828,
      "grad_norm": 19.178951263427734,
      "learning_rate": 8.565406288983526e-06,
      "loss": 0.6663,
      "step": 5918
    },
    {
      "epoch": 2.291521486643438,
      "grad_norm": 14.272316932678223,
      "learning_rate": 8.564976125951735e-06,
      "loss": 0.9713,
      "step": 5919
    },
    {
      "epoch": 2.291908633372048,
      "grad_norm": 18.88576889038086,
      "learning_rate": 8.564545962919948e-06,
      "loss": 1.4723,
      "step": 5920
    },
    {
      "epoch": 2.292295780100658,
      "grad_norm": 19.24614715576172,
      "learning_rate": 8.564115799888158e-06,
      "loss": 1.5263,
      "step": 5921
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 19.383962631225586,
      "learning_rate": 8.56368563685637e-06,
      "loss": 1.3854,
      "step": 5922
    },
    {
      "epoch": 2.2930700735578786,
      "grad_norm": 15.138973236083984,
      "learning_rate": 8.56325547382458e-06,
      "loss": 1.1169,
      "step": 5923
    },
    {
      "epoch": 2.2934572202864887,
      "grad_norm": 12.800238609313965,
      "learning_rate": 8.562825310792792e-06,
      "loss": 1.1677,
      "step": 5924
    },
    {
      "epoch": 2.2938443670150988,
      "grad_norm": 25.966190338134766,
      "learning_rate": 8.562395147761002e-06,
      "loss": 2.343,
      "step": 5925
    },
    {
      "epoch": 2.294231513743709,
      "grad_norm": 40.625755310058594,
      "learning_rate": 8.561964984729214e-06,
      "loss": 1.4016,
      "step": 5926
    },
    {
      "epoch": 2.294618660472319,
      "grad_norm": 15.58193302154541,
      "learning_rate": 8.561534821697423e-06,
      "loss": 1.0292,
      "step": 5927
    },
    {
      "epoch": 2.295005807200929,
      "grad_norm": 11.691841125488281,
      "learning_rate": 8.561104658665635e-06,
      "loss": 0.7751,
      "step": 5928
    },
    {
      "epoch": 2.2953929539295395,
      "grad_norm": 13.226194381713867,
      "learning_rate": 8.560674495633846e-06,
      "loss": 1.0705,
      "step": 5929
    },
    {
      "epoch": 2.2957801006581495,
      "grad_norm": 9.521918296813965,
      "learning_rate": 8.560244332602058e-06,
      "loss": 0.4957,
      "step": 5930
    },
    {
      "epoch": 2.2961672473867596,
      "grad_norm": 13.955345153808594,
      "learning_rate": 8.559814169570267e-06,
      "loss": 1.151,
      "step": 5931
    },
    {
      "epoch": 2.2965543941153697,
      "grad_norm": 20.08509635925293,
      "learning_rate": 8.559384006538479e-06,
      "loss": 1.8241,
      "step": 5932
    },
    {
      "epoch": 2.2969415408439797,
      "grad_norm": 22.820274353027344,
      "learning_rate": 8.55895384350669e-06,
      "loss": 1.3219,
      "step": 5933
    },
    {
      "epoch": 2.2973286875725902,
      "grad_norm": 15.127276420593262,
      "learning_rate": 8.5585236804749e-06,
      "loss": 1.392,
      "step": 5934
    },
    {
      "epoch": 2.2977158343012003,
      "grad_norm": 6.989736080169678,
      "learning_rate": 8.558093517443111e-06,
      "loss": 0.3507,
      "step": 5935
    },
    {
      "epoch": 2.2981029810298104,
      "grad_norm": 23.379444122314453,
      "learning_rate": 8.557663354411323e-06,
      "loss": 1.9281,
      "step": 5936
    },
    {
      "epoch": 2.2984901277584204,
      "grad_norm": 22.79583168029785,
      "learning_rate": 8.557233191379534e-06,
      "loss": 1.776,
      "step": 5937
    },
    {
      "epoch": 2.2988772744870305,
      "grad_norm": 27.01609992980957,
      "learning_rate": 8.556803028347744e-06,
      "loss": 1.4755,
      "step": 5938
    },
    {
      "epoch": 2.2992644212156406,
      "grad_norm": 12.313289642333984,
      "learning_rate": 8.556372865315955e-06,
      "loss": 0.4925,
      "step": 5939
    },
    {
      "epoch": 2.2996515679442506,
      "grad_norm": 18.008575439453125,
      "learning_rate": 8.555942702284167e-06,
      "loss": 1.149,
      "step": 5940
    },
    {
      "epoch": 2.300038714672861,
      "grad_norm": 13.5087308883667,
      "learning_rate": 8.555512539252378e-06,
      "loss": 1.3881,
      "step": 5941
    },
    {
      "epoch": 2.300425861401471,
      "grad_norm": 18.88750457763672,
      "learning_rate": 8.555082376220588e-06,
      "loss": 1.1879,
      "step": 5942
    },
    {
      "epoch": 2.3008130081300813,
      "grad_norm": 30.213258743286133,
      "learning_rate": 8.5546522131888e-06,
      "loss": 1.9437,
      "step": 5943
    },
    {
      "epoch": 2.3012001548586913,
      "grad_norm": 35.8561897277832,
      "learning_rate": 8.55422205015701e-06,
      "loss": 1.7054,
      "step": 5944
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 15.559099197387695,
      "learning_rate": 8.553791887125222e-06,
      "loss": 1.376,
      "step": 5945
    },
    {
      "epoch": 2.301974448315912,
      "grad_norm": 22.6722469329834,
      "learning_rate": 8.553361724093432e-06,
      "loss": 1.9733,
      "step": 5946
    },
    {
      "epoch": 2.302361595044522,
      "grad_norm": 37.25730895996094,
      "learning_rate": 8.552931561061643e-06,
      "loss": 2.6373,
      "step": 5947
    },
    {
      "epoch": 2.302748741773132,
      "grad_norm": 13.856497764587402,
      "learning_rate": 8.552501398029855e-06,
      "loss": 0.9558,
      "step": 5948
    },
    {
      "epoch": 2.303135888501742,
      "grad_norm": 10.715883255004883,
      "learning_rate": 8.552071234998064e-06,
      "loss": 1.3297,
      "step": 5949
    },
    {
      "epoch": 2.303523035230352,
      "grad_norm": 30.69430160522461,
      "learning_rate": 8.551641071966276e-06,
      "loss": 1.844,
      "step": 5950
    },
    {
      "epoch": 2.3039101819589627,
      "grad_norm": 27.17557716369629,
      "learning_rate": 8.551210908934487e-06,
      "loss": 1.6999,
      "step": 5951
    },
    {
      "epoch": 2.3042973286875728,
      "grad_norm": 42.0362663269043,
      "learning_rate": 8.550780745902699e-06,
      "loss": 2.4551,
      "step": 5952
    },
    {
      "epoch": 2.304684475416183,
      "grad_norm": 15.861359596252441,
      "learning_rate": 8.550350582870908e-06,
      "loss": 0.9026,
      "step": 5953
    },
    {
      "epoch": 2.305071622144793,
      "grad_norm": 26.903596878051758,
      "learning_rate": 8.54992041983912e-06,
      "loss": 2.3438,
      "step": 5954
    },
    {
      "epoch": 2.305458768873403,
      "grad_norm": 31.09304428100586,
      "learning_rate": 8.54949025680733e-06,
      "loss": 1.1481,
      "step": 5955
    },
    {
      "epoch": 2.305845915602013,
      "grad_norm": 21.779882431030273,
      "learning_rate": 8.549060093775543e-06,
      "loss": 2.2754,
      "step": 5956
    },
    {
      "epoch": 2.306233062330623,
      "grad_norm": 16.760425567626953,
      "learning_rate": 8.548629930743752e-06,
      "loss": 0.7791,
      "step": 5957
    },
    {
      "epoch": 2.3066202090592336,
      "grad_norm": 33.45551681518555,
      "learning_rate": 8.548199767711964e-06,
      "loss": 1.3851,
      "step": 5958
    },
    {
      "epoch": 2.3070073557878437,
      "grad_norm": 24.65604019165039,
      "learning_rate": 8.547769604680175e-06,
      "loss": 1.2353,
      "step": 5959
    },
    {
      "epoch": 2.3073945025164537,
      "grad_norm": 30.857446670532227,
      "learning_rate": 8.547339441648386e-06,
      "loss": 1.9504,
      "step": 5960
    },
    {
      "epoch": 2.307781649245064,
      "grad_norm": 44.32413864135742,
      "learning_rate": 8.546909278616596e-06,
      "loss": 1.6748,
      "step": 5961
    },
    {
      "epoch": 2.308168795973674,
      "grad_norm": 12.473164558410645,
      "learning_rate": 8.546479115584808e-06,
      "loss": 0.7328,
      "step": 5962
    },
    {
      "epoch": 2.3085559427022844,
      "grad_norm": 33.396087646484375,
      "learning_rate": 8.546048952553019e-06,
      "loss": 0.7923,
      "step": 5963
    },
    {
      "epoch": 2.3089430894308944,
      "grad_norm": 11.87527847290039,
      "learning_rate": 8.545618789521229e-06,
      "loss": 1.0179,
      "step": 5964
    },
    {
      "epoch": 2.3093302361595045,
      "grad_norm": 14.695366859436035,
      "learning_rate": 8.54518862648944e-06,
      "loss": 0.8934,
      "step": 5965
    },
    {
      "epoch": 2.3097173828881146,
      "grad_norm": 39.04562759399414,
      "learning_rate": 8.544758463457652e-06,
      "loss": 1.513,
      "step": 5966
    },
    {
      "epoch": 2.3101045296167246,
      "grad_norm": 9.210719108581543,
      "learning_rate": 8.544328300425863e-06,
      "loss": 0.4254,
      "step": 5967
    },
    {
      "epoch": 2.3104916763453347,
      "grad_norm": 15.635775566101074,
      "learning_rate": 8.543898137394073e-06,
      "loss": 0.8854,
      "step": 5968
    },
    {
      "epoch": 2.310878823073945,
      "grad_norm": 22.521528244018555,
      "learning_rate": 8.543467974362284e-06,
      "loss": 1.4518,
      "step": 5969
    },
    {
      "epoch": 2.3112659698025553,
      "grad_norm": 14.389398574829102,
      "learning_rate": 8.543037811330494e-06,
      "loss": 1.0929,
      "step": 5970
    },
    {
      "epoch": 2.3116531165311653,
      "grad_norm": 15.698540687561035,
      "learning_rate": 8.542607648298707e-06,
      "loss": 0.9974,
      "step": 5971
    },
    {
      "epoch": 2.3120402632597754,
      "grad_norm": 23.57647705078125,
      "learning_rate": 8.542177485266917e-06,
      "loss": 1.7007,
      "step": 5972
    },
    {
      "epoch": 2.3124274099883855,
      "grad_norm": 43.24846267700195,
      "learning_rate": 8.541747322235128e-06,
      "loss": 2.3337,
      "step": 5973
    },
    {
      "epoch": 2.3128145567169955,
      "grad_norm": 23.26882553100586,
      "learning_rate": 8.541317159203338e-06,
      "loss": 1.4089,
      "step": 5974
    },
    {
      "epoch": 2.313201703445606,
      "grad_norm": 24.226516723632812,
      "learning_rate": 8.540886996171551e-06,
      "loss": 2.7477,
      "step": 5975
    },
    {
      "epoch": 2.313588850174216,
      "grad_norm": 28.15823745727539,
      "learning_rate": 8.54045683313976e-06,
      "loss": 1.3248,
      "step": 5976
    },
    {
      "epoch": 2.313975996902826,
      "grad_norm": 28.797555923461914,
      "learning_rate": 8.540026670107972e-06,
      "loss": 2.2061,
      "step": 5977
    },
    {
      "epoch": 2.3143631436314362,
      "grad_norm": 18.74573516845703,
      "learning_rate": 8.539596507076182e-06,
      "loss": 1.2345,
      "step": 5978
    },
    {
      "epoch": 2.3147502903600463,
      "grad_norm": 28.362958908081055,
      "learning_rate": 8.539166344044393e-06,
      "loss": 1.0318,
      "step": 5979
    },
    {
      "epoch": 2.315137437088657,
      "grad_norm": 17.23097038269043,
      "learning_rate": 8.538736181012605e-06,
      "loss": 1.655,
      "step": 5980
    },
    {
      "epoch": 2.315524583817267,
      "grad_norm": 17.251867294311523,
      "learning_rate": 8.538306017980816e-06,
      "loss": 1.5163,
      "step": 5981
    },
    {
      "epoch": 2.315911730545877,
      "grad_norm": 28.28480339050293,
      "learning_rate": 8.537875854949026e-06,
      "loss": 2.4346,
      "step": 5982
    },
    {
      "epoch": 2.316298877274487,
      "grad_norm": 21.688993453979492,
      "learning_rate": 8.537445691917237e-06,
      "loss": 1.3536,
      "step": 5983
    },
    {
      "epoch": 2.316686024003097,
      "grad_norm": 43.519012451171875,
      "learning_rate": 8.537015528885449e-06,
      "loss": 2.5382,
      "step": 5984
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 27.816585540771484,
      "learning_rate": 8.536585365853658e-06,
      "loss": 1.8367,
      "step": 5985
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 35.30522918701172,
      "learning_rate": 8.53615520282187e-06,
      "loss": 1.5206,
      "step": 5986
    },
    {
      "epoch": 2.3178474641889277,
      "grad_norm": 13.695777893066406,
      "learning_rate": 8.535725039790081e-06,
      "loss": 0.8895,
      "step": 5987
    },
    {
      "epoch": 2.318234610917538,
      "grad_norm": 27.723520278930664,
      "learning_rate": 8.535294876758293e-06,
      "loss": 1.2489,
      "step": 5988
    },
    {
      "epoch": 2.318621757646148,
      "grad_norm": 19.440977096557617,
      "learning_rate": 8.534864713726502e-06,
      "loss": 0.8122,
      "step": 5989
    },
    {
      "epoch": 2.319008904374758,
      "grad_norm": 14.704282760620117,
      "learning_rate": 8.534434550694714e-06,
      "loss": 1.5016,
      "step": 5990
    },
    {
      "epoch": 2.319396051103368,
      "grad_norm": 22.71788215637207,
      "learning_rate": 8.534004387662925e-06,
      "loss": 1.4607,
      "step": 5991
    },
    {
      "epoch": 2.3197831978319785,
      "grad_norm": 52.53942108154297,
      "learning_rate": 8.533574224631137e-06,
      "loss": 3.3961,
      "step": 5992
    },
    {
      "epoch": 2.3201703445605886,
      "grad_norm": 14.533709526062012,
      "learning_rate": 8.533144061599346e-06,
      "loss": 2.1342,
      "step": 5993
    },
    {
      "epoch": 2.3205574912891986,
      "grad_norm": 15.700026512145996,
      "learning_rate": 8.532713898567558e-06,
      "loss": 1.2357,
      "step": 5994
    },
    {
      "epoch": 2.3209446380178087,
      "grad_norm": 23.105125427246094,
      "learning_rate": 8.532283735535769e-06,
      "loss": 1.0016,
      "step": 5995
    },
    {
      "epoch": 2.3213317847464188,
      "grad_norm": 23.308025360107422,
      "learning_rate": 8.53185357250398e-06,
      "loss": 1.1822,
      "step": 5996
    },
    {
      "epoch": 2.3217189314750293,
      "grad_norm": 23.91897201538086,
      "learning_rate": 8.53142340947219e-06,
      "loss": 1.9838,
      "step": 5997
    },
    {
      "epoch": 2.3221060782036393,
      "grad_norm": 17.86998176574707,
      "learning_rate": 8.530993246440402e-06,
      "loss": 1.1892,
      "step": 5998
    },
    {
      "epoch": 2.3224932249322494,
      "grad_norm": 13.668244361877441,
      "learning_rate": 8.530563083408613e-06,
      "loss": 0.861,
      "step": 5999
    },
    {
      "epoch": 2.3228803716608595,
      "grad_norm": 23.004535675048828,
      "learning_rate": 8.530132920376823e-06,
      "loss": 1.398,
      "step": 6000
    },
    {
      "epoch": 2.3232675183894695,
      "grad_norm": 27.174230575561523,
      "learning_rate": 8.529702757345034e-06,
      "loss": 1.5479,
      "step": 6001
    },
    {
      "epoch": 2.3236546651180796,
      "grad_norm": 26.426645278930664,
      "learning_rate": 8.529272594313246e-06,
      "loss": 2.3814,
      "step": 6002
    },
    {
      "epoch": 2.3240418118466897,
      "grad_norm": 16.85911750793457,
      "learning_rate": 8.528842431281457e-06,
      "loss": 1.4816,
      "step": 6003
    },
    {
      "epoch": 2.3244289585753,
      "grad_norm": 28.22122573852539,
      "learning_rate": 8.528412268249667e-06,
      "loss": 1.8296,
      "step": 6004
    },
    {
      "epoch": 2.3248161053039103,
      "grad_norm": 23.26055908203125,
      "learning_rate": 8.527982105217878e-06,
      "loss": 1.4539,
      "step": 6005
    },
    {
      "epoch": 2.3252032520325203,
      "grad_norm": 47.227012634277344,
      "learning_rate": 8.52755194218609e-06,
      "loss": 2.0327,
      "step": 6006
    },
    {
      "epoch": 2.3255903987611304,
      "grad_norm": 16.82936668395996,
      "learning_rate": 8.527121779154301e-06,
      "loss": 0.8086,
      "step": 6007
    },
    {
      "epoch": 2.3259775454897405,
      "grad_norm": 28.137964248657227,
      "learning_rate": 8.52669161612251e-06,
      "loss": 2.0425,
      "step": 6008
    },
    {
      "epoch": 2.326364692218351,
      "grad_norm": 21.705463409423828,
      "learning_rate": 8.526261453090722e-06,
      "loss": 1.1915,
      "step": 6009
    },
    {
      "epoch": 2.326751838946961,
      "grad_norm": 24.850717544555664,
      "learning_rate": 8.525831290058934e-06,
      "loss": 1.417,
      "step": 6010
    },
    {
      "epoch": 2.327138985675571,
      "grad_norm": 26.41318130493164,
      "learning_rate": 8.525401127027145e-06,
      "loss": 1.6294,
      "step": 6011
    },
    {
      "epoch": 2.327526132404181,
      "grad_norm": 23.339990615844727,
      "learning_rate": 8.524970963995355e-06,
      "loss": 1.8494,
      "step": 6012
    },
    {
      "epoch": 2.3279132791327912,
      "grad_norm": 26.83950424194336,
      "learning_rate": 8.524540800963566e-06,
      "loss": 1.1214,
      "step": 6013
    },
    {
      "epoch": 2.3283004258614013,
      "grad_norm": 10.084704399108887,
      "learning_rate": 8.524110637931778e-06,
      "loss": 1.006,
      "step": 6014
    },
    {
      "epoch": 2.328687572590012,
      "grad_norm": 11.148438453674316,
      "learning_rate": 8.523680474899987e-06,
      "loss": 0.6643,
      "step": 6015
    },
    {
      "epoch": 2.329074719318622,
      "grad_norm": 22.79640769958496,
      "learning_rate": 8.523250311868199e-06,
      "loss": 1.269,
      "step": 6016
    },
    {
      "epoch": 2.329461866047232,
      "grad_norm": 23.92296028137207,
      "learning_rate": 8.52282014883641e-06,
      "loss": 1.7822,
      "step": 6017
    },
    {
      "epoch": 2.329849012775842,
      "grad_norm": 15.65219497680664,
      "learning_rate": 8.522389985804621e-06,
      "loss": 1.2299,
      "step": 6018
    },
    {
      "epoch": 2.330236159504452,
      "grad_norm": 15.058509826660156,
      "learning_rate": 8.521959822772831e-06,
      "loss": 1.1115,
      "step": 6019
    },
    {
      "epoch": 2.330623306233062,
      "grad_norm": 18.064149856567383,
      "learning_rate": 8.521529659741043e-06,
      "loss": 0.5786,
      "step": 6020
    },
    {
      "epoch": 2.3310104529616726,
      "grad_norm": 27.42302131652832,
      "learning_rate": 8.521099496709252e-06,
      "loss": 1.3913,
      "step": 6021
    },
    {
      "epoch": 2.3313975996902827,
      "grad_norm": 15.220134735107422,
      "learning_rate": 8.520669333677465e-06,
      "loss": 1.4896,
      "step": 6022
    },
    {
      "epoch": 2.3317847464188928,
      "grad_norm": 31.88533592224121,
      "learning_rate": 8.520239170645675e-06,
      "loss": 1.229,
      "step": 6023
    },
    {
      "epoch": 2.332171893147503,
      "grad_norm": 14.431973457336426,
      "learning_rate": 8.519809007613887e-06,
      "loss": 1.1737,
      "step": 6024
    },
    {
      "epoch": 2.332559039876113,
      "grad_norm": 14.14016342163086,
      "learning_rate": 8.519378844582096e-06,
      "loss": 1.1932,
      "step": 6025
    },
    {
      "epoch": 2.3329461866047234,
      "grad_norm": 33.09461975097656,
      "learning_rate": 8.51894868155031e-06,
      "loss": 1.1033,
      "step": 6026
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 14.268733978271484,
      "learning_rate": 8.518518518518519e-06,
      "loss": 1.3387,
      "step": 6027
    },
    {
      "epoch": 2.3337204800619435,
      "grad_norm": 15.690348625183105,
      "learning_rate": 8.51808835548673e-06,
      "loss": 0.8477,
      "step": 6028
    },
    {
      "epoch": 2.3341076267905536,
      "grad_norm": 12.931178092956543,
      "learning_rate": 8.51765819245494e-06,
      "loss": 0.4314,
      "step": 6029
    },
    {
      "epoch": 2.3344947735191637,
      "grad_norm": 18.43888282775879,
      "learning_rate": 8.517228029423152e-06,
      "loss": 1.7637,
      "step": 6030
    },
    {
      "epoch": 2.3348819202477737,
      "grad_norm": 12.144197463989258,
      "learning_rate": 8.516797866391363e-06,
      "loss": 0.9531,
      "step": 6031
    },
    {
      "epoch": 2.335269066976384,
      "grad_norm": 46.465736389160156,
      "learning_rate": 8.516367703359575e-06,
      "loss": 1.7667,
      "step": 6032
    },
    {
      "epoch": 2.3356562137049943,
      "grad_norm": 25.471223831176758,
      "learning_rate": 8.515937540327784e-06,
      "loss": 1.386,
      "step": 6033
    },
    {
      "epoch": 2.3360433604336044,
      "grad_norm": 40.71974563598633,
      "learning_rate": 8.515507377295996e-06,
      "loss": 2.8473,
      "step": 6034
    },
    {
      "epoch": 2.3364305071622145,
      "grad_norm": 15.707852363586426,
      "learning_rate": 8.515077214264207e-06,
      "loss": 1.0611,
      "step": 6035
    },
    {
      "epoch": 2.3368176538908245,
      "grad_norm": 25.430994033813477,
      "learning_rate": 8.514647051232417e-06,
      "loss": 1.479,
      "step": 6036
    },
    {
      "epoch": 2.3372048006194346,
      "grad_norm": 30.84294891357422,
      "learning_rate": 8.514216888200628e-06,
      "loss": 1.6998,
      "step": 6037
    },
    {
      "epoch": 2.337591947348045,
      "grad_norm": 22.88399314880371,
      "learning_rate": 8.51378672516884e-06,
      "loss": 1.885,
      "step": 6038
    },
    {
      "epoch": 2.337979094076655,
      "grad_norm": 15.586702346801758,
      "learning_rate": 8.513356562137051e-06,
      "loss": 0.9641,
      "step": 6039
    },
    {
      "epoch": 2.3383662408052652,
      "grad_norm": 27.131685256958008,
      "learning_rate": 8.51292639910526e-06,
      "loss": 2.1385,
      "step": 6040
    },
    {
      "epoch": 2.3387533875338753,
      "grad_norm": 27.42022705078125,
      "learning_rate": 8.512496236073474e-06,
      "loss": 2.7376,
      "step": 6041
    },
    {
      "epoch": 2.3391405342624854,
      "grad_norm": 19.00783920288086,
      "learning_rate": 8.512066073041684e-06,
      "loss": 1.1372,
      "step": 6042
    },
    {
      "epoch": 2.339527680991096,
      "grad_norm": 25.46596908569336,
      "learning_rate": 8.511635910009895e-06,
      "loss": 0.6437,
      "step": 6043
    },
    {
      "epoch": 2.339914827719706,
      "grad_norm": 22.971677780151367,
      "learning_rate": 8.511205746978105e-06,
      "loss": 1.9523,
      "step": 6044
    },
    {
      "epoch": 2.340301974448316,
      "grad_norm": 25.419803619384766,
      "learning_rate": 8.510775583946316e-06,
      "loss": 1.6105,
      "step": 6045
    },
    {
      "epoch": 2.340689121176926,
      "grad_norm": 19.76164436340332,
      "learning_rate": 8.510345420914528e-06,
      "loss": 1.3991,
      "step": 6046
    },
    {
      "epoch": 2.341076267905536,
      "grad_norm": 24.852937698364258,
      "learning_rate": 8.509915257882739e-06,
      "loss": 1.6922,
      "step": 6047
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 15.184470176696777,
      "learning_rate": 8.509485094850949e-06,
      "loss": 0.9656,
      "step": 6048
    },
    {
      "epoch": 2.3418505613627563,
      "grad_norm": 25.268535614013672,
      "learning_rate": 8.50905493181916e-06,
      "loss": 1.4864,
      "step": 6049
    },
    {
      "epoch": 2.3422377080913668,
      "grad_norm": 10.595870018005371,
      "learning_rate": 8.508624768787372e-06,
      "loss": 0.6277,
      "step": 6050
    },
    {
      "epoch": 2.342624854819977,
      "grad_norm": 15.22640609741211,
      "learning_rate": 8.508194605755581e-06,
      "loss": 1.1454,
      "step": 6051
    },
    {
      "epoch": 2.343012001548587,
      "grad_norm": 22.945003509521484,
      "learning_rate": 8.507764442723793e-06,
      "loss": 1.5842,
      "step": 6052
    },
    {
      "epoch": 2.343399148277197,
      "grad_norm": 47.86716079711914,
      "learning_rate": 8.507334279692004e-06,
      "loss": 2.0097,
      "step": 6053
    },
    {
      "epoch": 2.343786295005807,
      "grad_norm": 23.228858947753906,
      "learning_rate": 8.506904116660216e-06,
      "loss": 1.87,
      "step": 6054
    },
    {
      "epoch": 2.3441734417344176,
      "grad_norm": 29.96240997314453,
      "learning_rate": 8.506473953628425e-06,
      "loss": 1.5753,
      "step": 6055
    },
    {
      "epoch": 2.3445605884630276,
      "grad_norm": 17.012042999267578,
      "learning_rate": 8.506043790596637e-06,
      "loss": 1.0612,
      "step": 6056
    },
    {
      "epoch": 2.3449477351916377,
      "grad_norm": 22.388755798339844,
      "learning_rate": 8.505613627564848e-06,
      "loss": 2.2929,
      "step": 6057
    },
    {
      "epoch": 2.3453348819202477,
      "grad_norm": 17.4726505279541,
      "learning_rate": 8.50518346453306e-06,
      "loss": 1.1956,
      "step": 6058
    },
    {
      "epoch": 2.345722028648858,
      "grad_norm": 17.633182525634766,
      "learning_rate": 8.50475330150127e-06,
      "loss": 2.0481,
      "step": 6059
    },
    {
      "epoch": 2.346109175377468,
      "grad_norm": 9.100809097290039,
      "learning_rate": 8.50432313846948e-06,
      "loss": 0.58,
      "step": 6060
    },
    {
      "epoch": 2.3464963221060784,
      "grad_norm": 29.160661697387695,
      "learning_rate": 8.503892975437692e-06,
      "loss": 2.1093,
      "step": 6061
    },
    {
      "epoch": 2.3468834688346885,
      "grad_norm": 27.791297912597656,
      "learning_rate": 8.503462812405903e-06,
      "loss": 1.7204,
      "step": 6062
    },
    {
      "epoch": 2.3472706155632985,
      "grad_norm": 46.624237060546875,
      "learning_rate": 8.503032649374113e-06,
      "loss": 2.1875,
      "step": 6063
    },
    {
      "epoch": 2.3476577622919086,
      "grad_norm": 21.44854736328125,
      "learning_rate": 8.502602486342325e-06,
      "loss": 0.8581,
      "step": 6064
    },
    {
      "epoch": 2.3480449090205187,
      "grad_norm": 23.176271438598633,
      "learning_rate": 8.502172323310536e-06,
      "loss": 1.1746,
      "step": 6065
    },
    {
      "epoch": 2.3484320557491287,
      "grad_norm": 32.72173309326172,
      "learning_rate": 8.501742160278746e-06,
      "loss": 2.0371,
      "step": 6066
    },
    {
      "epoch": 2.3488192024777392,
      "grad_norm": 30.156156539916992,
      "learning_rate": 8.501311997246957e-06,
      "loss": 1.927,
      "step": 6067
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 47.236881256103516,
      "learning_rate": 8.500881834215169e-06,
      "loss": 1.9221,
      "step": 6068
    },
    {
      "epoch": 2.3495934959349594,
      "grad_norm": 24.219877243041992,
      "learning_rate": 8.50045167118338e-06,
      "loss": 1.8527,
      "step": 6069
    },
    {
      "epoch": 2.3499806426635694,
      "grad_norm": 19.510251998901367,
      "learning_rate": 8.50002150815159e-06,
      "loss": 1.235,
      "step": 6070
    },
    {
      "epoch": 2.3503677893921795,
      "grad_norm": 19.755388259887695,
      "learning_rate": 8.499591345119801e-06,
      "loss": 1.2081,
      "step": 6071
    },
    {
      "epoch": 2.35075493612079,
      "grad_norm": 13.861082077026367,
      "learning_rate": 8.49916118208801e-06,
      "loss": 1.414,
      "step": 6072
    },
    {
      "epoch": 2.3511420828494,
      "grad_norm": 28.605873107910156,
      "learning_rate": 8.498731019056224e-06,
      "loss": 1.274,
      "step": 6073
    },
    {
      "epoch": 2.35152922957801,
      "grad_norm": 20.43401336669922,
      "learning_rate": 8.498300856024434e-06,
      "loss": 0.9724,
      "step": 6074
    },
    {
      "epoch": 2.35191637630662,
      "grad_norm": 20.86387825012207,
      "learning_rate": 8.497870692992645e-06,
      "loss": 1.4295,
      "step": 6075
    },
    {
      "epoch": 2.3523035230352303,
      "grad_norm": 28.791316986083984,
      "learning_rate": 8.497440529960855e-06,
      "loss": 1.3332,
      "step": 6076
    },
    {
      "epoch": 2.3526906697638403,
      "grad_norm": 22.333049774169922,
      "learning_rate": 8.497010366929068e-06,
      "loss": 1.0831,
      "step": 6077
    },
    {
      "epoch": 2.3530778164924504,
      "grad_norm": 13.751280784606934,
      "learning_rate": 8.496580203897278e-06,
      "loss": 1.416,
      "step": 6078
    },
    {
      "epoch": 2.353464963221061,
      "grad_norm": 19.074098587036133,
      "learning_rate": 8.496150040865489e-06,
      "loss": 1.2319,
      "step": 6079
    },
    {
      "epoch": 2.353852109949671,
      "grad_norm": 27.493257522583008,
      "learning_rate": 8.495719877833699e-06,
      "loss": 1.6841,
      "step": 6080
    },
    {
      "epoch": 2.354239256678281,
      "grad_norm": 18.577939987182617,
      "learning_rate": 8.49528971480191e-06,
      "loss": 1.3466,
      "step": 6081
    },
    {
      "epoch": 2.354626403406891,
      "grad_norm": 51.275001525878906,
      "learning_rate": 8.494859551770122e-06,
      "loss": 2.2817,
      "step": 6082
    },
    {
      "epoch": 2.355013550135501,
      "grad_norm": 30.395559310913086,
      "learning_rate": 8.494429388738333e-06,
      "loss": 1.8746,
      "step": 6083
    },
    {
      "epoch": 2.3554006968641117,
      "grad_norm": 29.6375732421875,
      "learning_rate": 8.493999225706544e-06,
      "loss": 1.5475,
      "step": 6084
    },
    {
      "epoch": 2.3557878435927218,
      "grad_norm": 13.170792579650879,
      "learning_rate": 8.493569062674754e-06,
      "loss": 0.9018,
      "step": 6085
    },
    {
      "epoch": 2.356174990321332,
      "grad_norm": 13.7869291305542,
      "learning_rate": 8.493138899642966e-06,
      "loss": 0.8453,
      "step": 6086
    },
    {
      "epoch": 2.356562137049942,
      "grad_norm": 16.83365821838379,
      "learning_rate": 8.492708736611175e-06,
      "loss": 1.1709,
      "step": 6087
    },
    {
      "epoch": 2.356949283778552,
      "grad_norm": 20.208683013916016,
      "learning_rate": 8.492278573579388e-06,
      "loss": 1.1886,
      "step": 6088
    },
    {
      "epoch": 2.3573364305071625,
      "grad_norm": 21.342334747314453,
      "learning_rate": 8.491848410547598e-06,
      "loss": 1.7302,
      "step": 6089
    },
    {
      "epoch": 2.3577235772357725,
      "grad_norm": 26.334938049316406,
      "learning_rate": 8.49141824751581e-06,
      "loss": 2.0816,
      "step": 6090
    },
    {
      "epoch": 2.3581107239643826,
      "grad_norm": 33.85717010498047,
      "learning_rate": 8.49098808448402e-06,
      "loss": 1.4588,
      "step": 6091
    },
    {
      "epoch": 2.3584978706929927,
      "grad_norm": 44.98942947387695,
      "learning_rate": 8.490557921452232e-06,
      "loss": 2.38,
      "step": 6092
    },
    {
      "epoch": 2.3588850174216027,
      "grad_norm": 10.720662117004395,
      "learning_rate": 8.490127758420442e-06,
      "loss": 0.6302,
      "step": 6093
    },
    {
      "epoch": 2.359272164150213,
      "grad_norm": 21.3302059173584,
      "learning_rate": 8.489697595388654e-06,
      "loss": 1.7719,
      "step": 6094
    },
    {
      "epoch": 2.359659310878823,
      "grad_norm": 17.52560806274414,
      "learning_rate": 8.489267432356863e-06,
      "loss": 1.4083,
      "step": 6095
    },
    {
      "epoch": 2.3600464576074334,
      "grad_norm": 23.760282516479492,
      "learning_rate": 8.488837269325075e-06,
      "loss": 1.6304,
      "step": 6096
    },
    {
      "epoch": 2.3604336043360434,
      "grad_norm": 27.8863468170166,
      "learning_rate": 8.488407106293286e-06,
      "loss": 1.5484,
      "step": 6097
    },
    {
      "epoch": 2.3608207510646535,
      "grad_norm": 31.611967086791992,
      "learning_rate": 8.487976943261497e-06,
      "loss": 1.8256,
      "step": 6098
    },
    {
      "epoch": 2.3612078977932636,
      "grad_norm": 12.421359062194824,
      "learning_rate": 8.487546780229707e-06,
      "loss": 0.7086,
      "step": 6099
    },
    {
      "epoch": 2.3615950445218736,
      "grad_norm": 27.077621459960938,
      "learning_rate": 8.487116617197919e-06,
      "loss": 1.326,
      "step": 6100
    },
    {
      "epoch": 2.361982191250484,
      "grad_norm": 22.83156394958496,
      "learning_rate": 8.48668645416613e-06,
      "loss": 1.6352,
      "step": 6101
    },
    {
      "epoch": 2.362369337979094,
      "grad_norm": 20.232633590698242,
      "learning_rate": 8.48625629113434e-06,
      "loss": 1.802,
      "step": 6102
    },
    {
      "epoch": 2.3627564847077043,
      "grad_norm": 16.33730125427246,
      "learning_rate": 8.485826128102551e-06,
      "loss": 1.5851,
      "step": 6103
    },
    {
      "epoch": 2.3631436314363143,
      "grad_norm": 46.755184173583984,
      "learning_rate": 8.485395965070763e-06,
      "loss": 2.3087,
      "step": 6104
    },
    {
      "epoch": 2.3635307781649244,
      "grad_norm": 14.439321517944336,
      "learning_rate": 8.484965802038974e-06,
      "loss": 1.0292,
      "step": 6105
    },
    {
      "epoch": 2.3639179248935345,
      "grad_norm": 29.716867446899414,
      "learning_rate": 8.484535639007184e-06,
      "loss": 2.0975,
      "step": 6106
    },
    {
      "epoch": 2.364305071622145,
      "grad_norm": 8.63700008392334,
      "learning_rate": 8.484105475975395e-06,
      "loss": 0.5378,
      "step": 6107
    },
    {
      "epoch": 2.364692218350755,
      "grad_norm": 26.96453094482422,
      "learning_rate": 8.483675312943607e-06,
      "loss": 1.6303,
      "step": 6108
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 25.89319610595703,
      "learning_rate": 8.483245149911818e-06,
      "loss": 1.7185,
      "step": 6109
    },
    {
      "epoch": 2.365466511807975,
      "grad_norm": 22.92190170288086,
      "learning_rate": 8.482814986880028e-06,
      "loss": 1.6592,
      "step": 6110
    },
    {
      "epoch": 2.3658536585365852,
      "grad_norm": 12.91634464263916,
      "learning_rate": 8.482384823848239e-06,
      "loss": 0.971,
      "step": 6111
    },
    {
      "epoch": 2.3662408052651953,
      "grad_norm": 21.975934982299805,
      "learning_rate": 8.48195466081645e-06,
      "loss": 1.9798,
      "step": 6112
    },
    {
      "epoch": 2.366627951993806,
      "grad_norm": 16.361608505249023,
      "learning_rate": 8.481524497784662e-06,
      "loss": 1.0327,
      "step": 6113
    },
    {
      "epoch": 2.367015098722416,
      "grad_norm": 27.19974136352539,
      "learning_rate": 8.481094334752872e-06,
      "loss": 1.7242,
      "step": 6114
    },
    {
      "epoch": 2.367402245451026,
      "grad_norm": 18.014638900756836,
      "learning_rate": 8.480664171721083e-06,
      "loss": 1.2694,
      "step": 6115
    },
    {
      "epoch": 2.367789392179636,
      "grad_norm": 31.203819274902344,
      "learning_rate": 8.480234008689294e-06,
      "loss": 1.7165,
      "step": 6116
    },
    {
      "epoch": 2.368176538908246,
      "grad_norm": 23.3695068359375,
      "learning_rate": 8.479803845657504e-06,
      "loss": 1.1797,
      "step": 6117
    },
    {
      "epoch": 2.3685636856368566,
      "grad_norm": 16.258609771728516,
      "learning_rate": 8.479373682625716e-06,
      "loss": 1.2415,
      "step": 6118
    },
    {
      "epoch": 2.3689508323654667,
      "grad_norm": 20.679019927978516,
      "learning_rate": 8.478943519593927e-06,
      "loss": 3.4297,
      "step": 6119
    },
    {
      "epoch": 2.3693379790940767,
      "grad_norm": 16.84478187561035,
      "learning_rate": 8.478513356562138e-06,
      "loss": 0.9796,
      "step": 6120
    },
    {
      "epoch": 2.369725125822687,
      "grad_norm": 15.321216583251953,
      "learning_rate": 8.478083193530348e-06,
      "loss": 0.8348,
      "step": 6121
    },
    {
      "epoch": 2.370112272551297,
      "grad_norm": 13.33958625793457,
      "learning_rate": 8.47765303049856e-06,
      "loss": 0.8527,
      "step": 6122
    },
    {
      "epoch": 2.370499419279907,
      "grad_norm": 14.990398406982422,
      "learning_rate": 8.477222867466771e-06,
      "loss": 1.1552,
      "step": 6123
    },
    {
      "epoch": 2.370886566008517,
      "grad_norm": 14.010930061340332,
      "learning_rate": 8.476792704434982e-06,
      "loss": 1.0301,
      "step": 6124
    },
    {
      "epoch": 2.3712737127371275,
      "grad_norm": 17.205331802368164,
      "learning_rate": 8.476362541403192e-06,
      "loss": 1.1093,
      "step": 6125
    },
    {
      "epoch": 2.3716608594657376,
      "grad_norm": 15.638218879699707,
      "learning_rate": 8.475932378371404e-06,
      "loss": 1.0271,
      "step": 6126
    },
    {
      "epoch": 2.3720480061943476,
      "grad_norm": 18.689308166503906,
      "learning_rate": 8.475502215339615e-06,
      "loss": 1.6234,
      "step": 6127
    },
    {
      "epoch": 2.3724351529229577,
      "grad_norm": 17.18704605102539,
      "learning_rate": 8.475072052307826e-06,
      "loss": 1.3337,
      "step": 6128
    },
    {
      "epoch": 2.3728222996515678,
      "grad_norm": 17.278926849365234,
      "learning_rate": 8.474641889276036e-06,
      "loss": 1.0539,
      "step": 6129
    },
    {
      "epoch": 2.3732094463801783,
      "grad_norm": 13.267315864562988,
      "learning_rate": 8.474211726244248e-06,
      "loss": 1.4297,
      "step": 6130
    },
    {
      "epoch": 2.3735965931087883,
      "grad_norm": 13.272462844848633,
      "learning_rate": 8.473781563212459e-06,
      "loss": 0.7923,
      "step": 6131
    },
    {
      "epoch": 2.3739837398373984,
      "grad_norm": 41.11898422241211,
      "learning_rate": 8.473351400180669e-06,
      "loss": 1.8332,
      "step": 6132
    },
    {
      "epoch": 2.3743708865660085,
      "grad_norm": 20.689790725708008,
      "learning_rate": 8.47292123714888e-06,
      "loss": 1.7742,
      "step": 6133
    },
    {
      "epoch": 2.3747580332946185,
      "grad_norm": 32.169715881347656,
      "learning_rate": 8.472491074117092e-06,
      "loss": 1.2065,
      "step": 6134
    },
    {
      "epoch": 2.375145180023229,
      "grad_norm": 32.09081268310547,
      "learning_rate": 8.472060911085303e-06,
      "loss": 0.7741,
      "step": 6135
    },
    {
      "epoch": 2.375532326751839,
      "grad_norm": 19.06886100769043,
      "learning_rate": 8.471630748053513e-06,
      "loss": 1.9915,
      "step": 6136
    },
    {
      "epoch": 2.375919473480449,
      "grad_norm": 16.05754280090332,
      "learning_rate": 8.471200585021724e-06,
      "loss": 1.4816,
      "step": 6137
    },
    {
      "epoch": 2.3763066202090593,
      "grad_norm": 20.451671600341797,
      "learning_rate": 8.470770421989934e-06,
      "loss": 1.4275,
      "step": 6138
    },
    {
      "epoch": 2.3766937669376693,
      "grad_norm": 12.860904693603516,
      "learning_rate": 8.470340258958147e-06,
      "loss": 0.8048,
      "step": 6139
    },
    {
      "epoch": 2.3770809136662794,
      "grad_norm": 28.373546600341797,
      "learning_rate": 8.469910095926357e-06,
      "loss": 1.7396,
      "step": 6140
    },
    {
      "epoch": 2.3774680603948894,
      "grad_norm": 21.92537498474121,
      "learning_rate": 8.469479932894568e-06,
      "loss": 1.0258,
      "step": 6141
    },
    {
      "epoch": 2.3778552071235,
      "grad_norm": 13.645694732666016,
      "learning_rate": 8.469049769862778e-06,
      "loss": 1.2459,
      "step": 6142
    },
    {
      "epoch": 2.37824235385211,
      "grad_norm": 14.863818168640137,
      "learning_rate": 8.468619606830991e-06,
      "loss": 1.1859,
      "step": 6143
    },
    {
      "epoch": 2.37862950058072,
      "grad_norm": 31.637245178222656,
      "learning_rate": 8.4681894437992e-06,
      "loss": 1.8177,
      "step": 6144
    },
    {
      "epoch": 2.37901664730933,
      "grad_norm": 10.758285522460938,
      "learning_rate": 8.467759280767412e-06,
      "loss": 1.2157,
      "step": 6145
    },
    {
      "epoch": 2.3794037940379402,
      "grad_norm": 21.893104553222656,
      "learning_rate": 8.467329117735622e-06,
      "loss": 1.7846,
      "step": 6146
    },
    {
      "epoch": 2.3797909407665507,
      "grad_norm": 20.615049362182617,
      "learning_rate": 8.466898954703833e-06,
      "loss": 1.2225,
      "step": 6147
    },
    {
      "epoch": 2.380178087495161,
      "grad_norm": 13.625354766845703,
      "learning_rate": 8.466468791672045e-06,
      "loss": 0.7828,
      "step": 6148
    },
    {
      "epoch": 2.380565234223771,
      "grad_norm": 30.742494583129883,
      "learning_rate": 8.466038628640256e-06,
      "loss": 1.4898,
      "step": 6149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 29.587425231933594,
      "learning_rate": 8.465608465608466e-06,
      "loss": 1.4183,
      "step": 6150
    },
    {
      "epoch": 2.381339527680991,
      "grad_norm": 23.32748794555664,
      "learning_rate": 8.465178302576677e-06,
      "loss": 0.5691,
      "step": 6151
    },
    {
      "epoch": 2.381726674409601,
      "grad_norm": 13.65196418762207,
      "learning_rate": 8.464748139544889e-06,
      "loss": 0.565,
      "step": 6152
    },
    {
      "epoch": 2.3821138211382116,
      "grad_norm": 23.106916427612305,
      "learning_rate": 8.464317976513098e-06,
      "loss": 1.8138,
      "step": 6153
    },
    {
      "epoch": 2.3825009678668216,
      "grad_norm": 18.947338104248047,
      "learning_rate": 8.46388781348131e-06,
      "loss": 1.7676,
      "step": 6154
    },
    {
      "epoch": 2.3828881145954317,
      "grad_norm": 14.468155860900879,
      "learning_rate": 8.463457650449521e-06,
      "loss": 0.9875,
      "step": 6155
    },
    {
      "epoch": 2.3832752613240418,
      "grad_norm": 15.500000953674316,
      "learning_rate": 8.463027487417732e-06,
      "loss": 1.1251,
      "step": 6156
    },
    {
      "epoch": 2.383662408052652,
      "grad_norm": 22.20153045654297,
      "learning_rate": 8.462597324385942e-06,
      "loss": 1.1533,
      "step": 6157
    },
    {
      "epoch": 2.384049554781262,
      "grad_norm": 27.002073287963867,
      "learning_rate": 8.462167161354154e-06,
      "loss": 1.2561,
      "step": 6158
    },
    {
      "epoch": 2.3844367015098724,
      "grad_norm": 27.043781280517578,
      "learning_rate": 8.461736998322365e-06,
      "loss": 1.6631,
      "step": 6159
    },
    {
      "epoch": 2.3848238482384825,
      "grad_norm": 6.751690864562988,
      "learning_rate": 8.461306835290576e-06,
      "loss": 0.3417,
      "step": 6160
    },
    {
      "epoch": 2.3852109949670925,
      "grad_norm": 25.416337966918945,
      "learning_rate": 8.460876672258786e-06,
      "loss": 1.8296,
      "step": 6161
    },
    {
      "epoch": 2.3855981416957026,
      "grad_norm": 26.449615478515625,
      "learning_rate": 8.460446509226998e-06,
      "loss": 1.8682,
      "step": 6162
    },
    {
      "epoch": 2.3859852884243127,
      "grad_norm": 12.947225570678711,
      "learning_rate": 8.460016346195209e-06,
      "loss": 1.3646,
      "step": 6163
    },
    {
      "epoch": 2.386372435152923,
      "grad_norm": 13.648802757263184,
      "learning_rate": 8.45958618316342e-06,
      "loss": 1.3365,
      "step": 6164
    },
    {
      "epoch": 2.3867595818815333,
      "grad_norm": 16.826839447021484,
      "learning_rate": 8.45915602013163e-06,
      "loss": 1.5698,
      "step": 6165
    },
    {
      "epoch": 2.3871467286101433,
      "grad_norm": 14.67233657836914,
      "learning_rate": 8.458725857099842e-06,
      "loss": 1.2084,
      "step": 6166
    },
    {
      "epoch": 2.3875338753387534,
      "grad_norm": 25.69815444946289,
      "learning_rate": 8.458295694068053e-06,
      "loss": 3.3997,
      "step": 6167
    },
    {
      "epoch": 2.3879210220673635,
      "grad_norm": 13.230786323547363,
      "learning_rate": 8.457865531036263e-06,
      "loss": 0.7642,
      "step": 6168
    },
    {
      "epoch": 2.3883081687959735,
      "grad_norm": 27.106529235839844,
      "learning_rate": 8.457435368004474e-06,
      "loss": 1.3783,
      "step": 6169
    },
    {
      "epoch": 2.3886953155245836,
      "grad_norm": 16.979764938354492,
      "learning_rate": 8.457005204972686e-06,
      "loss": 3.2798,
      "step": 6170
    },
    {
      "epoch": 2.389082462253194,
      "grad_norm": 13.429558753967285,
      "learning_rate": 8.456575041940897e-06,
      "loss": 0.8175,
      "step": 6171
    },
    {
      "epoch": 2.389469608981804,
      "grad_norm": 13.622315406799316,
      "learning_rate": 8.456144878909107e-06,
      "loss": 1.0925,
      "step": 6172
    },
    {
      "epoch": 2.3898567557104142,
      "grad_norm": 15.997929573059082,
      "learning_rate": 8.455714715877318e-06,
      "loss": 0.7987,
      "step": 6173
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 22.174062728881836,
      "learning_rate": 8.45528455284553e-06,
      "loss": 1.3563,
      "step": 6174
    },
    {
      "epoch": 2.3906310491676344,
      "grad_norm": 52.18630599975586,
      "learning_rate": 8.454854389813741e-06,
      "loss": 2.8837,
      "step": 6175
    },
    {
      "epoch": 2.391018195896245,
      "grad_norm": 17.37371063232422,
      "learning_rate": 8.45442422678195e-06,
      "loss": 2.2351,
      "step": 6176
    },
    {
      "epoch": 2.391405342624855,
      "grad_norm": 31.46874237060547,
      "learning_rate": 8.453994063750162e-06,
      "loss": 2.4034,
      "step": 6177
    },
    {
      "epoch": 2.391792489353465,
      "grad_norm": 10.050087928771973,
      "learning_rate": 8.453563900718373e-06,
      "loss": 1.2147,
      "step": 6178
    },
    {
      "epoch": 2.392179636082075,
      "grad_norm": 14.958104133605957,
      "learning_rate": 8.453133737686585e-06,
      "loss": 1.5153,
      "step": 6179
    },
    {
      "epoch": 2.392566782810685,
      "grad_norm": 33.69038391113281,
      "learning_rate": 8.452703574654795e-06,
      "loss": 1.937,
      "step": 6180
    },
    {
      "epoch": 2.392953929539295,
      "grad_norm": 28.364397048950195,
      "learning_rate": 8.452273411623006e-06,
      "loss": 1.3856,
      "step": 6181
    },
    {
      "epoch": 2.3933410762679057,
      "grad_norm": 17.69939422607422,
      "learning_rate": 8.451843248591217e-06,
      "loss": 1.6063,
      "step": 6182
    },
    {
      "epoch": 2.3937282229965158,
      "grad_norm": 47.944053649902344,
      "learning_rate": 8.451413085559427e-06,
      "loss": 2.0517,
      "step": 6183
    },
    {
      "epoch": 2.394115369725126,
      "grad_norm": 18.30397605895996,
      "learning_rate": 8.450982922527639e-06,
      "loss": 1.1341,
      "step": 6184
    },
    {
      "epoch": 2.394502516453736,
      "grad_norm": 22.43625831604004,
      "learning_rate": 8.45055275949585e-06,
      "loss": 2.9175,
      "step": 6185
    },
    {
      "epoch": 2.394889663182346,
      "grad_norm": 33.548057556152344,
      "learning_rate": 8.450122596464061e-06,
      "loss": 2.3909,
      "step": 6186
    },
    {
      "epoch": 2.395276809910956,
      "grad_norm": 24.49785804748535,
      "learning_rate": 8.449692433432271e-06,
      "loss": 1.2637,
      "step": 6187
    },
    {
      "epoch": 2.3956639566395665,
      "grad_norm": 25.441099166870117,
      "learning_rate": 8.449262270400483e-06,
      "loss": 1.5446,
      "step": 6188
    },
    {
      "epoch": 2.3960511033681766,
      "grad_norm": 20.550729751586914,
      "learning_rate": 8.448832107368692e-06,
      "loss": 1.9529,
      "step": 6189
    },
    {
      "epoch": 2.3964382500967867,
      "grad_norm": 16.824447631835938,
      "learning_rate": 8.448401944336905e-06,
      "loss": 0.6866,
      "step": 6190
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 36.36467742919922,
      "learning_rate": 8.447971781305115e-06,
      "loss": 1.0464,
      "step": 6191
    },
    {
      "epoch": 2.397212543554007,
      "grad_norm": 25.0352725982666,
      "learning_rate": 8.447541618273327e-06,
      "loss": 1.9778,
      "step": 6192
    },
    {
      "epoch": 2.3975996902826173,
      "grad_norm": 13.430075645446777,
      "learning_rate": 8.447111455241536e-06,
      "loss": 1.3274,
      "step": 6193
    },
    {
      "epoch": 2.3979868370112274,
      "grad_norm": 12.178522109985352,
      "learning_rate": 8.44668129220975e-06,
      "loss": 0.7229,
      "step": 6194
    },
    {
      "epoch": 2.3983739837398375,
      "grad_norm": 13.322046279907227,
      "learning_rate": 8.446251129177959e-06,
      "loss": 0.9781,
      "step": 6195
    },
    {
      "epoch": 2.3987611304684475,
      "grad_norm": 26.51669692993164,
      "learning_rate": 8.44582096614617e-06,
      "loss": 2.0042,
      "step": 6196
    },
    {
      "epoch": 2.3991482771970576,
      "grad_norm": 33.902828216552734,
      "learning_rate": 8.44539080311438e-06,
      "loss": 1.6249,
      "step": 6197
    },
    {
      "epoch": 2.3995354239256677,
      "grad_norm": 24.165435791015625,
      "learning_rate": 8.444960640082592e-06,
      "loss": 1.2928,
      "step": 6198
    },
    {
      "epoch": 2.399922570654278,
      "grad_norm": 20.970224380493164,
      "learning_rate": 8.444530477050803e-06,
      "loss": 1.0963,
      "step": 6199
    },
    {
      "epoch": 2.4003097173828882,
      "grad_norm": 50.71638107299805,
      "learning_rate": 8.444100314019014e-06,
      "loss": 1.3768,
      "step": 6200
    },
    {
      "epoch": 2.4006968641114983,
      "grad_norm": 30.728515625,
      "learning_rate": 8.443670150987224e-06,
      "loss": 1.731,
      "step": 6201
    },
    {
      "epoch": 2.4010840108401084,
      "grad_norm": 12.475939750671387,
      "learning_rate": 8.443239987955436e-06,
      "loss": 0.7981,
      "step": 6202
    },
    {
      "epoch": 2.4014711575687184,
      "grad_norm": 17.596189498901367,
      "learning_rate": 8.442809824923647e-06,
      "loss": 1.4477,
      "step": 6203
    },
    {
      "epoch": 2.4018583042973285,
      "grad_norm": 13.58961296081543,
      "learning_rate": 8.442379661891857e-06,
      "loss": 1.0604,
      "step": 6204
    },
    {
      "epoch": 2.402245451025939,
      "grad_norm": 49.33101272583008,
      "learning_rate": 8.44194949886007e-06,
      "loss": 2.6679,
      "step": 6205
    },
    {
      "epoch": 2.402632597754549,
      "grad_norm": 15.072225570678711,
      "learning_rate": 8.44151933582828e-06,
      "loss": 1.2259,
      "step": 6206
    },
    {
      "epoch": 2.403019744483159,
      "grad_norm": 25.48409652709961,
      "learning_rate": 8.441089172796491e-06,
      "loss": 1.4076,
      "step": 6207
    },
    {
      "epoch": 2.403406891211769,
      "grad_norm": 19.68397331237793,
      "learning_rate": 8.4406590097647e-06,
      "loss": 1.6226,
      "step": 6208
    },
    {
      "epoch": 2.4037940379403793,
      "grad_norm": 19.79789161682129,
      "learning_rate": 8.440228846732914e-06,
      "loss": 2.1312,
      "step": 6209
    },
    {
      "epoch": 2.40418118466899,
      "grad_norm": 19.362154006958008,
      "learning_rate": 8.439798683701124e-06,
      "loss": 1.5154,
      "step": 6210
    },
    {
      "epoch": 2.4045683313976,
      "grad_norm": 18.985248565673828,
      "learning_rate": 8.439368520669335e-06,
      "loss": 1.5978,
      "step": 6211
    },
    {
      "epoch": 2.40495547812621,
      "grad_norm": 29.675020217895508,
      "learning_rate": 8.438938357637545e-06,
      "loss": 1.0472,
      "step": 6212
    },
    {
      "epoch": 2.40534262485482,
      "grad_norm": 16.620840072631836,
      "learning_rate": 8.438508194605756e-06,
      "loss": 1.3791,
      "step": 6213
    },
    {
      "epoch": 2.40572977158343,
      "grad_norm": 14.677718162536621,
      "learning_rate": 8.438078031573968e-06,
      "loss": 1.1026,
      "step": 6214
    },
    {
      "epoch": 2.40611691831204,
      "grad_norm": 20.88092803955078,
      "learning_rate": 8.437647868542179e-06,
      "loss": 1.7607,
      "step": 6215
    },
    {
      "epoch": 2.40650406504065,
      "grad_norm": 25.328107833862305,
      "learning_rate": 8.437217705510389e-06,
      "loss": 1.5865,
      "step": 6216
    },
    {
      "epoch": 2.4068912117692607,
      "grad_norm": 23.251537322998047,
      "learning_rate": 8.4367875424786e-06,
      "loss": 1.6613,
      "step": 6217
    },
    {
      "epoch": 2.4072783584978708,
      "grad_norm": 23.738025665283203,
      "learning_rate": 8.436357379446811e-06,
      "loss": 1.549,
      "step": 6218
    },
    {
      "epoch": 2.407665505226481,
      "grad_norm": 13.182076454162598,
      "learning_rate": 8.435927216415021e-06,
      "loss": 1.3755,
      "step": 6219
    },
    {
      "epoch": 2.408052651955091,
      "grad_norm": 14.490857124328613,
      "learning_rate": 8.435497053383233e-06,
      "loss": 1.4084,
      "step": 6220
    },
    {
      "epoch": 2.408439798683701,
      "grad_norm": 17.54180145263672,
      "learning_rate": 8.435066890351444e-06,
      "loss": 1.447,
      "step": 6221
    },
    {
      "epoch": 2.4088269454123115,
      "grad_norm": 14.478226661682129,
      "learning_rate": 8.434636727319655e-06,
      "loss": 1.4666,
      "step": 6222
    },
    {
      "epoch": 2.4092140921409215,
      "grad_norm": 17.122493743896484,
      "learning_rate": 8.434206564287865e-06,
      "loss": 0.7495,
      "step": 6223
    },
    {
      "epoch": 2.4096012388695316,
      "grad_norm": 36.415401458740234,
      "learning_rate": 8.433776401256077e-06,
      "loss": 1.711,
      "step": 6224
    },
    {
      "epoch": 2.4099883855981417,
      "grad_norm": 21.006866455078125,
      "learning_rate": 8.433346238224288e-06,
      "loss": 1.5105,
      "step": 6225
    },
    {
      "epoch": 2.4103755323267517,
      "grad_norm": 20.45578956604004,
      "learning_rate": 8.4329160751925e-06,
      "loss": 1.7227,
      "step": 6226
    },
    {
      "epoch": 2.410762679055362,
      "grad_norm": 30.741228103637695,
      "learning_rate": 8.432485912160709e-06,
      "loss": 1.6261,
      "step": 6227
    },
    {
      "epoch": 2.4111498257839723,
      "grad_norm": 23.10300064086914,
      "learning_rate": 8.43205574912892e-06,
      "loss": 1.7521,
      "step": 6228
    },
    {
      "epoch": 2.4115369725125824,
      "grad_norm": 17.516172409057617,
      "learning_rate": 8.431625586097132e-06,
      "loss": 1.0332,
      "step": 6229
    },
    {
      "epoch": 2.4119241192411924,
      "grad_norm": 22.151060104370117,
      "learning_rate": 8.431195423065343e-06,
      "loss": 1.4941,
      "step": 6230
    },
    {
      "epoch": 2.4123112659698025,
      "grad_norm": 25.709291458129883,
      "learning_rate": 8.430765260033553e-06,
      "loss": 1.4034,
      "step": 6231
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 23.865503311157227,
      "learning_rate": 8.430335097001765e-06,
      "loss": 1.1569,
      "step": 6232
    },
    {
      "epoch": 2.4130855594270226,
      "grad_norm": 24.322237014770508,
      "learning_rate": 8.429904933969976e-06,
      "loss": 2.8859,
      "step": 6233
    },
    {
      "epoch": 2.413472706155633,
      "grad_norm": 23.0448055267334,
      "learning_rate": 8.429474770938186e-06,
      "loss": 2.715,
      "step": 6234
    },
    {
      "epoch": 2.413859852884243,
      "grad_norm": 40.101768493652344,
      "learning_rate": 8.429044607906397e-06,
      "loss": 2.287,
      "step": 6235
    },
    {
      "epoch": 2.4142469996128533,
      "grad_norm": 23.89995765686035,
      "learning_rate": 8.428614444874608e-06,
      "loss": 2.1142,
      "step": 6236
    },
    {
      "epoch": 2.4146341463414633,
      "grad_norm": 39.29611587524414,
      "learning_rate": 8.42818428184282e-06,
      "loss": 0.867,
      "step": 6237
    },
    {
      "epoch": 2.4150212930700734,
      "grad_norm": 13.253388404846191,
      "learning_rate": 8.42775411881103e-06,
      "loss": 1.3103,
      "step": 6238
    },
    {
      "epoch": 2.415408439798684,
      "grad_norm": 27.254573822021484,
      "learning_rate": 8.427323955779241e-06,
      "loss": 1.5247,
      "step": 6239
    },
    {
      "epoch": 2.415795586527294,
      "grad_norm": 42.580440521240234,
      "learning_rate": 8.42689379274745e-06,
      "loss": 1.9903,
      "step": 6240
    },
    {
      "epoch": 2.416182733255904,
      "grad_norm": 14.56130599975586,
      "learning_rate": 8.426463629715664e-06,
      "loss": 0.8502,
      "step": 6241
    },
    {
      "epoch": 2.416569879984514,
      "grad_norm": 21.609086990356445,
      "learning_rate": 8.426033466683874e-06,
      "loss": 0.6911,
      "step": 6242
    },
    {
      "epoch": 2.416957026713124,
      "grad_norm": 23.44732666015625,
      "learning_rate": 8.425603303652085e-06,
      "loss": 2.2463,
      "step": 6243
    },
    {
      "epoch": 2.4173441734417342,
      "grad_norm": 25.21940040588379,
      "learning_rate": 8.425173140620295e-06,
      "loss": 2.4894,
      "step": 6244
    },
    {
      "epoch": 2.4177313201703443,
      "grad_norm": 22.477615356445312,
      "learning_rate": 8.424742977588508e-06,
      "loss": 1.4275,
      "step": 6245
    },
    {
      "epoch": 2.418118466898955,
      "grad_norm": 17.075342178344727,
      "learning_rate": 8.424312814556718e-06,
      "loss": 1.3552,
      "step": 6246
    },
    {
      "epoch": 2.418505613627565,
      "grad_norm": 27.75156021118164,
      "learning_rate": 8.423882651524929e-06,
      "loss": 1.195,
      "step": 6247
    },
    {
      "epoch": 2.418892760356175,
      "grad_norm": 19.09430503845215,
      "learning_rate": 8.42345248849314e-06,
      "loss": 1.2296,
      "step": 6248
    },
    {
      "epoch": 2.419279907084785,
      "grad_norm": 26.630151748657227,
      "learning_rate": 8.42302232546135e-06,
      "loss": 2.324,
      "step": 6249
    },
    {
      "epoch": 2.419667053813395,
      "grad_norm": 16.66210174560547,
      "learning_rate": 8.422592162429562e-06,
      "loss": 1.2151,
      "step": 6250
    },
    {
      "epoch": 2.4200542005420056,
      "grad_norm": 17.32227325439453,
      "learning_rate": 8.422161999397773e-06,
      "loss": 1.7544,
      "step": 6251
    },
    {
      "epoch": 2.4204413472706157,
      "grad_norm": 27.18492317199707,
      "learning_rate": 8.421731836365984e-06,
      "loss": 1.6082,
      "step": 6252
    },
    {
      "epoch": 2.4208284939992257,
      "grad_norm": 14.895359992980957,
      "learning_rate": 8.421301673334194e-06,
      "loss": 1.4036,
      "step": 6253
    },
    {
      "epoch": 2.421215640727836,
      "grad_norm": 24.46790313720703,
      "learning_rate": 8.420871510302405e-06,
      "loss": 2.4383,
      "step": 6254
    },
    {
      "epoch": 2.421602787456446,
      "grad_norm": 14.973274230957031,
      "learning_rate": 8.420441347270615e-06,
      "loss": 0.842,
      "step": 6255
    },
    {
      "epoch": 2.4219899341850564,
      "grad_norm": 23.393075942993164,
      "learning_rate": 8.420011184238828e-06,
      "loss": 1.3373,
      "step": 6256
    },
    {
      "epoch": 2.4223770809136664,
      "grad_norm": 20.537246704101562,
      "learning_rate": 8.419581021207038e-06,
      "loss": 1.2956,
      "step": 6257
    },
    {
      "epoch": 2.4227642276422765,
      "grad_norm": 25.455684661865234,
      "learning_rate": 8.41915085817525e-06,
      "loss": 1.4185,
      "step": 6258
    },
    {
      "epoch": 2.4231513743708866,
      "grad_norm": 21.06663703918457,
      "learning_rate": 8.41872069514346e-06,
      "loss": 0.6718,
      "step": 6259
    },
    {
      "epoch": 2.4235385210994966,
      "grad_norm": 13.819876670837402,
      "learning_rate": 8.418290532111672e-06,
      "loss": 0.5479,
      "step": 6260
    },
    {
      "epoch": 2.4239256678281067,
      "grad_norm": 14.822278022766113,
      "learning_rate": 8.417860369079882e-06,
      "loss": 0.8755,
      "step": 6261
    },
    {
      "epoch": 2.4243128145567168,
      "grad_norm": 14.556396484375,
      "learning_rate": 8.417430206048093e-06,
      "loss": 0.9213,
      "step": 6262
    },
    {
      "epoch": 2.4246999612853273,
      "grad_norm": 14.175599098205566,
      "learning_rate": 8.417000043016303e-06,
      "loss": 1.0313,
      "step": 6263
    },
    {
      "epoch": 2.4250871080139373,
      "grad_norm": 37.236690521240234,
      "learning_rate": 8.416569879984515e-06,
      "loss": 0.95,
      "step": 6264
    },
    {
      "epoch": 2.4254742547425474,
      "grad_norm": 19.559814453125,
      "learning_rate": 8.416139716952726e-06,
      "loss": 1.2442,
      "step": 6265
    },
    {
      "epoch": 2.4258614014711575,
      "grad_norm": 16.57164192199707,
      "learning_rate": 8.415709553920937e-06,
      "loss": 0.6427,
      "step": 6266
    },
    {
      "epoch": 2.4262485481997675,
      "grad_norm": 22.21108055114746,
      "learning_rate": 8.415279390889147e-06,
      "loss": 1.3235,
      "step": 6267
    },
    {
      "epoch": 2.426635694928378,
      "grad_norm": 12.977739334106445,
      "learning_rate": 8.414849227857359e-06,
      "loss": 1.362,
      "step": 6268
    },
    {
      "epoch": 2.427022841656988,
      "grad_norm": 22.848722457885742,
      "learning_rate": 8.41441906482557e-06,
      "loss": 0.9623,
      "step": 6269
    },
    {
      "epoch": 2.427409988385598,
      "grad_norm": 12.501619338989258,
      "learning_rate": 8.41398890179378e-06,
      "loss": 0.5279,
      "step": 6270
    },
    {
      "epoch": 2.4277971351142082,
      "grad_norm": 14.756318092346191,
      "learning_rate": 8.413558738761991e-06,
      "loss": 0.7671,
      "step": 6271
    },
    {
      "epoch": 2.4281842818428183,
      "grad_norm": 16.94761085510254,
      "learning_rate": 8.413128575730203e-06,
      "loss": 1.2813,
      "step": 6272
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 23.33285140991211,
      "learning_rate": 8.412698412698414e-06,
      "loss": 1.7662,
      "step": 6273
    },
    {
      "epoch": 2.428958575300039,
      "grad_norm": 68.5396499633789,
      "learning_rate": 8.412268249666624e-06,
      "loss": 1.4679,
      "step": 6274
    },
    {
      "epoch": 2.429345722028649,
      "grad_norm": 21.875844955444336,
      "learning_rate": 8.411838086634835e-06,
      "loss": 2.8793,
      "step": 6275
    },
    {
      "epoch": 2.429732868757259,
      "grad_norm": 39.89899826049805,
      "learning_rate": 8.411407923603046e-06,
      "loss": 1.6539,
      "step": 6276
    },
    {
      "epoch": 2.430120015485869,
      "grad_norm": 22.30874252319336,
      "learning_rate": 8.410977760571258e-06,
      "loss": 1.6633,
      "step": 6277
    },
    {
      "epoch": 2.430507162214479,
      "grad_norm": 19.621410369873047,
      "learning_rate": 8.410547597539468e-06,
      "loss": 1.7478,
      "step": 6278
    },
    {
      "epoch": 2.430894308943089,
      "grad_norm": 49.17440414428711,
      "learning_rate": 8.410117434507679e-06,
      "loss": 3.0444,
      "step": 6279
    },
    {
      "epoch": 2.4312814556716997,
      "grad_norm": 26.821142196655273,
      "learning_rate": 8.40968727147589e-06,
      "loss": 1.108,
      "step": 6280
    },
    {
      "epoch": 2.43166860240031,
      "grad_norm": 18.438276290893555,
      "learning_rate": 8.409257108444102e-06,
      "loss": 0.9674,
      "step": 6281
    },
    {
      "epoch": 2.43205574912892,
      "grad_norm": 25.706966400146484,
      "learning_rate": 8.408826945412312e-06,
      "loss": 1.5115,
      "step": 6282
    },
    {
      "epoch": 2.43244289585753,
      "grad_norm": 12.55277156829834,
      "learning_rate": 8.408396782380523e-06,
      "loss": 0.858,
      "step": 6283
    },
    {
      "epoch": 2.43283004258614,
      "grad_norm": 16.736812591552734,
      "learning_rate": 8.407966619348734e-06,
      "loss": 0.9655,
      "step": 6284
    },
    {
      "epoch": 2.4332171893147505,
      "grad_norm": 22.058584213256836,
      "learning_rate": 8.407536456316944e-06,
      "loss": 1.9966,
      "step": 6285
    },
    {
      "epoch": 2.4336043360433606,
      "grad_norm": 25.74349021911621,
      "learning_rate": 8.407106293285156e-06,
      "loss": 0.9378,
      "step": 6286
    },
    {
      "epoch": 2.4339914827719706,
      "grad_norm": 18.05889892578125,
      "learning_rate": 8.406676130253367e-06,
      "loss": 1.485,
      "step": 6287
    },
    {
      "epoch": 2.4343786295005807,
      "grad_norm": 25.272245407104492,
      "learning_rate": 8.406245967221578e-06,
      "loss": 1.3058,
      "step": 6288
    },
    {
      "epoch": 2.4347657762291908,
      "grad_norm": 10.123165130615234,
      "learning_rate": 8.405815804189788e-06,
      "loss": 1.2517,
      "step": 6289
    },
    {
      "epoch": 2.435152922957801,
      "grad_norm": 20.4592227935791,
      "learning_rate": 8.405385641158e-06,
      "loss": 1.2465,
      "step": 6290
    },
    {
      "epoch": 2.435540069686411,
      "grad_norm": 23.55632209777832,
      "learning_rate": 8.404955478126211e-06,
      "loss": 1.3937,
      "step": 6291
    },
    {
      "epoch": 2.4359272164150214,
      "grad_norm": 53.29998779296875,
      "learning_rate": 8.404525315094422e-06,
      "loss": 1.0956,
      "step": 6292
    },
    {
      "epoch": 2.4363143631436315,
      "grad_norm": 33.13041305541992,
      "learning_rate": 8.404095152062632e-06,
      "loss": 1.3955,
      "step": 6293
    },
    {
      "epoch": 2.4367015098722415,
      "grad_norm": 39.90620040893555,
      "learning_rate": 8.403664989030843e-06,
      "loss": 2.2177,
      "step": 6294
    },
    {
      "epoch": 2.4370886566008516,
      "grad_norm": 41.448570251464844,
      "learning_rate": 8.403234825999055e-06,
      "loss": 1.4731,
      "step": 6295
    },
    {
      "epoch": 2.4374758033294617,
      "grad_norm": 15.625779151916504,
      "learning_rate": 8.402804662967266e-06,
      "loss": 0.9031,
      "step": 6296
    },
    {
      "epoch": 2.437862950058072,
      "grad_norm": 24.292388916015625,
      "learning_rate": 8.402374499935476e-06,
      "loss": 1.4137,
      "step": 6297
    },
    {
      "epoch": 2.4382500967866823,
      "grad_norm": 29.515207290649414,
      "learning_rate": 8.401944336903687e-06,
      "loss": 1.831,
      "step": 6298
    },
    {
      "epoch": 2.4386372435152923,
      "grad_norm": 26.404340744018555,
      "learning_rate": 8.401514173871899e-06,
      "loss": 2.5469,
      "step": 6299
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 26.713592529296875,
      "learning_rate": 8.401084010840109e-06,
      "loss": 1.7978,
      "step": 6300
    },
    {
      "epoch": 2.4394115369725125,
      "grad_norm": 33.061912536621094,
      "learning_rate": 8.40065384780832e-06,
      "loss": 2.9946,
      "step": 6301
    },
    {
      "epoch": 2.439798683701123,
      "grad_norm": 18.038585662841797,
      "learning_rate": 8.400223684776531e-06,
      "loss": 1.3512,
      "step": 6302
    },
    {
      "epoch": 2.440185830429733,
      "grad_norm": 16.372522354125977,
      "learning_rate": 8.399793521744743e-06,
      "loss": 1.4022,
      "step": 6303
    },
    {
      "epoch": 2.440572977158343,
      "grad_norm": 24.492897033691406,
      "learning_rate": 8.399363358712953e-06,
      "loss": 1.3218,
      "step": 6304
    },
    {
      "epoch": 2.440960123886953,
      "grad_norm": 24.998523712158203,
      "learning_rate": 8.398933195681164e-06,
      "loss": 2.0003,
      "step": 6305
    },
    {
      "epoch": 2.4413472706155632,
      "grad_norm": 19.79254722595215,
      "learning_rate": 8.398503032649374e-06,
      "loss": 1.7504,
      "step": 6306
    },
    {
      "epoch": 2.4417344173441733,
      "grad_norm": 18.14030647277832,
      "learning_rate": 8.398072869617587e-06,
      "loss": 1.0493,
      "step": 6307
    },
    {
      "epoch": 2.4421215640727834,
      "grad_norm": 16.4326114654541,
      "learning_rate": 8.397642706585797e-06,
      "loss": 1.4219,
      "step": 6308
    },
    {
      "epoch": 2.442508710801394,
      "grad_norm": 42.04150390625,
      "learning_rate": 8.397212543554008e-06,
      "loss": 1.2428,
      "step": 6309
    },
    {
      "epoch": 2.442895857530004,
      "grad_norm": 16.29375648498535,
      "learning_rate": 8.396782380522218e-06,
      "loss": 1.4741,
      "step": 6310
    },
    {
      "epoch": 2.443283004258614,
      "grad_norm": 23.86681365966797,
      "learning_rate": 8.39635221749043e-06,
      "loss": 1.7387,
      "step": 6311
    },
    {
      "epoch": 2.443670150987224,
      "grad_norm": 29.657541275024414,
      "learning_rate": 8.39592205445864e-06,
      "loss": 1.5742,
      "step": 6312
    },
    {
      "epoch": 2.444057297715834,
      "grad_norm": 23.127756118774414,
      "learning_rate": 8.395491891426852e-06,
      "loss": 1.8045,
      "step": 6313
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 18.95789337158203,
      "learning_rate": 8.395061728395062e-06,
      "loss": 1.8676,
      "step": 6314
    },
    {
      "epoch": 2.4448315911730547,
      "grad_norm": 18.45952796936035,
      "learning_rate": 8.394631565363273e-06,
      "loss": 1.0997,
      "step": 6315
    },
    {
      "epoch": 2.4452187379016648,
      "grad_norm": 27.50907325744629,
      "learning_rate": 8.394201402331484e-06,
      "loss": 1.28,
      "step": 6316
    },
    {
      "epoch": 2.445605884630275,
      "grad_norm": 45.35536193847656,
      "learning_rate": 8.393771239299696e-06,
      "loss": 1.3175,
      "step": 6317
    },
    {
      "epoch": 2.445993031358885,
      "grad_norm": 19.08108139038086,
      "learning_rate": 8.393341076267906e-06,
      "loss": 1.4162,
      "step": 6318
    },
    {
      "epoch": 2.446380178087495,
      "grad_norm": 18.915834426879883,
      "learning_rate": 8.392910913236117e-06,
      "loss": 1.3588,
      "step": 6319
    },
    {
      "epoch": 2.4467673248161055,
      "grad_norm": 19.92026710510254,
      "learning_rate": 8.392480750204328e-06,
      "loss": 1.4655,
      "step": 6320
    },
    {
      "epoch": 2.4471544715447155,
      "grad_norm": 16.403217315673828,
      "learning_rate": 8.392050587172538e-06,
      "loss": 1.5695,
      "step": 6321
    },
    {
      "epoch": 2.4475416182733256,
      "grad_norm": 15.074677467346191,
      "learning_rate": 8.39162042414075e-06,
      "loss": 1.5251,
      "step": 6322
    },
    {
      "epoch": 2.4479287650019357,
      "grad_norm": 19.825637817382812,
      "learning_rate": 8.391190261108961e-06,
      "loss": 1.1483,
      "step": 6323
    },
    {
      "epoch": 2.4483159117305457,
      "grad_norm": 31.022438049316406,
      "learning_rate": 8.390760098077172e-06,
      "loss": 1.2674,
      "step": 6324
    },
    {
      "epoch": 2.448703058459156,
      "grad_norm": 20.800016403198242,
      "learning_rate": 8.390329935045382e-06,
      "loss": 1.9972,
      "step": 6325
    },
    {
      "epoch": 2.4490902051877663,
      "grad_norm": 12.31654167175293,
      "learning_rate": 8.389899772013594e-06,
      "loss": 0.7053,
      "step": 6326
    },
    {
      "epoch": 2.4494773519163764,
      "grad_norm": 12.888931274414062,
      "learning_rate": 8.389469608981805e-06,
      "loss": 0.4339,
      "step": 6327
    },
    {
      "epoch": 2.4498644986449865,
      "grad_norm": 15.141844749450684,
      "learning_rate": 8.389039445950016e-06,
      "loss": 1.4021,
      "step": 6328
    },
    {
      "epoch": 2.4502516453735965,
      "grad_norm": 14.302535057067871,
      "learning_rate": 8.388609282918226e-06,
      "loss": 1.3901,
      "step": 6329
    },
    {
      "epoch": 2.4506387921022066,
      "grad_norm": 11.26046085357666,
      "learning_rate": 8.388179119886438e-06,
      "loss": 0.4115,
      "step": 6330
    },
    {
      "epoch": 2.451025938830817,
      "grad_norm": 94.68094635009766,
      "learning_rate": 8.387748956854649e-06,
      "loss": 0.5937,
      "step": 6331
    },
    {
      "epoch": 2.451413085559427,
      "grad_norm": 25.672924041748047,
      "learning_rate": 8.38731879382286e-06,
      "loss": 1.1068,
      "step": 6332
    },
    {
      "epoch": 2.4518002322880372,
      "grad_norm": 21.65131187438965,
      "learning_rate": 8.38688863079107e-06,
      "loss": 1.1915,
      "step": 6333
    },
    {
      "epoch": 2.4521873790166473,
      "grad_norm": 16.419160842895508,
      "learning_rate": 8.386458467759281e-06,
      "loss": 1.2489,
      "step": 6334
    },
    {
      "epoch": 2.4525745257452574,
      "grad_norm": 7.331625461578369,
      "learning_rate": 8.386028304727493e-06,
      "loss": 0.2425,
      "step": 6335
    },
    {
      "epoch": 2.4529616724738674,
      "grad_norm": 22.443077087402344,
      "learning_rate": 8.385598141695703e-06,
      "loss": 2.0199,
      "step": 6336
    },
    {
      "epoch": 2.4533488192024775,
      "grad_norm": 15.088494300842285,
      "learning_rate": 8.385167978663914e-06,
      "loss": 0.9519,
      "step": 6337
    },
    {
      "epoch": 2.453735965931088,
      "grad_norm": 19.03240203857422,
      "learning_rate": 8.384737815632125e-06,
      "loss": 1.7798,
      "step": 6338
    },
    {
      "epoch": 2.454123112659698,
      "grad_norm": 51.68601989746094,
      "learning_rate": 8.384307652600337e-06,
      "loss": 2.0561,
      "step": 6339
    },
    {
      "epoch": 2.454510259388308,
      "grad_norm": 22.642478942871094,
      "learning_rate": 8.383877489568547e-06,
      "loss": 1.4016,
      "step": 6340
    },
    {
      "epoch": 2.454897406116918,
      "grad_norm": 21.68618392944336,
      "learning_rate": 8.383447326536758e-06,
      "loss": 1.4674,
      "step": 6341
    },
    {
      "epoch": 2.4552845528455283,
      "grad_norm": 13.92357349395752,
      "learning_rate": 8.38301716350497e-06,
      "loss": 1.4743,
      "step": 6342
    },
    {
      "epoch": 2.4556716995741388,
      "grad_norm": 15.177433967590332,
      "learning_rate": 8.38258700047318e-06,
      "loss": 0.9476,
      "step": 6343
    },
    {
      "epoch": 2.456058846302749,
      "grad_norm": 13.003645896911621,
      "learning_rate": 8.38215683744139e-06,
      "loss": 0.4819,
      "step": 6344
    },
    {
      "epoch": 2.456445993031359,
      "grad_norm": 38.822750091552734,
      "learning_rate": 8.381726674409602e-06,
      "loss": 1.6674,
      "step": 6345
    },
    {
      "epoch": 2.456833139759969,
      "grad_norm": 30.124799728393555,
      "learning_rate": 8.381296511377813e-06,
      "loss": 1.5944,
      "step": 6346
    },
    {
      "epoch": 2.457220286488579,
      "grad_norm": 39.977962493896484,
      "learning_rate": 8.380866348346025e-06,
      "loss": 1.8369,
      "step": 6347
    },
    {
      "epoch": 2.4576074332171896,
      "grad_norm": 28.07257843017578,
      "learning_rate": 8.380436185314235e-06,
      "loss": 2.0751,
      "step": 6348
    },
    {
      "epoch": 2.4579945799457996,
      "grad_norm": 50.843929290771484,
      "learning_rate": 8.380006022282446e-06,
      "loss": 1.5064,
      "step": 6349
    },
    {
      "epoch": 2.4583817266744097,
      "grad_norm": 15.909407615661621,
      "learning_rate": 8.379575859250657e-06,
      "loss": 0.7494,
      "step": 6350
    },
    {
      "epoch": 2.4587688734030198,
      "grad_norm": 16.481687545776367,
      "learning_rate": 8.379145696218867e-06,
      "loss": 0.9217,
      "step": 6351
    },
    {
      "epoch": 2.45915602013163,
      "grad_norm": 27.587350845336914,
      "learning_rate": 8.378715533187078e-06,
      "loss": 1.9889,
      "step": 6352
    },
    {
      "epoch": 2.45954316686024,
      "grad_norm": 15.80490493774414,
      "learning_rate": 8.37828537015529e-06,
      "loss": 1.0739,
      "step": 6353
    },
    {
      "epoch": 2.45993031358885,
      "grad_norm": 20.261703491210938,
      "learning_rate": 8.377855207123501e-06,
      "loss": 1.1433,
      "step": 6354
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 17.32928466796875,
      "learning_rate": 8.377425044091711e-06,
      "loss": 1.5428,
      "step": 6355
    },
    {
      "epoch": 2.4607046070460705,
      "grad_norm": 42.903724670410156,
      "learning_rate": 8.376994881059922e-06,
      "loss": 2.3693,
      "step": 6356
    },
    {
      "epoch": 2.4610917537746806,
      "grad_norm": 41.31612014770508,
      "learning_rate": 8.376564718028132e-06,
      "loss": 2.6168,
      "step": 6357
    },
    {
      "epoch": 2.4614789005032907,
      "grad_norm": 38.12701416015625,
      "learning_rate": 8.376134554996345e-06,
      "loss": 2.636,
      "step": 6358
    },
    {
      "epoch": 2.4618660472319007,
      "grad_norm": 23.95599365234375,
      "learning_rate": 8.375704391964555e-06,
      "loss": 1.508,
      "step": 6359
    },
    {
      "epoch": 2.4622531939605112,
      "grad_norm": 36.11648941040039,
      "learning_rate": 8.375274228932766e-06,
      "loss": 1.8334,
      "step": 6360
    },
    {
      "epoch": 2.4626403406891213,
      "grad_norm": 18.93311309814453,
      "learning_rate": 8.374844065900976e-06,
      "loss": 0.592,
      "step": 6361
    },
    {
      "epoch": 2.4630274874177314,
      "grad_norm": 13.422120094299316,
      "learning_rate": 8.37441390286919e-06,
      "loss": 0.4775,
      "step": 6362
    },
    {
      "epoch": 2.4634146341463414,
      "grad_norm": 39.44029235839844,
      "learning_rate": 8.373983739837399e-06,
      "loss": 2.1909,
      "step": 6363
    },
    {
      "epoch": 2.4638017808749515,
      "grad_norm": 22.295650482177734,
      "learning_rate": 8.37355357680561e-06,
      "loss": 1.1986,
      "step": 6364
    },
    {
      "epoch": 2.4641889276035616,
      "grad_norm": 20.437030792236328,
      "learning_rate": 8.37312341377382e-06,
      "loss": 1.7676,
      "step": 6365
    },
    {
      "epoch": 2.464576074332172,
      "grad_norm": 31.021055221557617,
      "learning_rate": 8.372693250742032e-06,
      "loss": 1.6402,
      "step": 6366
    },
    {
      "epoch": 2.464963221060782,
      "grad_norm": 34.13609313964844,
      "learning_rate": 8.372263087710243e-06,
      "loss": 1.9091,
      "step": 6367
    },
    {
      "epoch": 2.465350367789392,
      "grad_norm": 10.763948440551758,
      "learning_rate": 8.371832924678454e-06,
      "loss": 0.5666,
      "step": 6368
    },
    {
      "epoch": 2.4657375145180023,
      "grad_norm": 19.3569278717041,
      "learning_rate": 8.371402761646666e-06,
      "loss": 0.9033,
      "step": 6369
    },
    {
      "epoch": 2.4661246612466123,
      "grad_norm": 19.78533363342285,
      "learning_rate": 8.370972598614876e-06,
      "loss": 1.5448,
      "step": 6370
    },
    {
      "epoch": 2.4665118079752224,
      "grad_norm": 30.69906234741211,
      "learning_rate": 8.370542435583087e-06,
      "loss": 1.6682,
      "step": 6371
    },
    {
      "epoch": 2.466898954703833,
      "grad_norm": 33.40437316894531,
      "learning_rate": 8.370112272551297e-06,
      "loss": 1.4918,
      "step": 6372
    },
    {
      "epoch": 2.467286101432443,
      "grad_norm": 27.83169174194336,
      "learning_rate": 8.36968210951951e-06,
      "loss": 2.2167,
      "step": 6373
    },
    {
      "epoch": 2.467673248161053,
      "grad_norm": 24.697975158691406,
      "learning_rate": 8.36925194648772e-06,
      "loss": 1.2551,
      "step": 6374
    },
    {
      "epoch": 2.468060394889663,
      "grad_norm": 28.488380432128906,
      "learning_rate": 8.368821783455931e-06,
      "loss": 1.7834,
      "step": 6375
    },
    {
      "epoch": 2.468447541618273,
      "grad_norm": 13.82490348815918,
      "learning_rate": 8.36839162042414e-06,
      "loss": 1.0499,
      "step": 6376
    },
    {
      "epoch": 2.4688346883468837,
      "grad_norm": 22.374711990356445,
      "learning_rate": 8.367961457392354e-06,
      "loss": 2.0092,
      "step": 6377
    },
    {
      "epoch": 2.4692218350754938,
      "grad_norm": 20.638517379760742,
      "learning_rate": 8.367531294360563e-06,
      "loss": 0.3966,
      "step": 6378
    },
    {
      "epoch": 2.469608981804104,
      "grad_norm": 26.712934494018555,
      "learning_rate": 8.367101131328775e-06,
      "loss": 1.1923,
      "step": 6379
    },
    {
      "epoch": 2.469996128532714,
      "grad_norm": 23.937971115112305,
      "learning_rate": 8.366670968296985e-06,
      "loss": 3.1783,
      "step": 6380
    },
    {
      "epoch": 2.470383275261324,
      "grad_norm": 10.513495445251465,
      "learning_rate": 8.366240805265196e-06,
      "loss": 0.5665,
      "step": 6381
    },
    {
      "epoch": 2.470770421989934,
      "grad_norm": 22.92256736755371,
      "learning_rate": 8.365810642233407e-06,
      "loss": 1.5818,
      "step": 6382
    },
    {
      "epoch": 2.471157568718544,
      "grad_norm": 18.993579864501953,
      "learning_rate": 8.365380479201619e-06,
      "loss": 0.6446,
      "step": 6383
    },
    {
      "epoch": 2.4715447154471546,
      "grad_norm": 22.955923080444336,
      "learning_rate": 8.364950316169829e-06,
      "loss": 2.1175,
      "step": 6384
    },
    {
      "epoch": 2.4719318621757647,
      "grad_norm": 31.878978729248047,
      "learning_rate": 8.36452015313804e-06,
      "loss": 1.6742,
      "step": 6385
    },
    {
      "epoch": 2.4723190089043747,
      "grad_norm": 26.647581100463867,
      "learning_rate": 8.364089990106251e-06,
      "loss": 1.7651,
      "step": 6386
    },
    {
      "epoch": 2.472706155632985,
      "grad_norm": 13.991878509521484,
      "learning_rate": 8.363659827074461e-06,
      "loss": 1.1961,
      "step": 6387
    },
    {
      "epoch": 2.473093302361595,
      "grad_norm": 28.434051513671875,
      "learning_rate": 8.363229664042673e-06,
      "loss": 2.0341,
      "step": 6388
    },
    {
      "epoch": 2.4734804490902054,
      "grad_norm": 21.190807342529297,
      "learning_rate": 8.362799501010884e-06,
      "loss": 1.8229,
      "step": 6389
    },
    {
      "epoch": 2.4738675958188154,
      "grad_norm": 14.663166046142578,
      "learning_rate": 8.362369337979095e-06,
      "loss": 0.9699,
      "step": 6390
    },
    {
      "epoch": 2.4742547425474255,
      "grad_norm": 16.903968811035156,
      "learning_rate": 8.361939174947305e-06,
      "loss": 1.2111,
      "step": 6391
    },
    {
      "epoch": 2.4746418892760356,
      "grad_norm": 12.333232879638672,
      "learning_rate": 8.361509011915516e-06,
      "loss": 0.7889,
      "step": 6392
    },
    {
      "epoch": 2.4750290360046456,
      "grad_norm": 32.46806335449219,
      "learning_rate": 8.361078848883728e-06,
      "loss": 1.1843,
      "step": 6393
    },
    {
      "epoch": 2.475416182733256,
      "grad_norm": 12.413578987121582,
      "learning_rate": 8.36064868585194e-06,
      "loss": 0.596,
      "step": 6394
    },
    {
      "epoch": 2.475803329461866,
      "grad_norm": 34.290435791015625,
      "learning_rate": 8.360218522820149e-06,
      "loss": 1.8103,
      "step": 6395
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 33.80609893798828,
      "learning_rate": 8.35978835978836e-06,
      "loss": 1.8133,
      "step": 6396
    },
    {
      "epoch": 2.4765776229190863,
      "grad_norm": 20.611927032470703,
      "learning_rate": 8.359358196756572e-06,
      "loss": 1.4829,
      "step": 6397
    },
    {
      "epoch": 2.4769647696476964,
      "grad_norm": 20.88433265686035,
      "learning_rate": 8.358928033724783e-06,
      "loss": 1.4924,
      "step": 6398
    },
    {
      "epoch": 2.4773519163763065,
      "grad_norm": 29.755470275878906,
      "learning_rate": 8.358497870692993e-06,
      "loss": 2.4644,
      "step": 6399
    },
    {
      "epoch": 2.4777390631049165,
      "grad_norm": 20.209327697753906,
      "learning_rate": 8.358067707661204e-06,
      "loss": 1.0658,
      "step": 6400
    },
    {
      "epoch": 2.478126209833527,
      "grad_norm": 17.276151657104492,
      "learning_rate": 8.357637544629416e-06,
      "loss": 1.0288,
      "step": 6401
    },
    {
      "epoch": 2.478513356562137,
      "grad_norm": 21.16608238220215,
      "learning_rate": 8.357207381597626e-06,
      "loss": 1.3382,
      "step": 6402
    },
    {
      "epoch": 2.478900503290747,
      "grad_norm": 25.167957305908203,
      "learning_rate": 8.356777218565837e-06,
      "loss": 1.6119,
      "step": 6403
    },
    {
      "epoch": 2.4792876500193572,
      "grad_norm": 11.307354927062988,
      "learning_rate": 8.356347055534048e-06,
      "loss": 0.8336,
      "step": 6404
    },
    {
      "epoch": 2.4796747967479673,
      "grad_norm": 29.29197883605957,
      "learning_rate": 8.35591689250226e-06,
      "loss": 1.3927,
      "step": 6405
    },
    {
      "epoch": 2.480061943476578,
      "grad_norm": 27.577444076538086,
      "learning_rate": 8.35548672947047e-06,
      "loss": 1.7345,
      "step": 6406
    },
    {
      "epoch": 2.480449090205188,
      "grad_norm": 20.10213279724121,
      "learning_rate": 8.355056566438681e-06,
      "loss": 1.3878,
      "step": 6407
    },
    {
      "epoch": 2.480836236933798,
      "grad_norm": 23.97292709350586,
      "learning_rate": 8.35462640340689e-06,
      "loss": 0.9595,
      "step": 6408
    },
    {
      "epoch": 2.481223383662408,
      "grad_norm": 10.995614051818848,
      "learning_rate": 8.354196240375104e-06,
      "loss": 0.6218,
      "step": 6409
    },
    {
      "epoch": 2.481610530391018,
      "grad_norm": 23.653371810913086,
      "learning_rate": 8.353766077343314e-06,
      "loss": 0.9506,
      "step": 6410
    },
    {
      "epoch": 2.481997677119628,
      "grad_norm": 46.29795455932617,
      "learning_rate": 8.353335914311525e-06,
      "loss": 2.3458,
      "step": 6411
    },
    {
      "epoch": 2.4823848238482387,
      "grad_norm": 27.498046875,
      "learning_rate": 8.352905751279736e-06,
      "loss": 1.7224,
      "step": 6412
    },
    {
      "epoch": 2.4827719705768487,
      "grad_norm": 41.8160514831543,
      "learning_rate": 8.352475588247948e-06,
      "loss": 1.5843,
      "step": 6413
    },
    {
      "epoch": 2.483159117305459,
      "grad_norm": 9.707711219787598,
      "learning_rate": 8.352045425216157e-06,
      "loss": 0.5373,
      "step": 6414
    },
    {
      "epoch": 2.483546264034069,
      "grad_norm": 18.43319320678711,
      "learning_rate": 8.351615262184369e-06,
      "loss": 1.3349,
      "step": 6415
    },
    {
      "epoch": 2.483933410762679,
      "grad_norm": 29.621658325195312,
      "learning_rate": 8.35118509915258e-06,
      "loss": 0.6948,
      "step": 6416
    },
    {
      "epoch": 2.484320557491289,
      "grad_norm": 20.605670928955078,
      "learning_rate": 8.35075493612079e-06,
      "loss": 3.5896,
      "step": 6417
    },
    {
      "epoch": 2.4847077042198995,
      "grad_norm": 10.676511764526367,
      "learning_rate": 8.350324773089001e-06,
      "loss": 0.6222,
      "step": 6418
    },
    {
      "epoch": 2.4850948509485096,
      "grad_norm": 18.480836868286133,
      "learning_rate": 8.349894610057213e-06,
      "loss": 1.0457,
      "step": 6419
    },
    {
      "epoch": 2.4854819976771196,
      "grad_norm": 14.305941581726074,
      "learning_rate": 8.349464447025424e-06,
      "loss": 1.4758,
      "step": 6420
    },
    {
      "epoch": 2.4858691444057297,
      "grad_norm": 41.0087776184082,
      "learning_rate": 8.349034283993634e-06,
      "loss": 2.4319,
      "step": 6421
    },
    {
      "epoch": 2.4862562911343398,
      "grad_norm": 23.945878982543945,
      "learning_rate": 8.348604120961845e-06,
      "loss": 1.865,
      "step": 6422
    },
    {
      "epoch": 2.4866434378629503,
      "grad_norm": 14.438483238220215,
      "learning_rate": 8.348173957930055e-06,
      "loss": 3.0088,
      "step": 6423
    },
    {
      "epoch": 2.4870305845915603,
      "grad_norm": 23.855649948120117,
      "learning_rate": 8.347743794898268e-06,
      "loss": 1.6305,
      "step": 6424
    },
    {
      "epoch": 2.4874177313201704,
      "grad_norm": 33.56563186645508,
      "learning_rate": 8.347313631866478e-06,
      "loss": 1.9426,
      "step": 6425
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 16.804811477661133,
      "learning_rate": 8.34688346883469e-06,
      "loss": 1.6176,
      "step": 6426
    },
    {
      "epoch": 2.4881920247773905,
      "grad_norm": 9.574609756469727,
      "learning_rate": 8.346453305802899e-06,
      "loss": 1.1984,
      "step": 6427
    },
    {
      "epoch": 2.4885791715060006,
      "grad_norm": 20.52532958984375,
      "learning_rate": 8.34602314277111e-06,
      "loss": 1.2537,
      "step": 6428
    },
    {
      "epoch": 2.4889663182346107,
      "grad_norm": 11.642061233520508,
      "learning_rate": 8.345592979739322e-06,
      "loss": 0.7643,
      "step": 6429
    },
    {
      "epoch": 2.489353464963221,
      "grad_norm": 13.016176223754883,
      "learning_rate": 8.345162816707533e-06,
      "loss": 1.3334,
      "step": 6430
    },
    {
      "epoch": 2.4897406116918313,
      "grad_norm": 20.34708023071289,
      "learning_rate": 8.344732653675743e-06,
      "loss": 1.488,
      "step": 6431
    },
    {
      "epoch": 2.4901277584204413,
      "grad_norm": 27.672040939331055,
      "learning_rate": 8.344302490643954e-06,
      "loss": 1.4394,
      "step": 6432
    },
    {
      "epoch": 2.4905149051490514,
      "grad_norm": 10.745226860046387,
      "learning_rate": 8.343872327612166e-06,
      "loss": 0.5169,
      "step": 6433
    },
    {
      "epoch": 2.4909020518776614,
      "grad_norm": 24.006359100341797,
      "learning_rate": 8.343442164580377e-06,
      "loss": 1.6964,
      "step": 6434
    },
    {
      "epoch": 2.491289198606272,
      "grad_norm": 23.519914627075195,
      "learning_rate": 8.343012001548587e-06,
      "loss": 0.9926,
      "step": 6435
    },
    {
      "epoch": 2.491676345334882,
      "grad_norm": 20.41346549987793,
      "learning_rate": 8.342581838516798e-06,
      "loss": 0.6993,
      "step": 6436
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 28.504972457885742,
      "learning_rate": 8.34215167548501e-06,
      "loss": 1.4923,
      "step": 6437
    },
    {
      "epoch": 2.492450638792102,
      "grad_norm": 27.80693244934082,
      "learning_rate": 8.34172151245322e-06,
      "loss": 1.6295,
      "step": 6438
    },
    {
      "epoch": 2.4928377855207122,
      "grad_norm": 19.527021408081055,
      "learning_rate": 8.341291349421431e-06,
      "loss": 2.0295,
      "step": 6439
    },
    {
      "epoch": 2.4932249322493227,
      "grad_norm": 14.342367172241211,
      "learning_rate": 8.340861186389642e-06,
      "loss": 1.1332,
      "step": 6440
    },
    {
      "epoch": 2.493612078977933,
      "grad_norm": 13.387900352478027,
      "learning_rate": 8.340431023357854e-06,
      "loss": 0.966,
      "step": 6441
    },
    {
      "epoch": 2.493999225706543,
      "grad_norm": 21.203462600708008,
      "learning_rate": 8.340000860326064e-06,
      "loss": 1.6638,
      "step": 6442
    },
    {
      "epoch": 2.494386372435153,
      "grad_norm": 23.74569320678711,
      "learning_rate": 8.339570697294275e-06,
      "loss": 1.4196,
      "step": 6443
    },
    {
      "epoch": 2.494773519163763,
      "grad_norm": 14.787854194641113,
      "learning_rate": 8.339140534262486e-06,
      "loss": 1.5132,
      "step": 6444
    },
    {
      "epoch": 2.495160665892373,
      "grad_norm": 22.600061416625977,
      "learning_rate": 8.338710371230698e-06,
      "loss": 1.3245,
      "step": 6445
    },
    {
      "epoch": 2.495547812620983,
      "grad_norm": 15.041440963745117,
      "learning_rate": 8.338280208198908e-06,
      "loss": 1.0762,
      "step": 6446
    },
    {
      "epoch": 2.4959349593495936,
      "grad_norm": 14.043095588684082,
      "learning_rate": 8.337850045167119e-06,
      "loss": 1.2987,
      "step": 6447
    },
    {
      "epoch": 2.4963221060782037,
      "grad_norm": 19.8508358001709,
      "learning_rate": 8.33741988213533e-06,
      "loss": 1.8151,
      "step": 6448
    },
    {
      "epoch": 2.4967092528068138,
      "grad_norm": 15.666501998901367,
      "learning_rate": 8.336989719103542e-06,
      "loss": 1.53,
      "step": 6449
    },
    {
      "epoch": 2.497096399535424,
      "grad_norm": 25.57322883605957,
      "learning_rate": 8.336559556071752e-06,
      "loss": 0.8348,
      "step": 6450
    },
    {
      "epoch": 2.497483546264034,
      "grad_norm": 39.89375686645508,
      "learning_rate": 8.336129393039963e-06,
      "loss": 1.6078,
      "step": 6451
    },
    {
      "epoch": 2.4978706929926444,
      "grad_norm": 12.980964660644531,
      "learning_rate": 8.335699230008174e-06,
      "loss": 0.811,
      "step": 6452
    },
    {
      "epoch": 2.4982578397212545,
      "grad_norm": 12.289501190185547,
      "learning_rate": 8.335269066976384e-06,
      "loss": 1.1072,
      "step": 6453
    },
    {
      "epoch": 2.4986449864498645,
      "grad_norm": 14.83763313293457,
      "learning_rate": 8.334838903944595e-06,
      "loss": 1.353,
      "step": 6454
    },
    {
      "epoch": 2.4990321331784746,
      "grad_norm": 19.241870880126953,
      "learning_rate": 8.334408740912807e-06,
      "loss": 1.1406,
      "step": 6455
    },
    {
      "epoch": 2.4994192799070847,
      "grad_norm": 20.09542465209961,
      "learning_rate": 8.333978577881018e-06,
      "loss": 1.4104,
      "step": 6456
    },
    {
      "epoch": 2.4998064266356947,
      "grad_norm": 15.59621524810791,
      "learning_rate": 8.333548414849228e-06,
      "loss": 1.47,
      "step": 6457
    },
    {
      "epoch": 2.500193573364305,
      "grad_norm": 13.046630859375,
      "learning_rate": 8.33311825181744e-06,
      "loss": 1.3439,
      "step": 6458
    },
    {
      "epoch": 2.5005807200929153,
      "grad_norm": 21.735898971557617,
      "learning_rate": 8.332688088785651e-06,
      "loss": 1.7002,
      "step": 6459
    },
    {
      "epoch": 2.5009678668215254,
      "grad_norm": 7.594080448150635,
      "learning_rate": 8.332257925753862e-06,
      "loss": 0.1666,
      "step": 6460
    },
    {
      "epoch": 2.5013550135501355,
      "grad_norm": 18.987966537475586,
      "learning_rate": 8.331827762722072e-06,
      "loss": 0.6736,
      "step": 6461
    },
    {
      "epoch": 2.5017421602787455,
      "grad_norm": 21.67728042602539,
      "learning_rate": 8.331397599690283e-06,
      "loss": 1.735,
      "step": 6462
    },
    {
      "epoch": 2.5021293070073556,
      "grad_norm": 13.738360404968262,
      "learning_rate": 8.330967436658495e-06,
      "loss": 1.24,
      "step": 6463
    },
    {
      "epoch": 2.502516453735966,
      "grad_norm": 36.09787368774414,
      "learning_rate": 8.330537273626705e-06,
      "loss": 1.9508,
      "step": 6464
    },
    {
      "epoch": 2.502903600464576,
      "grad_norm": 19.52826690673828,
      "learning_rate": 8.330107110594916e-06,
      "loss": 1.8614,
      "step": 6465
    },
    {
      "epoch": 2.5032907471931862,
      "grad_norm": 19.25617218017578,
      "learning_rate": 8.329676947563127e-06,
      "loss": 1.6853,
      "step": 6466
    },
    {
      "epoch": 2.5036778939217963,
      "grad_norm": 28.533321380615234,
      "learning_rate": 8.329246784531339e-06,
      "loss": 1.7192,
      "step": 6467
    },
    {
      "epoch": 2.5040650406504064,
      "grad_norm": 17.58362579345703,
      "learning_rate": 8.328816621499549e-06,
      "loss": 0.9344,
      "step": 6468
    },
    {
      "epoch": 2.504452187379017,
      "grad_norm": 33.76087188720703,
      "learning_rate": 8.32838645846776e-06,
      "loss": 1.9695,
      "step": 6469
    },
    {
      "epoch": 2.504839334107627,
      "grad_norm": 28.106014251708984,
      "learning_rate": 8.327956295435971e-06,
      "loss": 1.1405,
      "step": 6470
    },
    {
      "epoch": 2.505226480836237,
      "grad_norm": 15.277581214904785,
      "learning_rate": 8.327526132404183e-06,
      "loss": 1.0601,
      "step": 6471
    },
    {
      "epoch": 2.505613627564847,
      "grad_norm": 12.834311485290527,
      "learning_rate": 8.327095969372392e-06,
      "loss": 1.3614,
      "step": 6472
    },
    {
      "epoch": 2.506000774293457,
      "grad_norm": 18.883420944213867,
      "learning_rate": 8.326665806340604e-06,
      "loss": 1.5467,
      "step": 6473
    },
    {
      "epoch": 2.506387921022067,
      "grad_norm": 63.41770553588867,
      "learning_rate": 8.326235643308814e-06,
      "loss": 1.7111,
      "step": 6474
    },
    {
      "epoch": 2.5067750677506773,
      "grad_norm": 23.538307189941406,
      "learning_rate": 8.325805480277027e-06,
      "loss": 1.5271,
      "step": 6475
    },
    {
      "epoch": 2.5071622144792878,
      "grad_norm": 22.240676879882812,
      "learning_rate": 8.325375317245236e-06,
      "loss": 1.4742,
      "step": 6476
    },
    {
      "epoch": 2.507549361207898,
      "grad_norm": 28.552532196044922,
      "learning_rate": 8.324945154213448e-06,
      "loss": 2.0131,
      "step": 6477
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 21.698610305786133,
      "learning_rate": 8.324514991181658e-06,
      "loss": 1.474,
      "step": 6478
    },
    {
      "epoch": 2.508323654665118,
      "grad_norm": 28.08293342590332,
      "learning_rate": 8.324084828149869e-06,
      "loss": 1.5724,
      "step": 6479
    },
    {
      "epoch": 2.508710801393728,
      "grad_norm": 23.935626983642578,
      "learning_rate": 8.32365466511808e-06,
      "loss": 1.5438,
      "step": 6480
    },
    {
      "epoch": 2.5090979481223386,
      "grad_norm": 25.333839416503906,
      "learning_rate": 8.323224502086292e-06,
      "loss": 1.3869,
      "step": 6481
    },
    {
      "epoch": 2.5094850948509486,
      "grad_norm": 25.06500244140625,
      "learning_rate": 8.322794339054502e-06,
      "loss": 1.5841,
      "step": 6482
    },
    {
      "epoch": 2.5098722415795587,
      "grad_norm": 23.306232452392578,
      "learning_rate": 8.322364176022713e-06,
      "loss": 1.8951,
      "step": 6483
    },
    {
      "epoch": 2.5102593883081687,
      "grad_norm": 18.03692054748535,
      "learning_rate": 8.321934012990924e-06,
      "loss": 1.6451,
      "step": 6484
    },
    {
      "epoch": 2.510646535036779,
      "grad_norm": 14.486477851867676,
      "learning_rate": 8.321503849959136e-06,
      "loss": 1.1125,
      "step": 6485
    },
    {
      "epoch": 2.5110336817653893,
      "grad_norm": 16.916582107543945,
      "learning_rate": 8.321073686927346e-06,
      "loss": 1.3136,
      "step": 6486
    },
    {
      "epoch": 2.5114208284939994,
      "grad_norm": 20.608919143676758,
      "learning_rate": 8.320643523895557e-06,
      "loss": 1.2984,
      "step": 6487
    },
    {
      "epoch": 2.5118079752226095,
      "grad_norm": 20.64354705810547,
      "learning_rate": 8.320213360863768e-06,
      "loss": 1.0927,
      "step": 6488
    },
    {
      "epoch": 2.5121951219512195,
      "grad_norm": 15.281045913696289,
      "learning_rate": 8.319783197831978e-06,
      "loss": 1.678,
      "step": 6489
    },
    {
      "epoch": 2.5125822686798296,
      "grad_norm": 19.368629455566406,
      "learning_rate": 8.31935303480019e-06,
      "loss": 1.0193,
      "step": 6490
    },
    {
      "epoch": 2.5129694154084397,
      "grad_norm": 14.889111518859863,
      "learning_rate": 8.318922871768401e-06,
      "loss": 1.0936,
      "step": 6491
    },
    {
      "epoch": 2.5133565621370497,
      "grad_norm": 14.584795951843262,
      "learning_rate": 8.318492708736612e-06,
      "loss": 1.1287,
      "step": 6492
    },
    {
      "epoch": 2.5137437088656602,
      "grad_norm": 14.170954704284668,
      "learning_rate": 8.318062545704822e-06,
      "loss": 1.3151,
      "step": 6493
    },
    {
      "epoch": 2.5141308555942703,
      "grad_norm": 19.815357208251953,
      "learning_rate": 8.317632382673033e-06,
      "loss": 1.735,
      "step": 6494
    },
    {
      "epoch": 2.5145180023228804,
      "grad_norm": 28.827762603759766,
      "learning_rate": 8.317202219641245e-06,
      "loss": 1.9013,
      "step": 6495
    },
    {
      "epoch": 2.5149051490514904,
      "grad_norm": 20.33135414123535,
      "learning_rate": 8.316772056609456e-06,
      "loss": 1.3839,
      "step": 6496
    },
    {
      "epoch": 2.5152922957801005,
      "grad_norm": 31.223426818847656,
      "learning_rate": 8.316341893577666e-06,
      "loss": 2.6853,
      "step": 6497
    },
    {
      "epoch": 2.515679442508711,
      "grad_norm": 14.141624450683594,
      "learning_rate": 8.315911730545877e-06,
      "loss": 1.1311,
      "step": 6498
    },
    {
      "epoch": 2.516066589237321,
      "grad_norm": 17.355968475341797,
      "learning_rate": 8.315481567514089e-06,
      "loss": 1.199,
      "step": 6499
    },
    {
      "epoch": 2.516453735965931,
      "grad_norm": 29.37929916381836,
      "learning_rate": 8.315051404482299e-06,
      "loss": 1.2436,
      "step": 6500
    },
    {
      "epoch": 2.516840882694541,
      "grad_norm": 8.809768676757812,
      "learning_rate": 8.31462124145051e-06,
      "loss": 0.3423,
      "step": 6501
    },
    {
      "epoch": 2.5172280294231513,
      "grad_norm": 19.977331161499023,
      "learning_rate": 8.314191078418721e-06,
      "loss": 1.2763,
      "step": 6502
    },
    {
      "epoch": 2.517615176151762,
      "grad_norm": 25.794357299804688,
      "learning_rate": 8.313760915386933e-06,
      "loss": 1.3346,
      "step": 6503
    },
    {
      "epoch": 2.5180023228803714,
      "grad_norm": 35.61018753051758,
      "learning_rate": 8.313330752355143e-06,
      "loss": 2.4292,
      "step": 6504
    },
    {
      "epoch": 2.518389469608982,
      "grad_norm": 15.333882331848145,
      "learning_rate": 8.312900589323354e-06,
      "loss": 0.7514,
      "step": 6505
    },
    {
      "epoch": 2.518776616337592,
      "grad_norm": 36.842864990234375,
      "learning_rate": 8.312470426291565e-06,
      "loss": 1.3513,
      "step": 6506
    },
    {
      "epoch": 2.519163763066202,
      "grad_norm": 15.649866104125977,
      "learning_rate": 8.312040263259777e-06,
      "loss": 0.6189,
      "step": 6507
    },
    {
      "epoch": 2.519550909794812,
      "grad_norm": 17.140108108520508,
      "learning_rate": 8.311610100227987e-06,
      "loss": 0.5212,
      "step": 6508
    },
    {
      "epoch": 2.519938056523422,
      "grad_norm": 50.83640670776367,
      "learning_rate": 8.311179937196198e-06,
      "loss": 1.9118,
      "step": 6509
    },
    {
      "epoch": 2.5203252032520327,
      "grad_norm": 16.325197219848633,
      "learning_rate": 8.31074977416441e-06,
      "loss": 1.4408,
      "step": 6510
    },
    {
      "epoch": 2.5207123499806428,
      "grad_norm": 16.226789474487305,
      "learning_rate": 8.31031961113262e-06,
      "loss": 1.715,
      "step": 6511
    },
    {
      "epoch": 2.521099496709253,
      "grad_norm": 33.354801177978516,
      "learning_rate": 8.30988944810083e-06,
      "loss": 1.2734,
      "step": 6512
    },
    {
      "epoch": 2.521486643437863,
      "grad_norm": 13.489748001098633,
      "learning_rate": 8.309459285069042e-06,
      "loss": 0.8103,
      "step": 6513
    },
    {
      "epoch": 2.521873790166473,
      "grad_norm": 39.03090286254883,
      "learning_rate": 8.309029122037253e-06,
      "loss": 0.8007,
      "step": 6514
    },
    {
      "epoch": 2.5222609368950835,
      "grad_norm": 15.683109283447266,
      "learning_rate": 8.308598959005463e-06,
      "loss": 0.9459,
      "step": 6515
    },
    {
      "epoch": 2.5226480836236935,
      "grad_norm": 26.64459800720215,
      "learning_rate": 8.308168795973674e-06,
      "loss": 1.5149,
      "step": 6516
    },
    {
      "epoch": 2.5230352303523036,
      "grad_norm": 34.58442306518555,
      "learning_rate": 8.307738632941886e-06,
      "loss": 2.2941,
      "step": 6517
    },
    {
      "epoch": 2.5234223770809137,
      "grad_norm": 25.853322982788086,
      "learning_rate": 8.307308469910097e-06,
      "loss": 1.331,
      "step": 6518
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 24.89281463623047,
      "learning_rate": 8.306878306878307e-06,
      "loss": 1.6568,
      "step": 6519
    },
    {
      "epoch": 2.524196670538134,
      "grad_norm": 12.714157104492188,
      "learning_rate": 8.306448143846518e-06,
      "loss": 1.5689,
      "step": 6520
    },
    {
      "epoch": 2.524583817266744,
      "grad_norm": 14.161920547485352,
      "learning_rate": 8.30601798081473e-06,
      "loss": 1.2942,
      "step": 6521
    },
    {
      "epoch": 2.5249709639953544,
      "grad_norm": 24.90749740600586,
      "learning_rate": 8.305587817782941e-06,
      "loss": 1.3608,
      "step": 6522
    },
    {
      "epoch": 2.5253581107239644,
      "grad_norm": 14.179140090942383,
      "learning_rate": 8.305157654751151e-06,
      "loss": 1.3587,
      "step": 6523
    },
    {
      "epoch": 2.5257452574525745,
      "grad_norm": 29.312105178833008,
      "learning_rate": 8.304727491719362e-06,
      "loss": 1.0185,
      "step": 6524
    },
    {
      "epoch": 2.5261324041811846,
      "grad_norm": 22.672313690185547,
      "learning_rate": 8.304297328687572e-06,
      "loss": 1.7353,
      "step": 6525
    },
    {
      "epoch": 2.5265195509097946,
      "grad_norm": 26.254566192626953,
      "learning_rate": 8.303867165655785e-06,
      "loss": 1.6653,
      "step": 6526
    },
    {
      "epoch": 2.526906697638405,
      "grad_norm": 23.89554214477539,
      "learning_rate": 8.303437002623995e-06,
      "loss": 1.2728,
      "step": 6527
    },
    {
      "epoch": 2.527293844367015,
      "grad_norm": 27.262222290039062,
      "learning_rate": 8.303006839592206e-06,
      "loss": 1.7846,
      "step": 6528
    },
    {
      "epoch": 2.5276809910956253,
      "grad_norm": 21.05702781677246,
      "learning_rate": 8.302576676560416e-06,
      "loss": 0.9575,
      "step": 6529
    },
    {
      "epoch": 2.5280681378242353,
      "grad_norm": 15.961860656738281,
      "learning_rate": 8.302146513528627e-06,
      "loss": 1.0459,
      "step": 6530
    },
    {
      "epoch": 2.5284552845528454,
      "grad_norm": 17.563133239746094,
      "learning_rate": 8.301716350496839e-06,
      "loss": 0.5317,
      "step": 6531
    },
    {
      "epoch": 2.528842431281456,
      "grad_norm": 13.472872734069824,
      "learning_rate": 8.30128618746505e-06,
      "loss": 0.8168,
      "step": 6532
    },
    {
      "epoch": 2.5292295780100655,
      "grad_norm": 17.71633529663086,
      "learning_rate": 8.300856024433262e-06,
      "loss": 0.9624,
      "step": 6533
    },
    {
      "epoch": 2.529616724738676,
      "grad_norm": 13.85167407989502,
      "learning_rate": 8.300425861401471e-06,
      "loss": 1.3042,
      "step": 6534
    },
    {
      "epoch": 2.530003871467286,
      "grad_norm": 14.888191223144531,
      "learning_rate": 8.299995698369683e-06,
      "loss": 1.0568,
      "step": 6535
    },
    {
      "epoch": 2.530391018195896,
      "grad_norm": 23.399309158325195,
      "learning_rate": 8.299565535337893e-06,
      "loss": 1.5571,
      "step": 6536
    },
    {
      "epoch": 2.5307781649245062,
      "grad_norm": 22.988798141479492,
      "learning_rate": 8.299135372306106e-06,
      "loss": 1.8694,
      "step": 6537
    },
    {
      "epoch": 2.5311653116531163,
      "grad_norm": 33.81867980957031,
      "learning_rate": 8.298705209274315e-06,
      "loss": 1.3096,
      "step": 6538
    },
    {
      "epoch": 2.531552458381727,
      "grad_norm": 37.10335159301758,
      "learning_rate": 8.298275046242527e-06,
      "loss": 0.6355,
      "step": 6539
    },
    {
      "epoch": 2.531939605110337,
      "grad_norm": 21.470989227294922,
      "learning_rate": 8.297844883210737e-06,
      "loss": 2.1576,
      "step": 6540
    },
    {
      "epoch": 2.532326751838947,
      "grad_norm": 16.406248092651367,
      "learning_rate": 8.29741472017895e-06,
      "loss": 1.0712,
      "step": 6541
    },
    {
      "epoch": 2.532713898567557,
      "grad_norm": 17.541515350341797,
      "learning_rate": 8.29698455714716e-06,
      "loss": 1.7659,
      "step": 6542
    },
    {
      "epoch": 2.533101045296167,
      "grad_norm": 15.30136489868164,
      "learning_rate": 8.29655439411537e-06,
      "loss": 0.8662,
      "step": 6543
    },
    {
      "epoch": 2.5334881920247776,
      "grad_norm": 24.657596588134766,
      "learning_rate": 8.29612423108358e-06,
      "loss": 2.1528,
      "step": 6544
    },
    {
      "epoch": 2.5338753387533877,
      "grad_norm": 12.448860168457031,
      "learning_rate": 8.295694068051792e-06,
      "loss": 0.4971,
      "step": 6545
    },
    {
      "epoch": 2.5342624854819977,
      "grad_norm": 26.62841033935547,
      "learning_rate": 8.295263905020003e-06,
      "loss": 1.9401,
      "step": 6546
    },
    {
      "epoch": 2.534649632210608,
      "grad_norm": 37.81526565551758,
      "learning_rate": 8.294833741988215e-06,
      "loss": 0.9558,
      "step": 6547
    },
    {
      "epoch": 2.535036778939218,
      "grad_norm": 22.582250595092773,
      "learning_rate": 8.294403578956425e-06,
      "loss": 1.9868,
      "step": 6548
    },
    {
      "epoch": 2.5354239256678284,
      "grad_norm": 37.88092041015625,
      "learning_rate": 8.293973415924636e-06,
      "loss": 1.3662,
      "step": 6549
    },
    {
      "epoch": 2.535811072396438,
      "grad_norm": 20.81193733215332,
      "learning_rate": 8.293543252892847e-06,
      "loss": 1.2515,
      "step": 6550
    },
    {
      "epoch": 2.5361982191250485,
      "grad_norm": 22.651832580566406,
      "learning_rate": 8.293113089861057e-06,
      "loss": 2.7326,
      "step": 6551
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 21.105417251586914,
      "learning_rate": 8.292682926829268e-06,
      "loss": 2.2014,
      "step": 6552
    },
    {
      "epoch": 2.5369725125822686,
      "grad_norm": 20.03730583190918,
      "learning_rate": 8.29225276379748e-06,
      "loss": 2.3818,
      "step": 6553
    },
    {
      "epoch": 2.5373596593108787,
      "grad_norm": 15.309534072875977,
      "learning_rate": 8.291822600765691e-06,
      "loss": 1.0403,
      "step": 6554
    },
    {
      "epoch": 2.5377468060394888,
      "grad_norm": 20.777042388916016,
      "learning_rate": 8.291392437733901e-06,
      "loss": 1.1601,
      "step": 6555
    },
    {
      "epoch": 2.5381339527680993,
      "grad_norm": 13.041324615478516,
      "learning_rate": 8.290962274702112e-06,
      "loss": 0.8506,
      "step": 6556
    },
    {
      "epoch": 2.5385210994967093,
      "grad_norm": 13.339285850524902,
      "learning_rate": 8.290532111670324e-06,
      "loss": 0.9665,
      "step": 6557
    },
    {
      "epoch": 2.5389082462253194,
      "grad_norm": 20.779035568237305,
      "learning_rate": 8.290101948638535e-06,
      "loss": 1.1393,
      "step": 6558
    },
    {
      "epoch": 2.5392953929539295,
      "grad_norm": 16.650367736816406,
      "learning_rate": 8.289671785606745e-06,
      "loss": 1.509,
      "step": 6559
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 18.446025848388672,
      "learning_rate": 8.289241622574956e-06,
      "loss": 1.0955,
      "step": 6560
    },
    {
      "epoch": 2.54006968641115,
      "grad_norm": 18.988876342773438,
      "learning_rate": 8.288811459543168e-06,
      "loss": 1.6318,
      "step": 6561
    },
    {
      "epoch": 2.54045683313976,
      "grad_norm": 27.805871963500977,
      "learning_rate": 8.28838129651138e-06,
      "loss": 1.4704,
      "step": 6562
    },
    {
      "epoch": 2.54084397986837,
      "grad_norm": 17.83941650390625,
      "learning_rate": 8.287951133479589e-06,
      "loss": 1.0169,
      "step": 6563
    },
    {
      "epoch": 2.5412311265969802,
      "grad_norm": 18.225038528442383,
      "learning_rate": 8.2875209704478e-06,
      "loss": 1.1306,
      "step": 6564
    },
    {
      "epoch": 2.5416182733255903,
      "grad_norm": 31.15797233581543,
      "learning_rate": 8.287090807416012e-06,
      "loss": 1.0881,
      "step": 6565
    },
    {
      "epoch": 2.5420054200542004,
      "grad_norm": 13.38355541229248,
      "learning_rate": 8.286660644384222e-06,
      "loss": 1.1921,
      "step": 6566
    },
    {
      "epoch": 2.5423925667828104,
      "grad_norm": 12.401418685913086,
      "learning_rate": 8.286230481352433e-06,
      "loss": 0.8079,
      "step": 6567
    },
    {
      "epoch": 2.542779713511421,
      "grad_norm": 20.618940353393555,
      "learning_rate": 8.285800318320644e-06,
      "loss": 1.4352,
      "step": 6568
    },
    {
      "epoch": 2.543166860240031,
      "grad_norm": 19.85950469970703,
      "learning_rate": 8.285370155288856e-06,
      "loss": 0.8567,
      "step": 6569
    },
    {
      "epoch": 2.543554006968641,
      "grad_norm": 13.236684799194336,
      "learning_rate": 8.284939992257065e-06,
      "loss": 1.3265,
      "step": 6570
    },
    {
      "epoch": 2.543941153697251,
      "grad_norm": 14.778871536254883,
      "learning_rate": 8.284509829225277e-06,
      "loss": 0.9277,
      "step": 6571
    },
    {
      "epoch": 2.5443283004258612,
      "grad_norm": 21.356082916259766,
      "learning_rate": 8.284079666193487e-06,
      "loss": 0.8995,
      "step": 6572
    },
    {
      "epoch": 2.5447154471544717,
      "grad_norm": 14.519862174987793,
      "learning_rate": 8.2836495031617e-06,
      "loss": 1.131,
      "step": 6573
    },
    {
      "epoch": 2.545102593883082,
      "grad_norm": 56.01575469970703,
      "learning_rate": 8.28321934012991e-06,
      "loss": 1.1907,
      "step": 6574
    },
    {
      "epoch": 2.545489740611692,
      "grad_norm": 25.45133399963379,
      "learning_rate": 8.282789177098121e-06,
      "loss": 1.6357,
      "step": 6575
    },
    {
      "epoch": 2.545876887340302,
      "grad_norm": 11.217961311340332,
      "learning_rate": 8.282359014066332e-06,
      "loss": 1.5074,
      "step": 6576
    },
    {
      "epoch": 2.546264034068912,
      "grad_norm": 15.927596092224121,
      "learning_rate": 8.281928851034544e-06,
      "loss": 0.9475,
      "step": 6577
    },
    {
      "epoch": 2.5466511807975225,
      "grad_norm": 20.719797134399414,
      "learning_rate": 8.281498688002753e-06,
      "loss": 1.0782,
      "step": 6578
    },
    {
      "epoch": 2.547038327526132,
      "grad_norm": 27.61850929260254,
      "learning_rate": 8.281068524970965e-06,
      "loss": 1.7027,
      "step": 6579
    },
    {
      "epoch": 2.5474254742547426,
      "grad_norm": 24.408353805541992,
      "learning_rate": 8.280638361939176e-06,
      "loss": 1.1343,
      "step": 6580
    },
    {
      "epoch": 2.5478126209833527,
      "grad_norm": 25.307329177856445,
      "learning_rate": 8.280208198907386e-06,
      "loss": 1.3239,
      "step": 6581
    },
    {
      "epoch": 2.5481997677119628,
      "grad_norm": 17.948484420776367,
      "learning_rate": 8.279778035875597e-06,
      "loss": 1.555,
      "step": 6582
    },
    {
      "epoch": 2.548586914440573,
      "grad_norm": 27.30409812927246,
      "learning_rate": 8.279347872843809e-06,
      "loss": 2.5787,
      "step": 6583
    },
    {
      "epoch": 2.548974061169183,
      "grad_norm": 25.592233657836914,
      "learning_rate": 8.27891770981202e-06,
      "loss": 2.9019,
      "step": 6584
    },
    {
      "epoch": 2.5493612078977934,
      "grad_norm": 14.838194847106934,
      "learning_rate": 8.27848754678023e-06,
      "loss": 0.7765,
      "step": 6585
    },
    {
      "epoch": 2.5497483546264035,
      "grad_norm": 22.911224365234375,
      "learning_rate": 8.278057383748441e-06,
      "loss": 2.0426,
      "step": 6586
    },
    {
      "epoch": 2.5501355013550135,
      "grad_norm": 16.12553596496582,
      "learning_rate": 8.277627220716651e-06,
      "loss": 1.3695,
      "step": 6587
    },
    {
      "epoch": 2.5505226480836236,
      "grad_norm": 33.09804153442383,
      "learning_rate": 8.277197057684864e-06,
      "loss": 2.5141,
      "step": 6588
    },
    {
      "epoch": 2.5509097948122337,
      "grad_norm": 28.810632705688477,
      "learning_rate": 8.276766894653074e-06,
      "loss": 2.9433,
      "step": 6589
    },
    {
      "epoch": 2.551296941540844,
      "grad_norm": 28.94549560546875,
      "learning_rate": 8.276336731621285e-06,
      "loss": 1.9487,
      "step": 6590
    },
    {
      "epoch": 2.5516840882694543,
      "grad_norm": 22.084768295288086,
      "learning_rate": 8.275906568589495e-06,
      "loss": 0.9135,
      "step": 6591
    },
    {
      "epoch": 2.5520712349980643,
      "grad_norm": 18.49317169189453,
      "learning_rate": 8.275476405557708e-06,
      "loss": 1.2264,
      "step": 6592
    },
    {
      "epoch": 2.5524583817266744,
      "grad_norm": 24.061599731445312,
      "learning_rate": 8.275046242525918e-06,
      "loss": 2.2454,
      "step": 6593
    },
    {
      "epoch": 2.5528455284552845,
      "grad_norm": 15.64351749420166,
      "learning_rate": 8.27461607949413e-06,
      "loss": 0.9802,
      "step": 6594
    },
    {
      "epoch": 2.553232675183895,
      "grad_norm": 14.469648361206055,
      "learning_rate": 8.274185916462339e-06,
      "loss": 0.6363,
      "step": 6595
    },
    {
      "epoch": 2.5536198219125046,
      "grad_norm": 12.576984405517578,
      "learning_rate": 8.27375575343055e-06,
      "loss": 0.586,
      "step": 6596
    },
    {
      "epoch": 2.554006968641115,
      "grad_norm": 21.39670753479004,
      "learning_rate": 8.273325590398762e-06,
      "loss": 1.1181,
      "step": 6597
    },
    {
      "epoch": 2.554394115369725,
      "grad_norm": 11.058210372924805,
      "learning_rate": 8.272895427366973e-06,
      "loss": 1.1898,
      "step": 6598
    },
    {
      "epoch": 2.5547812620983352,
      "grad_norm": 29.94928741455078,
      "learning_rate": 8.272465264335183e-06,
      "loss": 1.5197,
      "step": 6599
    },
    {
      "epoch": 2.5551684088269453,
      "grad_norm": 14.808874130249023,
      "learning_rate": 8.272035101303394e-06,
      "loss": 1.3728,
      "step": 6600
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 30.00786018371582,
      "learning_rate": 8.271604938271606e-06,
      "loss": 1.7327,
      "step": 6601
    },
    {
      "epoch": 2.555942702284166,
      "grad_norm": 14.83565902709961,
      "learning_rate": 8.271174775239816e-06,
      "loss": 1.1027,
      "step": 6602
    },
    {
      "epoch": 2.556329849012776,
      "grad_norm": 17.9450626373291,
      "learning_rate": 8.270744612208027e-06,
      "loss": 0.8226,
      "step": 6603
    },
    {
      "epoch": 2.556716995741386,
      "grad_norm": 28.154172897338867,
      "learning_rate": 8.270314449176238e-06,
      "loss": 1.5411,
      "step": 6604
    },
    {
      "epoch": 2.557104142469996,
      "grad_norm": 24.93691062927246,
      "learning_rate": 8.26988428614445e-06,
      "loss": 1.2118,
      "step": 6605
    },
    {
      "epoch": 2.557491289198606,
      "grad_norm": 24.76024627685547,
      "learning_rate": 8.26945412311266e-06,
      "loss": 1.4407,
      "step": 6606
    },
    {
      "epoch": 2.5578784359272166,
      "grad_norm": 15.929967880249023,
      "learning_rate": 8.269023960080871e-06,
      "loss": 1.3612,
      "step": 6607
    },
    {
      "epoch": 2.5582655826558267,
      "grad_norm": 39.07257080078125,
      "learning_rate": 8.268593797049082e-06,
      "loss": 1.7237,
      "step": 6608
    },
    {
      "epoch": 2.5586527293844368,
      "grad_norm": 21.56123924255371,
      "learning_rate": 8.268163634017294e-06,
      "loss": 2.1688,
      "step": 6609
    },
    {
      "epoch": 2.559039876113047,
      "grad_norm": 40.65340805053711,
      "learning_rate": 8.267733470985503e-06,
      "loss": 2.4897,
      "step": 6610
    },
    {
      "epoch": 2.559427022841657,
      "grad_norm": 16.493982315063477,
      "learning_rate": 8.267303307953715e-06,
      "loss": 1.0713,
      "step": 6611
    },
    {
      "epoch": 2.559814169570267,
      "grad_norm": 27.161054611206055,
      "learning_rate": 8.266873144921926e-06,
      "loss": 1.4839,
      "step": 6612
    },
    {
      "epoch": 2.560201316298877,
      "grad_norm": 35.59758758544922,
      "learning_rate": 8.266442981890138e-06,
      "loss": 1.4989,
      "step": 6613
    },
    {
      "epoch": 2.5605884630274875,
      "grad_norm": 15.312708854675293,
      "learning_rate": 8.266012818858347e-06,
      "loss": 1.1165,
      "step": 6614
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 18.324682235717773,
      "learning_rate": 8.265582655826559e-06,
      "loss": 1.6297,
      "step": 6615
    },
    {
      "epoch": 2.5613627564847077,
      "grad_norm": 19.052082061767578,
      "learning_rate": 8.26515249279477e-06,
      "loss": 1.6705,
      "step": 6616
    },
    {
      "epoch": 2.5617499032133177,
      "grad_norm": 19.095172882080078,
      "learning_rate": 8.26472232976298e-06,
      "loss": 1.8329,
      "step": 6617
    },
    {
      "epoch": 2.562137049941928,
      "grad_norm": 18.2680721282959,
      "learning_rate": 8.264292166731191e-06,
      "loss": 1.755,
      "step": 6618
    },
    {
      "epoch": 2.5625241966705383,
      "grad_norm": 21.651376724243164,
      "learning_rate": 8.263862003699403e-06,
      "loss": 2.5246,
      "step": 6619
    },
    {
      "epoch": 2.5629113433991484,
      "grad_norm": 15.219318389892578,
      "learning_rate": 8.263431840667614e-06,
      "loss": 1.3221,
      "step": 6620
    },
    {
      "epoch": 2.5632984901277585,
      "grad_norm": 19.764413833618164,
      "learning_rate": 8.263001677635824e-06,
      "loss": 1.6083,
      "step": 6621
    },
    {
      "epoch": 2.5636856368563685,
      "grad_norm": 45.56130599975586,
      "learning_rate": 8.262571514604035e-06,
      "loss": 2.3542,
      "step": 6622
    },
    {
      "epoch": 2.5640727835849786,
      "grad_norm": 17.24356460571289,
      "learning_rate": 8.262141351572247e-06,
      "loss": 1.5973,
      "step": 6623
    },
    {
      "epoch": 2.564459930313589,
      "grad_norm": 24.91828727722168,
      "learning_rate": 8.261711188540458e-06,
      "loss": 1.5784,
      "step": 6624
    },
    {
      "epoch": 2.5648470770421987,
      "grad_norm": 22.893909454345703,
      "learning_rate": 8.261281025508668e-06,
      "loss": 1.818,
      "step": 6625
    },
    {
      "epoch": 2.5652342237708092,
      "grad_norm": 21.413818359375,
      "learning_rate": 8.26085086247688e-06,
      "loss": 1.7441,
      "step": 6626
    },
    {
      "epoch": 2.5656213704994193,
      "grad_norm": 35.25287628173828,
      "learning_rate": 8.26042069944509e-06,
      "loss": 1.8809,
      "step": 6627
    },
    {
      "epoch": 2.5660085172280294,
      "grad_norm": 25.19228172302246,
      "learning_rate": 8.259990536413302e-06,
      "loss": 1.2152,
      "step": 6628
    },
    {
      "epoch": 2.5663956639566394,
      "grad_norm": 47.46689224243164,
      "learning_rate": 8.259560373381512e-06,
      "loss": 2.3069,
      "step": 6629
    },
    {
      "epoch": 2.5667828106852495,
      "grad_norm": 23.812740325927734,
      "learning_rate": 8.259130210349723e-06,
      "loss": 1.2442,
      "step": 6630
    },
    {
      "epoch": 2.56716995741386,
      "grad_norm": 21.82427215576172,
      "learning_rate": 8.258700047317935e-06,
      "loss": 0.9511,
      "step": 6631
    },
    {
      "epoch": 2.56755710414247,
      "grad_norm": 24.050264358520508,
      "learning_rate": 8.258269884286144e-06,
      "loss": 1.6458,
      "step": 6632
    },
    {
      "epoch": 2.56794425087108,
      "grad_norm": 19.121042251586914,
      "learning_rate": 8.257839721254356e-06,
      "loss": 1.9195,
      "step": 6633
    },
    {
      "epoch": 2.56833139759969,
      "grad_norm": 15.840896606445312,
      "learning_rate": 8.257409558222567e-06,
      "loss": 1.4247,
      "step": 6634
    },
    {
      "epoch": 2.5687185443283003,
      "grad_norm": 11.309069633483887,
      "learning_rate": 8.256979395190779e-06,
      "loss": 1.0257,
      "step": 6635
    },
    {
      "epoch": 2.569105691056911,
      "grad_norm": 27.5294246673584,
      "learning_rate": 8.256549232158988e-06,
      "loss": 1.0417,
      "step": 6636
    },
    {
      "epoch": 2.569492837785521,
      "grad_norm": 12.484399795532227,
      "learning_rate": 8.2561190691272e-06,
      "loss": 0.52,
      "step": 6637
    },
    {
      "epoch": 2.569879984514131,
      "grad_norm": 16.576677322387695,
      "learning_rate": 8.25568890609541e-06,
      "loss": 1.2114,
      "step": 6638
    },
    {
      "epoch": 2.570267131242741,
      "grad_norm": 33.57497787475586,
      "learning_rate": 8.255258743063623e-06,
      "loss": 1.3264,
      "step": 6639
    },
    {
      "epoch": 2.570654277971351,
      "grad_norm": 24.345352172851562,
      "learning_rate": 8.254828580031832e-06,
      "loss": 1.5692,
      "step": 6640
    },
    {
      "epoch": 2.5710414246999616,
      "grad_norm": 13.552007675170898,
      "learning_rate": 8.254398417000044e-06,
      "loss": 1.3887,
      "step": 6641
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 13.998462677001953,
      "learning_rate": 8.253968253968254e-06,
      "loss": 1.0315,
      "step": 6642
    },
    {
      "epoch": 2.5718157181571817,
      "grad_norm": 36.938514709472656,
      "learning_rate": 8.253538090936467e-06,
      "loss": 1.2733,
      "step": 6643
    },
    {
      "epoch": 2.5722028648857918,
      "grad_norm": 34.79233169555664,
      "learning_rate": 8.253107927904676e-06,
      "loss": 0.9813,
      "step": 6644
    },
    {
      "epoch": 2.572590011614402,
      "grad_norm": 14.253339767456055,
      "learning_rate": 8.252677764872888e-06,
      "loss": 1.0408,
      "step": 6645
    },
    {
      "epoch": 2.572977158343012,
      "grad_norm": 13.641956329345703,
      "learning_rate": 8.252247601841098e-06,
      "loss": 1.0656,
      "step": 6646
    },
    {
      "epoch": 2.573364305071622,
      "grad_norm": 54.87945556640625,
      "learning_rate": 8.251817438809309e-06,
      "loss": 2.3274,
      "step": 6647
    },
    {
      "epoch": 2.5737514518002325,
      "grad_norm": 25.931884765625,
      "learning_rate": 8.25138727577752e-06,
      "loss": 0.9169,
      "step": 6648
    },
    {
      "epoch": 2.5741385985288425,
      "grad_norm": 25.884532928466797,
      "learning_rate": 8.250957112745732e-06,
      "loss": 2.1432,
      "step": 6649
    },
    {
      "epoch": 2.5745257452574526,
      "grad_norm": 34.68015670776367,
      "learning_rate": 8.250526949713941e-06,
      "loss": 1.482,
      "step": 6650
    },
    {
      "epoch": 2.5749128919860627,
      "grad_norm": 43.105045318603516,
      "learning_rate": 8.250096786682153e-06,
      "loss": 2.1268,
      "step": 6651
    },
    {
      "epoch": 2.5753000387146727,
      "grad_norm": 24.116527557373047,
      "learning_rate": 8.249666623650364e-06,
      "loss": 1.2031,
      "step": 6652
    },
    {
      "epoch": 2.5756871854432832,
      "grad_norm": 12.900115013122559,
      "learning_rate": 8.249236460618574e-06,
      "loss": 1.3536,
      "step": 6653
    },
    {
      "epoch": 2.5760743321718933,
      "grad_norm": 12.836307525634766,
      "learning_rate": 8.248806297586785e-06,
      "loss": 0.97,
      "step": 6654
    },
    {
      "epoch": 2.5764614789005034,
      "grad_norm": 25.235376358032227,
      "learning_rate": 8.248376134554997e-06,
      "loss": 1.3382,
      "step": 6655
    },
    {
      "epoch": 2.5768486256291134,
      "grad_norm": 35.96395492553711,
      "learning_rate": 8.247945971523208e-06,
      "loss": 1.822,
      "step": 6656
    },
    {
      "epoch": 2.5772357723577235,
      "grad_norm": 19.99747085571289,
      "learning_rate": 8.247515808491418e-06,
      "loss": 2.7686,
      "step": 6657
    },
    {
      "epoch": 2.5776229190863336,
      "grad_norm": 17.794403076171875,
      "learning_rate": 8.247085645459631e-06,
      "loss": 1.4594,
      "step": 6658
    },
    {
      "epoch": 2.5780100658149436,
      "grad_norm": 15.80921745300293,
      "learning_rate": 8.24665548242784e-06,
      "loss": 1.6486,
      "step": 6659
    },
    {
      "epoch": 2.578397212543554,
      "grad_norm": 17.07121467590332,
      "learning_rate": 8.246225319396052e-06,
      "loss": 1.4251,
      "step": 6660
    },
    {
      "epoch": 2.578784359272164,
      "grad_norm": 35.73323440551758,
      "learning_rate": 8.245795156364262e-06,
      "loss": 1.7617,
      "step": 6661
    },
    {
      "epoch": 2.5791715060007743,
      "grad_norm": 33.160980224609375,
      "learning_rate": 8.245364993332473e-06,
      "loss": 1.6204,
      "step": 6662
    },
    {
      "epoch": 2.5795586527293843,
      "grad_norm": 14.230320930480957,
      "learning_rate": 8.244934830300685e-06,
      "loss": 1.1391,
      "step": 6663
    },
    {
      "epoch": 2.5799457994579944,
      "grad_norm": 11.780088424682617,
      "learning_rate": 8.244504667268896e-06,
      "loss": 0.5051,
      "step": 6664
    },
    {
      "epoch": 2.580332946186605,
      "grad_norm": 12.839888572692871,
      "learning_rate": 8.244074504237106e-06,
      "loss": 0.8102,
      "step": 6665
    },
    {
      "epoch": 2.580720092915215,
      "grad_norm": 20.04764747619629,
      "learning_rate": 8.243644341205317e-06,
      "loss": 1.6449,
      "step": 6666
    },
    {
      "epoch": 2.581107239643825,
      "grad_norm": 15.532073020935059,
      "learning_rate": 8.243214178173529e-06,
      "loss": 0.8572,
      "step": 6667
    },
    {
      "epoch": 2.581494386372435,
      "grad_norm": 12.088016510009766,
      "learning_rate": 8.242784015141738e-06,
      "loss": 0.7544,
      "step": 6668
    },
    {
      "epoch": 2.581881533101045,
      "grad_norm": 15.428149223327637,
      "learning_rate": 8.24235385210995e-06,
      "loss": 0.9772,
      "step": 6669
    },
    {
      "epoch": 2.5822686798296557,
      "grad_norm": 23.381261825561523,
      "learning_rate": 8.241923689078161e-06,
      "loss": 1.4907,
      "step": 6670
    },
    {
      "epoch": 2.5826558265582653,
      "grad_norm": 24.0771484375,
      "learning_rate": 8.241493526046373e-06,
      "loss": 1.3928,
      "step": 6671
    },
    {
      "epoch": 2.583042973286876,
      "grad_norm": 11.126252174377441,
      "learning_rate": 8.241063363014582e-06,
      "loss": 0.7459,
      "step": 6672
    },
    {
      "epoch": 2.583430120015486,
      "grad_norm": 14.005887031555176,
      "learning_rate": 8.240633199982794e-06,
      "loss": 1.0787,
      "step": 6673
    },
    {
      "epoch": 2.583817266744096,
      "grad_norm": 7.303370952606201,
      "learning_rate": 8.240203036951005e-06,
      "loss": 0.1921,
      "step": 6674
    },
    {
      "epoch": 2.584204413472706,
      "grad_norm": 16.63590431213379,
      "learning_rate": 8.239772873919217e-06,
      "loss": 1.4519,
      "step": 6675
    },
    {
      "epoch": 2.584591560201316,
      "grad_norm": 22.3074893951416,
      "learning_rate": 8.239342710887426e-06,
      "loss": 1.5886,
      "step": 6676
    },
    {
      "epoch": 2.5849787069299266,
      "grad_norm": 15.053366661071777,
      "learning_rate": 8.238912547855638e-06,
      "loss": 0.8149,
      "step": 6677
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 24.70357894897461,
      "learning_rate": 8.23848238482385e-06,
      "loss": 1.5997,
      "step": 6678
    },
    {
      "epoch": 2.5857530003871467,
      "grad_norm": 23.569862365722656,
      "learning_rate": 8.23805222179206e-06,
      "loss": 1.3978,
      "step": 6679
    },
    {
      "epoch": 2.586140147115757,
      "grad_norm": 13.371706008911133,
      "learning_rate": 8.23762205876027e-06,
      "loss": 1.3023,
      "step": 6680
    },
    {
      "epoch": 2.586527293844367,
      "grad_norm": 12.310827255249023,
      "learning_rate": 8.237191895728482e-06,
      "loss": 0.7329,
      "step": 6681
    },
    {
      "epoch": 2.5869144405729774,
      "grad_norm": 28.878637313842773,
      "learning_rate": 8.236761732696693e-06,
      "loss": 1.7473,
      "step": 6682
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 15.83698844909668,
      "learning_rate": 8.236331569664903e-06,
      "loss": 1.4928,
      "step": 6683
    },
    {
      "epoch": 2.5876887340301975,
      "grad_norm": 14.096611976623535,
      "learning_rate": 8.235901406633114e-06,
      "loss": 1.4046,
      "step": 6684
    },
    {
      "epoch": 2.5880758807588076,
      "grad_norm": 21.72480010986328,
      "learning_rate": 8.235471243601326e-06,
      "loss": 1.2702,
      "step": 6685
    },
    {
      "epoch": 2.5884630274874176,
      "grad_norm": 43.18500900268555,
      "learning_rate": 8.235041080569537e-06,
      "loss": 1.0228,
      "step": 6686
    },
    {
      "epoch": 2.588850174216028,
      "grad_norm": 27.827970504760742,
      "learning_rate": 8.234610917537747e-06,
      "loss": 1.429,
      "step": 6687
    },
    {
      "epoch": 2.5892373209446378,
      "grad_norm": 74.8778305053711,
      "learning_rate": 8.234180754505958e-06,
      "loss": 0.979,
      "step": 6688
    },
    {
      "epoch": 2.5896244676732483,
      "grad_norm": 22.101787567138672,
      "learning_rate": 8.233750591474168e-06,
      "loss": 1.4822,
      "step": 6689
    },
    {
      "epoch": 2.5900116144018583,
      "grad_norm": 25.46770477294922,
      "learning_rate": 8.233320428442381e-06,
      "loss": 1.6081,
      "step": 6690
    },
    {
      "epoch": 2.5903987611304684,
      "grad_norm": 20.439462661743164,
      "learning_rate": 8.232890265410591e-06,
      "loss": 3.525,
      "step": 6691
    },
    {
      "epoch": 2.5907859078590785,
      "grad_norm": 14.230329513549805,
      "learning_rate": 8.232460102378802e-06,
      "loss": 0.9834,
      "step": 6692
    },
    {
      "epoch": 2.5911730545876885,
      "grad_norm": 25.02967071533203,
      "learning_rate": 8.232029939347012e-06,
      "loss": 1.8624,
      "step": 6693
    },
    {
      "epoch": 2.591560201316299,
      "grad_norm": 21.635534286499023,
      "learning_rate": 8.231599776315225e-06,
      "loss": 0.6095,
      "step": 6694
    },
    {
      "epoch": 2.591947348044909,
      "grad_norm": 22.86844825744629,
      "learning_rate": 8.231169613283435e-06,
      "loss": 1.8083,
      "step": 6695
    },
    {
      "epoch": 2.592334494773519,
      "grad_norm": 34.126895904541016,
      "learning_rate": 8.230739450251646e-06,
      "loss": 1.9565,
      "step": 6696
    },
    {
      "epoch": 2.5927216415021292,
      "grad_norm": 29.96186065673828,
      "learning_rate": 8.230309287219856e-06,
      "loss": 2.7418,
      "step": 6697
    },
    {
      "epoch": 2.5931087882307393,
      "grad_norm": 25.066612243652344,
      "learning_rate": 8.229879124188067e-06,
      "loss": 1.3536,
      "step": 6698
    },
    {
      "epoch": 2.59349593495935,
      "grad_norm": 21.28375244140625,
      "learning_rate": 8.229448961156279e-06,
      "loss": 3.8366,
      "step": 6699
    },
    {
      "epoch": 2.59388308168796,
      "grad_norm": 12.04384708404541,
      "learning_rate": 8.22901879812449e-06,
      "loss": 1.3027,
      "step": 6700
    },
    {
      "epoch": 2.59427022841657,
      "grad_norm": 23.570035934448242,
      "learning_rate": 8.228588635092702e-06,
      "loss": 1.3276,
      "step": 6701
    },
    {
      "epoch": 2.59465737514518,
      "grad_norm": 55.755592346191406,
      "learning_rate": 8.228158472060911e-06,
      "loss": 1.9606,
      "step": 6702
    },
    {
      "epoch": 2.59504452187379,
      "grad_norm": 40.123924255371094,
      "learning_rate": 8.227728309029123e-06,
      "loss": 1.1107,
      "step": 6703
    },
    {
      "epoch": 2.5954316686024,
      "grad_norm": 11.444648742675781,
      "learning_rate": 8.227298145997333e-06,
      "loss": 1.0916,
      "step": 6704
    },
    {
      "epoch": 2.59581881533101,
      "grad_norm": 23.783538818359375,
      "learning_rate": 8.226867982965546e-06,
      "loss": 1.6467,
      "step": 6705
    },
    {
      "epoch": 2.5962059620596207,
      "grad_norm": 13.466407775878906,
      "learning_rate": 8.226437819933755e-06,
      "loss": 1.3777,
      "step": 6706
    },
    {
      "epoch": 2.596593108788231,
      "grad_norm": 5.34808349609375,
      "learning_rate": 8.226007656901967e-06,
      "loss": 0.298,
      "step": 6707
    },
    {
      "epoch": 2.596980255516841,
      "grad_norm": 28.449743270874023,
      "learning_rate": 8.225577493870176e-06,
      "loss": 1.2746,
      "step": 6708
    },
    {
      "epoch": 2.597367402245451,
      "grad_norm": 43.86897277832031,
      "learning_rate": 8.22514733083839e-06,
      "loss": 1.3724,
      "step": 6709
    },
    {
      "epoch": 2.597754548974061,
      "grad_norm": 22.454666137695312,
      "learning_rate": 8.2247171678066e-06,
      "loss": 2.2473,
      "step": 6710
    },
    {
      "epoch": 2.5981416957026715,
      "grad_norm": 22.488393783569336,
      "learning_rate": 8.22428700477481e-06,
      "loss": 1.8882,
      "step": 6711
    },
    {
      "epoch": 2.5985288424312816,
      "grad_norm": 12.292214393615723,
      "learning_rate": 8.22385684174302e-06,
      "loss": 0.7053,
      "step": 6712
    },
    {
      "epoch": 2.5989159891598916,
      "grad_norm": 11.58292293548584,
      "learning_rate": 8.223426678711232e-06,
      "loss": 0.9687,
      "step": 6713
    },
    {
      "epoch": 2.5993031358885017,
      "grad_norm": 17.94167709350586,
      "learning_rate": 8.222996515679443e-06,
      "loss": 2.7174,
      "step": 6714
    },
    {
      "epoch": 2.5996902826171118,
      "grad_norm": 63.20341873168945,
      "learning_rate": 8.222566352647655e-06,
      "loss": 2.4527,
      "step": 6715
    },
    {
      "epoch": 2.6000774293457223,
      "grad_norm": 48.883602142333984,
      "learning_rate": 8.222136189615864e-06,
      "loss": 1.6893,
      "step": 6716
    },
    {
      "epoch": 2.600464576074332,
      "grad_norm": 91.7459487915039,
      "learning_rate": 8.221706026584076e-06,
      "loss": 0.7148,
      "step": 6717
    },
    {
      "epoch": 2.6008517228029424,
      "grad_norm": 25.709245681762695,
      "learning_rate": 8.221275863552287e-06,
      "loss": 1.3524,
      "step": 6718
    },
    {
      "epoch": 2.6012388695315525,
      "grad_norm": 16.083005905151367,
      "learning_rate": 8.220845700520497e-06,
      "loss": 1.7166,
      "step": 6719
    },
    {
      "epoch": 2.6016260162601625,
      "grad_norm": 37.258392333984375,
      "learning_rate": 8.220415537488708e-06,
      "loss": 2.5066,
      "step": 6720
    },
    {
      "epoch": 2.6020131629887726,
      "grad_norm": 27.439565658569336,
      "learning_rate": 8.21998537445692e-06,
      "loss": 0.6865,
      "step": 6721
    },
    {
      "epoch": 2.6024003097173827,
      "grad_norm": 24.61700439453125,
      "learning_rate": 8.219555211425131e-06,
      "loss": 1.2553,
      "step": 6722
    },
    {
      "epoch": 2.602787456445993,
      "grad_norm": 58.37273406982422,
      "learning_rate": 8.219125048393341e-06,
      "loss": 2.1957,
      "step": 6723
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 14.360011100769043,
      "learning_rate": 8.218694885361552e-06,
      "loss": 0.9198,
      "step": 6724
    },
    {
      "epoch": 2.6035617499032133,
      "grad_norm": 28.011436462402344,
      "learning_rate": 8.218264722329764e-06,
      "loss": 1.6365,
      "step": 6725
    },
    {
      "epoch": 2.6039488966318234,
      "grad_norm": 14.291027069091797,
      "learning_rate": 8.217834559297975e-06,
      "loss": 0.9373,
      "step": 6726
    },
    {
      "epoch": 2.6043360433604335,
      "grad_norm": 16.05330467224121,
      "learning_rate": 8.217404396266185e-06,
      "loss": 0.9205,
      "step": 6727
    },
    {
      "epoch": 2.604723190089044,
      "grad_norm": 14.775154113769531,
      "learning_rate": 8.216974233234396e-06,
      "loss": 1.3886,
      "step": 6728
    },
    {
      "epoch": 2.605110336817654,
      "grad_norm": 8.388107299804688,
      "learning_rate": 8.216544070202608e-06,
      "loss": 0.3997,
      "step": 6729
    },
    {
      "epoch": 2.605497483546264,
      "grad_norm": 10.436429977416992,
      "learning_rate": 8.21611390717082e-06,
      "loss": 1.2887,
      "step": 6730
    },
    {
      "epoch": 2.605884630274874,
      "grad_norm": 12.22391128540039,
      "learning_rate": 8.215683744139029e-06,
      "loss": 0.7365,
      "step": 6731
    },
    {
      "epoch": 2.6062717770034842,
      "grad_norm": 45.749568939208984,
      "learning_rate": 8.21525358110724e-06,
      "loss": 2.0681,
      "step": 6732
    },
    {
      "epoch": 2.6066589237320947,
      "grad_norm": 26.13079261779785,
      "learning_rate": 8.214823418075452e-06,
      "loss": 1.1861,
      "step": 6733
    },
    {
      "epoch": 2.6070460704607044,
      "grad_norm": 14.267942428588867,
      "learning_rate": 8.214393255043661e-06,
      "loss": 1.269,
      "step": 6734
    },
    {
      "epoch": 2.607433217189315,
      "grad_norm": 12.925281524658203,
      "learning_rate": 8.213963092011873e-06,
      "loss": 1.3556,
      "step": 6735
    },
    {
      "epoch": 2.607820363917925,
      "grad_norm": 11.719027519226074,
      "learning_rate": 8.213532928980084e-06,
      "loss": 0.5716,
      "step": 6736
    },
    {
      "epoch": 2.608207510646535,
      "grad_norm": 24.158313751220703,
      "learning_rate": 8.213102765948296e-06,
      "loss": 1.424,
      "step": 6737
    },
    {
      "epoch": 2.608594657375145,
      "grad_norm": 28.980613708496094,
      "learning_rate": 8.212672602916505e-06,
      "loss": 1.4352,
      "step": 6738
    },
    {
      "epoch": 2.608981804103755,
      "grad_norm": 29.59172821044922,
      "learning_rate": 8.212242439884717e-06,
      "loss": 1.0382,
      "step": 6739
    },
    {
      "epoch": 2.6093689508323656,
      "grad_norm": 16.144550323486328,
      "learning_rate": 8.211812276852928e-06,
      "loss": 1.2038,
      "step": 6740
    },
    {
      "epoch": 2.6097560975609757,
      "grad_norm": 21.780866622924805,
      "learning_rate": 8.21138211382114e-06,
      "loss": 1.0079,
      "step": 6741
    },
    {
      "epoch": 2.6101432442895858,
      "grad_norm": 15.077619552612305,
      "learning_rate": 8.21095195078935e-06,
      "loss": 1.3998,
      "step": 6742
    },
    {
      "epoch": 2.610530391018196,
      "grad_norm": 26.3847599029541,
      "learning_rate": 8.21052178775756e-06,
      "loss": 1.9007,
      "step": 6743
    },
    {
      "epoch": 2.610917537746806,
      "grad_norm": 14.014765739440918,
      "learning_rate": 8.210091624725772e-06,
      "loss": 1.1837,
      "step": 6744
    },
    {
      "epoch": 2.6113046844754164,
      "grad_norm": 23.973491668701172,
      "learning_rate": 8.209661461693984e-06,
      "loss": 1.2668,
      "step": 6745
    },
    {
      "epoch": 2.6116918312040265,
      "grad_norm": 17.041614532470703,
      "learning_rate": 8.209231298662193e-06,
      "loss": 1.0502,
      "step": 6746
    },
    {
      "epoch": 2.6120789779326365,
      "grad_norm": 15.05147647857666,
      "learning_rate": 8.208801135630405e-06,
      "loss": 0.8167,
      "step": 6747
    },
    {
      "epoch": 2.6124661246612466,
      "grad_norm": 33.58732604980469,
      "learning_rate": 8.208370972598616e-06,
      "loss": 0.6934,
      "step": 6748
    },
    {
      "epoch": 2.6128532713898567,
      "grad_norm": 51.4859504699707,
      "learning_rate": 8.207940809566826e-06,
      "loss": 1.6162,
      "step": 6749
    },
    {
      "epoch": 2.6132404181184667,
      "grad_norm": 18.139850616455078,
      "learning_rate": 8.207510646535037e-06,
      "loss": 1.4468,
      "step": 6750
    },
    {
      "epoch": 2.613627564847077,
      "grad_norm": 42.75245666503906,
      "learning_rate": 8.207080483503249e-06,
      "loss": 0.9941,
      "step": 6751
    },
    {
      "epoch": 2.6140147115756873,
      "grad_norm": 25.072532653808594,
      "learning_rate": 8.20665032047146e-06,
      "loss": 1.3403,
      "step": 6752
    },
    {
      "epoch": 2.6144018583042974,
      "grad_norm": 24.755939483642578,
      "learning_rate": 8.20622015743967e-06,
      "loss": 1.4395,
      "step": 6753
    },
    {
      "epoch": 2.6147890050329075,
      "grad_norm": 25.58188819885254,
      "learning_rate": 8.205789994407881e-06,
      "loss": 2.1013,
      "step": 6754
    },
    {
      "epoch": 2.6151761517615175,
      "grad_norm": 25.143844604492188,
      "learning_rate": 8.205359831376091e-06,
      "loss": 2.2646,
      "step": 6755
    },
    {
      "epoch": 2.6155632984901276,
      "grad_norm": 12.5060453414917,
      "learning_rate": 8.204929668344304e-06,
      "loss": 1.3205,
      "step": 6756
    },
    {
      "epoch": 2.615950445218738,
      "grad_norm": 21.612586975097656,
      "learning_rate": 8.204499505312514e-06,
      "loss": 1.465,
      "step": 6757
    },
    {
      "epoch": 2.616337591947348,
      "grad_norm": 16.688875198364258,
      "learning_rate": 8.204069342280725e-06,
      "loss": 1.3299,
      "step": 6758
    },
    {
      "epoch": 2.6167247386759582,
      "grad_norm": 29.44492530822754,
      "learning_rate": 8.203639179248935e-06,
      "loss": 1.372,
      "step": 6759
    },
    {
      "epoch": 2.6171118854045683,
      "grad_norm": 35.771575927734375,
      "learning_rate": 8.203209016217148e-06,
      "loss": 1.2435,
      "step": 6760
    },
    {
      "epoch": 2.6174990321331784,
      "grad_norm": 38.248382568359375,
      "learning_rate": 8.202778853185358e-06,
      "loss": 1.4723,
      "step": 6761
    },
    {
      "epoch": 2.617886178861789,
      "grad_norm": 32.690528869628906,
      "learning_rate": 8.20234869015357e-06,
      "loss": 3.7085,
      "step": 6762
    },
    {
      "epoch": 2.6182733255903985,
      "grad_norm": 21.3464412689209,
      "learning_rate": 8.201918527121779e-06,
      "loss": 0.9204,
      "step": 6763
    },
    {
      "epoch": 2.618660472319009,
      "grad_norm": 28.959510803222656,
      "learning_rate": 8.20148836408999e-06,
      "loss": 1.8898,
      "step": 6764
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 23.22504234313965,
      "learning_rate": 8.201058201058202e-06,
      "loss": 1.5908,
      "step": 6765
    },
    {
      "epoch": 2.619434765776229,
      "grad_norm": 25.01123046875,
      "learning_rate": 8.200628038026413e-06,
      "loss": 2.3087,
      "step": 6766
    },
    {
      "epoch": 2.619821912504839,
      "grad_norm": 32.77255630493164,
      "learning_rate": 8.200197874994623e-06,
      "loss": 2.1669,
      "step": 6767
    },
    {
      "epoch": 2.6202090592334493,
      "grad_norm": 45.42306137084961,
      "learning_rate": 8.199767711962834e-06,
      "loss": 0.9106,
      "step": 6768
    },
    {
      "epoch": 2.6205962059620598,
      "grad_norm": 6.818270206451416,
      "learning_rate": 8.199337548931046e-06,
      "loss": 0.3635,
      "step": 6769
    },
    {
      "epoch": 2.62098335269067,
      "grad_norm": 17.08702278137207,
      "learning_rate": 8.198907385899255e-06,
      "loss": 1.3182,
      "step": 6770
    },
    {
      "epoch": 2.62137049941928,
      "grad_norm": 15.267102241516113,
      "learning_rate": 8.198477222867467e-06,
      "loss": 0.3628,
      "step": 6771
    },
    {
      "epoch": 2.62175764614789,
      "grad_norm": 13.584625244140625,
      "learning_rate": 8.198047059835678e-06,
      "loss": 0.896,
      "step": 6772
    },
    {
      "epoch": 2.6221447928765,
      "grad_norm": 28.148693084716797,
      "learning_rate": 8.19761689680389e-06,
      "loss": 1.5382,
      "step": 6773
    },
    {
      "epoch": 2.6225319396051106,
      "grad_norm": 18.597227096557617,
      "learning_rate": 8.1971867337721e-06,
      "loss": 0.6212,
      "step": 6774
    },
    {
      "epoch": 2.6229190863337206,
      "grad_norm": 19.353792190551758,
      "learning_rate": 8.196756570740311e-06,
      "loss": 1.8649,
      "step": 6775
    },
    {
      "epoch": 2.6233062330623307,
      "grad_norm": 28.676856994628906,
      "learning_rate": 8.196326407708522e-06,
      "loss": 1.6222,
      "step": 6776
    },
    {
      "epoch": 2.6236933797909407,
      "grad_norm": 14.287002563476562,
      "learning_rate": 8.195896244676734e-06,
      "loss": 1.03,
      "step": 6777
    },
    {
      "epoch": 2.624080526519551,
      "grad_norm": 17.211612701416016,
      "learning_rate": 8.195466081644943e-06,
      "loss": 1.2141,
      "step": 6778
    },
    {
      "epoch": 2.6244676732481613,
      "grad_norm": 23.607275009155273,
      "learning_rate": 8.195035918613155e-06,
      "loss": 1.7107,
      "step": 6779
    },
    {
      "epoch": 2.624854819976771,
      "grad_norm": 10.326000213623047,
      "learning_rate": 8.194605755581366e-06,
      "loss": 1.334,
      "step": 6780
    },
    {
      "epoch": 2.6252419667053815,
      "grad_norm": 31.889787673950195,
      "learning_rate": 8.194175592549578e-06,
      "loss": 1.0187,
      "step": 6781
    },
    {
      "epoch": 2.6256291134339915,
      "grad_norm": 34.21887969970703,
      "learning_rate": 8.193745429517787e-06,
      "loss": 1.4267,
      "step": 6782
    },
    {
      "epoch": 2.6260162601626016,
      "grad_norm": 42.11840057373047,
      "learning_rate": 8.193315266485999e-06,
      "loss": 1.7238,
      "step": 6783
    },
    {
      "epoch": 2.6264034068912117,
      "grad_norm": 14.327359199523926,
      "learning_rate": 8.19288510345421e-06,
      "loss": 0.9127,
      "step": 6784
    },
    {
      "epoch": 2.6267905536198217,
      "grad_norm": 4.328949928283691,
      "learning_rate": 8.19245494042242e-06,
      "loss": 0.1398,
      "step": 6785
    },
    {
      "epoch": 2.6271777003484322,
      "grad_norm": 25.115259170532227,
      "learning_rate": 8.192024777390631e-06,
      "loss": 1.6619,
      "step": 6786
    },
    {
      "epoch": 2.6275648470770423,
      "grad_norm": 16.844348907470703,
      "learning_rate": 8.191594614358843e-06,
      "loss": 1.5239,
      "step": 6787
    },
    {
      "epoch": 2.6279519938056524,
      "grad_norm": 20.329212188720703,
      "learning_rate": 8.191164451327054e-06,
      "loss": 1.7941,
      "step": 6788
    },
    {
      "epoch": 2.6283391405342624,
      "grad_norm": 4.23396635055542,
      "learning_rate": 8.190734288295264e-06,
      "loss": 0.1396,
      "step": 6789
    },
    {
      "epoch": 2.6287262872628725,
      "grad_norm": 4.516357898712158,
      "learning_rate": 8.190304125263475e-06,
      "loss": 0.2607,
      "step": 6790
    },
    {
      "epoch": 2.629113433991483,
      "grad_norm": 24.293888092041016,
      "learning_rate": 8.189873962231687e-06,
      "loss": 2.5151,
      "step": 6791
    },
    {
      "epoch": 2.629500580720093,
      "grad_norm": 21.401498794555664,
      "learning_rate": 8.189443799199898e-06,
      "loss": 1.9127,
      "step": 6792
    },
    {
      "epoch": 2.629887727448703,
      "grad_norm": 23.699113845825195,
      "learning_rate": 8.189013636168108e-06,
      "loss": 1.6373,
      "step": 6793
    },
    {
      "epoch": 2.630274874177313,
      "grad_norm": 14.917882919311523,
      "learning_rate": 8.18858347313632e-06,
      "loss": 1.1449,
      "step": 6794
    },
    {
      "epoch": 2.6306620209059233,
      "grad_norm": 15.138022422790527,
      "learning_rate": 8.18815331010453e-06,
      "loss": 0.5192,
      "step": 6795
    },
    {
      "epoch": 2.6310491676345333,
      "grad_norm": 33.855873107910156,
      "learning_rate": 8.187723147072742e-06,
      "loss": 2.1341,
      "step": 6796
    },
    {
      "epoch": 2.6314363143631434,
      "grad_norm": 26.152545928955078,
      "learning_rate": 8.187292984040952e-06,
      "loss": 2.7438,
      "step": 6797
    },
    {
      "epoch": 2.631823461091754,
      "grad_norm": 70.29946899414062,
      "learning_rate": 8.186862821009163e-06,
      "loss": 2.414,
      "step": 6798
    },
    {
      "epoch": 2.632210607820364,
      "grad_norm": 20.2047119140625,
      "learning_rate": 8.186432657977375e-06,
      "loss": 1.5399,
      "step": 6799
    },
    {
      "epoch": 2.632597754548974,
      "grad_norm": 23.607492446899414,
      "learning_rate": 8.186002494945584e-06,
      "loss": 1.5795,
      "step": 6800
    },
    {
      "epoch": 2.632984901277584,
      "grad_norm": 4.55867862701416,
      "learning_rate": 8.185572331913796e-06,
      "loss": 0.1507,
      "step": 6801
    },
    {
      "epoch": 2.633372048006194,
      "grad_norm": 12.966931343078613,
      "learning_rate": 8.185142168882007e-06,
      "loss": 0.7151,
      "step": 6802
    },
    {
      "epoch": 2.6337591947348047,
      "grad_norm": 12.58231258392334,
      "learning_rate": 8.184712005850219e-06,
      "loss": 0.8694,
      "step": 6803
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 35.72650909423828,
      "learning_rate": 8.184281842818428e-06,
      "loss": 1.7943,
      "step": 6804
    },
    {
      "epoch": 2.634533488192025,
      "grad_norm": 22.86780548095703,
      "learning_rate": 8.18385167978664e-06,
      "loss": 1.9576,
      "step": 6805
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 23.918926239013672,
      "learning_rate": 8.18342151675485e-06,
      "loss": 2.1379,
      "step": 6806
    },
    {
      "epoch": 2.635307781649245,
      "grad_norm": 34.962066650390625,
      "learning_rate": 8.182991353723063e-06,
      "loss": 1.5648,
      "step": 6807
    },
    {
      "epoch": 2.6356949283778555,
      "grad_norm": 13.979260444641113,
      "learning_rate": 8.182561190691272e-06,
      "loss": 1.0496,
      "step": 6808
    },
    {
      "epoch": 2.636082075106465,
      "grad_norm": 42.99247741699219,
      "learning_rate": 8.182131027659484e-06,
      "loss": 1.7907,
      "step": 6809
    },
    {
      "epoch": 2.6364692218350756,
      "grad_norm": 17.598604202270508,
      "learning_rate": 8.181700864627693e-06,
      "loss": 0.9736,
      "step": 6810
    },
    {
      "epoch": 2.6368563685636857,
      "grad_norm": 34.99984359741211,
      "learning_rate": 8.181270701595907e-06,
      "loss": 3.1024,
      "step": 6811
    },
    {
      "epoch": 2.6372435152922957,
      "grad_norm": 22.52560806274414,
      "learning_rate": 8.180840538564116e-06,
      "loss": 1.2197,
      "step": 6812
    },
    {
      "epoch": 2.637630662020906,
      "grad_norm": 21.42768669128418,
      "learning_rate": 8.180410375532328e-06,
      "loss": 2.6411,
      "step": 6813
    },
    {
      "epoch": 2.638017808749516,
      "grad_norm": 22.535953521728516,
      "learning_rate": 8.179980212500537e-06,
      "loss": 1.672,
      "step": 6814
    },
    {
      "epoch": 2.6384049554781264,
      "grad_norm": 43.11405944824219,
      "learning_rate": 8.179550049468749e-06,
      "loss": 1.4385,
      "step": 6815
    },
    {
      "epoch": 2.6387921022067364,
      "grad_norm": 20.74634552001953,
      "learning_rate": 8.17911988643696e-06,
      "loss": 1.8094,
      "step": 6816
    },
    {
      "epoch": 2.6391792489353465,
      "grad_norm": 34.913509368896484,
      "learning_rate": 8.178689723405172e-06,
      "loss": 1.6755,
      "step": 6817
    },
    {
      "epoch": 2.6395663956639566,
      "grad_norm": 18.65781593322754,
      "learning_rate": 8.178259560373381e-06,
      "loss": 1.8384,
      "step": 6818
    },
    {
      "epoch": 2.6399535423925666,
      "grad_norm": 19.49274253845215,
      "learning_rate": 8.177829397341593e-06,
      "loss": 1.1593,
      "step": 6819
    },
    {
      "epoch": 2.640340689121177,
      "grad_norm": 26.351980209350586,
      "learning_rate": 8.177399234309804e-06,
      "loss": 1.4605,
      "step": 6820
    },
    {
      "epoch": 2.640727835849787,
      "grad_norm": 9.116948127746582,
      "learning_rate": 8.176969071278014e-06,
      "loss": 1.3028,
      "step": 6821
    },
    {
      "epoch": 2.6411149825783973,
      "grad_norm": 12.702764511108398,
      "learning_rate": 8.176538908246227e-06,
      "loss": 0.9591,
      "step": 6822
    },
    {
      "epoch": 2.6415021293070073,
      "grad_norm": 17.619953155517578,
      "learning_rate": 8.176108745214437e-06,
      "loss": 1.4811,
      "step": 6823
    },
    {
      "epoch": 2.6418892760356174,
      "grad_norm": 42.35275650024414,
      "learning_rate": 8.175678582182648e-06,
      "loss": 3.0044,
      "step": 6824
    },
    {
      "epoch": 2.642276422764228,
      "grad_norm": 35.443580627441406,
      "learning_rate": 8.175248419150858e-06,
      "loss": 1.7628,
      "step": 6825
    },
    {
      "epoch": 2.6426635694928375,
      "grad_norm": 13.061493873596191,
      "learning_rate": 8.174818256119071e-06,
      "loss": 0.8077,
      "step": 6826
    },
    {
      "epoch": 2.643050716221448,
      "grad_norm": 38.69757080078125,
      "learning_rate": 8.17438809308728e-06,
      "loss": 1.0347,
      "step": 6827
    },
    {
      "epoch": 2.643437862950058,
      "grad_norm": 23.38625144958496,
      "learning_rate": 8.173957930055492e-06,
      "loss": 1.9193,
      "step": 6828
    },
    {
      "epoch": 2.643825009678668,
      "grad_norm": 34.39397048950195,
      "learning_rate": 8.173527767023702e-06,
      "loss": 1.2205,
      "step": 6829
    },
    {
      "epoch": 2.6442121564072782,
      "grad_norm": 5.19598913192749,
      "learning_rate": 8.173097603991913e-06,
      "loss": 0.1717,
      "step": 6830
    },
    {
      "epoch": 2.6445993031358883,
      "grad_norm": 16.458358764648438,
      "learning_rate": 8.172667440960125e-06,
      "loss": 1.0132,
      "step": 6831
    },
    {
      "epoch": 2.644986449864499,
      "grad_norm": 20.629411697387695,
      "learning_rate": 8.172237277928336e-06,
      "loss": 1.4804,
      "step": 6832
    },
    {
      "epoch": 2.645373596593109,
      "grad_norm": 27.80439567565918,
      "learning_rate": 8.171807114896546e-06,
      "loss": 1.5259,
      "step": 6833
    },
    {
      "epoch": 2.645760743321719,
      "grad_norm": 17.359024047851562,
      "learning_rate": 8.171376951864757e-06,
      "loss": 1.377,
      "step": 6834
    },
    {
      "epoch": 2.646147890050329,
      "grad_norm": 22.10283088684082,
      "learning_rate": 8.170946788832969e-06,
      "loss": 1.4688,
      "step": 6835
    },
    {
      "epoch": 2.646535036778939,
      "grad_norm": 14.450483322143555,
      "learning_rate": 8.170516625801178e-06,
      "loss": 0.8348,
      "step": 6836
    },
    {
      "epoch": 2.6469221835075496,
      "grad_norm": 11.347043991088867,
      "learning_rate": 8.17008646276939e-06,
      "loss": 1.0931,
      "step": 6837
    },
    {
      "epoch": 2.6473093302361597,
      "grad_norm": 13.298074722290039,
      "learning_rate": 8.169656299737601e-06,
      "loss": 1.3011,
      "step": 6838
    },
    {
      "epoch": 2.6476964769647697,
      "grad_norm": 36.73592758178711,
      "learning_rate": 8.169226136705813e-06,
      "loss": 2.4097,
      "step": 6839
    },
    {
      "epoch": 2.64808362369338,
      "grad_norm": 12.693828582763672,
      "learning_rate": 8.168795973674022e-06,
      "loss": 0.8613,
      "step": 6840
    },
    {
      "epoch": 2.64847077042199,
      "grad_norm": 38.02173614501953,
      "learning_rate": 8.168365810642234e-06,
      "loss": 1.6257,
      "step": 6841
    },
    {
      "epoch": 2.6488579171506,
      "grad_norm": 13.683550834655762,
      "learning_rate": 8.167935647610445e-06,
      "loss": 1.107,
      "step": 6842
    },
    {
      "epoch": 2.64924506387921,
      "grad_norm": 31.00716781616211,
      "learning_rate": 8.167505484578657e-06,
      "loss": 1.6647,
      "step": 6843
    },
    {
      "epoch": 2.6496322106078205,
      "grad_norm": 25.85207176208496,
      "learning_rate": 8.167075321546866e-06,
      "loss": 0.9698,
      "step": 6844
    },
    {
      "epoch": 2.6500193573364306,
      "grad_norm": 34.33409118652344,
      "learning_rate": 8.166645158515078e-06,
      "loss": 1.9656,
      "step": 6845
    },
    {
      "epoch": 2.6504065040650406,
      "grad_norm": 14.55190372467041,
      "learning_rate": 8.16621499548329e-06,
      "loss": 1.4178,
      "step": 6846
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 20.65404510498047,
      "learning_rate": 8.1657848324515e-06,
      "loss": 1.578,
      "step": 6847
    },
    {
      "epoch": 2.6511807975222608,
      "grad_norm": 17.034399032592773,
      "learning_rate": 8.16535466941971e-06,
      "loss": 1.4611,
      "step": 6848
    },
    {
      "epoch": 2.6515679442508713,
      "grad_norm": 42.88684844970703,
      "learning_rate": 8.164924506387922e-06,
      "loss": 1.8885,
      "step": 6849
    },
    {
      "epoch": 2.6519550909794813,
      "grad_norm": 26.775917053222656,
      "learning_rate": 8.164494343356133e-06,
      "loss": 1.5193,
      "step": 6850
    },
    {
      "epoch": 2.6523422377080914,
      "grad_norm": 6.3697404861450195,
      "learning_rate": 8.164064180324343e-06,
      "loss": 0.3484,
      "step": 6851
    },
    {
      "epoch": 2.6527293844367015,
      "grad_norm": 12.143036842346191,
      "learning_rate": 8.163634017292554e-06,
      "loss": 0.7453,
      "step": 6852
    },
    {
      "epoch": 2.6531165311653115,
      "grad_norm": 17.643939971923828,
      "learning_rate": 8.163203854260766e-06,
      "loss": 1.1829,
      "step": 6853
    },
    {
      "epoch": 2.653503677893922,
      "grad_norm": 16.148378372192383,
      "learning_rate": 8.162773691228977e-06,
      "loss": 1.1359,
      "step": 6854
    },
    {
      "epoch": 2.6538908246225317,
      "grad_norm": 15.776527404785156,
      "learning_rate": 8.162343528197187e-06,
      "loss": 1.3572,
      "step": 6855
    },
    {
      "epoch": 2.654277971351142,
      "grad_norm": 24.14165496826172,
      "learning_rate": 8.161913365165398e-06,
      "loss": 1.7366,
      "step": 6856
    },
    {
      "epoch": 2.6546651180797523,
      "grad_norm": 46.310142517089844,
      "learning_rate": 8.161483202133608e-06,
      "loss": 1.9539,
      "step": 6857
    },
    {
      "epoch": 2.6550522648083623,
      "grad_norm": 18.078153610229492,
      "learning_rate": 8.161053039101821e-06,
      "loss": 0.9244,
      "step": 6858
    },
    {
      "epoch": 2.6554394115369724,
      "grad_norm": 13.782843589782715,
      "learning_rate": 8.16062287607003e-06,
      "loss": 0.851,
      "step": 6859
    },
    {
      "epoch": 2.6558265582655824,
      "grad_norm": 25.177974700927734,
      "learning_rate": 8.160192713038242e-06,
      "loss": 0.6868,
      "step": 6860
    },
    {
      "epoch": 2.656213704994193,
      "grad_norm": 68.21583557128906,
      "learning_rate": 8.159762550006452e-06,
      "loss": 2.6572,
      "step": 6861
    },
    {
      "epoch": 2.656600851722803,
      "grad_norm": 18.408918380737305,
      "learning_rate": 8.159332386974665e-06,
      "loss": 1.181,
      "step": 6862
    },
    {
      "epoch": 2.656987998451413,
      "grad_norm": 59.991058349609375,
      "learning_rate": 8.158902223942875e-06,
      "loss": 0.9375,
      "step": 6863
    },
    {
      "epoch": 2.657375145180023,
      "grad_norm": 20.742544174194336,
      "learning_rate": 8.158472060911086e-06,
      "loss": 1.5921,
      "step": 6864
    },
    {
      "epoch": 2.6577622919086332,
      "grad_norm": 9.835295677185059,
      "learning_rate": 8.158041897879298e-06,
      "loss": 1.2734,
      "step": 6865
    },
    {
      "epoch": 2.6581494386372437,
      "grad_norm": 15.690967559814453,
      "learning_rate": 8.157611734847507e-06,
      "loss": 0.9704,
      "step": 6866
    },
    {
      "epoch": 2.658536585365854,
      "grad_norm": 62.20330047607422,
      "learning_rate": 8.157181571815719e-06,
      "loss": 2.2876,
      "step": 6867
    },
    {
      "epoch": 2.658923732094464,
      "grad_norm": 21.094884872436523,
      "learning_rate": 8.15675140878393e-06,
      "loss": 3.1784,
      "step": 6868
    },
    {
      "epoch": 2.659310878823074,
      "grad_norm": 26.899858474731445,
      "learning_rate": 8.156321245752142e-06,
      "loss": 1.4875,
      "step": 6869
    },
    {
      "epoch": 2.659698025551684,
      "grad_norm": 17.32151222229004,
      "learning_rate": 8.155891082720351e-06,
      "loss": 1.7969,
      "step": 6870
    },
    {
      "epoch": 2.6600851722802945,
      "grad_norm": 31.508182525634766,
      "learning_rate": 8.155460919688563e-06,
      "loss": 2.8475,
      "step": 6871
    },
    {
      "epoch": 2.660472319008904,
      "grad_norm": 23.738811492919922,
      "learning_rate": 8.155030756656772e-06,
      "loss": 1.32,
      "step": 6872
    },
    {
      "epoch": 2.6608594657375146,
      "grad_norm": 21.34337043762207,
      "learning_rate": 8.154600593624986e-06,
      "loss": 3.3149,
      "step": 6873
    },
    {
      "epoch": 2.6612466124661247,
      "grad_norm": 15.578225135803223,
      "learning_rate": 8.154170430593195e-06,
      "loss": 1.3133,
      "step": 6874
    },
    {
      "epoch": 2.6616337591947348,
      "grad_norm": 13.601556777954102,
      "learning_rate": 8.153740267561407e-06,
      "loss": 0.7949,
      "step": 6875
    },
    {
      "epoch": 2.662020905923345,
      "grad_norm": 39.00292205810547,
      "learning_rate": 8.153310104529616e-06,
      "loss": 2.1708,
      "step": 6876
    },
    {
      "epoch": 2.662408052651955,
      "grad_norm": 22.417593002319336,
      "learning_rate": 8.15287994149783e-06,
      "loss": 2.2689,
      "step": 6877
    },
    {
      "epoch": 2.6627951993805654,
      "grad_norm": 17.277069091796875,
      "learning_rate": 8.15244977846604e-06,
      "loss": 1.1222,
      "step": 6878
    },
    {
      "epoch": 2.6631823461091755,
      "grad_norm": 16.578256607055664,
      "learning_rate": 8.15201961543425e-06,
      "loss": 1.067,
      "step": 6879
    },
    {
      "epoch": 2.6635694928377855,
      "grad_norm": 17.356595993041992,
      "learning_rate": 8.15158945240246e-06,
      "loss": 0.7,
      "step": 6880
    },
    {
      "epoch": 2.6639566395663956,
      "grad_norm": 29.139114379882812,
      "learning_rate": 8.151159289370672e-06,
      "loss": 1.578,
      "step": 6881
    },
    {
      "epoch": 2.6643437862950057,
      "grad_norm": 22.611135482788086,
      "learning_rate": 8.150729126338883e-06,
      "loss": 1.6843,
      "step": 6882
    },
    {
      "epoch": 2.664730933023616,
      "grad_norm": 12.277639389038086,
      "learning_rate": 8.150298963307095e-06,
      "loss": 0.8456,
      "step": 6883
    },
    {
      "epoch": 2.6651180797522263,
      "grad_norm": 20.367599487304688,
      "learning_rate": 8.149868800275304e-06,
      "loss": 0.8388,
      "step": 6884
    },
    {
      "epoch": 2.6655052264808363,
      "grad_norm": 15.244606018066406,
      "learning_rate": 8.149438637243516e-06,
      "loss": 1.4295,
      "step": 6885
    },
    {
      "epoch": 2.6658923732094464,
      "grad_norm": 30.012054443359375,
      "learning_rate": 8.149008474211727e-06,
      "loss": 2.2438,
      "step": 6886
    },
    {
      "epoch": 2.6662795199380565,
      "grad_norm": 15.00890827178955,
      "learning_rate": 8.148578311179937e-06,
      "loss": 1.3062,
      "step": 6887
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 6.317322254180908,
      "learning_rate": 8.148148148148148e-06,
      "loss": 0.3025,
      "step": 6888
    },
    {
      "epoch": 2.6670538133952766,
      "grad_norm": 26.591049194335938,
      "learning_rate": 8.14771798511636e-06,
      "loss": 0.669,
      "step": 6889
    },
    {
      "epoch": 2.667440960123887,
      "grad_norm": 14.759337425231934,
      "learning_rate": 8.147287822084571e-06,
      "loss": 1.3228,
      "step": 6890
    },
    {
      "epoch": 2.667828106852497,
      "grad_norm": 15.049444198608398,
      "learning_rate": 8.146857659052781e-06,
      "loss": 1.4739,
      "step": 6891
    },
    {
      "epoch": 2.6682152535811072,
      "grad_norm": 22.575977325439453,
      "learning_rate": 8.146427496020992e-06,
      "loss": 0.9305,
      "step": 6892
    },
    {
      "epoch": 2.6686024003097173,
      "grad_norm": 25.128395080566406,
      "learning_rate": 8.145997332989204e-06,
      "loss": 1.7911,
      "step": 6893
    },
    {
      "epoch": 2.6689895470383274,
      "grad_norm": 21.06012535095215,
      "learning_rate": 8.145567169957415e-06,
      "loss": 1.5744,
      "step": 6894
    },
    {
      "epoch": 2.669376693766938,
      "grad_norm": 19.792896270751953,
      "learning_rate": 8.145137006925625e-06,
      "loss": 1.2452,
      "step": 6895
    },
    {
      "epoch": 2.669763840495548,
      "grad_norm": 20.91855239868164,
      "learning_rate": 8.144706843893836e-06,
      "loss": 1.8876,
      "step": 6896
    },
    {
      "epoch": 2.670150987224158,
      "grad_norm": 58.633541107177734,
      "learning_rate": 8.144276680862048e-06,
      "loss": 1.4324,
      "step": 6897
    },
    {
      "epoch": 2.670538133952768,
      "grad_norm": 13.016094207763672,
      "learning_rate": 8.143846517830259e-06,
      "loss": 1.37,
      "step": 6898
    },
    {
      "epoch": 2.670925280681378,
      "grad_norm": 18.221662521362305,
      "learning_rate": 8.143416354798469e-06,
      "loss": 1.7443,
      "step": 6899
    },
    {
      "epoch": 2.6713124274099886,
      "grad_norm": 27.506872177124023,
      "learning_rate": 8.14298619176668e-06,
      "loss": 1.7021,
      "step": 6900
    },
    {
      "epoch": 2.6716995741385983,
      "grad_norm": 13.61814022064209,
      "learning_rate": 8.142556028734892e-06,
      "loss": 0.9904,
      "step": 6901
    },
    {
      "epoch": 2.6720867208672088,
      "grad_norm": 12.41499137878418,
      "learning_rate": 8.142125865703101e-06,
      "loss": 0.7208,
      "step": 6902
    },
    {
      "epoch": 2.672473867595819,
      "grad_norm": 19.15549659729004,
      "learning_rate": 8.141695702671313e-06,
      "loss": 1.0295,
      "step": 6903
    },
    {
      "epoch": 2.672861014324429,
      "grad_norm": 24.522232055664062,
      "learning_rate": 8.141265539639524e-06,
      "loss": 2.0013,
      "step": 6904
    },
    {
      "epoch": 2.673248161053039,
      "grad_norm": 15.922727584838867,
      "learning_rate": 8.140835376607736e-06,
      "loss": 1.3183,
      "step": 6905
    },
    {
      "epoch": 2.673635307781649,
      "grad_norm": 58.172096252441406,
      "learning_rate": 8.140405213575945e-06,
      "loss": 1.9962,
      "step": 6906
    },
    {
      "epoch": 2.6740224545102595,
      "grad_norm": 22.36731719970703,
      "learning_rate": 8.139975050544157e-06,
      "loss": 1.668,
      "step": 6907
    },
    {
      "epoch": 2.6744096012388696,
      "grad_norm": 15.982538223266602,
      "learning_rate": 8.139544887512368e-06,
      "loss": 1.3345,
      "step": 6908
    },
    {
      "epoch": 2.6747967479674797,
      "grad_norm": 24.61739158630371,
      "learning_rate": 8.13911472448058e-06,
      "loss": 1.4104,
      "step": 6909
    },
    {
      "epoch": 2.6751838946960897,
      "grad_norm": 14.568934440612793,
      "learning_rate": 8.13868456144879e-06,
      "loss": 1.3233,
      "step": 6910
    },
    {
      "epoch": 2.6755710414247,
      "grad_norm": 13.278650283813477,
      "learning_rate": 8.138254398417e-06,
      "loss": 0.8657,
      "step": 6911
    },
    {
      "epoch": 2.6759581881533103,
      "grad_norm": 31.710447311401367,
      "learning_rate": 8.137824235385212e-06,
      "loss": 0.8848,
      "step": 6912
    },
    {
      "epoch": 2.6763453348819204,
      "grad_norm": 32.453983306884766,
      "learning_rate": 8.137394072353424e-06,
      "loss": 1.3827,
      "step": 6913
    },
    {
      "epoch": 2.6767324816105305,
      "grad_norm": 26.001544952392578,
      "learning_rate": 8.136963909321633e-06,
      "loss": 1.4959,
      "step": 6914
    },
    {
      "epoch": 2.6771196283391405,
      "grad_norm": 14.383692741394043,
      "learning_rate": 8.136533746289845e-06,
      "loss": 1.3299,
      "step": 6915
    },
    {
      "epoch": 2.6775067750677506,
      "grad_norm": 17.752735137939453,
      "learning_rate": 8.136103583258056e-06,
      "loss": 1.3318,
      "step": 6916
    },
    {
      "epoch": 2.6778939217963607,
      "grad_norm": 24.682825088500977,
      "learning_rate": 8.135673420226266e-06,
      "loss": 1.2567,
      "step": 6917
    },
    {
      "epoch": 2.6782810685249707,
      "grad_norm": 16.129119873046875,
      "learning_rate": 8.135243257194477e-06,
      "loss": 1.3508,
      "step": 6918
    },
    {
      "epoch": 2.6786682152535812,
      "grad_norm": 18.40675926208496,
      "learning_rate": 8.134813094162689e-06,
      "loss": 1.6052,
      "step": 6919
    },
    {
      "epoch": 2.6790553619821913,
      "grad_norm": 12.099274635314941,
      "learning_rate": 8.1343829311309e-06,
      "loss": 0.7806,
      "step": 6920
    },
    {
      "epoch": 2.6794425087108014,
      "grad_norm": 23.01643180847168,
      "learning_rate": 8.13395276809911e-06,
      "loss": 1.1806,
      "step": 6921
    },
    {
      "epoch": 2.6798296554394114,
      "grad_norm": 20.722675323486328,
      "learning_rate": 8.133522605067321e-06,
      "loss": 1.926,
      "step": 6922
    },
    {
      "epoch": 2.6802168021680215,
      "grad_norm": 15.011143684387207,
      "learning_rate": 8.133092442035531e-06,
      "loss": 1.4306,
      "step": 6923
    },
    {
      "epoch": 2.680603948896632,
      "grad_norm": 26.7178897857666,
      "learning_rate": 8.132662279003744e-06,
      "loss": 1.1535,
      "step": 6924
    },
    {
      "epoch": 2.680991095625242,
      "grad_norm": 41.94824981689453,
      "learning_rate": 8.132232115971954e-06,
      "loss": 1.0941,
      "step": 6925
    },
    {
      "epoch": 2.681378242353852,
      "grad_norm": 5.577817440032959,
      "learning_rate": 8.131801952940165e-06,
      "loss": 0.1779,
      "step": 6926
    },
    {
      "epoch": 2.681765389082462,
      "grad_norm": 23.203231811523438,
      "learning_rate": 8.131371789908375e-06,
      "loss": 1.537,
      "step": 6927
    },
    {
      "epoch": 2.6821525358110723,
      "grad_norm": 9.610389709472656,
      "learning_rate": 8.130941626876588e-06,
      "loss": 0.5149,
      "step": 6928
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 23.800106048583984,
      "learning_rate": 8.130511463844798e-06,
      "loss": 2.0112,
      "step": 6929
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 24.39564323425293,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.1564,
      "step": 6930
    },
    {
      "epoch": 2.683313975996903,
      "grad_norm": 32.54556655883789,
      "learning_rate": 8.129651137781219e-06,
      "loss": 1.8207,
      "step": 6931
    },
    {
      "epoch": 2.683701122725513,
      "grad_norm": 34.4888916015625,
      "learning_rate": 8.12922097474943e-06,
      "loss": 0.942,
      "step": 6932
    },
    {
      "epoch": 2.684088269454123,
      "grad_norm": 14.427404403686523,
      "learning_rate": 8.128790811717642e-06,
      "loss": 0.9904,
      "step": 6933
    },
    {
      "epoch": 2.684475416182733,
      "grad_norm": 24.897756576538086,
      "learning_rate": 8.128360648685853e-06,
      "loss": 1.2172,
      "step": 6934
    },
    {
      "epoch": 2.684862562911343,
      "grad_norm": 23.55988311767578,
      "learning_rate": 8.127930485654063e-06,
      "loss": 2.7467,
      "step": 6935
    },
    {
      "epoch": 2.6852497096399537,
      "grad_norm": 12.097504615783691,
      "learning_rate": 8.127500322622274e-06,
      "loss": 1.3307,
      "step": 6936
    },
    {
      "epoch": 2.6856368563685638,
      "grad_norm": 12.343101501464844,
      "learning_rate": 8.127070159590486e-06,
      "loss": 1.174,
      "step": 6937
    },
    {
      "epoch": 2.686024003097174,
      "grad_norm": 59.28813934326172,
      "learning_rate": 8.126639996558695e-06,
      "loss": 1.6725,
      "step": 6938
    },
    {
      "epoch": 2.686411149825784,
      "grad_norm": 47.93258285522461,
      "learning_rate": 8.126209833526907e-06,
      "loss": 2.8693,
      "step": 6939
    },
    {
      "epoch": 2.686798296554394,
      "grad_norm": 18.121198654174805,
      "learning_rate": 8.125779670495118e-06,
      "loss": 1.2278,
      "step": 6940
    },
    {
      "epoch": 2.6871854432830045,
      "grad_norm": 14.434223175048828,
      "learning_rate": 8.12534950746333e-06,
      "loss": 1.0343,
      "step": 6941
    },
    {
      "epoch": 2.6875725900116145,
      "grad_norm": 15.920064926147461,
      "learning_rate": 8.12491934443154e-06,
      "loss": 0.9254,
      "step": 6942
    },
    {
      "epoch": 2.6879597367402246,
      "grad_norm": 22.641130447387695,
      "learning_rate": 8.12448918139975e-06,
      "loss": 1.819,
      "step": 6943
    },
    {
      "epoch": 2.6883468834688347,
      "grad_norm": 33.96339797973633,
      "learning_rate": 8.124059018367962e-06,
      "loss": 1.443,
      "step": 6944
    },
    {
      "epoch": 2.6887340301974447,
      "grad_norm": 23.854766845703125,
      "learning_rate": 8.123628855336174e-06,
      "loss": 0.8638,
      "step": 6945
    },
    {
      "epoch": 2.6891211769260552,
      "grad_norm": 20.297819137573242,
      "learning_rate": 8.123198692304383e-06,
      "loss": 1.7825,
      "step": 6946
    },
    {
      "epoch": 2.689508323654665,
      "grad_norm": 16.012596130371094,
      "learning_rate": 8.122768529272595e-06,
      "loss": 1.4179,
      "step": 6947
    },
    {
      "epoch": 2.6898954703832754,
      "grad_norm": 24.108814239501953,
      "learning_rate": 8.122338366240806e-06,
      "loss": 1.9823,
      "step": 6948
    },
    {
      "epoch": 2.6902826171118854,
      "grad_norm": 11.294517517089844,
      "learning_rate": 8.121908203209018e-06,
      "loss": 0.6776,
      "step": 6949
    },
    {
      "epoch": 2.6906697638404955,
      "grad_norm": 9.98677921295166,
      "learning_rate": 8.121478040177227e-06,
      "loss": 1.3975,
      "step": 6950
    },
    {
      "epoch": 2.6910569105691056,
      "grad_norm": 27.344152450561523,
      "learning_rate": 8.121047877145439e-06,
      "loss": 1.6124,
      "step": 6951
    },
    {
      "epoch": 2.6914440572977156,
      "grad_norm": 27.98661994934082,
      "learning_rate": 8.12061771411365e-06,
      "loss": 1.6204,
      "step": 6952
    },
    {
      "epoch": 2.691831204026326,
      "grad_norm": 22.096054077148438,
      "learning_rate": 8.12018755108186e-06,
      "loss": 2.9565,
      "step": 6953
    },
    {
      "epoch": 2.692218350754936,
      "grad_norm": 26.15512466430664,
      "learning_rate": 8.119757388050071e-06,
      "loss": 1.8015,
      "step": 6954
    },
    {
      "epoch": 2.6926054974835463,
      "grad_norm": 89.75724792480469,
      "learning_rate": 8.119327225018283e-06,
      "loss": 0.935,
      "step": 6955
    },
    {
      "epoch": 2.6929926442121563,
      "grad_norm": 25.75307273864746,
      "learning_rate": 8.118897061986494e-06,
      "loss": 2.1894,
      "step": 6956
    },
    {
      "epoch": 2.6933797909407664,
      "grad_norm": 37.13037872314453,
      "learning_rate": 8.118466898954704e-06,
      "loss": 1.4161,
      "step": 6957
    },
    {
      "epoch": 2.693766937669377,
      "grad_norm": 21.43087387084961,
      "learning_rate": 8.118036735922915e-06,
      "loss": 1.4888,
      "step": 6958
    },
    {
      "epoch": 2.694154084397987,
      "grad_norm": 12.629855155944824,
      "learning_rate": 8.117606572891127e-06,
      "loss": 0.598,
      "step": 6959
    },
    {
      "epoch": 2.694541231126597,
      "grad_norm": 14.333861351013184,
      "learning_rate": 8.117176409859338e-06,
      "loss": 1.4319,
      "step": 6960
    },
    {
      "epoch": 2.694928377855207,
      "grad_norm": 14.215580940246582,
      "learning_rate": 8.116746246827548e-06,
      "loss": 1.4095,
      "step": 6961
    },
    {
      "epoch": 2.695315524583817,
      "grad_norm": 21.171207427978516,
      "learning_rate": 8.11631608379576e-06,
      "loss": 1.4529,
      "step": 6962
    },
    {
      "epoch": 2.6957026713124272,
      "grad_norm": 33.36982345581055,
      "learning_rate": 8.11588592076397e-06,
      "loss": 1.7492,
      "step": 6963
    },
    {
      "epoch": 2.6960898180410373,
      "grad_norm": 12.37049388885498,
      "learning_rate": 8.115455757732182e-06,
      "loss": 0.9272,
      "step": 6964
    },
    {
      "epoch": 2.696476964769648,
      "grad_norm": 28.69647979736328,
      "learning_rate": 8.115025594700392e-06,
      "loss": 1.5271,
      "step": 6965
    },
    {
      "epoch": 2.696864111498258,
      "grad_norm": 4.860927581787109,
      "learning_rate": 8.114595431668603e-06,
      "loss": 0.2423,
      "step": 6966
    },
    {
      "epoch": 2.697251258226868,
      "grad_norm": 11.489604949951172,
      "learning_rate": 8.114165268636815e-06,
      "loss": 1.1217,
      "step": 6967
    },
    {
      "epoch": 2.697638404955478,
      "grad_norm": 20.816389083862305,
      "learning_rate": 8.113735105605024e-06,
      "loss": 0.9073,
      "step": 6968
    },
    {
      "epoch": 2.698025551684088,
      "grad_norm": 17.855342864990234,
      "learning_rate": 8.113304942573236e-06,
      "loss": 1.5571,
      "step": 6969
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 25.371082305908203,
      "learning_rate": 8.112874779541447e-06,
      "loss": 0.9996,
      "step": 6970
    },
    {
      "epoch": 2.6987998451413087,
      "grad_norm": 20.547344207763672,
      "learning_rate": 8.112444616509659e-06,
      "loss": 1.6268,
      "step": 6971
    },
    {
      "epoch": 2.6991869918699187,
      "grad_norm": 48.07506561279297,
      "learning_rate": 8.112014453477868e-06,
      "loss": 1.5094,
      "step": 6972
    },
    {
      "epoch": 2.699574138598529,
      "grad_norm": 20.532865524291992,
      "learning_rate": 8.11158429044608e-06,
      "loss": 1.6714,
      "step": 6973
    },
    {
      "epoch": 2.699961285327139,
      "grad_norm": 14.21374797821045,
      "learning_rate": 8.11115412741429e-06,
      "loss": 0.9414,
      "step": 6974
    },
    {
      "epoch": 2.7003484320557494,
      "grad_norm": 16.28261375427246,
      "learning_rate": 8.110723964382503e-06,
      "loss": 1.5685,
      "step": 6975
    },
    {
      "epoch": 2.7007355787843594,
      "grad_norm": 12.837137222290039,
      "learning_rate": 8.110293801350712e-06,
      "loss": 0.7039,
      "step": 6976
    },
    {
      "epoch": 2.7011227255129695,
      "grad_norm": 15.382270812988281,
      "learning_rate": 8.109863638318924e-06,
      "loss": 1.7159,
      "step": 6977
    },
    {
      "epoch": 2.7015098722415796,
      "grad_norm": 20.767440795898438,
      "learning_rate": 8.109433475287133e-06,
      "loss": 1.6216,
      "step": 6978
    },
    {
      "epoch": 2.7018970189701896,
      "grad_norm": 20.54705238342285,
      "learning_rate": 8.109003312255346e-06,
      "loss": 2.0129,
      "step": 6979
    },
    {
      "epoch": 2.7022841656987997,
      "grad_norm": 24.147335052490234,
      "learning_rate": 8.108573149223556e-06,
      "loss": 1.5915,
      "step": 6980
    },
    {
      "epoch": 2.7026713124274098,
      "grad_norm": 15.81137466430664,
      "learning_rate": 8.108142986191768e-06,
      "loss": 0.8536,
      "step": 6981
    },
    {
      "epoch": 2.7030584591560203,
      "grad_norm": 17.387022018432617,
      "learning_rate": 8.107712823159977e-06,
      "loss": 1.5908,
      "step": 6982
    },
    {
      "epoch": 2.7034456058846303,
      "grad_norm": 33.798912048339844,
      "learning_rate": 8.107282660128189e-06,
      "loss": 2.2044,
      "step": 6983
    },
    {
      "epoch": 2.7038327526132404,
      "grad_norm": 17.290990829467773,
      "learning_rate": 8.1068524970964e-06,
      "loss": 1.5533,
      "step": 6984
    },
    {
      "epoch": 2.7042198993418505,
      "grad_norm": 31.911649703979492,
      "learning_rate": 8.106422334064612e-06,
      "loss": 1.264,
      "step": 6985
    },
    {
      "epoch": 2.7046070460704605,
      "grad_norm": 15.855948448181152,
      "learning_rate": 8.105992171032823e-06,
      "loss": 1.6158,
      "step": 6986
    },
    {
      "epoch": 2.704994192799071,
      "grad_norm": 26.374900817871094,
      "learning_rate": 8.105562008001033e-06,
      "loss": 1.7096,
      "step": 6987
    },
    {
      "epoch": 2.705381339527681,
      "grad_norm": 20.237546920776367,
      "learning_rate": 8.105131844969244e-06,
      "loss": 1.1341,
      "step": 6988
    },
    {
      "epoch": 2.705768486256291,
      "grad_norm": 26.54198455810547,
      "learning_rate": 8.104701681937454e-06,
      "loss": 1.7487,
      "step": 6989
    },
    {
      "epoch": 2.7061556329849012,
      "grad_norm": 26.40629768371582,
      "learning_rate": 8.104271518905667e-06,
      "loss": 0.8285,
      "step": 6990
    },
    {
      "epoch": 2.7065427797135113,
      "grad_norm": 21.979511260986328,
      "learning_rate": 8.103841355873877e-06,
      "loss": 2.7603,
      "step": 6991
    },
    {
      "epoch": 2.706929926442122,
      "grad_norm": 19.314130783081055,
      "learning_rate": 8.103411192842088e-06,
      "loss": 1.3698,
      "step": 6992
    },
    {
      "epoch": 2.7073170731707314,
      "grad_norm": 34.470252990722656,
      "learning_rate": 8.102981029810298e-06,
      "loss": 2.0861,
      "step": 6993
    },
    {
      "epoch": 2.707704219899342,
      "grad_norm": 34.04669189453125,
      "learning_rate": 8.102550866778511e-06,
      "loss": 0.9778,
      "step": 6994
    },
    {
      "epoch": 2.708091366627952,
      "grad_norm": 17.781618118286133,
      "learning_rate": 8.10212070374672e-06,
      "loss": 1.569,
      "step": 6995
    },
    {
      "epoch": 2.708478513356562,
      "grad_norm": 37.5426025390625,
      "learning_rate": 8.101690540714932e-06,
      "loss": 1.7771,
      "step": 6996
    },
    {
      "epoch": 2.708865660085172,
      "grad_norm": 10.223275184631348,
      "learning_rate": 8.101260377683142e-06,
      "loss": 0.5162,
      "step": 6997
    },
    {
      "epoch": 2.709252806813782,
      "grad_norm": 51.00102996826172,
      "learning_rate": 8.100830214651353e-06,
      "loss": 2.5827,
      "step": 6998
    },
    {
      "epoch": 2.7096399535423927,
      "grad_norm": 23.393230438232422,
      "learning_rate": 8.100400051619565e-06,
      "loss": 1.1148,
      "step": 6999
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 13.598454475402832,
      "learning_rate": 8.099969888587776e-06,
      "loss": 0.862,
      "step": 7000
    },
    {
      "epoch": 2.710414246999613,
      "grad_norm": 34.58905029296875,
      "learning_rate": 8.099539725555986e-06,
      "loss": 1.9727,
      "step": 7001
    },
    {
      "epoch": 2.710801393728223,
      "grad_norm": 13.812541961669922,
      "learning_rate": 8.099109562524197e-06,
      "loss": 0.7898,
      "step": 7002
    },
    {
      "epoch": 2.711188540456833,
      "grad_norm": 15.493898391723633,
      "learning_rate": 8.098679399492409e-06,
      "loss": 1.2865,
      "step": 7003
    },
    {
      "epoch": 2.7115756871854435,
      "grad_norm": 20.855762481689453,
      "learning_rate": 8.098249236460618e-06,
      "loss": 1.57,
      "step": 7004
    },
    {
      "epoch": 2.7119628339140536,
      "grad_norm": 19.143583297729492,
      "learning_rate": 8.09781907342883e-06,
      "loss": 0.5565,
      "step": 7005
    },
    {
      "epoch": 2.7123499806426636,
      "grad_norm": 15.171135902404785,
      "learning_rate": 8.097388910397041e-06,
      "loss": 1.4513,
      "step": 7006
    },
    {
      "epoch": 2.7127371273712737,
      "grad_norm": 30.848833084106445,
      "learning_rate": 8.096958747365253e-06,
      "loss": 1.6439,
      "step": 7007
    },
    {
      "epoch": 2.7131242740998838,
      "grad_norm": 27.461389541625977,
      "learning_rate": 8.096528584333462e-06,
      "loss": 0.4956,
      "step": 7008
    },
    {
      "epoch": 2.713511420828494,
      "grad_norm": 15.725780487060547,
      "learning_rate": 8.096098421301674e-06,
      "loss": 1.4533,
      "step": 7009
    },
    {
      "epoch": 2.713898567557104,
      "grad_norm": 16.5645751953125,
      "learning_rate": 8.095668258269885e-06,
      "loss": 1.1524,
      "step": 7010
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 19.79360008239746,
      "learning_rate": 8.095238095238097e-06,
      "loss": 2.0193,
      "step": 7011
    },
    {
      "epoch": 2.7146728610143245,
      "grad_norm": 11.74072265625,
      "learning_rate": 8.094807932206306e-06,
      "loss": 1.463,
      "step": 7012
    },
    {
      "epoch": 2.7150600077429345,
      "grad_norm": 47.31709671020508,
      "learning_rate": 8.094377769174518e-06,
      "loss": 1.526,
      "step": 7013
    },
    {
      "epoch": 2.7154471544715446,
      "grad_norm": 18.03563690185547,
      "learning_rate": 8.093947606142729e-06,
      "loss": 1.507,
      "step": 7014
    },
    {
      "epoch": 2.7158343012001547,
      "grad_norm": 18.414831161499023,
      "learning_rate": 8.09351744311094e-06,
      "loss": 1.0083,
      "step": 7015
    },
    {
      "epoch": 2.716221447928765,
      "grad_norm": 16.092111587524414,
      "learning_rate": 8.09308728007915e-06,
      "loss": 1.4158,
      "step": 7016
    },
    {
      "epoch": 2.7166085946573753,
      "grad_norm": 17.223430633544922,
      "learning_rate": 8.092657117047362e-06,
      "loss": 1.5137,
      "step": 7017
    },
    {
      "epoch": 2.7169957413859853,
      "grad_norm": 14.20837688446045,
      "learning_rate": 8.092226954015573e-06,
      "loss": 1.0928,
      "step": 7018
    },
    {
      "epoch": 2.7173828881145954,
      "grad_norm": 44.71728515625,
      "learning_rate": 8.091796790983783e-06,
      "loss": 2.3202,
      "step": 7019
    },
    {
      "epoch": 2.7177700348432055,
      "grad_norm": 51.03029251098633,
      "learning_rate": 8.091366627951994e-06,
      "loss": 1.984,
      "step": 7020
    },
    {
      "epoch": 2.718157181571816,
      "grad_norm": 13.59841251373291,
      "learning_rate": 8.090936464920206e-06,
      "loss": 0.7859,
      "step": 7021
    },
    {
      "epoch": 2.718544328300426,
      "grad_norm": 36.581764221191406,
      "learning_rate": 8.090506301888417e-06,
      "loss": 1.2111,
      "step": 7022
    },
    {
      "epoch": 2.718931475029036,
      "grad_norm": 27.275564193725586,
      "learning_rate": 8.090076138856627e-06,
      "loss": 1.8271,
      "step": 7023
    },
    {
      "epoch": 2.719318621757646,
      "grad_norm": 20.876951217651367,
      "learning_rate": 8.089645975824838e-06,
      "loss": 0.9426,
      "step": 7024
    },
    {
      "epoch": 2.7197057684862562,
      "grad_norm": 39.83933639526367,
      "learning_rate": 8.089215812793048e-06,
      "loss": 1.1679,
      "step": 7025
    },
    {
      "epoch": 2.7200929152148663,
      "grad_norm": 21.043827056884766,
      "learning_rate": 8.088785649761261e-06,
      "loss": 1.7135,
      "step": 7026
    },
    {
      "epoch": 2.7204800619434764,
      "grad_norm": 18.112430572509766,
      "learning_rate": 8.08835548672947e-06,
      "loss": 1.0347,
      "step": 7027
    },
    {
      "epoch": 2.720867208672087,
      "grad_norm": 21.740612030029297,
      "learning_rate": 8.087925323697682e-06,
      "loss": 3.2433,
      "step": 7028
    },
    {
      "epoch": 2.721254355400697,
      "grad_norm": 26.09868812561035,
      "learning_rate": 8.087495160665894e-06,
      "loss": 1.5711,
      "step": 7029
    },
    {
      "epoch": 2.721641502129307,
      "grad_norm": 17.404617309570312,
      "learning_rate": 8.087064997634105e-06,
      "loss": 1.2152,
      "step": 7030
    },
    {
      "epoch": 2.722028648857917,
      "grad_norm": 19.696462631225586,
      "learning_rate": 8.086634834602315e-06,
      "loss": 1.0269,
      "step": 7031
    },
    {
      "epoch": 2.722415795586527,
      "grad_norm": 14.317967414855957,
      "learning_rate": 8.086204671570526e-06,
      "loss": 1.0923,
      "step": 7032
    },
    {
      "epoch": 2.7228029423151376,
      "grad_norm": 38.5226936340332,
      "learning_rate": 8.085774508538738e-06,
      "loss": 1.4144,
      "step": 7033
    },
    {
      "epoch": 2.7231900890437477,
      "grad_norm": 21.871938705444336,
      "learning_rate": 8.085344345506947e-06,
      "loss": 1.0238,
      "step": 7034
    },
    {
      "epoch": 2.7235772357723578,
      "grad_norm": 13.487154006958008,
      "learning_rate": 8.084914182475159e-06,
      "loss": 0.7277,
      "step": 7035
    },
    {
      "epoch": 2.723964382500968,
      "grad_norm": 20.224977493286133,
      "learning_rate": 8.08448401944337e-06,
      "loss": 1.5712,
      "step": 7036
    },
    {
      "epoch": 2.724351529229578,
      "grad_norm": 29.110342025756836,
      "learning_rate": 8.084053856411581e-06,
      "loss": 0.6008,
      "step": 7037
    },
    {
      "epoch": 2.7247386759581884,
      "grad_norm": 14.053235054016113,
      "learning_rate": 8.083623693379791e-06,
      "loss": 0.7156,
      "step": 7038
    },
    {
      "epoch": 2.725125822686798,
      "grad_norm": 12.81679630279541,
      "learning_rate": 8.083193530348003e-06,
      "loss": 0.7405,
      "step": 7039
    },
    {
      "epoch": 2.7255129694154085,
      "grad_norm": 35.30990982055664,
      "learning_rate": 8.082763367316212e-06,
      "loss": 2.0735,
      "step": 7040
    },
    {
      "epoch": 2.7259001161440186,
      "grad_norm": 22.00933265686035,
      "learning_rate": 8.082333204284425e-06,
      "loss": 0.6907,
      "step": 7041
    },
    {
      "epoch": 2.7262872628726287,
      "grad_norm": 21.890897750854492,
      "learning_rate": 8.081903041252635e-06,
      "loss": 1.0723,
      "step": 7042
    },
    {
      "epoch": 2.7266744096012387,
      "grad_norm": 37.540184020996094,
      "learning_rate": 8.081472878220847e-06,
      "loss": 1.486,
      "step": 7043
    },
    {
      "epoch": 2.727061556329849,
      "grad_norm": 26.49849510192871,
      "learning_rate": 8.081042715189056e-06,
      "loss": 1.67,
      "step": 7044
    },
    {
      "epoch": 2.7274487030584593,
      "grad_norm": 23.866226196289062,
      "learning_rate": 8.08061255215727e-06,
      "loss": 2.5621,
      "step": 7045
    },
    {
      "epoch": 2.7278358497870694,
      "grad_norm": 28.182321548461914,
      "learning_rate": 8.08018238912548e-06,
      "loss": 1.427,
      "step": 7046
    },
    {
      "epoch": 2.7282229965156795,
      "grad_norm": 17.635828018188477,
      "learning_rate": 8.07975222609369e-06,
      "loss": 1.7119,
      "step": 7047
    },
    {
      "epoch": 2.7286101432442895,
      "grad_norm": 16.83719253540039,
      "learning_rate": 8.0793220630619e-06,
      "loss": 1.5289,
      "step": 7048
    },
    {
      "epoch": 2.7289972899728996,
      "grad_norm": 17.31296157836914,
      "learning_rate": 8.078891900030112e-06,
      "loss": 1.5062,
      "step": 7049
    },
    {
      "epoch": 2.72938443670151,
      "grad_norm": 18.88442039489746,
      "learning_rate": 8.078461736998323e-06,
      "loss": 1.5459,
      "step": 7050
    },
    {
      "epoch": 2.72977158343012,
      "grad_norm": 19.38784408569336,
      "learning_rate": 8.078031573966535e-06,
      "loss": 0.6251,
      "step": 7051
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 12.625946998596191,
      "learning_rate": 8.077601410934744e-06,
      "loss": 0.8099,
      "step": 7052
    },
    {
      "epoch": 2.7305458768873403,
      "grad_norm": 23.7990665435791,
      "learning_rate": 8.077171247902956e-06,
      "loss": 2.4635,
      "step": 7053
    },
    {
      "epoch": 2.7309330236159504,
      "grad_norm": 16.5762996673584,
      "learning_rate": 8.076741084871167e-06,
      "loss": 1.1481,
      "step": 7054
    },
    {
      "epoch": 2.7313201703445604,
      "grad_norm": 13.760950088500977,
      "learning_rate": 8.076310921839377e-06,
      "loss": 0.8119,
      "step": 7055
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 34.06745529174805,
      "learning_rate": 8.075880758807588e-06,
      "loss": 1.6232,
      "step": 7056
    },
    {
      "epoch": 2.732094463801781,
      "grad_norm": 22.69525718688965,
      "learning_rate": 8.0754505957758e-06,
      "loss": 1.9882,
      "step": 7057
    },
    {
      "epoch": 2.732481610530391,
      "grad_norm": 22.570791244506836,
      "learning_rate": 8.075020432744011e-06,
      "loss": 1.5838,
      "step": 7058
    },
    {
      "epoch": 2.732868757259001,
      "grad_norm": 14.956437110900879,
      "learning_rate": 8.07459026971222e-06,
      "loss": 1.0884,
      "step": 7059
    },
    {
      "epoch": 2.733255903987611,
      "grad_norm": 10.855314254760742,
      "learning_rate": 8.074160106680432e-06,
      "loss": 1.0693,
      "step": 7060
    },
    {
      "epoch": 2.7336430507162213,
      "grad_norm": 33.23270797729492,
      "learning_rate": 8.073729943648644e-06,
      "loss": 0.9933,
      "step": 7061
    },
    {
      "epoch": 2.7340301974448318,
      "grad_norm": 12.767133712768555,
      "learning_rate": 8.073299780616855e-06,
      "loss": 0.7321,
      "step": 7062
    },
    {
      "epoch": 2.734417344173442,
      "grad_norm": 16.639972686767578,
      "learning_rate": 8.072869617585065e-06,
      "loss": 1.0405,
      "step": 7063
    },
    {
      "epoch": 2.734804490902052,
      "grad_norm": 21.27292251586914,
      "learning_rate": 8.072439454553276e-06,
      "loss": 0.7078,
      "step": 7064
    },
    {
      "epoch": 2.735191637630662,
      "grad_norm": 15.400503158569336,
      "learning_rate": 8.072009291521488e-06,
      "loss": 0.551,
      "step": 7065
    },
    {
      "epoch": 2.735578784359272,
      "grad_norm": 16.40639877319336,
      "learning_rate": 8.071579128489699e-06,
      "loss": 1.1349,
      "step": 7066
    },
    {
      "epoch": 2.7359659310878826,
      "grad_norm": 30.84383773803711,
      "learning_rate": 8.071148965457909e-06,
      "loss": 1.1122,
      "step": 7067
    },
    {
      "epoch": 2.736353077816492,
      "grad_norm": 12.992862701416016,
      "learning_rate": 8.07071880242612e-06,
      "loss": 1.0267,
      "step": 7068
    },
    {
      "epoch": 2.7367402245451027,
      "grad_norm": 48.6314582824707,
      "learning_rate": 8.070288639394332e-06,
      "loss": 1.7679,
      "step": 7069
    },
    {
      "epoch": 2.7371273712737128,
      "grad_norm": 12.006497383117676,
      "learning_rate": 8.069858476362541e-06,
      "loss": 0.9274,
      "step": 7070
    },
    {
      "epoch": 2.737514518002323,
      "grad_norm": 14.069965362548828,
      "learning_rate": 8.069428313330753e-06,
      "loss": 1.0089,
      "step": 7071
    },
    {
      "epoch": 2.737901664730933,
      "grad_norm": 24.451231002807617,
      "learning_rate": 8.068998150298964e-06,
      "loss": 1.4643,
      "step": 7072
    },
    {
      "epoch": 2.738288811459543,
      "grad_norm": 26.04349136352539,
      "learning_rate": 8.068567987267176e-06,
      "loss": 1.6018,
      "step": 7073
    },
    {
      "epoch": 2.7386759581881535,
      "grad_norm": 28.694772720336914,
      "learning_rate": 8.068137824235385e-06,
      "loss": 2.2164,
      "step": 7074
    },
    {
      "epoch": 2.7390631049167635,
      "grad_norm": 12.493948936462402,
      "learning_rate": 8.067707661203597e-06,
      "loss": 0.7883,
      "step": 7075
    },
    {
      "epoch": 2.7394502516453736,
      "grad_norm": 21.89902114868164,
      "learning_rate": 8.067277498171808e-06,
      "loss": 1.5411,
      "step": 7076
    },
    {
      "epoch": 2.7398373983739837,
      "grad_norm": 26.68691635131836,
      "learning_rate": 8.06684733514002e-06,
      "loss": 2.237,
      "step": 7077
    },
    {
      "epoch": 2.7402245451025937,
      "grad_norm": 16.910701751708984,
      "learning_rate": 8.06641717210823e-06,
      "loss": 0.9293,
      "step": 7078
    },
    {
      "epoch": 2.7406116918312042,
      "grad_norm": 12.327709197998047,
      "learning_rate": 8.06598700907644e-06,
      "loss": 0.9557,
      "step": 7079
    },
    {
      "epoch": 2.7409988385598143,
      "grad_norm": 50.73070526123047,
      "learning_rate": 8.065556846044652e-06,
      "loss": 2.6174,
      "step": 7080
    },
    {
      "epoch": 2.7413859852884244,
      "grad_norm": 21.639705657958984,
      "learning_rate": 8.065126683012863e-06,
      "loss": 1.9422,
      "step": 7081
    },
    {
      "epoch": 2.7417731320170344,
      "grad_norm": 21.901521682739258,
      "learning_rate": 8.064696519981073e-06,
      "loss": 1.9327,
      "step": 7082
    },
    {
      "epoch": 2.7421602787456445,
      "grad_norm": 32.9222412109375,
      "learning_rate": 8.064266356949285e-06,
      "loss": 1.4713,
      "step": 7083
    },
    {
      "epoch": 2.742547425474255,
      "grad_norm": 16.314945220947266,
      "learning_rate": 8.063836193917496e-06,
      "loss": 1.3837,
      "step": 7084
    },
    {
      "epoch": 2.7429345722028646,
      "grad_norm": 18.285863876342773,
      "learning_rate": 8.063406030885706e-06,
      "loss": 1.1522,
      "step": 7085
    },
    {
      "epoch": 2.743321718931475,
      "grad_norm": 10.812846183776855,
      "learning_rate": 8.062975867853917e-06,
      "loss": 0.5116,
      "step": 7086
    },
    {
      "epoch": 2.743708865660085,
      "grad_norm": 12.659780502319336,
      "learning_rate": 8.062545704822129e-06,
      "loss": 1.335,
      "step": 7087
    },
    {
      "epoch": 2.7440960123886953,
      "grad_norm": 15.0938720703125,
      "learning_rate": 8.06211554179034e-06,
      "loss": 1.2911,
      "step": 7088
    },
    {
      "epoch": 2.7444831591173053,
      "grad_norm": 25.296175003051758,
      "learning_rate": 8.06168537875855e-06,
      "loss": 1.3714,
      "step": 7089
    },
    {
      "epoch": 2.7448703058459154,
      "grad_norm": 14.813282012939453,
      "learning_rate": 8.061255215726761e-06,
      "loss": 1.1937,
      "step": 7090
    },
    {
      "epoch": 2.745257452574526,
      "grad_norm": 14.404796600341797,
      "learning_rate": 8.06082505269497e-06,
      "loss": 0.9406,
      "step": 7091
    },
    {
      "epoch": 2.745644599303136,
      "grad_norm": 14.258151054382324,
      "learning_rate": 8.060394889663184e-06,
      "loss": 1.0262,
      "step": 7092
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 47.13587951660156,
      "learning_rate": 8.059964726631394e-06,
      "loss": 3.6544,
      "step": 7093
    },
    {
      "epoch": 2.746418892760356,
      "grad_norm": 48.927207946777344,
      "learning_rate": 8.059534563599605e-06,
      "loss": 0.711,
      "step": 7094
    },
    {
      "epoch": 2.746806039488966,
      "grad_norm": 16.06431770324707,
      "learning_rate": 8.059104400567815e-06,
      "loss": 0.6095,
      "step": 7095
    },
    {
      "epoch": 2.7471931862175767,
      "grad_norm": 24.337495803833008,
      "learning_rate": 8.058674237536028e-06,
      "loss": 0.8022,
      "step": 7096
    },
    {
      "epoch": 2.7475803329461868,
      "grad_norm": 21.448123931884766,
      "learning_rate": 8.058244074504238e-06,
      "loss": 0.8707,
      "step": 7097
    },
    {
      "epoch": 2.747967479674797,
      "grad_norm": 38.28279495239258,
      "learning_rate": 8.057813911472449e-06,
      "loss": 2.1931,
      "step": 7098
    },
    {
      "epoch": 2.748354626403407,
      "grad_norm": 14.036734580993652,
      "learning_rate": 8.057383748440659e-06,
      "loss": 0.9747,
      "step": 7099
    },
    {
      "epoch": 2.748741773132017,
      "grad_norm": 5.117849349975586,
      "learning_rate": 8.05695358540887e-06,
      "loss": 0.1603,
      "step": 7100
    },
    {
      "epoch": 2.749128919860627,
      "grad_norm": 16.053030014038086,
      "learning_rate": 8.056523422377082e-06,
      "loss": 1.0706,
      "step": 7101
    },
    {
      "epoch": 2.749516066589237,
      "grad_norm": 30.612056732177734,
      "learning_rate": 8.056093259345293e-06,
      "loss": 2.1657,
      "step": 7102
    },
    {
      "epoch": 2.7499032133178476,
      "grad_norm": 22.44410514831543,
      "learning_rate": 8.055663096313503e-06,
      "loss": 1.2387,
      "step": 7103
    },
    {
      "epoch": 2.7502903600464577,
      "grad_norm": 12.544965744018555,
      "learning_rate": 8.055232933281714e-06,
      "loss": 0.7121,
      "step": 7104
    },
    {
      "epoch": 2.7506775067750677,
      "grad_norm": 36.130104064941406,
      "learning_rate": 8.054802770249926e-06,
      "loss": 2.7807,
      "step": 7105
    },
    {
      "epoch": 2.751064653503678,
      "grad_norm": 26.609365463256836,
      "learning_rate": 8.054372607218135e-06,
      "loss": 2.7592,
      "step": 7106
    },
    {
      "epoch": 2.751451800232288,
      "grad_norm": 13.738825798034668,
      "learning_rate": 8.053942444186347e-06,
      "loss": 0.9317,
      "step": 7107
    },
    {
      "epoch": 2.7518389469608984,
      "grad_norm": 16.581331253051758,
      "learning_rate": 8.053512281154558e-06,
      "loss": 1.5146,
      "step": 7108
    },
    {
      "epoch": 2.7522260936895084,
      "grad_norm": 58.10212326049805,
      "learning_rate": 8.05308211812277e-06,
      "loss": 1.881,
      "step": 7109
    },
    {
      "epoch": 2.7526132404181185,
      "grad_norm": 29.435550689697266,
      "learning_rate": 8.05265195509098e-06,
      "loss": 2.2473,
      "step": 7110
    },
    {
      "epoch": 2.7530003871467286,
      "grad_norm": 9.462213516235352,
      "learning_rate": 8.052221792059192e-06,
      "loss": 0.476,
      "step": 7111
    },
    {
      "epoch": 2.7533875338753386,
      "grad_norm": 12.546217918395996,
      "learning_rate": 8.051791629027402e-06,
      "loss": 1.3498,
      "step": 7112
    },
    {
      "epoch": 2.753774680603949,
      "grad_norm": 13.134109497070312,
      "learning_rate": 8.051361465995614e-06,
      "loss": 1.0146,
      "step": 7113
    },
    {
      "epoch": 2.7541618273325588,
      "grad_norm": 25.903669357299805,
      "learning_rate": 8.050931302963823e-06,
      "loss": 1.1616,
      "step": 7114
    },
    {
      "epoch": 2.7545489740611693,
      "grad_norm": 11.690214157104492,
      "learning_rate": 8.050501139932035e-06,
      "loss": 1.4387,
      "step": 7115
    },
    {
      "epoch": 2.7549361207897793,
      "grad_norm": 42.81486511230469,
      "learning_rate": 8.050070976900246e-06,
      "loss": 1.6976,
      "step": 7116
    },
    {
      "epoch": 2.7553232675183894,
      "grad_norm": 17.701738357543945,
      "learning_rate": 8.049640813868457e-06,
      "loss": 0.7821,
      "step": 7117
    },
    {
      "epoch": 2.7557104142469995,
      "grad_norm": 13.259284973144531,
      "learning_rate": 8.049210650836667e-06,
      "loss": 1.4018,
      "step": 7118
    },
    {
      "epoch": 2.7560975609756095,
      "grad_norm": 26.890241622924805,
      "learning_rate": 8.048780487804879e-06,
      "loss": 2.8417,
      "step": 7119
    },
    {
      "epoch": 2.75648470770422,
      "grad_norm": 34.374603271484375,
      "learning_rate": 8.04835032477309e-06,
      "loss": 1.3754,
      "step": 7120
    },
    {
      "epoch": 2.75687185443283,
      "grad_norm": 9.199748992919922,
      "learning_rate": 8.0479201617413e-06,
      "loss": 0.4841,
      "step": 7121
    },
    {
      "epoch": 2.75725900116144,
      "grad_norm": 26.843158721923828,
      "learning_rate": 8.047489998709511e-06,
      "loss": 1.6736,
      "step": 7122
    },
    {
      "epoch": 2.7576461478900502,
      "grad_norm": 14.326163291931152,
      "learning_rate": 8.047059835677723e-06,
      "loss": 0.8508,
      "step": 7123
    },
    {
      "epoch": 2.7580332946186603,
      "grad_norm": 34.606101989746094,
      "learning_rate": 8.046629672645934e-06,
      "loss": 1.7504,
      "step": 7124
    },
    {
      "epoch": 2.758420441347271,
      "grad_norm": 16.73243522644043,
      "learning_rate": 8.046199509614144e-06,
      "loss": 1.2913,
      "step": 7125
    },
    {
      "epoch": 2.758807588075881,
      "grad_norm": 28.691635131835938,
      "learning_rate": 8.045769346582355e-06,
      "loss": 1.5438,
      "step": 7126
    },
    {
      "epoch": 2.759194734804491,
      "grad_norm": 21.34273910522461,
      "learning_rate": 8.045339183550567e-06,
      "loss": 1.2247,
      "step": 7127
    },
    {
      "epoch": 2.759581881533101,
      "grad_norm": 56.78982162475586,
      "learning_rate": 8.044909020518778e-06,
      "loss": 2.1773,
      "step": 7128
    },
    {
      "epoch": 2.759969028261711,
      "grad_norm": 18.980478286743164,
      "learning_rate": 8.044478857486988e-06,
      "loss": 1.0483,
      "step": 7129
    },
    {
      "epoch": 2.7603561749903216,
      "grad_norm": 20.368921279907227,
      "learning_rate": 8.044048694455199e-06,
      "loss": 1.4411,
      "step": 7130
    },
    {
      "epoch": 2.760743321718931,
      "grad_norm": 23.70928192138672,
      "learning_rate": 8.04361853142341e-06,
      "loss": 1.1116,
      "step": 7131
    },
    {
      "epoch": 2.7611304684475417,
      "grad_norm": 12.605256080627441,
      "learning_rate": 8.043188368391622e-06,
      "loss": 1.0048,
      "step": 7132
    },
    {
      "epoch": 2.761517615176152,
      "grad_norm": 11.36899185180664,
      "learning_rate": 8.042758205359832e-06,
      "loss": 0.7137,
      "step": 7133
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 36.6869010925293,
      "learning_rate": 8.042328042328043e-06,
      "loss": 1.4236,
      "step": 7134
    },
    {
      "epoch": 2.762291908633372,
      "grad_norm": 25.339696884155273,
      "learning_rate": 8.041897879296255e-06,
      "loss": 1.5018,
      "step": 7135
    },
    {
      "epoch": 2.762679055361982,
      "grad_norm": 23.239112854003906,
      "learning_rate": 8.041467716264464e-06,
      "loss": 1.5348,
      "step": 7136
    },
    {
      "epoch": 2.7630662020905925,
      "grad_norm": 6.089605808258057,
      "learning_rate": 8.041037553232676e-06,
      "loss": 0.174,
      "step": 7137
    },
    {
      "epoch": 2.7634533488192026,
      "grad_norm": 10.33807373046875,
      "learning_rate": 8.040607390200887e-06,
      "loss": 0.4913,
      "step": 7138
    },
    {
      "epoch": 2.7638404955478126,
      "grad_norm": 15.254080772399902,
      "learning_rate": 8.040177227169098e-06,
      "loss": 1.3126,
      "step": 7139
    },
    {
      "epoch": 2.7642276422764227,
      "grad_norm": 17.12805938720703,
      "learning_rate": 8.039747064137308e-06,
      "loss": 1.4796,
      "step": 7140
    },
    {
      "epoch": 2.7646147890050328,
      "grad_norm": 15.019600868225098,
      "learning_rate": 8.03931690110552e-06,
      "loss": 0.8264,
      "step": 7141
    },
    {
      "epoch": 2.7650019357336433,
      "grad_norm": 14.92647933959961,
      "learning_rate": 8.03888673807373e-06,
      "loss": 0.8558,
      "step": 7142
    },
    {
      "epoch": 2.7653890824622533,
      "grad_norm": 12.205437660217285,
      "learning_rate": 8.038456575041942e-06,
      "loss": 0.5404,
      "step": 7143
    },
    {
      "epoch": 2.7657762291908634,
      "grad_norm": 24.69274139404297,
      "learning_rate": 8.038026412010152e-06,
      "loss": 1.9923,
      "step": 7144
    },
    {
      "epoch": 2.7661633759194735,
      "grad_norm": 20.33807945251465,
      "learning_rate": 8.037596248978364e-06,
      "loss": 1.1836,
      "step": 7145
    },
    {
      "epoch": 2.7665505226480835,
      "grad_norm": 20.222909927368164,
      "learning_rate": 8.037166085946573e-06,
      "loss": 1.8777,
      "step": 7146
    },
    {
      "epoch": 2.7669376693766936,
      "grad_norm": 16.306547164916992,
      "learning_rate": 8.036735922914786e-06,
      "loss": 1.2231,
      "step": 7147
    },
    {
      "epoch": 2.7673248161053037,
      "grad_norm": 35.098697662353516,
      "learning_rate": 8.036305759882996e-06,
      "loss": 1.6388,
      "step": 7148
    },
    {
      "epoch": 2.767711962833914,
      "grad_norm": 63.56222915649414,
      "learning_rate": 8.035875596851208e-06,
      "loss": 2.3004,
      "step": 7149
    },
    {
      "epoch": 2.7680991095625243,
      "grad_norm": 13.597322463989258,
      "learning_rate": 8.035445433819417e-06,
      "loss": 0.7704,
      "step": 7150
    },
    {
      "epoch": 2.7684862562911343,
      "grad_norm": 12.321859359741211,
      "learning_rate": 8.035015270787629e-06,
      "loss": 1.3157,
      "step": 7151
    },
    {
      "epoch": 2.7688734030197444,
      "grad_norm": 16.353179931640625,
      "learning_rate": 8.03458510775584e-06,
      "loss": 0.9214,
      "step": 7152
    },
    {
      "epoch": 2.7692605497483544,
      "grad_norm": 21.717071533203125,
      "learning_rate": 8.034154944724052e-06,
      "loss": 1.6251,
      "step": 7153
    },
    {
      "epoch": 2.769647696476965,
      "grad_norm": 22.45531463623047,
      "learning_rate": 8.033724781692263e-06,
      "loss": 1.5271,
      "step": 7154
    },
    {
      "epoch": 2.770034843205575,
      "grad_norm": 33.10977554321289,
      "learning_rate": 8.033294618660473e-06,
      "loss": 1.8679,
      "step": 7155
    },
    {
      "epoch": 2.770421989934185,
      "grad_norm": 13.36409854888916,
      "learning_rate": 8.032864455628684e-06,
      "loss": 0.9863,
      "step": 7156
    },
    {
      "epoch": 2.770809136662795,
      "grad_norm": 18.305419921875,
      "learning_rate": 8.032434292596894e-06,
      "loss": 0.6173,
      "step": 7157
    },
    {
      "epoch": 2.7711962833914052,
      "grad_norm": 19.277990341186523,
      "learning_rate": 8.032004129565107e-06,
      "loss": 1.7989,
      "step": 7158
    },
    {
      "epoch": 2.7715834301200157,
      "grad_norm": 17.688213348388672,
      "learning_rate": 8.031573966533317e-06,
      "loss": 1.6851,
      "step": 7159
    },
    {
      "epoch": 2.7719705768486254,
      "grad_norm": 26.46255874633789,
      "learning_rate": 8.031143803501528e-06,
      "loss": 1.2884,
      "step": 7160
    },
    {
      "epoch": 2.772357723577236,
      "grad_norm": 16.44368553161621,
      "learning_rate": 8.030713640469738e-06,
      "loss": 1.3951,
      "step": 7161
    },
    {
      "epoch": 2.772744870305846,
      "grad_norm": 15.590792655944824,
      "learning_rate": 8.030283477437951e-06,
      "loss": 1.4605,
      "step": 7162
    },
    {
      "epoch": 2.773132017034456,
      "grad_norm": 19.449783325195312,
      "learning_rate": 8.02985331440616e-06,
      "loss": 0.8191,
      "step": 7163
    },
    {
      "epoch": 2.773519163763066,
      "grad_norm": 17.262001037597656,
      "learning_rate": 8.029423151374372e-06,
      "loss": 0.8499,
      "step": 7164
    },
    {
      "epoch": 2.773906310491676,
      "grad_norm": 14.768202781677246,
      "learning_rate": 8.028992988342582e-06,
      "loss": 0.8758,
      "step": 7165
    },
    {
      "epoch": 2.7742934572202866,
      "grad_norm": 13.061513900756836,
      "learning_rate": 8.028562825310793e-06,
      "loss": 0.7279,
      "step": 7166
    },
    {
      "epoch": 2.7746806039488967,
      "grad_norm": 18.90823745727539,
      "learning_rate": 8.028132662279005e-06,
      "loss": 0.9072,
      "step": 7167
    },
    {
      "epoch": 2.7750677506775068,
      "grad_norm": 25.349576950073242,
      "learning_rate": 8.027702499247216e-06,
      "loss": 1.2478,
      "step": 7168
    },
    {
      "epoch": 2.775454897406117,
      "grad_norm": 33.47298812866211,
      "learning_rate": 8.027272336215426e-06,
      "loss": 1.8293,
      "step": 7169
    },
    {
      "epoch": 2.775842044134727,
      "grad_norm": 25.9862060546875,
      "learning_rate": 8.026842173183637e-06,
      "loss": 1.9124,
      "step": 7170
    },
    {
      "epoch": 2.7762291908633374,
      "grad_norm": 24.49104881286621,
      "learning_rate": 8.026412010151849e-06,
      "loss": 1.3394,
      "step": 7171
    },
    {
      "epoch": 2.7766163375919475,
      "grad_norm": 20.582948684692383,
      "learning_rate": 8.025981847120058e-06,
      "loss": 1.4466,
      "step": 7172
    },
    {
      "epoch": 2.7770034843205575,
      "grad_norm": 29.285728454589844,
      "learning_rate": 8.02555168408827e-06,
      "loss": 1.806,
      "step": 7173
    },
    {
      "epoch": 2.7773906310491676,
      "grad_norm": 15.187931060791016,
      "learning_rate": 8.025121521056481e-06,
      "loss": 1.4365,
      "step": 7174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 11.081944465637207,
      "learning_rate": 8.024691358024692e-06,
      "loss": 0.9475,
      "step": 7175
    },
    {
      "epoch": 2.778164924506388,
      "grad_norm": 12.630402565002441,
      "learning_rate": 8.024261194992902e-06,
      "loss": 1.2564,
      "step": 7176
    },
    {
      "epoch": 2.778552071234998,
      "grad_norm": 28.152185440063477,
      "learning_rate": 8.023831031961114e-06,
      "loss": 1.4673,
      "step": 7177
    },
    {
      "epoch": 2.7789392179636083,
      "grad_norm": 33.72768783569336,
      "learning_rate": 8.023400868929325e-06,
      "loss": 1.5926,
      "step": 7178
    },
    {
      "epoch": 2.7793263646922184,
      "grad_norm": 14.592655181884766,
      "learning_rate": 8.022970705897536e-06,
      "loss": 1.5317,
      "step": 7179
    },
    {
      "epoch": 2.7797135114208285,
      "grad_norm": 31.27548599243164,
      "learning_rate": 8.022540542865746e-06,
      "loss": 1.9829,
      "step": 7180
    },
    {
      "epoch": 2.7801006581494385,
      "grad_norm": 14.678866386413574,
      "learning_rate": 8.022110379833958e-06,
      "loss": 0.5816,
      "step": 7181
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 38.03860092163086,
      "learning_rate": 8.021680216802169e-06,
      "loss": 1.4832,
      "step": 7182
    },
    {
      "epoch": 2.780874951606659,
      "grad_norm": 49.43368148803711,
      "learning_rate": 8.02125005377038e-06,
      "loss": 3.0106,
      "step": 7183
    },
    {
      "epoch": 2.781262098335269,
      "grad_norm": 23.218029022216797,
      "learning_rate": 8.02081989073859e-06,
      "loss": 2.082,
      "step": 7184
    },
    {
      "epoch": 2.7816492450638792,
      "grad_norm": 17.508729934692383,
      "learning_rate": 8.020389727706802e-06,
      "loss": 0.9358,
      "step": 7185
    },
    {
      "epoch": 2.7820363917924893,
      "grad_norm": 25.326658248901367,
      "learning_rate": 8.019959564675013e-06,
      "loss": 2.1277,
      "step": 7186
    },
    {
      "epoch": 2.7824235385210994,
      "grad_norm": 63.521324157714844,
      "learning_rate": 8.019529401643223e-06,
      "loss": 1.442,
      "step": 7187
    },
    {
      "epoch": 2.78281068524971,
      "grad_norm": 12.094059944152832,
      "learning_rate": 8.019099238611434e-06,
      "loss": 0.7327,
      "step": 7188
    },
    {
      "epoch": 2.78319783197832,
      "grad_norm": 15.002274513244629,
      "learning_rate": 8.018669075579646e-06,
      "loss": 0.856,
      "step": 7189
    },
    {
      "epoch": 2.78358497870693,
      "grad_norm": 17.28476333618164,
      "learning_rate": 8.018238912547857e-06,
      "loss": 1.0244,
      "step": 7190
    },
    {
      "epoch": 2.78397212543554,
      "grad_norm": 42.87890625,
      "learning_rate": 8.017808749516067e-06,
      "loss": 1.1388,
      "step": 7191
    },
    {
      "epoch": 2.78435927216415,
      "grad_norm": 21.22772979736328,
      "learning_rate": 8.017378586484278e-06,
      "loss": 1.5031,
      "step": 7192
    },
    {
      "epoch": 2.78474641889276,
      "grad_norm": 27.626585006713867,
      "learning_rate": 8.01694842345249e-06,
      "loss": 1.3419,
      "step": 7193
    },
    {
      "epoch": 2.7851335656213703,
      "grad_norm": 19.79791831970215,
      "learning_rate": 8.016518260420701e-06,
      "loss": 1.8624,
      "step": 7194
    },
    {
      "epoch": 2.7855207123499808,
      "grad_norm": 30.731721878051758,
      "learning_rate": 8.01608809738891e-06,
      "loss": 1.91,
      "step": 7195
    },
    {
      "epoch": 2.785907859078591,
      "grad_norm": 15.063294410705566,
      "learning_rate": 8.015657934357122e-06,
      "loss": 1.3759,
      "step": 7196
    },
    {
      "epoch": 2.786295005807201,
      "grad_norm": 11.706151008605957,
      "learning_rate": 8.015227771325333e-06,
      "loss": 0.7343,
      "step": 7197
    },
    {
      "epoch": 2.786682152535811,
      "grad_norm": 65.98474884033203,
      "learning_rate": 8.014797608293545e-06,
      "loss": 2.6619,
      "step": 7198
    },
    {
      "epoch": 2.787069299264421,
      "grad_norm": 23.248340606689453,
      "learning_rate": 8.014367445261755e-06,
      "loss": 1.0453,
      "step": 7199
    },
    {
      "epoch": 2.7874564459930316,
      "grad_norm": 18.438508987426758,
      "learning_rate": 8.013937282229966e-06,
      "loss": 1.1633,
      "step": 7200
    },
    {
      "epoch": 2.7878435927216416,
      "grad_norm": 14.132196426391602,
      "learning_rate": 8.013507119198177e-06,
      "loss": 0.8454,
      "step": 7201
    },
    {
      "epoch": 2.7882307394502517,
      "grad_norm": 32.141761779785156,
      "learning_rate": 8.013076956166387e-06,
      "loss": 1.6137,
      "step": 7202
    },
    {
      "epoch": 2.7886178861788617,
      "grad_norm": 8.927716255187988,
      "learning_rate": 8.012646793134599e-06,
      "loss": 1.159,
      "step": 7203
    },
    {
      "epoch": 2.789005032907472,
      "grad_norm": 11.59013843536377,
      "learning_rate": 8.01221663010281e-06,
      "loss": 0.7692,
      "step": 7204
    },
    {
      "epoch": 2.7893921796360823,
      "grad_norm": 38.82855224609375,
      "learning_rate": 8.011786467071021e-06,
      "loss": 1.0909,
      "step": 7205
    },
    {
      "epoch": 2.789779326364692,
      "grad_norm": 19.517528533935547,
      "learning_rate": 8.011356304039231e-06,
      "loss": 0.9452,
      "step": 7206
    },
    {
      "epoch": 2.7901664730933025,
      "grad_norm": 22.725194931030273,
      "learning_rate": 8.010926141007443e-06,
      "loss": 0.6341,
      "step": 7207
    },
    {
      "epoch": 2.7905536198219125,
      "grad_norm": 29.739097595214844,
      "learning_rate": 8.010495977975652e-06,
      "loss": 1.3639,
      "step": 7208
    },
    {
      "epoch": 2.7909407665505226,
      "grad_norm": 17.68157386779785,
      "learning_rate": 8.010065814943865e-06,
      "loss": 1.1348,
      "step": 7209
    },
    {
      "epoch": 2.7913279132791327,
      "grad_norm": 33.010108947753906,
      "learning_rate": 8.009635651912075e-06,
      "loss": 1.3217,
      "step": 7210
    },
    {
      "epoch": 2.7917150600077427,
      "grad_norm": 11.510595321655273,
      "learning_rate": 8.009205488880287e-06,
      "loss": 0.4863,
      "step": 7211
    },
    {
      "epoch": 2.7921022067363532,
      "grad_norm": 15.064359664916992,
      "learning_rate": 8.008775325848496e-06,
      "loss": 0.7013,
      "step": 7212
    },
    {
      "epoch": 2.7924893534649633,
      "grad_norm": 26.140178680419922,
      "learning_rate": 8.00834516281671e-06,
      "loss": 1.4659,
      "step": 7213
    },
    {
      "epoch": 2.7928765001935734,
      "grad_norm": 18.414260864257812,
      "learning_rate": 8.007914999784919e-06,
      "loss": 2.8706,
      "step": 7214
    },
    {
      "epoch": 2.7932636469221834,
      "grad_norm": 22.81218719482422,
      "learning_rate": 8.00748483675313e-06,
      "loss": 1.6616,
      "step": 7215
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 15.013833045959473,
      "learning_rate": 8.00705467372134e-06,
      "loss": 1.3505,
      "step": 7216
    },
    {
      "epoch": 2.794037940379404,
      "grad_norm": 33.568241119384766,
      "learning_rate": 8.006624510689552e-06,
      "loss": 2.4556,
      "step": 7217
    },
    {
      "epoch": 2.794425087108014,
      "grad_norm": 15.504402160644531,
      "learning_rate": 8.006194347657763e-06,
      "loss": 1.1132,
      "step": 7218
    },
    {
      "epoch": 2.794812233836624,
      "grad_norm": 23.488828659057617,
      "learning_rate": 8.005764184625974e-06,
      "loss": 1.8503,
      "step": 7219
    },
    {
      "epoch": 2.795199380565234,
      "grad_norm": 26.648950576782227,
      "learning_rate": 8.005334021594184e-06,
      "loss": 2.0754,
      "step": 7220
    },
    {
      "epoch": 2.7955865272938443,
      "grad_norm": 14.075849533081055,
      "learning_rate": 8.004903858562396e-06,
      "loss": 0.822,
      "step": 7221
    },
    {
      "epoch": 2.795973674022455,
      "grad_norm": 12.697378158569336,
      "learning_rate": 8.004473695530607e-06,
      "loss": 0.9958,
      "step": 7222
    },
    {
      "epoch": 2.7963608207510644,
      "grad_norm": 14.436689376831055,
      "learning_rate": 8.004043532498817e-06,
      "loss": 1.288,
      "step": 7223
    },
    {
      "epoch": 2.796747967479675,
      "grad_norm": 21.41312599182129,
      "learning_rate": 8.003613369467028e-06,
      "loss": 1.5431,
      "step": 7224
    },
    {
      "epoch": 2.797135114208285,
      "grad_norm": 11.80262565612793,
      "learning_rate": 8.00318320643524e-06,
      "loss": 0.9826,
      "step": 7225
    },
    {
      "epoch": 2.797522260936895,
      "grad_norm": 13.525383949279785,
      "learning_rate": 8.002753043403451e-06,
      "loss": 0.8087,
      "step": 7226
    },
    {
      "epoch": 2.797909407665505,
      "grad_norm": 14.81161117553711,
      "learning_rate": 8.00232288037166e-06,
      "loss": 0.2155,
      "step": 7227
    },
    {
      "epoch": 2.798296554394115,
      "grad_norm": 19.789997100830078,
      "learning_rate": 8.001892717339872e-06,
      "loss": 1.0939,
      "step": 7228
    },
    {
      "epoch": 2.7986837011227257,
      "grad_norm": 40.92793273925781,
      "learning_rate": 8.001462554308084e-06,
      "loss": 1.0615,
      "step": 7229
    },
    {
      "epoch": 2.7990708478513358,
      "grad_norm": 7.3455376625061035,
      "learning_rate": 8.001032391276295e-06,
      "loss": 1.203,
      "step": 7230
    },
    {
      "epoch": 2.799457994579946,
      "grad_norm": 11.871594429016113,
      "learning_rate": 8.000602228244505e-06,
      "loss": 0.7215,
      "step": 7231
    },
    {
      "epoch": 2.799845141308556,
      "grad_norm": 13.90105152130127,
      "learning_rate": 8.000172065212716e-06,
      "loss": 0.7431,
      "step": 7232
    },
    {
      "epoch": 2.800232288037166,
      "grad_norm": 18.27199363708496,
      "learning_rate": 7.999741902180928e-06,
      "loss": 0.7634,
      "step": 7233
    },
    {
      "epoch": 2.8006194347657765,
      "grad_norm": 23.26997184753418,
      "learning_rate": 7.999311739149139e-06,
      "loss": 0.9187,
      "step": 7234
    },
    {
      "epoch": 2.8010065814943865,
      "grad_norm": 27.62308692932129,
      "learning_rate": 7.998881576117349e-06,
      "loss": 1.4652,
      "step": 7235
    },
    {
      "epoch": 2.8013937282229966,
      "grad_norm": 35.949867248535156,
      "learning_rate": 7.99845141308556e-06,
      "loss": 3.2867,
      "step": 7236
    },
    {
      "epoch": 2.8017808749516067,
      "grad_norm": 54.09250259399414,
      "learning_rate": 7.998021250053771e-06,
      "loss": 2.0937,
      "step": 7237
    },
    {
      "epoch": 2.8021680216802167,
      "grad_norm": 13.488533973693848,
      "learning_rate": 7.997591087021981e-06,
      "loss": 0.7748,
      "step": 7238
    },
    {
      "epoch": 2.802555168408827,
      "grad_norm": 30.609251022338867,
      "learning_rate": 7.997160923990193e-06,
      "loss": 2.0426,
      "step": 7239
    },
    {
      "epoch": 2.802942315137437,
      "grad_norm": 25.43604278564453,
      "learning_rate": 7.996730760958404e-06,
      "loss": 2.0942,
      "step": 7240
    },
    {
      "epoch": 2.8033294618660474,
      "grad_norm": 12.456792831420898,
      "learning_rate": 7.996300597926615e-06,
      "loss": 0.7317,
      "step": 7241
    },
    {
      "epoch": 2.8037166085946574,
      "grad_norm": 4.809971809387207,
      "learning_rate": 7.995870434894825e-06,
      "loss": 0.1457,
      "step": 7242
    },
    {
      "epoch": 2.8041037553232675,
      "grad_norm": 19.653186798095703,
      "learning_rate": 7.995440271863037e-06,
      "loss": 1.1161,
      "step": 7243
    },
    {
      "epoch": 2.8044909020518776,
      "grad_norm": 45.79845428466797,
      "learning_rate": 7.995010108831248e-06,
      "loss": 0.545,
      "step": 7244
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 18.34200668334961,
      "learning_rate": 7.99457994579946e-06,
      "loss": 1.2,
      "step": 7245
    },
    {
      "epoch": 2.805265195509098,
      "grad_norm": 16.640336990356445,
      "learning_rate": 7.994149782767669e-06,
      "loss": 1.0005,
      "step": 7246
    },
    {
      "epoch": 2.805652342237708,
      "grad_norm": 72.86869812011719,
      "learning_rate": 7.99371961973588e-06,
      "loss": 1.9046,
      "step": 7247
    },
    {
      "epoch": 2.8060394889663183,
      "grad_norm": 52.07019805908203,
      "learning_rate": 7.993289456704092e-06,
      "loss": 2.6172,
      "step": 7248
    },
    {
      "epoch": 2.8064266356949283,
      "grad_norm": 33.383399963378906,
      "learning_rate": 7.992859293672303e-06,
      "loss": 1.6598,
      "step": 7249
    },
    {
      "epoch": 2.8068137824235384,
      "grad_norm": 12.770208358764648,
      "learning_rate": 7.992429130640513e-06,
      "loss": 0.5425,
      "step": 7250
    },
    {
      "epoch": 2.807200929152149,
      "grad_norm": 23.538270950317383,
      "learning_rate": 7.991998967608725e-06,
      "loss": 0.5046,
      "step": 7251
    },
    {
      "epoch": 2.8075880758807585,
      "grad_norm": 13.078351020812988,
      "learning_rate": 7.991568804576936e-06,
      "loss": 0.8908,
      "step": 7252
    },
    {
      "epoch": 2.807975222609369,
      "grad_norm": 26.69074249267578,
      "learning_rate": 7.991138641545146e-06,
      "loss": 1.8102,
      "step": 7253
    },
    {
      "epoch": 2.808362369337979,
      "grad_norm": 28.34469985961914,
      "learning_rate": 7.990708478513357e-06,
      "loss": 2.8553,
      "step": 7254
    },
    {
      "epoch": 2.808749516066589,
      "grad_norm": 33.43528366088867,
      "learning_rate": 7.990278315481568e-06,
      "loss": 1.2697,
      "step": 7255
    },
    {
      "epoch": 2.8091366627951992,
      "grad_norm": 18.134544372558594,
      "learning_rate": 7.98984815244978e-06,
      "loss": 1.1915,
      "step": 7256
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 58.62785720825195,
      "learning_rate": 7.98941798941799e-06,
      "loss": 2.6464,
      "step": 7257
    },
    {
      "epoch": 2.80991095625242,
      "grad_norm": 42.17241668701172,
      "learning_rate": 7.988987826386201e-06,
      "loss": 2.3999,
      "step": 7258
    },
    {
      "epoch": 2.81029810298103,
      "grad_norm": 4.987955570220947,
      "learning_rate": 7.98855766335441e-06,
      "loss": 0.165,
      "step": 7259
    },
    {
      "epoch": 2.81068524970964,
      "grad_norm": 8.574647903442383,
      "learning_rate": 7.988127500322624e-06,
      "loss": 0.2426,
      "step": 7260
    },
    {
      "epoch": 2.81107239643825,
      "grad_norm": 29.910539627075195,
      "learning_rate": 7.987697337290834e-06,
      "loss": 0.8961,
      "step": 7261
    },
    {
      "epoch": 2.81145954316686,
      "grad_norm": 5.495007514953613,
      "learning_rate": 7.987267174259045e-06,
      "loss": 0.326,
      "step": 7262
    },
    {
      "epoch": 2.8118466898954706,
      "grad_norm": 22.82708168029785,
      "learning_rate": 7.986837011227255e-06,
      "loss": 0.8027,
      "step": 7263
    },
    {
      "epoch": 2.8122338366240807,
      "grad_norm": 15.276567459106445,
      "learning_rate": 7.986406848195468e-06,
      "loss": 1.0887,
      "step": 7264
    },
    {
      "epoch": 2.8126209833526907,
      "grad_norm": 31.29368782043457,
      "learning_rate": 7.985976685163678e-06,
      "loss": 1.7122,
      "step": 7265
    },
    {
      "epoch": 2.813008130081301,
      "grad_norm": 23.942689895629883,
      "learning_rate": 7.985546522131889e-06,
      "loss": 1.7479,
      "step": 7266
    },
    {
      "epoch": 2.813395276809911,
      "grad_norm": 30.341882705688477,
      "learning_rate": 7.985116359100099e-06,
      "loss": 0.9557,
      "step": 7267
    },
    {
      "epoch": 2.8137824235385214,
      "grad_norm": 23.846067428588867,
      "learning_rate": 7.98468619606831e-06,
      "loss": 1.0913,
      "step": 7268
    },
    {
      "epoch": 2.814169570267131,
      "grad_norm": 18.31607437133789,
      "learning_rate": 7.984256033036522e-06,
      "loss": 1.0976,
      "step": 7269
    },
    {
      "epoch": 2.8145567169957415,
      "grad_norm": 25.8137149810791,
      "learning_rate": 7.983825870004733e-06,
      "loss": 1.3217,
      "step": 7270
    },
    {
      "epoch": 2.8149438637243516,
      "grad_norm": 40.195613861083984,
      "learning_rate": 7.983395706972943e-06,
      "loss": 1.3901,
      "step": 7271
    },
    {
      "epoch": 2.8153310104529616,
      "grad_norm": 41.45471954345703,
      "learning_rate": 7.982965543941154e-06,
      "loss": 2.3439,
      "step": 7272
    },
    {
      "epoch": 2.8157181571815717,
      "grad_norm": 58.269775390625,
      "learning_rate": 7.982535380909365e-06,
      "loss": 2.0952,
      "step": 7273
    },
    {
      "epoch": 2.8161053039101818,
      "grad_norm": 20.16917610168457,
      "learning_rate": 7.982105217877575e-06,
      "loss": 1.0476,
      "step": 7274
    },
    {
      "epoch": 2.8164924506387923,
      "grad_norm": 21.882661819458008,
      "learning_rate": 7.981675054845788e-06,
      "loss": 1.6284,
      "step": 7275
    },
    {
      "epoch": 2.8168795973674023,
      "grad_norm": 13.810257911682129,
      "learning_rate": 7.981244891813998e-06,
      "loss": 0.8112,
      "step": 7276
    },
    {
      "epoch": 2.8172667440960124,
      "grad_norm": 26.084716796875,
      "learning_rate": 7.98081472878221e-06,
      "loss": 1.7496,
      "step": 7277
    },
    {
      "epoch": 2.8176538908246225,
      "grad_norm": 30.62770652770996,
      "learning_rate": 7.98038456575042e-06,
      "loss": 1.9541,
      "step": 7278
    },
    {
      "epoch": 2.8180410375532325,
      "grad_norm": 38.710350036621094,
      "learning_rate": 7.979954402718632e-06,
      "loss": 2.1443,
      "step": 7279
    },
    {
      "epoch": 2.818428184281843,
      "grad_norm": 22.315025329589844,
      "learning_rate": 7.979524239686842e-06,
      "loss": 2.6736,
      "step": 7280
    },
    {
      "epoch": 2.818815331010453,
      "grad_norm": 19.920869827270508,
      "learning_rate": 7.979094076655053e-06,
      "loss": 1.7932,
      "step": 7281
    },
    {
      "epoch": 2.819202477739063,
      "grad_norm": 20.036724090576172,
      "learning_rate": 7.978663913623263e-06,
      "loss": 1.1852,
      "step": 7282
    },
    {
      "epoch": 2.8195896244676733,
      "grad_norm": 10.070487976074219,
      "learning_rate": 7.978233750591475e-06,
      "loss": 0.4707,
      "step": 7283
    },
    {
      "epoch": 2.8199767711962833,
      "grad_norm": 11.844327926635742,
      "learning_rate": 7.977803587559686e-06,
      "loss": 0.6673,
      "step": 7284
    },
    {
      "epoch": 2.8203639179248934,
      "grad_norm": 17.553237915039062,
      "learning_rate": 7.977373424527897e-06,
      "loss": 1.73,
      "step": 7285
    },
    {
      "epoch": 2.8207510646535034,
      "grad_norm": 14.360601425170898,
      "learning_rate": 7.976943261496107e-06,
      "loss": 0.9183,
      "step": 7286
    },
    {
      "epoch": 2.821138211382114,
      "grad_norm": 23.00815200805664,
      "learning_rate": 7.976513098464319e-06,
      "loss": 1.5004,
      "step": 7287
    },
    {
      "epoch": 2.821525358110724,
      "grad_norm": 26.29194450378418,
      "learning_rate": 7.97608293543253e-06,
      "loss": 2.1203,
      "step": 7288
    },
    {
      "epoch": 2.821912504839334,
      "grad_norm": 32.35588836669922,
      "learning_rate": 7.97565277240074e-06,
      "loss": 1.3585,
      "step": 7289
    },
    {
      "epoch": 2.822299651567944,
      "grad_norm": 73.02574920654297,
      "learning_rate": 7.975222609368951e-06,
      "loss": 2.8841,
      "step": 7290
    },
    {
      "epoch": 2.8226867982965542,
      "grad_norm": 21.333065032958984,
      "learning_rate": 7.974792446337163e-06,
      "loss": 2.5263,
      "step": 7291
    },
    {
      "epoch": 2.8230739450251647,
      "grad_norm": 23.467432022094727,
      "learning_rate": 7.974362283305374e-06,
      "loss": 1.143,
      "step": 7292
    },
    {
      "epoch": 2.823461091753775,
      "grad_norm": 26.84126853942871,
      "learning_rate": 7.973932120273584e-06,
      "loss": 1.4877,
      "step": 7293
    },
    {
      "epoch": 2.823848238482385,
      "grad_norm": 15.027612686157227,
      "learning_rate": 7.973501957241795e-06,
      "loss": 0.7315,
      "step": 7294
    },
    {
      "epoch": 2.824235385210995,
      "grad_norm": 20.567485809326172,
      "learning_rate": 7.973071794210006e-06,
      "loss": 0.7983,
      "step": 7295
    },
    {
      "epoch": 2.824622531939605,
      "grad_norm": 28.12624168395996,
      "learning_rate": 7.972641631178218e-06,
      "loss": 1.6336,
      "step": 7296
    },
    {
      "epoch": 2.8250096786682155,
      "grad_norm": 27.059345245361328,
      "learning_rate": 7.972211468146428e-06,
      "loss": 1.0937,
      "step": 7297
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 34.68313980102539,
      "learning_rate": 7.971781305114639e-06,
      "loss": 1.4825,
      "step": 7298
    },
    {
      "epoch": 2.8257839721254356,
      "grad_norm": 11.600642204284668,
      "learning_rate": 7.97135114208285e-06,
      "loss": 0.6906,
      "step": 7299
    },
    {
      "epoch": 2.8261711188540457,
      "grad_norm": 8.12117862701416,
      "learning_rate": 7.970920979051062e-06,
      "loss": 0.2503,
      "step": 7300
    },
    {
      "epoch": 2.8265582655826558,
      "grad_norm": 16.63116455078125,
      "learning_rate": 7.970490816019272e-06,
      "loss": 1.6161,
      "step": 7301
    },
    {
      "epoch": 2.826945412311266,
      "grad_norm": 24.003128051757812,
      "learning_rate": 7.970060652987483e-06,
      "loss": 2.2227,
      "step": 7302
    },
    {
      "epoch": 2.827332559039876,
      "grad_norm": 15.413222312927246,
      "learning_rate": 7.969630489955694e-06,
      "loss": 1.3622,
      "step": 7303
    },
    {
      "epoch": 2.8277197057684864,
      "grad_norm": 24.63412094116211,
      "learning_rate": 7.969200326923904e-06,
      "loss": 1.68,
      "step": 7304
    },
    {
      "epoch": 2.8281068524970965,
      "grad_norm": 46.68855667114258,
      "learning_rate": 7.968770163892116e-06,
      "loss": 1.3491,
      "step": 7305
    },
    {
      "epoch": 2.8284939992257065,
      "grad_norm": 9.615629196166992,
      "learning_rate": 7.968340000860327e-06,
      "loss": 0.736,
      "step": 7306
    },
    {
      "epoch": 2.8288811459543166,
      "grad_norm": 36.49643325805664,
      "learning_rate": 7.967909837828538e-06,
      "loss": 1.7084,
      "step": 7307
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 42.00132369995117,
      "learning_rate": 7.967479674796748e-06,
      "loss": 1.4709,
      "step": 7308
    },
    {
      "epoch": 2.829655439411537,
      "grad_norm": 16.106550216674805,
      "learning_rate": 7.96704951176496e-06,
      "loss": 1.3671,
      "step": 7309
    },
    {
      "epoch": 2.8300425861401473,
      "grad_norm": 22.943349838256836,
      "learning_rate": 7.96661934873317e-06,
      "loss": 1.2843,
      "step": 7310
    },
    {
      "epoch": 2.8304297328687573,
      "grad_norm": 19.337045669555664,
      "learning_rate": 7.966189185701382e-06,
      "loss": 2.2028,
      "step": 7311
    },
    {
      "epoch": 2.8308168795973674,
      "grad_norm": 23.67597198486328,
      "learning_rate": 7.965759022669592e-06,
      "loss": 0.6833,
      "step": 7312
    },
    {
      "epoch": 2.8312040263259775,
      "grad_norm": 17.888206481933594,
      "learning_rate": 7.965328859637803e-06,
      "loss": 0.9719,
      "step": 7313
    },
    {
      "epoch": 2.831591173054588,
      "grad_norm": 13.110998153686523,
      "learning_rate": 7.964898696606013e-06,
      "loss": 1.2226,
      "step": 7314
    },
    {
      "epoch": 2.8319783197831976,
      "grad_norm": 20.212627410888672,
      "learning_rate": 7.964468533574226e-06,
      "loss": 0.8146,
      "step": 7315
    },
    {
      "epoch": 2.832365466511808,
      "grad_norm": 17.160293579101562,
      "learning_rate": 7.964038370542436e-06,
      "loss": 1.4415,
      "step": 7316
    },
    {
      "epoch": 2.832752613240418,
      "grad_norm": 47.53095626831055,
      "learning_rate": 7.963608207510647e-06,
      "loss": 0.8704,
      "step": 7317
    },
    {
      "epoch": 2.8331397599690282,
      "grad_norm": 32.77962875366211,
      "learning_rate": 7.963178044478859e-06,
      "loss": 0.9018,
      "step": 7318
    },
    {
      "epoch": 2.8335269066976383,
      "grad_norm": 6.311821460723877,
      "learning_rate": 7.962747881447069e-06,
      "loss": 0.3148,
      "step": 7319
    },
    {
      "epoch": 2.8339140534262484,
      "grad_norm": 21.609590530395508,
      "learning_rate": 7.96231771841528e-06,
      "loss": 1.1765,
      "step": 7320
    },
    {
      "epoch": 2.834301200154859,
      "grad_norm": 37.167972564697266,
      "learning_rate": 7.961887555383491e-06,
      "loss": 3.9607,
      "step": 7321
    },
    {
      "epoch": 2.834688346883469,
      "grad_norm": 43.89338302612305,
      "learning_rate": 7.961457392351703e-06,
      "loss": 1.6862,
      "step": 7322
    },
    {
      "epoch": 2.835075493612079,
      "grad_norm": 11.455789566040039,
      "learning_rate": 7.961027229319913e-06,
      "loss": 1.2789,
      "step": 7323
    },
    {
      "epoch": 2.835462640340689,
      "grad_norm": 21.04143714904785,
      "learning_rate": 7.960597066288124e-06,
      "loss": 1.7572,
      "step": 7324
    },
    {
      "epoch": 2.835849787069299,
      "grad_norm": 39.654296875,
      "learning_rate": 7.960166903256334e-06,
      "loss": 1.5451,
      "step": 7325
    },
    {
      "epoch": 2.8362369337979096,
      "grad_norm": 11.877128601074219,
      "learning_rate": 7.959736740224547e-06,
      "loss": 0.657,
      "step": 7326
    },
    {
      "epoch": 2.8366240805265197,
      "grad_norm": 68.37645721435547,
      "learning_rate": 7.959306577192757e-06,
      "loss": 1.7501,
      "step": 7327
    },
    {
      "epoch": 2.8370112272551298,
      "grad_norm": 22.803016662597656,
      "learning_rate": 7.958876414160968e-06,
      "loss": 1.2708,
      "step": 7328
    },
    {
      "epoch": 2.83739837398374,
      "grad_norm": 13.10368824005127,
      "learning_rate": 7.958446251129178e-06,
      "loss": 0.8295,
      "step": 7329
    },
    {
      "epoch": 2.83778552071235,
      "grad_norm": 18.930757522583008,
      "learning_rate": 7.95801608809739e-06,
      "loss": 1.1212,
      "step": 7330
    },
    {
      "epoch": 2.83817266744096,
      "grad_norm": 27.44232177734375,
      "learning_rate": 7.9575859250656e-06,
      "loss": 1.8003,
      "step": 7331
    },
    {
      "epoch": 2.83855981416957,
      "grad_norm": 36.77300262451172,
      "learning_rate": 7.957155762033812e-06,
      "loss": 1.4271,
      "step": 7332
    },
    {
      "epoch": 2.8389469608981805,
      "grad_norm": 19.549407958984375,
      "learning_rate": 7.956725599002022e-06,
      "loss": 1.5669,
      "step": 7333
    },
    {
      "epoch": 2.8393341076267906,
      "grad_norm": 16.098052978515625,
      "learning_rate": 7.956295435970233e-06,
      "loss": 0.6847,
      "step": 7334
    },
    {
      "epoch": 2.8397212543554007,
      "grad_norm": 76.73174285888672,
      "learning_rate": 7.955865272938444e-06,
      "loss": 1.9789,
      "step": 7335
    },
    {
      "epoch": 2.8401084010840107,
      "grad_norm": 13.514798164367676,
      "learning_rate": 7.955435109906656e-06,
      "loss": 0.917,
      "step": 7336
    },
    {
      "epoch": 2.840495547812621,
      "grad_norm": 14.754523277282715,
      "learning_rate": 7.955004946874866e-06,
      "loss": 0.6428,
      "step": 7337
    },
    {
      "epoch": 2.8408826945412313,
      "grad_norm": 51.75570297241211,
      "learning_rate": 7.954574783843077e-06,
      "loss": 2.0135,
      "step": 7338
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 21.160247802734375,
      "learning_rate": 7.954144620811288e-06,
      "loss": 1.1227,
      "step": 7339
    },
    {
      "epoch": 2.8416569879984515,
      "grad_norm": 15.362080574035645,
      "learning_rate": 7.953714457779498e-06,
      "loss": 1.4341,
      "step": 7340
    },
    {
      "epoch": 2.8420441347270615,
      "grad_norm": 32.92377471923828,
      "learning_rate": 7.95328429474771e-06,
      "loss": 1.2125,
      "step": 7341
    },
    {
      "epoch": 2.8424312814556716,
      "grad_norm": 20.850942611694336,
      "learning_rate": 7.952854131715921e-06,
      "loss": 1.2879,
      "step": 7342
    },
    {
      "epoch": 2.842818428184282,
      "grad_norm": 8.743510246276855,
      "learning_rate": 7.952423968684132e-06,
      "loss": 0.639,
      "step": 7343
    },
    {
      "epoch": 2.8432055749128917,
      "grad_norm": 20.826658248901367,
      "learning_rate": 7.951993805652342e-06,
      "loss": 1.3635,
      "step": 7344
    },
    {
      "epoch": 2.8435927216415022,
      "grad_norm": 43.740291595458984,
      "learning_rate": 7.951563642620554e-06,
      "loss": 0.8835,
      "step": 7345
    },
    {
      "epoch": 2.8439798683701123,
      "grad_norm": 47.284217834472656,
      "learning_rate": 7.951133479588765e-06,
      "loss": 0.5163,
      "step": 7346
    },
    {
      "epoch": 2.8443670150987224,
      "grad_norm": 11.178693771362305,
      "learning_rate": 7.950703316556976e-06,
      "loss": 0.8099,
      "step": 7347
    },
    {
      "epoch": 2.8447541618273324,
      "grad_norm": 53.25722885131836,
      "learning_rate": 7.950273153525186e-06,
      "loss": 1.4067,
      "step": 7348
    },
    {
      "epoch": 2.8451413085559425,
      "grad_norm": 16.187124252319336,
      "learning_rate": 7.949842990493398e-06,
      "loss": 1.0214,
      "step": 7349
    },
    {
      "epoch": 2.845528455284553,
      "grad_norm": 28.30167579650879,
      "learning_rate": 7.949412827461609e-06,
      "loss": 1.7177,
      "step": 7350
    },
    {
      "epoch": 2.845915602013163,
      "grad_norm": 16.70755958557129,
      "learning_rate": 7.94898266442982e-06,
      "loss": 1.8511,
      "step": 7351
    },
    {
      "epoch": 2.846302748741773,
      "grad_norm": 25.79487419128418,
      "learning_rate": 7.94855250139803e-06,
      "loss": 1.5835,
      "step": 7352
    },
    {
      "epoch": 2.846689895470383,
      "grad_norm": 34.44961929321289,
      "learning_rate": 7.948122338366241e-06,
      "loss": 1.3629,
      "step": 7353
    },
    {
      "epoch": 2.8470770421989933,
      "grad_norm": 11.93077564239502,
      "learning_rate": 7.947692175334453e-06,
      "loss": 1.5904,
      "step": 7354
    },
    {
      "epoch": 2.847464188927604,
      "grad_norm": 26.051830291748047,
      "learning_rate": 7.947262012302663e-06,
      "loss": 1.8483,
      "step": 7355
    },
    {
      "epoch": 2.847851335656214,
      "grad_norm": 32.99980926513672,
      "learning_rate": 7.946831849270874e-06,
      "loss": 2.1225,
      "step": 7356
    },
    {
      "epoch": 2.848238482384824,
      "grad_norm": 16.938135147094727,
      "learning_rate": 7.946401686239085e-06,
      "loss": 1.2421,
      "step": 7357
    },
    {
      "epoch": 2.848625629113434,
      "grad_norm": 12.457690238952637,
      "learning_rate": 7.945971523207297e-06,
      "loss": 1.1269,
      "step": 7358
    },
    {
      "epoch": 2.849012775842044,
      "grad_norm": 44.33602523803711,
      "learning_rate": 7.945541360175507e-06,
      "loss": 1.9206,
      "step": 7359
    },
    {
      "epoch": 2.8493999225706546,
      "grad_norm": 16.42288589477539,
      "learning_rate": 7.945111197143718e-06,
      "loss": 0.8041,
      "step": 7360
    },
    {
      "epoch": 2.849787069299264,
      "grad_norm": 22.975231170654297,
      "learning_rate": 7.94468103411193e-06,
      "loss": 1.2554,
      "step": 7361
    },
    {
      "epoch": 2.8501742160278747,
      "grad_norm": 27.70033836364746,
      "learning_rate": 7.94425087108014e-06,
      "loss": 1.3021,
      "step": 7362
    },
    {
      "epoch": 2.8505613627564848,
      "grad_norm": 15.168691635131836,
      "learning_rate": 7.94382070804835e-06,
      "loss": 1.2798,
      "step": 7363
    },
    {
      "epoch": 2.850948509485095,
      "grad_norm": 60.63220977783203,
      "learning_rate": 7.943390545016562e-06,
      "loss": 1.396,
      "step": 7364
    },
    {
      "epoch": 2.851335656213705,
      "grad_norm": 22.904258728027344,
      "learning_rate": 7.942960381984773e-06,
      "loss": 0.9398,
      "step": 7365
    },
    {
      "epoch": 2.851722802942315,
      "grad_norm": 15.262042999267578,
      "learning_rate": 7.942530218952985e-06,
      "loss": 1.0912,
      "step": 7366
    },
    {
      "epoch": 2.8521099496709255,
      "grad_norm": 31.745758056640625,
      "learning_rate": 7.942100055921195e-06,
      "loss": 0.8315,
      "step": 7367
    },
    {
      "epoch": 2.8524970963995355,
      "grad_norm": 27.255273818969727,
      "learning_rate": 7.941669892889406e-06,
      "loss": 2.0615,
      "step": 7368
    },
    {
      "epoch": 2.8528842431281456,
      "grad_norm": 44.15190505981445,
      "learning_rate": 7.941239729857617e-06,
      "loss": 1.7414,
      "step": 7369
    },
    {
      "epoch": 2.8532713898567557,
      "grad_norm": 10.931353569030762,
      "learning_rate": 7.940809566825827e-06,
      "loss": 0.5687,
      "step": 7370
    },
    {
      "epoch": 2.8536585365853657,
      "grad_norm": 35.64372253417969,
      "learning_rate": 7.940379403794039e-06,
      "loss": 2.1466,
      "step": 7371
    },
    {
      "epoch": 2.8540456833139762,
      "grad_norm": 66.4782943725586,
      "learning_rate": 7.93994924076225e-06,
      "loss": 1.7962,
      "step": 7372
    },
    {
      "epoch": 2.8544328300425863,
      "grad_norm": 12.031343460083008,
      "learning_rate": 7.939519077730461e-06,
      "loss": 0.7771,
      "step": 7373
    },
    {
      "epoch": 2.8548199767711964,
      "grad_norm": 113.78084564208984,
      "learning_rate": 7.939088914698671e-06,
      "loss": 1.2786,
      "step": 7374
    },
    {
      "epoch": 2.8552071234998064,
      "grad_norm": 13.98550796508789,
      "learning_rate": 7.938658751666882e-06,
      "loss": 1.03,
      "step": 7375
    },
    {
      "epoch": 2.8555942702284165,
      "grad_norm": 3.905940055847168,
      "learning_rate": 7.938228588635092e-06,
      "loss": 0.1158,
      "step": 7376
    },
    {
      "epoch": 2.8559814169570266,
      "grad_norm": 19.057296752929688,
      "learning_rate": 7.937798425603305e-06,
      "loss": 0.9487,
      "step": 7377
    },
    {
      "epoch": 2.8563685636856366,
      "grad_norm": 27.44889259338379,
      "learning_rate": 7.937368262571515e-06,
      "loss": 1.5425,
      "step": 7378
    },
    {
      "epoch": 2.856755710414247,
      "grad_norm": 13.65168285369873,
      "learning_rate": 7.936938099539726e-06,
      "loss": 0.8249,
      "step": 7379
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 12.1820707321167,
      "learning_rate": 7.936507936507936e-06,
      "loss": 0.6575,
      "step": 7380
    },
    {
      "epoch": 2.8575300038714673,
      "grad_norm": 12.551448822021484,
      "learning_rate": 7.93607777347615e-06,
      "loss": 0.9255,
      "step": 7381
    },
    {
      "epoch": 2.8579171506000773,
      "grad_norm": 19.2925968170166,
      "learning_rate": 7.935647610444359e-06,
      "loss": 2.1755,
      "step": 7382
    },
    {
      "epoch": 2.8583042973286874,
      "grad_norm": 19.057514190673828,
      "learning_rate": 7.93521744741257e-06,
      "loss": 1.3809,
      "step": 7383
    },
    {
      "epoch": 2.858691444057298,
      "grad_norm": 13.677562713623047,
      "learning_rate": 7.93478728438078e-06,
      "loss": 0.7541,
      "step": 7384
    },
    {
      "epoch": 2.859078590785908,
      "grad_norm": 23.285106658935547,
      "learning_rate": 7.934357121348992e-06,
      "loss": 0.8944,
      "step": 7385
    },
    {
      "epoch": 2.859465737514518,
      "grad_norm": 31.59455680847168,
      "learning_rate": 7.933926958317203e-06,
      "loss": 1.4082,
      "step": 7386
    },
    {
      "epoch": 2.859852884243128,
      "grad_norm": 38.13277816772461,
      "learning_rate": 7.933496795285414e-06,
      "loss": 2.9918,
      "step": 7387
    },
    {
      "epoch": 2.860240030971738,
      "grad_norm": 29.593042373657227,
      "learning_rate": 7.933066632253624e-06,
      "loss": 1.4079,
      "step": 7388
    },
    {
      "epoch": 2.8606271777003487,
      "grad_norm": 71.35503387451172,
      "learning_rate": 7.932636469221836e-06,
      "loss": 1.2536,
      "step": 7389
    },
    {
      "epoch": 2.8610143244289583,
      "grad_norm": 24.359895706176758,
      "learning_rate": 7.932206306190047e-06,
      "loss": 1.4949,
      "step": 7390
    },
    {
      "epoch": 2.861401471157569,
      "grad_norm": 21.383909225463867,
      "learning_rate": 7.931776143158257e-06,
      "loss": 0.8609,
      "step": 7391
    },
    {
      "epoch": 2.861788617886179,
      "grad_norm": 11.038924217224121,
      "learning_rate": 7.931345980126468e-06,
      "loss": 0.74,
      "step": 7392
    },
    {
      "epoch": 2.862175764614789,
      "grad_norm": 21.58751106262207,
      "learning_rate": 7.93091581709468e-06,
      "loss": 1.4239,
      "step": 7393
    },
    {
      "epoch": 2.862562911343399,
      "grad_norm": 34.58320999145508,
      "learning_rate": 7.930485654062891e-06,
      "loss": 1.5442,
      "step": 7394
    },
    {
      "epoch": 2.862950058072009,
      "grad_norm": 39.736412048339844,
      "learning_rate": 7.9300554910311e-06,
      "loss": 1.4517,
      "step": 7395
    },
    {
      "epoch": 2.8633372048006196,
      "grad_norm": 20.094406127929688,
      "learning_rate": 7.929625327999312e-06,
      "loss": 1.6025,
      "step": 7396
    },
    {
      "epoch": 2.8637243515292297,
      "grad_norm": 27.895315170288086,
      "learning_rate": 7.929195164967523e-06,
      "loss": 1.7698,
      "step": 7397
    },
    {
      "epoch": 2.8641114982578397,
      "grad_norm": 39.4733772277832,
      "learning_rate": 7.928765001935735e-06,
      "loss": 1.3241,
      "step": 7398
    },
    {
      "epoch": 2.86449864498645,
      "grad_norm": 19.78913116455078,
      "learning_rate": 7.928334838903945e-06,
      "loss": 2.2696,
      "step": 7399
    },
    {
      "epoch": 2.86488579171506,
      "grad_norm": 30.919567108154297,
      "learning_rate": 7.927904675872156e-06,
      "loss": 1.4873,
      "step": 7400
    },
    {
      "epoch": 2.8652729384436704,
      "grad_norm": 17.469507217407227,
      "learning_rate": 7.927474512840367e-06,
      "loss": 1.1905,
      "step": 7401
    },
    {
      "epoch": 2.8656600851722804,
      "grad_norm": 17.164142608642578,
      "learning_rate": 7.927044349808579e-06,
      "loss": 0.7247,
      "step": 7402
    },
    {
      "epoch": 2.8660472319008905,
      "grad_norm": 30.49341583251953,
      "learning_rate": 7.926614186776789e-06,
      "loss": 1.5261,
      "step": 7403
    },
    {
      "epoch": 2.8664343786295006,
      "grad_norm": 32.301265716552734,
      "learning_rate": 7.926184023745e-06,
      "loss": 0.6774,
      "step": 7404
    },
    {
      "epoch": 2.8668215253581106,
      "grad_norm": 4.295449733734131,
      "learning_rate": 7.925753860713211e-06,
      "loss": 0.1325,
      "step": 7405
    },
    {
      "epoch": 2.867208672086721,
      "grad_norm": 20.785709381103516,
      "learning_rate": 7.925323697681421e-06,
      "loss": 1.2267,
      "step": 7406
    },
    {
      "epoch": 2.8675958188153308,
      "grad_norm": 57.09174728393555,
      "learning_rate": 7.924893534649633e-06,
      "loss": 1.8025,
      "step": 7407
    },
    {
      "epoch": 2.8679829655439413,
      "grad_norm": 10.61821460723877,
      "learning_rate": 7.924463371617844e-06,
      "loss": 0.4868,
      "step": 7408
    },
    {
      "epoch": 2.8683701122725513,
      "grad_norm": 21.172927856445312,
      "learning_rate": 7.924033208586055e-06,
      "loss": 1.1719,
      "step": 7409
    },
    {
      "epoch": 2.8687572590011614,
      "grad_norm": 22.824182510375977,
      "learning_rate": 7.923603045554265e-06,
      "loss": 1.5039,
      "step": 7410
    },
    {
      "epoch": 2.8691444057297715,
      "grad_norm": 53.78929901123047,
      "learning_rate": 7.923172882522476e-06,
      "loss": 4.4524,
      "step": 7411
    },
    {
      "epoch": 2.8695315524583815,
      "grad_norm": 45.388240814208984,
      "learning_rate": 7.922742719490688e-06,
      "loss": 2.6788,
      "step": 7412
    },
    {
      "epoch": 2.869918699186992,
      "grad_norm": 21.82183265686035,
      "learning_rate": 7.9223125564589e-06,
      "loss": 1.3299,
      "step": 7413
    },
    {
      "epoch": 2.870305845915602,
      "grad_norm": 12.137054443359375,
      "learning_rate": 7.921882393427109e-06,
      "loss": 0.5258,
      "step": 7414
    },
    {
      "epoch": 2.870692992644212,
      "grad_norm": 29.033794403076172,
      "learning_rate": 7.92145223039532e-06,
      "loss": 3.1342,
      "step": 7415
    },
    {
      "epoch": 2.8710801393728222,
      "grad_norm": 17.48978042602539,
      "learning_rate": 7.921022067363532e-06,
      "loss": 1.3646,
      "step": 7416
    },
    {
      "epoch": 2.8714672861014323,
      "grad_norm": 41.15259552001953,
      "learning_rate": 7.920591904331743e-06,
      "loss": 2.6473,
      "step": 7417
    },
    {
      "epoch": 2.871854432830043,
      "grad_norm": 29.741987228393555,
      "learning_rate": 7.920161741299953e-06,
      "loss": 0.9697,
      "step": 7418
    },
    {
      "epoch": 2.872241579558653,
      "grad_norm": 25.65536880493164,
      "learning_rate": 7.919731578268164e-06,
      "loss": 2.5189,
      "step": 7419
    },
    {
      "epoch": 2.872628726287263,
      "grad_norm": 13.721902847290039,
      "learning_rate": 7.919301415236376e-06,
      "loss": 1.3695,
      "step": 7420
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 44.75494384765625,
      "learning_rate": 7.918871252204586e-06,
      "loss": 2.6512,
      "step": 7421
    },
    {
      "epoch": 2.873403019744483,
      "grad_norm": 32.58513259887695,
      "learning_rate": 7.918441089172797e-06,
      "loss": 1.1377,
      "step": 7422
    },
    {
      "epoch": 2.873790166473093,
      "grad_norm": 27.605220794677734,
      "learning_rate": 7.918010926141008e-06,
      "loss": 1.5467,
      "step": 7423
    },
    {
      "epoch": 2.874177313201703,
      "grad_norm": 11.929109573364258,
      "learning_rate": 7.91758076310922e-06,
      "loss": 0.7945,
      "step": 7424
    },
    {
      "epoch": 2.8745644599303137,
      "grad_norm": 24.440223693847656,
      "learning_rate": 7.91715060007743e-06,
      "loss": 1.5819,
      "step": 7425
    },
    {
      "epoch": 2.874951606658924,
      "grad_norm": 15.578191757202148,
      "learning_rate": 7.916720437045641e-06,
      "loss": 1.1516,
      "step": 7426
    },
    {
      "epoch": 2.875338753387534,
      "grad_norm": 19.802515029907227,
      "learning_rate": 7.91629027401385e-06,
      "loss": 1.4936,
      "step": 7427
    },
    {
      "epoch": 2.875725900116144,
      "grad_norm": 24.30876350402832,
      "learning_rate": 7.915860110982064e-06,
      "loss": 1.3615,
      "step": 7428
    },
    {
      "epoch": 2.876113046844754,
      "grad_norm": 28.624046325683594,
      "learning_rate": 7.915429947950274e-06,
      "loss": 2.4266,
      "step": 7429
    },
    {
      "epoch": 2.8765001935733645,
      "grad_norm": 17.61773681640625,
      "learning_rate": 7.914999784918485e-06,
      "loss": 1.519,
      "step": 7430
    },
    {
      "epoch": 2.8768873403019746,
      "grad_norm": 21.999263763427734,
      "learning_rate": 7.914569621886695e-06,
      "loss": 1.5851,
      "step": 7431
    },
    {
      "epoch": 2.8772744870305846,
      "grad_norm": 52.2398681640625,
      "learning_rate": 7.914139458854908e-06,
      "loss": 1.5409,
      "step": 7432
    },
    {
      "epoch": 2.8776616337591947,
      "grad_norm": 21.91822052001953,
      "learning_rate": 7.913709295823117e-06,
      "loss": 1.5888,
      "step": 7433
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 13.176401138305664,
      "learning_rate": 7.913279132791329e-06,
      "loss": 1.0051,
      "step": 7434
    },
    {
      "epoch": 2.8784359272164153,
      "grad_norm": 57.47438430786133,
      "learning_rate": 7.912848969759539e-06,
      "loss": 2.1808,
      "step": 7435
    },
    {
      "epoch": 2.878823073945025,
      "grad_norm": 22.219524383544922,
      "learning_rate": 7.91241880672775e-06,
      "loss": 1.6488,
      "step": 7436
    },
    {
      "epoch": 2.8792102206736354,
      "grad_norm": 39.43246841430664,
      "learning_rate": 7.911988643695961e-06,
      "loss": 1.3024,
      "step": 7437
    },
    {
      "epoch": 2.8795973674022455,
      "grad_norm": 31.788198471069336,
      "learning_rate": 7.911558480664173e-06,
      "loss": 1.7305,
      "step": 7438
    },
    {
      "epoch": 2.8799845141308555,
      "grad_norm": 55.65043258666992,
      "learning_rate": 7.911128317632384e-06,
      "loss": 2.6221,
      "step": 7439
    },
    {
      "epoch": 2.8803716608594656,
      "grad_norm": 32.92521667480469,
      "learning_rate": 7.910698154600594e-06,
      "loss": 1.0396,
      "step": 7440
    },
    {
      "epoch": 2.8807588075880757,
      "grad_norm": 16.675830841064453,
      "learning_rate": 7.910267991568805e-06,
      "loss": 1.144,
      "step": 7441
    },
    {
      "epoch": 2.881145954316686,
      "grad_norm": 27.308795928955078,
      "learning_rate": 7.909837828537015e-06,
      "loss": 0.8581,
      "step": 7442
    },
    {
      "epoch": 2.8815331010452963,
      "grad_norm": 20.067785263061523,
      "learning_rate": 7.909407665505228e-06,
      "loss": 1.0968,
      "step": 7443
    },
    {
      "epoch": 2.8819202477739063,
      "grad_norm": 37.445980072021484,
      "learning_rate": 7.908977502473438e-06,
      "loss": 2.7375,
      "step": 7444
    },
    {
      "epoch": 2.8823073945025164,
      "grad_norm": 68.42594909667969,
      "learning_rate": 7.90854733944165e-06,
      "loss": 1.1163,
      "step": 7445
    },
    {
      "epoch": 2.8826945412311265,
      "grad_norm": 26.279348373413086,
      "learning_rate": 7.908117176409859e-06,
      "loss": 3.2978,
      "step": 7446
    },
    {
      "epoch": 2.883081687959737,
      "grad_norm": 19.221954345703125,
      "learning_rate": 7.907687013378072e-06,
      "loss": 0.9805,
      "step": 7447
    },
    {
      "epoch": 2.883468834688347,
      "grad_norm": 34.83165740966797,
      "learning_rate": 7.907256850346282e-06,
      "loss": 1.3976,
      "step": 7448
    },
    {
      "epoch": 2.883855981416957,
      "grad_norm": 41.42762756347656,
      "learning_rate": 7.906826687314493e-06,
      "loss": 1.3082,
      "step": 7449
    },
    {
      "epoch": 2.884243128145567,
      "grad_norm": 59.82026672363281,
      "learning_rate": 7.906396524282703e-06,
      "loss": 2.0866,
      "step": 7450
    },
    {
      "epoch": 2.8846302748741772,
      "grad_norm": 52.88538360595703,
      "learning_rate": 7.905966361250914e-06,
      "loss": 1.5555,
      "step": 7451
    },
    {
      "epoch": 2.8850174216027873,
      "grad_norm": 12.823830604553223,
      "learning_rate": 7.905536198219126e-06,
      "loss": 1.4349,
      "step": 7452
    },
    {
      "epoch": 2.8854045683313974,
      "grad_norm": 16.670686721801758,
      "learning_rate": 7.905106035187337e-06,
      "loss": 1.3198,
      "step": 7453
    },
    {
      "epoch": 2.885791715060008,
      "grad_norm": 15.870999336242676,
      "learning_rate": 7.904675872155547e-06,
      "loss": 1.4183,
      "step": 7454
    },
    {
      "epoch": 2.886178861788618,
      "grad_norm": 13.996475219726562,
      "learning_rate": 7.904245709123758e-06,
      "loss": 0.5106,
      "step": 7455
    },
    {
      "epoch": 2.886566008517228,
      "grad_norm": 26.929668426513672,
      "learning_rate": 7.90381554609197e-06,
      "loss": 1.8601,
      "step": 7456
    },
    {
      "epoch": 2.886953155245838,
      "grad_norm": 20.589277267456055,
      "learning_rate": 7.90338538306018e-06,
      "loss": 1.1229,
      "step": 7457
    },
    {
      "epoch": 2.887340301974448,
      "grad_norm": 16.919536590576172,
      "learning_rate": 7.902955220028391e-06,
      "loss": 0.9311,
      "step": 7458
    },
    {
      "epoch": 2.8877274487030586,
      "grad_norm": 19.04384994506836,
      "learning_rate": 7.902525056996602e-06,
      "loss": 1.0293,
      "step": 7459
    },
    {
      "epoch": 2.8881145954316687,
      "grad_norm": 62.94681930541992,
      "learning_rate": 7.902094893964814e-06,
      "loss": 1.5753,
      "step": 7460
    },
    {
      "epoch": 2.8885017421602788,
      "grad_norm": 19.06062889099121,
      "learning_rate": 7.901664730933024e-06,
      "loss": 1.6013,
      "step": 7461
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 11.62189769744873,
      "learning_rate": 7.901234567901235e-06,
      "loss": 0.6444,
      "step": 7462
    },
    {
      "epoch": 2.889276035617499,
      "grad_norm": 13.669617652893066,
      "learning_rate": 7.900804404869446e-06,
      "loss": 0.5279,
      "step": 7463
    },
    {
      "epoch": 2.8896631823461094,
      "grad_norm": 65.77359771728516,
      "learning_rate": 7.900374241837658e-06,
      "loss": 2.6448,
      "step": 7464
    },
    {
      "epoch": 2.8900503290747195,
      "grad_norm": 32.89047622680664,
      "learning_rate": 7.899944078805868e-06,
      "loss": 1.6555,
      "step": 7465
    },
    {
      "epoch": 2.8904374758033295,
      "grad_norm": 10.329208374023438,
      "learning_rate": 7.899513915774079e-06,
      "loss": 0.674,
      "step": 7466
    },
    {
      "epoch": 2.8908246225319396,
      "grad_norm": 23.262950897216797,
      "learning_rate": 7.89908375274229e-06,
      "loss": 0.8277,
      "step": 7467
    },
    {
      "epoch": 2.8912117692605497,
      "grad_norm": 43.72859191894531,
      "learning_rate": 7.898653589710502e-06,
      "loss": 0.6197,
      "step": 7468
    },
    {
      "epoch": 2.8915989159891597,
      "grad_norm": 33.98553466796875,
      "learning_rate": 7.898223426678712e-06,
      "loss": 1.1831,
      "step": 7469
    },
    {
      "epoch": 2.89198606271777,
      "grad_norm": 13.330536842346191,
      "learning_rate": 7.897793263646923e-06,
      "loss": 0.8033,
      "step": 7470
    },
    {
      "epoch": 2.8923732094463803,
      "grad_norm": 46.97468566894531,
      "learning_rate": 7.897363100615134e-06,
      "loss": 1.2408,
      "step": 7471
    },
    {
      "epoch": 2.8927603561749904,
      "grad_norm": 40.47684097290039,
      "learning_rate": 7.896932937583344e-06,
      "loss": 0.754,
      "step": 7472
    },
    {
      "epoch": 2.8931475029036005,
      "grad_norm": 21.028635025024414,
      "learning_rate": 7.896502774551555e-06,
      "loss": 1.3938,
      "step": 7473
    },
    {
      "epoch": 2.8935346496322105,
      "grad_norm": 11.830492973327637,
      "learning_rate": 7.896072611519767e-06,
      "loss": 0.6229,
      "step": 7474
    },
    {
      "epoch": 2.8939217963608206,
      "grad_norm": 17.536874771118164,
      "learning_rate": 7.895642448487978e-06,
      "loss": 1.1112,
      "step": 7475
    },
    {
      "epoch": 2.894308943089431,
      "grad_norm": 11.80813217163086,
      "learning_rate": 7.895212285456188e-06,
      "loss": 1.3869,
      "step": 7476
    },
    {
      "epoch": 2.894696089818041,
      "grad_norm": 13.25986099243164,
      "learning_rate": 7.8947821224244e-06,
      "loss": 0.7611,
      "step": 7477
    },
    {
      "epoch": 2.8950832365466512,
      "grad_norm": 40.416873931884766,
      "learning_rate": 7.89435195939261e-06,
      "loss": 1.7199,
      "step": 7478
    },
    {
      "epoch": 2.8954703832752613,
      "grad_norm": 15.487855911254883,
      "learning_rate": 7.893921796360822e-06,
      "loss": 0.9654,
      "step": 7479
    },
    {
      "epoch": 2.8958575300038714,
      "grad_norm": 20.0258846282959,
      "learning_rate": 7.893491633329032e-06,
      "loss": 1.6397,
      "step": 7480
    },
    {
      "epoch": 2.896244676732482,
      "grad_norm": 37.77161407470703,
      "learning_rate": 7.893061470297243e-06,
      "loss": 1.8416,
      "step": 7481
    },
    {
      "epoch": 2.8966318234610915,
      "grad_norm": 13.165506362915039,
      "learning_rate": 7.892631307265455e-06,
      "loss": 1.6231,
      "step": 7482
    },
    {
      "epoch": 2.897018970189702,
      "grad_norm": 12.221134185791016,
      "learning_rate": 7.892201144233666e-06,
      "loss": 0.7562,
      "step": 7483
    },
    {
      "epoch": 2.897406116918312,
      "grad_norm": 20.16607093811035,
      "learning_rate": 7.891770981201876e-06,
      "loss": 1.05,
      "step": 7484
    },
    {
      "epoch": 2.897793263646922,
      "grad_norm": 19.979936599731445,
      "learning_rate": 7.891340818170087e-06,
      "loss": 1.565,
      "step": 7485
    },
    {
      "epoch": 2.898180410375532,
      "grad_norm": 23.567182540893555,
      "learning_rate": 7.890910655138299e-06,
      "loss": 1.7748,
      "step": 7486
    },
    {
      "epoch": 2.8985675571041423,
      "grad_norm": 17.488971710205078,
      "learning_rate": 7.890480492106509e-06,
      "loss": 0.6863,
      "step": 7487
    },
    {
      "epoch": 2.8989547038327528,
      "grad_norm": 21.128328323364258,
      "learning_rate": 7.89005032907472e-06,
      "loss": 1.1669,
      "step": 7488
    },
    {
      "epoch": 2.899341850561363,
      "grad_norm": 11.873275756835938,
      "learning_rate": 7.889620166042931e-06,
      "loss": 0.7253,
      "step": 7489
    },
    {
      "epoch": 2.899728997289973,
      "grad_norm": 12.634366989135742,
      "learning_rate": 7.889190003011143e-06,
      "loss": 0.7151,
      "step": 7490
    },
    {
      "epoch": 2.900116144018583,
      "grad_norm": 23.262584686279297,
      "learning_rate": 7.888759839979352e-06,
      "loss": 0.9333,
      "step": 7491
    },
    {
      "epoch": 2.900503290747193,
      "grad_norm": 20.063323974609375,
      "learning_rate": 7.888329676947564e-06,
      "loss": 1.337,
      "step": 7492
    },
    {
      "epoch": 2.9008904374758036,
      "grad_norm": 17.58413314819336,
      "learning_rate": 7.887899513915774e-06,
      "loss": 1.4017,
      "step": 7493
    },
    {
      "epoch": 2.9012775842044136,
      "grad_norm": 23.38795280456543,
      "learning_rate": 7.887469350883987e-06,
      "loss": 1.0943,
      "step": 7494
    },
    {
      "epoch": 2.9016647309330237,
      "grad_norm": 95.04936218261719,
      "learning_rate": 7.887039187852196e-06,
      "loss": 1.0745,
      "step": 7495
    },
    {
      "epoch": 2.9020518776616337,
      "grad_norm": 11.728462219238281,
      "learning_rate": 7.886609024820408e-06,
      "loss": 1.0217,
      "step": 7496
    },
    {
      "epoch": 2.902439024390244,
      "grad_norm": 25.093082427978516,
      "learning_rate": 7.886178861788618e-06,
      "loss": 1.2632,
      "step": 7497
    },
    {
      "epoch": 2.902826171118854,
      "grad_norm": 14.93930435180664,
      "learning_rate": 7.88574869875683e-06,
      "loss": 0.8494,
      "step": 7498
    },
    {
      "epoch": 2.903213317847464,
      "grad_norm": 13.054893493652344,
      "learning_rate": 7.88531853572504e-06,
      "loss": 0.7627,
      "step": 7499
    },
    {
      "epoch": 2.9036004645760745,
      "grad_norm": 19.643661499023438,
      "learning_rate": 7.884888372693252e-06,
      "loss": 1.5553,
      "step": 7500
    },
    {
      "epoch": 2.9039876113046845,
      "grad_norm": 15.128693580627441,
      "learning_rate": 7.884458209661462e-06,
      "loss": 0.8883,
      "step": 7501
    },
    {
      "epoch": 2.9043747580332946,
      "grad_norm": 27.20090103149414,
      "learning_rate": 7.884028046629673e-06,
      "loss": 1.4964,
      "step": 7502
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 11.558565139770508,
      "learning_rate": 7.883597883597884e-06,
      "loss": 1.269,
      "step": 7503
    },
    {
      "epoch": 2.9051490514905147,
      "grad_norm": 21.486549377441406,
      "learning_rate": 7.883167720566096e-06,
      "loss": 1.1011,
      "step": 7504
    },
    {
      "epoch": 2.9055361982191252,
      "grad_norm": 32.09672546386719,
      "learning_rate": 7.882737557534306e-06,
      "loss": 1.433,
      "step": 7505
    },
    {
      "epoch": 2.9059233449477353,
      "grad_norm": 23.555675506591797,
      "learning_rate": 7.882307394502517e-06,
      "loss": 1.0852,
      "step": 7506
    },
    {
      "epoch": 2.9063104916763454,
      "grad_norm": 12.119900703430176,
      "learning_rate": 7.881877231470728e-06,
      "loss": 1.3776,
      "step": 7507
    },
    {
      "epoch": 2.9066976384049554,
      "grad_norm": 34.57555389404297,
      "learning_rate": 7.881447068438938e-06,
      "loss": 1.2389,
      "step": 7508
    },
    {
      "epoch": 2.9070847851335655,
      "grad_norm": 23.638946533203125,
      "learning_rate": 7.88101690540715e-06,
      "loss": 1.217,
      "step": 7509
    },
    {
      "epoch": 2.907471931862176,
      "grad_norm": 5.454827785491943,
      "learning_rate": 7.880586742375361e-06,
      "loss": 0.1575,
      "step": 7510
    },
    {
      "epoch": 2.907859078590786,
      "grad_norm": 13.115774154663086,
      "learning_rate": 7.880156579343572e-06,
      "loss": 1.2643,
      "step": 7511
    },
    {
      "epoch": 2.908246225319396,
      "grad_norm": 20.90462875366211,
      "learning_rate": 7.879726416311782e-06,
      "loss": 1.1403,
      "step": 7512
    },
    {
      "epoch": 2.908633372048006,
      "grad_norm": 31.74289321899414,
      "learning_rate": 7.879296253279993e-06,
      "loss": 1.8887,
      "step": 7513
    },
    {
      "epoch": 2.9090205187766163,
      "grad_norm": 26.842302322387695,
      "learning_rate": 7.878866090248205e-06,
      "loss": 1.4524,
      "step": 7514
    },
    {
      "epoch": 2.9094076655052263,
      "grad_norm": 10.93935775756836,
      "learning_rate": 7.878435927216416e-06,
      "loss": 0.7429,
      "step": 7515
    },
    {
      "epoch": 2.9097948122338364,
      "grad_norm": 27.72509765625,
      "learning_rate": 7.878005764184626e-06,
      "loss": 1.7527,
      "step": 7516
    },
    {
      "epoch": 2.910181958962447,
      "grad_norm": 43.674888610839844,
      "learning_rate": 7.877575601152837e-06,
      "loss": 0.778,
      "step": 7517
    },
    {
      "epoch": 2.910569105691057,
      "grad_norm": 13.179491996765137,
      "learning_rate": 7.877145438121049e-06,
      "loss": 1.0028,
      "step": 7518
    },
    {
      "epoch": 2.910956252419667,
      "grad_norm": 22.81224250793457,
      "learning_rate": 7.87671527508926e-06,
      "loss": 1.6759,
      "step": 7519
    },
    {
      "epoch": 2.911343399148277,
      "grad_norm": 18.877471923828125,
      "learning_rate": 7.87628511205747e-06,
      "loss": 1.3883,
      "step": 7520
    },
    {
      "epoch": 2.911730545876887,
      "grad_norm": 13.85293197631836,
      "learning_rate": 7.875854949025681e-06,
      "loss": 0.9021,
      "step": 7521
    },
    {
      "epoch": 2.9121176926054977,
      "grad_norm": 17.433425903320312,
      "learning_rate": 7.875424785993893e-06,
      "loss": 1.3168,
      "step": 7522
    },
    {
      "epoch": 2.9125048393341078,
      "grad_norm": 11.060133934020996,
      "learning_rate": 7.874994622962103e-06,
      "loss": 0.856,
      "step": 7523
    },
    {
      "epoch": 2.912891986062718,
      "grad_norm": 14.897860527038574,
      "learning_rate": 7.874564459930314e-06,
      "loss": 1.0181,
      "step": 7524
    },
    {
      "epoch": 2.913279132791328,
      "grad_norm": 15.942066192626953,
      "learning_rate": 7.874134296898525e-06,
      "loss": 1.4823,
      "step": 7525
    },
    {
      "epoch": 2.913666279519938,
      "grad_norm": 14.29617977142334,
      "learning_rate": 7.873704133866737e-06,
      "loss": 0.8085,
      "step": 7526
    },
    {
      "epoch": 2.9140534262485485,
      "grad_norm": 19.41498565673828,
      "learning_rate": 7.873273970834947e-06,
      "loss": 1.2368,
      "step": 7527
    },
    {
      "epoch": 2.914440572977158,
      "grad_norm": 12.142794609069824,
      "learning_rate": 7.872843807803158e-06,
      "loss": 1.2619,
      "step": 7528
    },
    {
      "epoch": 2.9148277197057686,
      "grad_norm": 37.76754379272461,
      "learning_rate": 7.87241364477137e-06,
      "loss": 2.905,
      "step": 7529
    },
    {
      "epoch": 2.9152148664343787,
      "grad_norm": 14.188928604125977,
      "learning_rate": 7.87198348173958e-06,
      "loss": 1.0277,
      "step": 7530
    },
    {
      "epoch": 2.9156020131629887,
      "grad_norm": 10.407779693603516,
      "learning_rate": 7.87155331870779e-06,
      "loss": 0.4562,
      "step": 7531
    },
    {
      "epoch": 2.915989159891599,
      "grad_norm": 16.525474548339844,
      "learning_rate": 7.871123155676002e-06,
      "loss": 1.2979,
      "step": 7532
    },
    {
      "epoch": 2.916376306620209,
      "grad_norm": 45.80781173706055,
      "learning_rate": 7.870692992644213e-06,
      "loss": 0.4794,
      "step": 7533
    },
    {
      "epoch": 2.9167634533488194,
      "grad_norm": 22.973691940307617,
      "learning_rate": 7.870262829612425e-06,
      "loss": 1.3431,
      "step": 7534
    },
    {
      "epoch": 2.9171506000774294,
      "grad_norm": 13.195252418518066,
      "learning_rate": 7.869832666580634e-06,
      "loss": 0.8953,
      "step": 7535
    },
    {
      "epoch": 2.9175377468060395,
      "grad_norm": 26.05246925354004,
      "learning_rate": 7.869402503548846e-06,
      "loss": 1.4006,
      "step": 7536
    },
    {
      "epoch": 2.9179248935346496,
      "grad_norm": 17.150325775146484,
      "learning_rate": 7.868972340517057e-06,
      "loss": 0.4781,
      "step": 7537
    },
    {
      "epoch": 2.9183120402632596,
      "grad_norm": 27.498619079589844,
      "learning_rate": 7.868542177485267e-06,
      "loss": 1.5495,
      "step": 7538
    },
    {
      "epoch": 2.91869918699187,
      "grad_norm": 20.835845947265625,
      "learning_rate": 7.868112014453478e-06,
      "loss": 1.7281,
      "step": 7539
    },
    {
      "epoch": 2.91908633372048,
      "grad_norm": 30.841064453125,
      "learning_rate": 7.86768185142169e-06,
      "loss": 1.5197,
      "step": 7540
    },
    {
      "epoch": 2.9194734804490903,
      "grad_norm": 24.75371551513672,
      "learning_rate": 7.867251688389901e-06,
      "loss": 1.4939,
      "step": 7541
    },
    {
      "epoch": 2.9198606271777003,
      "grad_norm": 20.541250228881836,
      "learning_rate": 7.866821525358111e-06,
      "loss": 1.6417,
      "step": 7542
    },
    {
      "epoch": 2.9202477739063104,
      "grad_norm": 22.2336368560791,
      "learning_rate": 7.866391362326322e-06,
      "loss": 1.7669,
      "step": 7543
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 26.327068328857422,
      "learning_rate": 7.865961199294532e-06,
      "loss": 1.5786,
      "step": 7544
    },
    {
      "epoch": 2.9210220673635305,
      "grad_norm": 12.935742378234863,
      "learning_rate": 7.865531036262745e-06,
      "loss": 0.8505,
      "step": 7545
    },
    {
      "epoch": 2.921409214092141,
      "grad_norm": 17.43747901916504,
      "learning_rate": 7.865100873230955e-06,
      "loss": 0.8768,
      "step": 7546
    },
    {
      "epoch": 2.921796360820751,
      "grad_norm": 21.99923324584961,
      "learning_rate": 7.864670710199166e-06,
      "loss": 1.5355,
      "step": 7547
    },
    {
      "epoch": 2.922183507549361,
      "grad_norm": 18.23906707763672,
      "learning_rate": 7.864240547167376e-06,
      "loss": 1.5093,
      "step": 7548
    },
    {
      "epoch": 2.9225706542779712,
      "grad_norm": 11.594046592712402,
      "learning_rate": 7.86381038413559e-06,
      "loss": 0.7764,
      "step": 7549
    },
    {
      "epoch": 2.9229578010065813,
      "grad_norm": 30.295896530151367,
      "learning_rate": 7.863380221103799e-06,
      "loss": 1.4435,
      "step": 7550
    },
    {
      "epoch": 2.923344947735192,
      "grad_norm": 21.262060165405273,
      "learning_rate": 7.86295005807201e-06,
      "loss": 0.5894,
      "step": 7551
    },
    {
      "epoch": 2.923732094463802,
      "grad_norm": 16.498138427734375,
      "learning_rate": 7.86251989504022e-06,
      "loss": 0.7282,
      "step": 7552
    },
    {
      "epoch": 2.924119241192412,
      "grad_norm": 29.364686965942383,
      "learning_rate": 7.862089732008431e-06,
      "loss": 0.7065,
      "step": 7553
    },
    {
      "epoch": 2.924506387921022,
      "grad_norm": 26.37733268737793,
      "learning_rate": 7.861659568976643e-06,
      "loss": 1.6267,
      "step": 7554
    },
    {
      "epoch": 2.924893534649632,
      "grad_norm": 45.55252456665039,
      "learning_rate": 7.861229405944854e-06,
      "loss": 0.9352,
      "step": 7555
    },
    {
      "epoch": 2.9252806813782426,
      "grad_norm": 21.764244079589844,
      "learning_rate": 7.860799242913064e-06,
      "loss": 1.2924,
      "step": 7556
    },
    {
      "epoch": 2.925667828106852,
      "grad_norm": 25.731975555419922,
      "learning_rate": 7.860369079881275e-06,
      "loss": 1.0084,
      "step": 7557
    },
    {
      "epoch": 2.9260549748354627,
      "grad_norm": 18.687742233276367,
      "learning_rate": 7.859938916849487e-06,
      "loss": 1.3695,
      "step": 7558
    },
    {
      "epoch": 2.926442121564073,
      "grad_norm": 59.554595947265625,
      "learning_rate": 7.859508753817697e-06,
      "loss": 1.2751,
      "step": 7559
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 41.83247756958008,
      "learning_rate": 7.859078590785908e-06,
      "loss": 1.5781,
      "step": 7560
    },
    {
      "epoch": 2.927216415021293,
      "grad_norm": 22.513748168945312,
      "learning_rate": 7.85864842775412e-06,
      "loss": 1.7862,
      "step": 7561
    },
    {
      "epoch": 2.927603561749903,
      "grad_norm": 89.81108856201172,
      "learning_rate": 7.85821826472233e-06,
      "loss": 1.3373,
      "step": 7562
    },
    {
      "epoch": 2.9279907084785135,
      "grad_norm": 30.43745231628418,
      "learning_rate": 7.85778810169054e-06,
      "loss": 1.3471,
      "step": 7563
    },
    {
      "epoch": 2.9283778552071236,
      "grad_norm": 14.35595703125,
      "learning_rate": 7.857357938658754e-06,
      "loss": 0.9909,
      "step": 7564
    },
    {
      "epoch": 2.9287650019357336,
      "grad_norm": 23.521522521972656,
      "learning_rate": 7.856927775626963e-06,
      "loss": 2.2075,
      "step": 7565
    },
    {
      "epoch": 2.9291521486643437,
      "grad_norm": 34.04800796508789,
      "learning_rate": 7.856497612595175e-06,
      "loss": 1.0886,
      "step": 7566
    },
    {
      "epoch": 2.9295392953929538,
      "grad_norm": 26.37222671508789,
      "learning_rate": 7.856067449563385e-06,
      "loss": 2.2333,
      "step": 7567
    },
    {
      "epoch": 2.9299264421215643,
      "grad_norm": 16.62346839904785,
      "learning_rate": 7.855637286531596e-06,
      "loss": 0.8029,
      "step": 7568
    },
    {
      "epoch": 2.9303135888501743,
      "grad_norm": 4.348937034606934,
      "learning_rate": 7.855207123499807e-06,
      "loss": 0.2435,
      "step": 7569
    },
    {
      "epoch": 2.9307007355787844,
      "grad_norm": 20.297374725341797,
      "learning_rate": 7.854776960468019e-06,
      "loss": 1.0316,
      "step": 7570
    },
    {
      "epoch": 2.9310878823073945,
      "grad_norm": 12.813425064086914,
      "learning_rate": 7.854346797436228e-06,
      "loss": 0.5537,
      "step": 7571
    },
    {
      "epoch": 2.9314750290360045,
      "grad_norm": 10.68740177154541,
      "learning_rate": 7.85391663440444e-06,
      "loss": 0.6214,
      "step": 7572
    },
    {
      "epoch": 2.931862175764615,
      "grad_norm": 28.381267547607422,
      "learning_rate": 7.853486471372651e-06,
      "loss": 1.123,
      "step": 7573
    },
    {
      "epoch": 2.9322493224932247,
      "grad_norm": 29.72842025756836,
      "learning_rate": 7.853056308340861e-06,
      "loss": 1.1387,
      "step": 7574
    },
    {
      "epoch": 2.932636469221835,
      "grad_norm": 23.9423828125,
      "learning_rate": 7.852626145309072e-06,
      "loss": 1.2894,
      "step": 7575
    },
    {
      "epoch": 2.9330236159504453,
      "grad_norm": 11.93856430053711,
      "learning_rate": 7.852195982277284e-06,
      "loss": 0.7153,
      "step": 7576
    },
    {
      "epoch": 2.9334107626790553,
      "grad_norm": 58.82192611694336,
      "learning_rate": 7.851765819245495e-06,
      "loss": 1.3739,
      "step": 7577
    },
    {
      "epoch": 2.9337979094076654,
      "grad_norm": 15.57844066619873,
      "learning_rate": 7.851335656213705e-06,
      "loss": 0.8959,
      "step": 7578
    },
    {
      "epoch": 2.9341850561362754,
      "grad_norm": 14.428125381469727,
      "learning_rate": 7.850905493181916e-06,
      "loss": 1.5347,
      "step": 7579
    },
    {
      "epoch": 2.934572202864886,
      "grad_norm": 88.04529571533203,
      "learning_rate": 7.850475330150128e-06,
      "loss": 1.6636,
      "step": 7580
    },
    {
      "epoch": 2.934959349593496,
      "grad_norm": 16.810909271240234,
      "learning_rate": 7.85004516711834e-06,
      "loss": 1.2325,
      "step": 7581
    },
    {
      "epoch": 2.935346496322106,
      "grad_norm": 23.160694122314453,
      "learning_rate": 7.849615004086549e-06,
      "loss": 1.3442,
      "step": 7582
    },
    {
      "epoch": 2.935733643050716,
      "grad_norm": 22.55115509033203,
      "learning_rate": 7.84918484105476e-06,
      "loss": 1.6082,
      "step": 7583
    },
    {
      "epoch": 2.9361207897793262,
      "grad_norm": 15.965059280395508,
      "learning_rate": 7.848754678022972e-06,
      "loss": 1.1613,
      "step": 7584
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 12.494799613952637,
      "learning_rate": 7.848324514991183e-06,
      "loss": 0.7396,
      "step": 7585
    },
    {
      "epoch": 2.936895083236547,
      "grad_norm": 16.887178421020508,
      "learning_rate": 7.847894351959393e-06,
      "loss": 1.2961,
      "step": 7586
    },
    {
      "epoch": 2.937282229965157,
      "grad_norm": 13.349729537963867,
      "learning_rate": 7.847464188927604e-06,
      "loss": 1.1177,
      "step": 7587
    },
    {
      "epoch": 2.937669376693767,
      "grad_norm": 10.055732727050781,
      "learning_rate": 7.847034025895816e-06,
      "loss": 0.447,
      "step": 7588
    },
    {
      "epoch": 2.938056523422377,
      "grad_norm": 70.28915405273438,
      "learning_rate": 7.846603862864025e-06,
      "loss": 1.7503,
      "step": 7589
    },
    {
      "epoch": 2.938443670150987,
      "grad_norm": 21.154338836669922,
      "learning_rate": 7.846173699832237e-06,
      "loss": 1.5707,
      "step": 7590
    },
    {
      "epoch": 2.938830816879597,
      "grad_norm": 10.255411148071289,
      "learning_rate": 7.845743536800448e-06,
      "loss": 1.2872,
      "step": 7591
    },
    {
      "epoch": 2.9392179636082076,
      "grad_norm": 52.86012649536133,
      "learning_rate": 7.84531337376866e-06,
      "loss": 2.6864,
      "step": 7592
    },
    {
      "epoch": 2.9396051103368177,
      "grad_norm": 17.550188064575195,
      "learning_rate": 7.84488321073687e-06,
      "loss": 1.9115,
      "step": 7593
    },
    {
      "epoch": 2.9399922570654278,
      "grad_norm": 23.32989501953125,
      "learning_rate": 7.844453047705081e-06,
      "loss": 0.9718,
      "step": 7594
    },
    {
      "epoch": 2.940379403794038,
      "grad_norm": 8.765739440917969,
      "learning_rate": 7.84402288467329e-06,
      "loss": 0.4059,
      "step": 7595
    },
    {
      "epoch": 2.940766550522648,
      "grad_norm": 9.236712455749512,
      "learning_rate": 7.843592721641504e-06,
      "loss": 0.34,
      "step": 7596
    },
    {
      "epoch": 2.9411536972512584,
      "grad_norm": 35.89055252075195,
      "learning_rate": 7.843162558609713e-06,
      "loss": 2.7863,
      "step": 7597
    },
    {
      "epoch": 2.9415408439798685,
      "grad_norm": 6.481848239898682,
      "learning_rate": 7.842732395577925e-06,
      "loss": 0.3542,
      "step": 7598
    },
    {
      "epoch": 2.9419279907084785,
      "grad_norm": 17.52792739868164,
      "learning_rate": 7.842302232546135e-06,
      "loss": 1.0741,
      "step": 7599
    },
    {
      "epoch": 2.9423151374370886,
      "grad_norm": 43.391151428222656,
      "learning_rate": 7.841872069514348e-06,
      "loss": 2.3014,
      "step": 7600
    },
    {
      "epoch": 2.9427022841656987,
      "grad_norm": 17.087888717651367,
      "learning_rate": 7.841441906482557e-06,
      "loss": 1.7008,
      "step": 7601
    },
    {
      "epoch": 2.943089430894309,
      "grad_norm": 13.946194648742676,
      "learning_rate": 7.841011743450769e-06,
      "loss": 1.0133,
      "step": 7602
    },
    {
      "epoch": 2.943476577622919,
      "grad_norm": 22.17657470703125,
      "learning_rate": 7.84058158041898e-06,
      "loss": 2.7914,
      "step": 7603
    },
    {
      "epoch": 2.9438637243515293,
      "grad_norm": 25.69478988647461,
      "learning_rate": 7.84015141738719e-06,
      "loss": 0.9551,
      "step": 7604
    },
    {
      "epoch": 2.9442508710801394,
      "grad_norm": 25.71767807006836,
      "learning_rate": 7.839721254355401e-06,
      "loss": 2.1437,
      "step": 7605
    },
    {
      "epoch": 2.9446380178087495,
      "grad_norm": 14.32331657409668,
      "learning_rate": 7.839291091323613e-06,
      "loss": 0.9589,
      "step": 7606
    },
    {
      "epoch": 2.9450251645373595,
      "grad_norm": 17.4555606842041,
      "learning_rate": 7.838860928291824e-06,
      "loss": 1.3935,
      "step": 7607
    },
    {
      "epoch": 2.9454123112659696,
      "grad_norm": 21.628738403320312,
      "learning_rate": 7.838430765260034e-06,
      "loss": 1.6593,
      "step": 7608
    },
    {
      "epoch": 2.94579945799458,
      "grad_norm": 47.01352310180664,
      "learning_rate": 7.838000602228245e-06,
      "loss": 1.3904,
      "step": 7609
    },
    {
      "epoch": 2.94618660472319,
      "grad_norm": 26.731470108032227,
      "learning_rate": 7.837570439196455e-06,
      "loss": 1.0077,
      "step": 7610
    },
    {
      "epoch": 2.9465737514518002,
      "grad_norm": 77.44677734375,
      "learning_rate": 7.837140276164668e-06,
      "loss": 1.4165,
      "step": 7611
    },
    {
      "epoch": 2.9469608981804103,
      "grad_norm": 25.297651290893555,
      "learning_rate": 7.836710113132878e-06,
      "loss": 2.1059,
      "step": 7612
    },
    {
      "epoch": 2.9473480449090204,
      "grad_norm": 12.035430908203125,
      "learning_rate": 7.83627995010109e-06,
      "loss": 0.8245,
      "step": 7613
    },
    {
      "epoch": 2.947735191637631,
      "grad_norm": 20.51219367980957,
      "learning_rate": 7.835849787069299e-06,
      "loss": 1.9322,
      "step": 7614
    },
    {
      "epoch": 2.948122338366241,
      "grad_norm": 49.84027862548828,
      "learning_rate": 7.835419624037512e-06,
      "loss": 1.0444,
      "step": 7615
    },
    {
      "epoch": 2.948509485094851,
      "grad_norm": 26.643211364746094,
      "learning_rate": 7.834989461005722e-06,
      "loss": 0.9861,
      "step": 7616
    },
    {
      "epoch": 2.948896631823461,
      "grad_norm": 16.239465713500977,
      "learning_rate": 7.834559297973933e-06,
      "loss": 2.7917,
      "step": 7617
    },
    {
      "epoch": 2.949283778552071,
      "grad_norm": 24.668933868408203,
      "learning_rate": 7.834129134942143e-06,
      "loss": 1.5427,
      "step": 7618
    },
    {
      "epoch": 2.9496709252806816,
      "grad_norm": 12.775620460510254,
      "learning_rate": 7.833698971910354e-06,
      "loss": 0.705,
      "step": 7619
    },
    {
      "epoch": 2.9500580720092913,
      "grad_norm": 23.898914337158203,
      "learning_rate": 7.833268808878566e-06,
      "loss": 1.3836,
      "step": 7620
    },
    {
      "epoch": 2.9504452187379018,
      "grad_norm": 11.41406536102295,
      "learning_rate": 7.832838645846777e-06,
      "loss": 0.7207,
      "step": 7621
    },
    {
      "epoch": 2.950832365466512,
      "grad_norm": 16.80011749267578,
      "learning_rate": 7.832408482814987e-06,
      "loss": 1.1277,
      "step": 7622
    },
    {
      "epoch": 2.951219512195122,
      "grad_norm": 34.51325988769531,
      "learning_rate": 7.831978319783198e-06,
      "loss": 1.4443,
      "step": 7623
    },
    {
      "epoch": 2.951606658923732,
      "grad_norm": 44.85136413574219,
      "learning_rate": 7.83154815675141e-06,
      "loss": 1.6768,
      "step": 7624
    },
    {
      "epoch": 2.951993805652342,
      "grad_norm": 11.292993545532227,
      "learning_rate": 7.83111799371962e-06,
      "loss": 0.8313,
      "step": 7625
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 45.00840759277344,
      "learning_rate": 7.830687830687831e-06,
      "loss": 1.8899,
      "step": 7626
    },
    {
      "epoch": 2.9527680991095626,
      "grad_norm": 26.419065475463867,
      "learning_rate": 7.830257667656042e-06,
      "loss": 1.7738,
      "step": 7627
    },
    {
      "epoch": 2.9531552458381727,
      "grad_norm": 30.476673126220703,
      "learning_rate": 7.829827504624254e-06,
      "loss": 2.0102,
      "step": 7628
    },
    {
      "epoch": 2.9535423925667827,
      "grad_norm": 69.88998413085938,
      "learning_rate": 7.829397341592463e-06,
      "loss": 3.299,
      "step": 7629
    },
    {
      "epoch": 2.953929539295393,
      "grad_norm": 15.567957878112793,
      "learning_rate": 7.828967178560675e-06,
      "loss": 1.4363,
      "step": 7630
    },
    {
      "epoch": 2.9543166860240033,
      "grad_norm": 17.10489273071289,
      "learning_rate": 7.828537015528886e-06,
      "loss": 0.9096,
      "step": 7631
    },
    {
      "epoch": 2.9547038327526134,
      "grad_norm": 11.594610214233398,
      "learning_rate": 7.828106852497098e-06,
      "loss": 0.9812,
      "step": 7632
    },
    {
      "epoch": 2.9550909794812235,
      "grad_norm": 27.542848587036133,
      "learning_rate": 7.827676689465307e-06,
      "loss": 1.4238,
      "step": 7633
    },
    {
      "epoch": 2.9554781262098335,
      "grad_norm": 19.786611557006836,
      "learning_rate": 7.827246526433519e-06,
      "loss": 1.6653,
      "step": 7634
    },
    {
      "epoch": 2.9558652729384436,
      "grad_norm": 31.399694442749023,
      "learning_rate": 7.82681636340173e-06,
      "loss": 1.5543,
      "step": 7635
    },
    {
      "epoch": 2.9562524196670537,
      "grad_norm": 28.331134796142578,
      "learning_rate": 7.826386200369942e-06,
      "loss": 0.9998,
      "step": 7636
    },
    {
      "epoch": 2.9566395663956637,
      "grad_norm": 59.111331939697266,
      "learning_rate": 7.825956037338151e-06,
      "loss": 1.0387,
      "step": 7637
    },
    {
      "epoch": 2.9570267131242742,
      "grad_norm": 73.40115356445312,
      "learning_rate": 7.825525874306363e-06,
      "loss": 1.5984,
      "step": 7638
    },
    {
      "epoch": 2.9574138598528843,
      "grad_norm": 148.47483825683594,
      "learning_rate": 7.825095711274574e-06,
      "loss": 1.8602,
      "step": 7639
    },
    {
      "epoch": 2.9578010065814944,
      "grad_norm": 15.824188232421875,
      "learning_rate": 7.824665548242784e-06,
      "loss": 1.0164,
      "step": 7640
    },
    {
      "epoch": 2.9581881533101044,
      "grad_norm": 35.717254638671875,
      "learning_rate": 7.824235385210995e-06,
      "loss": 1.3575,
      "step": 7641
    },
    {
      "epoch": 2.9585753000387145,
      "grad_norm": 25.066896438598633,
      "learning_rate": 7.823805222179207e-06,
      "loss": 2.0715,
      "step": 7642
    },
    {
      "epoch": 2.958962446767325,
      "grad_norm": 82.25820922851562,
      "learning_rate": 7.823375059147418e-06,
      "loss": 1.4676,
      "step": 7643
    },
    {
      "epoch": 2.959349593495935,
      "grad_norm": 11.589202880859375,
      "learning_rate": 7.822944896115628e-06,
      "loss": 0.7655,
      "step": 7644
    },
    {
      "epoch": 2.959736740224545,
      "grad_norm": 5.196530342102051,
      "learning_rate": 7.82251473308384e-06,
      "loss": 0.2734,
      "step": 7645
    },
    {
      "epoch": 2.960123886953155,
      "grad_norm": 17.273422241210938,
      "learning_rate": 7.82208457005205e-06,
      "loss": 0.8208,
      "step": 7646
    },
    {
      "epoch": 2.9605110336817653,
      "grad_norm": 44.78836441040039,
      "learning_rate": 7.821654407020262e-06,
      "loss": 2.3593,
      "step": 7647
    },
    {
      "epoch": 2.960898180410376,
      "grad_norm": 22.03194808959961,
      "learning_rate": 7.821224243988472e-06,
      "loss": 1.339,
      "step": 7648
    },
    {
      "epoch": 2.9612853271389854,
      "grad_norm": 19.275638580322266,
      "learning_rate": 7.820794080956683e-06,
      "loss": 2.9706,
      "step": 7649
    },
    {
      "epoch": 2.961672473867596,
      "grad_norm": 43.652374267578125,
      "learning_rate": 7.820363917924895e-06,
      "loss": 1.6197,
      "step": 7650
    },
    {
      "epoch": 2.962059620596206,
      "grad_norm": 9.240982055664062,
      "learning_rate": 7.819933754893106e-06,
      "loss": 0.586,
      "step": 7651
    },
    {
      "epoch": 2.962446767324816,
      "grad_norm": 77.10714721679688,
      "learning_rate": 7.819503591861316e-06,
      "loss": 2.0924,
      "step": 7652
    },
    {
      "epoch": 2.962833914053426,
      "grad_norm": 16.98588752746582,
      "learning_rate": 7.819073428829527e-06,
      "loss": 1.1673,
      "step": 7653
    },
    {
      "epoch": 2.963221060782036,
      "grad_norm": 15.91903018951416,
      "learning_rate": 7.818643265797739e-06,
      "loss": 1.5038,
      "step": 7654
    },
    {
      "epoch": 2.9636082075106467,
      "grad_norm": 82.9590835571289,
      "learning_rate": 7.818213102765948e-06,
      "loss": 2.3049,
      "step": 7655
    },
    {
      "epoch": 2.9639953542392568,
      "grad_norm": 15.006303787231445,
      "learning_rate": 7.81778293973416e-06,
      "loss": 0.9673,
      "step": 7656
    },
    {
      "epoch": 2.964382500967867,
      "grad_norm": 27.301868438720703,
      "learning_rate": 7.817352776702371e-06,
      "loss": 0.1368,
      "step": 7657
    },
    {
      "epoch": 2.964769647696477,
      "grad_norm": 12.772469520568848,
      "learning_rate": 7.816922613670583e-06,
      "loss": 0.7674,
      "step": 7658
    },
    {
      "epoch": 2.965156794425087,
      "grad_norm": 9.646075248718262,
      "learning_rate": 7.816492450638792e-06,
      "loss": 0.6115,
      "step": 7659
    },
    {
      "epoch": 2.9655439411536975,
      "grad_norm": 23.816368103027344,
      "learning_rate": 7.816062287607004e-06,
      "loss": 1.2671,
      "step": 7660
    },
    {
      "epoch": 2.9659310878823075,
      "grad_norm": 14.153557777404785,
      "learning_rate": 7.815632124575214e-06,
      "loss": 1.4802,
      "step": 7661
    },
    {
      "epoch": 2.9663182346109176,
      "grad_norm": 27.313461303710938,
      "learning_rate": 7.815201961543427e-06,
      "loss": 1.968,
      "step": 7662
    },
    {
      "epoch": 2.9667053813395277,
      "grad_norm": 32.77643966674805,
      "learning_rate": 7.814771798511636e-06,
      "loss": 2.0709,
      "step": 7663
    },
    {
      "epoch": 2.9670925280681377,
      "grad_norm": 18.505828857421875,
      "learning_rate": 7.814341635479848e-06,
      "loss": 0.8473,
      "step": 7664
    },
    {
      "epoch": 2.9674796747967482,
      "grad_norm": 21.427196502685547,
      "learning_rate": 7.813911472448058e-06,
      "loss": 1.5508,
      "step": 7665
    },
    {
      "epoch": 2.967866821525358,
      "grad_norm": 28.640403747558594,
      "learning_rate": 7.81348130941627e-06,
      "loss": 1.4248,
      "step": 7666
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 47.117984771728516,
      "learning_rate": 7.81305114638448e-06,
      "loss": 2.0329,
      "step": 7667
    },
    {
      "epoch": 2.9686411149825784,
      "grad_norm": 18.83332061767578,
      "learning_rate": 7.812620983352692e-06,
      "loss": 0.9712,
      "step": 7668
    },
    {
      "epoch": 2.9690282617111885,
      "grad_norm": 5.2791242599487305,
      "learning_rate": 7.812190820320901e-06,
      "loss": 0.1835,
      "step": 7669
    },
    {
      "epoch": 2.9694154084397986,
      "grad_norm": 12.313138008117676,
      "learning_rate": 7.811760657289113e-06,
      "loss": 0.7042,
      "step": 7670
    },
    {
      "epoch": 2.9698025551684086,
      "grad_norm": 16.474143981933594,
      "learning_rate": 7.811330494257324e-06,
      "loss": 1.0089,
      "step": 7671
    },
    {
      "epoch": 2.970189701897019,
      "grad_norm": 36.530189514160156,
      "learning_rate": 7.810900331225536e-06,
      "loss": 2.2292,
      "step": 7672
    },
    {
      "epoch": 2.970576848625629,
      "grad_norm": 38.242515563964844,
      "learning_rate": 7.810470168193745e-06,
      "loss": 1.4064,
      "step": 7673
    },
    {
      "epoch": 2.9709639953542393,
      "grad_norm": 21.60844612121582,
      "learning_rate": 7.810040005161957e-06,
      "loss": 1.2989,
      "step": 7674
    },
    {
      "epoch": 2.9713511420828493,
      "grad_norm": 17.23124122619629,
      "learning_rate": 7.809609842130168e-06,
      "loss": 1.6613,
      "step": 7675
    },
    {
      "epoch": 2.9717382888114594,
      "grad_norm": 29.38550567626953,
      "learning_rate": 7.809179679098378e-06,
      "loss": 1.7785,
      "step": 7676
    },
    {
      "epoch": 2.97212543554007,
      "grad_norm": 36.084529876708984,
      "learning_rate": 7.80874951606659e-06,
      "loss": 1.7782,
      "step": 7677
    },
    {
      "epoch": 2.97251258226868,
      "grad_norm": 10.927180290222168,
      "learning_rate": 7.8083193530348e-06,
      "loss": 0.9234,
      "step": 7678
    },
    {
      "epoch": 2.97289972899729,
      "grad_norm": 19.092378616333008,
      "learning_rate": 7.807889190003012e-06,
      "loss": 1.2638,
      "step": 7679
    },
    {
      "epoch": 2.9732868757259,
      "grad_norm": 23.379833221435547,
      "learning_rate": 7.807459026971222e-06,
      "loss": 1.8974,
      "step": 7680
    },
    {
      "epoch": 2.97367402245451,
      "grad_norm": 20.28824806213379,
      "learning_rate": 7.807028863939433e-06,
      "loss": 1.0433,
      "step": 7681
    },
    {
      "epoch": 2.9740611691831202,
      "grad_norm": 19.18601417541504,
      "learning_rate": 7.806598700907645e-06,
      "loss": 1.1779,
      "step": 7682
    },
    {
      "epoch": 2.9744483159117303,
      "grad_norm": 129.82688903808594,
      "learning_rate": 7.806168537875856e-06,
      "loss": 1.384,
      "step": 7683
    },
    {
      "epoch": 2.974835462640341,
      "grad_norm": 43.131858825683594,
      "learning_rate": 7.805738374844066e-06,
      "loss": 1.4995,
      "step": 7684
    },
    {
      "epoch": 2.975222609368951,
      "grad_norm": 17.70462417602539,
      "learning_rate": 7.805308211812277e-06,
      "loss": 1.6879,
      "step": 7685
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 15.093474388122559,
      "learning_rate": 7.804878048780489e-06,
      "loss": 1.1208,
      "step": 7686
    },
    {
      "epoch": 2.975996902826171,
      "grad_norm": 16.016138076782227,
      "learning_rate": 7.8044478857487e-06,
      "loss": 1.5023,
      "step": 7687
    },
    {
      "epoch": 2.976384049554781,
      "grad_norm": 29.931941986083984,
      "learning_rate": 7.80401772271691e-06,
      "loss": 1.4179,
      "step": 7688
    },
    {
      "epoch": 2.9767711962833916,
      "grad_norm": 21.727596282958984,
      "learning_rate": 7.803587559685121e-06,
      "loss": 1.5661,
      "step": 7689
    },
    {
      "epoch": 2.9771583430120017,
      "grad_norm": 28.548112869262695,
      "learning_rate": 7.803157396653333e-06,
      "loss": 1.3783,
      "step": 7690
    },
    {
      "epoch": 2.9775454897406117,
      "grad_norm": 12.934340476989746,
      "learning_rate": 7.802727233621542e-06,
      "loss": 0.8766,
      "step": 7691
    },
    {
      "epoch": 2.977932636469222,
      "grad_norm": 23.93756866455078,
      "learning_rate": 7.802297070589754e-06,
      "loss": 1.0129,
      "step": 7692
    },
    {
      "epoch": 2.978319783197832,
      "grad_norm": 22.113100051879883,
      "learning_rate": 7.801866907557965e-06,
      "loss": 1.6293,
      "step": 7693
    },
    {
      "epoch": 2.9787069299264424,
      "grad_norm": 22.02153968811035,
      "learning_rate": 7.801436744526177e-06,
      "loss": 1.2684,
      "step": 7694
    },
    {
      "epoch": 2.979094076655052,
      "grad_norm": 43.82540512084961,
      "learning_rate": 7.801006581494386e-06,
      "loss": 0.9396,
      "step": 7695
    },
    {
      "epoch": 2.9794812233836625,
      "grad_norm": 32.137962341308594,
      "learning_rate": 7.800576418462598e-06,
      "loss": 1.8266,
      "step": 7696
    },
    {
      "epoch": 2.9798683701122726,
      "grad_norm": 38.128028869628906,
      "learning_rate": 7.80014625543081e-06,
      "loss": 2.2733,
      "step": 7697
    },
    {
      "epoch": 2.9802555168408826,
      "grad_norm": 31.798913955688477,
      "learning_rate": 7.79971609239902e-06,
      "loss": 1.5281,
      "step": 7698
    },
    {
      "epoch": 2.9806426635694927,
      "grad_norm": 17.188316345214844,
      "learning_rate": 7.79928592936723e-06,
      "loss": 1.5719,
      "step": 7699
    },
    {
      "epoch": 2.9810298102981028,
      "grad_norm": 54.26243591308594,
      "learning_rate": 7.798855766335442e-06,
      "loss": 1.1134,
      "step": 7700
    },
    {
      "epoch": 2.9814169570267133,
      "grad_norm": 13.169770240783691,
      "learning_rate": 7.798425603303653e-06,
      "loss": 1.2909,
      "step": 7701
    },
    {
      "epoch": 2.9818041037553233,
      "grad_norm": 26.270004272460938,
      "learning_rate": 7.797995440271865e-06,
      "loss": 1.6159,
      "step": 7702
    },
    {
      "epoch": 2.9821912504839334,
      "grad_norm": 17.638534545898438,
      "learning_rate": 7.797565277240074e-06,
      "loss": 1.2895,
      "step": 7703
    },
    {
      "epoch": 2.9825783972125435,
      "grad_norm": 26.850343704223633,
      "learning_rate": 7.797135114208286e-06,
      "loss": 1.9032,
      "step": 7704
    },
    {
      "epoch": 2.9829655439411535,
      "grad_norm": 41.008819580078125,
      "learning_rate": 7.796704951176497e-06,
      "loss": 1.451,
      "step": 7705
    },
    {
      "epoch": 2.983352690669764,
      "grad_norm": 35.8395881652832,
      "learning_rate": 7.796274788144707e-06,
      "loss": 0.8619,
      "step": 7706
    },
    {
      "epoch": 2.983739837398374,
      "grad_norm": 16.528902053833008,
      "learning_rate": 7.795844625112918e-06,
      "loss": 1.5121,
      "step": 7707
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 10.790200233459473,
      "learning_rate": 7.79541446208113e-06,
      "loss": 0.7091,
      "step": 7708
    },
    {
      "epoch": 2.9845141308555942,
      "grad_norm": 17.019336700439453,
      "learning_rate": 7.794984299049341e-06,
      "loss": 1.7264,
      "step": 7709
    },
    {
      "epoch": 2.9849012775842043,
      "grad_norm": 25.079681396484375,
      "learning_rate": 7.794554136017551e-06,
      "loss": 1.9034,
      "step": 7710
    },
    {
      "epoch": 2.985288424312815,
      "grad_norm": 18.498109817504883,
      "learning_rate": 7.794123972985762e-06,
      "loss": 1.1993,
      "step": 7711
    },
    {
      "epoch": 2.9856755710414244,
      "grad_norm": 25.541589736938477,
      "learning_rate": 7.793693809953972e-06,
      "loss": 1.5349,
      "step": 7712
    },
    {
      "epoch": 2.986062717770035,
      "grad_norm": 40.90619659423828,
      "learning_rate": 7.793263646922185e-06,
      "loss": 1.0795,
      "step": 7713
    },
    {
      "epoch": 2.986449864498645,
      "grad_norm": 30.693634033203125,
      "learning_rate": 7.792833483890395e-06,
      "loss": 2.4962,
      "step": 7714
    },
    {
      "epoch": 2.986837011227255,
      "grad_norm": 30.30902099609375,
      "learning_rate": 7.792403320858606e-06,
      "loss": 0.527,
      "step": 7715
    },
    {
      "epoch": 2.987224157955865,
      "grad_norm": 33.825889587402344,
      "learning_rate": 7.791973157826816e-06,
      "loss": 1.3322,
      "step": 7716
    },
    {
      "epoch": 2.987611304684475,
      "grad_norm": 24.192195892333984,
      "learning_rate": 7.791542994795029e-06,
      "loss": 1.4953,
      "step": 7717
    },
    {
      "epoch": 2.9879984514130857,
      "grad_norm": 24.882539749145508,
      "learning_rate": 7.791112831763239e-06,
      "loss": 1.6958,
      "step": 7718
    },
    {
      "epoch": 2.988385598141696,
      "grad_norm": 18.953907012939453,
      "learning_rate": 7.79068266873145e-06,
      "loss": 0.7709,
      "step": 7719
    },
    {
      "epoch": 2.988772744870306,
      "grad_norm": 11.8904390335083,
      "learning_rate": 7.79025250569966e-06,
      "loss": 0.8275,
      "step": 7720
    },
    {
      "epoch": 2.989159891598916,
      "grad_norm": 18.810344696044922,
      "learning_rate": 7.789822342667871e-06,
      "loss": 1.6795,
      "step": 7721
    },
    {
      "epoch": 2.989547038327526,
      "grad_norm": 18.047544479370117,
      "learning_rate": 7.789392179636083e-06,
      "loss": 2.0353,
      "step": 7722
    },
    {
      "epoch": 2.9899341850561365,
      "grad_norm": 28.835813522338867,
      "learning_rate": 7.788962016604294e-06,
      "loss": 1.639,
      "step": 7723
    },
    {
      "epoch": 2.9903213317847466,
      "grad_norm": 44.48983383178711,
      "learning_rate": 7.788531853572504e-06,
      "loss": 4.9751,
      "step": 7724
    },
    {
      "epoch": 2.9907084785133566,
      "grad_norm": 16.88945198059082,
      "learning_rate": 7.788101690540715e-06,
      "loss": 1.1643,
      "step": 7725
    },
    {
      "epoch": 2.9910956252419667,
      "grad_norm": 11.621156692504883,
      "learning_rate": 7.787671527508927e-06,
      "loss": 0.8114,
      "step": 7726
    },
    {
      "epoch": 2.9914827719705768,
      "grad_norm": 20.28190803527832,
      "learning_rate": 7.787241364477136e-06,
      "loss": 1.4851,
      "step": 7727
    },
    {
      "epoch": 2.991869918699187,
      "grad_norm": 14.883934020996094,
      "learning_rate": 7.78681120144535e-06,
      "loss": 1.2436,
      "step": 7728
    },
    {
      "epoch": 2.992257065427797,
      "grad_norm": 24.936351776123047,
      "learning_rate": 7.78638103841356e-06,
      "loss": 0.7104,
      "step": 7729
    },
    {
      "epoch": 2.9926442121564074,
      "grad_norm": 50.67856216430664,
      "learning_rate": 7.78595087538177e-06,
      "loss": 1.6678,
      "step": 7730
    },
    {
      "epoch": 2.9930313588850175,
      "grad_norm": 14.979793548583984,
      "learning_rate": 7.78552071234998e-06,
      "loss": 0.9234,
      "step": 7731
    },
    {
      "epoch": 2.9934185056136275,
      "grad_norm": 29.643400192260742,
      "learning_rate": 7.785090549318194e-06,
      "loss": 2.0833,
      "step": 7732
    },
    {
      "epoch": 2.9938056523422376,
      "grad_norm": 18.159196853637695,
      "learning_rate": 7.784660386286403e-06,
      "loss": 2.1282,
      "step": 7733
    },
    {
      "epoch": 2.9941927990708477,
      "grad_norm": 17.988203048706055,
      "learning_rate": 7.784230223254615e-06,
      "loss": 1.671,
      "step": 7734
    },
    {
      "epoch": 2.994579945799458,
      "grad_norm": 31.355192184448242,
      "learning_rate": 7.783800060222824e-06,
      "loss": 2.0197,
      "step": 7735
    },
    {
      "epoch": 2.9949670925280683,
      "grad_norm": 21.594505310058594,
      "learning_rate": 7.783369897191036e-06,
      "loss": 1.342,
      "step": 7736
    },
    {
      "epoch": 2.9953542392566783,
      "grad_norm": 8.880645751953125,
      "learning_rate": 7.782939734159247e-06,
      "loss": 0.409,
      "step": 7737
    },
    {
      "epoch": 2.9957413859852884,
      "grad_norm": 21.901714324951172,
      "learning_rate": 7.782509571127459e-06,
      "loss": 1.6835,
      "step": 7738
    },
    {
      "epoch": 2.9961285327138985,
      "grad_norm": 16.139680862426758,
      "learning_rate": 7.782079408095668e-06,
      "loss": 1.0025,
      "step": 7739
    },
    {
      "epoch": 2.996515679442509,
      "grad_norm": 29.698129653930664,
      "learning_rate": 7.78164924506388e-06,
      "loss": 2.5466,
      "step": 7740
    },
    {
      "epoch": 2.9969028261711186,
      "grad_norm": 37.663516998291016,
      "learning_rate": 7.781219082032091e-06,
      "loss": 1.6845,
      "step": 7741
    },
    {
      "epoch": 2.997289972899729,
      "grad_norm": 23.102161407470703,
      "learning_rate": 7.780788919000301e-06,
      "loss": 1.3012,
      "step": 7742
    },
    {
      "epoch": 2.997677119628339,
      "grad_norm": 17.479473114013672,
      "learning_rate": 7.780358755968512e-06,
      "loss": 0.6094,
      "step": 7743
    },
    {
      "epoch": 2.9980642663569492,
      "grad_norm": 38.64918518066406,
      "learning_rate": 7.779928592936724e-06,
      "loss": 2.0012,
      "step": 7744
    },
    {
      "epoch": 2.9984514130855593,
      "grad_norm": 19.661022186279297,
      "learning_rate": 7.779498429904935e-06,
      "loss": 1.017,
      "step": 7745
    },
    {
      "epoch": 2.9988385598141694,
      "grad_norm": 9.985368728637695,
      "learning_rate": 7.779068266873145e-06,
      "loss": 1.2527,
      "step": 7746
    },
    {
      "epoch": 2.99922570654278,
      "grad_norm": 19.406240463256836,
      "learning_rate": 7.778638103841356e-06,
      "loss": 1.2118,
      "step": 7747
    },
    {
      "epoch": 2.99961285327139,
      "grad_norm": 23.592937469482422,
      "learning_rate": 7.778207940809568e-06,
      "loss": 1.1619,
      "step": 7748
    },
    {
      "epoch": 3.0,
      "grad_norm": 51.24742126464844,
      "learning_rate": 7.77777777777778e-06,
      "loss": 4.7363,
      "step": 7749
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.4386792452830189,
      "eval_f1": 0.3958868115711651,
      "eval_loss": 1.7091596126556396,
      "eval_runtime": 383.3463,
      "eval_samples_per_second": 2.765,
      "eval_steps_per_second": 1.383,
      "step": 7749
    },
    {
      "epoch": 3.00038714672861,
      "grad_norm": 15.874259948730469,
      "learning_rate": 7.777347614745989e-06,
      "loss": 1.203,
      "step": 7750
    },
    {
      "epoch": 3.00077429345722,
      "grad_norm": 22.765357971191406,
      "learning_rate": 7.7769174517142e-06,
      "loss": 1.7916,
      "step": 7751
    },
    {
      "epoch": 3.0011614401858306,
      "grad_norm": 32.31345748901367,
      "learning_rate": 7.776487288682412e-06,
      "loss": 1.5012,
      "step": 7752
    },
    {
      "epoch": 3.0015485869144407,
      "grad_norm": 20.285295486450195,
      "learning_rate": 7.776057125650623e-06,
      "loss": 1.1315,
      "step": 7753
    },
    {
      "epoch": 3.0019357336430508,
      "grad_norm": 19.474323272705078,
      "learning_rate": 7.775626962618833e-06,
      "loss": 0.9247,
      "step": 7754
    },
    {
      "epoch": 3.002322880371661,
      "grad_norm": 39.23411178588867,
      "learning_rate": 7.775196799587044e-06,
      "loss": 1.6015,
      "step": 7755
    },
    {
      "epoch": 3.002710027100271,
      "grad_norm": 29.44226837158203,
      "learning_rate": 7.774766636555256e-06,
      "loss": 2.4707,
      "step": 7756
    },
    {
      "epoch": 3.003097173828881,
      "grad_norm": 75.90967559814453,
      "learning_rate": 7.774336473523465e-06,
      "loss": 1.2226,
      "step": 7757
    },
    {
      "epoch": 3.0034843205574915,
      "grad_norm": 19.310726165771484,
      "learning_rate": 7.773906310491677e-06,
      "loss": 1.2553,
      "step": 7758
    },
    {
      "epoch": 3.0038714672861015,
      "grad_norm": 16.42661476135254,
      "learning_rate": 7.773476147459888e-06,
      "loss": 1.1724,
      "step": 7759
    },
    {
      "epoch": 3.0042586140147116,
      "grad_norm": 18.701906204223633,
      "learning_rate": 7.7730459844281e-06,
      "loss": 1.7028,
      "step": 7760
    },
    {
      "epoch": 3.0046457607433217,
      "grad_norm": 34.8809700012207,
      "learning_rate": 7.77261582139631e-06,
      "loss": 1.5966,
      "step": 7761
    },
    {
      "epoch": 3.0050329074719317,
      "grad_norm": 14.013284683227539,
      "learning_rate": 7.77218565836452e-06,
      "loss": 0.6606,
      "step": 7762
    },
    {
      "epoch": 3.005420054200542,
      "grad_norm": 18.577253341674805,
      "learning_rate": 7.77175549533273e-06,
      "loss": 1.4341,
      "step": 7763
    },
    {
      "epoch": 3.0058072009291523,
      "grad_norm": 30.59852409362793,
      "learning_rate": 7.771325332300944e-06,
      "loss": 1.7054,
      "step": 7764
    },
    {
      "epoch": 3.0061943476577624,
      "grad_norm": 33.61268997192383,
      "learning_rate": 7.770895169269153e-06,
      "loss": 2.4129,
      "step": 7765
    },
    {
      "epoch": 3.0065814943863725,
      "grad_norm": 36.26161193847656,
      "learning_rate": 7.770465006237365e-06,
      "loss": 2.1524,
      "step": 7766
    },
    {
      "epoch": 3.0069686411149825,
      "grad_norm": 26.28333854675293,
      "learning_rate": 7.770034843205574e-06,
      "loss": 2.5589,
      "step": 7767
    },
    {
      "epoch": 3.0073557878435926,
      "grad_norm": 17.5554256439209,
      "learning_rate": 7.769604680173788e-06,
      "loss": 0.9985,
      "step": 7768
    },
    {
      "epoch": 3.0077429345722027,
      "grad_norm": 25.839323043823242,
      "learning_rate": 7.769174517141997e-06,
      "loss": 1.4763,
      "step": 7769
    },
    {
      "epoch": 3.008130081300813,
      "grad_norm": 18.137252807617188,
      "learning_rate": 7.768744354110209e-06,
      "loss": 1.1861,
      "step": 7770
    },
    {
      "epoch": 3.0085172280294232,
      "grad_norm": 16.402189254760742,
      "learning_rate": 7.76831419107842e-06,
      "loss": 1.4298,
      "step": 7771
    },
    {
      "epoch": 3.0089043747580333,
      "grad_norm": 7.939216613769531,
      "learning_rate": 7.76788402804663e-06,
      "loss": 1.2804,
      "step": 7772
    },
    {
      "epoch": 3.0092915214866434,
      "grad_norm": 19.11491584777832,
      "learning_rate": 7.767453865014841e-06,
      "loss": 1.732,
      "step": 7773
    },
    {
      "epoch": 3.0096786682152534,
      "grad_norm": 21.179908752441406,
      "learning_rate": 7.767023701983053e-06,
      "loss": 2.1342,
      "step": 7774
    },
    {
      "epoch": 3.010065814943864,
      "grad_norm": 37.35902404785156,
      "learning_rate": 7.766593538951264e-06,
      "loss": 2.9727,
      "step": 7775
    },
    {
      "epoch": 3.010452961672474,
      "grad_norm": 10.123458862304688,
      "learning_rate": 7.766163375919474e-06,
      "loss": 1.1618,
      "step": 7776
    },
    {
      "epoch": 3.010840108401084,
      "grad_norm": 27.192306518554688,
      "learning_rate": 7.765733212887685e-06,
      "loss": 1.4532,
      "step": 7777
    },
    {
      "epoch": 3.011227255129694,
      "grad_norm": 27.76751136779785,
      "learning_rate": 7.765303049855895e-06,
      "loss": 1.084,
      "step": 7778
    },
    {
      "epoch": 3.011614401858304,
      "grad_norm": 17.633590698242188,
      "learning_rate": 7.764872886824108e-06,
      "loss": 0.8621,
      "step": 7779
    },
    {
      "epoch": 3.0120015485869143,
      "grad_norm": 47.8470573425293,
      "learning_rate": 7.764442723792318e-06,
      "loss": 1.0291,
      "step": 7780
    },
    {
      "epoch": 3.012388695315525,
      "grad_norm": 34.28200149536133,
      "learning_rate": 7.76401256076053e-06,
      "loss": 0.7342,
      "step": 7781
    },
    {
      "epoch": 3.012775842044135,
      "grad_norm": 27.822477340698242,
      "learning_rate": 7.763582397728739e-06,
      "loss": 2.0318,
      "step": 7782
    },
    {
      "epoch": 3.013162988772745,
      "grad_norm": 13.466251373291016,
      "learning_rate": 7.763152234696952e-06,
      "loss": 0.6376,
      "step": 7783
    },
    {
      "epoch": 3.013550135501355,
      "grad_norm": 14.062880516052246,
      "learning_rate": 7.762722071665162e-06,
      "loss": 1.208,
      "step": 7784
    },
    {
      "epoch": 3.013937282229965,
      "grad_norm": 11.239494323730469,
      "learning_rate": 7.762291908633373e-06,
      "loss": 0.4426,
      "step": 7785
    },
    {
      "epoch": 3.014324428958575,
      "grad_norm": 12.382352828979492,
      "learning_rate": 7.761861745601583e-06,
      "loss": 1.2591,
      "step": 7786
    },
    {
      "epoch": 3.0147115756871856,
      "grad_norm": 17.159963607788086,
      "learning_rate": 7.761431582569794e-06,
      "loss": 1.5761,
      "step": 7787
    },
    {
      "epoch": 3.0150987224157957,
      "grad_norm": 17.706771850585938,
      "learning_rate": 7.761001419538006e-06,
      "loss": 0.7952,
      "step": 7788
    },
    {
      "epoch": 3.0154858691444058,
      "grad_norm": 14.706048965454102,
      "learning_rate": 7.760571256506217e-06,
      "loss": 1.4993,
      "step": 7789
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 14.478745460510254,
      "learning_rate": 7.760141093474427e-06,
      "loss": 0.9008,
      "step": 7790
    },
    {
      "epoch": 3.016260162601626,
      "grad_norm": 42.762420654296875,
      "learning_rate": 7.759710930442638e-06,
      "loss": 1.688,
      "step": 7791
    },
    {
      "epoch": 3.016647309330236,
      "grad_norm": 124.58927154541016,
      "learning_rate": 7.75928076741085e-06,
      "loss": 0.9734,
      "step": 7792
    },
    {
      "epoch": 3.0170344560588465,
      "grad_norm": 32.2765007019043,
      "learning_rate": 7.75885060437906e-06,
      "loss": 2.0798,
      "step": 7793
    },
    {
      "epoch": 3.0174216027874565,
      "grad_norm": 18.795045852661133,
      "learning_rate": 7.758420441347271e-06,
      "loss": 0.8754,
      "step": 7794
    },
    {
      "epoch": 3.0178087495160666,
      "grad_norm": 32.83319091796875,
      "learning_rate": 7.757990278315482e-06,
      "loss": 2.2614,
      "step": 7795
    },
    {
      "epoch": 3.0181958962446767,
      "grad_norm": 22.640140533447266,
      "learning_rate": 7.757560115283694e-06,
      "loss": 2.0862,
      "step": 7796
    },
    {
      "epoch": 3.0185830429732867,
      "grad_norm": 5.523979663848877,
      "learning_rate": 7.757129952251903e-06,
      "loss": 0.2697,
      "step": 7797
    },
    {
      "epoch": 3.0189701897018972,
      "grad_norm": 20.096118927001953,
      "learning_rate": 7.756699789220115e-06,
      "loss": 1.4139,
      "step": 7798
    },
    {
      "epoch": 3.0193573364305073,
      "grad_norm": 38.28183364868164,
      "learning_rate": 7.756269626188326e-06,
      "loss": 1.5958,
      "step": 7799
    },
    {
      "epoch": 3.0197444831591174,
      "grad_norm": 18.685476303100586,
      "learning_rate": 7.755839463156538e-06,
      "loss": 0.7452,
      "step": 7800
    },
    {
      "epoch": 3.0201316298877274,
      "grad_norm": 20.78768539428711,
      "learning_rate": 7.755409300124747e-06,
      "loss": 1.7676,
      "step": 7801
    },
    {
      "epoch": 3.0205187766163375,
      "grad_norm": 16.452455520629883,
      "learning_rate": 7.754979137092959e-06,
      "loss": 1.1919,
      "step": 7802
    },
    {
      "epoch": 3.0209059233449476,
      "grad_norm": 24.200786590576172,
      "learning_rate": 7.75454897406117e-06,
      "loss": 1.6418,
      "step": 7803
    },
    {
      "epoch": 3.021293070073558,
      "grad_norm": 14.791814804077148,
      "learning_rate": 7.754118811029382e-06,
      "loss": 1.4453,
      "step": 7804
    },
    {
      "epoch": 3.021680216802168,
      "grad_norm": 20.659833908081055,
      "learning_rate": 7.753688647997591e-06,
      "loss": 1.3027,
      "step": 7805
    },
    {
      "epoch": 3.022067363530778,
      "grad_norm": 16.032520294189453,
      "learning_rate": 7.753258484965803e-06,
      "loss": 1.704,
      "step": 7806
    },
    {
      "epoch": 3.0224545102593883,
      "grad_norm": 10.629820823669434,
      "learning_rate": 7.752828321934014e-06,
      "loss": 0.6049,
      "step": 7807
    },
    {
      "epoch": 3.0228416569879983,
      "grad_norm": 31.936567306518555,
      "learning_rate": 7.752398158902224e-06,
      "loss": 1.2449,
      "step": 7808
    },
    {
      "epoch": 3.0232288037166084,
      "grad_norm": 9.229829788208008,
      "learning_rate": 7.751967995870435e-06,
      "loss": 0.4541,
      "step": 7809
    },
    {
      "epoch": 3.023615950445219,
      "grad_norm": 29.689897537231445,
      "learning_rate": 7.751537832838647e-06,
      "loss": 1.5169,
      "step": 7810
    },
    {
      "epoch": 3.024003097173829,
      "grad_norm": 31.15234375,
      "learning_rate": 7.751107669806858e-06,
      "loss": 1.0893,
      "step": 7811
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 24.29122543334961,
      "learning_rate": 7.750677506775068e-06,
      "loss": 1.3724,
      "step": 7812
    },
    {
      "epoch": 3.024777390631049,
      "grad_norm": 15.153080940246582,
      "learning_rate": 7.75024734374328e-06,
      "loss": 1.138,
      "step": 7813
    },
    {
      "epoch": 3.025164537359659,
      "grad_norm": 67.5451889038086,
      "learning_rate": 7.74981718071149e-06,
      "loss": 1.1895,
      "step": 7814
    },
    {
      "epoch": 3.0255516840882692,
      "grad_norm": 18.293087005615234,
      "learning_rate": 7.749387017679702e-06,
      "loss": 0.8955,
      "step": 7815
    },
    {
      "epoch": 3.0259388308168798,
      "grad_norm": 48.71675109863281,
      "learning_rate": 7.748956854647912e-06,
      "loss": 0.826,
      "step": 7816
    },
    {
      "epoch": 3.02632597754549,
      "grad_norm": 14.235394477844238,
      "learning_rate": 7.748526691616123e-06,
      "loss": 1.7754,
      "step": 7817
    },
    {
      "epoch": 3.0267131242741,
      "grad_norm": 23.80897331237793,
      "learning_rate": 7.748096528584335e-06,
      "loss": 1.7262,
      "step": 7818
    },
    {
      "epoch": 3.02710027100271,
      "grad_norm": 12.54161548614502,
      "learning_rate": 7.747666365552546e-06,
      "loss": 0.3704,
      "step": 7819
    },
    {
      "epoch": 3.02748741773132,
      "grad_norm": 16.40167808532715,
      "learning_rate": 7.747236202520756e-06,
      "loss": 0.9761,
      "step": 7820
    },
    {
      "epoch": 3.0278745644599305,
      "grad_norm": 12.082124710083008,
      "learning_rate": 7.746806039488967e-06,
      "loss": 1.4023,
      "step": 7821
    },
    {
      "epoch": 3.0282617111885406,
      "grad_norm": 15.72426700592041,
      "learning_rate": 7.746375876457179e-06,
      "loss": 1.6551,
      "step": 7822
    },
    {
      "epoch": 3.0286488579171507,
      "grad_norm": 18.70979118347168,
      "learning_rate": 7.745945713425388e-06,
      "loss": 1.658,
      "step": 7823
    },
    {
      "epoch": 3.0290360046457607,
      "grad_norm": 15.74149227142334,
      "learning_rate": 7.7455155503936e-06,
      "loss": 1.0736,
      "step": 7824
    },
    {
      "epoch": 3.029423151374371,
      "grad_norm": 7.701109409332275,
      "learning_rate": 7.745085387361811e-06,
      "loss": 0.5544,
      "step": 7825
    },
    {
      "epoch": 3.029810298102981,
      "grad_norm": 19.641101837158203,
      "learning_rate": 7.744655224330023e-06,
      "loss": 0.8909,
      "step": 7826
    },
    {
      "epoch": 3.0301974448315914,
      "grad_norm": 15.985676765441895,
      "learning_rate": 7.744225061298232e-06,
      "loss": 1.5321,
      "step": 7827
    },
    {
      "epoch": 3.0305845915602014,
      "grad_norm": 12.615474700927734,
      "learning_rate": 7.743794898266444e-06,
      "loss": 0.8441,
      "step": 7828
    },
    {
      "epoch": 3.0309717382888115,
      "grad_norm": 17.610706329345703,
      "learning_rate": 7.743364735234653e-06,
      "loss": 1.0193,
      "step": 7829
    },
    {
      "epoch": 3.0313588850174216,
      "grad_norm": 18.572935104370117,
      "learning_rate": 7.742934572202867e-06,
      "loss": 0.8953,
      "step": 7830
    },
    {
      "epoch": 3.0317460317460316,
      "grad_norm": 19.57613754272461,
      "learning_rate": 7.742504409171076e-06,
      "loss": 1.3962,
      "step": 7831
    },
    {
      "epoch": 3.0321331784746417,
      "grad_norm": 12.031983375549316,
      "learning_rate": 7.742074246139288e-06,
      "loss": 0.7468,
      "step": 7832
    },
    {
      "epoch": 3.032520325203252,
      "grad_norm": 83.05596923828125,
      "learning_rate": 7.741644083107497e-06,
      "loss": 0.4299,
      "step": 7833
    },
    {
      "epoch": 3.0329074719318623,
      "grad_norm": 27.336305618286133,
      "learning_rate": 7.74121392007571e-06,
      "loss": 2.8182,
      "step": 7834
    },
    {
      "epoch": 3.0332946186604723,
      "grad_norm": 22.25367546081543,
      "learning_rate": 7.74078375704392e-06,
      "loss": 1.2595,
      "step": 7835
    },
    {
      "epoch": 3.0336817653890824,
      "grad_norm": 13.430075645446777,
      "learning_rate": 7.740353594012132e-06,
      "loss": 0.8254,
      "step": 7836
    },
    {
      "epoch": 3.0340689121176925,
      "grad_norm": 16.9909610748291,
      "learning_rate": 7.739923430980341e-06,
      "loss": 1.4018,
      "step": 7837
    },
    {
      "epoch": 3.0344560588463025,
      "grad_norm": 22.644468307495117,
      "learning_rate": 7.739493267948553e-06,
      "loss": 0.9652,
      "step": 7838
    },
    {
      "epoch": 3.034843205574913,
      "grad_norm": 24.44171905517578,
      "learning_rate": 7.739063104916764e-06,
      "loss": 1.1876,
      "step": 7839
    },
    {
      "epoch": 3.035230352303523,
      "grad_norm": 25.095932006835938,
      "learning_rate": 7.738632941884976e-06,
      "loss": 1.5735,
      "step": 7840
    },
    {
      "epoch": 3.035617499032133,
      "grad_norm": 160.271728515625,
      "learning_rate": 7.738202778853185e-06,
      "loss": 1.8124,
      "step": 7841
    },
    {
      "epoch": 3.0360046457607432,
      "grad_norm": 26.57893180847168,
      "learning_rate": 7.737772615821397e-06,
      "loss": 1.9021,
      "step": 7842
    },
    {
      "epoch": 3.0363917924893533,
      "grad_norm": 48.289100646972656,
      "learning_rate": 7.737342452789608e-06,
      "loss": 2.3134,
      "step": 7843
    },
    {
      "epoch": 3.036778939217964,
      "grad_norm": 14.8665189743042,
      "learning_rate": 7.736912289757818e-06,
      "loss": 1.2798,
      "step": 7844
    },
    {
      "epoch": 3.037166085946574,
      "grad_norm": 25.080751419067383,
      "learning_rate": 7.73648212672603e-06,
      "loss": 1.7046,
      "step": 7845
    },
    {
      "epoch": 3.037553232675184,
      "grad_norm": 12.942913055419922,
      "learning_rate": 7.73605196369424e-06,
      "loss": 1.0367,
      "step": 7846
    },
    {
      "epoch": 3.037940379403794,
      "grad_norm": 11.491679191589355,
      "learning_rate": 7.735621800662452e-06,
      "loss": 0.9885,
      "step": 7847
    },
    {
      "epoch": 3.038327526132404,
      "grad_norm": 43.54719924926758,
      "learning_rate": 7.735191637630662e-06,
      "loss": 1.3811,
      "step": 7848
    },
    {
      "epoch": 3.038714672861014,
      "grad_norm": 14.41680908203125,
      "learning_rate": 7.734761474598873e-06,
      "loss": 0.8569,
      "step": 7849
    },
    {
      "epoch": 3.0391018195896247,
      "grad_norm": 48.195796966552734,
      "learning_rate": 7.734331311567085e-06,
      "loss": 1.5699,
      "step": 7850
    },
    {
      "epoch": 3.0394889663182347,
      "grad_norm": 41.7444953918457,
      "learning_rate": 7.733901148535296e-06,
      "loss": 2.0099,
      "step": 7851
    },
    {
      "epoch": 3.039876113046845,
      "grad_norm": 6.489053249359131,
      "learning_rate": 7.733470985503506e-06,
      "loss": 0.1189,
      "step": 7852
    },
    {
      "epoch": 3.040263259775455,
      "grad_norm": 39.46516418457031,
      "learning_rate": 7.733040822471717e-06,
      "loss": 2.198,
      "step": 7853
    },
    {
      "epoch": 3.040650406504065,
      "grad_norm": 11.006874084472656,
      "learning_rate": 7.732610659439929e-06,
      "loss": 0.6264,
      "step": 7854
    },
    {
      "epoch": 3.041037553232675,
      "grad_norm": 28.18151092529297,
      "learning_rate": 7.73218049640814e-06,
      "loss": 1.1615,
      "step": 7855
    },
    {
      "epoch": 3.0414246999612855,
      "grad_norm": 66.18220520019531,
      "learning_rate": 7.73175033337635e-06,
      "loss": 2.7414,
      "step": 7856
    },
    {
      "epoch": 3.0418118466898956,
      "grad_norm": 20.225568771362305,
      "learning_rate": 7.731320170344561e-06,
      "loss": 0.6955,
      "step": 7857
    },
    {
      "epoch": 3.0421989934185056,
      "grad_norm": 42.96414566040039,
      "learning_rate": 7.730890007312773e-06,
      "loss": 1.5363,
      "step": 7858
    },
    {
      "epoch": 3.0425861401471157,
      "grad_norm": 17.355859756469727,
      "learning_rate": 7.730459844280982e-06,
      "loss": 1.3407,
      "step": 7859
    },
    {
      "epoch": 3.0429732868757258,
      "grad_norm": 8.672869682312012,
      "learning_rate": 7.730029681249194e-06,
      "loss": 0.4567,
      "step": 7860
    },
    {
      "epoch": 3.043360433604336,
      "grad_norm": 19.29083251953125,
      "learning_rate": 7.729599518217405e-06,
      "loss": 1.5089,
      "step": 7861
    },
    {
      "epoch": 3.0437475803329463,
      "grad_norm": 19.032962799072266,
      "learning_rate": 7.729169355185617e-06,
      "loss": 1.4743,
      "step": 7862
    },
    {
      "epoch": 3.0441347270615564,
      "grad_norm": 9.869587898254395,
      "learning_rate": 7.728739192153826e-06,
      "loss": 0.5869,
      "step": 7863
    },
    {
      "epoch": 3.0445218737901665,
      "grad_norm": 100.7891845703125,
      "learning_rate": 7.728309029122038e-06,
      "loss": 2.4242,
      "step": 7864
    },
    {
      "epoch": 3.0449090205187765,
      "grad_norm": 2.7217655181884766,
      "learning_rate": 7.72787886609025e-06,
      "loss": 0.0868,
      "step": 7865
    },
    {
      "epoch": 3.0452961672473866,
      "grad_norm": 9.230437278747559,
      "learning_rate": 7.72744870305846e-06,
      "loss": 0.5622,
      "step": 7866
    },
    {
      "epoch": 3.045683313975997,
      "grad_norm": 11.11642074584961,
      "learning_rate": 7.72701854002667e-06,
      "loss": 1.4286,
      "step": 7867
    },
    {
      "epoch": 3.046070460704607,
      "grad_norm": 43.193359375,
      "learning_rate": 7.726588376994882e-06,
      "loss": 1.5937,
      "step": 7868
    },
    {
      "epoch": 3.0464576074332173,
      "grad_norm": 12.562929153442383,
      "learning_rate": 7.726158213963093e-06,
      "loss": 0.6079,
      "step": 7869
    },
    {
      "epoch": 3.0468447541618273,
      "grad_norm": 25.912681579589844,
      "learning_rate": 7.725728050931305e-06,
      "loss": 2.0935,
      "step": 7870
    },
    {
      "epoch": 3.0472319008904374,
      "grad_norm": 32.7350959777832,
      "learning_rate": 7.725297887899514e-06,
      "loss": 1.4825,
      "step": 7871
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 17.31521224975586,
      "learning_rate": 7.724867724867726e-06,
      "loss": 1.5815,
      "step": 7872
    },
    {
      "epoch": 3.048006194347658,
      "grad_norm": 16.854490280151367,
      "learning_rate": 7.724437561835937e-06,
      "loss": 1.4,
      "step": 7873
    },
    {
      "epoch": 3.048393341076268,
      "grad_norm": 8.773134231567383,
      "learning_rate": 7.724007398804147e-06,
      "loss": 1.2223,
      "step": 7874
    },
    {
      "epoch": 3.048780487804878,
      "grad_norm": 13.932432174682617,
      "learning_rate": 7.723577235772358e-06,
      "loss": 1.0533,
      "step": 7875
    },
    {
      "epoch": 3.049167634533488,
      "grad_norm": 18.896425247192383,
      "learning_rate": 7.72314707274057e-06,
      "loss": 1.3487,
      "step": 7876
    },
    {
      "epoch": 3.0495547812620982,
      "grad_norm": 20.750621795654297,
      "learning_rate": 7.722716909708781e-06,
      "loss": 1.5279,
      "step": 7877
    },
    {
      "epoch": 3.0499419279907083,
      "grad_norm": 12.297271728515625,
      "learning_rate": 7.72228674667699e-06,
      "loss": 1.149,
      "step": 7878
    },
    {
      "epoch": 3.050329074719319,
      "grad_norm": 25.08781623840332,
      "learning_rate": 7.721856583645202e-06,
      "loss": 1.9171,
      "step": 7879
    },
    {
      "epoch": 3.050716221447929,
      "grad_norm": 24.521207809448242,
      "learning_rate": 7.721426420613412e-06,
      "loss": 1.155,
      "step": 7880
    },
    {
      "epoch": 3.051103368176539,
      "grad_norm": 11.910888671875,
      "learning_rate": 7.720996257581625e-06,
      "loss": 0.8073,
      "step": 7881
    },
    {
      "epoch": 3.051490514905149,
      "grad_norm": 19.44586944580078,
      "learning_rate": 7.720566094549835e-06,
      "loss": 1.5822,
      "step": 7882
    },
    {
      "epoch": 3.051877661633759,
      "grad_norm": 17.49958610534668,
      "learning_rate": 7.720135931518046e-06,
      "loss": 1.2573,
      "step": 7883
    },
    {
      "epoch": 3.052264808362369,
      "grad_norm": 27.062416076660156,
      "learning_rate": 7.719705768486256e-06,
      "loss": 1.1638,
      "step": 7884
    },
    {
      "epoch": 3.0526519550909796,
      "grad_norm": 13.774312973022461,
      "learning_rate": 7.719275605454469e-06,
      "loss": 0.7603,
      "step": 7885
    },
    {
      "epoch": 3.0530391018195897,
      "grad_norm": 21.397377014160156,
      "learning_rate": 7.718845442422679e-06,
      "loss": 1.8186,
      "step": 7886
    },
    {
      "epoch": 3.0534262485481998,
      "grad_norm": 12.619738578796387,
      "learning_rate": 7.71841527939089e-06,
      "loss": 0.849,
      "step": 7887
    },
    {
      "epoch": 3.05381339527681,
      "grad_norm": 21.392568588256836,
      "learning_rate": 7.7179851163591e-06,
      "loss": 1.5357,
      "step": 7888
    },
    {
      "epoch": 3.05420054200542,
      "grad_norm": 15.133628845214844,
      "learning_rate": 7.717554953327311e-06,
      "loss": 0.6918,
      "step": 7889
    },
    {
      "epoch": 3.0545876887340304,
      "grad_norm": 16.173778533935547,
      "learning_rate": 7.717124790295523e-06,
      "loss": 0.7851,
      "step": 7890
    },
    {
      "epoch": 3.0549748354626405,
      "grad_norm": 18.292495727539062,
      "learning_rate": 7.716694627263734e-06,
      "loss": 2.2984,
      "step": 7891
    },
    {
      "epoch": 3.0553619821912505,
      "grad_norm": 18.466642379760742,
      "learning_rate": 7.716264464231946e-06,
      "loss": 1.2999,
      "step": 7892
    },
    {
      "epoch": 3.0557491289198606,
      "grad_norm": 28.528621673583984,
      "learning_rate": 7.715834301200155e-06,
      "loss": 3.423,
      "step": 7893
    },
    {
      "epoch": 3.0561362756484707,
      "grad_norm": 14.969815254211426,
      "learning_rate": 7.715404138168367e-06,
      "loss": 0.6533,
      "step": 7894
    },
    {
      "epoch": 3.0565234223770807,
      "grad_norm": 12.806403160095215,
      "learning_rate": 7.714973975136576e-06,
      "loss": 1.2841,
      "step": 7895
    },
    {
      "epoch": 3.0569105691056913,
      "grad_norm": 17.81599235534668,
      "learning_rate": 7.71454381210479e-06,
      "loss": 3.0002,
      "step": 7896
    },
    {
      "epoch": 3.0572977158343013,
      "grad_norm": 32.97578811645508,
      "learning_rate": 7.714113649073e-06,
      "loss": 3.0913,
      "step": 7897
    },
    {
      "epoch": 3.0576848625629114,
      "grad_norm": 11.526058197021484,
      "learning_rate": 7.71368348604121e-06,
      "loss": 1.2406,
      "step": 7898
    },
    {
      "epoch": 3.0580720092915215,
      "grad_norm": 18.40317726135254,
      "learning_rate": 7.71325332300942e-06,
      "loss": 1.0289,
      "step": 7899
    },
    {
      "epoch": 3.0584591560201315,
      "grad_norm": 18.748016357421875,
      "learning_rate": 7.712823159977633e-06,
      "loss": 1.6646,
      "step": 7900
    },
    {
      "epoch": 3.0588463027487416,
      "grad_norm": 21.661266326904297,
      "learning_rate": 7.712392996945843e-06,
      "loss": 1.8617,
      "step": 7901
    },
    {
      "epoch": 3.059233449477352,
      "grad_norm": 15.146499633789062,
      "learning_rate": 7.711962833914055e-06,
      "loss": 0.9408,
      "step": 7902
    },
    {
      "epoch": 3.059620596205962,
      "grad_norm": 16.066627502441406,
      "learning_rate": 7.711532670882264e-06,
      "loss": 1.3208,
      "step": 7903
    },
    {
      "epoch": 3.0600077429345722,
      "grad_norm": 21.960351943969727,
      "learning_rate": 7.711102507850476e-06,
      "loss": 1.1301,
      "step": 7904
    },
    {
      "epoch": 3.0603948896631823,
      "grad_norm": 16.14084815979004,
      "learning_rate": 7.710672344818687e-06,
      "loss": 1.1534,
      "step": 7905
    },
    {
      "epoch": 3.0607820363917924,
      "grad_norm": 16.88636016845703,
      "learning_rate": 7.710242181786899e-06,
      "loss": 1.4057,
      "step": 7906
    },
    {
      "epoch": 3.0611691831204024,
      "grad_norm": 27.424230575561523,
      "learning_rate": 7.709812018755108e-06,
      "loss": 1.5396,
      "step": 7907
    },
    {
      "epoch": 3.061556329849013,
      "grad_norm": 18.94650650024414,
      "learning_rate": 7.70938185572332e-06,
      "loss": 1.416,
      "step": 7908
    },
    {
      "epoch": 3.061943476577623,
      "grad_norm": 30.319210052490234,
      "learning_rate": 7.708951692691531e-06,
      "loss": 1.4827,
      "step": 7909
    },
    {
      "epoch": 3.062330623306233,
      "grad_norm": 23.72726821899414,
      "learning_rate": 7.708521529659741e-06,
      "loss": 1.864,
      "step": 7910
    },
    {
      "epoch": 3.062717770034843,
      "grad_norm": 26.733020782470703,
      "learning_rate": 7.708091366627952e-06,
      "loss": 0.9992,
      "step": 7911
    },
    {
      "epoch": 3.063104916763453,
      "grad_norm": 31.259532928466797,
      "learning_rate": 7.707661203596164e-06,
      "loss": 1.4319,
      "step": 7912
    },
    {
      "epoch": 3.0634920634920633,
      "grad_norm": 59.957305908203125,
      "learning_rate": 7.707231040564375e-06,
      "loss": 1.0157,
      "step": 7913
    },
    {
      "epoch": 3.0638792102206738,
      "grad_norm": 19.499765396118164,
      "learning_rate": 7.706800877532585e-06,
      "loss": 1.6794,
      "step": 7914
    },
    {
      "epoch": 3.064266356949284,
      "grad_norm": 24.42443084716797,
      "learning_rate": 7.706370714500796e-06,
      "loss": 1.264,
      "step": 7915
    },
    {
      "epoch": 3.064653503677894,
      "grad_norm": 10.149423599243164,
      "learning_rate": 7.705940551469008e-06,
      "loss": 1.233,
      "step": 7916
    },
    {
      "epoch": 3.065040650406504,
      "grad_norm": 18.134628295898438,
      "learning_rate": 7.705510388437219e-06,
      "loss": 1.5172,
      "step": 7917
    },
    {
      "epoch": 3.065427797135114,
      "grad_norm": 16.23931121826172,
      "learning_rate": 7.705080225405429e-06,
      "loss": 1.4501,
      "step": 7918
    },
    {
      "epoch": 3.0658149438637246,
      "grad_norm": 54.54631805419922,
      "learning_rate": 7.70465006237364e-06,
      "loss": 2.0123,
      "step": 7919
    },
    {
      "epoch": 3.0662020905923346,
      "grad_norm": 63.44896697998047,
      "learning_rate": 7.704219899341852e-06,
      "loss": 2.3749,
      "step": 7920
    },
    {
      "epoch": 3.0665892373209447,
      "grad_norm": 11.894289016723633,
      "learning_rate": 7.703789736310063e-06,
      "loss": 0.8089,
      "step": 7921
    },
    {
      "epoch": 3.0669763840495547,
      "grad_norm": 10.982605934143066,
      "learning_rate": 7.703359573278273e-06,
      "loss": 0.5715,
      "step": 7922
    },
    {
      "epoch": 3.067363530778165,
      "grad_norm": 23.628995895385742,
      "learning_rate": 7.702929410246484e-06,
      "loss": 0.9204,
      "step": 7923
    },
    {
      "epoch": 3.067750677506775,
      "grad_norm": 20.786497116088867,
      "learning_rate": 7.702499247214696e-06,
      "loss": 1.2726,
      "step": 7924
    },
    {
      "epoch": 3.0681378242353854,
      "grad_norm": 54.68044662475586,
      "learning_rate": 7.702069084182905e-06,
      "loss": 2.2986,
      "step": 7925
    },
    {
      "epoch": 3.0685249709639955,
      "grad_norm": 30.323869705200195,
      "learning_rate": 7.701638921151117e-06,
      "loss": 1.2878,
      "step": 7926
    },
    {
      "epoch": 3.0689121176926055,
      "grad_norm": 57.87139892578125,
      "learning_rate": 7.701208758119328e-06,
      "loss": 0.8759,
      "step": 7927
    },
    {
      "epoch": 3.0692992644212156,
      "grad_norm": 24.395000457763672,
      "learning_rate": 7.70077859508754e-06,
      "loss": 2.1934,
      "step": 7928
    },
    {
      "epoch": 3.0696864111498257,
      "grad_norm": 24.015579223632812,
      "learning_rate": 7.70034843205575e-06,
      "loss": 1.118,
      "step": 7929
    },
    {
      "epoch": 3.0700735578784357,
      "grad_norm": 31.9249267578125,
      "learning_rate": 7.69991826902396e-06,
      "loss": 1.1909,
      "step": 7930
    },
    {
      "epoch": 3.0704607046070462,
      "grad_norm": 36.304012298583984,
      "learning_rate": 7.69948810599217e-06,
      "loss": 1.4143,
      "step": 7931
    },
    {
      "epoch": 3.0708478513356563,
      "grad_norm": 16.414033889770508,
      "learning_rate": 7.699057942960384e-06,
      "loss": 1.207,
      "step": 7932
    },
    {
      "epoch": 3.0712349980642664,
      "grad_norm": 22.51284408569336,
      "learning_rate": 7.698627779928593e-06,
      "loss": 1.3737,
      "step": 7933
    },
    {
      "epoch": 3.0716221447928764,
      "grad_norm": 27.518444061279297,
      "learning_rate": 7.698197616896805e-06,
      "loss": 2.642,
      "step": 7934
    },
    {
      "epoch": 3.0720092915214865,
      "grad_norm": 25.1991024017334,
      "learning_rate": 7.697767453865016e-06,
      "loss": 1.8542,
      "step": 7935
    },
    {
      "epoch": 3.072396438250097,
      "grad_norm": 21.75663185119629,
      "learning_rate": 7.697337290833228e-06,
      "loss": 1.8477,
      "step": 7936
    },
    {
      "epoch": 3.072783584978707,
      "grad_norm": 20.80278968811035,
      "learning_rate": 7.696907127801437e-06,
      "loss": 1.4386,
      "step": 7937
    },
    {
      "epoch": 3.073170731707317,
      "grad_norm": 16.330007553100586,
      "learning_rate": 7.696476964769649e-06,
      "loss": 1.2831,
      "step": 7938
    },
    {
      "epoch": 3.073557878435927,
      "grad_norm": 13.919360160827637,
      "learning_rate": 7.69604680173786e-06,
      "loss": 1.0609,
      "step": 7939
    },
    {
      "epoch": 3.0739450251645373,
      "grad_norm": 37.09540939331055,
      "learning_rate": 7.69561663870607e-06,
      "loss": 1.7611,
      "step": 7940
    },
    {
      "epoch": 3.0743321718931473,
      "grad_norm": 12.842449188232422,
      "learning_rate": 7.695186475674281e-06,
      "loss": 0.7748,
      "step": 7941
    },
    {
      "epoch": 3.074719318621758,
      "grad_norm": 13.81563663482666,
      "learning_rate": 7.694756312642493e-06,
      "loss": 1.35,
      "step": 7942
    },
    {
      "epoch": 3.075106465350368,
      "grad_norm": 13.658260345458984,
      "learning_rate": 7.694326149610704e-06,
      "loss": 1.1398,
      "step": 7943
    },
    {
      "epoch": 3.075493612078978,
      "grad_norm": 24.834829330444336,
      "learning_rate": 7.693895986578914e-06,
      "loss": 2.3245,
      "step": 7944
    },
    {
      "epoch": 3.075880758807588,
      "grad_norm": 24.562166213989258,
      "learning_rate": 7.693465823547125e-06,
      "loss": 1.5328,
      "step": 7945
    },
    {
      "epoch": 3.076267905536198,
      "grad_norm": 13.79731273651123,
      "learning_rate": 7.693035660515335e-06,
      "loss": 1.464,
      "step": 7946
    },
    {
      "epoch": 3.076655052264808,
      "grad_norm": 22.973983764648438,
      "learning_rate": 7.692605497483548e-06,
      "loss": 0.5475,
      "step": 7947
    },
    {
      "epoch": 3.0770421989934187,
      "grad_norm": 13.689237594604492,
      "learning_rate": 7.692175334451758e-06,
      "loss": 1.5398,
      "step": 7948
    },
    {
      "epoch": 3.0774293457220288,
      "grad_norm": 29.702739715576172,
      "learning_rate": 7.691745171419969e-06,
      "loss": 0.7625,
      "step": 7949
    },
    {
      "epoch": 3.077816492450639,
      "grad_norm": 16.931188583374023,
      "learning_rate": 7.691315008388179e-06,
      "loss": 0.5844,
      "step": 7950
    },
    {
      "epoch": 3.078203639179249,
      "grad_norm": 37.533042907714844,
      "learning_rate": 7.69088484535639e-06,
      "loss": 1.5777,
      "step": 7951
    },
    {
      "epoch": 3.078590785907859,
      "grad_norm": 17.150026321411133,
      "learning_rate": 7.690454682324602e-06,
      "loss": 1.3402,
      "step": 7952
    },
    {
      "epoch": 3.078977932636469,
      "grad_norm": 32.433441162109375,
      "learning_rate": 7.690024519292813e-06,
      "loss": 0.8988,
      "step": 7953
    },
    {
      "epoch": 3.0793650793650795,
      "grad_norm": 27.48526954650879,
      "learning_rate": 7.689594356261023e-06,
      "loss": 1.2102,
      "step": 7954
    },
    {
      "epoch": 3.0797522260936896,
      "grad_norm": 21.232892990112305,
      "learning_rate": 7.689164193229234e-06,
      "loss": 1.7084,
      "step": 7955
    },
    {
      "epoch": 3.0801393728222997,
      "grad_norm": 30.824020385742188,
      "learning_rate": 7.688734030197446e-06,
      "loss": 2.3592,
      "step": 7956
    },
    {
      "epoch": 3.0805265195509097,
      "grad_norm": 13.951249122619629,
      "learning_rate": 7.688303867165657e-06,
      "loss": 0.7723,
      "step": 7957
    },
    {
      "epoch": 3.08091366627952,
      "grad_norm": 20.197879791259766,
      "learning_rate": 7.687873704133867e-06,
      "loss": 1.0978,
      "step": 7958
    },
    {
      "epoch": 3.08130081300813,
      "grad_norm": 13.167186737060547,
      "learning_rate": 7.687443541102078e-06,
      "loss": 1.0157,
      "step": 7959
    },
    {
      "epoch": 3.0816879597367404,
      "grad_norm": 22.02625274658203,
      "learning_rate": 7.68701337807029e-06,
      "loss": 1.3816,
      "step": 7960
    },
    {
      "epoch": 3.0820751064653504,
      "grad_norm": 29.411352157592773,
      "learning_rate": 7.6865832150385e-06,
      "loss": 1.2238,
      "step": 7961
    },
    {
      "epoch": 3.0824622531939605,
      "grad_norm": 23.623308181762695,
      "learning_rate": 7.68615305200671e-06,
      "loss": 2.1559,
      "step": 7962
    },
    {
      "epoch": 3.0828493999225706,
      "grad_norm": 32.05080795288086,
      "learning_rate": 7.685722888974922e-06,
      "loss": 1.4125,
      "step": 7963
    },
    {
      "epoch": 3.0832365466511806,
      "grad_norm": 21.339454650878906,
      "learning_rate": 7.685292725943134e-06,
      "loss": 1.0764,
      "step": 7964
    },
    {
      "epoch": 3.083623693379791,
      "grad_norm": 11.990060806274414,
      "learning_rate": 7.684862562911343e-06,
      "loss": 1.027,
      "step": 7965
    },
    {
      "epoch": 3.084010840108401,
      "grad_norm": 27.771398544311523,
      "learning_rate": 7.684432399879555e-06,
      "loss": 2.5268,
      "step": 7966
    },
    {
      "epoch": 3.0843979868370113,
      "grad_norm": 25.513484954833984,
      "learning_rate": 7.684002236847766e-06,
      "loss": 1.5756,
      "step": 7967
    },
    {
      "epoch": 3.0847851335656213,
      "grad_norm": 20.688936233520508,
      "learning_rate": 7.683572073815978e-06,
      "loss": 1.6669,
      "step": 7968
    },
    {
      "epoch": 3.0851722802942314,
      "grad_norm": 16.832395553588867,
      "learning_rate": 7.683141910784187e-06,
      "loss": 1.3054,
      "step": 7969
    },
    {
      "epoch": 3.0855594270228415,
      "grad_norm": 10.130840301513672,
      "learning_rate": 7.682711747752399e-06,
      "loss": 1.2519,
      "step": 7970
    },
    {
      "epoch": 3.085946573751452,
      "grad_norm": 50.92195129394531,
      "learning_rate": 7.68228158472061e-06,
      "loss": 1.4624,
      "step": 7971
    },
    {
      "epoch": 3.086333720480062,
      "grad_norm": 23.637706756591797,
      "learning_rate": 7.681851421688822e-06,
      "loss": 1.3275,
      "step": 7972
    },
    {
      "epoch": 3.086720867208672,
      "grad_norm": 20.81322479248047,
      "learning_rate": 7.681421258657031e-06,
      "loss": 1.3511,
      "step": 7973
    },
    {
      "epoch": 3.087108013937282,
      "grad_norm": 10.721146583557129,
      "learning_rate": 7.680991095625243e-06,
      "loss": 1.2942,
      "step": 7974
    },
    {
      "epoch": 3.0874951606658922,
      "grad_norm": 12.289082527160645,
      "learning_rate": 7.680560932593454e-06,
      "loss": 0.8052,
      "step": 7975
    },
    {
      "epoch": 3.0878823073945023,
      "grad_norm": 23.342063903808594,
      "learning_rate": 7.680130769561664e-06,
      "loss": 1.6064,
      "step": 7976
    },
    {
      "epoch": 3.088269454123113,
      "grad_norm": 23.29978370666504,
      "learning_rate": 7.679700606529875e-06,
      "loss": 0.8605,
      "step": 7977
    },
    {
      "epoch": 3.088656600851723,
      "grad_norm": 17.82806968688965,
      "learning_rate": 7.679270443498087e-06,
      "loss": 1.238,
      "step": 7978
    },
    {
      "epoch": 3.089043747580333,
      "grad_norm": 2.9196810722351074,
      "learning_rate": 7.678840280466298e-06,
      "loss": 0.0795,
      "step": 7979
    },
    {
      "epoch": 3.089430894308943,
      "grad_norm": 44.87968063354492,
      "learning_rate": 7.678410117434508e-06,
      "loss": 1.6413,
      "step": 7980
    },
    {
      "epoch": 3.089818041037553,
      "grad_norm": 19.487865447998047,
      "learning_rate": 7.67797995440272e-06,
      "loss": 1.4616,
      "step": 7981
    },
    {
      "epoch": 3.0902051877661636,
      "grad_norm": 36.60261917114258,
      "learning_rate": 7.67754979137093e-06,
      "loss": 1.8572,
      "step": 7982
    },
    {
      "epoch": 3.0905923344947737,
      "grad_norm": 22.315914154052734,
      "learning_rate": 7.677119628339142e-06,
      "loss": 1.0565,
      "step": 7983
    },
    {
      "epoch": 3.0909794812233837,
      "grad_norm": 20.24180030822754,
      "learning_rate": 7.676689465307352e-06,
      "loss": 1.6708,
      "step": 7984
    },
    {
      "epoch": 3.091366627951994,
      "grad_norm": 15.713356018066406,
      "learning_rate": 7.676259302275563e-06,
      "loss": 1.0787,
      "step": 7985
    },
    {
      "epoch": 3.091753774680604,
      "grad_norm": 16.902118682861328,
      "learning_rate": 7.675829139243775e-06,
      "loss": 1.0547,
      "step": 7986
    },
    {
      "epoch": 3.092140921409214,
      "grad_norm": 21.16485595703125,
      "learning_rate": 7.675398976211984e-06,
      "loss": 1.0444,
      "step": 7987
    },
    {
      "epoch": 3.0925280681378244,
      "grad_norm": 24.311065673828125,
      "learning_rate": 7.674968813180196e-06,
      "loss": 1.5536,
      "step": 7988
    },
    {
      "epoch": 3.0929152148664345,
      "grad_norm": 33.045658111572266,
      "learning_rate": 7.674538650148407e-06,
      "loss": 1.8108,
      "step": 7989
    },
    {
      "epoch": 3.0933023615950446,
      "grad_norm": 48.85230255126953,
      "learning_rate": 7.674108487116619e-06,
      "loss": 1.2952,
      "step": 7990
    },
    {
      "epoch": 3.0936895083236546,
      "grad_norm": 68.72792053222656,
      "learning_rate": 7.673678324084828e-06,
      "loss": 3.8074,
      "step": 7991
    },
    {
      "epoch": 3.0940766550522647,
      "grad_norm": 58.076377868652344,
      "learning_rate": 7.67324816105304e-06,
      "loss": 0.9437,
      "step": 7992
    },
    {
      "epoch": 3.0944638017808748,
      "grad_norm": 30.55199432373047,
      "learning_rate": 7.672817998021251e-06,
      "loss": 1.0612,
      "step": 7993
    },
    {
      "epoch": 3.0948509485094853,
      "grad_norm": 16.544403076171875,
      "learning_rate": 7.672387834989463e-06,
      "loss": 1.156,
      "step": 7994
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 19.568138122558594,
      "learning_rate": 7.671957671957672e-06,
      "loss": 1.3519,
      "step": 7995
    },
    {
      "epoch": 3.0956252419667054,
      "grad_norm": 27.72904396057129,
      "learning_rate": 7.671527508925884e-06,
      "loss": 0.4115,
      "step": 7996
    },
    {
      "epoch": 3.0960123886953155,
      "grad_norm": 14.825565338134766,
      "learning_rate": 7.671097345894093e-06,
      "loss": 0.9024,
      "step": 7997
    },
    {
      "epoch": 3.0963995354239255,
      "grad_norm": 38.505523681640625,
      "learning_rate": 7.670667182862306e-06,
      "loss": 2.0884,
      "step": 7998
    },
    {
      "epoch": 3.0967866821525356,
      "grad_norm": 24.295629501342773,
      "learning_rate": 7.670237019830516e-06,
      "loss": 1.7694,
      "step": 7999
    },
    {
      "epoch": 3.097173828881146,
      "grad_norm": 21.90023422241211,
      "learning_rate": 7.669806856798728e-06,
      "loss": 1.2104,
      "step": 8000
    },
    {
      "epoch": 3.097560975609756,
      "grad_norm": 37.43101501464844,
      "learning_rate": 7.669376693766937e-06,
      "loss": 1.9173,
      "step": 8001
    },
    {
      "epoch": 3.0979481223383663,
      "grad_norm": 19.889280319213867,
      "learning_rate": 7.668946530735149e-06,
      "loss": 0.8962,
      "step": 8002
    },
    {
      "epoch": 3.0983352690669763,
      "grad_norm": 35.06937789916992,
      "learning_rate": 7.66851636770336e-06,
      "loss": 1.107,
      "step": 8003
    },
    {
      "epoch": 3.0987224157955864,
      "grad_norm": 23.481611251831055,
      "learning_rate": 7.668086204671572e-06,
      "loss": 2.5434,
      "step": 8004
    },
    {
      "epoch": 3.0991095625241964,
      "grad_norm": 13.063689231872559,
      "learning_rate": 7.667656041639781e-06,
      "loss": 0.5544,
      "step": 8005
    },
    {
      "epoch": 3.099496709252807,
      "grad_norm": 23.023799896240234,
      "learning_rate": 7.667225878607993e-06,
      "loss": 0.8386,
      "step": 8006
    },
    {
      "epoch": 3.099883855981417,
      "grad_norm": 75.93403625488281,
      "learning_rate": 7.666795715576204e-06,
      "loss": 1.5025,
      "step": 8007
    },
    {
      "epoch": 3.100271002710027,
      "grad_norm": 23.70749855041504,
      "learning_rate": 7.666365552544416e-06,
      "loss": 1.6826,
      "step": 8008
    },
    {
      "epoch": 3.100658149438637,
      "grad_norm": 40.0280876159668,
      "learning_rate": 7.665935389512625e-06,
      "loss": 1.7733,
      "step": 8009
    },
    {
      "epoch": 3.1010452961672472,
      "grad_norm": 27.515409469604492,
      "learning_rate": 7.665505226480837e-06,
      "loss": 0.7897,
      "step": 8010
    },
    {
      "epoch": 3.1014324428958577,
      "grad_norm": 31.870283126831055,
      "learning_rate": 7.665075063449048e-06,
      "loss": 1.9367,
      "step": 8011
    },
    {
      "epoch": 3.101819589624468,
      "grad_norm": 75.4875717163086,
      "learning_rate": 7.664644900417258e-06,
      "loss": 0.6817,
      "step": 8012
    },
    {
      "epoch": 3.102206736353078,
      "grad_norm": 19.062440872192383,
      "learning_rate": 7.66421473738547e-06,
      "loss": 1.789,
      "step": 8013
    },
    {
      "epoch": 3.102593883081688,
      "grad_norm": 30.945215225219727,
      "learning_rate": 7.66378457435368e-06,
      "loss": 1.783,
      "step": 8014
    },
    {
      "epoch": 3.102981029810298,
      "grad_norm": 40.07442855834961,
      "learning_rate": 7.663354411321892e-06,
      "loss": 2.0774,
      "step": 8015
    },
    {
      "epoch": 3.103368176538908,
      "grad_norm": 18.67133331298828,
      "learning_rate": 7.662924248290102e-06,
      "loss": 1.5579,
      "step": 8016
    },
    {
      "epoch": 3.1037553232675186,
      "grad_norm": 64.00101470947266,
      "learning_rate": 7.662494085258313e-06,
      "loss": 1.5158,
      "step": 8017
    },
    {
      "epoch": 3.1041424699961286,
      "grad_norm": 20.45383071899414,
      "learning_rate": 7.662063922226525e-06,
      "loss": 1.5105,
      "step": 8018
    },
    {
      "epoch": 3.1045296167247387,
      "grad_norm": 24.99867057800293,
      "learning_rate": 7.661633759194736e-06,
      "loss": 1.0889,
      "step": 8019
    },
    {
      "epoch": 3.1049167634533488,
      "grad_norm": 13.414407730102539,
      "learning_rate": 7.661203596162946e-06,
      "loss": 1.1531,
      "step": 8020
    },
    {
      "epoch": 3.105303910181959,
      "grad_norm": 37.4216423034668,
      "learning_rate": 7.660773433131157e-06,
      "loss": 0.966,
      "step": 8021
    },
    {
      "epoch": 3.105691056910569,
      "grad_norm": 10.175148963928223,
      "learning_rate": 7.660343270099369e-06,
      "loss": 0.7183,
      "step": 8022
    },
    {
      "epoch": 3.1060782036391794,
      "grad_norm": 95.65585327148438,
      "learning_rate": 7.659913107067578e-06,
      "loss": 2.0323,
      "step": 8023
    },
    {
      "epoch": 3.1064653503677895,
      "grad_norm": 43.649070739746094,
      "learning_rate": 7.65948294403579e-06,
      "loss": 2.9558,
      "step": 8024
    },
    {
      "epoch": 3.1068524970963995,
      "grad_norm": 3.513536214828491,
      "learning_rate": 7.659052781004001e-06,
      "loss": 0.1101,
      "step": 8025
    },
    {
      "epoch": 3.1072396438250096,
      "grad_norm": 28.489770889282227,
      "learning_rate": 7.658622617972213e-06,
      "loss": 1.3636,
      "step": 8026
    },
    {
      "epoch": 3.1076267905536197,
      "grad_norm": 12.421858787536621,
      "learning_rate": 7.658192454940422e-06,
      "loss": 0.7516,
      "step": 8027
    },
    {
      "epoch": 3.10801393728223,
      "grad_norm": 22.7194881439209,
      "learning_rate": 7.657762291908634e-06,
      "loss": 3.5082,
      "step": 8028
    },
    {
      "epoch": 3.1084010840108403,
      "grad_norm": 30.12784767150879,
      "learning_rate": 7.657332128876845e-06,
      "loss": 1.2353,
      "step": 8029
    },
    {
      "epoch": 3.1087882307394503,
      "grad_norm": 19.24632453918457,
      "learning_rate": 7.656901965845057e-06,
      "loss": 1.8714,
      "step": 8030
    },
    {
      "epoch": 3.1091753774680604,
      "grad_norm": 21.01230812072754,
      "learning_rate": 7.656471802813266e-06,
      "loss": 0.8512,
      "step": 8031
    },
    {
      "epoch": 3.1095625241966705,
      "grad_norm": 21.401317596435547,
      "learning_rate": 7.656041639781478e-06,
      "loss": 0.6225,
      "step": 8032
    },
    {
      "epoch": 3.1099496709252805,
      "grad_norm": 70.59252166748047,
      "learning_rate": 7.655611476749689e-06,
      "loss": 1.8781,
      "step": 8033
    },
    {
      "epoch": 3.110336817653891,
      "grad_norm": 11.946953773498535,
      "learning_rate": 7.6551813137179e-06,
      "loss": 0.7098,
      "step": 8034
    },
    {
      "epoch": 3.110723964382501,
      "grad_norm": 22.640735626220703,
      "learning_rate": 7.65475115068611e-06,
      "loss": 1.1809,
      "step": 8035
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 19.460519790649414,
      "learning_rate": 7.654320987654322e-06,
      "loss": 0.7782,
      "step": 8036
    },
    {
      "epoch": 3.1114982578397212,
      "grad_norm": 20.200035095214844,
      "learning_rate": 7.653890824622533e-06,
      "loss": 1.7242,
      "step": 8037
    },
    {
      "epoch": 3.1118854045683313,
      "grad_norm": 13.246880531311035,
      "learning_rate": 7.653460661590743e-06,
      "loss": 1.0849,
      "step": 8038
    },
    {
      "epoch": 3.1122725512969414,
      "grad_norm": 23.75296974182129,
      "learning_rate": 7.653030498558954e-06,
      "loss": 1.2652,
      "step": 8039
    },
    {
      "epoch": 3.112659698025552,
      "grad_norm": 39.255714416503906,
      "learning_rate": 7.652600335527166e-06,
      "loss": 1.7132,
      "step": 8040
    },
    {
      "epoch": 3.113046844754162,
      "grad_norm": 13.805604934692383,
      "learning_rate": 7.652170172495377e-06,
      "loss": 1.0527,
      "step": 8041
    },
    {
      "epoch": 3.113433991482772,
      "grad_norm": 26.180391311645508,
      "learning_rate": 7.651740009463587e-06,
      "loss": 1.7156,
      "step": 8042
    },
    {
      "epoch": 3.113821138211382,
      "grad_norm": 7.914495468139648,
      "learning_rate": 7.651309846431798e-06,
      "loss": 0.119,
      "step": 8043
    },
    {
      "epoch": 3.114208284939992,
      "grad_norm": 26.1898136138916,
      "learning_rate": 7.65087968340001e-06,
      "loss": 1.5416,
      "step": 8044
    },
    {
      "epoch": 3.114595431668602,
      "grad_norm": 43.58442306518555,
      "learning_rate": 7.650449520368221e-06,
      "loss": 0.4323,
      "step": 8045
    },
    {
      "epoch": 3.1149825783972127,
      "grad_norm": 13.30744743347168,
      "learning_rate": 7.65001935733643e-06,
      "loss": 0.8937,
      "step": 8046
    },
    {
      "epoch": 3.1153697251258228,
      "grad_norm": 11.54301643371582,
      "learning_rate": 7.649589194304642e-06,
      "loss": 0.7072,
      "step": 8047
    },
    {
      "epoch": 3.115756871854433,
      "grad_norm": 30.165987014770508,
      "learning_rate": 7.649159031272852e-06,
      "loss": 3.0444,
      "step": 8048
    },
    {
      "epoch": 3.116144018583043,
      "grad_norm": 15.806379318237305,
      "learning_rate": 7.648728868241065e-06,
      "loss": 1.5856,
      "step": 8049
    },
    {
      "epoch": 3.116531165311653,
      "grad_norm": 16.5219783782959,
      "learning_rate": 7.648298705209275e-06,
      "loss": 1.7216,
      "step": 8050
    },
    {
      "epoch": 3.116918312040263,
      "grad_norm": 16.719026565551758,
      "learning_rate": 7.647868542177486e-06,
      "loss": 0.9902,
      "step": 8051
    },
    {
      "epoch": 3.1173054587688735,
      "grad_norm": 12.819450378417969,
      "learning_rate": 7.647438379145696e-06,
      "loss": 0.3902,
      "step": 8052
    },
    {
      "epoch": 3.1176926054974836,
      "grad_norm": 11.189939498901367,
      "learning_rate": 7.647008216113907e-06,
      "loss": 1.0046,
      "step": 8053
    },
    {
      "epoch": 3.1180797522260937,
      "grad_norm": 16.72840118408203,
      "learning_rate": 7.646578053082119e-06,
      "loss": 0.5264,
      "step": 8054
    },
    {
      "epoch": 3.1184668989547037,
      "grad_norm": 26.01786994934082,
      "learning_rate": 7.64614789005033e-06,
      "loss": 2.1175,
      "step": 8055
    },
    {
      "epoch": 3.118854045683314,
      "grad_norm": 8.472156524658203,
      "learning_rate": 7.645717727018542e-06,
      "loss": 1.0992,
      "step": 8056
    },
    {
      "epoch": 3.1192411924119243,
      "grad_norm": 15.63646125793457,
      "learning_rate": 7.645287563986751e-06,
      "loss": 1.5982,
      "step": 8057
    },
    {
      "epoch": 3.1196283391405344,
      "grad_norm": 19.821979522705078,
      "learning_rate": 7.644857400954963e-06,
      "loss": 1.6824,
      "step": 8058
    },
    {
      "epoch": 3.1200154858691445,
      "grad_norm": 32.668643951416016,
      "learning_rate": 7.644427237923172e-06,
      "loss": 2.4897,
      "step": 8059
    },
    {
      "epoch": 3.1204026325977545,
      "grad_norm": 12.2615385055542,
      "learning_rate": 7.643997074891385e-06,
      "loss": 1.0339,
      "step": 8060
    },
    {
      "epoch": 3.1207897793263646,
      "grad_norm": 15.246477127075195,
      "learning_rate": 7.643566911859595e-06,
      "loss": 1.5851,
      "step": 8061
    },
    {
      "epoch": 3.1211769260549747,
      "grad_norm": 34.159175872802734,
      "learning_rate": 7.643136748827807e-06,
      "loss": 1.8404,
      "step": 8062
    },
    {
      "epoch": 3.121564072783585,
      "grad_norm": 24.714847564697266,
      "learning_rate": 7.642706585796016e-06,
      "loss": 1.7251,
      "step": 8063
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 11.180889129638672,
      "learning_rate": 7.64227642276423e-06,
      "loss": 0.984,
      "step": 8064
    },
    {
      "epoch": 3.1223383662408053,
      "grad_norm": 12.426727294921875,
      "learning_rate": 7.64184625973244e-06,
      "loss": 1.2852,
      "step": 8065
    },
    {
      "epoch": 3.1227255129694154,
      "grad_norm": 20.182804107666016,
      "learning_rate": 7.64141609670065e-06,
      "loss": 1.7052,
      "step": 8066
    },
    {
      "epoch": 3.1231126596980254,
      "grad_norm": 12.987895011901855,
      "learning_rate": 7.64098593366886e-06,
      "loss": 0.5269,
      "step": 8067
    },
    {
      "epoch": 3.1234998064266355,
      "grad_norm": 21.218889236450195,
      "learning_rate": 7.640555770637072e-06,
      "loss": 1.8982,
      "step": 8068
    },
    {
      "epoch": 3.123886953155246,
      "grad_norm": 12.307960510253906,
      "learning_rate": 7.640125607605283e-06,
      "loss": 0.8602,
      "step": 8069
    },
    {
      "epoch": 3.124274099883856,
      "grad_norm": 58.22745132446289,
      "learning_rate": 7.639695444573495e-06,
      "loss": 1.4003,
      "step": 8070
    },
    {
      "epoch": 3.124661246612466,
      "grad_norm": 27.108518600463867,
      "learning_rate": 7.639265281541704e-06,
      "loss": 1.3856,
      "step": 8071
    },
    {
      "epoch": 3.125048393341076,
      "grad_norm": 14.389798164367676,
      "learning_rate": 7.638835118509916e-06,
      "loss": 0.4796,
      "step": 8072
    },
    {
      "epoch": 3.1254355400696863,
      "grad_norm": 20.093942642211914,
      "learning_rate": 7.638404955478127e-06,
      "loss": 1.5112,
      "step": 8073
    },
    {
      "epoch": 3.125822686798297,
      "grad_norm": 15.21730899810791,
      "learning_rate": 7.637974792446337e-06,
      "loss": 1.2089,
      "step": 8074
    },
    {
      "epoch": 3.126209833526907,
      "grad_norm": 35.10111999511719,
      "learning_rate": 7.637544629414548e-06,
      "loss": 1.7051,
      "step": 8075
    },
    {
      "epoch": 3.126596980255517,
      "grad_norm": 16.242223739624023,
      "learning_rate": 7.63711446638276e-06,
      "loss": 1.5612,
      "step": 8076
    },
    {
      "epoch": 3.126984126984127,
      "grad_norm": 15.873141288757324,
      "learning_rate": 7.636684303350971e-06,
      "loss": 1.3451,
      "step": 8077
    },
    {
      "epoch": 3.127371273712737,
      "grad_norm": 17.419139862060547,
      "learning_rate": 7.63625414031918e-06,
      "loss": 1.8289,
      "step": 8078
    },
    {
      "epoch": 3.127758420441347,
      "grad_norm": 42.507606506347656,
      "learning_rate": 7.635823977287392e-06,
      "loss": 2.4737,
      "step": 8079
    },
    {
      "epoch": 3.1281455671699576,
      "grad_norm": 25.604198455810547,
      "learning_rate": 7.635393814255604e-06,
      "loss": 0.7973,
      "step": 8080
    },
    {
      "epoch": 3.1285327138985677,
      "grad_norm": 77.28517150878906,
      "learning_rate": 7.634963651223815e-06,
      "loss": 0.3928,
      "step": 8081
    },
    {
      "epoch": 3.1289198606271778,
      "grad_norm": 10.56557846069336,
      "learning_rate": 7.634533488192025e-06,
      "loss": 1.1392,
      "step": 8082
    },
    {
      "epoch": 3.129307007355788,
      "grad_norm": 107.24596405029297,
      "learning_rate": 7.634103325160236e-06,
      "loss": 1.79,
      "step": 8083
    },
    {
      "epoch": 3.129694154084398,
      "grad_norm": 11.151923179626465,
      "learning_rate": 7.633673162128448e-06,
      "loss": 0.81,
      "step": 8084
    },
    {
      "epoch": 3.130081300813008,
      "grad_norm": 24.72348976135254,
      "learning_rate": 7.633242999096659e-06,
      "loss": 1.9114,
      "step": 8085
    },
    {
      "epoch": 3.1304684475416185,
      "grad_norm": 26.960969924926758,
      "learning_rate": 7.632812836064869e-06,
      "loss": 1.1981,
      "step": 8086
    },
    {
      "epoch": 3.1308555942702285,
      "grad_norm": 14.973930358886719,
      "learning_rate": 7.63238267303308e-06,
      "loss": 0.6752,
      "step": 8087
    },
    {
      "epoch": 3.1312427409988386,
      "grad_norm": 21.492862701416016,
      "learning_rate": 7.631952510001292e-06,
      "loss": 1.7131,
      "step": 8088
    },
    {
      "epoch": 3.1316298877274487,
      "grad_norm": 13.186579704284668,
      "learning_rate": 7.631522346969501e-06,
      "loss": 1.6474,
      "step": 8089
    },
    {
      "epoch": 3.1320170344560587,
      "grad_norm": 12.392085075378418,
      "learning_rate": 7.631092183937713e-06,
      "loss": 0.837,
      "step": 8090
    },
    {
      "epoch": 3.132404181184669,
      "grad_norm": 25.116561889648438,
      "learning_rate": 7.630662020905924e-06,
      "loss": 1.4983,
      "step": 8091
    },
    {
      "epoch": 3.1327913279132793,
      "grad_norm": 32.046417236328125,
      "learning_rate": 7.630231857874136e-06,
      "loss": 1.5719,
      "step": 8092
    },
    {
      "epoch": 3.1331784746418894,
      "grad_norm": 18.415876388549805,
      "learning_rate": 7.629801694842345e-06,
      "loss": 1.7561,
      "step": 8093
    },
    {
      "epoch": 3.1335656213704994,
      "grad_norm": 36.1720085144043,
      "learning_rate": 7.629371531810557e-06,
      "loss": 1.3433,
      "step": 8094
    },
    {
      "epoch": 3.1339527680991095,
      "grad_norm": 8.7958402633667,
      "learning_rate": 7.628941368778767e-06,
      "loss": 0.5038,
      "step": 8095
    },
    {
      "epoch": 3.1343399148277196,
      "grad_norm": 36.46335983276367,
      "learning_rate": 7.628511205746979e-06,
      "loss": 3.9817,
      "step": 8096
    },
    {
      "epoch": 3.1347270615563296,
      "grad_norm": 20.713895797729492,
      "learning_rate": 7.62808104271519e-06,
      "loss": 2.8293,
      "step": 8097
    },
    {
      "epoch": 3.13511420828494,
      "grad_norm": 34.076759338378906,
      "learning_rate": 7.627650879683401e-06,
      "loss": 2.4013,
      "step": 8098
    },
    {
      "epoch": 3.13550135501355,
      "grad_norm": 11.728053092956543,
      "learning_rate": 7.627220716651612e-06,
      "loss": 1.4369,
      "step": 8099
    },
    {
      "epoch": 3.1358885017421603,
      "grad_norm": 2.8862762451171875,
      "learning_rate": 7.626790553619823e-06,
      "loss": 0.0859,
      "step": 8100
    },
    {
      "epoch": 3.1362756484707703,
      "grad_norm": 20.08749008178711,
      "learning_rate": 7.626360390588033e-06,
      "loss": 1.2704,
      "step": 8101
    },
    {
      "epoch": 3.1366627951993804,
      "grad_norm": 25.933870315551758,
      "learning_rate": 7.625930227556244e-06,
      "loss": 1.1244,
      "step": 8102
    },
    {
      "epoch": 3.137049941927991,
      "grad_norm": 13.40147876739502,
      "learning_rate": 7.625500064524456e-06,
      "loss": 1.3895,
      "step": 8103
    },
    {
      "epoch": 3.137437088656601,
      "grad_norm": 15.469743728637695,
      "learning_rate": 7.625069901492667e-06,
      "loss": 1.3532,
      "step": 8104
    },
    {
      "epoch": 3.137824235385211,
      "grad_norm": 29.49965476989746,
      "learning_rate": 7.624639738460877e-06,
      "loss": 0.9347,
      "step": 8105
    },
    {
      "epoch": 3.138211382113821,
      "grad_norm": 29.037137985229492,
      "learning_rate": 7.624209575429088e-06,
      "loss": 1.386,
      "step": 8106
    },
    {
      "epoch": 3.138598528842431,
      "grad_norm": 55.98076248168945,
      "learning_rate": 7.6237794123973e-06,
      "loss": 1.3491,
      "step": 8107
    },
    {
      "epoch": 3.1389856755710412,
      "grad_norm": 27.884626388549805,
      "learning_rate": 7.6233492493655106e-06,
      "loss": 1.584,
      "step": 8108
    },
    {
      "epoch": 3.1393728222996518,
      "grad_norm": 13.884079933166504,
      "learning_rate": 7.622919086333721e-06,
      "loss": 0.7544,
      "step": 8109
    },
    {
      "epoch": 3.139759969028262,
      "grad_norm": 12.426885604858398,
      "learning_rate": 7.622488923301932e-06,
      "loss": 0.8578,
      "step": 8110
    },
    {
      "epoch": 3.140147115756872,
      "grad_norm": 24.85352325439453,
      "learning_rate": 7.622058760270143e-06,
      "loss": 1.1583,
      "step": 8111
    },
    {
      "epoch": 3.140534262485482,
      "grad_norm": 34.44672393798828,
      "learning_rate": 7.6216285972383545e-06,
      "loss": 1.7339,
      "step": 8112
    },
    {
      "epoch": 3.140921409214092,
      "grad_norm": 10.743924140930176,
      "learning_rate": 7.621198434206565e-06,
      "loss": 0.8078,
      "step": 8113
    },
    {
      "epoch": 3.141308555942702,
      "grad_norm": 23.118925094604492,
      "learning_rate": 7.620768271174776e-06,
      "loss": 1.1749,
      "step": 8114
    },
    {
      "epoch": 3.1416957026713126,
      "grad_norm": 23.72393798828125,
      "learning_rate": 7.620338108142987e-06,
      "loss": 2.3158,
      "step": 8115
    },
    {
      "epoch": 3.1420828493999227,
      "grad_norm": 18.67843246459961,
      "learning_rate": 7.619907945111198e-06,
      "loss": 1.3431,
      "step": 8116
    },
    {
      "epoch": 3.1424699961285327,
      "grad_norm": 14.041958808898926,
      "learning_rate": 7.619477782079408e-06,
      "loss": 0.8854,
      "step": 8117
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 17.70280647277832,
      "learning_rate": 7.61904761904762e-06,
      "loss": 0.5909,
      "step": 8118
    },
    {
      "epoch": 3.143244289585753,
      "grad_norm": 13.381769180297852,
      "learning_rate": 7.618617456015831e-06,
      "loss": 0.8216,
      "step": 8119
    },
    {
      "epoch": 3.1436314363143634,
      "grad_norm": 10.901430130004883,
      "learning_rate": 7.618187292984042e-06,
      "loss": 0.7739,
      "step": 8120
    },
    {
      "epoch": 3.1440185830429734,
      "grad_norm": 24.07718276977539,
      "learning_rate": 7.617757129952252e-06,
      "loss": 1.1333,
      "step": 8121
    },
    {
      "epoch": 3.1444057297715835,
      "grad_norm": 16.228687286376953,
      "learning_rate": 7.617326966920463e-06,
      "loss": 1.7508,
      "step": 8122
    },
    {
      "epoch": 3.1447928765001936,
      "grad_norm": 19.026430130004883,
      "learning_rate": 7.616896803888675e-06,
      "loss": 1.2531,
      "step": 8123
    },
    {
      "epoch": 3.1451800232288036,
      "grad_norm": 8.404050827026367,
      "learning_rate": 7.616466640856886e-06,
      "loss": 1.1265,
      "step": 8124
    },
    {
      "epoch": 3.1455671699574137,
      "grad_norm": 43.78986358642578,
      "learning_rate": 7.616036477825096e-06,
      "loss": 1.5861,
      "step": 8125
    },
    {
      "epoch": 3.145954316686024,
      "grad_norm": 13.940960884094238,
      "learning_rate": 7.615606314793307e-06,
      "loss": 1.0483,
      "step": 8126
    },
    {
      "epoch": 3.1463414634146343,
      "grad_norm": 12.909177780151367,
      "learning_rate": 7.615176151761519e-06,
      "loss": 0.8447,
      "step": 8127
    },
    {
      "epoch": 3.1467286101432443,
      "grad_norm": 32.80483627319336,
      "learning_rate": 7.6147459887297296e-06,
      "loss": 1.5269,
      "step": 8128
    },
    {
      "epoch": 3.1471157568718544,
      "grad_norm": 17.130558013916016,
      "learning_rate": 7.61431582569794e-06,
      "loss": 1.326,
      "step": 8129
    },
    {
      "epoch": 3.1475029036004645,
      "grad_norm": 14.438132286071777,
      "learning_rate": 7.613885662666151e-06,
      "loss": 0.726,
      "step": 8130
    },
    {
      "epoch": 3.1478900503290745,
      "grad_norm": 15.387162208557129,
      "learning_rate": 7.613455499634362e-06,
      "loss": 1.0206,
      "step": 8131
    },
    {
      "epoch": 3.148277197057685,
      "grad_norm": 14.998979568481445,
      "learning_rate": 7.613025336602573e-06,
      "loss": 1.2045,
      "step": 8132
    },
    {
      "epoch": 3.148664343786295,
      "grad_norm": 15.188918113708496,
      "learning_rate": 7.612595173570784e-06,
      "loss": 1.4512,
      "step": 8133
    },
    {
      "epoch": 3.149051490514905,
      "grad_norm": 29.51179313659668,
      "learning_rate": 7.612165010538995e-06,
      "loss": 1.9522,
      "step": 8134
    },
    {
      "epoch": 3.1494386372435152,
      "grad_norm": 8.692093849182129,
      "learning_rate": 7.611734847507206e-06,
      "loss": 1.1053,
      "step": 8135
    },
    {
      "epoch": 3.1498257839721253,
      "grad_norm": 21.6678524017334,
      "learning_rate": 7.611304684475417e-06,
      "loss": 2.1637,
      "step": 8136
    },
    {
      "epoch": 3.1502129307007354,
      "grad_norm": 10.155863761901855,
      "learning_rate": 7.610874521443627e-06,
      "loss": 0.9612,
      "step": 8137
    },
    {
      "epoch": 3.150600077429346,
      "grad_norm": 17.10270118713379,
      "learning_rate": 7.6104443584118395e-06,
      "loss": 0.8733,
      "step": 8138
    },
    {
      "epoch": 3.150987224157956,
      "grad_norm": 27.014392852783203,
      "learning_rate": 7.61001419538005e-06,
      "loss": 2.0097,
      "step": 8139
    },
    {
      "epoch": 3.151374370886566,
      "grad_norm": 9.908782005310059,
      "learning_rate": 7.609584032348261e-06,
      "loss": 0.4441,
      "step": 8140
    },
    {
      "epoch": 3.151761517615176,
      "grad_norm": 14.569330215454102,
      "learning_rate": 7.609153869316471e-06,
      "loss": 0.9045,
      "step": 8141
    },
    {
      "epoch": 3.152148664343786,
      "grad_norm": 50.486934661865234,
      "learning_rate": 7.6087237062846834e-06,
      "loss": 2.1,
      "step": 8142
    },
    {
      "epoch": 3.152535811072396,
      "grad_norm": 42.2708854675293,
      "learning_rate": 7.608293543252894e-06,
      "loss": 2.9184,
      "step": 8143
    },
    {
      "epoch": 3.1529229578010067,
      "grad_norm": 9.85094928741455,
      "learning_rate": 7.607863380221105e-06,
      "loss": 1.0991,
      "step": 8144
    },
    {
      "epoch": 3.153310104529617,
      "grad_norm": 48.360530853271484,
      "learning_rate": 7.607433217189315e-06,
      "loss": 1.4691,
      "step": 8145
    },
    {
      "epoch": 3.153697251258227,
      "grad_norm": 24.584592819213867,
      "learning_rate": 7.6070030541575266e-06,
      "loss": 0.8432,
      "step": 8146
    },
    {
      "epoch": 3.154084397986837,
      "grad_norm": 30.84156608581543,
      "learning_rate": 7.606572891125737e-06,
      "loss": 2.0945,
      "step": 8147
    },
    {
      "epoch": 3.154471544715447,
      "grad_norm": 46.399837493896484,
      "learning_rate": 7.6061427280939486e-06,
      "loss": 0.7591,
      "step": 8148
    },
    {
      "epoch": 3.1548586914440575,
      "grad_norm": 7.92901611328125,
      "learning_rate": 7.605712565062159e-06,
      "loss": 0.4167,
      "step": 8149
    },
    {
      "epoch": 3.1552458381726676,
      "grad_norm": 33.79262924194336,
      "learning_rate": 7.6052824020303705e-06,
      "loss": 1.6936,
      "step": 8150
    },
    {
      "epoch": 3.1556329849012776,
      "grad_norm": 11.220378875732422,
      "learning_rate": 7.604852238998581e-06,
      "loss": 0.6437,
      "step": 8151
    },
    {
      "epoch": 3.1560201316298877,
      "grad_norm": 12.87000846862793,
      "learning_rate": 7.604422075966792e-06,
      "loss": 0.8878,
      "step": 8152
    },
    {
      "epoch": 3.1564072783584978,
      "grad_norm": 17.790973663330078,
      "learning_rate": 7.603991912935002e-06,
      "loss": 1.6675,
      "step": 8153
    },
    {
      "epoch": 3.156794425087108,
      "grad_norm": 25.5828914642334,
      "learning_rate": 7.6035617499032145e-06,
      "loss": 1.3607,
      "step": 8154
    },
    {
      "epoch": 3.1571815718157183,
      "grad_norm": 12.278804779052734,
      "learning_rate": 7.603131586871425e-06,
      "loss": 0.5051,
      "step": 8155
    },
    {
      "epoch": 3.1575687185443284,
      "grad_norm": 19.64697265625,
      "learning_rate": 7.602701423839636e-06,
      "loss": 1.3199,
      "step": 8156
    },
    {
      "epoch": 3.1579558652729385,
      "grad_norm": 26.74726104736328,
      "learning_rate": 7.602271260807846e-06,
      "loss": 1.4561,
      "step": 8157
    },
    {
      "epoch": 3.1583430120015485,
      "grad_norm": 24.72315788269043,
      "learning_rate": 7.6018410977760585e-06,
      "loss": 2.1017,
      "step": 8158
    },
    {
      "epoch": 3.1587301587301586,
      "grad_norm": 38.178192138671875,
      "learning_rate": 7.601410934744269e-06,
      "loss": 1.9657,
      "step": 8159
    },
    {
      "epoch": 3.1591173054587687,
      "grad_norm": 70.2862319946289,
      "learning_rate": 7.60098077171248e-06,
      "loss": 0.9527,
      "step": 8160
    },
    {
      "epoch": 3.159504452187379,
      "grad_norm": 12.279751777648926,
      "learning_rate": 7.60055060868069e-06,
      "loss": 0.7811,
      "step": 8161
    },
    {
      "epoch": 3.1598915989159893,
      "grad_norm": 10.34405517578125,
      "learning_rate": 7.600120445648902e-06,
      "loss": 0.3973,
      "step": 8162
    },
    {
      "epoch": 3.1602787456445993,
      "grad_norm": 27.01772117614746,
      "learning_rate": 7.599690282617113e-06,
      "loss": 0.929,
      "step": 8163
    },
    {
      "epoch": 3.1606658923732094,
      "grad_norm": 30.367929458618164,
      "learning_rate": 7.599260119585324e-06,
      "loss": 3.8241,
      "step": 8164
    },
    {
      "epoch": 3.1610530391018195,
      "grad_norm": 96.81155395507812,
      "learning_rate": 7.598829956553534e-06,
      "loss": 1.4812,
      "step": 8165
    },
    {
      "epoch": 3.16144018583043,
      "grad_norm": 23.661291122436523,
      "learning_rate": 7.5983997935217456e-06,
      "loss": 0.8959,
      "step": 8166
    },
    {
      "epoch": 3.16182733255904,
      "grad_norm": 65.92495727539062,
      "learning_rate": 7.597969630489956e-06,
      "loss": 3.176,
      "step": 8167
    },
    {
      "epoch": 3.16221447928765,
      "grad_norm": 19.14322853088379,
      "learning_rate": 7.597539467458167e-06,
      "loss": 1.4191,
      "step": 8168
    },
    {
      "epoch": 3.16260162601626,
      "grad_norm": 22.121479034423828,
      "learning_rate": 7.597109304426378e-06,
      "loss": 1.3725,
      "step": 8169
    },
    {
      "epoch": 3.1629887727448702,
      "grad_norm": 18.92061424255371,
      "learning_rate": 7.5966791413945895e-06,
      "loss": 0.9161,
      "step": 8170
    },
    {
      "epoch": 3.1633759194734803,
      "grad_norm": 52.23710632324219,
      "learning_rate": 7.5962489783628e-06,
      "loss": 2.5728,
      "step": 8171
    },
    {
      "epoch": 3.1637630662020904,
      "grad_norm": 57.180274963378906,
      "learning_rate": 7.595818815331011e-06,
      "loss": 1.8059,
      "step": 8172
    },
    {
      "epoch": 3.164150212930701,
      "grad_norm": 15.10168170928955,
      "learning_rate": 7.595388652299221e-06,
      "loss": 1.2096,
      "step": 8173
    },
    {
      "epoch": 3.164537359659311,
      "grad_norm": 3.947101354598999,
      "learning_rate": 7.5949584892674335e-06,
      "loss": 0.1079,
      "step": 8174
    },
    {
      "epoch": 3.164924506387921,
      "grad_norm": 21.242753982543945,
      "learning_rate": 7.594528326235644e-06,
      "loss": 1.3635,
      "step": 8175
    },
    {
      "epoch": 3.165311653116531,
      "grad_norm": 23.91987419128418,
      "learning_rate": 7.594098163203855e-06,
      "loss": 0.6128,
      "step": 8176
    },
    {
      "epoch": 3.165698799845141,
      "grad_norm": 29.746244430541992,
      "learning_rate": 7.593668000172065e-06,
      "loss": 1.4755,
      "step": 8177
    },
    {
      "epoch": 3.1660859465737516,
      "grad_norm": 23.950908660888672,
      "learning_rate": 7.5932378371402775e-06,
      "loss": 2.0266,
      "step": 8178
    },
    {
      "epoch": 3.1664730933023617,
      "grad_norm": 3.907865524291992,
      "learning_rate": 7.592807674108488e-06,
      "loss": 0.2266,
      "step": 8179
    },
    {
      "epoch": 3.1668602400309718,
      "grad_norm": 28.197402954101562,
      "learning_rate": 7.592377511076699e-06,
      "loss": 3.1913,
      "step": 8180
    },
    {
      "epoch": 3.167247386759582,
      "grad_norm": 12.877382278442383,
      "learning_rate": 7.59194734804491e-06,
      "loss": 1.5619,
      "step": 8181
    },
    {
      "epoch": 3.167634533488192,
      "grad_norm": 10.011256217956543,
      "learning_rate": 7.591517185013121e-06,
      "loss": 0.5936,
      "step": 8182
    },
    {
      "epoch": 3.168021680216802,
      "grad_norm": 26.049680709838867,
      "learning_rate": 7.591087021981331e-06,
      "loss": 2.8288,
      "step": 8183
    },
    {
      "epoch": 3.1684088269454125,
      "grad_norm": 23.561687469482422,
      "learning_rate": 7.590656858949543e-06,
      "loss": 1.8566,
      "step": 8184
    },
    {
      "epoch": 3.1687959736740225,
      "grad_norm": 16.955760955810547,
      "learning_rate": 7.590226695917754e-06,
      "loss": 0.6739,
      "step": 8185
    },
    {
      "epoch": 3.1691831204026326,
      "grad_norm": 24.353445053100586,
      "learning_rate": 7.5897965328859646e-06,
      "loss": 0.8344,
      "step": 8186
    },
    {
      "epoch": 3.1695702671312427,
      "grad_norm": 116.69087982177734,
      "learning_rate": 7.589366369854175e-06,
      "loss": 1.9322,
      "step": 8187
    },
    {
      "epoch": 3.1699574138598527,
      "grad_norm": 11.969639778137207,
      "learning_rate": 7.588936206822386e-06,
      "loss": 0.7669,
      "step": 8188
    },
    {
      "epoch": 3.170344560588463,
      "grad_norm": 3.733185291290283,
      "learning_rate": 7.588506043790598e-06,
      "loss": 0.2095,
      "step": 8189
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 19.99725914001465,
      "learning_rate": 7.5880758807588085e-06,
      "loss": 1.5784,
      "step": 8190
    },
    {
      "epoch": 3.1711188540456834,
      "grad_norm": 19.404560089111328,
      "learning_rate": 7.587645717727019e-06,
      "loss": 1.9218,
      "step": 8191
    },
    {
      "epoch": 3.1715060007742935,
      "grad_norm": 30.31483268737793,
      "learning_rate": 7.58721555469523e-06,
      "loss": 0.5279,
      "step": 8192
    },
    {
      "epoch": 3.1718931475029035,
      "grad_norm": 34.67690658569336,
      "learning_rate": 7.586785391663442e-06,
      "loss": 1.6756,
      "step": 8193
    },
    {
      "epoch": 3.1722802942315136,
      "grad_norm": 48.3863410949707,
      "learning_rate": 7.5863552286316525e-06,
      "loss": 0.5519,
      "step": 8194
    },
    {
      "epoch": 3.172667440960124,
      "grad_norm": 11.460039138793945,
      "learning_rate": 7.585925065599863e-06,
      "loss": 0.6313,
      "step": 8195
    },
    {
      "epoch": 3.173054587688734,
      "grad_norm": 14.650033950805664,
      "learning_rate": 7.585494902568074e-06,
      "loss": 1.6443,
      "step": 8196
    },
    {
      "epoch": 3.1734417344173442,
      "grad_norm": 23.275991439819336,
      "learning_rate": 7.585064739536285e-06,
      "loss": 1.7643,
      "step": 8197
    },
    {
      "epoch": 3.1738288811459543,
      "grad_norm": 23.122386932373047,
      "learning_rate": 7.584634576504496e-06,
      "loss": 1.9258,
      "step": 8198
    },
    {
      "epoch": 3.1742160278745644,
      "grad_norm": 12.746049880981445,
      "learning_rate": 7.584204413472707e-06,
      "loss": 1.0504,
      "step": 8199
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 20.141084671020508,
      "learning_rate": 7.583774250440918e-06,
      "loss": 1.5561,
      "step": 8200
    },
    {
      "epoch": 3.174990321331785,
      "grad_norm": 24.93028450012207,
      "learning_rate": 7.583344087409129e-06,
      "loss": 1.185,
      "step": 8201
    },
    {
      "epoch": 3.175377468060395,
      "grad_norm": 14.521720886230469,
      "learning_rate": 7.58291392437734e-06,
      "loss": 1.5712,
      "step": 8202
    },
    {
      "epoch": 3.175764614789005,
      "grad_norm": 17.07993507385254,
      "learning_rate": 7.58248376134555e-06,
      "loss": 1.1336,
      "step": 8203
    },
    {
      "epoch": 3.176151761517615,
      "grad_norm": 16.98235321044922,
      "learning_rate": 7.582053598313761e-06,
      "loss": 0.663,
      "step": 8204
    },
    {
      "epoch": 3.176538908246225,
      "grad_norm": 17.580760955810547,
      "learning_rate": 7.581623435281973e-06,
      "loss": 0.9173,
      "step": 8205
    },
    {
      "epoch": 3.1769260549748353,
      "grad_norm": 11.00987434387207,
      "learning_rate": 7.5811932722501836e-06,
      "loss": 0.7705,
      "step": 8206
    },
    {
      "epoch": 3.1773132017034458,
      "grad_norm": 57.82788848876953,
      "learning_rate": 7.580763109218394e-06,
      "loss": 0.8582,
      "step": 8207
    },
    {
      "epoch": 3.177700348432056,
      "grad_norm": 25.032150268554688,
      "learning_rate": 7.580332946186605e-06,
      "loss": 2.1234,
      "step": 8208
    },
    {
      "epoch": 3.178087495160666,
      "grad_norm": 55.230613708496094,
      "learning_rate": 7.579902783154817e-06,
      "loss": 1.7654,
      "step": 8209
    },
    {
      "epoch": 3.178474641889276,
      "grad_norm": 39.26974868774414,
      "learning_rate": 7.5794726201230275e-06,
      "loss": 0.7797,
      "step": 8210
    },
    {
      "epoch": 3.178861788617886,
      "grad_norm": 42.710140228271484,
      "learning_rate": 7.579042457091238e-06,
      "loss": 1.8919,
      "step": 8211
    },
    {
      "epoch": 3.1792489353464966,
      "grad_norm": 28.057924270629883,
      "learning_rate": 7.578612294059449e-06,
      "loss": 1.9769,
      "step": 8212
    },
    {
      "epoch": 3.1796360820751066,
      "grad_norm": 16.262054443359375,
      "learning_rate": 7.57818213102766e-06,
      "loss": 0.8692,
      "step": 8213
    },
    {
      "epoch": 3.1800232288037167,
      "grad_norm": 26.368206024169922,
      "learning_rate": 7.5777519679958715e-06,
      "loss": 1.9832,
      "step": 8214
    },
    {
      "epoch": 3.1804103755323267,
      "grad_norm": 14.525792121887207,
      "learning_rate": 7.577321804964082e-06,
      "loss": 1.1341,
      "step": 8215
    },
    {
      "epoch": 3.180797522260937,
      "grad_norm": 31.542341232299805,
      "learning_rate": 7.576891641932293e-06,
      "loss": 1.0657,
      "step": 8216
    },
    {
      "epoch": 3.181184668989547,
      "grad_norm": 29.94996452331543,
      "learning_rate": 7.576461478900504e-06,
      "loss": 1.3548,
      "step": 8217
    },
    {
      "epoch": 3.181571815718157,
      "grad_norm": 16.598886489868164,
      "learning_rate": 7.576031315868715e-06,
      "loss": 1.5871,
      "step": 8218
    },
    {
      "epoch": 3.1819589624467675,
      "grad_norm": 22.068384170532227,
      "learning_rate": 7.575601152836925e-06,
      "loss": 1.9786,
      "step": 8219
    },
    {
      "epoch": 3.1823461091753775,
      "grad_norm": 18.664011001586914,
      "learning_rate": 7.5751709898051375e-06,
      "loss": 1.4313,
      "step": 8220
    },
    {
      "epoch": 3.1827332559039876,
      "grad_norm": 34.52147674560547,
      "learning_rate": 7.574740826773348e-06,
      "loss": 1.8261,
      "step": 8221
    },
    {
      "epoch": 3.1831204026325977,
      "grad_norm": 19.882719039916992,
      "learning_rate": 7.574310663741559e-06,
      "loss": 0.8547,
      "step": 8222
    },
    {
      "epoch": 3.1835075493612077,
      "grad_norm": 20.273456573486328,
      "learning_rate": 7.573880500709769e-06,
      "loss": 0.7935,
      "step": 8223
    },
    {
      "epoch": 3.1838946960898182,
      "grad_norm": 11.145726203918457,
      "learning_rate": 7.573450337677981e-06,
      "loss": 0.4197,
      "step": 8224
    },
    {
      "epoch": 3.1842818428184283,
      "grad_norm": 17.654251098632812,
      "learning_rate": 7.573020174646192e-06,
      "loss": 0.7747,
      "step": 8225
    },
    {
      "epoch": 3.1846689895470384,
      "grad_norm": 16.467220306396484,
      "learning_rate": 7.5725900116144026e-06,
      "loss": 0.8971,
      "step": 8226
    },
    {
      "epoch": 3.1850561362756484,
      "grad_norm": 18.104530334472656,
      "learning_rate": 7.572159848582613e-06,
      "loss": 1.2946,
      "step": 8227
    },
    {
      "epoch": 3.1854432830042585,
      "grad_norm": 14.299819946289062,
      "learning_rate": 7.5717296855508245e-06,
      "loss": 1.2597,
      "step": 8228
    },
    {
      "epoch": 3.1858304297328686,
      "grad_norm": 13.3557767868042,
      "learning_rate": 7.571299522519036e-06,
      "loss": 0.686,
      "step": 8229
    },
    {
      "epoch": 3.186217576461479,
      "grad_norm": 12.793364524841309,
      "learning_rate": 7.5708693594872465e-06,
      "loss": 0.7542,
      "step": 8230
    },
    {
      "epoch": 3.186604723190089,
      "grad_norm": 42.74821090698242,
      "learning_rate": 7.570439196455457e-06,
      "loss": 2.0286,
      "step": 8231
    },
    {
      "epoch": 3.186991869918699,
      "grad_norm": 27.256999969482422,
      "learning_rate": 7.5700090334236685e-06,
      "loss": 1.7524,
      "step": 8232
    },
    {
      "epoch": 3.1873790166473093,
      "grad_norm": 11.633880615234375,
      "learning_rate": 7.569578870391879e-06,
      "loss": 1.2484,
      "step": 8233
    },
    {
      "epoch": 3.1877661633759193,
      "grad_norm": 18.400972366333008,
      "learning_rate": 7.56914870736009e-06,
      "loss": 0.6896,
      "step": 8234
    },
    {
      "epoch": 3.1881533101045294,
      "grad_norm": 8.877699851989746,
      "learning_rate": 7.568718544328301e-06,
      "loss": 0.4078,
      "step": 8235
    },
    {
      "epoch": 3.18854045683314,
      "grad_norm": 36.507408142089844,
      "learning_rate": 7.5682883812965125e-06,
      "loss": 1.5815,
      "step": 8236
    },
    {
      "epoch": 3.18892760356175,
      "grad_norm": 28.949844360351562,
      "learning_rate": 7.567858218264723e-06,
      "loss": 1.1171,
      "step": 8237
    },
    {
      "epoch": 3.18931475029036,
      "grad_norm": 21.19791030883789,
      "learning_rate": 7.567428055232934e-06,
      "loss": 1.5266,
      "step": 8238
    },
    {
      "epoch": 3.18970189701897,
      "grad_norm": 24.245582580566406,
      "learning_rate": 7.566997892201144e-06,
      "loss": 1.7472,
      "step": 8239
    },
    {
      "epoch": 3.19008904374758,
      "grad_norm": 40.689186096191406,
      "learning_rate": 7.5665677291693564e-06,
      "loss": 2.4808,
      "step": 8240
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 16.186979293823242,
      "learning_rate": 7.566137566137567e-06,
      "loss": 0.763,
      "step": 8241
    },
    {
      "epoch": 3.1908633372048008,
      "grad_norm": 12.743781089782715,
      "learning_rate": 7.565707403105778e-06,
      "loss": 1.0708,
      "step": 8242
    },
    {
      "epoch": 3.191250483933411,
      "grad_norm": 20.432451248168945,
      "learning_rate": 7.565277240073988e-06,
      "loss": 1.0532,
      "step": 8243
    },
    {
      "epoch": 3.191637630662021,
      "grad_norm": 17.69716453552246,
      "learning_rate": 7.5648470770422e-06,
      "loss": 1.3444,
      "step": 8244
    },
    {
      "epoch": 3.192024777390631,
      "grad_norm": 40.69775390625,
      "learning_rate": 7.564416914010411e-06,
      "loss": 2.2719,
      "step": 8245
    },
    {
      "epoch": 3.192411924119241,
      "grad_norm": 12.067389488220215,
      "learning_rate": 7.5639867509786216e-06,
      "loss": 0.8015,
      "step": 8246
    },
    {
      "epoch": 3.1927990708478515,
      "grad_norm": 16.67839241027832,
      "learning_rate": 7.563556587946832e-06,
      "loss": 0.3766,
      "step": 8247
    },
    {
      "epoch": 3.1931862175764616,
      "grad_norm": 10.126531600952148,
      "learning_rate": 7.5631264249150435e-06,
      "loss": 0.8031,
      "step": 8248
    },
    {
      "epoch": 3.1935733643050717,
      "grad_norm": 12.008974075317383,
      "learning_rate": 7.562696261883254e-06,
      "loss": 1.3223,
      "step": 8249
    },
    {
      "epoch": 3.1939605110336817,
      "grad_norm": 25.474119186401367,
      "learning_rate": 7.5622660988514655e-06,
      "loss": 2.633,
      "step": 8250
    },
    {
      "epoch": 3.194347657762292,
      "grad_norm": 13.66551685333252,
      "learning_rate": 7.561835935819676e-06,
      "loss": 1.233,
      "step": 8251
    },
    {
      "epoch": 3.194734804490902,
      "grad_norm": 30.921863555908203,
      "learning_rate": 7.5614057727878875e-06,
      "loss": 1.6541,
      "step": 8252
    },
    {
      "epoch": 3.1951219512195124,
      "grad_norm": 30.829877853393555,
      "learning_rate": 7.560975609756098e-06,
      "loss": 1.0772,
      "step": 8253
    },
    {
      "epoch": 3.1955090979481224,
      "grad_norm": 15.65014362335205,
      "learning_rate": 7.560545446724309e-06,
      "loss": 1.3046,
      "step": 8254
    },
    {
      "epoch": 3.1958962446767325,
      "grad_norm": 7.940112590789795,
      "learning_rate": 7.560115283692519e-06,
      "loss": 0.3435,
      "step": 8255
    },
    {
      "epoch": 3.1962833914053426,
      "grad_norm": 15.355805397033691,
      "learning_rate": 7.5596851206607315e-06,
      "loss": 0.7824,
      "step": 8256
    },
    {
      "epoch": 3.1966705381339526,
      "grad_norm": 41.94780731201172,
      "learning_rate": 7.559254957628942e-06,
      "loss": 1.0245,
      "step": 8257
    },
    {
      "epoch": 3.197057684862563,
      "grad_norm": 55.7736930847168,
      "learning_rate": 7.558824794597153e-06,
      "loss": 2.3339,
      "step": 8258
    },
    {
      "epoch": 3.197444831591173,
      "grad_norm": 35.754539489746094,
      "learning_rate": 7.558394631565363e-06,
      "loss": 1.885,
      "step": 8259
    },
    {
      "epoch": 3.1978319783197833,
      "grad_norm": 10.636026382446289,
      "learning_rate": 7.5579644685335754e-06,
      "loss": 0.3846,
      "step": 8260
    },
    {
      "epoch": 3.1982191250483933,
      "grad_norm": 51.16673278808594,
      "learning_rate": 7.557534305501786e-06,
      "loss": 1.1694,
      "step": 8261
    },
    {
      "epoch": 3.1986062717770034,
      "grad_norm": 12.385456085205078,
      "learning_rate": 7.557104142469997e-06,
      "loss": 0.6685,
      "step": 8262
    },
    {
      "epoch": 3.1989934185056135,
      "grad_norm": 19.04910659790039,
      "learning_rate": 7.556673979438208e-06,
      "loss": 1.6952,
      "step": 8263
    },
    {
      "epoch": 3.1993805652342235,
      "grad_norm": 31.063074111938477,
      "learning_rate": 7.5562438164064186e-06,
      "loss": 1.6566,
      "step": 8264
    },
    {
      "epoch": 3.199767711962834,
      "grad_norm": 27.954620361328125,
      "learning_rate": 7.55581365337463e-06,
      "loss": 1.9745,
      "step": 8265
    },
    {
      "epoch": 3.200154858691444,
      "grad_norm": 43.58378982543945,
      "learning_rate": 7.5553834903428406e-06,
      "loss": 1.1751,
      "step": 8266
    },
    {
      "epoch": 3.200542005420054,
      "grad_norm": 15.155888557434082,
      "learning_rate": 7.554953327311052e-06,
      "loss": 1.1806,
      "step": 8267
    },
    {
      "epoch": 3.2009291521486642,
      "grad_norm": 20.45465850830078,
      "learning_rate": 7.5545231642792625e-06,
      "loss": 1.9189,
      "step": 8268
    },
    {
      "epoch": 3.2013162988772743,
      "grad_norm": 24.27654457092285,
      "learning_rate": 7.554093001247473e-06,
      "loss": 2.2854,
      "step": 8269
    },
    {
      "epoch": 3.201703445605885,
      "grad_norm": 18.130165100097656,
      "learning_rate": 7.553662838215684e-06,
      "loss": 0.7797,
      "step": 8270
    },
    {
      "epoch": 3.202090592334495,
      "grad_norm": 14.341361045837402,
      "learning_rate": 7.553232675183896e-06,
      "loss": 1.3579,
      "step": 8271
    },
    {
      "epoch": 3.202477739063105,
      "grad_norm": 28.41510772705078,
      "learning_rate": 7.5528025121521065e-06,
      "loss": 1.2821,
      "step": 8272
    },
    {
      "epoch": 3.202864885791715,
      "grad_norm": 19.774574279785156,
      "learning_rate": 7.552372349120317e-06,
      "loss": 1.3216,
      "step": 8273
    },
    {
      "epoch": 3.203252032520325,
      "grad_norm": 38.20656967163086,
      "learning_rate": 7.551942186088528e-06,
      "loss": 1.1331,
      "step": 8274
    },
    {
      "epoch": 3.203639179248935,
      "grad_norm": 26.23871612548828,
      "learning_rate": 7.55151202305674e-06,
      "loss": 1.023,
      "step": 8275
    },
    {
      "epoch": 3.2040263259775457,
      "grad_norm": 13.965616226196289,
      "learning_rate": 7.5510818600249505e-06,
      "loss": 0.7244,
      "step": 8276
    },
    {
      "epoch": 3.2044134727061557,
      "grad_norm": 36.78569793701172,
      "learning_rate": 7.550651696993161e-06,
      "loss": 1.1242,
      "step": 8277
    },
    {
      "epoch": 3.204800619434766,
      "grad_norm": 36.38589859008789,
      "learning_rate": 7.550221533961372e-06,
      "loss": 1.3933,
      "step": 8278
    },
    {
      "epoch": 3.205187766163376,
      "grad_norm": 61.40016174316406,
      "learning_rate": 7.549791370929583e-06,
      "loss": 2.0613,
      "step": 8279
    },
    {
      "epoch": 3.205574912891986,
      "grad_norm": 25.245540618896484,
      "learning_rate": 7.5493612078977944e-06,
      "loss": 0.9772,
      "step": 8280
    },
    {
      "epoch": 3.205962059620596,
      "grad_norm": 12.184857368469238,
      "learning_rate": 7.548931044866005e-06,
      "loss": 0.9235,
      "step": 8281
    },
    {
      "epoch": 3.2063492063492065,
      "grad_norm": 27.012971878051758,
      "learning_rate": 7.548500881834216e-06,
      "loss": 1.9925,
      "step": 8282
    },
    {
      "epoch": 3.2067363530778166,
      "grad_norm": 14.689417839050293,
      "learning_rate": 7.548070718802427e-06,
      "loss": 0.9151,
      "step": 8283
    },
    {
      "epoch": 3.2071234998064266,
      "grad_norm": 25.331134796142578,
      "learning_rate": 7.5476405557706376e-06,
      "loss": 1.7699,
      "step": 8284
    },
    {
      "epoch": 3.2075106465350367,
      "grad_norm": 28.32502555847168,
      "learning_rate": 7.547210392738848e-06,
      "loss": 1.5923,
      "step": 8285
    },
    {
      "epoch": 3.2078977932636468,
      "grad_norm": 31.279869079589844,
      "learning_rate": 7.5467802297070595e-06,
      "loss": 1.3996,
      "step": 8286
    },
    {
      "epoch": 3.2082849399922573,
      "grad_norm": 39.33530044555664,
      "learning_rate": 7.546350066675271e-06,
      "loss": 1.8941,
      "step": 8287
    },
    {
      "epoch": 3.2086720867208673,
      "grad_norm": 39.25880813598633,
      "learning_rate": 7.5459199036434815e-06,
      "loss": 1.6712,
      "step": 8288
    },
    {
      "epoch": 3.2090592334494774,
      "grad_norm": 21.626943588256836,
      "learning_rate": 7.545489740611692e-06,
      "loss": 1.0264,
      "step": 8289
    },
    {
      "epoch": 3.2094463801780875,
      "grad_norm": 130.03916931152344,
      "learning_rate": 7.545059577579903e-06,
      "loss": 2.6175,
      "step": 8290
    },
    {
      "epoch": 3.2098335269066975,
      "grad_norm": 33.56053924560547,
      "learning_rate": 7.544629414548115e-06,
      "loss": 1.1413,
      "step": 8291
    },
    {
      "epoch": 3.2102206736353076,
      "grad_norm": 52.89215087890625,
      "learning_rate": 7.5441992515163255e-06,
      "loss": 1.8744,
      "step": 8292
    },
    {
      "epoch": 3.210607820363918,
      "grad_norm": 17.966773986816406,
      "learning_rate": 7.543769088484536e-06,
      "loss": 0.7145,
      "step": 8293
    },
    {
      "epoch": 3.210994967092528,
      "grad_norm": 25.021188735961914,
      "learning_rate": 7.543338925452747e-06,
      "loss": 2.1448,
      "step": 8294
    },
    {
      "epoch": 3.2113821138211383,
      "grad_norm": 25.4528865814209,
      "learning_rate": 7.542908762420959e-06,
      "loss": 2.0868,
      "step": 8295
    },
    {
      "epoch": 3.2117692605497483,
      "grad_norm": 19.53721046447754,
      "learning_rate": 7.5424785993891695e-06,
      "loss": 1.4105,
      "step": 8296
    },
    {
      "epoch": 3.2121564072783584,
      "grad_norm": 7.6509785652160645,
      "learning_rate": 7.54204843635738e-06,
      "loss": 0.2393,
      "step": 8297
    },
    {
      "epoch": 3.2125435540069684,
      "grad_norm": 28.995664596557617,
      "learning_rate": 7.541618273325591e-06,
      "loss": 2.7753,
      "step": 8298
    },
    {
      "epoch": 3.212930700735579,
      "grad_norm": 7.306319236755371,
      "learning_rate": 7.541188110293802e-06,
      "loss": 0.4636,
      "step": 8299
    },
    {
      "epoch": 3.213317847464189,
      "grad_norm": 25.47744369506836,
      "learning_rate": 7.540757947262013e-06,
      "loss": 1.2547,
      "step": 8300
    },
    {
      "epoch": 3.213704994192799,
      "grad_norm": 19.86543083190918,
      "learning_rate": 7.540327784230224e-06,
      "loss": 1.6294,
      "step": 8301
    },
    {
      "epoch": 3.214092140921409,
      "grad_norm": 12.756292343139648,
      "learning_rate": 7.539897621198435e-06,
      "loss": 0.8797,
      "step": 8302
    },
    {
      "epoch": 3.2144792876500192,
      "grad_norm": 27.278926849365234,
      "learning_rate": 7.539467458166646e-06,
      "loss": 2.9352,
      "step": 8303
    },
    {
      "epoch": 3.2148664343786297,
      "grad_norm": 16.406702041625977,
      "learning_rate": 7.5390372951348566e-06,
      "loss": 1.6247,
      "step": 8304
    },
    {
      "epoch": 3.21525358110724,
      "grad_norm": 12.029166221618652,
      "learning_rate": 7.538607132103067e-06,
      "loss": 0.8749,
      "step": 8305
    },
    {
      "epoch": 3.21564072783585,
      "grad_norm": 23.729936599731445,
      "learning_rate": 7.538176969071279e-06,
      "loss": 0.8261,
      "step": 8306
    },
    {
      "epoch": 3.21602787456446,
      "grad_norm": 29.77798080444336,
      "learning_rate": 7.53774680603949e-06,
      "loss": 1.5705,
      "step": 8307
    },
    {
      "epoch": 3.21641502129307,
      "grad_norm": 20.744876861572266,
      "learning_rate": 7.5373166430077005e-06,
      "loss": 1.2456,
      "step": 8308
    },
    {
      "epoch": 3.21680216802168,
      "grad_norm": 29.58012580871582,
      "learning_rate": 7.536886479975911e-06,
      "loss": 1.3234,
      "step": 8309
    },
    {
      "epoch": 3.21718931475029,
      "grad_norm": 33.26478576660156,
      "learning_rate": 7.536456316944123e-06,
      "loss": 2.1881,
      "step": 8310
    },
    {
      "epoch": 3.2175764614789006,
      "grad_norm": 29.5777645111084,
      "learning_rate": 7.536026153912334e-06,
      "loss": 1.9341,
      "step": 8311
    },
    {
      "epoch": 3.2179636082075107,
      "grad_norm": 27.858745574951172,
      "learning_rate": 7.5355959908805445e-06,
      "loss": 1.8455,
      "step": 8312
    },
    {
      "epoch": 3.2183507549361208,
      "grad_norm": 14.145604133605957,
      "learning_rate": 7.535165827848755e-06,
      "loss": 1.4141,
      "step": 8313
    },
    {
      "epoch": 3.218737901664731,
      "grad_norm": 22.39262580871582,
      "learning_rate": 7.5347356648169665e-06,
      "loss": 1.3732,
      "step": 8314
    },
    {
      "epoch": 3.219125048393341,
      "grad_norm": 16.658788681030273,
      "learning_rate": 7.534305501785177e-06,
      "loss": 1.9224,
      "step": 8315
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 23.638351440429688,
      "learning_rate": 7.5338753387533885e-06,
      "loss": 1.2738,
      "step": 8316
    },
    {
      "epoch": 3.2198993418505615,
      "grad_norm": 18.007709503173828,
      "learning_rate": 7.533445175721599e-06,
      "loss": 1.6766,
      "step": 8317
    },
    {
      "epoch": 3.2202864885791715,
      "grad_norm": 18.686262130737305,
      "learning_rate": 7.5330150126898105e-06,
      "loss": 1.0555,
      "step": 8318
    },
    {
      "epoch": 3.2206736353077816,
      "grad_norm": 26.327239990234375,
      "learning_rate": 7.532584849658021e-06,
      "loss": 1.3493,
      "step": 8319
    },
    {
      "epoch": 3.2210607820363917,
      "grad_norm": 33.323402404785156,
      "learning_rate": 7.532154686626232e-06,
      "loss": 2.1884,
      "step": 8320
    },
    {
      "epoch": 3.2214479287650017,
      "grad_norm": 41.03716278076172,
      "learning_rate": 7.531724523594442e-06,
      "loss": 1.3267,
      "step": 8321
    },
    {
      "epoch": 3.2218350754936123,
      "grad_norm": 14.072835922241211,
      "learning_rate": 7.531294360562654e-06,
      "loss": 1.0193,
      "step": 8322
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 16.738914489746094,
      "learning_rate": 7.530864197530865e-06,
      "loss": 1.6114,
      "step": 8323
    },
    {
      "epoch": 3.2226093689508324,
      "grad_norm": 14.784706115722656,
      "learning_rate": 7.5304340344990756e-06,
      "loss": 0.9623,
      "step": 8324
    },
    {
      "epoch": 3.2229965156794425,
      "grad_norm": 21.528661727905273,
      "learning_rate": 7.530003871467286e-06,
      "loss": 1.4263,
      "step": 8325
    },
    {
      "epoch": 3.2233836624080525,
      "grad_norm": 15.429008483886719,
      "learning_rate": 7.529573708435498e-06,
      "loss": 1.5627,
      "step": 8326
    },
    {
      "epoch": 3.2237708091366626,
      "grad_norm": 14.011658668518066,
      "learning_rate": 7.529143545403709e-06,
      "loss": 0.9773,
      "step": 8327
    },
    {
      "epoch": 3.224157955865273,
      "grad_norm": 22.080833435058594,
      "learning_rate": 7.5287133823719195e-06,
      "loss": 0.9227,
      "step": 8328
    },
    {
      "epoch": 3.224545102593883,
      "grad_norm": 26.19350242614746,
      "learning_rate": 7.52828321934013e-06,
      "loss": 0.7957,
      "step": 8329
    },
    {
      "epoch": 3.2249322493224932,
      "grad_norm": 26.134275436401367,
      "learning_rate": 7.5278530563083415e-06,
      "loss": 1.1925,
      "step": 8330
    },
    {
      "epoch": 3.2253193960511033,
      "grad_norm": 26.932279586791992,
      "learning_rate": 7.527422893276553e-06,
      "loss": 1.5065,
      "step": 8331
    },
    {
      "epoch": 3.2257065427797134,
      "grad_norm": 6.3772430419921875,
      "learning_rate": 7.5269927302447635e-06,
      "loss": 0.3011,
      "step": 8332
    },
    {
      "epoch": 3.226093689508324,
      "grad_norm": 37.628631591796875,
      "learning_rate": 7.526562567212974e-06,
      "loss": 0.5325,
      "step": 8333
    },
    {
      "epoch": 3.226480836236934,
      "grad_norm": 10.461122512817383,
      "learning_rate": 7.5261324041811855e-06,
      "loss": 0.6602,
      "step": 8334
    },
    {
      "epoch": 3.226867982965544,
      "grad_norm": 33.42028045654297,
      "learning_rate": 7.525702241149396e-06,
      "loss": 2.4779,
      "step": 8335
    },
    {
      "epoch": 3.227255129694154,
      "grad_norm": 33.6840705871582,
      "learning_rate": 7.525272078117607e-06,
      "loss": 2.1243,
      "step": 8336
    },
    {
      "epoch": 3.227642276422764,
      "grad_norm": 25.353260040283203,
      "learning_rate": 7.524841915085818e-06,
      "loss": 1.5772,
      "step": 8337
    },
    {
      "epoch": 3.228029423151374,
      "grad_norm": 17.403888702392578,
      "learning_rate": 7.5244117520540295e-06,
      "loss": 0.9183,
      "step": 8338
    },
    {
      "epoch": 3.2284165698799847,
      "grad_norm": 28.317543029785156,
      "learning_rate": 7.52398158902224e-06,
      "loss": 1.8793,
      "step": 8339
    },
    {
      "epoch": 3.2288037166085948,
      "grad_norm": 27.805156707763672,
      "learning_rate": 7.523551425990451e-06,
      "loss": 1.5407,
      "step": 8340
    },
    {
      "epoch": 3.229190863337205,
      "grad_norm": 21.191436767578125,
      "learning_rate": 7.523121262958661e-06,
      "loss": 1.7104,
      "step": 8341
    },
    {
      "epoch": 3.229578010065815,
      "grad_norm": 83.60004425048828,
      "learning_rate": 7.522691099926873e-06,
      "loss": 1.7078,
      "step": 8342
    },
    {
      "epoch": 3.229965156794425,
      "grad_norm": 17.66618537902832,
      "learning_rate": 7.522260936895084e-06,
      "loss": 1.2985,
      "step": 8343
    },
    {
      "epoch": 3.230352303523035,
      "grad_norm": 26.979890823364258,
      "learning_rate": 7.5218307738632946e-06,
      "loss": 1.971,
      "step": 8344
    },
    {
      "epoch": 3.2307394502516456,
      "grad_norm": 21.60443115234375,
      "learning_rate": 7.521400610831506e-06,
      "loss": 1.0153,
      "step": 8345
    },
    {
      "epoch": 3.2311265969802556,
      "grad_norm": 11.552708625793457,
      "learning_rate": 7.520970447799717e-06,
      "loss": 0.7295,
      "step": 8346
    },
    {
      "epoch": 3.2315137437088657,
      "grad_norm": 34.78135681152344,
      "learning_rate": 7.520540284767928e-06,
      "loss": 1.7715,
      "step": 8347
    },
    {
      "epoch": 3.2319008904374757,
      "grad_norm": 25.05089569091797,
      "learning_rate": 7.5201101217361385e-06,
      "loss": 1.3853,
      "step": 8348
    },
    {
      "epoch": 3.232288037166086,
      "grad_norm": 21.588594436645508,
      "learning_rate": 7.51967995870435e-06,
      "loss": 1.32,
      "step": 8349
    },
    {
      "epoch": 3.2326751838946963,
      "grad_norm": 15.05005931854248,
      "learning_rate": 7.5192497956725605e-06,
      "loss": 0.7124,
      "step": 8350
    },
    {
      "epoch": 3.2330623306233064,
      "grad_norm": 14.36750316619873,
      "learning_rate": 7.518819632640771e-06,
      "loss": 1.6399,
      "step": 8351
    },
    {
      "epoch": 3.2334494773519165,
      "grad_norm": 25.866294860839844,
      "learning_rate": 7.5183894696089825e-06,
      "loss": 0.6045,
      "step": 8352
    },
    {
      "epoch": 3.2338366240805265,
      "grad_norm": 57.8668212890625,
      "learning_rate": 7.517959306577194e-06,
      "loss": 1.6171,
      "step": 8353
    },
    {
      "epoch": 3.2342237708091366,
      "grad_norm": 15.503870010375977,
      "learning_rate": 7.5175291435454045e-06,
      "loss": 0.8489,
      "step": 8354
    },
    {
      "epoch": 3.2346109175377467,
      "grad_norm": 14.14773941040039,
      "learning_rate": 7.517098980513615e-06,
      "loss": 1.0022,
      "step": 8355
    },
    {
      "epoch": 3.2349980642663567,
      "grad_norm": 25.263294219970703,
      "learning_rate": 7.516668817481826e-06,
      "loss": 1.1923,
      "step": 8356
    },
    {
      "epoch": 3.2353852109949672,
      "grad_norm": 27.207679748535156,
      "learning_rate": 7.516238654450038e-06,
      "loss": 1.3632,
      "step": 8357
    },
    {
      "epoch": 3.2357723577235773,
      "grad_norm": 35.44719696044922,
      "learning_rate": 7.5158084914182484e-06,
      "loss": 1.5427,
      "step": 8358
    },
    {
      "epoch": 3.2361595044521874,
      "grad_norm": 22.398548126220703,
      "learning_rate": 7.515378328386459e-06,
      "loss": 1.3491,
      "step": 8359
    },
    {
      "epoch": 3.2365466511807974,
      "grad_norm": 32.9311637878418,
      "learning_rate": 7.51494816535467e-06,
      "loss": 1.6324,
      "step": 8360
    },
    {
      "epoch": 3.2369337979094075,
      "grad_norm": 41.81576156616211,
      "learning_rate": 7.514518002322881e-06,
      "loss": 1.4328,
      "step": 8361
    },
    {
      "epoch": 3.237320944638018,
      "grad_norm": 35.71304702758789,
      "learning_rate": 7.514087839291092e-06,
      "loss": 0.8133,
      "step": 8362
    },
    {
      "epoch": 3.237708091366628,
      "grad_norm": 25.972021102905273,
      "learning_rate": 7.513657676259303e-06,
      "loss": 2.6219,
      "step": 8363
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 22.587350845336914,
      "learning_rate": 7.5132275132275136e-06,
      "loss": 1.5616,
      "step": 8364
    },
    {
      "epoch": 3.238482384823848,
      "grad_norm": 7.665396690368652,
      "learning_rate": 7.512797350195725e-06,
      "loss": 0.641,
      "step": 8365
    },
    {
      "epoch": 3.2388695315524583,
      "grad_norm": 24.659069061279297,
      "learning_rate": 7.5123671871639355e-06,
      "loss": 0.6872,
      "step": 8366
    },
    {
      "epoch": 3.2392566782810683,
      "grad_norm": 27.44352149963379,
      "learning_rate": 7.511937024132147e-06,
      "loss": 0.6838,
      "step": 8367
    },
    {
      "epoch": 3.239643825009679,
      "grad_norm": 19.042394638061523,
      "learning_rate": 7.5115068611003575e-06,
      "loss": 1.2846,
      "step": 8368
    },
    {
      "epoch": 3.240030971738289,
      "grad_norm": 21.689453125,
      "learning_rate": 7.511076698068569e-06,
      "loss": 0.5453,
      "step": 8369
    },
    {
      "epoch": 3.240418118466899,
      "grad_norm": 16.535120010375977,
      "learning_rate": 7.5106465350367795e-06,
      "loss": 0.749,
      "step": 8370
    },
    {
      "epoch": 3.240805265195509,
      "grad_norm": 33.16820526123047,
      "learning_rate": 7.51021637200499e-06,
      "loss": 2.1978,
      "step": 8371
    },
    {
      "epoch": 3.241192411924119,
      "grad_norm": 20.321300506591797,
      "learning_rate": 7.509786208973201e-06,
      "loss": 1.7638,
      "step": 8372
    },
    {
      "epoch": 3.241579558652729,
      "grad_norm": 21.44126319885254,
      "learning_rate": 7.509356045941413e-06,
      "loss": 1.911,
      "step": 8373
    },
    {
      "epoch": 3.2419667053813397,
      "grad_norm": 37.062347412109375,
      "learning_rate": 7.5089258829096235e-06,
      "loss": 1.569,
      "step": 8374
    },
    {
      "epoch": 3.2423538521099498,
      "grad_norm": 17.95298194885254,
      "learning_rate": 7.508495719877834e-06,
      "loss": 0.9584,
      "step": 8375
    },
    {
      "epoch": 3.24274099883856,
      "grad_norm": 52.50609588623047,
      "learning_rate": 7.508065556846045e-06,
      "loss": 0.7916,
      "step": 8376
    },
    {
      "epoch": 3.24312814556717,
      "grad_norm": 30.536745071411133,
      "learning_rate": 7.507635393814257e-06,
      "loss": 1.9872,
      "step": 8377
    },
    {
      "epoch": 3.24351529229578,
      "grad_norm": 26.901653289794922,
      "learning_rate": 7.5072052307824674e-06,
      "loss": 1.9173,
      "step": 8378
    },
    {
      "epoch": 3.2439024390243905,
      "grad_norm": 29.869964599609375,
      "learning_rate": 7.506775067750678e-06,
      "loss": 0.8971,
      "step": 8379
    },
    {
      "epoch": 3.2442895857530005,
      "grad_norm": 12.35214900970459,
      "learning_rate": 7.506344904718889e-06,
      "loss": 0.891,
      "step": 8380
    },
    {
      "epoch": 3.2446767324816106,
      "grad_norm": 18.219196319580078,
      "learning_rate": 7.5059147416871e-06,
      "loss": 0.7498,
      "step": 8381
    },
    {
      "epoch": 3.2450638792102207,
      "grad_norm": 29.46119499206543,
      "learning_rate": 7.505484578655311e-06,
      "loss": 3.5612,
      "step": 8382
    },
    {
      "epoch": 3.2454510259388307,
      "grad_norm": 24.199758529663086,
      "learning_rate": 7.505054415623522e-06,
      "loss": 1.2449,
      "step": 8383
    },
    {
      "epoch": 3.245838172667441,
      "grad_norm": 40.46146011352539,
      "learning_rate": 7.5046242525917326e-06,
      "loss": 1.4802,
      "step": 8384
    },
    {
      "epoch": 3.2462253193960513,
      "grad_norm": 53.83738708496094,
      "learning_rate": 7.504194089559944e-06,
      "loss": 1.238,
      "step": 8385
    },
    {
      "epoch": 3.2466124661246614,
      "grad_norm": 11.905050277709961,
      "learning_rate": 7.5037639265281545e-06,
      "loss": 1.1453,
      "step": 8386
    },
    {
      "epoch": 3.2469996128532714,
      "grad_norm": 44.237701416015625,
      "learning_rate": 7.503333763496365e-06,
      "loss": 1.8062,
      "step": 8387
    },
    {
      "epoch": 3.2473867595818815,
      "grad_norm": 15.40993881225586,
      "learning_rate": 7.502903600464577e-06,
      "loss": 1.1585,
      "step": 8388
    },
    {
      "epoch": 3.2477739063104916,
      "grad_norm": 21.939287185668945,
      "learning_rate": 7.502473437432788e-06,
      "loss": 0.9532,
      "step": 8389
    },
    {
      "epoch": 3.2481610530391016,
      "grad_norm": 15.863160133361816,
      "learning_rate": 7.5020432744009985e-06,
      "loss": 0.5618,
      "step": 8390
    },
    {
      "epoch": 3.248548199767712,
      "grad_norm": 46.877525329589844,
      "learning_rate": 7.501613111369209e-06,
      "loss": 0.797,
      "step": 8391
    },
    {
      "epoch": 3.248935346496322,
      "grad_norm": 19.281736373901367,
      "learning_rate": 7.501182948337421e-06,
      "loss": 0.9084,
      "step": 8392
    },
    {
      "epoch": 3.2493224932249323,
      "grad_norm": 11.93481159210205,
      "learning_rate": 7.500752785305632e-06,
      "loss": 0.6445,
      "step": 8393
    },
    {
      "epoch": 3.2497096399535423,
      "grad_norm": 15.470588684082031,
      "learning_rate": 7.5003226222738425e-06,
      "loss": 1.1702,
      "step": 8394
    },
    {
      "epoch": 3.2500967866821524,
      "grad_norm": 17.765600204467773,
      "learning_rate": 7.499892459242053e-06,
      "loss": 2.195,
      "step": 8395
    },
    {
      "epoch": 3.250483933410763,
      "grad_norm": 21.37847900390625,
      "learning_rate": 7.4994622962102645e-06,
      "loss": 1.2195,
      "step": 8396
    },
    {
      "epoch": 3.250871080139373,
      "grad_norm": 34.48820877075195,
      "learning_rate": 7.499032133178475e-06,
      "loss": 1.2388,
      "step": 8397
    },
    {
      "epoch": 3.251258226867983,
      "grad_norm": 46.30366897583008,
      "learning_rate": 7.4986019701466864e-06,
      "loss": 1.7161,
      "step": 8398
    },
    {
      "epoch": 3.251645373596593,
      "grad_norm": 20.539302825927734,
      "learning_rate": 7.498171807114897e-06,
      "loss": 0.6818,
      "step": 8399
    },
    {
      "epoch": 3.252032520325203,
      "grad_norm": 19.645267486572266,
      "learning_rate": 7.4977416440831084e-06,
      "loss": 1.7386,
      "step": 8400
    },
    {
      "epoch": 3.2524196670538132,
      "grad_norm": 24.241886138916016,
      "learning_rate": 7.497311481051319e-06,
      "loss": 3.0565,
      "step": 8401
    },
    {
      "epoch": 3.2528068137824233,
      "grad_norm": 14.085926055908203,
      "learning_rate": 7.4968813180195296e-06,
      "loss": 0.7516,
      "step": 8402
    },
    {
      "epoch": 3.253193960511034,
      "grad_norm": 16.817638397216797,
      "learning_rate": 7.496451154987741e-06,
      "loss": 1.3917,
      "step": 8403
    },
    {
      "epoch": 3.253581107239644,
      "grad_norm": 19.179750442504883,
      "learning_rate": 7.496020991955952e-06,
      "loss": 0.7138,
      "step": 8404
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 22.327880859375,
      "learning_rate": 7.495590828924163e-06,
      "loss": 1.6389,
      "step": 8405
    },
    {
      "epoch": 3.254355400696864,
      "grad_norm": 61.142913818359375,
      "learning_rate": 7.4951606658923735e-06,
      "loss": 1.3978,
      "step": 8406
    },
    {
      "epoch": 3.254742547425474,
      "grad_norm": 14.669319152832031,
      "learning_rate": 7.494730502860584e-06,
      "loss": 0.8557,
      "step": 8407
    },
    {
      "epoch": 3.2551296941540846,
      "grad_norm": 28.804367065429688,
      "learning_rate": 7.494300339828796e-06,
      "loss": 1.5488,
      "step": 8408
    },
    {
      "epoch": 3.2555168408826947,
      "grad_norm": 43.265960693359375,
      "learning_rate": 7.493870176797007e-06,
      "loss": 2.0239,
      "step": 8409
    },
    {
      "epoch": 3.2559039876113047,
      "grad_norm": 18.54355812072754,
      "learning_rate": 7.4934400137652175e-06,
      "loss": 1.1237,
      "step": 8410
    },
    {
      "epoch": 3.256291134339915,
      "grad_norm": 25.322851181030273,
      "learning_rate": 7.493009850733428e-06,
      "loss": 1.4992,
      "step": 8411
    },
    {
      "epoch": 3.256678281068525,
      "grad_norm": 13.954813957214355,
      "learning_rate": 7.4925796877016395e-06,
      "loss": 0.7067,
      "step": 8412
    },
    {
      "epoch": 3.257065427797135,
      "grad_norm": 35.700435638427734,
      "learning_rate": 7.492149524669851e-06,
      "loss": 1.8746,
      "step": 8413
    },
    {
      "epoch": 3.2574525745257454,
      "grad_norm": 34.328407287597656,
      "learning_rate": 7.4917193616380615e-06,
      "loss": 1.1902,
      "step": 8414
    },
    {
      "epoch": 3.2578397212543555,
      "grad_norm": 26.21248435974121,
      "learning_rate": 7.491289198606272e-06,
      "loss": 1.801,
      "step": 8415
    },
    {
      "epoch": 3.2582268679829656,
      "grad_norm": 17.38834571838379,
      "learning_rate": 7.4908590355744835e-06,
      "loss": 1.2388,
      "step": 8416
    },
    {
      "epoch": 3.2586140147115756,
      "grad_norm": 17.331153869628906,
      "learning_rate": 7.490428872542694e-06,
      "loss": 1.0674,
      "step": 8417
    },
    {
      "epoch": 3.2590011614401857,
      "grad_norm": 22.99005889892578,
      "learning_rate": 7.4899987095109054e-06,
      "loss": 2.0479,
      "step": 8418
    },
    {
      "epoch": 3.2593883081687958,
      "grad_norm": 38.740867614746094,
      "learning_rate": 7.489568546479116e-06,
      "loss": 1.8079,
      "step": 8419
    },
    {
      "epoch": 3.2597754548974063,
      "grad_norm": 11.296636581420898,
      "learning_rate": 7.4891383834473274e-06,
      "loss": 0.6889,
      "step": 8420
    },
    {
      "epoch": 3.2601626016260163,
      "grad_norm": 14.413114547729492,
      "learning_rate": 7.488708220415538e-06,
      "loss": 1.124,
      "step": 8421
    },
    {
      "epoch": 3.2605497483546264,
      "grad_norm": 23.038116455078125,
      "learning_rate": 7.4882780573837486e-06,
      "loss": 2.6765,
      "step": 8422
    },
    {
      "epoch": 3.2609368950832365,
      "grad_norm": 25.756763458251953,
      "learning_rate": 7.487847894351959e-06,
      "loss": 1.6404,
      "step": 8423
    },
    {
      "epoch": 3.2613240418118465,
      "grad_norm": 39.779579162597656,
      "learning_rate": 7.487417731320171e-06,
      "loss": 2.7241,
      "step": 8424
    },
    {
      "epoch": 3.261711188540457,
      "grad_norm": 10.53752326965332,
      "learning_rate": 7.486987568288382e-06,
      "loss": 0.4528,
      "step": 8425
    },
    {
      "epoch": 3.262098335269067,
      "grad_norm": 37.952613830566406,
      "learning_rate": 7.4865574052565925e-06,
      "loss": 1.5779,
      "step": 8426
    },
    {
      "epoch": 3.262485481997677,
      "grad_norm": 32.601539611816406,
      "learning_rate": 7.486127242224804e-06,
      "loss": 1.9519,
      "step": 8427
    },
    {
      "epoch": 3.2628726287262872,
      "grad_norm": 15.449882507324219,
      "learning_rate": 7.485697079193015e-06,
      "loss": 1.2978,
      "step": 8428
    },
    {
      "epoch": 3.2632597754548973,
      "grad_norm": 11.528524398803711,
      "learning_rate": 7.485266916161226e-06,
      "loss": 1.1877,
      "step": 8429
    },
    {
      "epoch": 3.2636469221835074,
      "grad_norm": 31.332656860351562,
      "learning_rate": 7.4848367531294365e-06,
      "loss": 0.8692,
      "step": 8430
    },
    {
      "epoch": 3.2640340689121174,
      "grad_norm": 24.84908676147461,
      "learning_rate": 7.484406590097648e-06,
      "loss": 1.4022,
      "step": 8431
    },
    {
      "epoch": 3.264421215640728,
      "grad_norm": 13.260645866394043,
      "learning_rate": 7.4839764270658585e-06,
      "loss": 0.9434,
      "step": 8432
    },
    {
      "epoch": 3.264808362369338,
      "grad_norm": 11.211431503295898,
      "learning_rate": 7.483546264034069e-06,
      "loss": 0.6214,
      "step": 8433
    },
    {
      "epoch": 3.265195509097948,
      "grad_norm": 15.866742134094238,
      "learning_rate": 7.4831161010022805e-06,
      "loss": 1.1977,
      "step": 8434
    },
    {
      "epoch": 3.265582655826558,
      "grad_norm": 28.239482879638672,
      "learning_rate": 7.482685937970492e-06,
      "loss": 1.7247,
      "step": 8435
    },
    {
      "epoch": 3.2659698025551682,
      "grad_norm": 18.776203155517578,
      "learning_rate": 7.4822557749387025e-06,
      "loss": 1.0724,
      "step": 8436
    },
    {
      "epoch": 3.2663569492837787,
      "grad_norm": 31.346662521362305,
      "learning_rate": 7.481825611906913e-06,
      "loss": 2.0913,
      "step": 8437
    },
    {
      "epoch": 3.266744096012389,
      "grad_norm": 11.203535079956055,
      "learning_rate": 7.481395448875124e-06,
      "loss": 0.412,
      "step": 8438
    },
    {
      "epoch": 3.267131242740999,
      "grad_norm": 17.013151168823242,
      "learning_rate": 7.480965285843336e-06,
      "loss": 0.9449,
      "step": 8439
    },
    {
      "epoch": 3.267518389469609,
      "grad_norm": 10.637594223022461,
      "learning_rate": 7.4805351228115464e-06,
      "loss": 0.8403,
      "step": 8440
    },
    {
      "epoch": 3.267905536198219,
      "grad_norm": 17.813657760620117,
      "learning_rate": 7.480104959779757e-06,
      "loss": 1.3178,
      "step": 8441
    },
    {
      "epoch": 3.2682926829268295,
      "grad_norm": 39.26443862915039,
      "learning_rate": 7.4796747967479676e-06,
      "loss": 1.0929,
      "step": 8442
    },
    {
      "epoch": 3.2686798296554396,
      "grad_norm": 15.403000831604004,
      "learning_rate": 7.47924463371618e-06,
      "loss": 0.9029,
      "step": 8443
    },
    {
      "epoch": 3.2690669763840496,
      "grad_norm": 38.8190803527832,
      "learning_rate": 7.47881447068439e-06,
      "loss": 2.041,
      "step": 8444
    },
    {
      "epoch": 3.2694541231126597,
      "grad_norm": 16.2153263092041,
      "learning_rate": 7.478384307652601e-06,
      "loss": 1.374,
      "step": 8445
    },
    {
      "epoch": 3.2698412698412698,
      "grad_norm": 22.608341217041016,
      "learning_rate": 7.4779541446208115e-06,
      "loss": 1.5352,
      "step": 8446
    },
    {
      "epoch": 3.27022841656988,
      "grad_norm": 13.981579780578613,
      "learning_rate": 7.477523981589023e-06,
      "loss": 1.2118,
      "step": 8447
    },
    {
      "epoch": 3.27061556329849,
      "grad_norm": 29.175464630126953,
      "learning_rate": 7.4770938185572335e-06,
      "loss": 1.4013,
      "step": 8448
    },
    {
      "epoch": 3.2710027100271004,
      "grad_norm": 37.20111846923828,
      "learning_rate": 7.476663655525445e-06,
      "loss": 1.6292,
      "step": 8449
    },
    {
      "epoch": 3.2713898567557105,
      "grad_norm": 34.73200988769531,
      "learning_rate": 7.4762334924936555e-06,
      "loss": 1.9278,
      "step": 8450
    },
    {
      "epoch": 3.2717770034843205,
      "grad_norm": 46.040802001953125,
      "learning_rate": 7.475803329461867e-06,
      "loss": 2.0701,
      "step": 8451
    },
    {
      "epoch": 3.2721641502129306,
      "grad_norm": 41.56201171875,
      "learning_rate": 7.4753731664300775e-06,
      "loss": 2.761,
      "step": 8452
    },
    {
      "epoch": 3.2725512969415407,
      "grad_norm": 39.941162109375,
      "learning_rate": 7.474943003398288e-06,
      "loss": 2.0344,
      "step": 8453
    },
    {
      "epoch": 3.272938443670151,
      "grad_norm": 21.967897415161133,
      "learning_rate": 7.4745128403664995e-06,
      "loss": 1.7119,
      "step": 8454
    },
    {
      "epoch": 3.2733255903987613,
      "grad_norm": 29.756540298461914,
      "learning_rate": 7.474082677334711e-06,
      "loss": 1.3999,
      "step": 8455
    },
    {
      "epoch": 3.2737127371273713,
      "grad_norm": 18.28689193725586,
      "learning_rate": 7.4736525143029215e-06,
      "loss": 0.9781,
      "step": 8456
    },
    {
      "epoch": 3.2740998838559814,
      "grad_norm": 18.281476974487305,
      "learning_rate": 7.473222351271132e-06,
      "loss": 1.0372,
      "step": 8457
    },
    {
      "epoch": 3.2744870305845915,
      "grad_norm": 33.680152893066406,
      "learning_rate": 7.472792188239343e-06,
      "loss": 1.9186,
      "step": 8458
    },
    {
      "epoch": 3.2748741773132015,
      "grad_norm": 14.444271087646484,
      "learning_rate": 7.472362025207555e-06,
      "loss": 1.2415,
      "step": 8459
    },
    {
      "epoch": 3.275261324041812,
      "grad_norm": 25.188711166381836,
      "learning_rate": 7.471931862175765e-06,
      "loss": 1.4562,
      "step": 8460
    },
    {
      "epoch": 3.275648470770422,
      "grad_norm": 20.47494125366211,
      "learning_rate": 7.471501699143976e-06,
      "loss": 0.7143,
      "step": 8461
    },
    {
      "epoch": 3.276035617499032,
      "grad_norm": 21.235519409179688,
      "learning_rate": 7.4710715361121866e-06,
      "loss": 1.3776,
      "step": 8462
    },
    {
      "epoch": 3.2764227642276422,
      "grad_norm": 27.206806182861328,
      "learning_rate": 7.470641373080398e-06,
      "loss": 1.3998,
      "step": 8463
    },
    {
      "epoch": 3.2768099109562523,
      "grad_norm": 26.54330062866211,
      "learning_rate": 7.470211210048609e-06,
      "loss": 1.2213,
      "step": 8464
    },
    {
      "epoch": 3.2771970576848624,
      "grad_norm": 22.113197326660156,
      "learning_rate": 7.46978104701682e-06,
      "loss": 1.9552,
      "step": 8465
    },
    {
      "epoch": 3.277584204413473,
      "grad_norm": 21.163387298583984,
      "learning_rate": 7.4693508839850305e-06,
      "loss": 1.0881,
      "step": 8466
    },
    {
      "epoch": 3.277971351142083,
      "grad_norm": 23.38892936706543,
      "learning_rate": 7.468920720953242e-06,
      "loss": 1.3184,
      "step": 8467
    },
    {
      "epoch": 3.278358497870693,
      "grad_norm": 32.58964157104492,
      "learning_rate": 7.4684905579214525e-06,
      "loss": 1.0647,
      "step": 8468
    },
    {
      "epoch": 3.278745644599303,
      "grad_norm": 11.120036125183105,
      "learning_rate": 7.468060394889663e-06,
      "loss": 1.1323,
      "step": 8469
    },
    {
      "epoch": 3.279132791327913,
      "grad_norm": 16.365772247314453,
      "learning_rate": 7.467630231857875e-06,
      "loss": 1.5867,
      "step": 8470
    },
    {
      "epoch": 3.2795199380565236,
      "grad_norm": 26.241107940673828,
      "learning_rate": 7.467200068826086e-06,
      "loss": 1.3741,
      "step": 8471
    },
    {
      "epoch": 3.2799070847851337,
      "grad_norm": 13.45801067352295,
      "learning_rate": 7.4667699057942965e-06,
      "loss": 1.2131,
      "step": 8472
    },
    {
      "epoch": 3.2802942315137438,
      "grad_norm": 12.580692291259766,
      "learning_rate": 7.466339742762507e-06,
      "loss": 1.3811,
      "step": 8473
    },
    {
      "epoch": 3.280681378242354,
      "grad_norm": 24.075654983520508,
      "learning_rate": 7.465909579730719e-06,
      "loss": 0.7932,
      "step": 8474
    },
    {
      "epoch": 3.281068524970964,
      "grad_norm": 29.50515365600586,
      "learning_rate": 7.46547941669893e-06,
      "loss": 1.9438,
      "step": 8475
    },
    {
      "epoch": 3.281455671699574,
      "grad_norm": 24.030452728271484,
      "learning_rate": 7.4650492536671405e-06,
      "loss": 2.7651,
      "step": 8476
    },
    {
      "epoch": 3.281842818428184,
      "grad_norm": 15.991263389587402,
      "learning_rate": 7.464619090635351e-06,
      "loss": 0.5837,
      "step": 8477
    },
    {
      "epoch": 3.2822299651567945,
      "grad_norm": 17.244380950927734,
      "learning_rate": 7.4641889276035624e-06,
      "loss": 0.7435,
      "step": 8478
    },
    {
      "epoch": 3.2826171118854046,
      "grad_norm": 25.837940216064453,
      "learning_rate": 7.463758764571774e-06,
      "loss": 1.631,
      "step": 8479
    },
    {
      "epoch": 3.2830042586140147,
      "grad_norm": 11.208748817443848,
      "learning_rate": 7.463328601539984e-06,
      "loss": 0.7123,
      "step": 8480
    },
    {
      "epoch": 3.2833914053426247,
      "grad_norm": 32.405113220214844,
      "learning_rate": 7.462898438508195e-06,
      "loss": 1.5568,
      "step": 8481
    },
    {
      "epoch": 3.283778552071235,
      "grad_norm": 43.4219856262207,
      "learning_rate": 7.462468275476406e-06,
      "loss": 1.1779,
      "step": 8482
    },
    {
      "epoch": 3.2841656987998453,
      "grad_norm": 9.906697273254395,
      "learning_rate": 7.462038112444617e-06,
      "loss": 0.1948,
      "step": 8483
    },
    {
      "epoch": 3.2845528455284554,
      "grad_norm": 24.310577392578125,
      "learning_rate": 7.4616079494128275e-06,
      "loss": 0.9145,
      "step": 8484
    },
    {
      "epoch": 3.2849399922570655,
      "grad_norm": 24.53244972229004,
      "learning_rate": 7.461177786381039e-06,
      "loss": 1.4162,
      "step": 8485
    },
    {
      "epoch": 3.2853271389856755,
      "grad_norm": 25.008846282958984,
      "learning_rate": 7.46074762334925e-06,
      "loss": 1.3308,
      "step": 8486
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 12.52867317199707,
      "learning_rate": 7.460317460317461e-06,
      "loss": 0.5939,
      "step": 8487
    },
    {
      "epoch": 3.286101432442896,
      "grad_norm": 18.27109146118164,
      "learning_rate": 7.4598872972856715e-06,
      "loss": 1.7748,
      "step": 8488
    },
    {
      "epoch": 3.286488579171506,
      "grad_norm": 14.437108039855957,
      "learning_rate": 7.459457134253882e-06,
      "loss": 0.6388,
      "step": 8489
    },
    {
      "epoch": 3.2868757259001162,
      "grad_norm": 24.725662231445312,
      "learning_rate": 7.459026971222094e-06,
      "loss": 1.6118,
      "step": 8490
    },
    {
      "epoch": 3.2872628726287263,
      "grad_norm": 28.131052017211914,
      "learning_rate": 7.458596808190305e-06,
      "loss": 1.2934,
      "step": 8491
    },
    {
      "epoch": 3.2876500193573364,
      "grad_norm": 30.889286041259766,
      "learning_rate": 7.4581666451585155e-06,
      "loss": 1.1372,
      "step": 8492
    },
    {
      "epoch": 3.2880371660859464,
      "grad_norm": 20.32640266418457,
      "learning_rate": 7.457736482126726e-06,
      "loss": 2.1767,
      "step": 8493
    },
    {
      "epoch": 3.2884243128145565,
      "grad_norm": 33.673221588134766,
      "learning_rate": 7.457306319094938e-06,
      "loss": 3.4866,
      "step": 8494
    },
    {
      "epoch": 3.288811459543167,
      "grad_norm": 55.58403015136719,
      "learning_rate": 7.456876156063149e-06,
      "loss": 2.0567,
      "step": 8495
    },
    {
      "epoch": 3.289198606271777,
      "grad_norm": 19.73323631286621,
      "learning_rate": 7.4564459930313594e-06,
      "loss": 0.9623,
      "step": 8496
    },
    {
      "epoch": 3.289585753000387,
      "grad_norm": 24.508625030517578,
      "learning_rate": 7.45601582999957e-06,
      "loss": 0.6729,
      "step": 8497
    },
    {
      "epoch": 3.289972899728997,
      "grad_norm": 27.966899871826172,
      "learning_rate": 7.4555856669677814e-06,
      "loss": 1.3528,
      "step": 8498
    },
    {
      "epoch": 3.2903600464576073,
      "grad_norm": 45.867286682128906,
      "learning_rate": 7.455155503935992e-06,
      "loss": 2.0303,
      "step": 8499
    },
    {
      "epoch": 3.290747193186218,
      "grad_norm": 17.919588088989258,
      "learning_rate": 7.454725340904203e-06,
      "loss": 1.818,
      "step": 8500
    },
    {
      "epoch": 3.291134339914828,
      "grad_norm": 42.30781936645508,
      "learning_rate": 7.454295177872414e-06,
      "loss": 1.6904,
      "step": 8501
    },
    {
      "epoch": 3.291521486643438,
      "grad_norm": 14.772439002990723,
      "learning_rate": 7.453865014840625e-06,
      "loss": 0.6104,
      "step": 8502
    },
    {
      "epoch": 3.291908633372048,
      "grad_norm": 30.610414505004883,
      "learning_rate": 7.453434851808836e-06,
      "loss": 1.4869,
      "step": 8503
    },
    {
      "epoch": 3.292295780100658,
      "grad_norm": 19.190019607543945,
      "learning_rate": 7.4530046887770465e-06,
      "loss": 1.2264,
      "step": 8504
    },
    {
      "epoch": 3.292682926829268,
      "grad_norm": 22.779443740844727,
      "learning_rate": 7.452574525745257e-06,
      "loss": 1.2213,
      "step": 8505
    },
    {
      "epoch": 3.2930700735578786,
      "grad_norm": 37.88825225830078,
      "learning_rate": 7.452144362713469e-06,
      "loss": 2.317,
      "step": 8506
    },
    {
      "epoch": 3.2934572202864887,
      "grad_norm": 17.89738655090332,
      "learning_rate": 7.45171419968168e-06,
      "loss": 1.6262,
      "step": 8507
    },
    {
      "epoch": 3.2938443670150988,
      "grad_norm": 20.045446395874023,
      "learning_rate": 7.4512840366498905e-06,
      "loss": 1.492,
      "step": 8508
    },
    {
      "epoch": 3.294231513743709,
      "grad_norm": 28.28467559814453,
      "learning_rate": 7.450853873618103e-06,
      "loss": 1.4855,
      "step": 8509
    },
    {
      "epoch": 3.294618660472319,
      "grad_norm": 29.4374942779541,
      "learning_rate": 7.450423710586313e-06,
      "loss": 1.5343,
      "step": 8510
    },
    {
      "epoch": 3.295005807200929,
      "grad_norm": 32.2224006652832,
      "learning_rate": 7.449993547554524e-06,
      "loss": 3.0713,
      "step": 8511
    },
    {
      "epoch": 3.2953929539295395,
      "grad_norm": 32.39961624145508,
      "learning_rate": 7.4495633845227345e-06,
      "loss": 3.5257,
      "step": 8512
    },
    {
      "epoch": 3.2957801006581495,
      "grad_norm": 37.66704559326172,
      "learning_rate": 7.449133221490946e-06,
      "loss": 2.5311,
      "step": 8513
    },
    {
      "epoch": 3.2961672473867596,
      "grad_norm": 28.954378128051758,
      "learning_rate": 7.4487030584591565e-06,
      "loss": 1.4186,
      "step": 8514
    },
    {
      "epoch": 3.2965543941153697,
      "grad_norm": 17.655467987060547,
      "learning_rate": 7.448272895427368e-06,
      "loss": 1.0965,
      "step": 8515
    },
    {
      "epoch": 3.2969415408439797,
      "grad_norm": 13.246270179748535,
      "learning_rate": 7.4478427323955784e-06,
      "loss": 1.6812,
      "step": 8516
    },
    {
      "epoch": 3.2973286875725902,
      "grad_norm": 10.644852638244629,
      "learning_rate": 7.44741256936379e-06,
      "loss": 0.6872,
      "step": 8517
    },
    {
      "epoch": 3.2977158343012003,
      "grad_norm": 18.333019256591797,
      "learning_rate": 7.4469824063320004e-06,
      "loss": 1.1988,
      "step": 8518
    },
    {
      "epoch": 3.2981029810298104,
      "grad_norm": 20.564950942993164,
      "learning_rate": 7.446552243300211e-06,
      "loss": 1.3083,
      "step": 8519
    },
    {
      "epoch": 3.2984901277584204,
      "grad_norm": 35.875118255615234,
      "learning_rate": 7.4461220802684216e-06,
      "loss": 1.399,
      "step": 8520
    },
    {
      "epoch": 3.2988772744870305,
      "grad_norm": 12.677539825439453,
      "learning_rate": 7.445691917236634e-06,
      "loss": 1.1903,
      "step": 8521
    },
    {
      "epoch": 3.2992644212156406,
      "grad_norm": 25.50151252746582,
      "learning_rate": 7.445261754204844e-06,
      "loss": 0.7729,
      "step": 8522
    },
    {
      "epoch": 3.2996515679442506,
      "grad_norm": 14.158184051513672,
      "learning_rate": 7.444831591173055e-06,
      "loss": 1.0871,
      "step": 8523
    },
    {
      "epoch": 3.300038714672861,
      "grad_norm": 33.21588134765625,
      "learning_rate": 7.4444014281412655e-06,
      "loss": 1.9706,
      "step": 8524
    },
    {
      "epoch": 3.300425861401471,
      "grad_norm": 15.552470207214355,
      "learning_rate": 7.443971265109478e-06,
      "loss": 0.3957,
      "step": 8525
    },
    {
      "epoch": 3.3008130081300813,
      "grad_norm": 18.36062240600586,
      "learning_rate": 7.443541102077688e-06,
      "loss": 0.9769,
      "step": 8526
    },
    {
      "epoch": 3.3012001548586913,
      "grad_norm": 46.18899917602539,
      "learning_rate": 7.443110939045899e-06,
      "loss": 1.1046,
      "step": 8527
    },
    {
      "epoch": 3.3015873015873014,
      "grad_norm": 22.342636108398438,
      "learning_rate": 7.4426807760141095e-06,
      "loss": 1.7588,
      "step": 8528
    },
    {
      "epoch": 3.301974448315912,
      "grad_norm": 14.984692573547363,
      "learning_rate": 7.442250612982321e-06,
      "loss": 0.9345,
      "step": 8529
    },
    {
      "epoch": 3.302361595044522,
      "grad_norm": 4.780307292938232,
      "learning_rate": 7.441820449950532e-06,
      "loss": 0.1355,
      "step": 8530
    },
    {
      "epoch": 3.302748741773132,
      "grad_norm": 17.58686637878418,
      "learning_rate": 7.441390286918743e-06,
      "loss": 1.3936,
      "step": 8531
    },
    {
      "epoch": 3.303135888501742,
      "grad_norm": 23.1131591796875,
      "learning_rate": 7.4409601238869535e-06,
      "loss": 1.0117,
      "step": 8532
    },
    {
      "epoch": 3.303523035230352,
      "grad_norm": 16.9987735748291,
      "learning_rate": 7.440529960855165e-06,
      "loss": 1.5997,
      "step": 8533
    },
    {
      "epoch": 3.3039101819589627,
      "grad_norm": 21.65001678466797,
      "learning_rate": 7.4400997978233755e-06,
      "loss": 1.5967,
      "step": 8534
    },
    {
      "epoch": 3.3042973286875728,
      "grad_norm": 9.225872039794922,
      "learning_rate": 7.439669634791586e-06,
      "loss": 0.4001,
      "step": 8535
    },
    {
      "epoch": 3.304684475416183,
      "grad_norm": 17.479082107543945,
      "learning_rate": 7.4392394717597974e-06,
      "loss": 0.574,
      "step": 8536
    },
    {
      "epoch": 3.305071622144793,
      "grad_norm": 19.301326751708984,
      "learning_rate": 7.438809308728009e-06,
      "loss": 0.5103,
      "step": 8537
    },
    {
      "epoch": 3.305458768873403,
      "grad_norm": 31.44200325012207,
      "learning_rate": 7.4383791456962194e-06,
      "loss": 1.3627,
      "step": 8538
    },
    {
      "epoch": 3.305845915602013,
      "grad_norm": 20.94383430480957,
      "learning_rate": 7.43794898266443e-06,
      "loss": 0.9867,
      "step": 8539
    },
    {
      "epoch": 3.306233062330623,
      "grad_norm": 26.43388557434082,
      "learning_rate": 7.4375188196326406e-06,
      "loss": 0.5734,
      "step": 8540
    },
    {
      "epoch": 3.3066202090592336,
      "grad_norm": 34.301109313964844,
      "learning_rate": 7.437088656600853e-06,
      "loss": 0.9358,
      "step": 8541
    },
    {
      "epoch": 3.3070073557878437,
      "grad_norm": 14.306925773620605,
      "learning_rate": 7.436658493569063e-06,
      "loss": 0.9015,
      "step": 8542
    },
    {
      "epoch": 3.3073945025164537,
      "grad_norm": 31.823171615600586,
      "learning_rate": 7.436228330537274e-06,
      "loss": 2.2264,
      "step": 8543
    },
    {
      "epoch": 3.307781649245064,
      "grad_norm": 10.817758560180664,
      "learning_rate": 7.4357981675054845e-06,
      "loss": 1.2081,
      "step": 8544
    },
    {
      "epoch": 3.308168795973674,
      "grad_norm": 13.014093399047852,
      "learning_rate": 7.435368004473697e-06,
      "loss": 0.9871,
      "step": 8545
    },
    {
      "epoch": 3.3085559427022844,
      "grad_norm": 22.848995208740234,
      "learning_rate": 7.434937841441907e-06,
      "loss": 1.2307,
      "step": 8546
    },
    {
      "epoch": 3.3089430894308944,
      "grad_norm": 17.83072853088379,
      "learning_rate": 7.434507678410118e-06,
      "loss": 1.3316,
      "step": 8547
    },
    {
      "epoch": 3.3093302361595045,
      "grad_norm": 13.371493339538574,
      "learning_rate": 7.4340775153783285e-06,
      "loss": 0.8651,
      "step": 8548
    },
    {
      "epoch": 3.3097173828881146,
      "grad_norm": 47.33939743041992,
      "learning_rate": 7.43364735234654e-06,
      "loss": 1.3226,
      "step": 8549
    },
    {
      "epoch": 3.3101045296167246,
      "grad_norm": 13.777054786682129,
      "learning_rate": 7.4332171893147505e-06,
      "loss": 0.7346,
      "step": 8550
    },
    {
      "epoch": 3.3104916763453347,
      "grad_norm": 33.42641067504883,
      "learning_rate": 7.432787026282962e-06,
      "loss": 0.8574,
      "step": 8551
    },
    {
      "epoch": 3.310878823073945,
      "grad_norm": 18.606815338134766,
      "learning_rate": 7.432356863251173e-06,
      "loss": 0.9871,
      "step": 8552
    },
    {
      "epoch": 3.3112659698025553,
      "grad_norm": 12.310067176818848,
      "learning_rate": 7.431926700219384e-06,
      "loss": 0.9995,
      "step": 8553
    },
    {
      "epoch": 3.3116531165311653,
      "grad_norm": 23.228742599487305,
      "learning_rate": 7.4314965371875945e-06,
      "loss": 0.7949,
      "step": 8554
    },
    {
      "epoch": 3.3120402632597754,
      "grad_norm": 10.265222549438477,
      "learning_rate": 7.431066374155805e-06,
      "loss": 0.4776,
      "step": 8555
    },
    {
      "epoch": 3.3124274099883855,
      "grad_norm": 15.128460884094238,
      "learning_rate": 7.430636211124017e-06,
      "loss": 1.2259,
      "step": 8556
    },
    {
      "epoch": 3.3128145567169955,
      "grad_norm": 39.06562042236328,
      "learning_rate": 7.430206048092228e-06,
      "loss": 1.7322,
      "step": 8557
    },
    {
      "epoch": 3.313201703445606,
      "grad_norm": 24.284448623657227,
      "learning_rate": 7.4297758850604384e-06,
      "loss": 1.3387,
      "step": 8558
    },
    {
      "epoch": 3.313588850174216,
      "grad_norm": 30.593530654907227,
      "learning_rate": 7.429345722028649e-06,
      "loss": 1.4136,
      "step": 8559
    },
    {
      "epoch": 3.313975996902826,
      "grad_norm": 35.695823669433594,
      "learning_rate": 7.428915558996861e-06,
      "loss": 1.2134,
      "step": 8560
    },
    {
      "epoch": 3.3143631436314362,
      "grad_norm": 22.632204055786133,
      "learning_rate": 7.428485395965072e-06,
      "loss": 0.7127,
      "step": 8561
    },
    {
      "epoch": 3.3147502903600463,
      "grad_norm": 28.78428840637207,
      "learning_rate": 7.428055232933282e-06,
      "loss": 1.9334,
      "step": 8562
    },
    {
      "epoch": 3.315137437088657,
      "grad_norm": 15.944533348083496,
      "learning_rate": 7.427625069901493e-06,
      "loss": 0.9688,
      "step": 8563
    },
    {
      "epoch": 3.315524583817267,
      "grad_norm": 11.401278495788574,
      "learning_rate": 7.427194906869704e-06,
      "loss": 0.4294,
      "step": 8564
    },
    {
      "epoch": 3.315911730545877,
      "grad_norm": 37.719146728515625,
      "learning_rate": 7.426764743837915e-06,
      "loss": 1.2918,
      "step": 8565
    },
    {
      "epoch": 3.316298877274487,
      "grad_norm": 13.83078670501709,
      "learning_rate": 7.426334580806126e-06,
      "loss": 1.0799,
      "step": 8566
    },
    {
      "epoch": 3.316686024003097,
      "grad_norm": 21.991796493530273,
      "learning_rate": 7.425904417774337e-06,
      "loss": 1.5321,
      "step": 8567
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 16.727802276611328,
      "learning_rate": 7.425474254742548e-06,
      "loss": 1.3748,
      "step": 8568
    },
    {
      "epoch": 3.317460317460317,
      "grad_norm": 22.71185302734375,
      "learning_rate": 7.425044091710759e-06,
      "loss": 1.1079,
      "step": 8569
    },
    {
      "epoch": 3.3178474641889277,
      "grad_norm": 25.31514549255371,
      "learning_rate": 7.4246139286789695e-06,
      "loss": 1.4741,
      "step": 8570
    },
    {
      "epoch": 3.318234610917538,
      "grad_norm": 22.288742065429688,
      "learning_rate": 7.42418376564718e-06,
      "loss": 1.6092,
      "step": 8571
    },
    {
      "epoch": 3.318621757646148,
      "grad_norm": 28.925819396972656,
      "learning_rate": 7.423753602615392e-06,
      "loss": 1.6988,
      "step": 8572
    },
    {
      "epoch": 3.319008904374758,
      "grad_norm": 4.560169219970703,
      "learning_rate": 7.423323439583603e-06,
      "loss": 0.2342,
      "step": 8573
    },
    {
      "epoch": 3.319396051103368,
      "grad_norm": 29.124486923217773,
      "learning_rate": 7.4228932765518135e-06,
      "loss": 1.899,
      "step": 8574
    },
    {
      "epoch": 3.3197831978319785,
      "grad_norm": 13.733848571777344,
      "learning_rate": 7.422463113520024e-06,
      "loss": 1.0533,
      "step": 8575
    },
    {
      "epoch": 3.3201703445605886,
      "grad_norm": 22.328092575073242,
      "learning_rate": 7.422032950488236e-06,
      "loss": 1.3944,
      "step": 8576
    },
    {
      "epoch": 3.3205574912891986,
      "grad_norm": 22.36279296875,
      "learning_rate": 7.421602787456447e-06,
      "loss": 1.2694,
      "step": 8577
    },
    {
      "epoch": 3.3209446380178087,
      "grad_norm": 22.504419326782227,
      "learning_rate": 7.421172624424657e-06,
      "loss": 1.2706,
      "step": 8578
    },
    {
      "epoch": 3.3213317847464188,
      "grad_norm": 27.006744384765625,
      "learning_rate": 7.420742461392868e-06,
      "loss": 1.6528,
      "step": 8579
    },
    {
      "epoch": 3.3217189314750293,
      "grad_norm": 18.239803314208984,
      "learning_rate": 7.420312298361079e-06,
      "loss": 1.7938,
      "step": 8580
    },
    {
      "epoch": 3.3221060782036393,
      "grad_norm": 19.579322814941406,
      "learning_rate": 7.419882135329291e-06,
      "loss": 1.7959,
      "step": 8581
    },
    {
      "epoch": 3.3224932249322494,
      "grad_norm": 18.059412002563477,
      "learning_rate": 7.419451972297501e-06,
      "loss": 1.4208,
      "step": 8582
    },
    {
      "epoch": 3.3228803716608595,
      "grad_norm": 13.928442001342773,
      "learning_rate": 7.419021809265712e-06,
      "loss": 0.6771,
      "step": 8583
    },
    {
      "epoch": 3.3232675183894695,
      "grad_norm": 30.5150089263916,
      "learning_rate": 7.418591646233923e-06,
      "loss": 1.3812,
      "step": 8584
    },
    {
      "epoch": 3.3236546651180796,
      "grad_norm": 7.3488545417785645,
      "learning_rate": 7.418161483202134e-06,
      "loss": 0.5055,
      "step": 8585
    },
    {
      "epoch": 3.3240418118466897,
      "grad_norm": 21.322359085083008,
      "learning_rate": 7.4177313201703445e-06,
      "loss": 0.8962,
      "step": 8586
    },
    {
      "epoch": 3.3244289585753,
      "grad_norm": 20.83708953857422,
      "learning_rate": 7.417301157138556e-06,
      "loss": 1.1747,
      "step": 8587
    },
    {
      "epoch": 3.3248161053039103,
      "grad_norm": 30.591976165771484,
      "learning_rate": 7.416870994106767e-06,
      "loss": 1.6642,
      "step": 8588
    },
    {
      "epoch": 3.3252032520325203,
      "grad_norm": 28.370664596557617,
      "learning_rate": 7.416440831074978e-06,
      "loss": 1.0094,
      "step": 8589
    },
    {
      "epoch": 3.3255903987611304,
      "grad_norm": 12.001945495605469,
      "learning_rate": 7.4160106680431885e-06,
      "loss": 1.1531,
      "step": 8590
    },
    {
      "epoch": 3.3259775454897405,
      "grad_norm": 21.23565101623535,
      "learning_rate": 7.415580505011401e-06,
      "loss": 1.7245,
      "step": 8591
    },
    {
      "epoch": 3.326364692218351,
      "grad_norm": 40.32254409790039,
      "learning_rate": 7.415150341979611e-06,
      "loss": 0.8987,
      "step": 8592
    },
    {
      "epoch": 3.326751838946961,
      "grad_norm": 16.319448471069336,
      "learning_rate": 7.414720178947822e-06,
      "loss": 2.3668,
      "step": 8593
    },
    {
      "epoch": 3.327138985675571,
      "grad_norm": 18.16988754272461,
      "learning_rate": 7.4142900159160325e-06,
      "loss": 1.2293,
      "step": 8594
    },
    {
      "epoch": 3.327526132404181,
      "grad_norm": 26.93003273010254,
      "learning_rate": 7.413859852884244e-06,
      "loss": 1.4301,
      "step": 8595
    },
    {
      "epoch": 3.3279132791327912,
      "grad_norm": 16.20484733581543,
      "learning_rate": 7.413429689852455e-06,
      "loss": 1.578,
      "step": 8596
    },
    {
      "epoch": 3.3283004258614013,
      "grad_norm": 21.256546020507812,
      "learning_rate": 7.412999526820666e-06,
      "loss": 1.2558,
      "step": 8597
    },
    {
      "epoch": 3.328687572590012,
      "grad_norm": 27.07449722290039,
      "learning_rate": 7.412569363788876e-06,
      "loss": 1.3925,
      "step": 8598
    },
    {
      "epoch": 3.329074719318622,
      "grad_norm": 29.93339729309082,
      "learning_rate": 7.412139200757088e-06,
      "loss": 1.7911,
      "step": 8599
    },
    {
      "epoch": 3.329461866047232,
      "grad_norm": 29.80682373046875,
      "learning_rate": 7.411709037725298e-06,
      "loss": 1.1268,
      "step": 8600
    },
    {
      "epoch": 3.329849012775842,
      "grad_norm": 24.107318878173828,
      "learning_rate": 7.411278874693509e-06,
      "loss": 1.6213,
      "step": 8601
    },
    {
      "epoch": 3.330236159504452,
      "grad_norm": 41.59846878051758,
      "learning_rate": 7.41084871166172e-06,
      "loss": 1.4077,
      "step": 8602
    },
    {
      "epoch": 3.330623306233062,
      "grad_norm": 38.417484283447266,
      "learning_rate": 7.410418548629932e-06,
      "loss": 1.547,
      "step": 8603
    },
    {
      "epoch": 3.3310104529616726,
      "grad_norm": 18.75543785095215,
      "learning_rate": 7.409988385598142e-06,
      "loss": 1.0987,
      "step": 8604
    },
    {
      "epoch": 3.3313975996902827,
      "grad_norm": 7.678890228271484,
      "learning_rate": 7.409558222566353e-06,
      "loss": 0.3031,
      "step": 8605
    },
    {
      "epoch": 3.3317847464188928,
      "grad_norm": 21.721710205078125,
      "learning_rate": 7.4091280595345635e-06,
      "loss": 1.5561,
      "step": 8606
    },
    {
      "epoch": 3.332171893147503,
      "grad_norm": 25.80698585510254,
      "learning_rate": 7.408697896502776e-06,
      "loss": 1.6139,
      "step": 8607
    },
    {
      "epoch": 3.332559039876113,
      "grad_norm": 17.74406623840332,
      "learning_rate": 7.408267733470986e-06,
      "loss": 1.6781,
      "step": 8608
    },
    {
      "epoch": 3.3329461866047234,
      "grad_norm": 27.721134185791016,
      "learning_rate": 7.407837570439197e-06,
      "loss": 0.952,
      "step": 8609
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 60.14424514770508,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 1.6916,
      "step": 8610
    },
    {
      "epoch": 3.3337204800619435,
      "grad_norm": 26.505332946777344,
      "learning_rate": 7.40697724437562e-06,
      "loss": 1.68,
      "step": 8611
    },
    {
      "epoch": 3.3341076267905536,
      "grad_norm": 20.508930206298828,
      "learning_rate": 7.40654708134383e-06,
      "loss": 1.1416,
      "step": 8612
    },
    {
      "epoch": 3.3344947735191637,
      "grad_norm": 12.265193939208984,
      "learning_rate": 7.406116918312041e-06,
      "loss": 0.9403,
      "step": 8613
    },
    {
      "epoch": 3.3348819202477737,
      "grad_norm": 13.710248947143555,
      "learning_rate": 7.4056867552802514e-06,
      "loss": 1.3088,
      "step": 8614
    },
    {
      "epoch": 3.335269066976384,
      "grad_norm": 25.114049911499023,
      "learning_rate": 7.405256592248463e-06,
      "loss": 0.9987,
      "step": 8615
    },
    {
      "epoch": 3.3356562137049943,
      "grad_norm": 42.102108001708984,
      "learning_rate": 7.4048264292166734e-06,
      "loss": 0.7392,
      "step": 8616
    },
    {
      "epoch": 3.3360433604336044,
      "grad_norm": 19.38831329345703,
      "learning_rate": 7.404396266184885e-06,
      "loss": 0.5898,
      "step": 8617
    },
    {
      "epoch": 3.3364305071622145,
      "grad_norm": 19.54121208190918,
      "learning_rate": 7.403966103153095e-06,
      "loss": 1.9852,
      "step": 8618
    },
    {
      "epoch": 3.3368176538908245,
      "grad_norm": 11.648451805114746,
      "learning_rate": 7.403535940121307e-06,
      "loss": 0.4083,
      "step": 8619
    },
    {
      "epoch": 3.3372048006194346,
      "grad_norm": 29.553754806518555,
      "learning_rate": 7.403105777089517e-06,
      "loss": 1.6178,
      "step": 8620
    },
    {
      "epoch": 3.337591947348045,
      "grad_norm": 19.449705123901367,
      "learning_rate": 7.402675614057728e-06,
      "loss": 1.3734,
      "step": 8621
    },
    {
      "epoch": 3.337979094076655,
      "grad_norm": 18.07212257385254,
      "learning_rate": 7.4022454510259385e-06,
      "loss": 0.884,
      "step": 8622
    },
    {
      "epoch": 3.3383662408052652,
      "grad_norm": 11.273244857788086,
      "learning_rate": 7.401815287994151e-06,
      "loss": 0.3729,
      "step": 8623
    },
    {
      "epoch": 3.3387533875338753,
      "grad_norm": 26.56397247314453,
      "learning_rate": 7.401385124962361e-06,
      "loss": 2.6658,
      "step": 8624
    },
    {
      "epoch": 3.3391405342624854,
      "grad_norm": 23.30635643005371,
      "learning_rate": 7.400954961930572e-06,
      "loss": 1.1841,
      "step": 8625
    },
    {
      "epoch": 3.339527680991096,
      "grad_norm": 20.645008087158203,
      "learning_rate": 7.4005247988987825e-06,
      "loss": 1.7273,
      "step": 8626
    },
    {
      "epoch": 3.339914827719706,
      "grad_norm": 51.921634674072266,
      "learning_rate": 7.400094635866995e-06,
      "loss": 1.0533,
      "step": 8627
    },
    {
      "epoch": 3.340301974448316,
      "grad_norm": 18.11866569519043,
      "learning_rate": 7.399664472835205e-06,
      "loss": 1.0544,
      "step": 8628
    },
    {
      "epoch": 3.340689121176926,
      "grad_norm": 21.454137802124023,
      "learning_rate": 7.399234309803416e-06,
      "loss": 1.2905,
      "step": 8629
    },
    {
      "epoch": 3.341076267905536,
      "grad_norm": 18.442031860351562,
      "learning_rate": 7.3988041467716265e-06,
      "loss": 1.1095,
      "step": 8630
    },
    {
      "epoch": 3.341463414634146,
      "grad_norm": 4.122161865234375,
      "learning_rate": 7.398373983739838e-06,
      "loss": 0.2007,
      "step": 8631
    },
    {
      "epoch": 3.3418505613627563,
      "grad_norm": 12.382528305053711,
      "learning_rate": 7.397943820708049e-06,
      "loss": 1.1036,
      "step": 8632
    },
    {
      "epoch": 3.3422377080913668,
      "grad_norm": 30.36107635498047,
      "learning_rate": 7.39751365767626e-06,
      "loss": 2.168,
      "step": 8633
    },
    {
      "epoch": 3.342624854819977,
      "grad_norm": 19.51113510131836,
      "learning_rate": 7.397083494644471e-06,
      "loss": 1.2384,
      "step": 8634
    },
    {
      "epoch": 3.343012001548587,
      "grad_norm": 10.518168449401855,
      "learning_rate": 7.396653331612682e-06,
      "loss": 0.8654,
      "step": 8635
    },
    {
      "epoch": 3.343399148277197,
      "grad_norm": 10.376693725585938,
      "learning_rate": 7.3962231685808924e-06,
      "loss": 0.768,
      "step": 8636
    },
    {
      "epoch": 3.343786295005807,
      "grad_norm": 27.747676849365234,
      "learning_rate": 7.395793005549103e-06,
      "loss": 2.0321,
      "step": 8637
    },
    {
      "epoch": 3.3441734417344176,
      "grad_norm": 10.928935050964355,
      "learning_rate": 7.395362842517315e-06,
      "loss": 0.5922,
      "step": 8638
    },
    {
      "epoch": 3.3445605884630276,
      "grad_norm": 30.063079833984375,
      "learning_rate": 7.394932679485526e-06,
      "loss": 1.3486,
      "step": 8639
    },
    {
      "epoch": 3.3449477351916377,
      "grad_norm": 32.65694046020508,
      "learning_rate": 7.394502516453736e-06,
      "loss": 1.1876,
      "step": 8640
    },
    {
      "epoch": 3.3453348819202477,
      "grad_norm": 35.50968933105469,
      "learning_rate": 7.394072353421947e-06,
      "loss": 0.8199,
      "step": 8641
    },
    {
      "epoch": 3.345722028648858,
      "grad_norm": 19.438190460205078,
      "learning_rate": 7.393642190390159e-06,
      "loss": 1.1095,
      "step": 8642
    },
    {
      "epoch": 3.346109175377468,
      "grad_norm": 29.928544998168945,
      "learning_rate": 7.39321202735837e-06,
      "loss": 1.84,
      "step": 8643
    },
    {
      "epoch": 3.3464963221060784,
      "grad_norm": 58.038326263427734,
      "learning_rate": 7.39278186432658e-06,
      "loss": 0.8998,
      "step": 8644
    },
    {
      "epoch": 3.3468834688346885,
      "grad_norm": 73.92265319824219,
      "learning_rate": 7.392351701294791e-06,
      "loss": 1.0429,
      "step": 8645
    },
    {
      "epoch": 3.3472706155632985,
      "grad_norm": 12.365107536315918,
      "learning_rate": 7.391921538263002e-06,
      "loss": 0.8799,
      "step": 8646
    },
    {
      "epoch": 3.3476577622919086,
      "grad_norm": 17.58880043029785,
      "learning_rate": 7.391491375231214e-06,
      "loss": 1.4446,
      "step": 8647
    },
    {
      "epoch": 3.3480449090205187,
      "grad_norm": 39.956417083740234,
      "learning_rate": 7.391061212199424e-06,
      "loss": 1.9314,
      "step": 8648
    },
    {
      "epoch": 3.3484320557491287,
      "grad_norm": 5.740153789520264,
      "learning_rate": 7.390631049167635e-06,
      "loss": 0.253,
      "step": 8649
    },
    {
      "epoch": 3.3488192024777392,
      "grad_norm": 45.0794677734375,
      "learning_rate": 7.390200886135846e-06,
      "loss": 1.4862,
      "step": 8650
    },
    {
      "epoch": 3.3492063492063493,
      "grad_norm": 11.217680931091309,
      "learning_rate": 7.389770723104057e-06,
      "loss": 0.4406,
      "step": 8651
    },
    {
      "epoch": 3.3495934959349594,
      "grad_norm": 32.950897216796875,
      "learning_rate": 7.3893405600722675e-06,
      "loss": 1.7591,
      "step": 8652
    },
    {
      "epoch": 3.3499806426635694,
      "grad_norm": 86.462890625,
      "learning_rate": 7.388910397040479e-06,
      "loss": 1.5695,
      "step": 8653
    },
    {
      "epoch": 3.3503677893921795,
      "grad_norm": 31.803693771362305,
      "learning_rate": 7.38848023400869e-06,
      "loss": 1.0994,
      "step": 8654
    },
    {
      "epoch": 3.35075493612079,
      "grad_norm": 29.33769989013672,
      "learning_rate": 7.388050070976901e-06,
      "loss": 1.1573,
      "step": 8655
    },
    {
      "epoch": 3.3511420828494,
      "grad_norm": 13.673233985900879,
      "learning_rate": 7.3876199079451114e-06,
      "loss": 1.1362,
      "step": 8656
    },
    {
      "epoch": 3.35152922957801,
      "grad_norm": 35.80323791503906,
      "learning_rate": 7.387189744913322e-06,
      "loss": 2.826,
      "step": 8657
    },
    {
      "epoch": 3.35191637630662,
      "grad_norm": 88.60968017578125,
      "learning_rate": 7.386759581881534e-06,
      "loss": 1.8565,
      "step": 8658
    },
    {
      "epoch": 3.3523035230352303,
      "grad_norm": 9.745553016662598,
      "learning_rate": 7.386329418849745e-06,
      "loss": 1.2715,
      "step": 8659
    },
    {
      "epoch": 3.3526906697638403,
      "grad_norm": 56.571693420410156,
      "learning_rate": 7.385899255817955e-06,
      "loss": 1.7953,
      "step": 8660
    },
    {
      "epoch": 3.3530778164924504,
      "grad_norm": 20.481801986694336,
      "learning_rate": 7.385469092786166e-06,
      "loss": 0.9996,
      "step": 8661
    },
    {
      "epoch": 3.353464963221061,
      "grad_norm": 25.670333862304688,
      "learning_rate": 7.385038929754378e-06,
      "loss": 1.6293,
      "step": 8662
    },
    {
      "epoch": 3.353852109949671,
      "grad_norm": 33.81332015991211,
      "learning_rate": 7.384608766722589e-06,
      "loss": 1.67,
      "step": 8663
    },
    {
      "epoch": 3.354239256678281,
      "grad_norm": 31.121435165405273,
      "learning_rate": 7.384178603690799e-06,
      "loss": 1.8448,
      "step": 8664
    },
    {
      "epoch": 3.354626403406891,
      "grad_norm": 21.057340621948242,
      "learning_rate": 7.38374844065901e-06,
      "loss": 1.3957,
      "step": 8665
    },
    {
      "epoch": 3.355013550135501,
      "grad_norm": 13.319579124450684,
      "learning_rate": 7.383318277627221e-06,
      "loss": 0.8426,
      "step": 8666
    },
    {
      "epoch": 3.3554006968641117,
      "grad_norm": 27.986135482788086,
      "learning_rate": 7.382888114595432e-06,
      "loss": 1.1923,
      "step": 8667
    },
    {
      "epoch": 3.3557878435927218,
      "grad_norm": 18.2309513092041,
      "learning_rate": 7.382457951563643e-06,
      "loss": 1.6016,
      "step": 8668
    },
    {
      "epoch": 3.356174990321332,
      "grad_norm": 37.19880294799805,
      "learning_rate": 7.382027788531854e-06,
      "loss": 0.9556,
      "step": 8669
    },
    {
      "epoch": 3.356562137049942,
      "grad_norm": 33.34013748168945,
      "learning_rate": 7.381597625500065e-06,
      "loss": 2.0577,
      "step": 8670
    },
    {
      "epoch": 3.356949283778552,
      "grad_norm": 38.98297882080078,
      "learning_rate": 7.381167462468276e-06,
      "loss": 0.7516,
      "step": 8671
    },
    {
      "epoch": 3.3573364305071625,
      "grad_norm": 38.99757385253906,
      "learning_rate": 7.3807372994364865e-06,
      "loss": 1.3891,
      "step": 8672
    },
    {
      "epoch": 3.3577235772357725,
      "grad_norm": 30.942914962768555,
      "learning_rate": 7.380307136404699e-06,
      "loss": 0.4917,
      "step": 8673
    },
    {
      "epoch": 3.3581107239643826,
      "grad_norm": 16.373010635375977,
      "learning_rate": 7.379876973372909e-06,
      "loss": 0.8706,
      "step": 8674
    },
    {
      "epoch": 3.3584978706929927,
      "grad_norm": 32.41147994995117,
      "learning_rate": 7.37944681034112e-06,
      "loss": 1.8315,
      "step": 8675
    },
    {
      "epoch": 3.3588850174216027,
      "grad_norm": 9.688207626342773,
      "learning_rate": 7.3790166473093304e-06,
      "loss": 0.3898,
      "step": 8676
    },
    {
      "epoch": 3.359272164150213,
      "grad_norm": 17.40081024169922,
      "learning_rate": 7.378586484277543e-06,
      "loss": 0.9587,
      "step": 8677
    },
    {
      "epoch": 3.359659310878823,
      "grad_norm": 18.4771728515625,
      "learning_rate": 7.378156321245753e-06,
      "loss": 0.7754,
      "step": 8678
    },
    {
      "epoch": 3.3600464576074334,
      "grad_norm": 17.34423065185547,
      "learning_rate": 7.377726158213964e-06,
      "loss": 1.0858,
      "step": 8679
    },
    {
      "epoch": 3.3604336043360434,
      "grad_norm": 19.421266555786133,
      "learning_rate": 7.377295995182174e-06,
      "loss": 1.716,
      "step": 8680
    },
    {
      "epoch": 3.3608207510646535,
      "grad_norm": 36.75105285644531,
      "learning_rate": 7.376865832150386e-06,
      "loss": 2.3868,
      "step": 8681
    },
    {
      "epoch": 3.3612078977932636,
      "grad_norm": 64.5115737915039,
      "learning_rate": 7.376435669118596e-06,
      "loss": 1.3666,
      "step": 8682
    },
    {
      "epoch": 3.3615950445218736,
      "grad_norm": 22.159446716308594,
      "learning_rate": 7.376005506086808e-06,
      "loss": 1.8291,
      "step": 8683
    },
    {
      "epoch": 3.361982191250484,
      "grad_norm": 14.520597457885742,
      "learning_rate": 7.375575343055018e-06,
      "loss": 0.8479,
      "step": 8684
    },
    {
      "epoch": 3.362369337979094,
      "grad_norm": 10.672354698181152,
      "learning_rate": 7.37514518002323e-06,
      "loss": 0.5327,
      "step": 8685
    },
    {
      "epoch": 3.3627564847077043,
      "grad_norm": 12.67746353149414,
      "learning_rate": 7.37471501699144e-06,
      "loss": 1.0169,
      "step": 8686
    },
    {
      "epoch": 3.3631436314363143,
      "grad_norm": 20.655628204345703,
      "learning_rate": 7.374284853959651e-06,
      "loss": 0.9489,
      "step": 8687
    },
    {
      "epoch": 3.3635307781649244,
      "grad_norm": 17.94338035583496,
      "learning_rate": 7.3738546909278615e-06,
      "loss": 0.6942,
      "step": 8688
    },
    {
      "epoch": 3.3639179248935345,
      "grad_norm": 37.37736129760742,
      "learning_rate": 7.373424527896074e-06,
      "loss": 1.9758,
      "step": 8689
    },
    {
      "epoch": 3.364305071622145,
      "grad_norm": 22.158935546875,
      "learning_rate": 7.372994364864284e-06,
      "loss": 2.585,
      "step": 8690
    },
    {
      "epoch": 3.364692218350755,
      "grad_norm": 18.347023010253906,
      "learning_rate": 7.372564201832495e-06,
      "loss": 2.1478,
      "step": 8691
    },
    {
      "epoch": 3.365079365079365,
      "grad_norm": 42.78525161743164,
      "learning_rate": 7.3721340388007055e-06,
      "loss": 1.2694,
      "step": 8692
    },
    {
      "epoch": 3.365466511807975,
      "grad_norm": 35.96210861206055,
      "learning_rate": 7.371703875768918e-06,
      "loss": 3.7899,
      "step": 8693
    },
    {
      "epoch": 3.3658536585365852,
      "grad_norm": 13.685685157775879,
      "learning_rate": 7.371273712737128e-06,
      "loss": 0.7568,
      "step": 8694
    },
    {
      "epoch": 3.3662408052651953,
      "grad_norm": 48.2722053527832,
      "learning_rate": 7.370843549705339e-06,
      "loss": 1.2584,
      "step": 8695
    },
    {
      "epoch": 3.366627951993806,
      "grad_norm": 3.537146806716919,
      "learning_rate": 7.3704133866735494e-06,
      "loss": 0.1831,
      "step": 8696
    },
    {
      "epoch": 3.367015098722416,
      "grad_norm": 25.314294815063477,
      "learning_rate": 7.369983223641761e-06,
      "loss": 1.0704,
      "step": 8697
    },
    {
      "epoch": 3.367402245451026,
      "grad_norm": 5.887752056121826,
      "learning_rate": 7.369553060609972e-06,
      "loss": 0.248,
      "step": 8698
    },
    {
      "epoch": 3.367789392179636,
      "grad_norm": 10.498661994934082,
      "learning_rate": 7.369122897578183e-06,
      "loss": 1.2264,
      "step": 8699
    },
    {
      "epoch": 3.368176538908246,
      "grad_norm": 66.67816925048828,
      "learning_rate": 7.368692734546393e-06,
      "loss": 1.6945,
      "step": 8700
    },
    {
      "epoch": 3.3685636856368566,
      "grad_norm": 19.682580947875977,
      "learning_rate": 7.368262571514605e-06,
      "loss": 1.2711,
      "step": 8701
    },
    {
      "epoch": 3.3689508323654667,
      "grad_norm": 50.78077697753906,
      "learning_rate": 7.367832408482815e-06,
      "loss": 1.9079,
      "step": 8702
    },
    {
      "epoch": 3.3693379790940767,
      "grad_norm": 29.246145248413086,
      "learning_rate": 7.367402245451026e-06,
      "loss": 1.1896,
      "step": 8703
    },
    {
      "epoch": 3.369725125822687,
      "grad_norm": 30.192729949951172,
      "learning_rate": 7.366972082419237e-06,
      "loss": 1.981,
      "step": 8704
    },
    {
      "epoch": 3.370112272551297,
      "grad_norm": 39.48674011230469,
      "learning_rate": 7.366541919387449e-06,
      "loss": 2.2465,
      "step": 8705
    },
    {
      "epoch": 3.370499419279907,
      "grad_norm": 12.663094520568848,
      "learning_rate": 7.366111756355659e-06,
      "loss": 0.7506,
      "step": 8706
    },
    {
      "epoch": 3.370886566008517,
      "grad_norm": 11.440500259399414,
      "learning_rate": 7.36568159332387e-06,
      "loss": 1.1836,
      "step": 8707
    },
    {
      "epoch": 3.3712737127371275,
      "grad_norm": 53.46757507324219,
      "learning_rate": 7.3652514302920805e-06,
      "loss": 1.5081,
      "step": 8708
    },
    {
      "epoch": 3.3716608594657376,
      "grad_norm": 15.817028045654297,
      "learning_rate": 7.364821267260293e-06,
      "loss": 1.6908,
      "step": 8709
    },
    {
      "epoch": 3.3720480061943476,
      "grad_norm": 16.723491668701172,
      "learning_rate": 7.364391104228503e-06,
      "loss": 1.4779,
      "step": 8710
    },
    {
      "epoch": 3.3724351529229577,
      "grad_norm": 54.05531311035156,
      "learning_rate": 7.363960941196714e-06,
      "loss": 0.6699,
      "step": 8711
    },
    {
      "epoch": 3.3728222996515678,
      "grad_norm": 36.18538284301758,
      "learning_rate": 7.3635307781649245e-06,
      "loss": 1.8574,
      "step": 8712
    },
    {
      "epoch": 3.3732094463801783,
      "grad_norm": 32.445228576660156,
      "learning_rate": 7.363100615133137e-06,
      "loss": 1.582,
      "step": 8713
    },
    {
      "epoch": 3.3735965931087883,
      "grad_norm": 66.81782531738281,
      "learning_rate": 7.362670452101347e-06,
      "loss": 1.1537,
      "step": 8714
    },
    {
      "epoch": 3.3739837398373984,
      "grad_norm": 22.794591903686523,
      "learning_rate": 7.362240289069558e-06,
      "loss": 1.473,
      "step": 8715
    },
    {
      "epoch": 3.3743708865660085,
      "grad_norm": 48.45819854736328,
      "learning_rate": 7.361810126037769e-06,
      "loss": 1.6212,
      "step": 8716
    },
    {
      "epoch": 3.3747580332946185,
      "grad_norm": 25.7674560546875,
      "learning_rate": 7.36137996300598e-06,
      "loss": 1.968,
      "step": 8717
    },
    {
      "epoch": 3.375145180023229,
      "grad_norm": 10.873093605041504,
      "learning_rate": 7.36094979997419e-06,
      "loss": 0.3804,
      "step": 8718
    },
    {
      "epoch": 3.375532326751839,
      "grad_norm": 19.51276969909668,
      "learning_rate": 7.360519636942402e-06,
      "loss": 1.8669,
      "step": 8719
    },
    {
      "epoch": 3.375919473480449,
      "grad_norm": 5.576115131378174,
      "learning_rate": 7.360089473910613e-06,
      "loss": 0.3004,
      "step": 8720
    },
    {
      "epoch": 3.3763066202090593,
      "grad_norm": 24.851577758789062,
      "learning_rate": 7.359659310878824e-06,
      "loss": 0.972,
      "step": 8721
    },
    {
      "epoch": 3.3766937669376693,
      "grad_norm": 35.5556755065918,
      "learning_rate": 7.359229147847034e-06,
      "loss": 1.1519,
      "step": 8722
    },
    {
      "epoch": 3.3770809136662794,
      "grad_norm": 44.72576141357422,
      "learning_rate": 7.358798984815245e-06,
      "loss": 1.5559,
      "step": 8723
    },
    {
      "epoch": 3.3774680603948894,
      "grad_norm": 20.02259063720703,
      "learning_rate": 7.358368821783457e-06,
      "loss": 0.8925,
      "step": 8724
    },
    {
      "epoch": 3.3778552071235,
      "grad_norm": 16.431842803955078,
      "learning_rate": 7.357938658751668e-06,
      "loss": 0.8315,
      "step": 8725
    },
    {
      "epoch": 3.37824235385211,
      "grad_norm": 17.887939453125,
      "learning_rate": 7.357508495719878e-06,
      "loss": 0.9707,
      "step": 8726
    },
    {
      "epoch": 3.37862950058072,
      "grad_norm": 15.075638771057129,
      "learning_rate": 7.357078332688089e-06,
      "loss": 1.0976,
      "step": 8727
    },
    {
      "epoch": 3.37901664730933,
      "grad_norm": 29.287919998168945,
      "learning_rate": 7.356648169656301e-06,
      "loss": 1.9063,
      "step": 8728
    },
    {
      "epoch": 3.3794037940379402,
      "grad_norm": 15.886198043823242,
      "learning_rate": 7.356218006624512e-06,
      "loss": 1.5574,
      "step": 8729
    },
    {
      "epoch": 3.3797909407665507,
      "grad_norm": 17.441646575927734,
      "learning_rate": 7.355787843592722e-06,
      "loss": 0.9779,
      "step": 8730
    },
    {
      "epoch": 3.380178087495161,
      "grad_norm": 19.693552017211914,
      "learning_rate": 7.355357680560933e-06,
      "loss": 1.9009,
      "step": 8731
    },
    {
      "epoch": 3.380565234223771,
      "grad_norm": 21.553600311279297,
      "learning_rate": 7.354927517529144e-06,
      "loss": 1.7909,
      "step": 8732
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 29.74437141418457,
      "learning_rate": 7.354497354497355e-06,
      "loss": 1.6666,
      "step": 8733
    },
    {
      "epoch": 3.381339527680991,
      "grad_norm": 45.16154098510742,
      "learning_rate": 7.354067191465566e-06,
      "loss": 1.4769,
      "step": 8734
    },
    {
      "epoch": 3.381726674409601,
      "grad_norm": 22.150991439819336,
      "learning_rate": 7.353637028433777e-06,
      "loss": 2.48,
      "step": 8735
    },
    {
      "epoch": 3.3821138211382116,
      "grad_norm": 7.964894771575928,
      "learning_rate": 7.353206865401988e-06,
      "loss": 0.3393,
      "step": 8736
    },
    {
      "epoch": 3.3825009678668216,
      "grad_norm": 24.886497497558594,
      "learning_rate": 7.352776702370199e-06,
      "loss": 1.0884,
      "step": 8737
    },
    {
      "epoch": 3.3828881145954317,
      "grad_norm": 36.9665641784668,
      "learning_rate": 7.352346539338409e-06,
      "loss": 1.7983,
      "step": 8738
    },
    {
      "epoch": 3.3832752613240418,
      "grad_norm": 60.44973373413086,
      "learning_rate": 7.35191637630662e-06,
      "loss": 1.2944,
      "step": 8739
    },
    {
      "epoch": 3.383662408052652,
      "grad_norm": 20.064708709716797,
      "learning_rate": 7.351486213274832e-06,
      "loss": 0.9689,
      "step": 8740
    },
    {
      "epoch": 3.384049554781262,
      "grad_norm": 9.487911224365234,
      "learning_rate": 7.351056050243043e-06,
      "loss": 1.2889,
      "step": 8741
    },
    {
      "epoch": 3.3844367015098724,
      "grad_norm": 13.173161506652832,
      "learning_rate": 7.350625887211253e-06,
      "loss": 0.6012,
      "step": 8742
    },
    {
      "epoch": 3.3848238482384825,
      "grad_norm": 20.066539764404297,
      "learning_rate": 7.350195724179464e-06,
      "loss": 0.8519,
      "step": 8743
    },
    {
      "epoch": 3.3852109949670925,
      "grad_norm": 22.66691780090332,
      "learning_rate": 7.349765561147676e-06,
      "loss": 1.4465,
      "step": 8744
    },
    {
      "epoch": 3.3855981416957026,
      "grad_norm": 33.00336456298828,
      "learning_rate": 7.349335398115887e-06,
      "loss": 2.8382,
      "step": 8745
    },
    {
      "epoch": 3.3859852884243127,
      "grad_norm": 7.837209224700928,
      "learning_rate": 7.348905235084097e-06,
      "loss": 1.0873,
      "step": 8746
    },
    {
      "epoch": 3.386372435152923,
      "grad_norm": 20.239608764648438,
      "learning_rate": 7.348475072052308e-06,
      "loss": 0.8412,
      "step": 8747
    },
    {
      "epoch": 3.3867595818815333,
      "grad_norm": 14.652234077453613,
      "learning_rate": 7.348044909020519e-06,
      "loss": 1.0822,
      "step": 8748
    },
    {
      "epoch": 3.3871467286101433,
      "grad_norm": 32.67737579345703,
      "learning_rate": 7.347614745988731e-06,
      "loss": 1.5016,
      "step": 8749
    },
    {
      "epoch": 3.3875338753387534,
      "grad_norm": 38.139434814453125,
      "learning_rate": 7.347184582956941e-06,
      "loss": 2.6957,
      "step": 8750
    },
    {
      "epoch": 3.3879210220673635,
      "grad_norm": 13.53370475769043,
      "learning_rate": 7.346754419925152e-06,
      "loss": 0.6104,
      "step": 8751
    },
    {
      "epoch": 3.3883081687959735,
      "grad_norm": 38.061622619628906,
      "learning_rate": 7.346324256893363e-06,
      "loss": 1.7146,
      "step": 8752
    },
    {
      "epoch": 3.3886953155245836,
      "grad_norm": 18.23175621032715,
      "learning_rate": 7.345894093861574e-06,
      "loss": 0.8474,
      "step": 8753
    },
    {
      "epoch": 3.389082462253194,
      "grad_norm": 17.206371307373047,
      "learning_rate": 7.3454639308297844e-06,
      "loss": 1.4227,
      "step": 8754
    },
    {
      "epoch": 3.389469608981804,
      "grad_norm": 12.832340240478516,
      "learning_rate": 7.345033767797997e-06,
      "loss": 0.731,
      "step": 8755
    },
    {
      "epoch": 3.3898567557104142,
      "grad_norm": 53.01462936401367,
      "learning_rate": 7.344603604766207e-06,
      "loss": 1.061,
      "step": 8756
    },
    {
      "epoch": 3.3902439024390243,
      "grad_norm": 57.781219482421875,
      "learning_rate": 7.344173441734418e-06,
      "loss": 1.5492,
      "step": 8757
    },
    {
      "epoch": 3.3906310491676344,
      "grad_norm": 8.66134262084961,
      "learning_rate": 7.343743278702628e-06,
      "loss": 0.367,
      "step": 8758
    },
    {
      "epoch": 3.391018195896245,
      "grad_norm": 15.980627059936523,
      "learning_rate": 7.343313115670841e-06,
      "loss": 1.4039,
      "step": 8759
    },
    {
      "epoch": 3.391405342624855,
      "grad_norm": 25.849775314331055,
      "learning_rate": 7.342882952639051e-06,
      "loss": 0.9506,
      "step": 8760
    },
    {
      "epoch": 3.391792489353465,
      "grad_norm": 40.365840911865234,
      "learning_rate": 7.342452789607262e-06,
      "loss": 0.9253,
      "step": 8761
    },
    {
      "epoch": 3.392179636082075,
      "grad_norm": 18.689054489135742,
      "learning_rate": 7.342022626575472e-06,
      "loss": 0.8977,
      "step": 8762
    },
    {
      "epoch": 3.392566782810685,
      "grad_norm": 19.954151153564453,
      "learning_rate": 7.341592463543684e-06,
      "loss": 0.8184,
      "step": 8763
    },
    {
      "epoch": 3.392953929539295,
      "grad_norm": 22.99823760986328,
      "learning_rate": 7.341162300511895e-06,
      "loss": 0.693,
      "step": 8764
    },
    {
      "epoch": 3.3933410762679057,
      "grad_norm": 21.108156204223633,
      "learning_rate": 7.340732137480106e-06,
      "loss": 1.1102,
      "step": 8765
    },
    {
      "epoch": 3.3937282229965158,
      "grad_norm": 9.025908470153809,
      "learning_rate": 7.340301974448316e-06,
      "loss": 0.8349,
      "step": 8766
    },
    {
      "epoch": 3.394115369725126,
      "grad_norm": 24.078176498413086,
      "learning_rate": 7.339871811416528e-06,
      "loss": 1.8815,
      "step": 8767
    },
    {
      "epoch": 3.394502516453736,
      "grad_norm": 15.128615379333496,
      "learning_rate": 7.339441648384738e-06,
      "loss": 1.8749,
      "step": 8768
    },
    {
      "epoch": 3.394889663182346,
      "grad_norm": 16.711284637451172,
      "learning_rate": 7.339011485352949e-06,
      "loss": 1.0117,
      "step": 8769
    },
    {
      "epoch": 3.395276809910956,
      "grad_norm": 25.751131057739258,
      "learning_rate": 7.33858132232116e-06,
      "loss": 2.178,
      "step": 8770
    },
    {
      "epoch": 3.3956639566395665,
      "grad_norm": 19.242198944091797,
      "learning_rate": 7.338151159289372e-06,
      "loss": 1.4222,
      "step": 8771
    },
    {
      "epoch": 3.3960511033681766,
      "grad_norm": 10.468677520751953,
      "learning_rate": 7.337720996257582e-06,
      "loss": 0.6963,
      "step": 8772
    },
    {
      "epoch": 3.3964382500967867,
      "grad_norm": 32.26785659790039,
      "learning_rate": 7.337290833225793e-06,
      "loss": 1.123,
      "step": 8773
    },
    {
      "epoch": 3.3968253968253967,
      "grad_norm": 250.48150634765625,
      "learning_rate": 7.3368606701940034e-06,
      "loss": 3.3913,
      "step": 8774
    },
    {
      "epoch": 3.397212543554007,
      "grad_norm": 43.52910232543945,
      "learning_rate": 7.336430507162216e-06,
      "loss": 2.3015,
      "step": 8775
    },
    {
      "epoch": 3.3975996902826173,
      "grad_norm": 15.32811164855957,
      "learning_rate": 7.336000344130426e-06,
      "loss": 1.2111,
      "step": 8776
    },
    {
      "epoch": 3.3979868370112274,
      "grad_norm": 10.516883850097656,
      "learning_rate": 7.335570181098637e-06,
      "loss": 0.4846,
      "step": 8777
    },
    {
      "epoch": 3.3983739837398375,
      "grad_norm": 14.764639854431152,
      "learning_rate": 7.335140018066847e-06,
      "loss": 1.0877,
      "step": 8778
    },
    {
      "epoch": 3.3987611304684475,
      "grad_norm": 18.521404266357422,
      "learning_rate": 7.33470985503506e-06,
      "loss": 1.8429,
      "step": 8779
    },
    {
      "epoch": 3.3991482771970576,
      "grad_norm": 22.710840225219727,
      "learning_rate": 7.33427969200327e-06,
      "loss": 1.4021,
      "step": 8780
    },
    {
      "epoch": 3.3995354239256677,
      "grad_norm": 13.039287567138672,
      "learning_rate": 7.333849528971481e-06,
      "loss": 0.9673,
      "step": 8781
    },
    {
      "epoch": 3.399922570654278,
      "grad_norm": 49.573238372802734,
      "learning_rate": 7.333419365939691e-06,
      "loss": 0.8794,
      "step": 8782
    },
    {
      "epoch": 3.4003097173828882,
      "grad_norm": 19.582799911499023,
      "learning_rate": 7.332989202907903e-06,
      "loss": 0.8865,
      "step": 8783
    },
    {
      "epoch": 3.4006968641114983,
      "grad_norm": 20.8315372467041,
      "learning_rate": 7.332559039876113e-06,
      "loss": 1.2921,
      "step": 8784
    },
    {
      "epoch": 3.4010840108401084,
      "grad_norm": 21.4609432220459,
      "learning_rate": 7.332128876844325e-06,
      "loss": 2.219,
      "step": 8785
    },
    {
      "epoch": 3.4014711575687184,
      "grad_norm": 47.68800735473633,
      "learning_rate": 7.331698713812535e-06,
      "loss": 1.3402,
      "step": 8786
    },
    {
      "epoch": 3.4018583042973285,
      "grad_norm": 23.403642654418945,
      "learning_rate": 7.331268550780747e-06,
      "loss": 1.3686,
      "step": 8787
    },
    {
      "epoch": 3.402245451025939,
      "grad_norm": 19.996753692626953,
      "learning_rate": 7.330838387748957e-06,
      "loss": 0.8232,
      "step": 8788
    },
    {
      "epoch": 3.402632597754549,
      "grad_norm": 34.88168716430664,
      "learning_rate": 7.330408224717168e-06,
      "loss": 1.3498,
      "step": 8789
    },
    {
      "epoch": 3.403019744483159,
      "grad_norm": 22.22252655029297,
      "learning_rate": 7.3299780616853785e-06,
      "loss": 1.3459,
      "step": 8790
    },
    {
      "epoch": 3.403406891211769,
      "grad_norm": 15.948371887207031,
      "learning_rate": 7.329547898653591e-06,
      "loss": 1.128,
      "step": 8791
    },
    {
      "epoch": 3.4037940379403793,
      "grad_norm": 23.37686538696289,
      "learning_rate": 7.329117735621801e-06,
      "loss": 1.719,
      "step": 8792
    },
    {
      "epoch": 3.40418118466899,
      "grad_norm": 40.053627014160156,
      "learning_rate": 7.328687572590012e-06,
      "loss": 1.6836,
      "step": 8793
    },
    {
      "epoch": 3.4045683313976,
      "grad_norm": 24.813575744628906,
      "learning_rate": 7.3282574095582224e-06,
      "loss": 1.1726,
      "step": 8794
    },
    {
      "epoch": 3.40495547812621,
      "grad_norm": 16.65036392211914,
      "learning_rate": 7.327827246526435e-06,
      "loss": 1.0827,
      "step": 8795
    },
    {
      "epoch": 3.40534262485482,
      "grad_norm": 18.151140213012695,
      "learning_rate": 7.327397083494645e-06,
      "loss": 1.174,
      "step": 8796
    },
    {
      "epoch": 3.40572977158343,
      "grad_norm": 28.5552921295166,
      "learning_rate": 7.326966920462856e-06,
      "loss": 1.5943,
      "step": 8797
    },
    {
      "epoch": 3.40611691831204,
      "grad_norm": 22.772201538085938,
      "learning_rate": 7.326536757431067e-06,
      "loss": 1.7947,
      "step": 8798
    },
    {
      "epoch": 3.40650406504065,
      "grad_norm": 9.163771629333496,
      "learning_rate": 7.326106594399278e-06,
      "loss": 0.2999,
      "step": 8799
    },
    {
      "epoch": 3.4068912117692607,
      "grad_norm": 35.86444854736328,
      "learning_rate": 7.325676431367489e-06,
      "loss": 4.2292,
      "step": 8800
    },
    {
      "epoch": 3.4072783584978708,
      "grad_norm": 43.157798767089844,
      "learning_rate": 7.3252462683357e-06,
      "loss": 1.267,
      "step": 8801
    },
    {
      "epoch": 3.407665505226481,
      "grad_norm": 46.598899841308594,
      "learning_rate": 7.324816105303911e-06,
      "loss": 2.0956,
      "step": 8802
    },
    {
      "epoch": 3.408052651955091,
      "grad_norm": 38.39951705932617,
      "learning_rate": 7.324385942272122e-06,
      "loss": 1.2127,
      "step": 8803
    },
    {
      "epoch": 3.408439798683701,
      "grad_norm": 11.178964614868164,
      "learning_rate": 7.323955779240332e-06,
      "loss": 0.7607,
      "step": 8804
    },
    {
      "epoch": 3.4088269454123115,
      "grad_norm": 26.126590728759766,
      "learning_rate": 7.323525616208543e-06,
      "loss": 2.3866,
      "step": 8805
    },
    {
      "epoch": 3.4092140921409215,
      "grad_norm": 35.531314849853516,
      "learning_rate": 7.323095453176755e-06,
      "loss": 0.3232,
      "step": 8806
    },
    {
      "epoch": 3.4096012388695316,
      "grad_norm": 29.628944396972656,
      "learning_rate": 7.322665290144966e-06,
      "loss": 1.4144,
      "step": 8807
    },
    {
      "epoch": 3.4099883855981417,
      "grad_norm": 39.40626525878906,
      "learning_rate": 7.322235127113176e-06,
      "loss": 1.5054,
      "step": 8808
    },
    {
      "epoch": 3.4103755323267517,
      "grad_norm": 23.547609329223633,
      "learning_rate": 7.321804964081387e-06,
      "loss": 1.9949,
      "step": 8809
    },
    {
      "epoch": 3.410762679055362,
      "grad_norm": 11.733009338378906,
      "learning_rate": 7.321374801049599e-06,
      "loss": 0.6898,
      "step": 8810
    },
    {
      "epoch": 3.4111498257839723,
      "grad_norm": 43.003448486328125,
      "learning_rate": 7.32094463801781e-06,
      "loss": 4.0425,
      "step": 8811
    },
    {
      "epoch": 3.4115369725125824,
      "grad_norm": 12.144335746765137,
      "learning_rate": 7.32051447498602e-06,
      "loss": 0.8807,
      "step": 8812
    },
    {
      "epoch": 3.4119241192411924,
      "grad_norm": 18.81528091430664,
      "learning_rate": 7.320084311954231e-06,
      "loss": 1.7606,
      "step": 8813
    },
    {
      "epoch": 3.4123112659698025,
      "grad_norm": 30.585052490234375,
      "learning_rate": 7.319654148922442e-06,
      "loss": 1.6326,
      "step": 8814
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 15.114313125610352,
      "learning_rate": 7.319223985890654e-06,
      "loss": 1.3983,
      "step": 8815
    },
    {
      "epoch": 3.4130855594270226,
      "grad_norm": 14.614962577819824,
      "learning_rate": 7.318793822858864e-06,
      "loss": 0.8517,
      "step": 8816
    },
    {
      "epoch": 3.413472706155633,
      "grad_norm": 57.75067138671875,
      "learning_rate": 7.318363659827075e-06,
      "loss": 2.0801,
      "step": 8817
    },
    {
      "epoch": 3.413859852884243,
      "grad_norm": 17.087926864624023,
      "learning_rate": 7.317933496795286e-06,
      "loss": 0.2053,
      "step": 8818
    },
    {
      "epoch": 3.4142469996128533,
      "grad_norm": 11.17368221282959,
      "learning_rate": 7.317503333763497e-06,
      "loss": 0.8448,
      "step": 8819
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 23.89006996154785,
      "learning_rate": 7.317073170731707e-06,
      "loss": 0.7891,
      "step": 8820
    },
    {
      "epoch": 3.4150212930700734,
      "grad_norm": 15.790128707885742,
      "learning_rate": 7.316643007699919e-06,
      "loss": 1.3979,
      "step": 8821
    },
    {
      "epoch": 3.415408439798684,
      "grad_norm": 89.57909393310547,
      "learning_rate": 7.31621284466813e-06,
      "loss": 2.0712,
      "step": 8822
    },
    {
      "epoch": 3.415795586527294,
      "grad_norm": 22.43593406677246,
      "learning_rate": 7.315782681636341e-06,
      "loss": 2.2354,
      "step": 8823
    },
    {
      "epoch": 3.416182733255904,
      "grad_norm": 20.69708251953125,
      "learning_rate": 7.315352518604551e-06,
      "loss": 1.42,
      "step": 8824
    },
    {
      "epoch": 3.416569879984514,
      "grad_norm": 12.932147979736328,
      "learning_rate": 7.314922355572762e-06,
      "loss": 0.9876,
      "step": 8825
    },
    {
      "epoch": 3.416957026713124,
      "grad_norm": 23.247825622558594,
      "learning_rate": 7.314492192540974e-06,
      "loss": 1.6532,
      "step": 8826
    },
    {
      "epoch": 3.4173441734417342,
      "grad_norm": 24.026500701904297,
      "learning_rate": 7.314062029509185e-06,
      "loss": 0.9405,
      "step": 8827
    },
    {
      "epoch": 3.4177313201703443,
      "grad_norm": 14.063337326049805,
      "learning_rate": 7.313631866477395e-06,
      "loss": 0.7188,
      "step": 8828
    },
    {
      "epoch": 3.418118466898955,
      "grad_norm": 19.369857788085938,
      "learning_rate": 7.313201703445606e-06,
      "loss": 0.9863,
      "step": 8829
    },
    {
      "epoch": 3.418505613627565,
      "grad_norm": 8.864901542663574,
      "learning_rate": 7.312771540413818e-06,
      "loss": 1.126,
      "step": 8830
    },
    {
      "epoch": 3.418892760356175,
      "grad_norm": 14.321003913879395,
      "learning_rate": 7.312341377382029e-06,
      "loss": 1.1123,
      "step": 8831
    },
    {
      "epoch": 3.419279907084785,
      "grad_norm": 23.18543243408203,
      "learning_rate": 7.311911214350239e-06,
      "loss": 1.1676,
      "step": 8832
    },
    {
      "epoch": 3.419667053813395,
      "grad_norm": 43.682716369628906,
      "learning_rate": 7.31148105131845e-06,
      "loss": 1.4743,
      "step": 8833
    },
    {
      "epoch": 3.4200542005420056,
      "grad_norm": 30.048795700073242,
      "learning_rate": 7.311050888286661e-06,
      "loss": 1.163,
      "step": 8834
    },
    {
      "epoch": 3.4204413472706157,
      "grad_norm": 48.66238021850586,
      "learning_rate": 7.310620725254872e-06,
      "loss": 0.6619,
      "step": 8835
    },
    {
      "epoch": 3.4208284939992257,
      "grad_norm": 37.74787521362305,
      "learning_rate": 7.310190562223083e-06,
      "loss": 1.3178,
      "step": 8836
    },
    {
      "epoch": 3.421215640727836,
      "grad_norm": 72.23200988769531,
      "learning_rate": 7.309760399191294e-06,
      "loss": 1.9485,
      "step": 8837
    },
    {
      "epoch": 3.421602787456446,
      "grad_norm": 35.68667221069336,
      "learning_rate": 7.309330236159505e-06,
      "loss": 1.5966,
      "step": 8838
    },
    {
      "epoch": 3.4219899341850564,
      "grad_norm": 20.928945541381836,
      "learning_rate": 7.308900073127716e-06,
      "loss": 2.1115,
      "step": 8839
    },
    {
      "epoch": 3.4223770809136664,
      "grad_norm": 17.639516830444336,
      "learning_rate": 7.308469910095926e-06,
      "loss": 1.3476,
      "step": 8840
    },
    {
      "epoch": 3.4227642276422765,
      "grad_norm": 16.576862335205078,
      "learning_rate": 7.308039747064139e-06,
      "loss": 1.0112,
      "step": 8841
    },
    {
      "epoch": 3.4231513743708866,
      "grad_norm": 31.686595916748047,
      "learning_rate": 7.307609584032349e-06,
      "loss": 1.543,
      "step": 8842
    },
    {
      "epoch": 3.4235385210994966,
      "grad_norm": 13.489665985107422,
      "learning_rate": 7.30717942100056e-06,
      "loss": 0.8603,
      "step": 8843
    },
    {
      "epoch": 3.4239256678281067,
      "grad_norm": 20.2971134185791,
      "learning_rate": 7.30674925796877e-06,
      "loss": 1.9501,
      "step": 8844
    },
    {
      "epoch": 3.4243128145567168,
      "grad_norm": 37.12980651855469,
      "learning_rate": 7.306319094936983e-06,
      "loss": 2.4021,
      "step": 8845
    },
    {
      "epoch": 3.4246999612853273,
      "grad_norm": 26.916996002197266,
      "learning_rate": 7.305888931905193e-06,
      "loss": 0.9153,
      "step": 8846
    },
    {
      "epoch": 3.4250871080139373,
      "grad_norm": 28.360719680786133,
      "learning_rate": 7.305458768873404e-06,
      "loss": 1.1171,
      "step": 8847
    },
    {
      "epoch": 3.4254742547425474,
      "grad_norm": 14.903935432434082,
      "learning_rate": 7.305028605841614e-06,
      "loss": 0.9584,
      "step": 8848
    },
    {
      "epoch": 3.4258614014711575,
      "grad_norm": 9.907395362854004,
      "learning_rate": 7.304598442809826e-06,
      "loss": 0.8908,
      "step": 8849
    },
    {
      "epoch": 3.4262485481997675,
      "grad_norm": 3.202587127685547,
      "learning_rate": 7.304168279778036e-06,
      "loss": 0.0983,
      "step": 8850
    },
    {
      "epoch": 3.426635694928378,
      "grad_norm": 17.87917709350586,
      "learning_rate": 7.303738116746248e-06,
      "loss": 1.4974,
      "step": 8851
    },
    {
      "epoch": 3.427022841656988,
      "grad_norm": 10.644868850708008,
      "learning_rate": 7.303307953714458e-06,
      "loss": 0.715,
      "step": 8852
    },
    {
      "epoch": 3.427409988385598,
      "grad_norm": 9.640436172485352,
      "learning_rate": 7.30287779068267e-06,
      "loss": 0.4819,
      "step": 8853
    },
    {
      "epoch": 3.4277971351142082,
      "grad_norm": 26.978275299072266,
      "learning_rate": 7.30244762765088e-06,
      "loss": 1.4403,
      "step": 8854
    },
    {
      "epoch": 3.4281842818428183,
      "grad_norm": 18.034555435180664,
      "learning_rate": 7.302017464619091e-06,
      "loss": 0.9361,
      "step": 8855
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 24.526567459106445,
      "learning_rate": 7.301587301587301e-06,
      "loss": 1.2672,
      "step": 8856
    },
    {
      "epoch": 3.428958575300039,
      "grad_norm": 4.5943474769592285,
      "learning_rate": 7.301157138555514e-06,
      "loss": 0.246,
      "step": 8857
    },
    {
      "epoch": 3.429345722028649,
      "grad_norm": 17.053516387939453,
      "learning_rate": 7.300726975523724e-06,
      "loss": 1.0357,
      "step": 8858
    },
    {
      "epoch": 3.429732868757259,
      "grad_norm": 27.1456356048584,
      "learning_rate": 7.300296812491935e-06,
      "loss": 0.9089,
      "step": 8859
    },
    {
      "epoch": 3.430120015485869,
      "grad_norm": 9.787580490112305,
      "learning_rate": 7.299866649460145e-06,
      "loss": 0.5222,
      "step": 8860
    },
    {
      "epoch": 3.430507162214479,
      "grad_norm": 38.26295471191406,
      "learning_rate": 7.299436486428358e-06,
      "loss": 1.6291,
      "step": 8861
    },
    {
      "epoch": 3.430894308943089,
      "grad_norm": 23.363773345947266,
      "learning_rate": 7.299006323396568e-06,
      "loss": 1.8449,
      "step": 8862
    },
    {
      "epoch": 3.4312814556716997,
      "grad_norm": 34.16771697998047,
      "learning_rate": 7.298576160364779e-06,
      "loss": 2.0958,
      "step": 8863
    },
    {
      "epoch": 3.43166860240031,
      "grad_norm": 24.557415008544922,
      "learning_rate": 7.298145997332989e-06,
      "loss": 1.2468,
      "step": 8864
    },
    {
      "epoch": 3.43205574912892,
      "grad_norm": 29.54440689086914,
      "learning_rate": 7.297715834301201e-06,
      "loss": 0.5241,
      "step": 8865
    },
    {
      "epoch": 3.43244289585753,
      "grad_norm": 18.924997329711914,
      "learning_rate": 7.297285671269412e-06,
      "loss": 1.4255,
      "step": 8866
    },
    {
      "epoch": 3.43283004258614,
      "grad_norm": 21.2852840423584,
      "learning_rate": 7.296855508237623e-06,
      "loss": 1.5005,
      "step": 8867
    },
    {
      "epoch": 3.4332171893147505,
      "grad_norm": 20.843582153320312,
      "learning_rate": 7.296425345205833e-06,
      "loss": 0.6069,
      "step": 8868
    },
    {
      "epoch": 3.4336043360433606,
      "grad_norm": 25.955665588378906,
      "learning_rate": 7.295995182174045e-06,
      "loss": 1.5925,
      "step": 8869
    },
    {
      "epoch": 3.4339914827719706,
      "grad_norm": 10.839675903320312,
      "learning_rate": 7.295565019142255e-06,
      "loss": 0.8632,
      "step": 8870
    },
    {
      "epoch": 3.4343786295005807,
      "grad_norm": 9.82548713684082,
      "learning_rate": 7.295134856110466e-06,
      "loss": 0.8095,
      "step": 8871
    },
    {
      "epoch": 3.4347657762291908,
      "grad_norm": 16.737154006958008,
      "learning_rate": 7.294704693078677e-06,
      "loss": 1.1421,
      "step": 8872
    },
    {
      "epoch": 3.435152922957801,
      "grad_norm": 28.365583419799805,
      "learning_rate": 7.294274530046889e-06,
      "loss": 1.5687,
      "step": 8873
    },
    {
      "epoch": 3.435540069686411,
      "grad_norm": 33.97214126586914,
      "learning_rate": 7.293844367015099e-06,
      "loss": 1.9578,
      "step": 8874
    },
    {
      "epoch": 3.4359272164150214,
      "grad_norm": 15.737982749938965,
      "learning_rate": 7.29341420398331e-06,
      "loss": 1.1084,
      "step": 8875
    },
    {
      "epoch": 3.4363143631436315,
      "grad_norm": 34.74448013305664,
      "learning_rate": 7.29298404095152e-06,
      "loss": 2.042,
      "step": 8876
    },
    {
      "epoch": 3.4367015098722415,
      "grad_norm": 15.165740013122559,
      "learning_rate": 7.292553877919733e-06,
      "loss": 0.9196,
      "step": 8877
    },
    {
      "epoch": 3.4370886566008516,
      "grad_norm": 24.456329345703125,
      "learning_rate": 7.292123714887943e-06,
      "loss": 2.6547,
      "step": 8878
    },
    {
      "epoch": 3.4374758033294617,
      "grad_norm": 43.54961013793945,
      "learning_rate": 7.291693551856154e-06,
      "loss": 1.4433,
      "step": 8879
    },
    {
      "epoch": 3.437862950058072,
      "grad_norm": 38.772315979003906,
      "learning_rate": 7.291263388824365e-06,
      "loss": 1.631,
      "step": 8880
    },
    {
      "epoch": 3.4382500967866823,
      "grad_norm": 21.721511840820312,
      "learning_rate": 7.290833225792577e-06,
      "loss": 1.2053,
      "step": 8881
    },
    {
      "epoch": 3.4386372435152923,
      "grad_norm": 7.612221717834473,
      "learning_rate": 7.290403062760787e-06,
      "loss": 0.5444,
      "step": 8882
    },
    {
      "epoch": 3.4390243902439024,
      "grad_norm": 13.52267837524414,
      "learning_rate": 7.289972899728998e-06,
      "loss": 1.1995,
      "step": 8883
    },
    {
      "epoch": 3.4394115369725125,
      "grad_norm": 12.828919410705566,
      "learning_rate": 7.289542736697209e-06,
      "loss": 0.5755,
      "step": 8884
    },
    {
      "epoch": 3.439798683701123,
      "grad_norm": 16.100902557373047,
      "learning_rate": 7.28911257366542e-06,
      "loss": 1.8313,
      "step": 8885
    },
    {
      "epoch": 3.440185830429733,
      "grad_norm": 49.940799713134766,
      "learning_rate": 7.28868241063363e-06,
      "loss": 1.9884,
      "step": 8886
    },
    {
      "epoch": 3.440572977158343,
      "grad_norm": 21.155685424804688,
      "learning_rate": 7.288252247601842e-06,
      "loss": 0.597,
      "step": 8887
    },
    {
      "epoch": 3.440960123886953,
      "grad_norm": 20.867111206054688,
      "learning_rate": 7.287822084570053e-06,
      "loss": 1.8093,
      "step": 8888
    },
    {
      "epoch": 3.4413472706155632,
      "grad_norm": 33.528076171875,
      "learning_rate": 7.287391921538264e-06,
      "loss": 1.7249,
      "step": 8889
    },
    {
      "epoch": 3.4417344173441733,
      "grad_norm": 29.884435653686523,
      "learning_rate": 7.286961758506474e-06,
      "loss": 1.4598,
      "step": 8890
    },
    {
      "epoch": 3.4421215640727834,
      "grad_norm": 19.537273406982422,
      "learning_rate": 7.286531595474685e-06,
      "loss": 1.6102,
      "step": 8891
    },
    {
      "epoch": 3.442508710801394,
      "grad_norm": 63.517757415771484,
      "learning_rate": 7.286101432442897e-06,
      "loss": 1.2847,
      "step": 8892
    },
    {
      "epoch": 3.442895857530004,
      "grad_norm": 45.420326232910156,
      "learning_rate": 7.285671269411108e-06,
      "loss": 1.5332,
      "step": 8893
    },
    {
      "epoch": 3.443283004258614,
      "grad_norm": 19.02275848388672,
      "learning_rate": 7.285241106379318e-06,
      "loss": 1.1792,
      "step": 8894
    },
    {
      "epoch": 3.443670150987224,
      "grad_norm": 18.672582626342773,
      "learning_rate": 7.284810943347529e-06,
      "loss": 1.151,
      "step": 8895
    },
    {
      "epoch": 3.444057297715834,
      "grad_norm": 10.699814796447754,
      "learning_rate": 7.284380780315741e-06,
      "loss": 1.3162,
      "step": 8896
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 13.099350929260254,
      "learning_rate": 7.283950617283952e-06,
      "loss": 1.8991,
      "step": 8897
    },
    {
      "epoch": 3.4448315911730547,
      "grad_norm": 22.613014221191406,
      "learning_rate": 7.283520454252162e-06,
      "loss": 0.6962,
      "step": 8898
    },
    {
      "epoch": 3.4452187379016648,
      "grad_norm": 21.74626922607422,
      "learning_rate": 7.283090291220373e-06,
      "loss": 2.1885,
      "step": 8899
    },
    {
      "epoch": 3.445605884630275,
      "grad_norm": 37.064453125,
      "learning_rate": 7.282660128188584e-06,
      "loss": 1.8188,
      "step": 8900
    },
    {
      "epoch": 3.445993031358885,
      "grad_norm": 9.484025001525879,
      "learning_rate": 7.282229965156795e-06,
      "loss": 1.0721,
      "step": 8901
    },
    {
      "epoch": 3.446380178087495,
      "grad_norm": 20.84376335144043,
      "learning_rate": 7.281799802125006e-06,
      "loss": 1.2597,
      "step": 8902
    },
    {
      "epoch": 3.4467673248161055,
      "grad_norm": 22.04924201965332,
      "learning_rate": 7.281369639093217e-06,
      "loss": 1.252,
      "step": 8903
    },
    {
      "epoch": 3.4471544715447155,
      "grad_norm": 23.826173782348633,
      "learning_rate": 7.280939476061428e-06,
      "loss": 1.6423,
      "step": 8904
    },
    {
      "epoch": 3.4475416182733256,
      "grad_norm": 52.55390930175781,
      "learning_rate": 7.280509313029639e-06,
      "loss": 1.6391,
      "step": 8905
    },
    {
      "epoch": 3.4479287650019357,
      "grad_norm": 15.34317684173584,
      "learning_rate": 7.280079149997849e-06,
      "loss": 1.1413,
      "step": 8906
    },
    {
      "epoch": 3.4483159117305457,
      "grad_norm": 17.046287536621094,
      "learning_rate": 7.27964898696606e-06,
      "loss": 0.8778,
      "step": 8907
    },
    {
      "epoch": 3.448703058459156,
      "grad_norm": 23.148008346557617,
      "learning_rate": 7.279218823934272e-06,
      "loss": 1.7477,
      "step": 8908
    },
    {
      "epoch": 3.4490902051877663,
      "grad_norm": 10.456557273864746,
      "learning_rate": 7.278788660902483e-06,
      "loss": 0.1123,
      "step": 8909
    },
    {
      "epoch": 3.4494773519163764,
      "grad_norm": 35.34552764892578,
      "learning_rate": 7.278358497870693e-06,
      "loss": 2.3885,
      "step": 8910
    },
    {
      "epoch": 3.4498644986449865,
      "grad_norm": 59.36891174316406,
      "learning_rate": 7.277928334838904e-06,
      "loss": 2.4015,
      "step": 8911
    },
    {
      "epoch": 3.4502516453735965,
      "grad_norm": 22.40181541442871,
      "learning_rate": 7.277498171807116e-06,
      "loss": 1.1992,
      "step": 8912
    },
    {
      "epoch": 3.4506387921022066,
      "grad_norm": 26.368907928466797,
      "learning_rate": 7.277068008775327e-06,
      "loss": 2.711,
      "step": 8913
    },
    {
      "epoch": 3.451025938830817,
      "grad_norm": 20.85713005065918,
      "learning_rate": 7.276637845743537e-06,
      "loss": 0.9267,
      "step": 8914
    },
    {
      "epoch": 3.451413085559427,
      "grad_norm": 37.21540451049805,
      "learning_rate": 7.276207682711748e-06,
      "loss": 1.3297,
      "step": 8915
    },
    {
      "epoch": 3.4518002322880372,
      "grad_norm": 25.5894775390625,
      "learning_rate": 7.275777519679959e-06,
      "loss": 1.2565,
      "step": 8916
    },
    {
      "epoch": 3.4521873790166473,
      "grad_norm": 53.67021179199219,
      "learning_rate": 7.275347356648171e-06,
      "loss": 0.955,
      "step": 8917
    },
    {
      "epoch": 3.4525745257452574,
      "grad_norm": 34.74226379394531,
      "learning_rate": 7.274917193616381e-06,
      "loss": 1.1057,
      "step": 8918
    },
    {
      "epoch": 3.4529616724738674,
      "grad_norm": 22.940980911254883,
      "learning_rate": 7.274487030584592e-06,
      "loss": 1.7275,
      "step": 8919
    },
    {
      "epoch": 3.4533488192024775,
      "grad_norm": 11.748199462890625,
      "learning_rate": 7.274056867552803e-06,
      "loss": 1.3362,
      "step": 8920
    },
    {
      "epoch": 3.453735965931088,
      "grad_norm": 67.3867416381836,
      "learning_rate": 7.273626704521014e-06,
      "loss": 1.5918,
      "step": 8921
    },
    {
      "epoch": 3.454123112659698,
      "grad_norm": 23.165176391601562,
      "learning_rate": 7.273196541489224e-06,
      "loss": 1.0569,
      "step": 8922
    },
    {
      "epoch": 3.454510259388308,
      "grad_norm": 32.847843170166016,
      "learning_rate": 7.272766378457437e-06,
      "loss": 1.6064,
      "step": 8923
    },
    {
      "epoch": 3.454897406116918,
      "grad_norm": 19.87928581237793,
      "learning_rate": 7.272336215425647e-06,
      "loss": 2.0276,
      "step": 8924
    },
    {
      "epoch": 3.4552845528455283,
      "grad_norm": 16.008256912231445,
      "learning_rate": 7.271906052393858e-06,
      "loss": 0.854,
      "step": 8925
    },
    {
      "epoch": 3.4556716995741388,
      "grad_norm": 32.93891143798828,
      "learning_rate": 7.271475889362068e-06,
      "loss": 1.5603,
      "step": 8926
    },
    {
      "epoch": 3.456058846302749,
      "grad_norm": 42.14109802246094,
      "learning_rate": 7.271045726330281e-06,
      "loss": 1.2242,
      "step": 8927
    },
    {
      "epoch": 3.456445993031359,
      "grad_norm": 17.74486541748047,
      "learning_rate": 7.270615563298491e-06,
      "loss": 0.5789,
      "step": 8928
    },
    {
      "epoch": 3.456833139759969,
      "grad_norm": 51.08684158325195,
      "learning_rate": 7.270185400266702e-06,
      "loss": 2.5857,
      "step": 8929
    },
    {
      "epoch": 3.457220286488579,
      "grad_norm": 25.79384994506836,
      "learning_rate": 7.269755237234912e-06,
      "loss": 0.6618,
      "step": 8930
    },
    {
      "epoch": 3.4576074332171896,
      "grad_norm": 13.881192207336426,
      "learning_rate": 7.269325074203124e-06,
      "loss": 0.5741,
      "step": 8931
    },
    {
      "epoch": 3.4579945799457996,
      "grad_norm": 25.157960891723633,
      "learning_rate": 7.268894911171335e-06,
      "loss": 1.5476,
      "step": 8932
    },
    {
      "epoch": 3.4583817266744097,
      "grad_norm": 32.14382553100586,
      "learning_rate": 7.268464748139546e-06,
      "loss": 2.013,
      "step": 8933
    },
    {
      "epoch": 3.4587688734030198,
      "grad_norm": 29.529340744018555,
      "learning_rate": 7.268034585107756e-06,
      "loss": 1.1616,
      "step": 8934
    },
    {
      "epoch": 3.45915602013163,
      "grad_norm": 87.01872253417969,
      "learning_rate": 7.267604422075968e-06,
      "loss": 1.8288,
      "step": 8935
    },
    {
      "epoch": 3.45954316686024,
      "grad_norm": 11.631622314453125,
      "learning_rate": 7.267174259044178e-06,
      "loss": 0.9628,
      "step": 8936
    },
    {
      "epoch": 3.45993031358885,
      "grad_norm": 15.269205093383789,
      "learning_rate": 7.266744096012389e-06,
      "loss": 0.5552,
      "step": 8937
    },
    {
      "epoch": 3.4603174603174605,
      "grad_norm": 42.58147048950195,
      "learning_rate": 7.2663139329806e-06,
      "loss": 2.2684,
      "step": 8938
    },
    {
      "epoch": 3.4607046070460705,
      "grad_norm": 21.7294921875,
      "learning_rate": 7.265883769948812e-06,
      "loss": 0.9948,
      "step": 8939
    },
    {
      "epoch": 3.4610917537746806,
      "grad_norm": 38.475406646728516,
      "learning_rate": 7.265453606917022e-06,
      "loss": 1.4711,
      "step": 8940
    },
    {
      "epoch": 3.4614789005032907,
      "grad_norm": 16.553747177124023,
      "learning_rate": 7.265023443885233e-06,
      "loss": 1.0968,
      "step": 8941
    },
    {
      "epoch": 3.4618660472319007,
      "grad_norm": 56.347930908203125,
      "learning_rate": 7.264593280853443e-06,
      "loss": 2.3016,
      "step": 8942
    },
    {
      "epoch": 3.4622531939605112,
      "grad_norm": 28.987333297729492,
      "learning_rate": 7.264163117821656e-06,
      "loss": 1.4975,
      "step": 8943
    },
    {
      "epoch": 3.4626403406891213,
      "grad_norm": 12.913782119750977,
      "learning_rate": 7.263732954789866e-06,
      "loss": 1.4625,
      "step": 8944
    },
    {
      "epoch": 3.4630274874177314,
      "grad_norm": 37.39287567138672,
      "learning_rate": 7.263302791758077e-06,
      "loss": 3.2742,
      "step": 8945
    },
    {
      "epoch": 3.4634146341463414,
      "grad_norm": 18.598880767822266,
      "learning_rate": 7.262872628726287e-06,
      "loss": 1.4987,
      "step": 8946
    },
    {
      "epoch": 3.4638017808749515,
      "grad_norm": 41.855953216552734,
      "learning_rate": 7.2624424656945e-06,
      "loss": 1.0302,
      "step": 8947
    },
    {
      "epoch": 3.4641889276035616,
      "grad_norm": 22.96485710144043,
      "learning_rate": 7.26201230266271e-06,
      "loss": 1.1512,
      "step": 8948
    },
    {
      "epoch": 3.464576074332172,
      "grad_norm": 12.233013153076172,
      "learning_rate": 7.261582139630921e-06,
      "loss": 1.2542,
      "step": 8949
    },
    {
      "epoch": 3.464963221060782,
      "grad_norm": 16.12986183166504,
      "learning_rate": 7.261151976599131e-06,
      "loss": 0.8075,
      "step": 8950
    },
    {
      "epoch": 3.465350367789392,
      "grad_norm": 13.642927169799805,
      "learning_rate": 7.260721813567343e-06,
      "loss": 0.5176,
      "step": 8951
    },
    {
      "epoch": 3.4657375145180023,
      "grad_norm": 26.67698860168457,
      "learning_rate": 7.260291650535553e-06,
      "loss": 0.5465,
      "step": 8952
    },
    {
      "epoch": 3.4661246612466123,
      "grad_norm": 11.549079895019531,
      "learning_rate": 7.259861487503765e-06,
      "loss": 0.5249,
      "step": 8953
    },
    {
      "epoch": 3.4665118079752224,
      "grad_norm": 174.05926513671875,
      "learning_rate": 7.259431324471975e-06,
      "loss": 1.6164,
      "step": 8954
    },
    {
      "epoch": 3.466898954703833,
      "grad_norm": 17.069385528564453,
      "learning_rate": 7.259001161440187e-06,
      "loss": 1.1256,
      "step": 8955
    },
    {
      "epoch": 3.467286101432443,
      "grad_norm": 10.650778770446777,
      "learning_rate": 7.258570998408397e-06,
      "loss": 0.8037,
      "step": 8956
    },
    {
      "epoch": 3.467673248161053,
      "grad_norm": 7.980023384094238,
      "learning_rate": 7.258140835376608e-06,
      "loss": 0.357,
      "step": 8957
    },
    {
      "epoch": 3.468060394889663,
      "grad_norm": 7.742191314697266,
      "learning_rate": 7.257710672344818e-06,
      "loss": 0.3056,
      "step": 8958
    },
    {
      "epoch": 3.468447541618273,
      "grad_norm": 29.71932029724121,
      "learning_rate": 7.257280509313031e-06,
      "loss": 2.1106,
      "step": 8959
    },
    {
      "epoch": 3.4688346883468837,
      "grad_norm": 9.135890007019043,
      "learning_rate": 7.256850346281241e-06,
      "loss": 1.1031,
      "step": 8960
    },
    {
      "epoch": 3.4692218350754938,
      "grad_norm": 16.119169235229492,
      "learning_rate": 7.256420183249452e-06,
      "loss": 0.8725,
      "step": 8961
    },
    {
      "epoch": 3.469608981804104,
      "grad_norm": 16.998361587524414,
      "learning_rate": 7.255990020217664e-06,
      "loss": 0.7723,
      "step": 8962
    },
    {
      "epoch": 3.469996128532714,
      "grad_norm": 36.94635772705078,
      "learning_rate": 7.255559857185875e-06,
      "loss": 1.2415,
      "step": 8963
    },
    {
      "epoch": 3.470383275261324,
      "grad_norm": 85.93429565429688,
      "learning_rate": 7.255129694154085e-06,
      "loss": 1.6461,
      "step": 8964
    },
    {
      "epoch": 3.470770421989934,
      "grad_norm": 31.385765075683594,
      "learning_rate": 7.254699531122296e-06,
      "loss": 1.9968,
      "step": 8965
    },
    {
      "epoch": 3.471157568718544,
      "grad_norm": 25.505098342895508,
      "learning_rate": 7.254269368090507e-06,
      "loss": 2.064,
      "step": 8966
    },
    {
      "epoch": 3.4715447154471546,
      "grad_norm": 25.060680389404297,
      "learning_rate": 7.253839205058718e-06,
      "loss": 1.2474,
      "step": 8967
    },
    {
      "epoch": 3.4719318621757647,
      "grad_norm": 19.875591278076172,
      "learning_rate": 7.253409042026929e-06,
      "loss": 1.0551,
      "step": 8968
    },
    {
      "epoch": 3.4723190089043747,
      "grad_norm": 9.324041366577148,
      "learning_rate": 7.25297887899514e-06,
      "loss": 0.6484,
      "step": 8969
    },
    {
      "epoch": 3.472706155632985,
      "grad_norm": 99.79590606689453,
      "learning_rate": 7.252548715963351e-06,
      "loss": 1.4674,
      "step": 8970
    },
    {
      "epoch": 3.473093302361595,
      "grad_norm": 28.602243423461914,
      "learning_rate": 7.252118552931562e-06,
      "loss": 1.5153,
      "step": 8971
    },
    {
      "epoch": 3.4734804490902054,
      "grad_norm": 12.307422637939453,
      "learning_rate": 7.251688389899772e-06,
      "loss": 1.1585,
      "step": 8972
    },
    {
      "epoch": 3.4738675958188154,
      "grad_norm": 19.026643753051758,
      "learning_rate": 7.251258226867983e-06,
      "loss": 1.9454,
      "step": 8973
    },
    {
      "epoch": 3.4742547425474255,
      "grad_norm": 17.858760833740234,
      "learning_rate": 7.250828063836195e-06,
      "loss": 1.0305,
      "step": 8974
    },
    {
      "epoch": 3.4746418892760356,
      "grad_norm": 14.297687530517578,
      "learning_rate": 7.250397900804406e-06,
      "loss": 0.9896,
      "step": 8975
    },
    {
      "epoch": 3.4750290360046456,
      "grad_norm": 19.590801239013672,
      "learning_rate": 7.249967737772616e-06,
      "loss": 0.8125,
      "step": 8976
    },
    {
      "epoch": 3.475416182733256,
      "grad_norm": 22.856945037841797,
      "learning_rate": 7.249537574740827e-06,
      "loss": 1.591,
      "step": 8977
    },
    {
      "epoch": 3.475803329461866,
      "grad_norm": 8.397576332092285,
      "learning_rate": 7.249107411709039e-06,
      "loss": 0.6646,
      "step": 8978
    },
    {
      "epoch": 3.4761904761904763,
      "grad_norm": 56.14427185058594,
      "learning_rate": 7.24867724867725e-06,
      "loss": 3.1622,
      "step": 8979
    },
    {
      "epoch": 3.4765776229190863,
      "grad_norm": 14.776817321777344,
      "learning_rate": 7.24824708564546e-06,
      "loss": 1.2309,
      "step": 8980
    },
    {
      "epoch": 3.4769647696476964,
      "grad_norm": 16.10367774963379,
      "learning_rate": 7.247816922613671e-06,
      "loss": 0.6657,
      "step": 8981
    },
    {
      "epoch": 3.4773519163763065,
      "grad_norm": 11.654605865478516,
      "learning_rate": 7.247386759581882e-06,
      "loss": 1.3009,
      "step": 8982
    },
    {
      "epoch": 3.4777390631049165,
      "grad_norm": 35.061790466308594,
      "learning_rate": 7.246956596550094e-06,
      "loss": 0.9116,
      "step": 8983
    },
    {
      "epoch": 3.478126209833527,
      "grad_norm": 16.97789764404297,
      "learning_rate": 7.246526433518304e-06,
      "loss": 0.7894,
      "step": 8984
    },
    {
      "epoch": 3.478513356562137,
      "grad_norm": 24.605127334594727,
      "learning_rate": 7.246096270486515e-06,
      "loss": 1.2855,
      "step": 8985
    },
    {
      "epoch": 3.478900503290747,
      "grad_norm": 8.931782722473145,
      "learning_rate": 7.245666107454726e-06,
      "loss": 0.51,
      "step": 8986
    },
    {
      "epoch": 3.4792876500193572,
      "grad_norm": 24.923748016357422,
      "learning_rate": 7.245235944422937e-06,
      "loss": 1.4523,
      "step": 8987
    },
    {
      "epoch": 3.4796747967479673,
      "grad_norm": 20.959285736083984,
      "learning_rate": 7.244805781391147e-06,
      "loss": 1.0899,
      "step": 8988
    },
    {
      "epoch": 3.480061943476578,
      "grad_norm": 26.48220443725586,
      "learning_rate": 7.244375618359359e-06,
      "loss": 1.4311,
      "step": 8989
    },
    {
      "epoch": 3.480449090205188,
      "grad_norm": 25.689228057861328,
      "learning_rate": 7.24394545532757e-06,
      "loss": 1.911,
      "step": 8990
    },
    {
      "epoch": 3.480836236933798,
      "grad_norm": 27.556344985961914,
      "learning_rate": 7.243515292295781e-06,
      "loss": 0.5828,
      "step": 8991
    },
    {
      "epoch": 3.481223383662408,
      "grad_norm": 21.83699607849121,
      "learning_rate": 7.243085129263991e-06,
      "loss": 1.3972,
      "step": 8992
    },
    {
      "epoch": 3.481610530391018,
      "grad_norm": 14.998790740966797,
      "learning_rate": 7.242654966232202e-06,
      "loss": 0.7721,
      "step": 8993
    },
    {
      "epoch": 3.481997677119628,
      "grad_norm": 20.312074661254883,
      "learning_rate": 7.242224803200414e-06,
      "loss": 0.5864,
      "step": 8994
    },
    {
      "epoch": 3.4823848238482387,
      "grad_norm": 32.671485900878906,
      "learning_rate": 7.241794640168625e-06,
      "loss": 1.4966,
      "step": 8995
    },
    {
      "epoch": 3.4827719705768487,
      "grad_norm": 20.6109561920166,
      "learning_rate": 7.241364477136835e-06,
      "loss": 1.456,
      "step": 8996
    },
    {
      "epoch": 3.483159117305459,
      "grad_norm": 27.23020362854004,
      "learning_rate": 7.240934314105046e-06,
      "loss": 0.8881,
      "step": 8997
    },
    {
      "epoch": 3.483546264034069,
      "grad_norm": 22.888696670532227,
      "learning_rate": 7.240504151073258e-06,
      "loss": 1.5938,
      "step": 8998
    },
    {
      "epoch": 3.483933410762679,
      "grad_norm": 32.76228332519531,
      "learning_rate": 7.240073988041469e-06,
      "loss": 1.5794,
      "step": 8999
    },
    {
      "epoch": 3.484320557491289,
      "grad_norm": 30.5672550201416,
      "learning_rate": 7.239643825009679e-06,
      "loss": 0.7199,
      "step": 9000
    },
    {
      "epoch": 3.4847077042198995,
      "grad_norm": 12.21448802947998,
      "learning_rate": 7.23921366197789e-06,
      "loss": 0.7719,
      "step": 9001
    },
    {
      "epoch": 3.4850948509485096,
      "grad_norm": 12.826371192932129,
      "learning_rate": 7.238783498946101e-06,
      "loss": 1.7759,
      "step": 9002
    },
    {
      "epoch": 3.4854819976771196,
      "grad_norm": 39.23196029663086,
      "learning_rate": 7.238353335914312e-06,
      "loss": 1.7675,
      "step": 9003
    },
    {
      "epoch": 3.4858691444057297,
      "grad_norm": 17.240842819213867,
      "learning_rate": 7.237923172882523e-06,
      "loss": 0.8125,
      "step": 9004
    },
    {
      "epoch": 3.4862562911343398,
      "grad_norm": 20.563901901245117,
      "learning_rate": 7.237493009850735e-06,
      "loss": 0.9818,
      "step": 9005
    },
    {
      "epoch": 3.4866434378629503,
      "grad_norm": 16.686981201171875,
      "learning_rate": 7.237062846818945e-06,
      "loss": 0.4941,
      "step": 9006
    },
    {
      "epoch": 3.4870305845915603,
      "grad_norm": 38.604957580566406,
      "learning_rate": 7.236632683787156e-06,
      "loss": 1.5971,
      "step": 9007
    },
    {
      "epoch": 3.4874177313201704,
      "grad_norm": 26.844648361206055,
      "learning_rate": 7.236202520755366e-06,
      "loss": 0.9343,
      "step": 9008
    },
    {
      "epoch": 3.4878048780487805,
      "grad_norm": 26.06935691833496,
      "learning_rate": 7.2357723577235786e-06,
      "loss": 1.7019,
      "step": 9009
    },
    {
      "epoch": 3.4881920247773905,
      "grad_norm": 11.480231285095215,
      "learning_rate": 7.235342194691789e-06,
      "loss": 0.4317,
      "step": 9010
    },
    {
      "epoch": 3.4885791715060006,
      "grad_norm": 26.322324752807617,
      "learning_rate": 7.23491203166e-06,
      "loss": 1.3537,
      "step": 9011
    },
    {
      "epoch": 3.4889663182346107,
      "grad_norm": 17.848222732543945,
      "learning_rate": 7.23448186862821e-06,
      "loss": 0.987,
      "step": 9012
    },
    {
      "epoch": 3.489353464963221,
      "grad_norm": 18.491819381713867,
      "learning_rate": 7.2340517055964225e-06,
      "loss": 0.9576,
      "step": 9013
    },
    {
      "epoch": 3.4897406116918313,
      "grad_norm": 32.53398895263672,
      "learning_rate": 7.233621542564633e-06,
      "loss": 1.6842,
      "step": 9014
    },
    {
      "epoch": 3.4901277584204413,
      "grad_norm": 46.68730545043945,
      "learning_rate": 7.233191379532844e-06,
      "loss": 2.1004,
      "step": 9015
    },
    {
      "epoch": 3.4905149051490514,
      "grad_norm": 29.533905029296875,
      "learning_rate": 7.232761216501054e-06,
      "loss": 1.1109,
      "step": 9016
    },
    {
      "epoch": 3.4909020518776614,
      "grad_norm": 12.366188049316406,
      "learning_rate": 7.232331053469266e-06,
      "loss": 0.8824,
      "step": 9017
    },
    {
      "epoch": 3.491289198606272,
      "grad_norm": 19.438095092773438,
      "learning_rate": 7.231900890437476e-06,
      "loss": 0.9984,
      "step": 9018
    },
    {
      "epoch": 3.491676345334882,
      "grad_norm": 35.95896911621094,
      "learning_rate": 7.231470727405688e-06,
      "loss": 1.2772,
      "step": 9019
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 25.861953735351562,
      "learning_rate": 7.231040564373898e-06,
      "loss": 1.6437,
      "step": 9020
    },
    {
      "epoch": 3.492450638792102,
      "grad_norm": 28.63344955444336,
      "learning_rate": 7.23061040134211e-06,
      "loss": 1.7232,
      "step": 9021
    },
    {
      "epoch": 3.4928377855207122,
      "grad_norm": 14.926535606384277,
      "learning_rate": 7.23018023831032e-06,
      "loss": 1.3658,
      "step": 9022
    },
    {
      "epoch": 3.4932249322493227,
      "grad_norm": 29.80790901184082,
      "learning_rate": 7.229750075278531e-06,
      "loss": 1.2858,
      "step": 9023
    },
    {
      "epoch": 3.493612078977933,
      "grad_norm": 36.549137115478516,
      "learning_rate": 7.229319912246741e-06,
      "loss": 2.7518,
      "step": 9024
    },
    {
      "epoch": 3.493999225706543,
      "grad_norm": 26.424219131469727,
      "learning_rate": 7.228889749214954e-06,
      "loss": 2.0607,
      "step": 9025
    },
    {
      "epoch": 3.494386372435153,
      "grad_norm": 14.80460262298584,
      "learning_rate": 7.228459586183164e-06,
      "loss": 1.3068,
      "step": 9026
    },
    {
      "epoch": 3.494773519163763,
      "grad_norm": 37.81233215332031,
      "learning_rate": 7.228029423151375e-06,
      "loss": 1.4693,
      "step": 9027
    },
    {
      "epoch": 3.495160665892373,
      "grad_norm": 10.76939582824707,
      "learning_rate": 7.227599260119585e-06,
      "loss": 1.2243,
      "step": 9028
    },
    {
      "epoch": 3.495547812620983,
      "grad_norm": 20.174476623535156,
      "learning_rate": 7.2271690970877976e-06,
      "loss": 1.7292,
      "step": 9029
    },
    {
      "epoch": 3.4959349593495936,
      "grad_norm": 34.69416427612305,
      "learning_rate": 7.226738934056008e-06,
      "loss": 1.4914,
      "step": 9030
    },
    {
      "epoch": 3.4963221060782037,
      "grad_norm": 23.880178451538086,
      "learning_rate": 7.226308771024219e-06,
      "loss": 0.9101,
      "step": 9031
    },
    {
      "epoch": 3.4967092528068138,
      "grad_norm": 12.98188591003418,
      "learning_rate": 7.225878607992429e-06,
      "loss": 1.3329,
      "step": 9032
    },
    {
      "epoch": 3.497096399535424,
      "grad_norm": 39.36307144165039,
      "learning_rate": 7.225448444960641e-06,
      "loss": 1.3489,
      "step": 9033
    },
    {
      "epoch": 3.497483546264034,
      "grad_norm": 22.47908592224121,
      "learning_rate": 7.225018281928852e-06,
      "loss": 1.5106,
      "step": 9034
    },
    {
      "epoch": 3.4978706929926444,
      "grad_norm": 48.273223876953125,
      "learning_rate": 7.224588118897063e-06,
      "loss": 2.0003,
      "step": 9035
    },
    {
      "epoch": 3.4982578397212545,
      "grad_norm": 10.2050199508667,
      "learning_rate": 7.224157955865273e-06,
      "loss": 1.2061,
      "step": 9036
    },
    {
      "epoch": 3.4986449864498645,
      "grad_norm": 25.535795211791992,
      "learning_rate": 7.223727792833485e-06,
      "loss": 1.653,
      "step": 9037
    },
    {
      "epoch": 3.4990321331784746,
      "grad_norm": 18.083690643310547,
      "learning_rate": 7.223297629801695e-06,
      "loss": 1.3368,
      "step": 9038
    },
    {
      "epoch": 3.4994192799070847,
      "grad_norm": 6.664615154266357,
      "learning_rate": 7.222867466769906e-06,
      "loss": 0.3714,
      "step": 9039
    },
    {
      "epoch": 3.4998064266356947,
      "grad_norm": 12.500166893005371,
      "learning_rate": 7.222437303738117e-06,
      "loss": 0.1513,
      "step": 9040
    },
    {
      "epoch": 3.500193573364305,
      "grad_norm": 10.204174041748047,
      "learning_rate": 7.222007140706329e-06,
      "loss": 0.3992,
      "step": 9041
    },
    {
      "epoch": 3.5005807200929153,
      "grad_norm": 41.21712112426758,
      "learning_rate": 7.221576977674539e-06,
      "loss": 1.7465,
      "step": 9042
    },
    {
      "epoch": 3.5009678668215254,
      "grad_norm": 58.71806335449219,
      "learning_rate": 7.22114681464275e-06,
      "loss": 0.833,
      "step": 9043
    },
    {
      "epoch": 3.5013550135501355,
      "grad_norm": 15.807611465454102,
      "learning_rate": 7.220716651610962e-06,
      "loss": 0.7378,
      "step": 9044
    },
    {
      "epoch": 3.5017421602787455,
      "grad_norm": 19.42245101928711,
      "learning_rate": 7.220286488579173e-06,
      "loss": 1.5456,
      "step": 9045
    },
    {
      "epoch": 3.5021293070073556,
      "grad_norm": 21.82555389404297,
      "learning_rate": 7.219856325547383e-06,
      "loss": 1.2559,
      "step": 9046
    },
    {
      "epoch": 3.502516453735966,
      "grad_norm": 24.453275680541992,
      "learning_rate": 7.219426162515594e-06,
      "loss": 1.1964,
      "step": 9047
    },
    {
      "epoch": 3.502903600464576,
      "grad_norm": 41.338436126708984,
      "learning_rate": 7.218995999483805e-06,
      "loss": 0.9865,
      "step": 9048
    },
    {
      "epoch": 3.5032907471931862,
      "grad_norm": 21.714679718017578,
      "learning_rate": 7.2185658364520166e-06,
      "loss": 1.5812,
      "step": 9049
    },
    {
      "epoch": 3.5036778939217963,
      "grad_norm": 28.333415985107422,
      "learning_rate": 7.218135673420227e-06,
      "loss": 3.0747,
      "step": 9050
    },
    {
      "epoch": 3.5040650406504064,
      "grad_norm": 17.96771240234375,
      "learning_rate": 7.217705510388438e-06,
      "loss": 0.4084,
      "step": 9051
    },
    {
      "epoch": 3.504452187379017,
      "grad_norm": 21.459211349487305,
      "learning_rate": 7.217275347356649e-06,
      "loss": 1.4199,
      "step": 9052
    },
    {
      "epoch": 3.504839334107627,
      "grad_norm": 24.91542625427246,
      "learning_rate": 7.21684518432486e-06,
      "loss": 1.9143,
      "step": 9053
    },
    {
      "epoch": 3.505226480836237,
      "grad_norm": 17.7110538482666,
      "learning_rate": 7.21641502129307e-06,
      "loss": 0.9936,
      "step": 9054
    },
    {
      "epoch": 3.505613627564847,
      "grad_norm": 62.35724639892578,
      "learning_rate": 7.215984858261282e-06,
      "loss": 1.1691,
      "step": 9055
    },
    {
      "epoch": 3.506000774293457,
      "grad_norm": 53.992210388183594,
      "learning_rate": 7.215554695229493e-06,
      "loss": 1.1358,
      "step": 9056
    },
    {
      "epoch": 3.506387921022067,
      "grad_norm": 47.547908782958984,
      "learning_rate": 7.215124532197704e-06,
      "loss": 2.5528,
      "step": 9057
    },
    {
      "epoch": 3.5067750677506773,
      "grad_norm": 36.28556823730469,
      "learning_rate": 7.214694369165914e-06,
      "loss": 1.1194,
      "step": 9058
    },
    {
      "epoch": 3.5071622144792878,
      "grad_norm": 21.994186401367188,
      "learning_rate": 7.214264206134125e-06,
      "loss": 0.8429,
      "step": 9059
    },
    {
      "epoch": 3.507549361207898,
      "grad_norm": 33.03361511230469,
      "learning_rate": 7.213834043102337e-06,
      "loss": 2.3034,
      "step": 9060
    },
    {
      "epoch": 3.507936507936508,
      "grad_norm": 62.099727630615234,
      "learning_rate": 7.213403880070548e-06,
      "loss": 2.6078,
      "step": 9061
    },
    {
      "epoch": 3.508323654665118,
      "grad_norm": 32.61631774902344,
      "learning_rate": 7.212973717038758e-06,
      "loss": 1.1809,
      "step": 9062
    },
    {
      "epoch": 3.508710801393728,
      "grad_norm": 17.437702178955078,
      "learning_rate": 7.212543554006969e-06,
      "loss": 0.4492,
      "step": 9063
    },
    {
      "epoch": 3.5090979481223386,
      "grad_norm": 70.65237426757812,
      "learning_rate": 7.212113390975181e-06,
      "loss": 1.5549,
      "step": 9064
    },
    {
      "epoch": 3.5094850948509486,
      "grad_norm": 24.161062240600586,
      "learning_rate": 7.211683227943392e-06,
      "loss": 0.5562,
      "step": 9065
    },
    {
      "epoch": 3.5098722415795587,
      "grad_norm": 30.379724502563477,
      "learning_rate": 7.211253064911602e-06,
      "loss": 1.2568,
      "step": 9066
    },
    {
      "epoch": 3.5102593883081687,
      "grad_norm": 37.157989501953125,
      "learning_rate": 7.210822901879813e-06,
      "loss": 1.0034,
      "step": 9067
    },
    {
      "epoch": 3.510646535036779,
      "grad_norm": 32.780311584472656,
      "learning_rate": 7.210392738848024e-06,
      "loss": 0.821,
      "step": 9068
    },
    {
      "epoch": 3.5110336817653893,
      "grad_norm": 31.198633193969727,
      "learning_rate": 7.209962575816235e-06,
      "loss": 1.1924,
      "step": 9069
    },
    {
      "epoch": 3.5114208284939994,
      "grad_norm": 41.083953857421875,
      "learning_rate": 7.209532412784446e-06,
      "loss": 0.8904,
      "step": 9070
    },
    {
      "epoch": 3.5118079752226095,
      "grad_norm": 19.415149688720703,
      "learning_rate": 7.209102249752657e-06,
      "loss": 1.4863,
      "step": 9071
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 17.222028732299805,
      "learning_rate": 7.208672086720868e-06,
      "loss": 1.1182,
      "step": 9072
    },
    {
      "epoch": 3.5125822686798296,
      "grad_norm": 39.555763244628906,
      "learning_rate": 7.208241923689079e-06,
      "loss": 2.5948,
      "step": 9073
    },
    {
      "epoch": 3.5129694154084397,
      "grad_norm": 66.74040985107422,
      "learning_rate": 7.207811760657289e-06,
      "loss": 2.0275,
      "step": 9074
    },
    {
      "epoch": 3.5133565621370497,
      "grad_norm": 17.92076301574707,
      "learning_rate": 7.2073815976255e-06,
      "loss": 1.0602,
      "step": 9075
    },
    {
      "epoch": 3.5137437088656602,
      "grad_norm": 24.841419219970703,
      "learning_rate": 7.206951434593712e-06,
      "loss": 1.2761,
      "step": 9076
    },
    {
      "epoch": 3.5141308555942703,
      "grad_norm": 23.956432342529297,
      "learning_rate": 7.206521271561923e-06,
      "loss": 1.1636,
      "step": 9077
    },
    {
      "epoch": 3.5145180023228804,
      "grad_norm": 13.942192077636719,
      "learning_rate": 7.206091108530133e-06,
      "loss": 1.2105,
      "step": 9078
    },
    {
      "epoch": 3.5149051490514904,
      "grad_norm": 30.080501556396484,
      "learning_rate": 7.205660945498344e-06,
      "loss": 0.6486,
      "step": 9079
    },
    {
      "epoch": 3.5152922957801005,
      "grad_norm": 20.496076583862305,
      "learning_rate": 7.205230782466556e-06,
      "loss": 1.4418,
      "step": 9080
    },
    {
      "epoch": 3.515679442508711,
      "grad_norm": 17.54263687133789,
      "learning_rate": 7.204800619434767e-06,
      "loss": 1.5563,
      "step": 9081
    },
    {
      "epoch": 3.516066589237321,
      "grad_norm": 12.930622100830078,
      "learning_rate": 7.204370456402977e-06,
      "loss": 0.6812,
      "step": 9082
    },
    {
      "epoch": 3.516453735965931,
      "grad_norm": 23.30498504638672,
      "learning_rate": 7.203940293371188e-06,
      "loss": 1.3783,
      "step": 9083
    },
    {
      "epoch": 3.516840882694541,
      "grad_norm": 25.96879768371582,
      "learning_rate": 7.203510130339399e-06,
      "loss": 1.49,
      "step": 9084
    },
    {
      "epoch": 3.5172280294231513,
      "grad_norm": 11.992779731750488,
      "learning_rate": 7.203079967307611e-06,
      "loss": 0.7619,
      "step": 9085
    },
    {
      "epoch": 3.517615176151762,
      "grad_norm": 15.570469856262207,
      "learning_rate": 7.202649804275821e-06,
      "loss": 1.0539,
      "step": 9086
    },
    {
      "epoch": 3.5180023228803714,
      "grad_norm": 22.44607162475586,
      "learning_rate": 7.2022196412440326e-06,
      "loss": 1.6041,
      "step": 9087
    },
    {
      "epoch": 3.518389469608982,
      "grad_norm": 55.37525939941406,
      "learning_rate": 7.201789478212243e-06,
      "loss": 2.1617,
      "step": 9088
    },
    {
      "epoch": 3.518776616337592,
      "grad_norm": 19.576744079589844,
      "learning_rate": 7.201359315180454e-06,
      "loss": 0.7151,
      "step": 9089
    },
    {
      "epoch": 3.519163763066202,
      "grad_norm": 16.282241821289062,
      "learning_rate": 7.200929152148664e-06,
      "loss": 0.9144,
      "step": 9090
    },
    {
      "epoch": 3.519550909794812,
      "grad_norm": 17.212474822998047,
      "learning_rate": 7.2004989891168765e-06,
      "loss": 0.8053,
      "step": 9091
    },
    {
      "epoch": 3.519938056523422,
      "grad_norm": 29.93344497680664,
      "learning_rate": 7.200068826085087e-06,
      "loss": 2.6962,
      "step": 9092
    },
    {
      "epoch": 3.5203252032520327,
      "grad_norm": 19.419658660888672,
      "learning_rate": 7.199638663053298e-06,
      "loss": 1.0027,
      "step": 9093
    },
    {
      "epoch": 3.5207123499806428,
      "grad_norm": 56.839561462402344,
      "learning_rate": 7.199208500021508e-06,
      "loss": 3.359,
      "step": 9094
    },
    {
      "epoch": 3.521099496709253,
      "grad_norm": 11.533596992492676,
      "learning_rate": 7.1987783369897205e-06,
      "loss": 0.7694,
      "step": 9095
    },
    {
      "epoch": 3.521486643437863,
      "grad_norm": 12.920646667480469,
      "learning_rate": 7.198348173957931e-06,
      "loss": 1.077,
      "step": 9096
    },
    {
      "epoch": 3.521873790166473,
      "grad_norm": 33.59873580932617,
      "learning_rate": 7.197918010926142e-06,
      "loss": 1.1531,
      "step": 9097
    },
    {
      "epoch": 3.5222609368950835,
      "grad_norm": 15.155253410339355,
      "learning_rate": 7.197487847894352e-06,
      "loss": 0.8984,
      "step": 9098
    },
    {
      "epoch": 3.5226480836236935,
      "grad_norm": 22.482938766479492,
      "learning_rate": 7.197057684862564e-06,
      "loss": 0.9858,
      "step": 9099
    },
    {
      "epoch": 3.5230352303523036,
      "grad_norm": 34.5816764831543,
      "learning_rate": 7.196627521830775e-06,
      "loss": 2.3206,
      "step": 9100
    },
    {
      "epoch": 3.5234223770809137,
      "grad_norm": 37.8127326965332,
      "learning_rate": 7.196197358798986e-06,
      "loss": 1.4281,
      "step": 9101
    },
    {
      "epoch": 3.5238095238095237,
      "grad_norm": 14.426301956176758,
      "learning_rate": 7.195767195767196e-06,
      "loss": 1.1095,
      "step": 9102
    },
    {
      "epoch": 3.524196670538134,
      "grad_norm": 9.972009658813477,
      "learning_rate": 7.195337032735408e-06,
      "loss": 0.639,
      "step": 9103
    },
    {
      "epoch": 3.524583817266744,
      "grad_norm": 28.7542781829834,
      "learning_rate": 7.194906869703618e-06,
      "loss": 1.3216,
      "step": 9104
    },
    {
      "epoch": 3.5249709639953544,
      "grad_norm": 22.813735961914062,
      "learning_rate": 7.194476706671829e-06,
      "loss": 1.8943,
      "step": 9105
    },
    {
      "epoch": 3.5253581107239644,
      "grad_norm": 24.137964248657227,
      "learning_rate": 7.19404654364004e-06,
      "loss": 1.9434,
      "step": 9106
    },
    {
      "epoch": 3.5257452574525745,
      "grad_norm": 13.905305862426758,
      "learning_rate": 7.1936163806082516e-06,
      "loss": 0.83,
      "step": 9107
    },
    {
      "epoch": 3.5261324041811846,
      "grad_norm": 10.474770545959473,
      "learning_rate": 7.193186217576462e-06,
      "loss": 0.7556,
      "step": 9108
    },
    {
      "epoch": 3.5265195509097946,
      "grad_norm": 21.357074737548828,
      "learning_rate": 7.192756054544673e-06,
      "loss": 1.4315,
      "step": 9109
    },
    {
      "epoch": 3.526906697638405,
      "grad_norm": 24.433151245117188,
      "learning_rate": 7.192325891512883e-06,
      "loss": 0.9151,
      "step": 9110
    },
    {
      "epoch": 3.527293844367015,
      "grad_norm": 9.176507949829102,
      "learning_rate": 7.1918957284810955e-06,
      "loss": 1.1947,
      "step": 9111
    },
    {
      "epoch": 3.5276809910956253,
      "grad_norm": 16.98780059814453,
      "learning_rate": 7.191465565449306e-06,
      "loss": 2.2639,
      "step": 9112
    },
    {
      "epoch": 3.5280681378242353,
      "grad_norm": 19.950807571411133,
      "learning_rate": 7.191035402417517e-06,
      "loss": 3.554,
      "step": 9113
    },
    {
      "epoch": 3.5284552845528454,
      "grad_norm": 48.657958984375,
      "learning_rate": 7.190605239385727e-06,
      "loss": 1.8389,
      "step": 9114
    },
    {
      "epoch": 3.528842431281456,
      "grad_norm": 20.573963165283203,
      "learning_rate": 7.1901750763539395e-06,
      "loss": 1.3451,
      "step": 9115
    },
    {
      "epoch": 3.5292295780100655,
      "grad_norm": 21.956209182739258,
      "learning_rate": 7.18974491332215e-06,
      "loss": 1.6083,
      "step": 9116
    },
    {
      "epoch": 3.529616724738676,
      "grad_norm": 7.182031154632568,
      "learning_rate": 7.189314750290361e-06,
      "loss": 0.4641,
      "step": 9117
    },
    {
      "epoch": 3.530003871467286,
      "grad_norm": 17.941755294799805,
      "learning_rate": 7.188884587258571e-06,
      "loss": 1.1561,
      "step": 9118
    },
    {
      "epoch": 3.530391018195896,
      "grad_norm": 36.52090072631836,
      "learning_rate": 7.188454424226783e-06,
      "loss": 0.9909,
      "step": 9119
    },
    {
      "epoch": 3.5307781649245062,
      "grad_norm": 13.982346534729004,
      "learning_rate": 7.188024261194993e-06,
      "loss": 0.8774,
      "step": 9120
    },
    {
      "epoch": 3.5311653116531163,
      "grad_norm": 32.00804901123047,
      "learning_rate": 7.187594098163205e-06,
      "loss": 1.8626,
      "step": 9121
    },
    {
      "epoch": 3.531552458381727,
      "grad_norm": 10.937893867492676,
      "learning_rate": 7.187163935131415e-06,
      "loss": 0.8212,
      "step": 9122
    },
    {
      "epoch": 3.531939605110337,
      "grad_norm": 23.111629486083984,
      "learning_rate": 7.186733772099627e-06,
      "loss": 1.2778,
      "step": 9123
    },
    {
      "epoch": 3.532326751838947,
      "grad_norm": 25.233123779296875,
      "learning_rate": 7.186303609067837e-06,
      "loss": 2.0136,
      "step": 9124
    },
    {
      "epoch": 3.532713898567557,
      "grad_norm": 19.268062591552734,
      "learning_rate": 7.185873446036048e-06,
      "loss": 0.5663,
      "step": 9125
    },
    {
      "epoch": 3.533101045296167,
      "grad_norm": 36.4506721496582,
      "learning_rate": 7.18544328300426e-06,
      "loss": 2.2775,
      "step": 9126
    },
    {
      "epoch": 3.5334881920247776,
      "grad_norm": 13.825254440307617,
      "learning_rate": 7.1850131199724706e-06,
      "loss": 1.4046,
      "step": 9127
    },
    {
      "epoch": 3.5338753387533877,
      "grad_norm": 30.59239959716797,
      "learning_rate": 7.184582956940681e-06,
      "loss": 3.1236,
      "step": 9128
    },
    {
      "epoch": 3.5342624854819977,
      "grad_norm": 5.011896133422852,
      "learning_rate": 7.184152793908892e-06,
      "loss": 0.2183,
      "step": 9129
    },
    {
      "epoch": 3.534649632210608,
      "grad_norm": 3.6441667079925537,
      "learning_rate": 7.183722630877103e-06,
      "loss": 0.1115,
      "step": 9130
    },
    {
      "epoch": 3.535036778939218,
      "grad_norm": 17.08465003967285,
      "learning_rate": 7.1832924678453145e-06,
      "loss": 1.0449,
      "step": 9131
    },
    {
      "epoch": 3.5354239256678284,
      "grad_norm": 9.43709945678711,
      "learning_rate": 7.182862304813525e-06,
      "loss": 0.5592,
      "step": 9132
    },
    {
      "epoch": 3.535811072396438,
      "grad_norm": 39.1024055480957,
      "learning_rate": 7.182432141781736e-06,
      "loss": 1.6787,
      "step": 9133
    },
    {
      "epoch": 3.5361982191250485,
      "grad_norm": 12.788370132446289,
      "learning_rate": 7.182001978749947e-06,
      "loss": 0.9983,
      "step": 9134
    },
    {
      "epoch": 3.5365853658536586,
      "grad_norm": 19.664016723632812,
      "learning_rate": 7.181571815718158e-06,
      "loss": 1.4399,
      "step": 9135
    },
    {
      "epoch": 3.5369725125822686,
      "grad_norm": 14.529646873474121,
      "learning_rate": 7.181141652686369e-06,
      "loss": 1.0797,
      "step": 9136
    },
    {
      "epoch": 3.5373596593108787,
      "grad_norm": 17.542818069458008,
      "learning_rate": 7.18071148965458e-06,
      "loss": 0.5829,
      "step": 9137
    },
    {
      "epoch": 3.5377468060394888,
      "grad_norm": 21.35061264038086,
      "learning_rate": 7.180281326622791e-06,
      "loss": 0.9636,
      "step": 9138
    },
    {
      "epoch": 3.5381339527680993,
      "grad_norm": 9.052470207214355,
      "learning_rate": 7.179851163591002e-06,
      "loss": 0.8532,
      "step": 9139
    },
    {
      "epoch": 3.5385210994967093,
      "grad_norm": 56.367069244384766,
      "learning_rate": 7.179421000559212e-06,
      "loss": 1.6207,
      "step": 9140
    },
    {
      "epoch": 3.5389082462253194,
      "grad_norm": 32.376102447509766,
      "learning_rate": 7.178990837527423e-06,
      "loss": 1.5632,
      "step": 9141
    },
    {
      "epoch": 3.5392953929539295,
      "grad_norm": 9.776467323303223,
      "learning_rate": 7.178560674495635e-06,
      "loss": 0.6787,
      "step": 9142
    },
    {
      "epoch": 3.5396825396825395,
      "grad_norm": 9.690337181091309,
      "learning_rate": 7.178130511463846e-06,
      "loss": 0.7175,
      "step": 9143
    },
    {
      "epoch": 3.54006968641115,
      "grad_norm": 31.43974494934082,
      "learning_rate": 7.177700348432056e-06,
      "loss": 1.5267,
      "step": 9144
    },
    {
      "epoch": 3.54045683313976,
      "grad_norm": 38.892677307128906,
      "learning_rate": 7.177270185400267e-06,
      "loss": 1.1437,
      "step": 9145
    },
    {
      "epoch": 3.54084397986837,
      "grad_norm": 11.48285961151123,
      "learning_rate": 7.176840022368479e-06,
      "loss": 0.6509,
      "step": 9146
    },
    {
      "epoch": 3.5412311265969802,
      "grad_norm": 23.09616470336914,
      "learning_rate": 7.1764098593366896e-06,
      "loss": 1.345,
      "step": 9147
    },
    {
      "epoch": 3.5416182733255903,
      "grad_norm": 15.094161987304688,
      "learning_rate": 7.1759796963049e-06,
      "loss": 0.66,
      "step": 9148
    },
    {
      "epoch": 3.5420054200542004,
      "grad_norm": 26.76664924621582,
      "learning_rate": 7.175549533273111e-06,
      "loss": 0.9192,
      "step": 9149
    },
    {
      "epoch": 3.5423925667828104,
      "grad_norm": 11.08694839477539,
      "learning_rate": 7.175119370241322e-06,
      "loss": 0.3329,
      "step": 9150
    },
    {
      "epoch": 3.542779713511421,
      "grad_norm": 29.10228729248047,
      "learning_rate": 7.1746892072095335e-06,
      "loss": 1.1763,
      "step": 9151
    },
    {
      "epoch": 3.543166860240031,
      "grad_norm": 11.195330619812012,
      "learning_rate": 7.174259044177744e-06,
      "loss": 1.1835,
      "step": 9152
    },
    {
      "epoch": 3.543554006968641,
      "grad_norm": 29.76239585876465,
      "learning_rate": 7.173828881145955e-06,
      "loss": 1.2622,
      "step": 9153
    },
    {
      "epoch": 3.543941153697251,
      "grad_norm": 37.90950012207031,
      "learning_rate": 7.173398718114166e-06,
      "loss": 0.5764,
      "step": 9154
    },
    {
      "epoch": 3.5443283004258612,
      "grad_norm": 25.93923568725586,
      "learning_rate": 7.172968555082377e-06,
      "loss": 1.6872,
      "step": 9155
    },
    {
      "epoch": 3.5447154471544717,
      "grad_norm": 12.507665634155273,
      "learning_rate": 7.172538392050587e-06,
      "loss": 1.4006,
      "step": 9156
    },
    {
      "epoch": 3.545102593883082,
      "grad_norm": 51.715572357177734,
      "learning_rate": 7.172108229018799e-06,
      "loss": 2.2812,
      "step": 9157
    },
    {
      "epoch": 3.545489740611692,
      "grad_norm": 11.045072555541992,
      "learning_rate": 7.17167806598701e-06,
      "loss": 1.2877,
      "step": 9158
    },
    {
      "epoch": 3.545876887340302,
      "grad_norm": 27.918004989624023,
      "learning_rate": 7.171247902955221e-06,
      "loss": 1.0145,
      "step": 9159
    },
    {
      "epoch": 3.546264034068912,
      "grad_norm": 44.910133361816406,
      "learning_rate": 7.170817739923431e-06,
      "loss": 0.9864,
      "step": 9160
    },
    {
      "epoch": 3.5466511807975225,
      "grad_norm": 78.98528289794922,
      "learning_rate": 7.170387576891642e-06,
      "loss": 2.4481,
      "step": 9161
    },
    {
      "epoch": 3.547038327526132,
      "grad_norm": 11.800017356872559,
      "learning_rate": 7.169957413859854e-06,
      "loss": 1.0703,
      "step": 9162
    },
    {
      "epoch": 3.5474254742547426,
      "grad_norm": 22.553287506103516,
      "learning_rate": 7.169527250828065e-06,
      "loss": 1.3803,
      "step": 9163
    },
    {
      "epoch": 3.5478126209833527,
      "grad_norm": 16.77250099182129,
      "learning_rate": 7.169097087796275e-06,
      "loss": 1.4695,
      "step": 9164
    },
    {
      "epoch": 3.5481997677119628,
      "grad_norm": 48.109954833984375,
      "learning_rate": 7.168666924764486e-06,
      "loss": 0.878,
      "step": 9165
    },
    {
      "epoch": 3.548586914440573,
      "grad_norm": 24.467803955078125,
      "learning_rate": 7.168236761732697e-06,
      "loss": 1.0763,
      "step": 9166
    },
    {
      "epoch": 3.548974061169183,
      "grad_norm": 65.54328155517578,
      "learning_rate": 7.1678065987009086e-06,
      "loss": 2.5869,
      "step": 9167
    },
    {
      "epoch": 3.5493612078977934,
      "grad_norm": 28.13791275024414,
      "learning_rate": 7.167376435669119e-06,
      "loss": 0.9627,
      "step": 9168
    },
    {
      "epoch": 3.5497483546264035,
      "grad_norm": 12.87497329711914,
      "learning_rate": 7.1669462726373305e-06,
      "loss": 0.5895,
      "step": 9169
    },
    {
      "epoch": 3.5501355013550135,
      "grad_norm": 31.97011375427246,
      "learning_rate": 7.166516109605541e-06,
      "loss": 2.1207,
      "step": 9170
    },
    {
      "epoch": 3.5505226480836236,
      "grad_norm": 32.499473571777344,
      "learning_rate": 7.166085946573752e-06,
      "loss": 0.6879,
      "step": 9171
    },
    {
      "epoch": 3.5509097948122337,
      "grad_norm": 23.22378158569336,
      "learning_rate": 7.165655783541963e-06,
      "loss": 0.9735,
      "step": 9172
    },
    {
      "epoch": 3.551296941540844,
      "grad_norm": 8.299759864807129,
      "learning_rate": 7.1652256205101745e-06,
      "loss": 0.6453,
      "step": 9173
    },
    {
      "epoch": 3.5516840882694543,
      "grad_norm": 10.043601036071777,
      "learning_rate": 7.164795457478385e-06,
      "loss": 0.2692,
      "step": 9174
    },
    {
      "epoch": 3.5520712349980643,
      "grad_norm": 13.458476066589355,
      "learning_rate": 7.164365294446596e-06,
      "loss": 1.866,
      "step": 9175
    },
    {
      "epoch": 3.5524583817266744,
      "grad_norm": 26.724634170532227,
      "learning_rate": 7.163935131414806e-06,
      "loss": 1.6465,
      "step": 9176
    },
    {
      "epoch": 3.5528455284552845,
      "grad_norm": 39.60627365112305,
      "learning_rate": 7.1635049683830185e-06,
      "loss": 2.9679,
      "step": 9177
    },
    {
      "epoch": 3.553232675183895,
      "grad_norm": 78.85836791992188,
      "learning_rate": 7.163074805351229e-06,
      "loss": 2.1491,
      "step": 9178
    },
    {
      "epoch": 3.5536198219125046,
      "grad_norm": 31.165319442749023,
      "learning_rate": 7.16264464231944e-06,
      "loss": 1.4212,
      "step": 9179
    },
    {
      "epoch": 3.554006968641115,
      "grad_norm": 24.8916015625,
      "learning_rate": 7.16221447928765e-06,
      "loss": 1.2091,
      "step": 9180
    },
    {
      "epoch": 3.554394115369725,
      "grad_norm": 33.521820068359375,
      "learning_rate": 7.161784316255862e-06,
      "loss": 1.8812,
      "step": 9181
    },
    {
      "epoch": 3.5547812620983352,
      "grad_norm": 25.780014038085938,
      "learning_rate": 7.161354153224073e-06,
      "loss": 0.6201,
      "step": 9182
    },
    {
      "epoch": 3.5551684088269453,
      "grad_norm": 50.551971435546875,
      "learning_rate": 7.160923990192284e-06,
      "loss": 1.1537,
      "step": 9183
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 20.84461212158203,
      "learning_rate": 7.160493827160494e-06,
      "loss": 2.0131,
      "step": 9184
    },
    {
      "epoch": 3.555942702284166,
      "grad_norm": 31.44686508178711,
      "learning_rate": 7.1600636641287056e-06,
      "loss": 2.0753,
      "step": 9185
    },
    {
      "epoch": 3.556329849012776,
      "grad_norm": 31.697185516357422,
      "learning_rate": 7.159633501096916e-06,
      "loss": 1.9837,
      "step": 9186
    },
    {
      "epoch": 3.556716995741386,
      "grad_norm": 36.896446228027344,
      "learning_rate": 7.1592033380651276e-06,
      "loss": 1.4989,
      "step": 9187
    },
    {
      "epoch": 3.557104142469996,
      "grad_norm": 21.891502380371094,
      "learning_rate": 7.158773175033338e-06,
      "loss": 1.4752,
      "step": 9188
    },
    {
      "epoch": 3.557491289198606,
      "grad_norm": 43.224090576171875,
      "learning_rate": 7.1583430120015495e-06,
      "loss": 1.3411,
      "step": 9189
    },
    {
      "epoch": 3.5578784359272166,
      "grad_norm": 11.778611183166504,
      "learning_rate": 7.15791284896976e-06,
      "loss": 0.6102,
      "step": 9190
    },
    {
      "epoch": 3.5582655826558267,
      "grad_norm": 12.602710723876953,
      "learning_rate": 7.157482685937971e-06,
      "loss": 0.7957,
      "step": 9191
    },
    {
      "epoch": 3.5586527293844368,
      "grad_norm": 19.111156463623047,
      "learning_rate": 7.157052522906181e-06,
      "loss": 0.4207,
      "step": 9192
    },
    {
      "epoch": 3.559039876113047,
      "grad_norm": 28.8452205657959,
      "learning_rate": 7.1566223598743935e-06,
      "loss": 1.2999,
      "step": 9193
    },
    {
      "epoch": 3.559427022841657,
      "grad_norm": 49.691650390625,
      "learning_rate": 7.156192196842604e-06,
      "loss": 1.2156,
      "step": 9194
    },
    {
      "epoch": 3.559814169570267,
      "grad_norm": 31.077945709228516,
      "learning_rate": 7.155762033810815e-06,
      "loss": 2.8305,
      "step": 9195
    },
    {
      "epoch": 3.560201316298877,
      "grad_norm": 64.7682113647461,
      "learning_rate": 7.155331870779025e-06,
      "loss": 2.593,
      "step": 9196
    },
    {
      "epoch": 3.5605884630274875,
      "grad_norm": 15.82957649230957,
      "learning_rate": 7.1549017077472375e-06,
      "loss": 0.6531,
      "step": 9197
    },
    {
      "epoch": 3.5609756097560976,
      "grad_norm": 34.41080093383789,
      "learning_rate": 7.154471544715448e-06,
      "loss": 1.379,
      "step": 9198
    },
    {
      "epoch": 3.5613627564847077,
      "grad_norm": 24.539337158203125,
      "learning_rate": 7.154041381683659e-06,
      "loss": 1.1528,
      "step": 9199
    },
    {
      "epoch": 3.5617499032133177,
      "grad_norm": 39.28875732421875,
      "learning_rate": 7.153611218651869e-06,
      "loss": 1.205,
      "step": 9200
    },
    {
      "epoch": 3.562137049941928,
      "grad_norm": 24.090965270996094,
      "learning_rate": 7.153181055620081e-06,
      "loss": 0.7114,
      "step": 9201
    },
    {
      "epoch": 3.5625241966705383,
      "grad_norm": 18.01904296875,
      "learning_rate": 7.152750892588291e-06,
      "loss": 1.4571,
      "step": 9202
    },
    {
      "epoch": 3.5629113433991484,
      "grad_norm": 18.54528045654297,
      "learning_rate": 7.152320729556503e-06,
      "loss": 0.8334,
      "step": 9203
    },
    {
      "epoch": 3.5632984901277585,
      "grad_norm": 11.932374000549316,
      "learning_rate": 7.151890566524713e-06,
      "loss": 1.592,
      "step": 9204
    },
    {
      "epoch": 3.5636856368563685,
      "grad_norm": 12.97823429107666,
      "learning_rate": 7.1514604034929246e-06,
      "loss": 1.1476,
      "step": 9205
    },
    {
      "epoch": 3.5640727835849786,
      "grad_norm": 42.02635955810547,
      "learning_rate": 7.151030240461135e-06,
      "loss": 1.0946,
      "step": 9206
    },
    {
      "epoch": 3.564459930313589,
      "grad_norm": 25.124771118164062,
      "learning_rate": 7.150600077429346e-06,
      "loss": 1.0401,
      "step": 9207
    },
    {
      "epoch": 3.5648470770421987,
      "grad_norm": 38.83120346069336,
      "learning_rate": 7.150169914397558e-06,
      "loss": 2.0643,
      "step": 9208
    },
    {
      "epoch": 3.5652342237708092,
      "grad_norm": 48.68737030029297,
      "learning_rate": 7.1497397513657685e-06,
      "loss": 1.905,
      "step": 9209
    },
    {
      "epoch": 3.5656213704994193,
      "grad_norm": 37.63062286376953,
      "learning_rate": 7.149309588333979e-06,
      "loss": 1.6258,
      "step": 9210
    },
    {
      "epoch": 3.5660085172280294,
      "grad_norm": 27.691255569458008,
      "learning_rate": 7.14887942530219e-06,
      "loss": 1.3862,
      "step": 9211
    },
    {
      "epoch": 3.5663956639566394,
      "grad_norm": 19.781736373901367,
      "learning_rate": 7.148449262270402e-06,
      "loss": 0.5812,
      "step": 9212
    },
    {
      "epoch": 3.5667828106852495,
      "grad_norm": 21.132511138916016,
      "learning_rate": 7.1480190992386125e-06,
      "loss": 0.6104,
      "step": 9213
    },
    {
      "epoch": 3.56716995741386,
      "grad_norm": 17.437145233154297,
      "learning_rate": 7.147588936206823e-06,
      "loss": 0.5671,
      "step": 9214
    },
    {
      "epoch": 3.56755710414247,
      "grad_norm": 45.69169616699219,
      "learning_rate": 7.147158773175034e-06,
      "loss": 0.8481,
      "step": 9215
    },
    {
      "epoch": 3.56794425087108,
      "grad_norm": 22.756393432617188,
      "learning_rate": 7.146728610143245e-06,
      "loss": 2.0326,
      "step": 9216
    },
    {
      "epoch": 3.56833139759969,
      "grad_norm": 27.017810821533203,
      "learning_rate": 7.146298447111456e-06,
      "loss": 0.8591,
      "step": 9217
    },
    {
      "epoch": 3.5687185443283003,
      "grad_norm": 80.52510070800781,
      "learning_rate": 7.145868284079667e-06,
      "loss": 1.0532,
      "step": 9218
    },
    {
      "epoch": 3.569105691056911,
      "grad_norm": 6.588240146636963,
      "learning_rate": 7.145438121047878e-06,
      "loss": 0.3201,
      "step": 9219
    },
    {
      "epoch": 3.569492837785521,
      "grad_norm": 39.93559265136719,
      "learning_rate": 7.145007958016089e-06,
      "loss": 1.7754,
      "step": 9220
    },
    {
      "epoch": 3.569879984514131,
      "grad_norm": 21.85750389099121,
      "learning_rate": 7.1445777949843e-06,
      "loss": 0.8506,
      "step": 9221
    },
    {
      "epoch": 3.570267131242741,
      "grad_norm": 19.647985458374023,
      "learning_rate": 7.14414763195251e-06,
      "loss": 1.4718,
      "step": 9222
    },
    {
      "epoch": 3.570654277971351,
      "grad_norm": 13.796710014343262,
      "learning_rate": 7.143717468920722e-06,
      "loss": 1.0437,
      "step": 9223
    },
    {
      "epoch": 3.5710414246999616,
      "grad_norm": 24.014488220214844,
      "learning_rate": 7.143287305888933e-06,
      "loss": 1.3482,
      "step": 9224
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 35.8082389831543,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 1.161,
      "step": 9225
    },
    {
      "epoch": 3.5718157181571817,
      "grad_norm": 7.766640663146973,
      "learning_rate": 7.142426979825354e-06,
      "loss": 0.4767,
      "step": 9226
    },
    {
      "epoch": 3.5722028648857918,
      "grad_norm": 14.947352409362793,
      "learning_rate": 7.141996816793565e-06,
      "loss": 0.8176,
      "step": 9227
    },
    {
      "epoch": 3.572590011614402,
      "grad_norm": 8.734914779663086,
      "learning_rate": 7.141566653761777e-06,
      "loss": 0.3536,
      "step": 9228
    },
    {
      "epoch": 3.572977158343012,
      "grad_norm": 12.511249542236328,
      "learning_rate": 7.1411364907299875e-06,
      "loss": 0.4365,
      "step": 9229
    },
    {
      "epoch": 3.573364305071622,
      "grad_norm": 16.587642669677734,
      "learning_rate": 7.140706327698198e-06,
      "loss": 0.5792,
      "step": 9230
    },
    {
      "epoch": 3.5737514518002325,
      "grad_norm": 36.99391174316406,
      "learning_rate": 7.140276164666409e-06,
      "loss": 1.1513,
      "step": 9231
    },
    {
      "epoch": 3.5741385985288425,
      "grad_norm": 28.23887825012207,
      "learning_rate": 7.13984600163462e-06,
      "loss": 1.5532,
      "step": 9232
    },
    {
      "epoch": 3.5745257452574526,
      "grad_norm": 23.61258316040039,
      "learning_rate": 7.1394158386028315e-06,
      "loss": 1.0064,
      "step": 9233
    },
    {
      "epoch": 3.5749128919860627,
      "grad_norm": 8.918707847595215,
      "learning_rate": 7.138985675571042e-06,
      "loss": 1.1916,
      "step": 9234
    },
    {
      "epoch": 3.5753000387146727,
      "grad_norm": 20.77668571472168,
      "learning_rate": 7.138555512539253e-06,
      "loss": 1.6697,
      "step": 9235
    },
    {
      "epoch": 3.5756871854432832,
      "grad_norm": 44.36560821533203,
      "learning_rate": 7.138125349507464e-06,
      "loss": 2.0893,
      "step": 9236
    },
    {
      "epoch": 3.5760743321718933,
      "grad_norm": 11.068135261535645,
      "learning_rate": 7.137695186475675e-06,
      "loss": 0.5436,
      "step": 9237
    },
    {
      "epoch": 3.5764614789005034,
      "grad_norm": 14.067500114440918,
      "learning_rate": 7.137265023443885e-06,
      "loss": 0.8229,
      "step": 9238
    },
    {
      "epoch": 3.5768486256291134,
      "grad_norm": 28.289318084716797,
      "learning_rate": 7.136834860412097e-06,
      "loss": 1.9318,
      "step": 9239
    },
    {
      "epoch": 3.5772357723577235,
      "grad_norm": 16.48219108581543,
      "learning_rate": 7.136404697380308e-06,
      "loss": 1.0524,
      "step": 9240
    },
    {
      "epoch": 3.5776229190863336,
      "grad_norm": 30.061994552612305,
      "learning_rate": 7.135974534348519e-06,
      "loss": 2.5751,
      "step": 9241
    },
    {
      "epoch": 3.5780100658149436,
      "grad_norm": 61.57959747314453,
      "learning_rate": 7.135544371316729e-06,
      "loss": 1.7269,
      "step": 9242
    },
    {
      "epoch": 3.578397212543554,
      "grad_norm": 14.865030288696289,
      "learning_rate": 7.13511420828494e-06,
      "loss": 0.896,
      "step": 9243
    },
    {
      "epoch": 3.578784359272164,
      "grad_norm": 32.24486541748047,
      "learning_rate": 7.134684045253152e-06,
      "loss": 2.1489,
      "step": 9244
    },
    {
      "epoch": 3.5791715060007743,
      "grad_norm": 16.624608993530273,
      "learning_rate": 7.1342538822213626e-06,
      "loss": 0.8998,
      "step": 9245
    },
    {
      "epoch": 3.5795586527293843,
      "grad_norm": 29.762920379638672,
      "learning_rate": 7.133823719189573e-06,
      "loss": 1.4367,
      "step": 9246
    },
    {
      "epoch": 3.5799457994579944,
      "grad_norm": 7.2837653160095215,
      "learning_rate": 7.133393556157784e-06,
      "loss": 0.4524,
      "step": 9247
    },
    {
      "epoch": 3.580332946186605,
      "grad_norm": 17.615005493164062,
      "learning_rate": 7.132963393125996e-06,
      "loss": 0.9431,
      "step": 9248
    },
    {
      "epoch": 3.580720092915215,
      "grad_norm": 15.736544609069824,
      "learning_rate": 7.1325332300942065e-06,
      "loss": 0.646,
      "step": 9249
    },
    {
      "epoch": 3.581107239643825,
      "grad_norm": 16.196924209594727,
      "learning_rate": 7.132103067062417e-06,
      "loss": 1.1429,
      "step": 9250
    },
    {
      "epoch": 3.581494386372435,
      "grad_norm": 112.98323822021484,
      "learning_rate": 7.1316729040306285e-06,
      "loss": 2.7662,
      "step": 9251
    },
    {
      "epoch": 3.581881533101045,
      "grad_norm": 38.019081115722656,
      "learning_rate": 7.131242740998839e-06,
      "loss": 0.8337,
      "step": 9252
    },
    {
      "epoch": 3.5822686798296557,
      "grad_norm": 26.756542205810547,
      "learning_rate": 7.13081257796705e-06,
      "loss": 1.7294,
      "step": 9253
    },
    {
      "epoch": 3.5826558265582653,
      "grad_norm": 25.011341094970703,
      "learning_rate": 7.130382414935261e-06,
      "loss": 1.3204,
      "step": 9254
    },
    {
      "epoch": 3.583042973286876,
      "grad_norm": 25.669862747192383,
      "learning_rate": 7.1299522519034725e-06,
      "loss": 1.3156,
      "step": 9255
    },
    {
      "epoch": 3.583430120015486,
      "grad_norm": 42.136474609375,
      "learning_rate": 7.129522088871683e-06,
      "loss": 1.7589,
      "step": 9256
    },
    {
      "epoch": 3.583817266744096,
      "grad_norm": 35.43233871459961,
      "learning_rate": 7.129091925839894e-06,
      "loss": 1.3712,
      "step": 9257
    },
    {
      "epoch": 3.584204413472706,
      "grad_norm": 53.9149055480957,
      "learning_rate": 7.128661762808104e-06,
      "loss": 1.209,
      "step": 9258
    },
    {
      "epoch": 3.584591560201316,
      "grad_norm": 71.45635223388672,
      "learning_rate": 7.1282315997763165e-06,
      "loss": 3.6641,
      "step": 9259
    },
    {
      "epoch": 3.5849787069299266,
      "grad_norm": 46.00813674926758,
      "learning_rate": 7.127801436744527e-06,
      "loss": 1.3529,
      "step": 9260
    },
    {
      "epoch": 3.5853658536585367,
      "grad_norm": 32.808372497558594,
      "learning_rate": 7.127371273712738e-06,
      "loss": 0.9092,
      "step": 9261
    },
    {
      "epoch": 3.5857530003871467,
      "grad_norm": 28.24630355834961,
      "learning_rate": 7.126941110680948e-06,
      "loss": 1.5356,
      "step": 9262
    },
    {
      "epoch": 3.586140147115757,
      "grad_norm": 24.743785858154297,
      "learning_rate": 7.12651094764916e-06,
      "loss": 1.2248,
      "step": 9263
    },
    {
      "epoch": 3.586527293844367,
      "grad_norm": 18.148738861083984,
      "learning_rate": 7.126080784617371e-06,
      "loss": 0.8162,
      "step": 9264
    },
    {
      "epoch": 3.5869144405729774,
      "grad_norm": 24.21944236755371,
      "learning_rate": 7.1256506215855816e-06,
      "loss": 1.3058,
      "step": 9265
    },
    {
      "epoch": 3.5873015873015874,
      "grad_norm": 33.70814895629883,
      "learning_rate": 7.125220458553792e-06,
      "loss": 1.3481,
      "step": 9266
    },
    {
      "epoch": 3.5876887340301975,
      "grad_norm": 19.677894592285156,
      "learning_rate": 7.1247902955220035e-06,
      "loss": 0.8369,
      "step": 9267
    },
    {
      "epoch": 3.5880758807588076,
      "grad_norm": 27.3638858795166,
      "learning_rate": 7.124360132490214e-06,
      "loss": 1.3341,
      "step": 9268
    },
    {
      "epoch": 3.5884630274874176,
      "grad_norm": 12.18527603149414,
      "learning_rate": 7.1239299694584255e-06,
      "loss": 0.3272,
      "step": 9269
    },
    {
      "epoch": 3.588850174216028,
      "grad_norm": 22.799489974975586,
      "learning_rate": 7.123499806426636e-06,
      "loss": 0.6776,
      "step": 9270
    },
    {
      "epoch": 3.5892373209446378,
      "grad_norm": 35.05756759643555,
      "learning_rate": 7.1230696433948475e-06,
      "loss": 0.9315,
      "step": 9271
    },
    {
      "epoch": 3.5896244676732483,
      "grad_norm": 31.664979934692383,
      "learning_rate": 7.122639480363058e-06,
      "loss": 0.6984,
      "step": 9272
    },
    {
      "epoch": 3.5900116144018583,
      "grad_norm": 27.25172996520996,
      "learning_rate": 7.122209317331269e-06,
      "loss": 0.8879,
      "step": 9273
    },
    {
      "epoch": 3.5903987611304684,
      "grad_norm": 39.25774383544922,
      "learning_rate": 7.121779154299479e-06,
      "loss": 0.5796,
      "step": 9274
    },
    {
      "epoch": 3.5907859078590785,
      "grad_norm": 59.47630310058594,
      "learning_rate": 7.1213489912676915e-06,
      "loss": 1.1233,
      "step": 9275
    },
    {
      "epoch": 3.5911730545876885,
      "grad_norm": 19.299800872802734,
      "learning_rate": 7.120918828235902e-06,
      "loss": 1.5694,
      "step": 9276
    },
    {
      "epoch": 3.591560201316299,
      "grad_norm": 24.258073806762695,
      "learning_rate": 7.120488665204113e-06,
      "loss": 1.4075,
      "step": 9277
    },
    {
      "epoch": 3.591947348044909,
      "grad_norm": 30.06468391418457,
      "learning_rate": 7.120058502172323e-06,
      "loss": 3.1785,
      "step": 9278
    },
    {
      "epoch": 3.592334494773519,
      "grad_norm": 26.462512969970703,
      "learning_rate": 7.1196283391405354e-06,
      "loss": 0.9,
      "step": 9279
    },
    {
      "epoch": 3.5927216415021292,
      "grad_norm": 30.382373809814453,
      "learning_rate": 7.119198176108746e-06,
      "loss": 0.7369,
      "step": 9280
    },
    {
      "epoch": 3.5931087882307393,
      "grad_norm": 28.343318939208984,
      "learning_rate": 7.118768013076957e-06,
      "loss": 1.8863,
      "step": 9281
    },
    {
      "epoch": 3.59349593495935,
      "grad_norm": 38.09796142578125,
      "learning_rate": 7.118337850045167e-06,
      "loss": 1.8372,
      "step": 9282
    },
    {
      "epoch": 3.59388308168796,
      "grad_norm": 49.51882553100586,
      "learning_rate": 7.1179076870133786e-06,
      "loss": 1.139,
      "step": 9283
    },
    {
      "epoch": 3.59427022841657,
      "grad_norm": 31.276565551757812,
      "learning_rate": 7.11747752398159e-06,
      "loss": 2.6294,
      "step": 9284
    },
    {
      "epoch": 3.59465737514518,
      "grad_norm": 31.89813232421875,
      "learning_rate": 7.1170473609498006e-06,
      "loss": 2.121,
      "step": 9285
    },
    {
      "epoch": 3.59504452187379,
      "grad_norm": 23.167375564575195,
      "learning_rate": 7.116617197918011e-06,
      "loss": 0.9723,
      "step": 9286
    },
    {
      "epoch": 3.5954316686024,
      "grad_norm": 16.8961238861084,
      "learning_rate": 7.1161870348862225e-06,
      "loss": 0.6236,
      "step": 9287
    },
    {
      "epoch": 3.59581881533101,
      "grad_norm": 40.378501892089844,
      "learning_rate": 7.115756871854433e-06,
      "loss": 1.1713,
      "step": 9288
    },
    {
      "epoch": 3.5962059620596207,
      "grad_norm": 20.516448974609375,
      "learning_rate": 7.115326708822644e-06,
      "loss": 0.5312,
      "step": 9289
    },
    {
      "epoch": 3.596593108788231,
      "grad_norm": 9.770082473754883,
      "learning_rate": 7.114896545790856e-06,
      "loss": 0.4443,
      "step": 9290
    },
    {
      "epoch": 3.596980255516841,
      "grad_norm": 31.17344093322754,
      "learning_rate": 7.1144663827590665e-06,
      "loss": 0.7295,
      "step": 9291
    },
    {
      "epoch": 3.597367402245451,
      "grad_norm": 77.82433319091797,
      "learning_rate": 7.114036219727277e-06,
      "loss": 1.4394,
      "step": 9292
    },
    {
      "epoch": 3.597754548974061,
      "grad_norm": 27.77997589111328,
      "learning_rate": 7.113606056695488e-06,
      "loss": 1.0439,
      "step": 9293
    },
    {
      "epoch": 3.5981416957026715,
      "grad_norm": 25.890295028686523,
      "learning_rate": 7.1131758936637e-06,
      "loss": 2.9933,
      "step": 9294
    },
    {
      "epoch": 3.5985288424312816,
      "grad_norm": 33.73106002807617,
      "learning_rate": 7.1127457306319105e-06,
      "loss": 0.9838,
      "step": 9295
    },
    {
      "epoch": 3.5989159891598916,
      "grad_norm": 15.69871997833252,
      "learning_rate": 7.112315567600121e-06,
      "loss": 1.2295,
      "step": 9296
    },
    {
      "epoch": 3.5993031358885017,
      "grad_norm": 36.1009635925293,
      "learning_rate": 7.111885404568332e-06,
      "loss": 2.3314,
      "step": 9297
    },
    {
      "epoch": 3.5996902826171118,
      "grad_norm": 34.92026901245117,
      "learning_rate": 7.111455241536543e-06,
      "loss": 0.8089,
      "step": 9298
    },
    {
      "epoch": 3.6000774293457223,
      "grad_norm": 32.48582458496094,
      "learning_rate": 7.1110250785047544e-06,
      "loss": 2.6211,
      "step": 9299
    },
    {
      "epoch": 3.600464576074332,
      "grad_norm": 35.210968017578125,
      "learning_rate": 7.110594915472965e-06,
      "loss": 1.8965,
      "step": 9300
    },
    {
      "epoch": 3.6008517228029424,
      "grad_norm": 58.3708381652832,
      "learning_rate": 7.110164752441176e-06,
      "loss": 1.0199,
      "step": 9301
    },
    {
      "epoch": 3.6012388695315525,
      "grad_norm": 28.221250534057617,
      "learning_rate": 7.109734589409387e-06,
      "loss": 1.5813,
      "step": 9302
    },
    {
      "epoch": 3.6016260162601625,
      "grad_norm": 20.811864852905273,
      "learning_rate": 7.1093044263775976e-06,
      "loss": 1.2623,
      "step": 9303
    },
    {
      "epoch": 3.6020131629887726,
      "grad_norm": 51.26493835449219,
      "learning_rate": 7.108874263345808e-06,
      "loss": 0.8706,
      "step": 9304
    },
    {
      "epoch": 3.6024003097173827,
      "grad_norm": 19.25501823425293,
      "learning_rate": 7.1084441003140196e-06,
      "loss": 1.5756,
      "step": 9305
    },
    {
      "epoch": 3.602787456445993,
      "grad_norm": 12.842351913452148,
      "learning_rate": 7.108013937282231e-06,
      "loss": 1.3476,
      "step": 9306
    },
    {
      "epoch": 3.6031746031746033,
      "grad_norm": 10.924917221069336,
      "learning_rate": 7.1075837742504415e-06,
      "loss": 0.66,
      "step": 9307
    },
    {
      "epoch": 3.6035617499032133,
      "grad_norm": 32.603694915771484,
      "learning_rate": 7.107153611218652e-06,
      "loss": 1.5983,
      "step": 9308
    },
    {
      "epoch": 3.6039488966318234,
      "grad_norm": 23.027400970458984,
      "learning_rate": 7.106723448186863e-06,
      "loss": 1.0536,
      "step": 9309
    },
    {
      "epoch": 3.6043360433604335,
      "grad_norm": 12.303709983825684,
      "learning_rate": 7.106293285155075e-06,
      "loss": 0.9566,
      "step": 9310
    },
    {
      "epoch": 3.604723190089044,
      "grad_norm": 15.256072044372559,
      "learning_rate": 7.1058631221232855e-06,
      "loss": 1.0401,
      "step": 9311
    },
    {
      "epoch": 3.605110336817654,
      "grad_norm": 35.04563903808594,
      "learning_rate": 7.105432959091496e-06,
      "loss": 1.3787,
      "step": 9312
    },
    {
      "epoch": 3.605497483546264,
      "grad_norm": 28.977142333984375,
      "learning_rate": 7.105002796059707e-06,
      "loss": 1.5638,
      "step": 9313
    },
    {
      "epoch": 3.605884630274874,
      "grad_norm": 14.48320484161377,
      "learning_rate": 7.104572633027919e-06,
      "loss": 1.3186,
      "step": 9314
    },
    {
      "epoch": 3.6062717770034842,
      "grad_norm": 24.047574996948242,
      "learning_rate": 7.1041424699961295e-06,
      "loss": 0.4729,
      "step": 9315
    },
    {
      "epoch": 3.6066589237320947,
      "grad_norm": 35.549835205078125,
      "learning_rate": 7.10371230696434e-06,
      "loss": 1.1502,
      "step": 9316
    },
    {
      "epoch": 3.6070460704607044,
      "grad_norm": 9.295628547668457,
      "learning_rate": 7.103282143932551e-06,
      "loss": 0.4104,
      "step": 9317
    },
    {
      "epoch": 3.607433217189315,
      "grad_norm": 56.93695831298828,
      "learning_rate": 7.102851980900762e-06,
      "loss": 2.0292,
      "step": 9318
    },
    {
      "epoch": 3.607820363917925,
      "grad_norm": 11.62222957611084,
      "learning_rate": 7.102421817868973e-06,
      "loss": 0.2453,
      "step": 9319
    },
    {
      "epoch": 3.608207510646535,
      "grad_norm": 13.108406066894531,
      "learning_rate": 7.101991654837184e-06,
      "loss": 1.2984,
      "step": 9320
    },
    {
      "epoch": 3.608594657375145,
      "grad_norm": 51.15052032470703,
      "learning_rate": 7.101561491805395e-06,
      "loss": 1.187,
      "step": 9321
    },
    {
      "epoch": 3.608981804103755,
      "grad_norm": 17.28125,
      "learning_rate": 7.101131328773606e-06,
      "loss": 0.7251,
      "step": 9322
    },
    {
      "epoch": 3.6093689508323656,
      "grad_norm": 14.877418518066406,
      "learning_rate": 7.1007011657418166e-06,
      "loss": 1.0025,
      "step": 9323
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 20.46582794189453,
      "learning_rate": 7.100271002710027e-06,
      "loss": 0.6614,
      "step": 9324
    },
    {
      "epoch": 3.6101432442895858,
      "grad_norm": 23.76131820678711,
      "learning_rate": 7.099840839678238e-06,
      "loss": 1.0297,
      "step": 9325
    },
    {
      "epoch": 3.610530391018196,
      "grad_norm": 34.1171989440918,
      "learning_rate": 7.09941067664645e-06,
      "loss": 1.9672,
      "step": 9326
    },
    {
      "epoch": 3.610917537746806,
      "grad_norm": 18.229381561279297,
      "learning_rate": 7.0989805136146605e-06,
      "loss": 1.0211,
      "step": 9327
    },
    {
      "epoch": 3.6113046844754164,
      "grad_norm": 13.101606369018555,
      "learning_rate": 7.098550350582871e-06,
      "loss": 0.7436,
      "step": 9328
    },
    {
      "epoch": 3.6116918312040265,
      "grad_norm": 3.0093860626220703,
      "learning_rate": 7.098120187551082e-06,
      "loss": 0.0885,
      "step": 9329
    },
    {
      "epoch": 3.6120789779326365,
      "grad_norm": 35.56382751464844,
      "learning_rate": 7.097690024519294e-06,
      "loss": 3.1596,
      "step": 9330
    },
    {
      "epoch": 3.6124661246612466,
      "grad_norm": 42.835391998291016,
      "learning_rate": 7.0972598614875045e-06,
      "loss": 1.5954,
      "step": 9331
    },
    {
      "epoch": 3.6128532713898567,
      "grad_norm": 80.3608169555664,
      "learning_rate": 7.096829698455715e-06,
      "loss": 1.701,
      "step": 9332
    },
    {
      "epoch": 3.6132404181184667,
      "grad_norm": 6.582252502441406,
      "learning_rate": 7.0963995354239265e-06,
      "loss": 0.4471,
      "step": 9333
    },
    {
      "epoch": 3.613627564847077,
      "grad_norm": 16.58310890197754,
      "learning_rate": 7.095969372392137e-06,
      "loss": 0.5699,
      "step": 9334
    },
    {
      "epoch": 3.6140147115756873,
      "grad_norm": 25.079113006591797,
      "learning_rate": 7.0955392093603485e-06,
      "loss": 1.7366,
      "step": 9335
    },
    {
      "epoch": 3.6144018583042974,
      "grad_norm": 15.908926010131836,
      "learning_rate": 7.095109046328559e-06,
      "loss": 0.6034,
      "step": 9336
    },
    {
      "epoch": 3.6147890050329075,
      "grad_norm": 28.47456169128418,
      "learning_rate": 7.0946788832967705e-06,
      "loss": 3.3656,
      "step": 9337
    },
    {
      "epoch": 3.6151761517615175,
      "grad_norm": 19.214881896972656,
      "learning_rate": 7.094248720264981e-06,
      "loss": 1.9947,
      "step": 9338
    },
    {
      "epoch": 3.6155632984901276,
      "grad_norm": 28.503049850463867,
      "learning_rate": 7.093818557233192e-06,
      "loss": 0.6007,
      "step": 9339
    },
    {
      "epoch": 3.615950445218738,
      "grad_norm": 39.6118049621582,
      "learning_rate": 7.093388394201402e-06,
      "loss": 1.7969,
      "step": 9340
    },
    {
      "epoch": 3.616337591947348,
      "grad_norm": 39.206687927246094,
      "learning_rate": 7.0929582311696144e-06,
      "loss": 2.1673,
      "step": 9341
    },
    {
      "epoch": 3.6167247386759582,
      "grad_norm": 28.594409942626953,
      "learning_rate": 7.092528068137825e-06,
      "loss": 1.5372,
      "step": 9342
    },
    {
      "epoch": 3.6171118854045683,
      "grad_norm": 43.14160919189453,
      "learning_rate": 7.0920979051060356e-06,
      "loss": 2.805,
      "step": 9343
    },
    {
      "epoch": 3.6174990321331784,
      "grad_norm": 48.35084915161133,
      "learning_rate": 7.091667742074246e-06,
      "loss": 1.6494,
      "step": 9344
    },
    {
      "epoch": 3.617886178861789,
      "grad_norm": 35.478389739990234,
      "learning_rate": 7.091237579042458e-06,
      "loss": 1.6393,
      "step": 9345
    },
    {
      "epoch": 3.6182733255903985,
      "grad_norm": 23.098344802856445,
      "learning_rate": 7.090807416010669e-06,
      "loss": 2.8061,
      "step": 9346
    },
    {
      "epoch": 3.618660472319009,
      "grad_norm": 26.102645874023438,
      "learning_rate": 7.0903772529788795e-06,
      "loss": 2.1646,
      "step": 9347
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 18.69910430908203,
      "learning_rate": 7.08994708994709e-06,
      "loss": 0.6746,
      "step": 9348
    },
    {
      "epoch": 3.619434765776229,
      "grad_norm": 9.800230026245117,
      "learning_rate": 7.0895169269153015e-06,
      "loss": 0.7332,
      "step": 9349
    },
    {
      "epoch": 3.619821912504839,
      "grad_norm": 31.841289520263672,
      "learning_rate": 7.089086763883513e-06,
      "loss": 1.3737,
      "step": 9350
    },
    {
      "epoch": 3.6202090592334493,
      "grad_norm": 8.990680694580078,
      "learning_rate": 7.0886566008517235e-06,
      "loss": 0.173,
      "step": 9351
    },
    {
      "epoch": 3.6205962059620598,
      "grad_norm": 19.868867874145508,
      "learning_rate": 7.088226437819934e-06,
      "loss": 1.1815,
      "step": 9352
    },
    {
      "epoch": 3.62098335269067,
      "grad_norm": 25.27667808532715,
      "learning_rate": 7.0877962747881455e-06,
      "loss": 0.9124,
      "step": 9353
    },
    {
      "epoch": 3.62137049941928,
      "grad_norm": 57.0646858215332,
      "learning_rate": 7.087366111756356e-06,
      "loss": 1.2076,
      "step": 9354
    },
    {
      "epoch": 3.62175764614789,
      "grad_norm": 18.828460693359375,
      "learning_rate": 7.086935948724567e-06,
      "loss": 0.9084,
      "step": 9355
    },
    {
      "epoch": 3.6221447928765,
      "grad_norm": 10.972291946411133,
      "learning_rate": 7.086505785692778e-06,
      "loss": 0.3288,
      "step": 9356
    },
    {
      "epoch": 3.6225319396051106,
      "grad_norm": 7.1737141609191895,
      "learning_rate": 7.0860756226609895e-06,
      "loss": 0.4393,
      "step": 9357
    },
    {
      "epoch": 3.6229190863337206,
      "grad_norm": 12.991506576538086,
      "learning_rate": 7.0856454596292e-06,
      "loss": 1.2443,
      "step": 9358
    },
    {
      "epoch": 3.6233062330623307,
      "grad_norm": 18.890911102294922,
      "learning_rate": 7.085215296597411e-06,
      "loss": 1.2607,
      "step": 9359
    },
    {
      "epoch": 3.6236933797909407,
      "grad_norm": 31.98001480102539,
      "learning_rate": 7.084785133565621e-06,
      "loss": 1.1815,
      "step": 9360
    },
    {
      "epoch": 3.624080526519551,
      "grad_norm": 37.31456756591797,
      "learning_rate": 7.0843549705338334e-06,
      "loss": 1.3369,
      "step": 9361
    },
    {
      "epoch": 3.6244676732481613,
      "grad_norm": 31.251697540283203,
      "learning_rate": 7.083924807502044e-06,
      "loss": 1.4613,
      "step": 9362
    },
    {
      "epoch": 3.624854819976771,
      "grad_norm": 25.468393325805664,
      "learning_rate": 7.0834946444702546e-06,
      "loss": 1.8012,
      "step": 9363
    },
    {
      "epoch": 3.6252419667053815,
      "grad_norm": 27.074228286743164,
      "learning_rate": 7.083064481438465e-06,
      "loss": 0.9572,
      "step": 9364
    },
    {
      "epoch": 3.6256291134339915,
      "grad_norm": 25.299558639526367,
      "learning_rate": 7.082634318406677e-06,
      "loss": 0.8514,
      "step": 9365
    },
    {
      "epoch": 3.6260162601626016,
      "grad_norm": 45.2540168762207,
      "learning_rate": 7.082204155374888e-06,
      "loss": 1.8944,
      "step": 9366
    },
    {
      "epoch": 3.6264034068912117,
      "grad_norm": 22.244491577148438,
      "learning_rate": 7.0817739923430985e-06,
      "loss": 1.2323,
      "step": 9367
    },
    {
      "epoch": 3.6267905536198217,
      "grad_norm": 19.734804153442383,
      "learning_rate": 7.081343829311309e-06,
      "loss": 0.8489,
      "step": 9368
    },
    {
      "epoch": 3.6271777003484322,
      "grad_norm": 29.636030197143555,
      "learning_rate": 7.0809136662795205e-06,
      "loss": 0.911,
      "step": 9369
    },
    {
      "epoch": 3.6275648470770423,
      "grad_norm": 11.457240104675293,
      "learning_rate": 7.080483503247731e-06,
      "loss": 0.8332,
      "step": 9370
    },
    {
      "epoch": 3.6279519938056524,
      "grad_norm": 7.273163795471191,
      "learning_rate": 7.0800533402159425e-06,
      "loss": 0.3679,
      "step": 9371
    },
    {
      "epoch": 3.6283391405342624,
      "grad_norm": 20.16249656677246,
      "learning_rate": 7.079623177184154e-06,
      "loss": 0.5144,
      "step": 9372
    },
    {
      "epoch": 3.6287262872628725,
      "grad_norm": 35.0586051940918,
      "learning_rate": 7.0791930141523645e-06,
      "loss": 2.0341,
      "step": 9373
    },
    {
      "epoch": 3.629113433991483,
      "grad_norm": 16.437259674072266,
      "learning_rate": 7.078762851120575e-06,
      "loss": 0.4203,
      "step": 9374
    },
    {
      "epoch": 3.629500580720093,
      "grad_norm": 10.784815788269043,
      "learning_rate": 7.078332688088786e-06,
      "loss": 0.5884,
      "step": 9375
    },
    {
      "epoch": 3.629887727448703,
      "grad_norm": 31.509037017822266,
      "learning_rate": 7.077902525056998e-06,
      "loss": 1.374,
      "step": 9376
    },
    {
      "epoch": 3.630274874177313,
      "grad_norm": 14.944653511047363,
      "learning_rate": 7.0774723620252085e-06,
      "loss": 0.8457,
      "step": 9377
    },
    {
      "epoch": 3.6306620209059233,
      "grad_norm": 9.305750846862793,
      "learning_rate": 7.077042198993419e-06,
      "loss": 0.4454,
      "step": 9378
    },
    {
      "epoch": 3.6310491676345333,
      "grad_norm": 54.54429626464844,
      "learning_rate": 7.07661203596163e-06,
      "loss": 1.1445,
      "step": 9379
    },
    {
      "epoch": 3.6314363143631434,
      "grad_norm": 32.4383659362793,
      "learning_rate": 7.076181872929842e-06,
      "loss": 1.5519,
      "step": 9380
    },
    {
      "epoch": 3.631823461091754,
      "grad_norm": 26.11466407775879,
      "learning_rate": 7.075751709898052e-06,
      "loss": 1.3666,
      "step": 9381
    },
    {
      "epoch": 3.632210607820364,
      "grad_norm": 45.24138641357422,
      "learning_rate": 7.075321546866263e-06,
      "loss": 2.216,
      "step": 9382
    },
    {
      "epoch": 3.632597754548974,
      "grad_norm": 32.13068771362305,
      "learning_rate": 7.0748913838344736e-06,
      "loss": 2.3065,
      "step": 9383
    },
    {
      "epoch": 3.632984901277584,
      "grad_norm": 31.952444076538086,
      "learning_rate": 7.074461220802685e-06,
      "loss": 1.3497,
      "step": 9384
    },
    {
      "epoch": 3.633372048006194,
      "grad_norm": 8.742724418640137,
      "learning_rate": 7.0740310577708955e-06,
      "loss": 1.2225,
      "step": 9385
    },
    {
      "epoch": 3.6337591947348047,
      "grad_norm": 26.59064483642578,
      "learning_rate": 7.073600894739107e-06,
      "loss": 0.9623,
      "step": 9386
    },
    {
      "epoch": 3.6341463414634148,
      "grad_norm": 21.09339714050293,
      "learning_rate": 7.0731707317073175e-06,
      "loss": 1.5455,
      "step": 9387
    },
    {
      "epoch": 3.634533488192025,
      "grad_norm": 19.246538162231445,
      "learning_rate": 7.072740568675529e-06,
      "loss": 1.1083,
      "step": 9388
    },
    {
      "epoch": 3.634920634920635,
      "grad_norm": 20.929824829101562,
      "learning_rate": 7.0723104056437395e-06,
      "loss": 0.8827,
      "step": 9389
    },
    {
      "epoch": 3.635307781649245,
      "grad_norm": 13.314988136291504,
      "learning_rate": 7.07188024261195e-06,
      "loss": 0.4502,
      "step": 9390
    },
    {
      "epoch": 3.6356949283778555,
      "grad_norm": 17.799345016479492,
      "learning_rate": 7.071450079580161e-06,
      "loss": 1.9407,
      "step": 9391
    },
    {
      "epoch": 3.636082075106465,
      "grad_norm": 134.15943908691406,
      "learning_rate": 7.071019916548373e-06,
      "loss": 2.2091,
      "step": 9392
    },
    {
      "epoch": 3.6364692218350756,
      "grad_norm": 40.1240348815918,
      "learning_rate": 7.0705897535165835e-06,
      "loss": 1.6374,
      "step": 9393
    },
    {
      "epoch": 3.6368563685636857,
      "grad_norm": 26.539255142211914,
      "learning_rate": 7.070159590484794e-06,
      "loss": 0.824,
      "step": 9394
    },
    {
      "epoch": 3.6372435152922957,
      "grad_norm": 11.03683090209961,
      "learning_rate": 7.069729427453005e-06,
      "loss": 1.0011,
      "step": 9395
    },
    {
      "epoch": 3.637630662020906,
      "grad_norm": 7.321800708770752,
      "learning_rate": 7.069299264421217e-06,
      "loss": 0.5311,
      "step": 9396
    },
    {
      "epoch": 3.638017808749516,
      "grad_norm": 6.661656856536865,
      "learning_rate": 7.0688691013894275e-06,
      "loss": 0.4206,
      "step": 9397
    },
    {
      "epoch": 3.6384049554781264,
      "grad_norm": 26.80825424194336,
      "learning_rate": 7.068438938357638e-06,
      "loss": 1.7642,
      "step": 9398
    },
    {
      "epoch": 3.6387921022067364,
      "grad_norm": 18.845916748046875,
      "learning_rate": 7.068008775325849e-06,
      "loss": 1.0341,
      "step": 9399
    },
    {
      "epoch": 3.6391792489353465,
      "grad_norm": 13.023694038391113,
      "learning_rate": 7.06757861229406e-06,
      "loss": 0.7011,
      "step": 9400
    },
    {
      "epoch": 3.6395663956639566,
      "grad_norm": 12.19284725189209,
      "learning_rate": 7.067148449262271e-06,
      "loss": 0.7057,
      "step": 9401
    },
    {
      "epoch": 3.6399535423925666,
      "grad_norm": 31.522422790527344,
      "learning_rate": 7.066718286230482e-06,
      "loss": 0.9535,
      "step": 9402
    },
    {
      "epoch": 3.640340689121177,
      "grad_norm": 13.383715629577637,
      "learning_rate": 7.0662881231986926e-06,
      "loss": 1.0302,
      "step": 9403
    },
    {
      "epoch": 3.640727835849787,
      "grad_norm": 23.73516082763672,
      "learning_rate": 7.065857960166904e-06,
      "loss": 0.5085,
      "step": 9404
    },
    {
      "epoch": 3.6411149825783973,
      "grad_norm": 15.636106491088867,
      "learning_rate": 7.0654277971351145e-06,
      "loss": 1.1094,
      "step": 9405
    },
    {
      "epoch": 3.6415021293070073,
      "grad_norm": 35.7205810546875,
      "learning_rate": 7.064997634103325e-06,
      "loss": 2.4186,
      "step": 9406
    },
    {
      "epoch": 3.6418892760356174,
      "grad_norm": 14.776636123657227,
      "learning_rate": 7.0645674710715365e-06,
      "loss": 1.1175,
      "step": 9407
    },
    {
      "epoch": 3.642276422764228,
      "grad_norm": 19.345508575439453,
      "learning_rate": 7.064137308039748e-06,
      "loss": 0.5637,
      "step": 9408
    },
    {
      "epoch": 3.6426635694928375,
      "grad_norm": 16.947599411010742,
      "learning_rate": 7.0637071450079585e-06,
      "loss": 1.2173,
      "step": 9409
    },
    {
      "epoch": 3.643050716221448,
      "grad_norm": 25.274051666259766,
      "learning_rate": 7.063276981976169e-06,
      "loss": 2.1385,
      "step": 9410
    },
    {
      "epoch": 3.643437862950058,
      "grad_norm": 20.011690139770508,
      "learning_rate": 7.06284681894438e-06,
      "loss": 0.428,
      "step": 9411
    },
    {
      "epoch": 3.643825009678668,
      "grad_norm": 47.3773193359375,
      "learning_rate": 7.062416655912592e-06,
      "loss": 1.0983,
      "step": 9412
    },
    {
      "epoch": 3.6442121564072782,
      "grad_norm": 35.90505599975586,
      "learning_rate": 7.0619864928808025e-06,
      "loss": 0.9212,
      "step": 9413
    },
    {
      "epoch": 3.6445993031358883,
      "grad_norm": 14.254467964172363,
      "learning_rate": 7.061556329849013e-06,
      "loss": 0.9686,
      "step": 9414
    },
    {
      "epoch": 3.644986449864499,
      "grad_norm": 24.799413681030273,
      "learning_rate": 7.0611261668172245e-06,
      "loss": 0.9782,
      "step": 9415
    },
    {
      "epoch": 3.645373596593109,
      "grad_norm": 40.23099136352539,
      "learning_rate": 7.060696003785436e-06,
      "loss": 2.0449,
      "step": 9416
    },
    {
      "epoch": 3.645760743321719,
      "grad_norm": 15.727972030639648,
      "learning_rate": 7.0602658407536464e-06,
      "loss": 0.8972,
      "step": 9417
    },
    {
      "epoch": 3.646147890050329,
      "grad_norm": 22.473764419555664,
      "learning_rate": 7.059835677721857e-06,
      "loss": 1.3776,
      "step": 9418
    },
    {
      "epoch": 3.646535036778939,
      "grad_norm": 57.23274230957031,
      "learning_rate": 7.0594055146900684e-06,
      "loss": 1.9313,
      "step": 9419
    },
    {
      "epoch": 3.6469221835075496,
      "grad_norm": 17.92875099182129,
      "learning_rate": 7.058975351658279e-06,
      "loss": 0.9995,
      "step": 9420
    },
    {
      "epoch": 3.6473093302361597,
      "grad_norm": 44.584007263183594,
      "learning_rate": 7.0585451886264896e-06,
      "loss": 1.4286,
      "step": 9421
    },
    {
      "epoch": 3.6476964769647697,
      "grad_norm": 13.513385772705078,
      "learning_rate": 7.058115025594701e-06,
      "loss": 0.7465,
      "step": 9422
    },
    {
      "epoch": 3.64808362369338,
      "grad_norm": 20.20501136779785,
      "learning_rate": 7.057684862562912e-06,
      "loss": 1.2309,
      "step": 9423
    },
    {
      "epoch": 3.64847077042199,
      "grad_norm": 14.524640083312988,
      "learning_rate": 7.057254699531123e-06,
      "loss": 0.8976,
      "step": 9424
    },
    {
      "epoch": 3.6488579171506,
      "grad_norm": 27.50432014465332,
      "learning_rate": 7.0568245364993335e-06,
      "loss": 0.8359,
      "step": 9425
    },
    {
      "epoch": 3.64924506387921,
      "grad_norm": 30.12266731262207,
      "learning_rate": 7.056394373467544e-06,
      "loss": 1.4868,
      "step": 9426
    },
    {
      "epoch": 3.6496322106078205,
      "grad_norm": 28.57353973388672,
      "learning_rate": 7.055964210435756e-06,
      "loss": 1.2742,
      "step": 9427
    },
    {
      "epoch": 3.6500193573364306,
      "grad_norm": 57.74441146850586,
      "learning_rate": 7.055534047403967e-06,
      "loss": 1.5674,
      "step": 9428
    },
    {
      "epoch": 3.6504065040650406,
      "grad_norm": 68.07331085205078,
      "learning_rate": 7.0551038843721775e-06,
      "loss": 1.5442,
      "step": 9429
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 15.710567474365234,
      "learning_rate": 7.054673721340388e-06,
      "loss": 1.5467,
      "step": 9430
    },
    {
      "epoch": 3.6511807975222608,
      "grad_norm": 30.189088821411133,
      "learning_rate": 7.0542435583086e-06,
      "loss": 1.1125,
      "step": 9431
    },
    {
      "epoch": 3.6515679442508713,
      "grad_norm": 23.145395278930664,
      "learning_rate": 7.053813395276811e-06,
      "loss": 0.8889,
      "step": 9432
    },
    {
      "epoch": 3.6519550909794813,
      "grad_norm": 17.526601791381836,
      "learning_rate": 7.0533832322450215e-06,
      "loss": 3.2055,
      "step": 9433
    },
    {
      "epoch": 3.6523422377080914,
      "grad_norm": 14.535768508911133,
      "learning_rate": 7.052953069213232e-06,
      "loss": 0.7701,
      "step": 9434
    },
    {
      "epoch": 3.6527293844367015,
      "grad_norm": 17.711688995361328,
      "learning_rate": 7.0525229061814435e-06,
      "loss": 1.1115,
      "step": 9435
    },
    {
      "epoch": 3.6531165311653115,
      "grad_norm": 25.091474533081055,
      "learning_rate": 7.052092743149654e-06,
      "loss": 1.5633,
      "step": 9436
    },
    {
      "epoch": 3.653503677893922,
      "grad_norm": 22.77608871459961,
      "learning_rate": 7.0516625801178654e-06,
      "loss": 1.071,
      "step": 9437
    },
    {
      "epoch": 3.6538908246225317,
      "grad_norm": 27.390888214111328,
      "learning_rate": 7.051232417086076e-06,
      "loss": 1.546,
      "step": 9438
    },
    {
      "epoch": 3.654277971351142,
      "grad_norm": 42.777523040771484,
      "learning_rate": 7.0508022540542874e-06,
      "loss": 2.0146,
      "step": 9439
    },
    {
      "epoch": 3.6546651180797523,
      "grad_norm": 33.181766510009766,
      "learning_rate": 7.050372091022498e-06,
      "loss": 1.4597,
      "step": 9440
    },
    {
      "epoch": 3.6550522648083623,
      "grad_norm": 56.671112060546875,
      "learning_rate": 7.0499419279907086e-06,
      "loss": 1.4421,
      "step": 9441
    },
    {
      "epoch": 3.6554394115369724,
      "grad_norm": 3.2550745010375977,
      "learning_rate": 7.049511764958919e-06,
      "loss": 0.0838,
      "step": 9442
    },
    {
      "epoch": 3.6558265582655824,
      "grad_norm": 32.440189361572266,
      "learning_rate": 7.049081601927131e-06,
      "loss": 1.1626,
      "step": 9443
    },
    {
      "epoch": 3.656213704994193,
      "grad_norm": 10.392643928527832,
      "learning_rate": 7.048651438895342e-06,
      "loss": 0.6831,
      "step": 9444
    },
    {
      "epoch": 3.656600851722803,
      "grad_norm": 21.853557586669922,
      "learning_rate": 7.0482212758635525e-06,
      "loss": 1.2353,
      "step": 9445
    },
    {
      "epoch": 3.656987998451413,
      "grad_norm": 10.37186050415039,
      "learning_rate": 7.047791112831763e-06,
      "loss": 0.7185,
      "step": 9446
    },
    {
      "epoch": 3.657375145180023,
      "grad_norm": 37.72003173828125,
      "learning_rate": 7.047360949799975e-06,
      "loss": 3.2182,
      "step": 9447
    },
    {
      "epoch": 3.6577622919086332,
      "grad_norm": 25.3124942779541,
      "learning_rate": 7.046930786768186e-06,
      "loss": 1.4621,
      "step": 9448
    },
    {
      "epoch": 3.6581494386372437,
      "grad_norm": 19.252525329589844,
      "learning_rate": 7.0465006237363965e-06,
      "loss": 0.9688,
      "step": 9449
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 67.3816146850586,
      "learning_rate": 7.046070460704607e-06,
      "loss": 1.2541,
      "step": 9450
    },
    {
      "epoch": 3.658923732094464,
      "grad_norm": 50.066402435302734,
      "learning_rate": 7.0456402976728185e-06,
      "loss": 2.6058,
      "step": 9451
    },
    {
      "epoch": 3.659310878823074,
      "grad_norm": 93.76300811767578,
      "learning_rate": 7.04521013464103e-06,
      "loss": 3.8396,
      "step": 9452
    },
    {
      "epoch": 3.659698025551684,
      "grad_norm": 30.634838104248047,
      "learning_rate": 7.0447799716092405e-06,
      "loss": 1.7845,
      "step": 9453
    },
    {
      "epoch": 3.6600851722802945,
      "grad_norm": 100.68927001953125,
      "learning_rate": 7.044349808577451e-06,
      "loss": 2.005,
      "step": 9454
    },
    {
      "epoch": 3.660472319008904,
      "grad_norm": 17.923072814941406,
      "learning_rate": 7.0439196455456625e-06,
      "loss": 0.9982,
      "step": 9455
    },
    {
      "epoch": 3.6608594657375146,
      "grad_norm": 24.094402313232422,
      "learning_rate": 7.043489482513873e-06,
      "loss": 2.0872,
      "step": 9456
    },
    {
      "epoch": 3.6612466124661247,
      "grad_norm": 16.027006149291992,
      "learning_rate": 7.043059319482084e-06,
      "loss": 0.6909,
      "step": 9457
    },
    {
      "epoch": 3.6616337591947348,
      "grad_norm": 31.394731521606445,
      "learning_rate": 7.042629156450296e-06,
      "loss": 1.4595,
      "step": 9458
    },
    {
      "epoch": 3.662020905923345,
      "grad_norm": 20.998268127441406,
      "learning_rate": 7.0421989934185064e-06,
      "loss": 1.2931,
      "step": 9459
    },
    {
      "epoch": 3.662408052651955,
      "grad_norm": 44.07227325439453,
      "learning_rate": 7.041768830386717e-06,
      "loss": 0.978,
      "step": 9460
    },
    {
      "epoch": 3.6627951993805654,
      "grad_norm": 25.036720275878906,
      "learning_rate": 7.0413386673549276e-06,
      "loss": 0.9529,
      "step": 9461
    },
    {
      "epoch": 3.6631823461091755,
      "grad_norm": 33.904571533203125,
      "learning_rate": 7.04090850432314e-06,
      "loss": 2.0336,
      "step": 9462
    },
    {
      "epoch": 3.6635694928377855,
      "grad_norm": 47.91767501831055,
      "learning_rate": 7.04047834129135e-06,
      "loss": 2.2241,
      "step": 9463
    },
    {
      "epoch": 3.6639566395663956,
      "grad_norm": 16.114665985107422,
      "learning_rate": 7.040048178259561e-06,
      "loss": 0.3224,
      "step": 9464
    },
    {
      "epoch": 3.6643437862950057,
      "grad_norm": 42.82501983642578,
      "learning_rate": 7.0396180152277715e-06,
      "loss": 0.7176,
      "step": 9465
    },
    {
      "epoch": 3.664730933023616,
      "grad_norm": 6.9058380126953125,
      "learning_rate": 7.039187852195983e-06,
      "loss": 0.1053,
      "step": 9466
    },
    {
      "epoch": 3.6651180797522263,
      "grad_norm": 39.86665344238281,
      "learning_rate": 7.038757689164194e-06,
      "loss": 3.704,
      "step": 9467
    },
    {
      "epoch": 3.6655052264808363,
      "grad_norm": 16.748851776123047,
      "learning_rate": 7.038327526132405e-06,
      "loss": 1.2524,
      "step": 9468
    },
    {
      "epoch": 3.6658923732094464,
      "grad_norm": 25.156314849853516,
      "learning_rate": 7.0378973631006155e-06,
      "loss": 0.7712,
      "step": 9469
    },
    {
      "epoch": 3.6662795199380565,
      "grad_norm": 52.07410430908203,
      "learning_rate": 7.037467200068827e-06,
      "loss": 1.8456,
      "step": 9470
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 43.767669677734375,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 1.9198,
      "step": 9471
    },
    {
      "epoch": 3.6670538133952766,
      "grad_norm": 32.39573669433594,
      "learning_rate": 7.036606874005248e-06,
      "loss": 1.7615,
      "step": 9472
    },
    {
      "epoch": 3.667440960123887,
      "grad_norm": 15.335465431213379,
      "learning_rate": 7.0361767109734595e-06,
      "loss": 1.2707,
      "step": 9473
    },
    {
      "epoch": 3.667828106852497,
      "grad_norm": 14.301694869995117,
      "learning_rate": 7.035746547941671e-06,
      "loss": 1.932,
      "step": 9474
    },
    {
      "epoch": 3.6682152535811072,
      "grad_norm": 10.63381290435791,
      "learning_rate": 7.0353163849098815e-06,
      "loss": 0.9421,
      "step": 9475
    },
    {
      "epoch": 3.6686024003097173,
      "grad_norm": 12.36989688873291,
      "learning_rate": 7.034886221878092e-06,
      "loss": 0.3957,
      "step": 9476
    },
    {
      "epoch": 3.6689895470383274,
      "grad_norm": 125.81879425048828,
      "learning_rate": 7.034456058846303e-06,
      "loss": 1.3319,
      "step": 9477
    },
    {
      "epoch": 3.669376693766938,
      "grad_norm": 26.486032485961914,
      "learning_rate": 7.034025895814515e-06,
      "loss": 3.1165,
      "step": 9478
    },
    {
      "epoch": 3.669763840495548,
      "grad_norm": 40.19227600097656,
      "learning_rate": 7.0335957327827254e-06,
      "loss": 1.4375,
      "step": 9479
    },
    {
      "epoch": 3.670150987224158,
      "grad_norm": 24.526689529418945,
      "learning_rate": 7.033165569750936e-06,
      "loss": 2.7986,
      "step": 9480
    },
    {
      "epoch": 3.670538133952768,
      "grad_norm": 25.51361656188965,
      "learning_rate": 7.0327354067191466e-06,
      "loss": 1.2284,
      "step": 9481
    },
    {
      "epoch": 3.670925280681378,
      "grad_norm": 50.73786926269531,
      "learning_rate": 7.032305243687359e-06,
      "loss": 1.7722,
      "step": 9482
    },
    {
      "epoch": 3.6713124274099886,
      "grad_norm": 19.86456298828125,
      "learning_rate": 7.031875080655569e-06,
      "loss": 0.651,
      "step": 9483
    },
    {
      "epoch": 3.6716995741385983,
      "grad_norm": 30.4882869720459,
      "learning_rate": 7.03144491762378e-06,
      "loss": 2.0695,
      "step": 9484
    },
    {
      "epoch": 3.6720867208672088,
      "grad_norm": 15.281006813049316,
      "learning_rate": 7.0310147545919905e-06,
      "loss": 1.3173,
      "step": 9485
    },
    {
      "epoch": 3.672473867595819,
      "grad_norm": 11.065316200256348,
      "learning_rate": 7.030584591560202e-06,
      "loss": 1.3965,
      "step": 9486
    },
    {
      "epoch": 3.672861014324429,
      "grad_norm": 50.034366607666016,
      "learning_rate": 7.0301544285284125e-06,
      "loss": 0.9294,
      "step": 9487
    },
    {
      "epoch": 3.673248161053039,
      "grad_norm": 26.31464385986328,
      "learning_rate": 7.029724265496624e-06,
      "loss": 1.5194,
      "step": 9488
    },
    {
      "epoch": 3.673635307781649,
      "grad_norm": 48.30597686767578,
      "learning_rate": 7.0292941024648345e-06,
      "loss": 1.4121,
      "step": 9489
    },
    {
      "epoch": 3.6740224545102595,
      "grad_norm": 51.90446853637695,
      "learning_rate": 7.028863939433046e-06,
      "loss": 1.5973,
      "step": 9490
    },
    {
      "epoch": 3.6744096012388696,
      "grad_norm": 34.17381286621094,
      "learning_rate": 7.0284337764012565e-06,
      "loss": 0.7909,
      "step": 9491
    },
    {
      "epoch": 3.6747967479674797,
      "grad_norm": 46.581687927246094,
      "learning_rate": 7.028003613369467e-06,
      "loss": 1.5452,
      "step": 9492
    },
    {
      "epoch": 3.6751838946960897,
      "grad_norm": 45.021732330322266,
      "learning_rate": 7.027573450337678e-06,
      "loss": 2.8412,
      "step": 9493
    },
    {
      "epoch": 3.6755710414247,
      "grad_norm": 8.779927253723145,
      "learning_rate": 7.02714328730589e-06,
      "loss": 0.3153,
      "step": 9494
    },
    {
      "epoch": 3.6759581881533103,
      "grad_norm": 42.14162826538086,
      "learning_rate": 7.0267131242741005e-06,
      "loss": 1.6874,
      "step": 9495
    },
    {
      "epoch": 3.6763453348819204,
      "grad_norm": 48.5448112487793,
      "learning_rate": 7.026282961242311e-06,
      "loss": 0.4573,
      "step": 9496
    },
    {
      "epoch": 3.6767324816105305,
      "grad_norm": 99.244384765625,
      "learning_rate": 7.025852798210523e-06,
      "loss": 2.5372,
      "step": 9497
    },
    {
      "epoch": 3.6771196283391405,
      "grad_norm": 15.650646209716797,
      "learning_rate": 7.025422635178734e-06,
      "loss": 0.4701,
      "step": 9498
    },
    {
      "epoch": 3.6775067750677506,
      "grad_norm": 15.927826881408691,
      "learning_rate": 7.024992472146944e-06,
      "loss": 0.9839,
      "step": 9499
    },
    {
      "epoch": 3.6778939217963607,
      "grad_norm": 60.872371673583984,
      "learning_rate": 7.024562309115155e-06,
      "loss": 1.7642,
      "step": 9500
    },
    {
      "epoch": 3.6782810685249707,
      "grad_norm": 26.183691024780273,
      "learning_rate": 7.024132146083366e-06,
      "loss": 0.6176,
      "step": 9501
    },
    {
      "epoch": 3.6786682152535812,
      "grad_norm": 142.51968383789062,
      "learning_rate": 7.023701983051577e-06,
      "loss": 1.22,
      "step": 9502
    },
    {
      "epoch": 3.6790553619821913,
      "grad_norm": 10.945009231567383,
      "learning_rate": 7.023271820019788e-06,
      "loss": 0.8329,
      "step": 9503
    },
    {
      "epoch": 3.6794425087108014,
      "grad_norm": 13.627541542053223,
      "learning_rate": 7.022841656987999e-06,
      "loss": 0.6831,
      "step": 9504
    },
    {
      "epoch": 3.6798296554394114,
      "grad_norm": 19.42472267150879,
      "learning_rate": 7.02241149395621e-06,
      "loss": 0.7528,
      "step": 9505
    },
    {
      "epoch": 3.6802168021680215,
      "grad_norm": 85.16787719726562,
      "learning_rate": 7.021981330924421e-06,
      "loss": 1.1632,
      "step": 9506
    },
    {
      "epoch": 3.680603948896632,
      "grad_norm": 21.756004333496094,
      "learning_rate": 7.0215511678926315e-06,
      "loss": 1.2204,
      "step": 9507
    },
    {
      "epoch": 3.680991095625242,
      "grad_norm": 22.451017379760742,
      "learning_rate": 7.021121004860842e-06,
      "loss": 0.9688,
      "step": 9508
    },
    {
      "epoch": 3.681378242353852,
      "grad_norm": 37.008975982666016,
      "learning_rate": 7.020690841829054e-06,
      "loss": 2.1848,
      "step": 9509
    },
    {
      "epoch": 3.681765389082462,
      "grad_norm": 7.325267314910889,
      "learning_rate": 7.020260678797265e-06,
      "loss": 0.3095,
      "step": 9510
    },
    {
      "epoch": 3.6821525358110723,
      "grad_norm": 31.860816955566406,
      "learning_rate": 7.0198305157654755e-06,
      "loss": 2.7405,
      "step": 9511
    },
    {
      "epoch": 3.682539682539683,
      "grad_norm": 90.80030059814453,
      "learning_rate": 7.019400352733686e-06,
      "loss": 1.2299,
      "step": 9512
    },
    {
      "epoch": 3.682926829268293,
      "grad_norm": 24.167768478393555,
      "learning_rate": 7.018970189701898e-06,
      "loss": 0.4779,
      "step": 9513
    },
    {
      "epoch": 3.683313975996903,
      "grad_norm": 15.991567611694336,
      "learning_rate": 7.018540026670109e-06,
      "loss": 0.8494,
      "step": 9514
    },
    {
      "epoch": 3.683701122725513,
      "grad_norm": 10.970586776733398,
      "learning_rate": 7.0181098636383195e-06,
      "loss": 0.6648,
      "step": 9515
    },
    {
      "epoch": 3.684088269454123,
      "grad_norm": 22.87424659729004,
      "learning_rate": 7.01767970060653e-06,
      "loss": 0.5482,
      "step": 9516
    },
    {
      "epoch": 3.684475416182733,
      "grad_norm": 10.548727989196777,
      "learning_rate": 7.0172495375747414e-06,
      "loss": 0.4591,
      "step": 9517
    },
    {
      "epoch": 3.684862562911343,
      "grad_norm": 17.429868698120117,
      "learning_rate": 7.016819374542953e-06,
      "loss": 0.5599,
      "step": 9518
    },
    {
      "epoch": 3.6852497096399537,
      "grad_norm": 25.767257690429688,
      "learning_rate": 7.016389211511163e-06,
      "loss": 1.3888,
      "step": 9519
    },
    {
      "epoch": 3.6856368563685638,
      "grad_norm": 25.836528778076172,
      "learning_rate": 7.015959048479374e-06,
      "loss": 1.5467,
      "step": 9520
    },
    {
      "epoch": 3.686024003097174,
      "grad_norm": 37.01865768432617,
      "learning_rate": 7.015528885447585e-06,
      "loss": 2.248,
      "step": 9521
    },
    {
      "epoch": 3.686411149825784,
      "grad_norm": 9.317631721496582,
      "learning_rate": 7.015098722415796e-06,
      "loss": 1.0415,
      "step": 9522
    },
    {
      "epoch": 3.686798296554394,
      "grad_norm": 11.744604110717773,
      "learning_rate": 7.0146685593840065e-06,
      "loss": 0.5091,
      "step": 9523
    },
    {
      "epoch": 3.6871854432830045,
      "grad_norm": 24.76842498779297,
      "learning_rate": 7.014238396352218e-06,
      "loss": 1.2784,
      "step": 9524
    },
    {
      "epoch": 3.6875725900116145,
      "grad_norm": 13.8989896774292,
      "learning_rate": 7.013808233320429e-06,
      "loss": 0.6795,
      "step": 9525
    },
    {
      "epoch": 3.6879597367402246,
      "grad_norm": 21.344871520996094,
      "learning_rate": 7.01337807028864e-06,
      "loss": 0.8673,
      "step": 9526
    },
    {
      "epoch": 3.6883468834688347,
      "grad_norm": 18.0760498046875,
      "learning_rate": 7.0129479072568505e-06,
      "loss": 0.618,
      "step": 9527
    },
    {
      "epoch": 3.6887340301974447,
      "grad_norm": 32.80089569091797,
      "learning_rate": 7.012517744225061e-06,
      "loss": 0.5765,
      "step": 9528
    },
    {
      "epoch": 3.6891211769260552,
      "grad_norm": 38.15322494506836,
      "learning_rate": 7.012087581193273e-06,
      "loss": 1.6242,
      "step": 9529
    },
    {
      "epoch": 3.689508323654665,
      "grad_norm": 26.088964462280273,
      "learning_rate": 7.011657418161484e-06,
      "loss": 1.5656,
      "step": 9530
    },
    {
      "epoch": 3.6898954703832754,
      "grad_norm": 32.72291946411133,
      "learning_rate": 7.0112272551296945e-06,
      "loss": 1.7962,
      "step": 9531
    },
    {
      "epoch": 3.6902826171118854,
      "grad_norm": 75.26573181152344,
      "learning_rate": 7.010797092097905e-06,
      "loss": 3.2615,
      "step": 9532
    },
    {
      "epoch": 3.6906697638404955,
      "grad_norm": 55.293148040771484,
      "learning_rate": 7.010366929066117e-06,
      "loss": 1.2669,
      "step": 9533
    },
    {
      "epoch": 3.6910569105691056,
      "grad_norm": 30.423065185546875,
      "learning_rate": 7.009936766034328e-06,
      "loss": 1.5377,
      "step": 9534
    },
    {
      "epoch": 3.6914440572977156,
      "grad_norm": 54.37146759033203,
      "learning_rate": 7.0095066030025384e-06,
      "loss": 1.5583,
      "step": 9535
    },
    {
      "epoch": 3.691831204026326,
      "grad_norm": 19.393953323364258,
      "learning_rate": 7.009076439970749e-06,
      "loss": 1.5114,
      "step": 9536
    },
    {
      "epoch": 3.692218350754936,
      "grad_norm": 54.829444885253906,
      "learning_rate": 7.0086462769389604e-06,
      "loss": 2.7801,
      "step": 9537
    },
    {
      "epoch": 3.6926054974835463,
      "grad_norm": 74.87928771972656,
      "learning_rate": 7.008216113907171e-06,
      "loss": 1.0712,
      "step": 9538
    },
    {
      "epoch": 3.6929926442121563,
      "grad_norm": 11.82902717590332,
      "learning_rate": 7.007785950875382e-06,
      "loss": 0.9067,
      "step": 9539
    },
    {
      "epoch": 3.6933797909407664,
      "grad_norm": 39.81972885131836,
      "learning_rate": 7.007355787843594e-06,
      "loss": 0.8972,
      "step": 9540
    },
    {
      "epoch": 3.693766937669377,
      "grad_norm": 33.45768737792969,
      "learning_rate": 7.006925624811804e-06,
      "loss": 1.5973,
      "step": 9541
    },
    {
      "epoch": 3.694154084397987,
      "grad_norm": 46.059757232666016,
      "learning_rate": 7.006495461780015e-06,
      "loss": 0.7016,
      "step": 9542
    },
    {
      "epoch": 3.694541231126597,
      "grad_norm": 14.191752433776855,
      "learning_rate": 7.0060652987482255e-06,
      "loss": 1.7485,
      "step": 9543
    },
    {
      "epoch": 3.694928377855207,
      "grad_norm": 19.87877655029297,
      "learning_rate": 7.005635135716438e-06,
      "loss": 1.2488,
      "step": 9544
    },
    {
      "epoch": 3.695315524583817,
      "grad_norm": 140.1790313720703,
      "learning_rate": 7.005204972684648e-06,
      "loss": 1.6324,
      "step": 9545
    },
    {
      "epoch": 3.6957026713124272,
      "grad_norm": 21.98398208618164,
      "learning_rate": 7.004774809652859e-06,
      "loss": 3.0698,
      "step": 9546
    },
    {
      "epoch": 3.6960898180410373,
      "grad_norm": 5.460865020751953,
      "learning_rate": 7.0043446466210695e-06,
      "loss": 0.115,
      "step": 9547
    },
    {
      "epoch": 3.696476964769648,
      "grad_norm": 3.6657674312591553,
      "learning_rate": 7.003914483589282e-06,
      "loss": 0.0912,
      "step": 9548
    },
    {
      "epoch": 3.696864111498258,
      "grad_norm": 39.119815826416016,
      "learning_rate": 7.003484320557492e-06,
      "loss": 0.8644,
      "step": 9549
    },
    {
      "epoch": 3.697251258226868,
      "grad_norm": 11.29654312133789,
      "learning_rate": 7.003054157525703e-06,
      "loss": 0.8321,
      "step": 9550
    },
    {
      "epoch": 3.697638404955478,
      "grad_norm": 34.33877182006836,
      "learning_rate": 7.0026239944939135e-06,
      "loss": 2.7533,
      "step": 9551
    },
    {
      "epoch": 3.698025551684088,
      "grad_norm": 17.576745986938477,
      "learning_rate": 7.002193831462125e-06,
      "loss": 1.0972,
      "step": 9552
    },
    {
      "epoch": 3.6984126984126986,
      "grad_norm": 21.6926326751709,
      "learning_rate": 7.0017636684303355e-06,
      "loss": 2.1119,
      "step": 9553
    },
    {
      "epoch": 3.6987998451413087,
      "grad_norm": 17.125263214111328,
      "learning_rate": 7.001333505398547e-06,
      "loss": 1.1568,
      "step": 9554
    },
    {
      "epoch": 3.6991869918699187,
      "grad_norm": 20.666900634765625,
      "learning_rate": 7.0009033423667574e-06,
      "loss": 1.511,
      "step": 9555
    },
    {
      "epoch": 3.699574138598529,
      "grad_norm": 62.503353118896484,
      "learning_rate": 7.000473179334969e-06,
      "loss": 1.4043,
      "step": 9556
    },
    {
      "epoch": 3.699961285327139,
      "grad_norm": 33.00088882446289,
      "learning_rate": 7.0000430163031794e-06,
      "loss": 1.7467,
      "step": 9557
    },
    {
      "epoch": 3.7003484320557494,
      "grad_norm": 54.65889358520508,
      "learning_rate": 6.99961285327139e-06,
      "loss": 1.0128,
      "step": 9558
    },
    {
      "epoch": 3.7007355787843594,
      "grad_norm": 15.990490913391113,
      "learning_rate": 6.9991826902396006e-06,
      "loss": 0.5403,
      "step": 9559
    },
    {
      "epoch": 3.7011227255129695,
      "grad_norm": 23.471107482910156,
      "learning_rate": 6.998752527207813e-06,
      "loss": 1.7265,
      "step": 9560
    },
    {
      "epoch": 3.7015098722415796,
      "grad_norm": 26.18837547302246,
      "learning_rate": 6.998322364176023e-06,
      "loss": 1.5104,
      "step": 9561
    },
    {
      "epoch": 3.7018970189701896,
      "grad_norm": 45.760433197021484,
      "learning_rate": 6.997892201144234e-06,
      "loss": 1.6641,
      "step": 9562
    },
    {
      "epoch": 3.7022841656987997,
      "grad_norm": 25.376012802124023,
      "learning_rate": 6.9974620381124445e-06,
      "loss": 0.7575,
      "step": 9563
    },
    {
      "epoch": 3.7026713124274098,
      "grad_norm": 30.420991897583008,
      "learning_rate": 6.997031875080657e-06,
      "loss": 0.5781,
      "step": 9564
    },
    {
      "epoch": 3.7030584591560203,
      "grad_norm": 16.70840835571289,
      "learning_rate": 6.996601712048867e-06,
      "loss": 1.1569,
      "step": 9565
    },
    {
      "epoch": 3.7034456058846303,
      "grad_norm": 35.71893310546875,
      "learning_rate": 6.996171549017078e-06,
      "loss": 2.0229,
      "step": 9566
    },
    {
      "epoch": 3.7038327526132404,
      "grad_norm": 31.783676147460938,
      "learning_rate": 6.9957413859852885e-06,
      "loss": 1.6454,
      "step": 9567
    },
    {
      "epoch": 3.7042198993418505,
      "grad_norm": 13.559247970581055,
      "learning_rate": 6.9953112229535e-06,
      "loss": 0.9791,
      "step": 9568
    },
    {
      "epoch": 3.7046070460704605,
      "grad_norm": 67.77037811279297,
      "learning_rate": 6.994881059921711e-06,
      "loss": 1.6463,
      "step": 9569
    },
    {
      "epoch": 3.704994192799071,
      "grad_norm": 22.410024642944336,
      "learning_rate": 6.994450896889922e-06,
      "loss": 0.9725,
      "step": 9570
    },
    {
      "epoch": 3.705381339527681,
      "grad_norm": 23.065162658691406,
      "learning_rate": 6.9940207338581325e-06,
      "loss": 0.8313,
      "step": 9571
    },
    {
      "epoch": 3.705768486256291,
      "grad_norm": 10.859575271606445,
      "learning_rate": 6.993590570826344e-06,
      "loss": 0.3219,
      "step": 9572
    },
    {
      "epoch": 3.7061556329849012,
      "grad_norm": 15.846582412719727,
      "learning_rate": 6.9931604077945545e-06,
      "loss": 0.7061,
      "step": 9573
    },
    {
      "epoch": 3.7065427797135113,
      "grad_norm": 11.700641632080078,
      "learning_rate": 6.992730244762765e-06,
      "loss": 0.1544,
      "step": 9574
    },
    {
      "epoch": 3.706929926442122,
      "grad_norm": 30.37710952758789,
      "learning_rate": 6.9923000817309764e-06,
      "loss": 3.1049,
      "step": 9575
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 29.559444427490234,
      "learning_rate": 6.991869918699188e-06,
      "loss": 0.9167,
      "step": 9576
    },
    {
      "epoch": 3.707704219899342,
      "grad_norm": 65.84650421142578,
      "learning_rate": 6.9914397556673984e-06,
      "loss": 1.3649,
      "step": 9577
    },
    {
      "epoch": 3.708091366627952,
      "grad_norm": 17.82686424255371,
      "learning_rate": 6.991009592635609e-06,
      "loss": 2.0922,
      "step": 9578
    },
    {
      "epoch": 3.708478513356562,
      "grad_norm": 68.63525390625,
      "learning_rate": 6.990579429603821e-06,
      "loss": 1.2128,
      "step": 9579
    },
    {
      "epoch": 3.708865660085172,
      "grad_norm": 7.946486949920654,
      "learning_rate": 6.990149266572032e-06,
      "loss": 1.1019,
      "step": 9580
    },
    {
      "epoch": 3.709252806813782,
      "grad_norm": 26.000856399536133,
      "learning_rate": 6.989719103540242e-06,
      "loss": 3.5357,
      "step": 9581
    },
    {
      "epoch": 3.7096399535423927,
      "grad_norm": 32.31845474243164,
      "learning_rate": 6.989288940508453e-06,
      "loss": 2.2893,
      "step": 9582
    },
    {
      "epoch": 3.710027100271003,
      "grad_norm": 30.519567489624023,
      "learning_rate": 6.988858777476664e-06,
      "loss": 0.8526,
      "step": 9583
    },
    {
      "epoch": 3.710414246999613,
      "grad_norm": 18.790935516357422,
      "learning_rate": 6.988428614444876e-06,
      "loss": 1.5564,
      "step": 9584
    },
    {
      "epoch": 3.710801393728223,
      "grad_norm": 46.976463317871094,
      "learning_rate": 6.987998451413086e-06,
      "loss": 1.4563,
      "step": 9585
    },
    {
      "epoch": 3.711188540456833,
      "grad_norm": 8.504783630371094,
      "learning_rate": 6.987568288381297e-06,
      "loss": 0.4957,
      "step": 9586
    },
    {
      "epoch": 3.7115756871854435,
      "grad_norm": 10.626859664916992,
      "learning_rate": 6.987138125349508e-06,
      "loss": 0.6729,
      "step": 9587
    },
    {
      "epoch": 3.7119628339140536,
      "grad_norm": 8.220521926879883,
      "learning_rate": 6.986707962317719e-06,
      "loss": 0.5152,
      "step": 9588
    },
    {
      "epoch": 3.7123499806426636,
      "grad_norm": 17.859140396118164,
      "learning_rate": 6.9862777992859295e-06,
      "loss": 0.8139,
      "step": 9589
    },
    {
      "epoch": 3.7127371273712737,
      "grad_norm": 19.436206817626953,
      "learning_rate": 6.985847636254141e-06,
      "loss": 1.8272,
      "step": 9590
    },
    {
      "epoch": 3.7131242740998838,
      "grad_norm": 43.07782745361328,
      "learning_rate": 6.985417473222352e-06,
      "loss": 1.3468,
      "step": 9591
    },
    {
      "epoch": 3.713511420828494,
      "grad_norm": 28.540634155273438,
      "learning_rate": 6.984987310190563e-06,
      "loss": 0.8224,
      "step": 9592
    },
    {
      "epoch": 3.713898567557104,
      "grad_norm": 35.324127197265625,
      "learning_rate": 6.9845571471587735e-06,
      "loss": 1.6013,
      "step": 9593
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 94.08590698242188,
      "learning_rate": 6.984126984126984e-06,
      "loss": 0.985,
      "step": 9594
    },
    {
      "epoch": 3.7146728610143245,
      "grad_norm": 15.717791557312012,
      "learning_rate": 6.983696821095196e-06,
      "loss": 0.8213,
      "step": 9595
    },
    {
      "epoch": 3.7150600077429345,
      "grad_norm": 14.027826309204102,
      "learning_rate": 6.983266658063407e-06,
      "loss": 1.1366,
      "step": 9596
    },
    {
      "epoch": 3.7154471544715446,
      "grad_norm": 9.971344947814941,
      "learning_rate": 6.9828364950316174e-06,
      "loss": 0.3034,
      "step": 9597
    },
    {
      "epoch": 3.7158343012001547,
      "grad_norm": 186.25204467773438,
      "learning_rate": 6.982406331999828e-06,
      "loss": 2.3214,
      "step": 9598
    },
    {
      "epoch": 3.716221447928765,
      "grad_norm": 22.89039421081543,
      "learning_rate": 6.98197616896804e-06,
      "loss": 1.0021,
      "step": 9599
    },
    {
      "epoch": 3.7166085946573753,
      "grad_norm": 55.86052703857422,
      "learning_rate": 6.981546005936251e-06,
      "loss": 2.2318,
      "step": 9600
    },
    {
      "epoch": 3.7169957413859853,
      "grad_norm": 27.8466796875,
      "learning_rate": 6.981115842904461e-06,
      "loss": 1.2519,
      "step": 9601
    },
    {
      "epoch": 3.7173828881145954,
      "grad_norm": 57.05837631225586,
      "learning_rate": 6.980685679872672e-06,
      "loss": 0.7986,
      "step": 9602
    },
    {
      "epoch": 3.7177700348432055,
      "grad_norm": 8.285183906555176,
      "learning_rate": 6.980255516840883e-06,
      "loss": 0.3592,
      "step": 9603
    },
    {
      "epoch": 3.718157181571816,
      "grad_norm": 12.192403793334961,
      "learning_rate": 6.979825353809094e-06,
      "loss": 1.3855,
      "step": 9604
    },
    {
      "epoch": 3.718544328300426,
      "grad_norm": 10.57614803314209,
      "learning_rate": 6.979395190777305e-06,
      "loss": 0.8216,
      "step": 9605
    },
    {
      "epoch": 3.718931475029036,
      "grad_norm": 55.93925094604492,
      "learning_rate": 6.978965027745516e-06,
      "loss": 2.291,
      "step": 9606
    },
    {
      "epoch": 3.719318621757646,
      "grad_norm": 18.156047821044922,
      "learning_rate": 6.978534864713727e-06,
      "loss": 0.7974,
      "step": 9607
    },
    {
      "epoch": 3.7197057684862562,
      "grad_norm": 26.140092849731445,
      "learning_rate": 6.978104701681938e-06,
      "loss": 2.3141,
      "step": 9608
    },
    {
      "epoch": 3.7200929152148663,
      "grad_norm": 23.811189651489258,
      "learning_rate": 6.9776745386501485e-06,
      "loss": 2.0111,
      "step": 9609
    },
    {
      "epoch": 3.7204800619434764,
      "grad_norm": 21.01827049255371,
      "learning_rate": 6.977244375618359e-06,
      "loss": 1.7431,
      "step": 9610
    },
    {
      "epoch": 3.720867208672087,
      "grad_norm": 22.48578643798828,
      "learning_rate": 6.976814212586571e-06,
      "loss": 2.077,
      "step": 9611
    },
    {
      "epoch": 3.721254355400697,
      "grad_norm": 14.184398651123047,
      "learning_rate": 6.976384049554782e-06,
      "loss": 0.759,
      "step": 9612
    },
    {
      "epoch": 3.721641502129307,
      "grad_norm": 13.403592109680176,
      "learning_rate": 6.9759538865229925e-06,
      "loss": 0.9361,
      "step": 9613
    },
    {
      "epoch": 3.722028648857917,
      "grad_norm": 37.47903060913086,
      "learning_rate": 6.975523723491203e-06,
      "loss": 1.7181,
      "step": 9614
    },
    {
      "epoch": 3.722415795586527,
      "grad_norm": 90.89710998535156,
      "learning_rate": 6.975093560459415e-06,
      "loss": 1.2863,
      "step": 9615
    },
    {
      "epoch": 3.7228029423151376,
      "grad_norm": 18.035070419311523,
      "learning_rate": 6.974663397427626e-06,
      "loss": 1.4797,
      "step": 9616
    },
    {
      "epoch": 3.7231900890437477,
      "grad_norm": 9.375057220458984,
      "learning_rate": 6.9742332343958364e-06,
      "loss": 0.4767,
      "step": 9617
    },
    {
      "epoch": 3.7235772357723578,
      "grad_norm": 21.473535537719727,
      "learning_rate": 6.973803071364047e-06,
      "loss": 1.3415,
      "step": 9618
    },
    {
      "epoch": 3.723964382500968,
      "grad_norm": 66.63093566894531,
      "learning_rate": 6.973372908332258e-06,
      "loss": 0.8421,
      "step": 9619
    },
    {
      "epoch": 3.724351529229578,
      "grad_norm": 13.318950653076172,
      "learning_rate": 6.97294274530047e-06,
      "loss": 1.1745,
      "step": 9620
    },
    {
      "epoch": 3.7247386759581884,
      "grad_norm": 19.02277374267578,
      "learning_rate": 6.97251258226868e-06,
      "loss": 1.5169,
      "step": 9621
    },
    {
      "epoch": 3.725125822686798,
      "grad_norm": 22.36050796508789,
      "learning_rate": 6.972082419236892e-06,
      "loss": 1.4652,
      "step": 9622
    },
    {
      "epoch": 3.7255129694154085,
      "grad_norm": 45.6334342956543,
      "learning_rate": 6.971652256205102e-06,
      "loss": 0.616,
      "step": 9623
    },
    {
      "epoch": 3.7259001161440186,
      "grad_norm": 13.972785949707031,
      "learning_rate": 6.971222093173313e-06,
      "loss": 1.0112,
      "step": 9624
    },
    {
      "epoch": 3.7262872628726287,
      "grad_norm": 53.965484619140625,
      "learning_rate": 6.9707919301415235e-06,
      "loss": 1.2338,
      "step": 9625
    },
    {
      "epoch": 3.7266744096012387,
      "grad_norm": 27.411989212036133,
      "learning_rate": 6.970361767109736e-06,
      "loss": 0.9101,
      "step": 9626
    },
    {
      "epoch": 3.727061556329849,
      "grad_norm": 13.36684799194336,
      "learning_rate": 6.969931604077946e-06,
      "loss": 0.812,
      "step": 9627
    },
    {
      "epoch": 3.7274487030584593,
      "grad_norm": 15.834432601928711,
      "learning_rate": 6.969501441046157e-06,
      "loss": 0.9686,
      "step": 9628
    },
    {
      "epoch": 3.7278358497870694,
      "grad_norm": 28.51997184753418,
      "learning_rate": 6.9690712780143675e-06,
      "loss": 1.5807,
      "step": 9629
    },
    {
      "epoch": 3.7282229965156795,
      "grad_norm": 31.81869125366211,
      "learning_rate": 6.96864111498258e-06,
      "loss": 2.0089,
      "step": 9630
    },
    {
      "epoch": 3.7286101432442895,
      "grad_norm": 25.24981689453125,
      "learning_rate": 6.96821095195079e-06,
      "loss": 1.4771,
      "step": 9631
    },
    {
      "epoch": 3.7289972899728996,
      "grad_norm": 12.18314266204834,
      "learning_rate": 6.967780788919001e-06,
      "loss": 0.4597,
      "step": 9632
    },
    {
      "epoch": 3.72938443670151,
      "grad_norm": 41.25823211669922,
      "learning_rate": 6.9673506258872115e-06,
      "loss": 1.2401,
      "step": 9633
    },
    {
      "epoch": 3.72977158343012,
      "grad_norm": 23.589502334594727,
      "learning_rate": 6.966920462855423e-06,
      "loss": 2.4648,
      "step": 9634
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 74.27194213867188,
      "learning_rate": 6.966490299823634e-06,
      "loss": 1.3786,
      "step": 9635
    },
    {
      "epoch": 3.7305458768873403,
      "grad_norm": 12.276688575744629,
      "learning_rate": 6.966060136791845e-06,
      "loss": 0.984,
      "step": 9636
    },
    {
      "epoch": 3.7309330236159504,
      "grad_norm": 51.15107345581055,
      "learning_rate": 6.965629973760055e-06,
      "loss": 1.3809,
      "step": 9637
    },
    {
      "epoch": 3.7313201703445604,
      "grad_norm": 40.18992614746094,
      "learning_rate": 6.965199810728267e-06,
      "loss": 2.4219,
      "step": 9638
    },
    {
      "epoch": 3.7317073170731705,
      "grad_norm": 19.42566680908203,
      "learning_rate": 6.964769647696477e-06,
      "loss": 0.9907,
      "step": 9639
    },
    {
      "epoch": 3.732094463801781,
      "grad_norm": 16.180606842041016,
      "learning_rate": 6.964339484664688e-06,
      "loss": 0.9746,
      "step": 9640
    },
    {
      "epoch": 3.732481610530391,
      "grad_norm": 8.813791275024414,
      "learning_rate": 6.963909321632899e-06,
      "loss": 0.4413,
      "step": 9641
    },
    {
      "epoch": 3.732868757259001,
      "grad_norm": 14.600349426269531,
      "learning_rate": 6.963479158601111e-06,
      "loss": 0.8794,
      "step": 9642
    },
    {
      "epoch": 3.733255903987611,
      "grad_norm": 6.8740010261535645,
      "learning_rate": 6.963048995569321e-06,
      "loss": 0.4132,
      "step": 9643
    },
    {
      "epoch": 3.7336430507162213,
      "grad_norm": 7.8333740234375,
      "learning_rate": 6.962618832537532e-06,
      "loss": 0.4339,
      "step": 9644
    },
    {
      "epoch": 3.7340301974448318,
      "grad_norm": 43.67527389526367,
      "learning_rate": 6.9621886695057425e-06,
      "loss": 2.2174,
      "step": 9645
    },
    {
      "epoch": 3.734417344173442,
      "grad_norm": 39.777523040771484,
      "learning_rate": 6.961758506473955e-06,
      "loss": 2.0914,
      "step": 9646
    },
    {
      "epoch": 3.734804490902052,
      "grad_norm": 37.50914001464844,
      "learning_rate": 6.961328343442165e-06,
      "loss": 1.5095,
      "step": 9647
    },
    {
      "epoch": 3.735191637630662,
      "grad_norm": 7.693500518798828,
      "learning_rate": 6.960898180410376e-06,
      "loss": 0.5378,
      "step": 9648
    },
    {
      "epoch": 3.735578784359272,
      "grad_norm": 29.506484985351562,
      "learning_rate": 6.9604680173785865e-06,
      "loss": 1.8827,
      "step": 9649
    },
    {
      "epoch": 3.7359659310878826,
      "grad_norm": 22.697168350219727,
      "learning_rate": 6.960037854346799e-06,
      "loss": 0.8647,
      "step": 9650
    },
    {
      "epoch": 3.736353077816492,
      "grad_norm": 16.79880142211914,
      "learning_rate": 6.959607691315009e-06,
      "loss": 1.9778,
      "step": 9651
    },
    {
      "epoch": 3.7367402245451027,
      "grad_norm": 48.36222457885742,
      "learning_rate": 6.95917752828322e-06,
      "loss": 1.8794,
      "step": 9652
    },
    {
      "epoch": 3.7371273712737128,
      "grad_norm": 33.14738082885742,
      "learning_rate": 6.9587473652514305e-06,
      "loss": 1.2851,
      "step": 9653
    },
    {
      "epoch": 3.737514518002323,
      "grad_norm": 30.891157150268555,
      "learning_rate": 6.958317202219642e-06,
      "loss": 2.4105,
      "step": 9654
    },
    {
      "epoch": 3.737901664730933,
      "grad_norm": 39.62685775756836,
      "learning_rate": 6.9578870391878524e-06,
      "loss": 0.653,
      "step": 9655
    },
    {
      "epoch": 3.738288811459543,
      "grad_norm": 56.94779968261719,
      "learning_rate": 6.957456876156064e-06,
      "loss": 2.1412,
      "step": 9656
    },
    {
      "epoch": 3.7386759581881535,
      "grad_norm": 23.9306583404541,
      "learning_rate": 6.957026713124274e-06,
      "loss": 2.2702,
      "step": 9657
    },
    {
      "epoch": 3.7390631049167635,
      "grad_norm": 16.515623092651367,
      "learning_rate": 6.956596550092486e-06,
      "loss": 1.9473,
      "step": 9658
    },
    {
      "epoch": 3.7394502516453736,
      "grad_norm": 18.272321701049805,
      "learning_rate": 6.956166387060696e-06,
      "loss": 0.9514,
      "step": 9659
    },
    {
      "epoch": 3.7398373983739837,
      "grad_norm": 46.67940902709961,
      "learning_rate": 6.955736224028907e-06,
      "loss": 3.3115,
      "step": 9660
    },
    {
      "epoch": 3.7402245451025937,
      "grad_norm": 28.91581916809082,
      "learning_rate": 6.955306060997119e-06,
      "loss": 0.9546,
      "step": 9661
    },
    {
      "epoch": 3.7406116918312042,
      "grad_norm": 15.242134094238281,
      "learning_rate": 6.95487589796533e-06,
      "loss": 1.4944,
      "step": 9662
    },
    {
      "epoch": 3.7409988385598143,
      "grad_norm": 27.878955841064453,
      "learning_rate": 6.95444573493354e-06,
      "loss": 2.2494,
      "step": 9663
    },
    {
      "epoch": 3.7413859852884244,
      "grad_norm": 27.869897842407227,
      "learning_rate": 6.954015571901751e-06,
      "loss": 1.6845,
      "step": 9664
    },
    {
      "epoch": 3.7417731320170344,
      "grad_norm": 17.63260269165039,
      "learning_rate": 6.953585408869963e-06,
      "loss": 1.5148,
      "step": 9665
    },
    {
      "epoch": 3.7421602787456445,
      "grad_norm": 7.484941005706787,
      "learning_rate": 6.953155245838174e-06,
      "loss": 0.4763,
      "step": 9666
    },
    {
      "epoch": 3.742547425474255,
      "grad_norm": 14.369401931762695,
      "learning_rate": 6.952725082806384e-06,
      "loss": 1.3012,
      "step": 9667
    },
    {
      "epoch": 3.7429345722028646,
      "grad_norm": 47.03867721557617,
      "learning_rate": 6.952294919774595e-06,
      "loss": 1.6538,
      "step": 9668
    },
    {
      "epoch": 3.743321718931475,
      "grad_norm": 6.754539489746094,
      "learning_rate": 6.951864756742806e-06,
      "loss": 1.0619,
      "step": 9669
    },
    {
      "epoch": 3.743708865660085,
      "grad_norm": 38.82560729980469,
      "learning_rate": 6.951434593711017e-06,
      "loss": 1.4455,
      "step": 9670
    },
    {
      "epoch": 3.7440960123886953,
      "grad_norm": 23.59238052368164,
      "learning_rate": 6.951004430679228e-06,
      "loss": 2.0761,
      "step": 9671
    },
    {
      "epoch": 3.7444831591173053,
      "grad_norm": 31.7410888671875,
      "learning_rate": 6.950574267647439e-06,
      "loss": 2.2697,
      "step": 9672
    },
    {
      "epoch": 3.7448703058459154,
      "grad_norm": 9.903877258300781,
      "learning_rate": 6.95014410461565e-06,
      "loss": 0.7443,
      "step": 9673
    },
    {
      "epoch": 3.745257452574526,
      "grad_norm": 32.30806350708008,
      "learning_rate": 6.949713941583861e-06,
      "loss": 0.9459,
      "step": 9674
    },
    {
      "epoch": 3.745644599303136,
      "grad_norm": 20.875953674316406,
      "learning_rate": 6.9492837785520714e-06,
      "loss": 1.4445,
      "step": 9675
    },
    {
      "epoch": 3.746031746031746,
      "grad_norm": 11.081308364868164,
      "learning_rate": 6.948853615520282e-06,
      "loss": 0.8383,
      "step": 9676
    },
    {
      "epoch": 3.746418892760356,
      "grad_norm": 5.241700649261475,
      "learning_rate": 6.948423452488494e-06,
      "loss": 0.2358,
      "step": 9677
    },
    {
      "epoch": 3.746806039488966,
      "grad_norm": 22.434354782104492,
      "learning_rate": 6.947993289456705e-06,
      "loss": 1.562,
      "step": 9678
    },
    {
      "epoch": 3.7471931862175767,
      "grad_norm": 30.706340789794922,
      "learning_rate": 6.947563126424915e-06,
      "loss": 1.9525,
      "step": 9679
    },
    {
      "epoch": 3.7475803329461868,
      "grad_norm": 27.973125457763672,
      "learning_rate": 6.947132963393126e-06,
      "loss": 0.9632,
      "step": 9680
    },
    {
      "epoch": 3.747967479674797,
      "grad_norm": 18.169675827026367,
      "learning_rate": 6.946702800361338e-06,
      "loss": 1.5653,
      "step": 9681
    },
    {
      "epoch": 3.748354626403407,
      "grad_norm": 29.53782844543457,
      "learning_rate": 6.946272637329549e-06,
      "loss": 2.134,
      "step": 9682
    },
    {
      "epoch": 3.748741773132017,
      "grad_norm": 24.863351821899414,
      "learning_rate": 6.945842474297759e-06,
      "loss": 1.6909,
      "step": 9683
    },
    {
      "epoch": 3.749128919860627,
      "grad_norm": 37.662105560302734,
      "learning_rate": 6.94541231126597e-06,
      "loss": 1.5997,
      "step": 9684
    },
    {
      "epoch": 3.749516066589237,
      "grad_norm": 52.107749938964844,
      "learning_rate": 6.944982148234181e-06,
      "loss": 0.7693,
      "step": 9685
    },
    {
      "epoch": 3.7499032133178476,
      "grad_norm": 7.317073822021484,
      "learning_rate": 6.944551985202393e-06,
      "loss": 1.028,
      "step": 9686
    },
    {
      "epoch": 3.7502903600464577,
      "grad_norm": 55.54295349121094,
      "learning_rate": 6.944121822170603e-06,
      "loss": 1.5978,
      "step": 9687
    },
    {
      "epoch": 3.7506775067750677,
      "grad_norm": 26.21439552307129,
      "learning_rate": 6.943691659138814e-06,
      "loss": 0.7547,
      "step": 9688
    },
    {
      "epoch": 3.751064653503678,
      "grad_norm": 14.607873916625977,
      "learning_rate": 6.943261496107025e-06,
      "loss": 1.3394,
      "step": 9689
    },
    {
      "epoch": 3.751451800232288,
      "grad_norm": 13.185483932495117,
      "learning_rate": 6.942831333075236e-06,
      "loss": 1.0141,
      "step": 9690
    },
    {
      "epoch": 3.7518389469608984,
      "grad_norm": 18.575368881225586,
      "learning_rate": 6.9424011700434465e-06,
      "loss": 1.5657,
      "step": 9691
    },
    {
      "epoch": 3.7522260936895084,
      "grad_norm": 14.889514923095703,
      "learning_rate": 6.941971007011658e-06,
      "loss": 0.9338,
      "step": 9692
    },
    {
      "epoch": 3.7526132404181185,
      "grad_norm": 24.385784149169922,
      "learning_rate": 6.941540843979869e-06,
      "loss": 2.0947,
      "step": 9693
    },
    {
      "epoch": 3.7530003871467286,
      "grad_norm": 17.138946533203125,
      "learning_rate": 6.94111068094808e-06,
      "loss": 0.5775,
      "step": 9694
    },
    {
      "epoch": 3.7533875338753386,
      "grad_norm": 21.89659881591797,
      "learning_rate": 6.9406805179162904e-06,
      "loss": 0.9386,
      "step": 9695
    },
    {
      "epoch": 3.753774680603949,
      "grad_norm": 46.33979034423828,
      "learning_rate": 6.940250354884501e-06,
      "loss": 1.5306,
      "step": 9696
    },
    {
      "epoch": 3.7541618273325588,
      "grad_norm": 55.07791519165039,
      "learning_rate": 6.939820191852713e-06,
      "loss": 1.4541,
      "step": 9697
    },
    {
      "epoch": 3.7545489740611693,
      "grad_norm": 15.291269302368164,
      "learning_rate": 6.939390028820924e-06,
      "loss": 0.9712,
      "step": 9698
    },
    {
      "epoch": 3.7549361207897793,
      "grad_norm": 66.11747741699219,
      "learning_rate": 6.938959865789134e-06,
      "loss": 1.2288,
      "step": 9699
    },
    {
      "epoch": 3.7553232675183894,
      "grad_norm": 45.558258056640625,
      "learning_rate": 6.938529702757345e-06,
      "loss": 2.0295,
      "step": 9700
    },
    {
      "epoch": 3.7557104142469995,
      "grad_norm": 13.116581916809082,
      "learning_rate": 6.938099539725557e-06,
      "loss": 1.1497,
      "step": 9701
    },
    {
      "epoch": 3.7560975609756095,
      "grad_norm": 9.632660865783691,
      "learning_rate": 6.937669376693768e-06,
      "loss": 0.5025,
      "step": 9702
    },
    {
      "epoch": 3.75648470770422,
      "grad_norm": 27.23385238647461,
      "learning_rate": 6.937239213661978e-06,
      "loss": 0.9666,
      "step": 9703
    },
    {
      "epoch": 3.75687185443283,
      "grad_norm": 21.22145652770996,
      "learning_rate": 6.93680905063019e-06,
      "loss": 2.0715,
      "step": 9704
    },
    {
      "epoch": 3.75725900116144,
      "grad_norm": 54.816978454589844,
      "learning_rate": 6.9363788875984e-06,
      "loss": 2.1638,
      "step": 9705
    },
    {
      "epoch": 3.7576461478900502,
      "grad_norm": 27.501672744750977,
      "learning_rate": 6.935948724566611e-06,
      "loss": 0.4786,
      "step": 9706
    },
    {
      "epoch": 3.7580332946186603,
      "grad_norm": 21.95712661743164,
      "learning_rate": 6.935518561534822e-06,
      "loss": 2.6878,
      "step": 9707
    },
    {
      "epoch": 3.758420441347271,
      "grad_norm": 28.327733993530273,
      "learning_rate": 6.935088398503034e-06,
      "loss": 0.89,
      "step": 9708
    },
    {
      "epoch": 3.758807588075881,
      "grad_norm": 36.33998107910156,
      "learning_rate": 6.934658235471244e-06,
      "loss": 2.0331,
      "step": 9709
    },
    {
      "epoch": 3.759194734804491,
      "grad_norm": 31.49016571044922,
      "learning_rate": 6.934228072439455e-06,
      "loss": 1.3144,
      "step": 9710
    },
    {
      "epoch": 3.759581881533101,
      "grad_norm": 32.90321350097656,
      "learning_rate": 6.9337979094076655e-06,
      "loss": 1.3857,
      "step": 9711
    },
    {
      "epoch": 3.759969028261711,
      "grad_norm": 16.356807708740234,
      "learning_rate": 6.933367746375878e-06,
      "loss": 1.2498,
      "step": 9712
    },
    {
      "epoch": 3.7603561749903216,
      "grad_norm": 21.08916664123535,
      "learning_rate": 6.932937583344088e-06,
      "loss": 0.9789,
      "step": 9713
    },
    {
      "epoch": 3.760743321718931,
      "grad_norm": 40.13283157348633,
      "learning_rate": 6.932507420312299e-06,
      "loss": 1.9709,
      "step": 9714
    },
    {
      "epoch": 3.7611304684475417,
      "grad_norm": 7.319893836975098,
      "learning_rate": 6.9320772572805094e-06,
      "loss": 0.4423,
      "step": 9715
    },
    {
      "epoch": 3.761517615176152,
      "grad_norm": 19.54836654663086,
      "learning_rate": 6.931647094248722e-06,
      "loss": 0.5424,
      "step": 9716
    },
    {
      "epoch": 3.761904761904762,
      "grad_norm": 108.2083511352539,
      "learning_rate": 6.931216931216932e-06,
      "loss": 1.8223,
      "step": 9717
    },
    {
      "epoch": 3.762291908633372,
      "grad_norm": 27.194101333618164,
      "learning_rate": 6.930786768185143e-06,
      "loss": 1.6474,
      "step": 9718
    },
    {
      "epoch": 3.762679055361982,
      "grad_norm": 54.92469024658203,
      "learning_rate": 6.930356605153353e-06,
      "loss": 2.4833,
      "step": 9719
    },
    {
      "epoch": 3.7630662020905925,
      "grad_norm": 16.588237762451172,
      "learning_rate": 6.929926442121565e-06,
      "loss": 1.4977,
      "step": 9720
    },
    {
      "epoch": 3.7634533488192026,
      "grad_norm": 25.942890167236328,
      "learning_rate": 6.929496279089775e-06,
      "loss": 0.6073,
      "step": 9721
    },
    {
      "epoch": 3.7638404955478126,
      "grad_norm": 22.88512420654297,
      "learning_rate": 6.929066116057987e-06,
      "loss": 1.1794,
      "step": 9722
    },
    {
      "epoch": 3.7642276422764227,
      "grad_norm": 26.219518661499023,
      "learning_rate": 6.928635953026197e-06,
      "loss": 3.5539,
      "step": 9723
    },
    {
      "epoch": 3.7646147890050328,
      "grad_norm": 12.946660041809082,
      "learning_rate": 6.928205789994409e-06,
      "loss": 0.9735,
      "step": 9724
    },
    {
      "epoch": 3.7650019357336433,
      "grad_norm": 175.62013244628906,
      "learning_rate": 6.927775626962619e-06,
      "loss": 1.2576,
      "step": 9725
    },
    {
      "epoch": 3.7653890824622533,
      "grad_norm": 26.56248664855957,
      "learning_rate": 6.92734546393083e-06,
      "loss": 1.8479,
      "step": 9726
    },
    {
      "epoch": 3.7657762291908634,
      "grad_norm": 19.59735679626465,
      "learning_rate": 6.9269153008990405e-06,
      "loss": 1.6072,
      "step": 9727
    },
    {
      "epoch": 3.7661633759194735,
      "grad_norm": 36.81205749511719,
      "learning_rate": 6.926485137867253e-06,
      "loss": 1.7794,
      "step": 9728
    },
    {
      "epoch": 3.7665505226480835,
      "grad_norm": 30.564292907714844,
      "learning_rate": 6.926054974835463e-06,
      "loss": 1.6258,
      "step": 9729
    },
    {
      "epoch": 3.7669376693766936,
      "grad_norm": 24.32056999206543,
      "learning_rate": 6.925624811803674e-06,
      "loss": 1.1125,
      "step": 9730
    },
    {
      "epoch": 3.7673248161053037,
      "grad_norm": 22.607336044311523,
      "learning_rate": 6.9251946487718845e-06,
      "loss": 1.356,
      "step": 9731
    },
    {
      "epoch": 3.767711962833914,
      "grad_norm": 18.290735244750977,
      "learning_rate": 6.924764485740097e-06,
      "loss": 0.8954,
      "step": 9732
    },
    {
      "epoch": 3.7680991095625243,
      "grad_norm": 21.194561004638672,
      "learning_rate": 6.924334322708307e-06,
      "loss": 1.1824,
      "step": 9733
    },
    {
      "epoch": 3.7684862562911343,
      "grad_norm": 33.48410415649414,
      "learning_rate": 6.923904159676518e-06,
      "loss": 2.2639,
      "step": 9734
    },
    {
      "epoch": 3.7688734030197444,
      "grad_norm": 29.53826141357422,
      "learning_rate": 6.9234739966447284e-06,
      "loss": 1.6854,
      "step": 9735
    },
    {
      "epoch": 3.7692605497483544,
      "grad_norm": 20.402366638183594,
      "learning_rate": 6.92304383361294e-06,
      "loss": 0.7461,
      "step": 9736
    },
    {
      "epoch": 3.769647696476965,
      "grad_norm": 46.49155044555664,
      "learning_rate": 6.922613670581151e-06,
      "loss": 2.4299,
      "step": 9737
    },
    {
      "epoch": 3.770034843205575,
      "grad_norm": 13.191988945007324,
      "learning_rate": 6.922183507549362e-06,
      "loss": 0.7863,
      "step": 9738
    },
    {
      "epoch": 3.770421989934185,
      "grad_norm": 14.755011558532715,
      "learning_rate": 6.921753344517572e-06,
      "loss": 0.4884,
      "step": 9739
    },
    {
      "epoch": 3.770809136662795,
      "grad_norm": 32.45353317260742,
      "learning_rate": 6.921323181485784e-06,
      "loss": 1.766,
      "step": 9740
    },
    {
      "epoch": 3.7711962833914052,
      "grad_norm": 24.301076889038086,
      "learning_rate": 6.920893018453994e-06,
      "loss": 0.775,
      "step": 9741
    },
    {
      "epoch": 3.7715834301200157,
      "grad_norm": 17.647686004638672,
      "learning_rate": 6.920462855422205e-06,
      "loss": 0.8014,
      "step": 9742
    },
    {
      "epoch": 3.7719705768486254,
      "grad_norm": 19.532573699951172,
      "learning_rate": 6.920032692390417e-06,
      "loss": 1.6206,
      "step": 9743
    },
    {
      "epoch": 3.772357723577236,
      "grad_norm": 30.8013858795166,
      "learning_rate": 6.919602529358628e-06,
      "loss": 1.2566,
      "step": 9744
    },
    {
      "epoch": 3.772744870305846,
      "grad_norm": 16.14752960205078,
      "learning_rate": 6.919172366326838e-06,
      "loss": 1.1105,
      "step": 9745
    },
    {
      "epoch": 3.773132017034456,
      "grad_norm": 36.1931037902832,
      "learning_rate": 6.918742203295049e-06,
      "loss": 0.7276,
      "step": 9746
    },
    {
      "epoch": 3.773519163763066,
      "grad_norm": 27.43754005432129,
      "learning_rate": 6.918312040263261e-06,
      "loss": 1.5267,
      "step": 9747
    },
    {
      "epoch": 3.773906310491676,
      "grad_norm": 22.049150466918945,
      "learning_rate": 6.917881877231472e-06,
      "loss": 1.4804,
      "step": 9748
    },
    {
      "epoch": 3.7742934572202866,
      "grad_norm": 38.23740768432617,
      "learning_rate": 6.917451714199682e-06,
      "loss": 2.3149,
      "step": 9749
    },
    {
      "epoch": 3.7746806039488967,
      "grad_norm": 47.7227668762207,
      "learning_rate": 6.917021551167893e-06,
      "loss": 1.1108,
      "step": 9750
    },
    {
      "epoch": 3.7750677506775068,
      "grad_norm": 24.45648765563965,
      "learning_rate": 6.916591388136104e-06,
      "loss": 1.4435,
      "step": 9751
    },
    {
      "epoch": 3.775454897406117,
      "grad_norm": 21.284000396728516,
      "learning_rate": 6.916161225104316e-06,
      "loss": 1.0924,
      "step": 9752
    },
    {
      "epoch": 3.775842044134727,
      "grad_norm": 73.73975372314453,
      "learning_rate": 6.915731062072526e-06,
      "loss": 1.0701,
      "step": 9753
    },
    {
      "epoch": 3.7762291908633374,
      "grad_norm": 18.820838928222656,
      "learning_rate": 6.915300899040737e-06,
      "loss": 0.577,
      "step": 9754
    },
    {
      "epoch": 3.7766163375919475,
      "grad_norm": 18.58391761779785,
      "learning_rate": 6.914870736008948e-06,
      "loss": 0.6501,
      "step": 9755
    },
    {
      "epoch": 3.7770034843205575,
      "grad_norm": 13.862425804138184,
      "learning_rate": 6.914440572977159e-06,
      "loss": 0.9499,
      "step": 9756
    },
    {
      "epoch": 3.7773906310491676,
      "grad_norm": 11.543389320373535,
      "learning_rate": 6.914010409945369e-06,
      "loss": 0.7628,
      "step": 9757
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 39.69993591308594,
      "learning_rate": 6.913580246913581e-06,
      "loss": 0.8453,
      "step": 9758
    },
    {
      "epoch": 3.778164924506388,
      "grad_norm": 12.218006134033203,
      "learning_rate": 6.913150083881792e-06,
      "loss": 0.7329,
      "step": 9759
    },
    {
      "epoch": 3.778552071234998,
      "grad_norm": 27.219301223754883,
      "learning_rate": 6.912719920850003e-06,
      "loss": 1.2815,
      "step": 9760
    },
    {
      "epoch": 3.7789392179636083,
      "grad_norm": 31.6162109375,
      "learning_rate": 6.912289757818213e-06,
      "loss": 1.0919,
      "step": 9761
    },
    {
      "epoch": 3.7793263646922184,
      "grad_norm": 40.50305938720703,
      "learning_rate": 6.911859594786424e-06,
      "loss": 1.1107,
      "step": 9762
    },
    {
      "epoch": 3.7797135114208285,
      "grad_norm": 7.574797630310059,
      "learning_rate": 6.911429431754636e-06,
      "loss": 1.0481,
      "step": 9763
    },
    {
      "epoch": 3.7801006581494385,
      "grad_norm": 24.565011978149414,
      "learning_rate": 6.910999268722847e-06,
      "loss": 0.8347,
      "step": 9764
    },
    {
      "epoch": 3.7804878048780486,
      "grad_norm": 7.991811752319336,
      "learning_rate": 6.910569105691057e-06,
      "loss": 0.4924,
      "step": 9765
    },
    {
      "epoch": 3.780874951606659,
      "grad_norm": 15.720172882080078,
      "learning_rate": 6.910138942659268e-06,
      "loss": 1.0633,
      "step": 9766
    },
    {
      "epoch": 3.781262098335269,
      "grad_norm": 26.885690689086914,
      "learning_rate": 6.90970877962748e-06,
      "loss": 2.366,
      "step": 9767
    },
    {
      "epoch": 3.7816492450638792,
      "grad_norm": 25.434690475463867,
      "learning_rate": 6.909278616595691e-06,
      "loss": 1.682,
      "step": 9768
    },
    {
      "epoch": 3.7820363917924893,
      "grad_norm": 10.580967903137207,
      "learning_rate": 6.908848453563901e-06,
      "loss": 0.3502,
      "step": 9769
    },
    {
      "epoch": 3.7824235385210994,
      "grad_norm": 31.624488830566406,
      "learning_rate": 6.908418290532112e-06,
      "loss": 1.2466,
      "step": 9770
    },
    {
      "epoch": 3.78281068524971,
      "grad_norm": 18.44713020324707,
      "learning_rate": 6.907988127500323e-06,
      "loss": 0.7168,
      "step": 9771
    },
    {
      "epoch": 3.78319783197832,
      "grad_norm": 57.09148025512695,
      "learning_rate": 6.907557964468534e-06,
      "loss": 1.2821,
      "step": 9772
    },
    {
      "epoch": 3.78358497870693,
      "grad_norm": 28.476938247680664,
      "learning_rate": 6.907127801436745e-06,
      "loss": 1.8895,
      "step": 9773
    },
    {
      "epoch": 3.78397212543554,
      "grad_norm": 22.493358612060547,
      "learning_rate": 6.906697638404956e-06,
      "loss": 1.984,
      "step": 9774
    },
    {
      "epoch": 3.78435927216415,
      "grad_norm": 45.838802337646484,
      "learning_rate": 6.906267475373167e-06,
      "loss": 1.2185,
      "step": 9775
    },
    {
      "epoch": 3.78474641889276,
      "grad_norm": 7.579496383666992,
      "learning_rate": 6.905837312341378e-06,
      "loss": 0.4291,
      "step": 9776
    },
    {
      "epoch": 3.7851335656213703,
      "grad_norm": 28.366308212280273,
      "learning_rate": 6.905407149309588e-06,
      "loss": 1.7578,
      "step": 9777
    },
    {
      "epoch": 3.7855207123499808,
      "grad_norm": 11.79002571105957,
      "learning_rate": 6.904976986277799e-06,
      "loss": 0.7622,
      "step": 9778
    },
    {
      "epoch": 3.785907859078591,
      "grad_norm": 26.683944702148438,
      "learning_rate": 6.904546823246011e-06,
      "loss": 1.7071,
      "step": 9779
    },
    {
      "epoch": 3.786295005807201,
      "grad_norm": 15.78892707824707,
      "learning_rate": 6.904116660214222e-06,
      "loss": 1.2398,
      "step": 9780
    },
    {
      "epoch": 3.786682152535811,
      "grad_norm": 27.73400115966797,
      "learning_rate": 6.903686497182432e-06,
      "loss": 1.4209,
      "step": 9781
    },
    {
      "epoch": 3.787069299264421,
      "grad_norm": 23.713159561157227,
      "learning_rate": 6.903256334150643e-06,
      "loss": 2.2619,
      "step": 9782
    },
    {
      "epoch": 3.7874564459930316,
      "grad_norm": 16.982664108276367,
      "learning_rate": 6.902826171118855e-06,
      "loss": 1.6652,
      "step": 9783
    },
    {
      "epoch": 3.7878435927216416,
      "grad_norm": 27.279760360717773,
      "learning_rate": 6.902396008087066e-06,
      "loss": 2.2267,
      "step": 9784
    },
    {
      "epoch": 3.7882307394502517,
      "grad_norm": 10.286520004272461,
      "learning_rate": 6.901965845055276e-06,
      "loss": 1.3164,
      "step": 9785
    },
    {
      "epoch": 3.7886178861788617,
      "grad_norm": 8.31246566772461,
      "learning_rate": 6.901535682023488e-06,
      "loss": 1.1141,
      "step": 9786
    },
    {
      "epoch": 3.789005032907472,
      "grad_norm": 23.533184051513672,
      "learning_rate": 6.901105518991698e-06,
      "loss": 1.1216,
      "step": 9787
    },
    {
      "epoch": 3.7893921796360823,
      "grad_norm": 8.810465812683105,
      "learning_rate": 6.90067535595991e-06,
      "loss": 0.4951,
      "step": 9788
    },
    {
      "epoch": 3.789779326364692,
      "grad_norm": 21.649883270263672,
      "learning_rate": 6.90024519292812e-06,
      "loss": 1.8688,
      "step": 9789
    },
    {
      "epoch": 3.7901664730933025,
      "grad_norm": 14.228108406066895,
      "learning_rate": 6.899815029896332e-06,
      "loss": 0.8466,
      "step": 9790
    },
    {
      "epoch": 3.7905536198219125,
      "grad_norm": 66.45243835449219,
      "learning_rate": 6.899384866864542e-06,
      "loss": 2.1695,
      "step": 9791
    },
    {
      "epoch": 3.7909407665505226,
      "grad_norm": 30.91569709777832,
      "learning_rate": 6.898954703832753e-06,
      "loss": 1.6161,
      "step": 9792
    },
    {
      "epoch": 3.7913279132791327,
      "grad_norm": 18.848783493041992,
      "learning_rate": 6.8985245408009634e-06,
      "loss": 1.3717,
      "step": 9793
    },
    {
      "epoch": 3.7917150600077427,
      "grad_norm": 27.14215087890625,
      "learning_rate": 6.898094377769176e-06,
      "loss": 1.2405,
      "step": 9794
    },
    {
      "epoch": 3.7921022067363532,
      "grad_norm": 16.43352508544922,
      "learning_rate": 6.897664214737386e-06,
      "loss": 1.7859,
      "step": 9795
    },
    {
      "epoch": 3.7924893534649633,
      "grad_norm": 7.836326599121094,
      "learning_rate": 6.897234051705597e-06,
      "loss": 0.4352,
      "step": 9796
    },
    {
      "epoch": 3.7928765001935734,
      "grad_norm": 32.36685562133789,
      "learning_rate": 6.896803888673807e-06,
      "loss": 1.5822,
      "step": 9797
    },
    {
      "epoch": 3.7932636469221834,
      "grad_norm": 40.51343536376953,
      "learning_rate": 6.89637372564202e-06,
      "loss": 1.7897,
      "step": 9798
    },
    {
      "epoch": 3.7936507936507935,
      "grad_norm": 49.591453552246094,
      "learning_rate": 6.89594356261023e-06,
      "loss": 0.9799,
      "step": 9799
    },
    {
      "epoch": 3.794037940379404,
      "grad_norm": 19.184032440185547,
      "learning_rate": 6.895513399578441e-06,
      "loss": 1.1042,
      "step": 9800
    },
    {
      "epoch": 3.794425087108014,
      "grad_norm": 20.689281463623047,
      "learning_rate": 6.895083236546651e-06,
      "loss": 1.4367,
      "step": 9801
    },
    {
      "epoch": 3.794812233836624,
      "grad_norm": 22.978038787841797,
      "learning_rate": 6.894653073514863e-06,
      "loss": 0.8754,
      "step": 9802
    },
    {
      "epoch": 3.795199380565234,
      "grad_norm": 16.276411056518555,
      "learning_rate": 6.894222910483074e-06,
      "loss": 0.9227,
      "step": 9803
    },
    {
      "epoch": 3.7955865272938443,
      "grad_norm": 40.2856559753418,
      "learning_rate": 6.893792747451285e-06,
      "loss": 1.1285,
      "step": 9804
    },
    {
      "epoch": 3.795973674022455,
      "grad_norm": 29.734134674072266,
      "learning_rate": 6.893362584419495e-06,
      "loss": 2.3209,
      "step": 9805
    },
    {
      "epoch": 3.7963608207510644,
      "grad_norm": 10.138577461242676,
      "learning_rate": 6.892932421387707e-06,
      "loss": 0.607,
      "step": 9806
    },
    {
      "epoch": 3.796747967479675,
      "grad_norm": 22.853010177612305,
      "learning_rate": 6.892502258355917e-06,
      "loss": 1.0872,
      "step": 9807
    },
    {
      "epoch": 3.797135114208285,
      "grad_norm": 13.76916790008545,
      "learning_rate": 6.892072095324128e-06,
      "loss": 0.8626,
      "step": 9808
    },
    {
      "epoch": 3.797522260936895,
      "grad_norm": 35.376922607421875,
      "learning_rate": 6.891641932292339e-06,
      "loss": 2.166,
      "step": 9809
    },
    {
      "epoch": 3.797909407665505,
      "grad_norm": 36.38115692138672,
      "learning_rate": 6.891211769260551e-06,
      "loss": 1.5807,
      "step": 9810
    },
    {
      "epoch": 3.798296554394115,
      "grad_norm": 8.866246223449707,
      "learning_rate": 6.890781606228761e-06,
      "loss": 0.5521,
      "step": 9811
    },
    {
      "epoch": 3.7986837011227257,
      "grad_norm": 48.7234001159668,
      "learning_rate": 6.890351443196972e-06,
      "loss": 1.5538,
      "step": 9812
    },
    {
      "epoch": 3.7990708478513358,
      "grad_norm": 37.591773986816406,
      "learning_rate": 6.8899212801651824e-06,
      "loss": 1.0266,
      "step": 9813
    },
    {
      "epoch": 3.799457994579946,
      "grad_norm": 12.939916610717773,
      "learning_rate": 6.889491117133395e-06,
      "loss": 0.7188,
      "step": 9814
    },
    {
      "epoch": 3.799845141308556,
      "grad_norm": 37.71722412109375,
      "learning_rate": 6.889060954101605e-06,
      "loss": 0.8364,
      "step": 9815
    },
    {
      "epoch": 3.800232288037166,
      "grad_norm": 23.30316162109375,
      "learning_rate": 6.888630791069816e-06,
      "loss": 1.9702,
      "step": 9816
    },
    {
      "epoch": 3.8006194347657765,
      "grad_norm": 21.37515640258789,
      "learning_rate": 6.888200628038026e-06,
      "loss": 0.8272,
      "step": 9817
    },
    {
      "epoch": 3.8010065814943865,
      "grad_norm": 110.61865997314453,
      "learning_rate": 6.887770465006239e-06,
      "loss": 1.1383,
      "step": 9818
    },
    {
      "epoch": 3.8013937282229966,
      "grad_norm": 41.722999572753906,
      "learning_rate": 6.887340301974449e-06,
      "loss": 0.8965,
      "step": 9819
    },
    {
      "epoch": 3.8017808749516067,
      "grad_norm": 84.84902954101562,
      "learning_rate": 6.88691013894266e-06,
      "loss": 1.6161,
      "step": 9820
    },
    {
      "epoch": 3.8021680216802167,
      "grad_norm": 10.954370498657227,
      "learning_rate": 6.88647997591087e-06,
      "loss": 1.1492,
      "step": 9821
    },
    {
      "epoch": 3.802555168408827,
      "grad_norm": 6.493521690368652,
      "learning_rate": 6.886049812879082e-06,
      "loss": 0.1233,
      "step": 9822
    },
    {
      "epoch": 3.802942315137437,
      "grad_norm": 36.00950622558594,
      "learning_rate": 6.885619649847292e-06,
      "loss": 1.5122,
      "step": 9823
    },
    {
      "epoch": 3.8033294618660474,
      "grad_norm": 15.972622871398926,
      "learning_rate": 6.885189486815504e-06,
      "loss": 0.5158,
      "step": 9824
    },
    {
      "epoch": 3.8037166085946574,
      "grad_norm": 6.007077217102051,
      "learning_rate": 6.884759323783715e-06,
      "loss": 0.2068,
      "step": 9825
    },
    {
      "epoch": 3.8041037553232675,
      "grad_norm": 23.024065017700195,
      "learning_rate": 6.884329160751926e-06,
      "loss": 1.2662,
      "step": 9826
    },
    {
      "epoch": 3.8044909020518776,
      "grad_norm": 3.653899669647217,
      "learning_rate": 6.883898997720136e-06,
      "loss": 0.1709,
      "step": 9827
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 21.044309616088867,
      "learning_rate": 6.883468834688347e-06,
      "loss": 1.4624,
      "step": 9828
    },
    {
      "epoch": 3.805265195509098,
      "grad_norm": 109.8953628540039,
      "learning_rate": 6.883038671656559e-06,
      "loss": 1.2058,
      "step": 9829
    },
    {
      "epoch": 3.805652342237708,
      "grad_norm": 21.418407440185547,
      "learning_rate": 6.88260850862477e-06,
      "loss": 1.2292,
      "step": 9830
    },
    {
      "epoch": 3.8060394889663183,
      "grad_norm": 22.423233032226562,
      "learning_rate": 6.88217834559298e-06,
      "loss": 1.1844,
      "step": 9831
    },
    {
      "epoch": 3.8064266356949283,
      "grad_norm": 12.640218734741211,
      "learning_rate": 6.881748182561191e-06,
      "loss": 0.511,
      "step": 9832
    },
    {
      "epoch": 3.8068137824235384,
      "grad_norm": 21.310546875,
      "learning_rate": 6.881318019529403e-06,
      "loss": 0.9659,
      "step": 9833
    },
    {
      "epoch": 3.807200929152149,
      "grad_norm": 23.165517807006836,
      "learning_rate": 6.880887856497614e-06,
      "loss": 0.8636,
      "step": 9834
    },
    {
      "epoch": 3.8075880758807585,
      "grad_norm": 19.50795555114746,
      "learning_rate": 6.880457693465824e-06,
      "loss": 1.3581,
      "step": 9835
    },
    {
      "epoch": 3.807975222609369,
      "grad_norm": 29.86993408203125,
      "learning_rate": 6.880027530434035e-06,
      "loss": 1.4238,
      "step": 9836
    },
    {
      "epoch": 3.808362369337979,
      "grad_norm": 30.623010635375977,
      "learning_rate": 6.879597367402246e-06,
      "loss": 1.5219,
      "step": 9837
    },
    {
      "epoch": 3.808749516066589,
      "grad_norm": 12.508309364318848,
      "learning_rate": 6.879167204370457e-06,
      "loss": 1.0966,
      "step": 9838
    },
    {
      "epoch": 3.8091366627951992,
      "grad_norm": 20.123748779296875,
      "learning_rate": 6.878737041338668e-06,
      "loss": 0.8832,
      "step": 9839
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 53.47480773925781,
      "learning_rate": 6.878306878306879e-06,
      "loss": 2.4897,
      "step": 9840
    },
    {
      "epoch": 3.80991095625242,
      "grad_norm": 23.115079879760742,
      "learning_rate": 6.87787671527509e-06,
      "loss": 1.1991,
      "step": 9841
    },
    {
      "epoch": 3.81029810298103,
      "grad_norm": 16.325666427612305,
      "learning_rate": 6.877446552243301e-06,
      "loss": 1.7294,
      "step": 9842
    },
    {
      "epoch": 3.81068524970964,
      "grad_norm": 11.676737785339355,
      "learning_rate": 6.877016389211511e-06,
      "loss": 0.393,
      "step": 9843
    },
    {
      "epoch": 3.81107239643825,
      "grad_norm": 19.15926742553711,
      "learning_rate": 6.876586226179722e-06,
      "loss": 1.2992,
      "step": 9844
    },
    {
      "epoch": 3.81145954316686,
      "grad_norm": 17.873779296875,
      "learning_rate": 6.876156063147934e-06,
      "loss": 0.2867,
      "step": 9845
    },
    {
      "epoch": 3.8118466898954706,
      "grad_norm": 53.430686950683594,
      "learning_rate": 6.875725900116145e-06,
      "loss": 1.5132,
      "step": 9846
    },
    {
      "epoch": 3.8122338366240807,
      "grad_norm": 25.748062133789062,
      "learning_rate": 6.875295737084355e-06,
      "loss": 0.9656,
      "step": 9847
    },
    {
      "epoch": 3.8126209833526907,
      "grad_norm": 8.03615665435791,
      "learning_rate": 6.874865574052566e-06,
      "loss": 0.4561,
      "step": 9848
    },
    {
      "epoch": 3.813008130081301,
      "grad_norm": 8.820985794067383,
      "learning_rate": 6.874435411020778e-06,
      "loss": 0.4924,
      "step": 9849
    },
    {
      "epoch": 3.813395276809911,
      "grad_norm": 18.43063735961914,
      "learning_rate": 6.874005247988989e-06,
      "loss": 1.3326,
      "step": 9850
    },
    {
      "epoch": 3.8137824235385214,
      "grad_norm": 25.280906677246094,
      "learning_rate": 6.873575084957199e-06,
      "loss": 1.5531,
      "step": 9851
    },
    {
      "epoch": 3.814169570267131,
      "grad_norm": 16.797698974609375,
      "learning_rate": 6.87314492192541e-06,
      "loss": 0.568,
      "step": 9852
    },
    {
      "epoch": 3.8145567169957415,
      "grad_norm": 16.134052276611328,
      "learning_rate": 6.872714758893621e-06,
      "loss": 0.9832,
      "step": 9853
    },
    {
      "epoch": 3.8149438637243516,
      "grad_norm": 12.773838996887207,
      "learning_rate": 6.872284595861833e-06,
      "loss": 0.3633,
      "step": 9854
    },
    {
      "epoch": 3.8153310104529616,
      "grad_norm": 43.656646728515625,
      "learning_rate": 6.871854432830043e-06,
      "loss": 0.7634,
      "step": 9855
    },
    {
      "epoch": 3.8157181571815717,
      "grad_norm": 64.38231658935547,
      "learning_rate": 6.871424269798254e-06,
      "loss": 1.4285,
      "step": 9856
    },
    {
      "epoch": 3.8161053039101818,
      "grad_norm": 8.101090431213379,
      "learning_rate": 6.870994106766465e-06,
      "loss": 0.4496,
      "step": 9857
    },
    {
      "epoch": 3.8164924506387923,
      "grad_norm": 35.43473434448242,
      "learning_rate": 6.870563943734676e-06,
      "loss": 1.8187,
      "step": 9858
    },
    {
      "epoch": 3.8168795973674023,
      "grad_norm": 35.94731140136719,
      "learning_rate": 6.870133780702886e-06,
      "loss": 1.7301,
      "step": 9859
    },
    {
      "epoch": 3.8172667440960124,
      "grad_norm": 27.244089126586914,
      "learning_rate": 6.869703617671098e-06,
      "loss": 1.0041,
      "step": 9860
    },
    {
      "epoch": 3.8176538908246225,
      "grad_norm": 54.50332260131836,
      "learning_rate": 6.869273454639309e-06,
      "loss": 1.0816,
      "step": 9861
    },
    {
      "epoch": 3.8180410375532325,
      "grad_norm": 16.26651954650879,
      "learning_rate": 6.86884329160752e-06,
      "loss": 0.8919,
      "step": 9862
    },
    {
      "epoch": 3.818428184281843,
      "grad_norm": 16.727680206298828,
      "learning_rate": 6.86841312857573e-06,
      "loss": 0.9649,
      "step": 9863
    },
    {
      "epoch": 3.818815331010453,
      "grad_norm": 21.893962860107422,
      "learning_rate": 6.867982965543941e-06,
      "loss": 2.8374,
      "step": 9864
    },
    {
      "epoch": 3.819202477739063,
      "grad_norm": 29.627450942993164,
      "learning_rate": 6.867552802512153e-06,
      "loss": 1.73,
      "step": 9865
    },
    {
      "epoch": 3.8195896244676733,
      "grad_norm": 58.30207443237305,
      "learning_rate": 6.867122639480364e-06,
      "loss": 1.6093,
      "step": 9866
    },
    {
      "epoch": 3.8199767711962833,
      "grad_norm": 28.057703018188477,
      "learning_rate": 6.866692476448574e-06,
      "loss": 2.1192,
      "step": 9867
    },
    {
      "epoch": 3.8203639179248934,
      "grad_norm": 57.47820281982422,
      "learning_rate": 6.866262313416786e-06,
      "loss": 1.553,
      "step": 9868
    },
    {
      "epoch": 3.8207510646535034,
      "grad_norm": 12.944856643676758,
      "learning_rate": 6.865832150384997e-06,
      "loss": 0.7594,
      "step": 9869
    },
    {
      "epoch": 3.821138211382114,
      "grad_norm": 90.04463195800781,
      "learning_rate": 6.865401987353208e-06,
      "loss": 2.0009,
      "step": 9870
    },
    {
      "epoch": 3.821525358110724,
      "grad_norm": 16.99788475036621,
      "learning_rate": 6.864971824321418e-06,
      "loss": 0.9242,
      "step": 9871
    },
    {
      "epoch": 3.821912504839334,
      "grad_norm": 44.359249114990234,
      "learning_rate": 6.86454166128963e-06,
      "loss": 2.1236,
      "step": 9872
    },
    {
      "epoch": 3.822299651567944,
      "grad_norm": 82.79621887207031,
      "learning_rate": 6.86411149825784e-06,
      "loss": 1.983,
      "step": 9873
    },
    {
      "epoch": 3.8226867982965542,
      "grad_norm": 20.87806510925293,
      "learning_rate": 6.863681335226051e-06,
      "loss": 0.9448,
      "step": 9874
    },
    {
      "epoch": 3.8230739450251647,
      "grad_norm": 1.994499683380127,
      "learning_rate": 6.863251172194262e-06,
      "loss": 0.0606,
      "step": 9875
    },
    {
      "epoch": 3.823461091753775,
      "grad_norm": 37.284706115722656,
      "learning_rate": 6.862821009162474e-06,
      "loss": 1.21,
      "step": 9876
    },
    {
      "epoch": 3.823848238482385,
      "grad_norm": 24.761844635009766,
      "learning_rate": 6.862390846130684e-06,
      "loss": 2.4312,
      "step": 9877
    },
    {
      "epoch": 3.824235385210995,
      "grad_norm": 15.6874361038208,
      "learning_rate": 6.861960683098895e-06,
      "loss": 0.8757,
      "step": 9878
    },
    {
      "epoch": 3.824622531939605,
      "grad_norm": 26.729127883911133,
      "learning_rate": 6.861530520067105e-06,
      "loss": 1.879,
      "step": 9879
    },
    {
      "epoch": 3.8250096786682155,
      "grad_norm": 26.375877380371094,
      "learning_rate": 6.861100357035318e-06,
      "loss": 2.2743,
      "step": 9880
    },
    {
      "epoch": 3.825396825396825,
      "grad_norm": 28.434099197387695,
      "learning_rate": 6.860670194003528e-06,
      "loss": 0.9833,
      "step": 9881
    },
    {
      "epoch": 3.8257839721254356,
      "grad_norm": 26.47661018371582,
      "learning_rate": 6.860240030971739e-06,
      "loss": 1.3533,
      "step": 9882
    },
    {
      "epoch": 3.8261711188540457,
      "grad_norm": 19.915239334106445,
      "learning_rate": 6.859809867939949e-06,
      "loss": 1.2344,
      "step": 9883
    },
    {
      "epoch": 3.8265582655826558,
      "grad_norm": 49.488128662109375,
      "learning_rate": 6.859379704908161e-06,
      "loss": 0.8919,
      "step": 9884
    },
    {
      "epoch": 3.826945412311266,
      "grad_norm": 76.68689727783203,
      "learning_rate": 6.858949541876372e-06,
      "loss": 1.1079,
      "step": 9885
    },
    {
      "epoch": 3.827332559039876,
      "grad_norm": 17.91691017150879,
      "learning_rate": 6.858519378844583e-06,
      "loss": 1.1104,
      "step": 9886
    },
    {
      "epoch": 3.8277197057684864,
      "grad_norm": 36.77508544921875,
      "learning_rate": 6.858089215812793e-06,
      "loss": 1.3704,
      "step": 9887
    },
    {
      "epoch": 3.8281068524970965,
      "grad_norm": 15.923147201538086,
      "learning_rate": 6.857659052781005e-06,
      "loss": 0.9293,
      "step": 9888
    },
    {
      "epoch": 3.8284939992257065,
      "grad_norm": 39.94101333618164,
      "learning_rate": 6.857228889749215e-06,
      "loss": 1.4703,
      "step": 9889
    },
    {
      "epoch": 3.8288811459543166,
      "grad_norm": 22.50558853149414,
      "learning_rate": 6.856798726717427e-06,
      "loss": 2.081,
      "step": 9890
    },
    {
      "epoch": 3.8292682926829267,
      "grad_norm": 59.92259979248047,
      "learning_rate": 6.856368563685637e-06,
      "loss": 1.2042,
      "step": 9891
    },
    {
      "epoch": 3.829655439411537,
      "grad_norm": 62.643524169921875,
      "learning_rate": 6.855938400653849e-06,
      "loss": 1.2943,
      "step": 9892
    },
    {
      "epoch": 3.8300425861401473,
      "grad_norm": 20.136993408203125,
      "learning_rate": 6.855508237622059e-06,
      "loss": 1.0924,
      "step": 9893
    },
    {
      "epoch": 3.8304297328687573,
      "grad_norm": 30.21108055114746,
      "learning_rate": 6.85507807459027e-06,
      "loss": 0.9451,
      "step": 9894
    },
    {
      "epoch": 3.8308168795973674,
      "grad_norm": 15.693825721740723,
      "learning_rate": 6.85464791155848e-06,
      "loss": 0.7368,
      "step": 9895
    },
    {
      "epoch": 3.8312040263259775,
      "grad_norm": 15.598639488220215,
      "learning_rate": 6.854217748526693e-06,
      "loss": 0.7906,
      "step": 9896
    },
    {
      "epoch": 3.831591173054588,
      "grad_norm": 20.06453514099121,
      "learning_rate": 6.853787585494903e-06,
      "loss": 1.2604,
      "step": 9897
    },
    {
      "epoch": 3.8319783197831976,
      "grad_norm": 49.78335952758789,
      "learning_rate": 6.853357422463114e-06,
      "loss": 2.0519,
      "step": 9898
    },
    {
      "epoch": 3.832365466511808,
      "grad_norm": 13.818788528442383,
      "learning_rate": 6.852927259431324e-06,
      "loss": 0.5587,
      "step": 9899
    },
    {
      "epoch": 3.832752613240418,
      "grad_norm": 32.56792449951172,
      "learning_rate": 6.852497096399537e-06,
      "loss": 2.2134,
      "step": 9900
    },
    {
      "epoch": 3.8331397599690282,
      "grad_norm": 55.69251251220703,
      "learning_rate": 6.852066933367747e-06,
      "loss": 0.8502,
      "step": 9901
    },
    {
      "epoch": 3.8335269066976383,
      "grad_norm": 26.8396053314209,
      "learning_rate": 6.851636770335958e-06,
      "loss": 0.483,
      "step": 9902
    },
    {
      "epoch": 3.8339140534262484,
      "grad_norm": 22.36964988708496,
      "learning_rate": 6.851206607304168e-06,
      "loss": 1.388,
      "step": 9903
    },
    {
      "epoch": 3.834301200154859,
      "grad_norm": 29.973535537719727,
      "learning_rate": 6.85077644427238e-06,
      "loss": 1.4111,
      "step": 9904
    },
    {
      "epoch": 3.834688346883469,
      "grad_norm": 22.656044006347656,
      "learning_rate": 6.850346281240591e-06,
      "loss": 2.5042,
      "step": 9905
    },
    {
      "epoch": 3.835075493612079,
      "grad_norm": 39.7213020324707,
      "learning_rate": 6.849916118208802e-06,
      "loss": 2.196,
      "step": 9906
    },
    {
      "epoch": 3.835462640340689,
      "grad_norm": 25.578441619873047,
      "learning_rate": 6.849485955177013e-06,
      "loss": 1.2564,
      "step": 9907
    },
    {
      "epoch": 3.835849787069299,
      "grad_norm": 10.438150405883789,
      "learning_rate": 6.849055792145224e-06,
      "loss": 0.7179,
      "step": 9908
    },
    {
      "epoch": 3.8362369337979096,
      "grad_norm": 27.846696853637695,
      "learning_rate": 6.848625629113434e-06,
      "loss": 1.8501,
      "step": 9909
    },
    {
      "epoch": 3.8366240805265197,
      "grad_norm": 26.81241798400879,
      "learning_rate": 6.848195466081645e-06,
      "loss": 1.9173,
      "step": 9910
    },
    {
      "epoch": 3.8370112272551298,
      "grad_norm": 29.594684600830078,
      "learning_rate": 6.847765303049857e-06,
      "loss": 1.771,
      "step": 9911
    },
    {
      "epoch": 3.83739837398374,
      "grad_norm": 20.276533126831055,
      "learning_rate": 6.847335140018068e-06,
      "loss": 0.8329,
      "step": 9912
    },
    {
      "epoch": 3.83778552071235,
      "grad_norm": 42.75206756591797,
      "learning_rate": 6.846904976986278e-06,
      "loss": 0.8689,
      "step": 9913
    },
    {
      "epoch": 3.83817266744096,
      "grad_norm": 8.101572036743164,
      "learning_rate": 6.846474813954489e-06,
      "loss": 0.4036,
      "step": 9914
    },
    {
      "epoch": 3.83855981416957,
      "grad_norm": 17.433866500854492,
      "learning_rate": 6.846044650922701e-06,
      "loss": 1.3179,
      "step": 9915
    },
    {
      "epoch": 3.8389469608981805,
      "grad_norm": 39.392578125,
      "learning_rate": 6.845614487890912e-06,
      "loss": 1.6461,
      "step": 9916
    },
    {
      "epoch": 3.8393341076267906,
      "grad_norm": 6.759511470794678,
      "learning_rate": 6.845184324859122e-06,
      "loss": 0.3698,
      "step": 9917
    },
    {
      "epoch": 3.8397212543554007,
      "grad_norm": 24.49129295349121,
      "learning_rate": 6.844754161827333e-06,
      "loss": 0.4767,
      "step": 9918
    },
    {
      "epoch": 3.8401084010840107,
      "grad_norm": 22.88874053955078,
      "learning_rate": 6.844323998795544e-06,
      "loss": 0.7819,
      "step": 9919
    },
    {
      "epoch": 3.840495547812621,
      "grad_norm": 11.589701652526855,
      "learning_rate": 6.843893835763755e-06,
      "loss": 0.7075,
      "step": 9920
    },
    {
      "epoch": 3.8408826945412313,
      "grad_norm": 25.00664520263672,
      "learning_rate": 6.843463672731966e-06,
      "loss": 1.2821,
      "step": 9921
    },
    {
      "epoch": 3.8412698412698414,
      "grad_norm": 12.97911548614502,
      "learning_rate": 6.843033509700177e-06,
      "loss": 1.2342,
      "step": 9922
    },
    {
      "epoch": 3.8416569879984515,
      "grad_norm": 24.986038208007812,
      "learning_rate": 6.842603346668388e-06,
      "loss": 1.6924,
      "step": 9923
    },
    {
      "epoch": 3.8420441347270615,
      "grad_norm": 34.64047622680664,
      "learning_rate": 6.842173183636599e-06,
      "loss": 0.8127,
      "step": 9924
    },
    {
      "epoch": 3.8424312814556716,
      "grad_norm": 20.18329620361328,
      "learning_rate": 6.841743020604809e-06,
      "loss": 0.8936,
      "step": 9925
    },
    {
      "epoch": 3.842818428184282,
      "grad_norm": 14.401901245117188,
      "learning_rate": 6.841312857573021e-06,
      "loss": 0.7436,
      "step": 9926
    },
    {
      "epoch": 3.8432055749128917,
      "grad_norm": 42.852054595947266,
      "learning_rate": 6.840882694541232e-06,
      "loss": 2.4793,
      "step": 9927
    },
    {
      "epoch": 3.8435927216415022,
      "grad_norm": 29.561506271362305,
      "learning_rate": 6.840452531509443e-06,
      "loss": 0.7896,
      "step": 9928
    },
    {
      "epoch": 3.8439798683701123,
      "grad_norm": 23.599565505981445,
      "learning_rate": 6.840022368477653e-06,
      "loss": 0.7725,
      "step": 9929
    },
    {
      "epoch": 3.8443670150987224,
      "grad_norm": 14.160633087158203,
      "learning_rate": 6.839592205445864e-06,
      "loss": 0.865,
      "step": 9930
    },
    {
      "epoch": 3.8447541618273324,
      "grad_norm": 20.088878631591797,
      "learning_rate": 6.839162042414076e-06,
      "loss": 0.9208,
      "step": 9931
    },
    {
      "epoch": 3.8451413085559425,
      "grad_norm": 11.806282997131348,
      "learning_rate": 6.838731879382287e-06,
      "loss": 0.8997,
      "step": 9932
    },
    {
      "epoch": 3.845528455284553,
      "grad_norm": 47.37636184692383,
      "learning_rate": 6.838301716350497e-06,
      "loss": 1.6604,
      "step": 9933
    },
    {
      "epoch": 3.845915602013163,
      "grad_norm": 12.094966888427734,
      "learning_rate": 6.837871553318708e-06,
      "loss": 1.3541,
      "step": 9934
    },
    {
      "epoch": 3.846302748741773,
      "grad_norm": 28.912553787231445,
      "learning_rate": 6.837441390286919e-06,
      "loss": 1.3941,
      "step": 9935
    },
    {
      "epoch": 3.846689895470383,
      "grad_norm": 8.524565696716309,
      "learning_rate": 6.837011227255131e-06,
      "loss": 0.4187,
      "step": 9936
    },
    {
      "epoch": 3.8470770421989933,
      "grad_norm": 1.9886529445648193,
      "learning_rate": 6.836581064223341e-06,
      "loss": 0.0621,
      "step": 9937
    },
    {
      "epoch": 3.847464188927604,
      "grad_norm": 50.902774810791016,
      "learning_rate": 6.836150901191552e-06,
      "loss": 1.044,
      "step": 9938
    },
    {
      "epoch": 3.847851335656214,
      "grad_norm": 25.89568519592285,
      "learning_rate": 6.835720738159763e-06,
      "loss": 1.5439,
      "step": 9939
    },
    {
      "epoch": 3.848238482384824,
      "grad_norm": 33.05809783935547,
      "learning_rate": 6.835290575127974e-06,
      "loss": 0.8736,
      "step": 9940
    },
    {
      "epoch": 3.848625629113434,
      "grad_norm": 33.49675369262695,
      "learning_rate": 6.834860412096185e-06,
      "loss": 1.4528,
      "step": 9941
    },
    {
      "epoch": 3.849012775842044,
      "grad_norm": 77.81382751464844,
      "learning_rate": 6.834430249064396e-06,
      "loss": 0.902,
      "step": 9942
    },
    {
      "epoch": 3.8493999225706546,
      "grad_norm": 20.572635650634766,
      "learning_rate": 6.834000086032607e-06,
      "loss": 1.5478,
      "step": 9943
    },
    {
      "epoch": 3.849787069299264,
      "grad_norm": 48.5692024230957,
      "learning_rate": 6.833569923000818e-06,
      "loss": 2.1932,
      "step": 9944
    },
    {
      "epoch": 3.8501742160278747,
      "grad_norm": 11.699117660522461,
      "learning_rate": 6.833139759969028e-06,
      "loss": 0.4765,
      "step": 9945
    },
    {
      "epoch": 3.8505613627564848,
      "grad_norm": 35.26179504394531,
      "learning_rate": 6.832709596937239e-06,
      "loss": 5.0513,
      "step": 9946
    },
    {
      "epoch": 3.850948509485095,
      "grad_norm": 51.83255386352539,
      "learning_rate": 6.832279433905451e-06,
      "loss": 1.8656,
      "step": 9947
    },
    {
      "epoch": 3.851335656213705,
      "grad_norm": 17.93193244934082,
      "learning_rate": 6.831849270873662e-06,
      "loss": 1.3771,
      "step": 9948
    },
    {
      "epoch": 3.851722802942315,
      "grad_norm": 21.672508239746094,
      "learning_rate": 6.831419107841872e-06,
      "loss": 0.6603,
      "step": 9949
    },
    {
      "epoch": 3.8521099496709255,
      "grad_norm": 32.04610824584961,
      "learning_rate": 6.830988944810084e-06,
      "loss": 1.7872,
      "step": 9950
    },
    {
      "epoch": 3.8524970963995355,
      "grad_norm": 39.80182647705078,
      "learning_rate": 6.830558781778295e-06,
      "loss": 1.4821,
      "step": 9951
    },
    {
      "epoch": 3.8528842431281456,
      "grad_norm": 36.45370101928711,
      "learning_rate": 6.830128618746506e-06,
      "loss": 0.6555,
      "step": 9952
    },
    {
      "epoch": 3.8532713898567557,
      "grad_norm": 14.176802635192871,
      "learning_rate": 6.829698455714716e-06,
      "loss": 0.9045,
      "step": 9953
    },
    {
      "epoch": 3.8536585365853657,
      "grad_norm": 13.61426067352295,
      "learning_rate": 6.829268292682928e-06,
      "loss": 1.4203,
      "step": 9954
    },
    {
      "epoch": 3.8540456833139762,
      "grad_norm": 13.444063186645508,
      "learning_rate": 6.828838129651138e-06,
      "loss": 1.3799,
      "step": 9955
    },
    {
      "epoch": 3.8544328300425863,
      "grad_norm": 31.40384292602539,
      "learning_rate": 6.828407966619349e-06,
      "loss": 1.5796,
      "step": 9956
    },
    {
      "epoch": 3.8548199767711964,
      "grad_norm": 18.803539276123047,
      "learning_rate": 6.82797780358756e-06,
      "loss": 1.3236,
      "step": 9957
    },
    {
      "epoch": 3.8552071234998064,
      "grad_norm": 24.22345542907715,
      "learning_rate": 6.827547640555772e-06,
      "loss": 1.4815,
      "step": 9958
    },
    {
      "epoch": 3.8555942702284165,
      "grad_norm": 10.667207717895508,
      "learning_rate": 6.827117477523982e-06,
      "loss": 0.4825,
      "step": 9959
    },
    {
      "epoch": 3.8559814169570266,
      "grad_norm": 22.58033561706543,
      "learning_rate": 6.826687314492193e-06,
      "loss": 1.9859,
      "step": 9960
    },
    {
      "epoch": 3.8563685636856366,
      "grad_norm": 65.8617172241211,
      "learning_rate": 6.826257151460403e-06,
      "loss": 1.5415,
      "step": 9961
    },
    {
      "epoch": 3.856755710414247,
      "grad_norm": 14.962545394897461,
      "learning_rate": 6.825826988428616e-06,
      "loss": 0.778,
      "step": 9962
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 28.59212303161621,
      "learning_rate": 6.825396825396826e-06,
      "loss": 2.1191,
      "step": 9963
    },
    {
      "epoch": 3.8575300038714673,
      "grad_norm": 17.688695907592773,
      "learning_rate": 6.824966662365037e-06,
      "loss": 2.3862,
      "step": 9964
    },
    {
      "epoch": 3.8579171506000773,
      "grad_norm": 11.714265823364258,
      "learning_rate": 6.824536499333247e-06,
      "loss": 0.784,
      "step": 9965
    },
    {
      "epoch": 3.8583042973286874,
      "grad_norm": 55.558837890625,
      "learning_rate": 6.82410633630146e-06,
      "loss": 1.673,
      "step": 9966
    },
    {
      "epoch": 3.858691444057298,
      "grad_norm": 13.118988990783691,
      "learning_rate": 6.82367617326967e-06,
      "loss": 0.6691,
      "step": 9967
    },
    {
      "epoch": 3.859078590785908,
      "grad_norm": 26.48818588256836,
      "learning_rate": 6.823246010237881e-06,
      "loss": 0.9175,
      "step": 9968
    },
    {
      "epoch": 3.859465737514518,
      "grad_norm": 34.5223274230957,
      "learning_rate": 6.822815847206091e-06,
      "loss": 1.5304,
      "step": 9969
    },
    {
      "epoch": 3.859852884243128,
      "grad_norm": 24.816747665405273,
      "learning_rate": 6.822385684174303e-06,
      "loss": 0.8427,
      "step": 9970
    },
    {
      "epoch": 3.860240030971738,
      "grad_norm": 25.137895584106445,
      "learning_rate": 6.821955521142513e-06,
      "loss": 1.8158,
      "step": 9971
    },
    {
      "epoch": 3.8606271777003487,
      "grad_norm": 18.879772186279297,
      "learning_rate": 6.821525358110725e-06,
      "loss": 0.8978,
      "step": 9972
    },
    {
      "epoch": 3.8610143244289583,
      "grad_norm": 22.673259735107422,
      "learning_rate": 6.821095195078935e-06,
      "loss": 1.6304,
      "step": 9973
    },
    {
      "epoch": 3.861401471157569,
      "grad_norm": 26.67819595336914,
      "learning_rate": 6.820665032047147e-06,
      "loss": 1.1612,
      "step": 9974
    },
    {
      "epoch": 3.861788617886179,
      "grad_norm": 29.65604019165039,
      "learning_rate": 6.820234869015357e-06,
      "loss": 1.1398,
      "step": 9975
    },
    {
      "epoch": 3.862175764614789,
      "grad_norm": 6.67780876159668,
      "learning_rate": 6.819804705983568e-06,
      "loss": 0.3474,
      "step": 9976
    },
    {
      "epoch": 3.862562911343399,
      "grad_norm": 15.713053703308105,
      "learning_rate": 6.819374542951779e-06,
      "loss": 1.0723,
      "step": 9977
    },
    {
      "epoch": 3.862950058072009,
      "grad_norm": 30.061248779296875,
      "learning_rate": 6.818944379919991e-06,
      "loss": 1.7023,
      "step": 9978
    },
    {
      "epoch": 3.8633372048006196,
      "grad_norm": 36.287925720214844,
      "learning_rate": 6.818514216888201e-06,
      "loss": 0.7393,
      "step": 9979
    },
    {
      "epoch": 3.8637243515292297,
      "grad_norm": 27.17439842224121,
      "learning_rate": 6.818084053856412e-06,
      "loss": 2.5109,
      "step": 9980
    },
    {
      "epoch": 3.8641114982578397,
      "grad_norm": 52.34415054321289,
      "learning_rate": 6.817653890824622e-06,
      "loss": 0.5721,
      "step": 9981
    },
    {
      "epoch": 3.86449864498645,
      "grad_norm": 20.017370223999023,
      "learning_rate": 6.817223727792835e-06,
      "loss": 1.2421,
      "step": 9982
    },
    {
      "epoch": 3.86488579171506,
      "grad_norm": 9.999661445617676,
      "learning_rate": 6.816793564761045e-06,
      "loss": 0.6611,
      "step": 9983
    },
    {
      "epoch": 3.8652729384436704,
      "grad_norm": 32.90719985961914,
      "learning_rate": 6.816363401729256e-06,
      "loss": 1.5356,
      "step": 9984
    },
    {
      "epoch": 3.8656600851722804,
      "grad_norm": 15.559035301208496,
      "learning_rate": 6.815933238697466e-06,
      "loss": 1.0467,
      "step": 9985
    },
    {
      "epoch": 3.8660472319008905,
      "grad_norm": 49.84255599975586,
      "learning_rate": 6.815503075665678e-06,
      "loss": 1.1767,
      "step": 9986
    },
    {
      "epoch": 3.8664343786295006,
      "grad_norm": 32.418121337890625,
      "learning_rate": 6.815072912633889e-06,
      "loss": 1.4179,
      "step": 9987
    },
    {
      "epoch": 3.8668215253581106,
      "grad_norm": 50.68314743041992,
      "learning_rate": 6.8146427496021e-06,
      "loss": 2.0713,
      "step": 9988
    },
    {
      "epoch": 3.867208672086721,
      "grad_norm": 47.51252746582031,
      "learning_rate": 6.81421258657031e-06,
      "loss": 0.7972,
      "step": 9989
    },
    {
      "epoch": 3.8675958188153308,
      "grad_norm": 24.6202449798584,
      "learning_rate": 6.813782423538522e-06,
      "loss": 1.1646,
      "step": 9990
    },
    {
      "epoch": 3.8679829655439413,
      "grad_norm": 18.017822265625,
      "learning_rate": 6.813352260506732e-06,
      "loss": 0.5508,
      "step": 9991
    },
    {
      "epoch": 3.8683701122725513,
      "grad_norm": 7.449109077453613,
      "learning_rate": 6.812922097474943e-06,
      "loss": 0.3902,
      "step": 9992
    },
    {
      "epoch": 3.8687572590011614,
      "grad_norm": 18.07179832458496,
      "learning_rate": 6.812491934443155e-06,
      "loss": 0.9881,
      "step": 9993
    },
    {
      "epoch": 3.8691444057297715,
      "grad_norm": 14.482453346252441,
      "learning_rate": 6.812061771411366e-06,
      "loss": 0.5612,
      "step": 9994
    },
    {
      "epoch": 3.8695315524583815,
      "grad_norm": 22.382314682006836,
      "learning_rate": 6.811631608379576e-06,
      "loss": 1.0402,
      "step": 9995
    },
    {
      "epoch": 3.869918699186992,
      "grad_norm": 26.576095581054688,
      "learning_rate": 6.811201445347787e-06,
      "loss": 0.556,
      "step": 9996
    },
    {
      "epoch": 3.870305845915602,
      "grad_norm": 93.2678451538086,
      "learning_rate": 6.810771282315999e-06,
      "loss": 1.2003,
      "step": 9997
    },
    {
      "epoch": 3.870692992644212,
      "grad_norm": 23.046998977661133,
      "learning_rate": 6.81034111928421e-06,
      "loss": 0.8202,
      "step": 9998
    },
    {
      "epoch": 3.8710801393728222,
      "grad_norm": 17.585277557373047,
      "learning_rate": 6.80991095625242e-06,
      "loss": 0.952,
      "step": 9999
    },
    {
      "epoch": 3.8714672861014323,
      "grad_norm": 34.08951950073242,
      "learning_rate": 6.809480793220631e-06,
      "loss": 1.8679,
      "step": 10000
    },
    {
      "epoch": 3.871854432830043,
      "grad_norm": 27.12589454650879,
      "learning_rate": 6.809050630188842e-06,
      "loss": 0.9799,
      "step": 10001
    },
    {
      "epoch": 3.872241579558653,
      "grad_norm": 33.978580474853516,
      "learning_rate": 6.808620467157054e-06,
      "loss": 0.4879,
      "step": 10002
    },
    {
      "epoch": 3.872628726287263,
      "grad_norm": 17.084936141967773,
      "learning_rate": 6.808190304125264e-06,
      "loss": 1.1958,
      "step": 10003
    },
    {
      "epoch": 3.873015873015873,
      "grad_norm": 24.14446258544922,
      "learning_rate": 6.807760141093475e-06,
      "loss": 0.7726,
      "step": 10004
    },
    {
      "epoch": 3.873403019744483,
      "grad_norm": 3.9201292991638184,
      "learning_rate": 6.807329978061686e-06,
      "loss": 0.1914,
      "step": 10005
    },
    {
      "epoch": 3.873790166473093,
      "grad_norm": 9.333969116210938,
      "learning_rate": 6.806899815029897e-06,
      "loss": 0.6725,
      "step": 10006
    },
    {
      "epoch": 3.874177313201703,
      "grad_norm": 34.569305419921875,
      "learning_rate": 6.806469651998107e-06,
      "loss": 0.6723,
      "step": 10007
    },
    {
      "epoch": 3.8745644599303137,
      "grad_norm": 34.38875961303711,
      "learning_rate": 6.806039488966319e-06,
      "loss": 1.5368,
      "step": 10008
    },
    {
      "epoch": 3.874951606658924,
      "grad_norm": 32.25977325439453,
      "learning_rate": 6.80560932593453e-06,
      "loss": 1.3678,
      "step": 10009
    },
    {
      "epoch": 3.875338753387534,
      "grad_norm": 26.217191696166992,
      "learning_rate": 6.805179162902741e-06,
      "loss": 0.6279,
      "step": 10010
    },
    {
      "epoch": 3.875725900116144,
      "grad_norm": 37.53472900390625,
      "learning_rate": 6.804748999870951e-06,
      "loss": 1.4165,
      "step": 10011
    },
    {
      "epoch": 3.876113046844754,
      "grad_norm": 12.955328941345215,
      "learning_rate": 6.804318836839162e-06,
      "loss": 0.7665,
      "step": 10012
    },
    {
      "epoch": 3.8765001935733645,
      "grad_norm": 30.69896125793457,
      "learning_rate": 6.803888673807374e-06,
      "loss": 1.5543,
      "step": 10013
    },
    {
      "epoch": 3.8768873403019746,
      "grad_norm": 12.588662147521973,
      "learning_rate": 6.803458510775585e-06,
      "loss": 0.695,
      "step": 10014
    },
    {
      "epoch": 3.8772744870305846,
      "grad_norm": 26.835289001464844,
      "learning_rate": 6.803028347743795e-06,
      "loss": 2.3348,
      "step": 10015
    },
    {
      "epoch": 3.8776616337591947,
      "grad_norm": 15.099007606506348,
      "learning_rate": 6.802598184712006e-06,
      "loss": 0.7153,
      "step": 10016
    },
    {
      "epoch": 3.8780487804878048,
      "grad_norm": 20.290971755981445,
      "learning_rate": 6.802168021680218e-06,
      "loss": 1.491,
      "step": 10017
    },
    {
      "epoch": 3.8784359272164153,
      "grad_norm": 22.575763702392578,
      "learning_rate": 6.801737858648429e-06,
      "loss": 1.1515,
      "step": 10018
    },
    {
      "epoch": 3.878823073945025,
      "grad_norm": 22.928482055664062,
      "learning_rate": 6.801307695616639e-06,
      "loss": 0.9735,
      "step": 10019
    },
    {
      "epoch": 3.8792102206736354,
      "grad_norm": 5.046524524688721,
      "learning_rate": 6.80087753258485e-06,
      "loss": 0.1896,
      "step": 10020
    },
    {
      "epoch": 3.8795973674022455,
      "grad_norm": 109.8013687133789,
      "learning_rate": 6.800447369553061e-06,
      "loss": 1.9987,
      "step": 10021
    },
    {
      "epoch": 3.8799845141308555,
      "grad_norm": 13.133204460144043,
      "learning_rate": 6.800017206521272e-06,
      "loss": 0.4171,
      "step": 10022
    },
    {
      "epoch": 3.8803716608594656,
      "grad_norm": 25.68781280517578,
      "learning_rate": 6.799587043489483e-06,
      "loss": 1.3546,
      "step": 10023
    },
    {
      "epoch": 3.8807588075880757,
      "grad_norm": 4.087897777557373,
      "learning_rate": 6.799156880457694e-06,
      "loss": 0.1809,
      "step": 10024
    },
    {
      "epoch": 3.881145954316686,
      "grad_norm": 20.851459503173828,
      "learning_rate": 6.798726717425905e-06,
      "loss": 2.0609,
      "step": 10025
    },
    {
      "epoch": 3.8815331010452963,
      "grad_norm": 15.084975242614746,
      "learning_rate": 6.798296554394116e-06,
      "loss": 1.2441,
      "step": 10026
    },
    {
      "epoch": 3.8819202477739063,
      "grad_norm": 20.299962997436523,
      "learning_rate": 6.797866391362326e-06,
      "loss": 1.3187,
      "step": 10027
    },
    {
      "epoch": 3.8823073945025164,
      "grad_norm": 26.599246978759766,
      "learning_rate": 6.797436228330537e-06,
      "loss": 1.015,
      "step": 10028
    },
    {
      "epoch": 3.8826945412311265,
      "grad_norm": 11.337212562561035,
      "learning_rate": 6.797006065298749e-06,
      "loss": 0.5211,
      "step": 10029
    },
    {
      "epoch": 3.883081687959737,
      "grad_norm": 30.51346206665039,
      "learning_rate": 6.79657590226696e-06,
      "loss": 1.723,
      "step": 10030
    },
    {
      "epoch": 3.883468834688347,
      "grad_norm": 13.479353904724121,
      "learning_rate": 6.79614573923517e-06,
      "loss": 0.4955,
      "step": 10031
    },
    {
      "epoch": 3.883855981416957,
      "grad_norm": 45.52840042114258,
      "learning_rate": 6.7957155762033825e-06,
      "loss": 1.6997,
      "step": 10032
    },
    {
      "epoch": 3.884243128145567,
      "grad_norm": 5.827211856842041,
      "learning_rate": 6.795285413171593e-06,
      "loss": 0.4284,
      "step": 10033
    },
    {
      "epoch": 3.8846302748741772,
      "grad_norm": 56.32426452636719,
      "learning_rate": 6.794855250139804e-06,
      "loss": 0.9137,
      "step": 10034
    },
    {
      "epoch": 3.8850174216027873,
      "grad_norm": 54.67045593261719,
      "learning_rate": 6.794425087108014e-06,
      "loss": 0.9871,
      "step": 10035
    },
    {
      "epoch": 3.8854045683313974,
      "grad_norm": 14.429759979248047,
      "learning_rate": 6.793994924076226e-06,
      "loss": 1.7397,
      "step": 10036
    },
    {
      "epoch": 3.885791715060008,
      "grad_norm": 28.389535903930664,
      "learning_rate": 6.793564761044436e-06,
      "loss": 1.3734,
      "step": 10037
    },
    {
      "epoch": 3.886178861788618,
      "grad_norm": 11.943266868591309,
      "learning_rate": 6.793134598012648e-06,
      "loss": 1.2651,
      "step": 10038
    },
    {
      "epoch": 3.886566008517228,
      "grad_norm": 23.383066177368164,
      "learning_rate": 6.792704434980858e-06,
      "loss": 0.7865,
      "step": 10039
    },
    {
      "epoch": 3.886953155245838,
      "grad_norm": 31.261093139648438,
      "learning_rate": 6.79227427194907e-06,
      "loss": 1.4688,
      "step": 10040
    },
    {
      "epoch": 3.887340301974448,
      "grad_norm": 35.0487060546875,
      "learning_rate": 6.79184410891728e-06,
      "loss": 2.0095,
      "step": 10041
    },
    {
      "epoch": 3.8877274487030586,
      "grad_norm": 25.433231353759766,
      "learning_rate": 6.791413945885491e-06,
      "loss": 1.1287,
      "step": 10042
    },
    {
      "epoch": 3.8881145954316687,
      "grad_norm": 20.23075294494629,
      "learning_rate": 6.790983782853701e-06,
      "loss": 1.652,
      "step": 10043
    },
    {
      "epoch": 3.8885017421602788,
      "grad_norm": 15.090448379516602,
      "learning_rate": 6.790553619821914e-06,
      "loss": 1.1951,
      "step": 10044
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 9.677526473999023,
      "learning_rate": 6.790123456790124e-06,
      "loss": 0.7172,
      "step": 10045
    },
    {
      "epoch": 3.889276035617499,
      "grad_norm": 12.719767570495605,
      "learning_rate": 6.789693293758335e-06,
      "loss": 1.0106,
      "step": 10046
    },
    {
      "epoch": 3.8896631823461094,
      "grad_norm": 12.824867248535156,
      "learning_rate": 6.789263130726545e-06,
      "loss": 0.6846,
      "step": 10047
    },
    {
      "epoch": 3.8900503290747195,
      "grad_norm": 12.936781883239746,
      "learning_rate": 6.7888329676947576e-06,
      "loss": 0.724,
      "step": 10048
    },
    {
      "epoch": 3.8904374758033295,
      "grad_norm": 27.59371566772461,
      "learning_rate": 6.788402804662968e-06,
      "loss": 2.0024,
      "step": 10049
    },
    {
      "epoch": 3.8908246225319396,
      "grad_norm": 19.03912353515625,
      "learning_rate": 6.787972641631179e-06,
      "loss": 1.6131,
      "step": 10050
    },
    {
      "epoch": 3.8912117692605497,
      "grad_norm": 11.278669357299805,
      "learning_rate": 6.787542478599389e-06,
      "loss": 0.4435,
      "step": 10051
    },
    {
      "epoch": 3.8915989159891597,
      "grad_norm": 28.75346565246582,
      "learning_rate": 6.787112315567601e-06,
      "loss": 1.9007,
      "step": 10052
    },
    {
      "epoch": 3.89198606271777,
      "grad_norm": 38.47142028808594,
      "learning_rate": 6.786682152535812e-06,
      "loss": 1.4522,
      "step": 10053
    },
    {
      "epoch": 3.8923732094463803,
      "grad_norm": 20.6784725189209,
      "learning_rate": 6.786251989504023e-06,
      "loss": 0.6557,
      "step": 10054
    },
    {
      "epoch": 3.8927603561749904,
      "grad_norm": 42.39875793457031,
      "learning_rate": 6.785821826472233e-06,
      "loss": 1.0364,
      "step": 10055
    },
    {
      "epoch": 3.8931475029036005,
      "grad_norm": 29.8577823638916,
      "learning_rate": 6.785391663440445e-06,
      "loss": 1.5201,
      "step": 10056
    },
    {
      "epoch": 3.8935346496322105,
      "grad_norm": 11.974794387817383,
      "learning_rate": 6.784961500408655e-06,
      "loss": 1.0698,
      "step": 10057
    },
    {
      "epoch": 3.8939217963608206,
      "grad_norm": 6.093240261077881,
      "learning_rate": 6.784531337376866e-06,
      "loss": 0.3158,
      "step": 10058
    },
    {
      "epoch": 3.894308943089431,
      "grad_norm": 11.730005264282227,
      "learning_rate": 6.784101174345077e-06,
      "loss": 0.4479,
      "step": 10059
    },
    {
      "epoch": 3.894696089818041,
      "grad_norm": 60.092803955078125,
      "learning_rate": 6.783671011313289e-06,
      "loss": 2.1638,
      "step": 10060
    },
    {
      "epoch": 3.8950832365466512,
      "grad_norm": 33.37146759033203,
      "learning_rate": 6.783240848281499e-06,
      "loss": 1.2258,
      "step": 10061
    },
    {
      "epoch": 3.8954703832752613,
      "grad_norm": 38.82744598388672,
      "learning_rate": 6.78281068524971e-06,
      "loss": 1.5364,
      "step": 10062
    },
    {
      "epoch": 3.8958575300038714,
      "grad_norm": 7.743875026702881,
      "learning_rate": 6.78238052221792e-06,
      "loss": 0.5202,
      "step": 10063
    },
    {
      "epoch": 3.896244676732482,
      "grad_norm": 25.176536560058594,
      "learning_rate": 6.781950359186133e-06,
      "loss": 1.5073,
      "step": 10064
    },
    {
      "epoch": 3.8966318234610915,
      "grad_norm": 41.13811492919922,
      "learning_rate": 6.781520196154343e-06,
      "loss": 1.8029,
      "step": 10065
    },
    {
      "epoch": 3.897018970189702,
      "grad_norm": 65.57515716552734,
      "learning_rate": 6.781090033122554e-06,
      "loss": 1.2545,
      "step": 10066
    },
    {
      "epoch": 3.897406116918312,
      "grad_norm": 35.261558532714844,
      "learning_rate": 6.780659870090764e-06,
      "loss": 1.8255,
      "step": 10067
    },
    {
      "epoch": 3.897793263646922,
      "grad_norm": 26.36850929260254,
      "learning_rate": 6.7802297070589766e-06,
      "loss": 0.818,
      "step": 10068
    },
    {
      "epoch": 3.898180410375532,
      "grad_norm": 35.852230072021484,
      "learning_rate": 6.779799544027187e-06,
      "loss": 0.866,
      "step": 10069
    },
    {
      "epoch": 3.8985675571041423,
      "grad_norm": 74.8326644897461,
      "learning_rate": 6.779369380995398e-06,
      "loss": 1.5386,
      "step": 10070
    },
    {
      "epoch": 3.8989547038327528,
      "grad_norm": 17.11307716369629,
      "learning_rate": 6.778939217963608e-06,
      "loss": 0.9155,
      "step": 10071
    },
    {
      "epoch": 3.899341850561363,
      "grad_norm": 29.87363052368164,
      "learning_rate": 6.77850905493182e-06,
      "loss": 3.2846,
      "step": 10072
    },
    {
      "epoch": 3.899728997289973,
      "grad_norm": 30.201770782470703,
      "learning_rate": 6.77807889190003e-06,
      "loss": 2.056,
      "step": 10073
    },
    {
      "epoch": 3.900116144018583,
      "grad_norm": 28.661043167114258,
      "learning_rate": 6.777648728868242e-06,
      "loss": 0.7875,
      "step": 10074
    },
    {
      "epoch": 3.900503290747193,
      "grad_norm": 12.269210815429688,
      "learning_rate": 6.777218565836453e-06,
      "loss": 0.6021,
      "step": 10075
    },
    {
      "epoch": 3.9008904374758036,
      "grad_norm": 33.27635192871094,
      "learning_rate": 6.776788402804664e-06,
      "loss": 1.4542,
      "step": 10076
    },
    {
      "epoch": 3.9012775842044136,
      "grad_norm": 6.379932880401611,
      "learning_rate": 6.776358239772874e-06,
      "loss": 0.4064,
      "step": 10077
    },
    {
      "epoch": 3.9016647309330237,
      "grad_norm": 49.01123809814453,
      "learning_rate": 6.775928076741085e-06,
      "loss": 1.0464,
      "step": 10078
    },
    {
      "epoch": 3.9020518776616337,
      "grad_norm": 63.2116813659668,
      "learning_rate": 6.775497913709297e-06,
      "loss": 1.0567,
      "step": 10079
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 37.2143669128418,
      "learning_rate": 6.775067750677508e-06,
      "loss": 1.524,
      "step": 10080
    },
    {
      "epoch": 3.902826171118854,
      "grad_norm": 34.51946258544922,
      "learning_rate": 6.774637587645718e-06,
      "loss": 0.9408,
      "step": 10081
    },
    {
      "epoch": 3.903213317847464,
      "grad_norm": 126.61283874511719,
      "learning_rate": 6.774207424613929e-06,
      "loss": 2.3785,
      "step": 10082
    },
    {
      "epoch": 3.9036004645760745,
      "grad_norm": 43.984527587890625,
      "learning_rate": 6.773777261582141e-06,
      "loss": 1.572,
      "step": 10083
    },
    {
      "epoch": 3.9039876113046845,
      "grad_norm": 41.795101165771484,
      "learning_rate": 6.773347098550352e-06,
      "loss": 0.7327,
      "step": 10084
    },
    {
      "epoch": 3.9043747580332946,
      "grad_norm": 24.172231674194336,
      "learning_rate": 6.772916935518562e-06,
      "loss": 2.2746,
      "step": 10085
    },
    {
      "epoch": 3.9047619047619047,
      "grad_norm": 11.163641929626465,
      "learning_rate": 6.772486772486773e-06,
      "loss": 0.8332,
      "step": 10086
    },
    {
      "epoch": 3.9051490514905147,
      "grad_norm": 36.42124557495117,
      "learning_rate": 6.772056609454984e-06,
      "loss": 2.5832,
      "step": 10087
    },
    {
      "epoch": 3.9055361982191252,
      "grad_norm": 22.589841842651367,
      "learning_rate": 6.771626446423195e-06,
      "loss": 1.862,
      "step": 10088
    },
    {
      "epoch": 3.9059233449477353,
      "grad_norm": 30.597612380981445,
      "learning_rate": 6.771196283391406e-06,
      "loss": 1.1607,
      "step": 10089
    },
    {
      "epoch": 3.9063104916763454,
      "grad_norm": 16.93421173095703,
      "learning_rate": 6.770766120359617e-06,
      "loss": 1.2037,
      "step": 10090
    },
    {
      "epoch": 3.9066976384049554,
      "grad_norm": 31.394729614257812,
      "learning_rate": 6.770335957327828e-06,
      "loss": 1.4961,
      "step": 10091
    },
    {
      "epoch": 3.9070847851335655,
      "grad_norm": 10.225064277648926,
      "learning_rate": 6.769905794296039e-06,
      "loss": 0.7301,
      "step": 10092
    },
    {
      "epoch": 3.907471931862176,
      "grad_norm": 52.22305679321289,
      "learning_rate": 6.769475631264249e-06,
      "loss": 2.665,
      "step": 10093
    },
    {
      "epoch": 3.907859078590786,
      "grad_norm": 20.28154182434082,
      "learning_rate": 6.76904546823246e-06,
      "loss": 1.1696,
      "step": 10094
    },
    {
      "epoch": 3.908246225319396,
      "grad_norm": 9.246828079223633,
      "learning_rate": 6.768615305200672e-06,
      "loss": 1.0076,
      "step": 10095
    },
    {
      "epoch": 3.908633372048006,
      "grad_norm": 27.855960845947266,
      "learning_rate": 6.768185142168883e-06,
      "loss": 1.0402,
      "step": 10096
    },
    {
      "epoch": 3.9090205187766163,
      "grad_norm": 42.37704086303711,
      "learning_rate": 6.767754979137093e-06,
      "loss": 0.9266,
      "step": 10097
    },
    {
      "epoch": 3.9094076655052263,
      "grad_norm": 10.350830078125,
      "learning_rate": 6.767324816105304e-06,
      "loss": 0.3897,
      "step": 10098
    },
    {
      "epoch": 3.9097948122338364,
      "grad_norm": 13.159006118774414,
      "learning_rate": 6.766894653073516e-06,
      "loss": 0.9969,
      "step": 10099
    },
    {
      "epoch": 3.910181958962447,
      "grad_norm": 19.526281356811523,
      "learning_rate": 6.766464490041727e-06,
      "loss": 1.1737,
      "step": 10100
    },
    {
      "epoch": 3.910569105691057,
      "grad_norm": 31.29922103881836,
      "learning_rate": 6.766034327009937e-06,
      "loss": 1.4453,
      "step": 10101
    },
    {
      "epoch": 3.910956252419667,
      "grad_norm": 25.12453269958496,
      "learning_rate": 6.765604163978148e-06,
      "loss": 1.1776,
      "step": 10102
    },
    {
      "epoch": 3.911343399148277,
      "grad_norm": 21.309925079345703,
      "learning_rate": 6.765174000946359e-06,
      "loss": 0.6626,
      "step": 10103
    },
    {
      "epoch": 3.911730545876887,
      "grad_norm": 30.703197479248047,
      "learning_rate": 6.764743837914571e-06,
      "loss": 1.5124,
      "step": 10104
    },
    {
      "epoch": 3.9121176926054977,
      "grad_norm": 18.993186950683594,
      "learning_rate": 6.764313674882781e-06,
      "loss": 2.2432,
      "step": 10105
    },
    {
      "epoch": 3.9125048393341078,
      "grad_norm": 4.1285176277160645,
      "learning_rate": 6.763883511850992e-06,
      "loss": 0.0938,
      "step": 10106
    },
    {
      "epoch": 3.912891986062718,
      "grad_norm": 16.701740264892578,
      "learning_rate": 6.763453348819203e-06,
      "loss": 3.412,
      "step": 10107
    },
    {
      "epoch": 3.913279132791328,
      "grad_norm": 50.668819427490234,
      "learning_rate": 6.763023185787414e-06,
      "loss": 1.1891,
      "step": 10108
    },
    {
      "epoch": 3.913666279519938,
      "grad_norm": 22.097261428833008,
      "learning_rate": 6.762593022755624e-06,
      "loss": 1.0201,
      "step": 10109
    },
    {
      "epoch": 3.9140534262485485,
      "grad_norm": 40.62300109863281,
      "learning_rate": 6.762162859723836e-06,
      "loss": 1.6182,
      "step": 10110
    },
    {
      "epoch": 3.914440572977158,
      "grad_norm": 86.60955047607422,
      "learning_rate": 6.761732696692047e-06,
      "loss": 1.9736,
      "step": 10111
    },
    {
      "epoch": 3.9148277197057686,
      "grad_norm": 3.998889923095703,
      "learning_rate": 6.761302533660258e-06,
      "loss": 0.1168,
      "step": 10112
    },
    {
      "epoch": 3.9152148664343787,
      "grad_norm": 16.747228622436523,
      "learning_rate": 6.760872370628468e-06,
      "loss": 0.5944,
      "step": 10113
    },
    {
      "epoch": 3.9156020131629887,
      "grad_norm": 18.510286331176758,
      "learning_rate": 6.7604422075966805e-06,
      "loss": 0.5116,
      "step": 10114
    },
    {
      "epoch": 3.915989159891599,
      "grad_norm": 15.148720741271973,
      "learning_rate": 6.760012044564891e-06,
      "loss": 0.5791,
      "step": 10115
    },
    {
      "epoch": 3.916376306620209,
      "grad_norm": 10.50108528137207,
      "learning_rate": 6.759581881533102e-06,
      "loss": 0.2975,
      "step": 10116
    },
    {
      "epoch": 3.9167634533488194,
      "grad_norm": 26.999038696289062,
      "learning_rate": 6.759151718501312e-06,
      "loss": 0.7836,
      "step": 10117
    },
    {
      "epoch": 3.9171506000774294,
      "grad_norm": 34.775634765625,
      "learning_rate": 6.758721555469524e-06,
      "loss": 1.0151,
      "step": 10118
    },
    {
      "epoch": 3.9175377468060395,
      "grad_norm": 35.697776794433594,
      "learning_rate": 6.758291392437735e-06,
      "loss": 1.7607,
      "step": 10119
    },
    {
      "epoch": 3.9179248935346496,
      "grad_norm": 2.9662420749664307,
      "learning_rate": 6.757861229405946e-06,
      "loss": 0.1577,
      "step": 10120
    },
    {
      "epoch": 3.9183120402632596,
      "grad_norm": 74.4100570678711,
      "learning_rate": 6.757431066374156e-06,
      "loss": 2.2438,
      "step": 10121
    },
    {
      "epoch": 3.91869918699187,
      "grad_norm": 7.58401346206665,
      "learning_rate": 6.757000903342368e-06,
      "loss": 1.088,
      "step": 10122
    },
    {
      "epoch": 3.91908633372048,
      "grad_norm": 9.650810241699219,
      "learning_rate": 6.756570740310578e-06,
      "loss": 0.7205,
      "step": 10123
    },
    {
      "epoch": 3.9194734804490903,
      "grad_norm": 7.728110313415527,
      "learning_rate": 6.756140577278789e-06,
      "loss": 0.1826,
      "step": 10124
    },
    {
      "epoch": 3.9198606271777003,
      "grad_norm": 25.028085708618164,
      "learning_rate": 6.755710414247e-06,
      "loss": 1.7004,
      "step": 10125
    },
    {
      "epoch": 3.9202477739063104,
      "grad_norm": 57.38784408569336,
      "learning_rate": 6.7552802512152116e-06,
      "loss": 3.0369,
      "step": 10126
    },
    {
      "epoch": 3.9206349206349205,
      "grad_norm": 4.767512798309326,
      "learning_rate": 6.754850088183422e-06,
      "loss": 0.1901,
      "step": 10127
    },
    {
      "epoch": 3.9210220673635305,
      "grad_norm": 34.6547737121582,
      "learning_rate": 6.754419925151633e-06,
      "loss": 2.2583,
      "step": 10128
    },
    {
      "epoch": 3.921409214092141,
      "grad_norm": 47.6759033203125,
      "learning_rate": 6.753989762119843e-06,
      "loss": 1.3674,
      "step": 10129
    },
    {
      "epoch": 3.921796360820751,
      "grad_norm": 28.789852142333984,
      "learning_rate": 6.7535595990880555e-06,
      "loss": 0.5336,
      "step": 10130
    },
    {
      "epoch": 3.922183507549361,
      "grad_norm": 50.769615173339844,
      "learning_rate": 6.753129436056266e-06,
      "loss": 3.3123,
      "step": 10131
    },
    {
      "epoch": 3.9225706542779712,
      "grad_norm": 12.324508666992188,
      "learning_rate": 6.752699273024477e-06,
      "loss": 0.4798,
      "step": 10132
    },
    {
      "epoch": 3.9229578010065813,
      "grad_norm": 30.032405853271484,
      "learning_rate": 6.752269109992687e-06,
      "loss": 0.8108,
      "step": 10133
    },
    {
      "epoch": 3.923344947735192,
      "grad_norm": 18.554668426513672,
      "learning_rate": 6.7518389469608995e-06,
      "loss": 1.6957,
      "step": 10134
    },
    {
      "epoch": 3.923732094463802,
      "grad_norm": 15.573196411132812,
      "learning_rate": 6.75140878392911e-06,
      "loss": 0.5809,
      "step": 10135
    },
    {
      "epoch": 3.924119241192412,
      "grad_norm": 14.29631233215332,
      "learning_rate": 6.750978620897321e-06,
      "loss": 1.5652,
      "step": 10136
    },
    {
      "epoch": 3.924506387921022,
      "grad_norm": 32.725502014160156,
      "learning_rate": 6.750548457865531e-06,
      "loss": 1.2617,
      "step": 10137
    },
    {
      "epoch": 3.924893534649632,
      "grad_norm": 21.93337059020996,
      "learning_rate": 6.750118294833743e-06,
      "loss": 0.5242,
      "step": 10138
    },
    {
      "epoch": 3.9252806813782426,
      "grad_norm": 38.61756896972656,
      "learning_rate": 6.749688131801953e-06,
      "loss": 2.2987,
      "step": 10139
    },
    {
      "epoch": 3.925667828106852,
      "grad_norm": 6.777271270751953,
      "learning_rate": 6.749257968770165e-06,
      "loss": 0.4194,
      "step": 10140
    },
    {
      "epoch": 3.9260549748354627,
      "grad_norm": 23.899843215942383,
      "learning_rate": 6.748827805738375e-06,
      "loss": 1.3136,
      "step": 10141
    },
    {
      "epoch": 3.926442121564073,
      "grad_norm": 49.27468490600586,
      "learning_rate": 6.748397642706587e-06,
      "loss": 1.3197,
      "step": 10142
    },
    {
      "epoch": 3.926829268292683,
      "grad_norm": 43.81256866455078,
      "learning_rate": 6.747967479674797e-06,
      "loss": 1.588,
      "step": 10143
    },
    {
      "epoch": 3.927216415021293,
      "grad_norm": 8.065597534179688,
      "learning_rate": 6.747537316643008e-06,
      "loss": 0.4271,
      "step": 10144
    },
    {
      "epoch": 3.927603561749903,
      "grad_norm": 24.279754638671875,
      "learning_rate": 6.747107153611218e-06,
      "loss": 2.2743,
      "step": 10145
    },
    {
      "epoch": 3.9279907084785135,
      "grad_norm": 41.61064910888672,
      "learning_rate": 6.7466769905794306e-06,
      "loss": 0.7812,
      "step": 10146
    },
    {
      "epoch": 3.9283778552071236,
      "grad_norm": 61.30540466308594,
      "learning_rate": 6.746246827547641e-06,
      "loss": 1.6592,
      "step": 10147
    },
    {
      "epoch": 3.9287650019357336,
      "grad_norm": 30.192153930664062,
      "learning_rate": 6.745816664515852e-06,
      "loss": 1.6584,
      "step": 10148
    },
    {
      "epoch": 3.9291521486643437,
      "grad_norm": 19.243083953857422,
      "learning_rate": 6.745386501484062e-06,
      "loss": 1.0211,
      "step": 10149
    },
    {
      "epoch": 3.9295392953929538,
      "grad_norm": 26.00996208190918,
      "learning_rate": 6.7449563384522745e-06,
      "loss": 2.3695,
      "step": 10150
    },
    {
      "epoch": 3.9299264421215643,
      "grad_norm": 49.3651237487793,
      "learning_rate": 6.744526175420485e-06,
      "loss": 1.8341,
      "step": 10151
    },
    {
      "epoch": 3.9303135888501743,
      "grad_norm": 20.85097312927246,
      "learning_rate": 6.744096012388696e-06,
      "loss": 0.8199,
      "step": 10152
    },
    {
      "epoch": 3.9307007355787844,
      "grad_norm": 54.79131317138672,
      "learning_rate": 6.743665849356906e-06,
      "loss": 0.6427,
      "step": 10153
    },
    {
      "epoch": 3.9310878823073945,
      "grad_norm": 27.09776496887207,
      "learning_rate": 6.743235686325118e-06,
      "loss": 1.862,
      "step": 10154
    },
    {
      "epoch": 3.9314750290360045,
      "grad_norm": 20.211545944213867,
      "learning_rate": 6.742805523293329e-06,
      "loss": 3.2002,
      "step": 10155
    },
    {
      "epoch": 3.931862175764615,
      "grad_norm": 55.54769515991211,
      "learning_rate": 6.74237536026154e-06,
      "loss": 1.712,
      "step": 10156
    },
    {
      "epoch": 3.9322493224932247,
      "grad_norm": 31.518596649169922,
      "learning_rate": 6.741945197229751e-06,
      "loss": 1.3285,
      "step": 10157
    },
    {
      "epoch": 3.932636469221835,
      "grad_norm": 23.013046264648438,
      "learning_rate": 6.741515034197962e-06,
      "loss": 1.1976,
      "step": 10158
    },
    {
      "epoch": 3.9330236159504453,
      "grad_norm": 33.6099967956543,
      "learning_rate": 6.741084871166172e-06,
      "loss": 1.7584,
      "step": 10159
    },
    {
      "epoch": 3.9334107626790553,
      "grad_norm": 24.875043869018555,
      "learning_rate": 6.740654708134383e-06,
      "loss": 2.17,
      "step": 10160
    },
    {
      "epoch": 3.9337979094076654,
      "grad_norm": 24.295639038085938,
      "learning_rate": 6.740224545102595e-06,
      "loss": 1.8245,
      "step": 10161
    },
    {
      "epoch": 3.9341850561362754,
      "grad_norm": 42.822391510009766,
      "learning_rate": 6.739794382070806e-06,
      "loss": 1.5142,
      "step": 10162
    },
    {
      "epoch": 3.934572202864886,
      "grad_norm": 29.232112884521484,
      "learning_rate": 6.739364219039016e-06,
      "loss": 2.342,
      "step": 10163
    },
    {
      "epoch": 3.934959349593496,
      "grad_norm": 75.17218017578125,
      "learning_rate": 6.738934056007227e-06,
      "loss": 0.9965,
      "step": 10164
    },
    {
      "epoch": 3.935346496322106,
      "grad_norm": 12.091519355773926,
      "learning_rate": 6.738503892975439e-06,
      "loss": 0.6625,
      "step": 10165
    },
    {
      "epoch": 3.935733643050716,
      "grad_norm": 16.81403160095215,
      "learning_rate": 6.7380737299436496e-06,
      "loss": 0.7756,
      "step": 10166
    },
    {
      "epoch": 3.9361207897793262,
      "grad_norm": 38.43091583251953,
      "learning_rate": 6.73764356691186e-06,
      "loss": 1.516,
      "step": 10167
    },
    {
      "epoch": 3.9365079365079367,
      "grad_norm": 16.662324905395508,
      "learning_rate": 6.737213403880071e-06,
      "loss": 1.548,
      "step": 10168
    },
    {
      "epoch": 3.936895083236547,
      "grad_norm": 44.42856216430664,
      "learning_rate": 6.736783240848282e-06,
      "loss": 1.6293,
      "step": 10169
    },
    {
      "epoch": 3.937282229965157,
      "grad_norm": 51.98243713378906,
      "learning_rate": 6.7363530778164935e-06,
      "loss": 1.2128,
      "step": 10170
    },
    {
      "epoch": 3.937669376693767,
      "grad_norm": 40.70476531982422,
      "learning_rate": 6.735922914784704e-06,
      "loss": 2.1289,
      "step": 10171
    },
    {
      "epoch": 3.938056523422377,
      "grad_norm": 41.034793853759766,
      "learning_rate": 6.735492751752915e-06,
      "loss": 2.9073,
      "step": 10172
    },
    {
      "epoch": 3.938443670150987,
      "grad_norm": 91.31812286376953,
      "learning_rate": 6.735062588721126e-06,
      "loss": 2.1203,
      "step": 10173
    },
    {
      "epoch": 3.938830816879597,
      "grad_norm": 29.54302978515625,
      "learning_rate": 6.734632425689337e-06,
      "loss": 0.9394,
      "step": 10174
    },
    {
      "epoch": 3.9392179636082076,
      "grad_norm": 30.5799560546875,
      "learning_rate": 6.734202262657547e-06,
      "loss": 2.1524,
      "step": 10175
    },
    {
      "epoch": 3.9396051103368177,
      "grad_norm": 28.975954055786133,
      "learning_rate": 6.733772099625759e-06,
      "loss": 1.3042,
      "step": 10176
    },
    {
      "epoch": 3.9399922570654278,
      "grad_norm": 17.049468994140625,
      "learning_rate": 6.73334193659397e-06,
      "loss": 0.8962,
      "step": 10177
    },
    {
      "epoch": 3.940379403794038,
      "grad_norm": 17.950973510742188,
      "learning_rate": 6.732911773562181e-06,
      "loss": 1.1477,
      "step": 10178
    },
    {
      "epoch": 3.940766550522648,
      "grad_norm": 40.56097412109375,
      "learning_rate": 6.732481610530391e-06,
      "loss": 1.5937,
      "step": 10179
    },
    {
      "epoch": 3.9411536972512584,
      "grad_norm": 25.41246223449707,
      "learning_rate": 6.732051447498602e-06,
      "loss": 1.4204,
      "step": 10180
    },
    {
      "epoch": 3.9415408439798685,
      "grad_norm": 38.0582389831543,
      "learning_rate": 6.731621284466814e-06,
      "loss": 1.1599,
      "step": 10181
    },
    {
      "epoch": 3.9419279907084785,
      "grad_norm": 21.329633712768555,
      "learning_rate": 6.731191121435025e-06,
      "loss": 1.2521,
      "step": 10182
    },
    {
      "epoch": 3.9423151374370886,
      "grad_norm": 40.550697326660156,
      "learning_rate": 6.730760958403235e-06,
      "loss": 0.9503,
      "step": 10183
    },
    {
      "epoch": 3.9427022841656987,
      "grad_norm": 19.846925735473633,
      "learning_rate": 6.730330795371446e-06,
      "loss": 1.6218,
      "step": 10184
    },
    {
      "epoch": 3.943089430894309,
      "grad_norm": 34.497554779052734,
      "learning_rate": 6.729900632339658e-06,
      "loss": 1.5151,
      "step": 10185
    },
    {
      "epoch": 3.943476577622919,
      "grad_norm": 36.9100227355957,
      "learning_rate": 6.7294704693078686e-06,
      "loss": 2.0967,
      "step": 10186
    },
    {
      "epoch": 3.9438637243515293,
      "grad_norm": 31.365028381347656,
      "learning_rate": 6.729040306276079e-06,
      "loss": 1.0181,
      "step": 10187
    },
    {
      "epoch": 3.9442508710801394,
      "grad_norm": 54.679290771484375,
      "learning_rate": 6.72861014324429e-06,
      "loss": 1.3767,
      "step": 10188
    },
    {
      "epoch": 3.9446380178087495,
      "grad_norm": 18.944801330566406,
      "learning_rate": 6.728179980212501e-06,
      "loss": 0.9504,
      "step": 10189
    },
    {
      "epoch": 3.9450251645373595,
      "grad_norm": 8.784278869628906,
      "learning_rate": 6.727749817180712e-06,
      "loss": 0.9909,
      "step": 10190
    },
    {
      "epoch": 3.9454123112659696,
      "grad_norm": 10.197705268859863,
      "learning_rate": 6.727319654148923e-06,
      "loss": 0.7031,
      "step": 10191
    },
    {
      "epoch": 3.94579945799458,
      "grad_norm": 42.20341110229492,
      "learning_rate": 6.726889491117134e-06,
      "loss": 1.0085,
      "step": 10192
    },
    {
      "epoch": 3.94618660472319,
      "grad_norm": 7.0450239181518555,
      "learning_rate": 6.726459328085345e-06,
      "loss": 0.4031,
      "step": 10193
    },
    {
      "epoch": 3.9465737514518002,
      "grad_norm": 13.579207420349121,
      "learning_rate": 6.726029165053556e-06,
      "loss": 0.8496,
      "step": 10194
    },
    {
      "epoch": 3.9469608981804103,
      "grad_norm": 74.71723175048828,
      "learning_rate": 6.725599002021766e-06,
      "loss": 0.7245,
      "step": 10195
    },
    {
      "epoch": 3.9473480449090204,
      "grad_norm": 25.480424880981445,
      "learning_rate": 6.7251688389899785e-06,
      "loss": 1.7588,
      "step": 10196
    },
    {
      "epoch": 3.947735191637631,
      "grad_norm": 23.07775115966797,
      "learning_rate": 6.724738675958189e-06,
      "loss": 0.8507,
      "step": 10197
    },
    {
      "epoch": 3.948122338366241,
      "grad_norm": 19.799257278442383,
      "learning_rate": 6.7243085129264e-06,
      "loss": 1.6727,
      "step": 10198
    },
    {
      "epoch": 3.948509485094851,
      "grad_norm": 57.46224594116211,
      "learning_rate": 6.72387834989461e-06,
      "loss": 2.7182,
      "step": 10199
    },
    {
      "epoch": 3.948896631823461,
      "grad_norm": 20.99034881591797,
      "learning_rate": 6.7234481868628224e-06,
      "loss": 1.0663,
      "step": 10200
    },
    {
      "epoch": 3.949283778552071,
      "grad_norm": 11.523439407348633,
      "learning_rate": 6.723018023831033e-06,
      "loss": 0.6414,
      "step": 10201
    },
    {
      "epoch": 3.9496709252806816,
      "grad_norm": 21.069623947143555,
      "learning_rate": 6.722587860799244e-06,
      "loss": 1.7249,
      "step": 10202
    },
    {
      "epoch": 3.9500580720092913,
      "grad_norm": 70.33483123779297,
      "learning_rate": 6.722157697767454e-06,
      "loss": 0.7064,
      "step": 10203
    },
    {
      "epoch": 3.9504452187379018,
      "grad_norm": 16.716938018798828,
      "learning_rate": 6.7217275347356656e-06,
      "loss": 1.393,
      "step": 10204
    },
    {
      "epoch": 3.950832365466512,
      "grad_norm": 41.300296783447266,
      "learning_rate": 6.721297371703876e-06,
      "loss": 0.5844,
      "step": 10205
    },
    {
      "epoch": 3.951219512195122,
      "grad_norm": 12.44469165802002,
      "learning_rate": 6.7208672086720876e-06,
      "loss": 0.6475,
      "step": 10206
    },
    {
      "epoch": 3.951606658923732,
      "grad_norm": 49.408226013183594,
      "learning_rate": 6.720437045640298e-06,
      "loss": 1.2887,
      "step": 10207
    },
    {
      "epoch": 3.951993805652342,
      "grad_norm": 21.95481300354004,
      "learning_rate": 6.7200068826085095e-06,
      "loss": 0.8559,
      "step": 10208
    },
    {
      "epoch": 3.9523809523809526,
      "grad_norm": 22.721370697021484,
      "learning_rate": 6.71957671957672e-06,
      "loss": 1.2004,
      "step": 10209
    },
    {
      "epoch": 3.9527680991095626,
      "grad_norm": 10.869961738586426,
      "learning_rate": 6.719146556544931e-06,
      "loss": 0.8559,
      "step": 10210
    },
    {
      "epoch": 3.9531552458381727,
      "grad_norm": 33.38911819458008,
      "learning_rate": 6.718716393513141e-06,
      "loss": 1.7752,
      "step": 10211
    },
    {
      "epoch": 3.9535423925667827,
      "grad_norm": 24.36989974975586,
      "learning_rate": 6.7182862304813535e-06,
      "loss": 0.3681,
      "step": 10212
    },
    {
      "epoch": 3.953929539295393,
      "grad_norm": 29.910602569580078,
      "learning_rate": 6.717856067449564e-06,
      "loss": 1.7483,
      "step": 10213
    },
    {
      "epoch": 3.9543166860240033,
      "grad_norm": 17.167078018188477,
      "learning_rate": 6.717425904417775e-06,
      "loss": 0.6215,
      "step": 10214
    },
    {
      "epoch": 3.9547038327526134,
      "grad_norm": 18.042736053466797,
      "learning_rate": 6.716995741385985e-06,
      "loss": 1.7184,
      "step": 10215
    },
    {
      "epoch": 3.9550909794812235,
      "grad_norm": 15.163987159729004,
      "learning_rate": 6.7165655783541975e-06,
      "loss": 1.0453,
      "step": 10216
    },
    {
      "epoch": 3.9554781262098335,
      "grad_norm": 4.547192096710205,
      "learning_rate": 6.716135415322408e-06,
      "loss": 0.2337,
      "step": 10217
    },
    {
      "epoch": 3.9558652729384436,
      "grad_norm": 23.618379592895508,
      "learning_rate": 6.715705252290619e-06,
      "loss": 1.1944,
      "step": 10218
    },
    {
      "epoch": 3.9562524196670537,
      "grad_norm": 28.324647903442383,
      "learning_rate": 6.715275089258829e-06,
      "loss": 0.6731,
      "step": 10219
    },
    {
      "epoch": 3.9566395663956637,
      "grad_norm": 19.88594627380371,
      "learning_rate": 6.714844926227041e-06,
      "loss": 1.0583,
      "step": 10220
    },
    {
      "epoch": 3.9570267131242742,
      "grad_norm": 40.36854934692383,
      "learning_rate": 6.714414763195252e-06,
      "loss": 0.9944,
      "step": 10221
    },
    {
      "epoch": 3.9574138598528843,
      "grad_norm": 21.098621368408203,
      "learning_rate": 6.713984600163463e-06,
      "loss": 0.6418,
      "step": 10222
    },
    {
      "epoch": 3.9578010065814944,
      "grad_norm": 11.494778633117676,
      "learning_rate": 6.713554437131673e-06,
      "loss": 0.8566,
      "step": 10223
    },
    {
      "epoch": 3.9581881533101044,
      "grad_norm": 2.4942493438720703,
      "learning_rate": 6.7131242740998846e-06,
      "loss": 0.0741,
      "step": 10224
    },
    {
      "epoch": 3.9585753000387145,
      "grad_norm": 9.117301940917969,
      "learning_rate": 6.712694111068095e-06,
      "loss": 0.6657,
      "step": 10225
    },
    {
      "epoch": 3.958962446767325,
      "grad_norm": 12.305556297302246,
      "learning_rate": 6.712263948036306e-06,
      "loss": 0.597,
      "step": 10226
    },
    {
      "epoch": 3.959349593495935,
      "grad_norm": 8.223468780517578,
      "learning_rate": 6.711833785004517e-06,
      "loss": 0.2026,
      "step": 10227
    },
    {
      "epoch": 3.959736740224545,
      "grad_norm": 10.354538917541504,
      "learning_rate": 6.7114036219727285e-06,
      "loss": 0.5946,
      "step": 10228
    },
    {
      "epoch": 3.960123886953155,
      "grad_norm": 4.174990177154541,
      "learning_rate": 6.710973458940939e-06,
      "loss": 0.1935,
      "step": 10229
    },
    {
      "epoch": 3.9605110336817653,
      "grad_norm": 15.663409233093262,
      "learning_rate": 6.71054329590915e-06,
      "loss": 1.2195,
      "step": 10230
    },
    {
      "epoch": 3.960898180410376,
      "grad_norm": 3.6736464500427246,
      "learning_rate": 6.71011313287736e-06,
      "loss": 0.1643,
      "step": 10231
    },
    {
      "epoch": 3.9612853271389854,
      "grad_norm": 47.24338150024414,
      "learning_rate": 6.7096829698455725e-06,
      "loss": 2.3185,
      "step": 10232
    },
    {
      "epoch": 3.961672473867596,
      "grad_norm": 10.473018646240234,
      "learning_rate": 6.709252806813783e-06,
      "loss": 0.6577,
      "step": 10233
    },
    {
      "epoch": 3.962059620596206,
      "grad_norm": 86.37158203125,
      "learning_rate": 6.708822643781994e-06,
      "loss": 1.4311,
      "step": 10234
    },
    {
      "epoch": 3.962446767324816,
      "grad_norm": 40.62025833129883,
      "learning_rate": 6.708392480750204e-06,
      "loss": 1.6942,
      "step": 10235
    },
    {
      "epoch": 3.962833914053426,
      "grad_norm": 2.044548511505127,
      "learning_rate": 6.7079623177184165e-06,
      "loss": 0.0613,
      "step": 10236
    },
    {
      "epoch": 3.963221060782036,
      "grad_norm": 9.637667655944824,
      "learning_rate": 6.707532154686627e-06,
      "loss": 0.6548,
      "step": 10237
    },
    {
      "epoch": 3.9636082075106467,
      "grad_norm": 33.581817626953125,
      "learning_rate": 6.707101991654838e-06,
      "loss": 1.3792,
      "step": 10238
    },
    {
      "epoch": 3.9639953542392568,
      "grad_norm": 41.745750427246094,
      "learning_rate": 6.706671828623049e-06,
      "loss": 1.9353,
      "step": 10239
    },
    {
      "epoch": 3.964382500967867,
      "grad_norm": 12.430282592773438,
      "learning_rate": 6.70624166559126e-06,
      "loss": 0.6756,
      "step": 10240
    },
    {
      "epoch": 3.964769647696477,
      "grad_norm": 41.54062271118164,
      "learning_rate": 6.70581150255947e-06,
      "loss": 2.3206,
      "step": 10241
    },
    {
      "epoch": 3.965156794425087,
      "grad_norm": 42.56486892700195,
      "learning_rate": 6.705381339527682e-06,
      "loss": 3.416,
      "step": 10242
    },
    {
      "epoch": 3.9655439411536975,
      "grad_norm": 26.13779067993164,
      "learning_rate": 6.704951176495893e-06,
      "loss": 1.84,
      "step": 10243
    },
    {
      "epoch": 3.9659310878823075,
      "grad_norm": 24.305448532104492,
      "learning_rate": 6.7045210134641036e-06,
      "loss": 0.286,
      "step": 10244
    },
    {
      "epoch": 3.9663182346109176,
      "grad_norm": 39.75332260131836,
      "learning_rate": 6.704090850432314e-06,
      "loss": 2.4424,
      "step": 10245
    },
    {
      "epoch": 3.9667053813395277,
      "grad_norm": 20.239887237548828,
      "learning_rate": 6.703660687400525e-06,
      "loss": 1.8569,
      "step": 10246
    },
    {
      "epoch": 3.9670925280681377,
      "grad_norm": 33.62478256225586,
      "learning_rate": 6.703230524368737e-06,
      "loss": 1.9424,
      "step": 10247
    },
    {
      "epoch": 3.9674796747967482,
      "grad_norm": 16.292320251464844,
      "learning_rate": 6.7028003613369475e-06,
      "loss": 1.9524,
      "step": 10248
    },
    {
      "epoch": 3.967866821525358,
      "grad_norm": 36.671791076660156,
      "learning_rate": 6.702370198305158e-06,
      "loss": 2.3272,
      "step": 10249
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 20.58478355407715,
      "learning_rate": 6.701940035273369e-06,
      "loss": 2.5347,
      "step": 10250
    },
    {
      "epoch": 3.9686411149825784,
      "grad_norm": 5.0934224128723145,
      "learning_rate": 6.701509872241581e-06,
      "loss": 0.1904,
      "step": 10251
    },
    {
      "epoch": 3.9690282617111885,
      "grad_norm": 116.83929443359375,
      "learning_rate": 6.7010797092097915e-06,
      "loss": 1.3308,
      "step": 10252
    },
    {
      "epoch": 3.9694154084397986,
      "grad_norm": 36.373435974121094,
      "learning_rate": 6.700649546178002e-06,
      "loss": 2.3752,
      "step": 10253
    },
    {
      "epoch": 3.9698025551684086,
      "grad_norm": 18.79348373413086,
      "learning_rate": 6.700219383146213e-06,
      "loss": 0.4291,
      "step": 10254
    },
    {
      "epoch": 3.970189701897019,
      "grad_norm": 48.89840316772461,
      "learning_rate": 6.699789220114424e-06,
      "loss": 2.2835,
      "step": 10255
    },
    {
      "epoch": 3.970576848625629,
      "grad_norm": 25.30666160583496,
      "learning_rate": 6.699359057082635e-06,
      "loss": 1.0594,
      "step": 10256
    },
    {
      "epoch": 3.9709639953542393,
      "grad_norm": 144.9860076904297,
      "learning_rate": 6.698928894050846e-06,
      "loss": 1.6615,
      "step": 10257
    },
    {
      "epoch": 3.9713511420828493,
      "grad_norm": 41.841617584228516,
      "learning_rate": 6.698498731019057e-06,
      "loss": 2.5708,
      "step": 10258
    },
    {
      "epoch": 3.9717382888114594,
      "grad_norm": 75.94661712646484,
      "learning_rate": 6.698068567987268e-06,
      "loss": 1.2727,
      "step": 10259
    },
    {
      "epoch": 3.97212543554007,
      "grad_norm": 47.60887145996094,
      "learning_rate": 6.697638404955479e-06,
      "loss": 1.4148,
      "step": 10260
    },
    {
      "epoch": 3.97251258226868,
      "grad_norm": 15.72955322265625,
      "learning_rate": 6.697208241923689e-06,
      "loss": 3.3765,
      "step": 10261
    },
    {
      "epoch": 3.97289972899729,
      "grad_norm": 32.796875,
      "learning_rate": 6.6967780788919e-06,
      "loss": 1.3562,
      "step": 10262
    },
    {
      "epoch": 3.9732868757259,
      "grad_norm": 25.595287322998047,
      "learning_rate": 6.696347915860112e-06,
      "loss": 1.1822,
      "step": 10263
    },
    {
      "epoch": 3.97367402245451,
      "grad_norm": 26.681095123291016,
      "learning_rate": 6.6959177528283226e-06,
      "loss": 3.7653,
      "step": 10264
    },
    {
      "epoch": 3.9740611691831202,
      "grad_norm": 13.265863418579102,
      "learning_rate": 6.695487589796533e-06,
      "loss": 0.7947,
      "step": 10265
    },
    {
      "epoch": 3.9744483159117303,
      "grad_norm": 8.307071685791016,
      "learning_rate": 6.695057426764744e-06,
      "loss": 1.0458,
      "step": 10266
    },
    {
      "epoch": 3.974835462640341,
      "grad_norm": 6.2602858543396,
      "learning_rate": 6.694627263732956e-06,
      "loss": 1.0235,
      "step": 10267
    },
    {
      "epoch": 3.975222609368951,
      "grad_norm": 14.517655372619629,
      "learning_rate": 6.6941971007011665e-06,
      "loss": 0.6103,
      "step": 10268
    },
    {
      "epoch": 3.975609756097561,
      "grad_norm": 7.49217414855957,
      "learning_rate": 6.693766937669377e-06,
      "loss": 0.4247,
      "step": 10269
    },
    {
      "epoch": 3.975996902826171,
      "grad_norm": 29.98346519470215,
      "learning_rate": 6.693336774637588e-06,
      "loss": 1.22,
      "step": 10270
    },
    {
      "epoch": 3.976384049554781,
      "grad_norm": 18.95924949645996,
      "learning_rate": 6.692906611605799e-06,
      "loss": 1.3559,
      "step": 10271
    },
    {
      "epoch": 3.9767711962833916,
      "grad_norm": 18.035526275634766,
      "learning_rate": 6.6924764485740105e-06,
      "loss": 0.7219,
      "step": 10272
    },
    {
      "epoch": 3.9771583430120017,
      "grad_norm": 27.369218826293945,
      "learning_rate": 6.692046285542221e-06,
      "loss": 2.1144,
      "step": 10273
    },
    {
      "epoch": 3.9775454897406117,
      "grad_norm": 25.759599685668945,
      "learning_rate": 6.691616122510432e-06,
      "loss": 0.6453,
      "step": 10274
    },
    {
      "epoch": 3.977932636469222,
      "grad_norm": 20.800228118896484,
      "learning_rate": 6.691185959478643e-06,
      "loss": 1.3208,
      "step": 10275
    },
    {
      "epoch": 3.978319783197832,
      "grad_norm": 18.168228149414062,
      "learning_rate": 6.690755796446854e-06,
      "loss": 0.5939,
      "step": 10276
    },
    {
      "epoch": 3.9787069299264424,
      "grad_norm": 44.58363342285156,
      "learning_rate": 6.690325633415064e-06,
      "loss": 1.5305,
      "step": 10277
    },
    {
      "epoch": 3.979094076655052,
      "grad_norm": 24.228336334228516,
      "learning_rate": 6.6898954703832765e-06,
      "loss": 0.7799,
      "step": 10278
    },
    {
      "epoch": 3.9794812233836625,
      "grad_norm": 40.353538513183594,
      "learning_rate": 6.689465307351487e-06,
      "loss": 0.5464,
      "step": 10279
    },
    {
      "epoch": 3.9798683701122726,
      "grad_norm": 37.02348709106445,
      "learning_rate": 6.689035144319698e-06,
      "loss": 0.9552,
      "step": 10280
    },
    {
      "epoch": 3.9802555168408826,
      "grad_norm": 49.838035583496094,
      "learning_rate": 6.688604981287908e-06,
      "loss": 2.3781,
      "step": 10281
    },
    {
      "epoch": 3.9806426635694927,
      "grad_norm": 20.634456634521484,
      "learning_rate": 6.6881748182561204e-06,
      "loss": 1.6318,
      "step": 10282
    },
    {
      "epoch": 3.9810298102981028,
      "grad_norm": 17.18379783630371,
      "learning_rate": 6.687744655224331e-06,
      "loss": 0.256,
      "step": 10283
    },
    {
      "epoch": 3.9814169570267133,
      "grad_norm": 17.919483184814453,
      "learning_rate": 6.6873144921925416e-06,
      "loss": 0.9137,
      "step": 10284
    },
    {
      "epoch": 3.9818041037553233,
      "grad_norm": 30.35283660888672,
      "learning_rate": 6.686884329160752e-06,
      "loss": 3.1844,
      "step": 10285
    },
    {
      "epoch": 3.9821912504839334,
      "grad_norm": 67.46269989013672,
      "learning_rate": 6.6864541661289635e-06,
      "loss": 1.2769,
      "step": 10286
    },
    {
      "epoch": 3.9825783972125435,
      "grad_norm": 1.6930346488952637,
      "learning_rate": 6.686024003097175e-06,
      "loss": 0.0533,
      "step": 10287
    },
    {
      "epoch": 3.9829655439411535,
      "grad_norm": 5.162554740905762,
      "learning_rate": 6.6855938400653855e-06,
      "loss": 0.2251,
      "step": 10288
    },
    {
      "epoch": 3.983352690669764,
      "grad_norm": 38.68720245361328,
      "learning_rate": 6.685163677033596e-06,
      "loss": 2.3812,
      "step": 10289
    },
    {
      "epoch": 3.983739837398374,
      "grad_norm": 7.520884990692139,
      "learning_rate": 6.6847335140018075e-06,
      "loss": 0.4133,
      "step": 10290
    },
    {
      "epoch": 3.984126984126984,
      "grad_norm": 28.874357223510742,
      "learning_rate": 6.684303350970018e-06,
      "loss": 0.6352,
      "step": 10291
    },
    {
      "epoch": 3.9845141308555942,
      "grad_norm": 11.934823036193848,
      "learning_rate": 6.683873187938229e-06,
      "loss": 0.6621,
      "step": 10292
    },
    {
      "epoch": 3.9849012775842043,
      "grad_norm": 15.47553539276123,
      "learning_rate": 6.68344302490644e-06,
      "loss": 0.5115,
      "step": 10293
    },
    {
      "epoch": 3.985288424312815,
      "grad_norm": 18.788619995117188,
      "learning_rate": 6.6830128618746515e-06,
      "loss": 1.5503,
      "step": 10294
    },
    {
      "epoch": 3.9856755710414244,
      "grad_norm": 17.701324462890625,
      "learning_rate": 6.682582698842862e-06,
      "loss": 1.5442,
      "step": 10295
    },
    {
      "epoch": 3.986062717770035,
      "grad_norm": 15.621858596801758,
      "learning_rate": 6.682152535811073e-06,
      "loss": 1.431,
      "step": 10296
    },
    {
      "epoch": 3.986449864498645,
      "grad_norm": 32.94131851196289,
      "learning_rate": 6.681722372779283e-06,
      "loss": 2.1796,
      "step": 10297
    },
    {
      "epoch": 3.986837011227255,
      "grad_norm": 43.518985748291016,
      "learning_rate": 6.6812922097474955e-06,
      "loss": 0.7716,
      "step": 10298
    },
    {
      "epoch": 3.987224157955865,
      "grad_norm": 36.65351867675781,
      "learning_rate": 6.680862046715706e-06,
      "loss": 0.6645,
      "step": 10299
    },
    {
      "epoch": 3.987611304684475,
      "grad_norm": 62.176116943359375,
      "learning_rate": 6.680431883683917e-06,
      "loss": 1.5123,
      "step": 10300
    },
    {
      "epoch": 3.9879984514130857,
      "grad_norm": 34.125389099121094,
      "learning_rate": 6.680001720652127e-06,
      "loss": 2.6148,
      "step": 10301
    },
    {
      "epoch": 3.988385598141696,
      "grad_norm": 24.06728172302246,
      "learning_rate": 6.679571557620339e-06,
      "loss": 0.8731,
      "step": 10302
    },
    {
      "epoch": 3.988772744870306,
      "grad_norm": 26.04237174987793,
      "learning_rate": 6.67914139458855e-06,
      "loss": 1.2477,
      "step": 10303
    },
    {
      "epoch": 3.989159891598916,
      "grad_norm": 38.19656753540039,
      "learning_rate": 6.6787112315567606e-06,
      "loss": 1.1037,
      "step": 10304
    },
    {
      "epoch": 3.989547038327526,
      "grad_norm": 62.30495071411133,
      "learning_rate": 6.678281068524971e-06,
      "loss": 2.1847,
      "step": 10305
    },
    {
      "epoch": 3.9899341850561365,
      "grad_norm": 122.47358703613281,
      "learning_rate": 6.6778509054931825e-06,
      "loss": 3.7441,
      "step": 10306
    },
    {
      "epoch": 3.9903213317847466,
      "grad_norm": 15.952422142028809,
      "learning_rate": 6.677420742461393e-06,
      "loss": 1.1385,
      "step": 10307
    },
    {
      "epoch": 3.9907084785133566,
      "grad_norm": 19.110897064208984,
      "learning_rate": 6.6769905794296045e-06,
      "loss": 1.3779,
      "step": 10308
    },
    {
      "epoch": 3.9910956252419667,
      "grad_norm": 16.77364730834961,
      "learning_rate": 6.676560416397815e-06,
      "loss": 1.8642,
      "step": 10309
    },
    {
      "epoch": 3.9914827719705768,
      "grad_norm": 26.130949020385742,
      "learning_rate": 6.6761302533660265e-06,
      "loss": 1.9464,
      "step": 10310
    },
    {
      "epoch": 3.991869918699187,
      "grad_norm": 5.151552200317383,
      "learning_rate": 6.675700090334237e-06,
      "loss": 0.2128,
      "step": 10311
    },
    {
      "epoch": 3.992257065427797,
      "grad_norm": 54.83205795288086,
      "learning_rate": 6.675269927302448e-06,
      "loss": 2.1434,
      "step": 10312
    },
    {
      "epoch": 3.9926442121564074,
      "grad_norm": 13.492067337036133,
      "learning_rate": 6.674839764270658e-06,
      "loss": 1.2021,
      "step": 10313
    },
    {
      "epoch": 3.9930313588850175,
      "grad_norm": 11.060521125793457,
      "learning_rate": 6.6744096012388705e-06,
      "loss": 0.3809,
      "step": 10314
    },
    {
      "epoch": 3.9934185056136275,
      "grad_norm": 10.91136646270752,
      "learning_rate": 6.673979438207081e-06,
      "loss": 0.6506,
      "step": 10315
    },
    {
      "epoch": 3.9938056523422376,
      "grad_norm": 11.339489936828613,
      "learning_rate": 6.673549275175292e-06,
      "loss": 0.6234,
      "step": 10316
    },
    {
      "epoch": 3.9941927990708477,
      "grad_norm": 18.916200637817383,
      "learning_rate": 6.673119112143502e-06,
      "loss": 0.6179,
      "step": 10317
    },
    {
      "epoch": 3.994579945799458,
      "grad_norm": 11.835350036621094,
      "learning_rate": 6.6726889491117145e-06,
      "loss": 1.2186,
      "step": 10318
    },
    {
      "epoch": 3.9949670925280683,
      "grad_norm": 32.929298400878906,
      "learning_rate": 6.672258786079925e-06,
      "loss": 1.4228,
      "step": 10319
    },
    {
      "epoch": 3.9953542392566783,
      "grad_norm": 15.995637893676758,
      "learning_rate": 6.671828623048136e-06,
      "loss": 0.4192,
      "step": 10320
    },
    {
      "epoch": 3.9957413859852884,
      "grad_norm": 11.49634838104248,
      "learning_rate": 6.671398460016347e-06,
      "loss": 0.2171,
      "step": 10321
    },
    {
      "epoch": 3.9961285327138985,
      "grad_norm": 37.91719436645508,
      "learning_rate": 6.6709682969845576e-06,
      "loss": 0.9988,
      "step": 10322
    },
    {
      "epoch": 3.996515679442509,
      "grad_norm": 24.84229278564453,
      "learning_rate": 6.670538133952769e-06,
      "loss": 0.6331,
      "step": 10323
    },
    {
      "epoch": 3.9969028261711186,
      "grad_norm": 31.117509841918945,
      "learning_rate": 6.6701079709209796e-06,
      "loss": 3.0027,
      "step": 10324
    },
    {
      "epoch": 3.997289972899729,
      "grad_norm": 39.68149948120117,
      "learning_rate": 6.669677807889191e-06,
      "loss": 0.8287,
      "step": 10325
    },
    {
      "epoch": 3.997677119628339,
      "grad_norm": 18.52871322631836,
      "learning_rate": 6.6692476448574015e-06,
      "loss": 0.7114,
      "step": 10326
    },
    {
      "epoch": 3.9980642663569492,
      "grad_norm": 13.225086212158203,
      "learning_rate": 6.668817481825612e-06,
      "loss": 0.7141,
      "step": 10327
    },
    {
      "epoch": 3.9984514130855593,
      "grad_norm": 7.473765850067139,
      "learning_rate": 6.668387318793823e-06,
      "loss": 0.4315,
      "step": 10328
    },
    {
      "epoch": 3.9988385598141694,
      "grad_norm": 79.96665954589844,
      "learning_rate": 6.667957155762035e-06,
      "loss": 1.0601,
      "step": 10329
    },
    {
      "epoch": 3.99922570654278,
      "grad_norm": 33.941871643066406,
      "learning_rate": 6.6675269927302455e-06,
      "loss": 1.0606,
      "step": 10330
    },
    {
      "epoch": 3.99961285327139,
      "grad_norm": 75.40877532958984,
      "learning_rate": 6.667096829698456e-06,
      "loss": 0.9756,
      "step": 10331
    },
    {
      "epoch": 4.0,
      "grad_norm": 19.842439651489258,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.4009,
      "step": 10332
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.4650943396226415,
      "eval_f1": 0.4375639647842197,
      "eval_loss": 1.4269495010375977,
      "eval_runtime": 385.2785,
      "eval_samples_per_second": 2.751,
      "eval_steps_per_second": 1.376,
      "step": 10332
    },
    {
      "epoch": 4.0003871467286105,
      "grad_norm": 25.445158004760742,
      "learning_rate": 6.666236503634879e-06,
      "loss": 0.857,
      "step": 10333
    },
    {
      "epoch": 4.00077429345722,
      "grad_norm": 48.39848709106445,
      "learning_rate": 6.6658063406030895e-06,
      "loss": 2.4721,
      "step": 10334
    },
    {
      "epoch": 4.001161440185831,
      "grad_norm": 22.930923461914062,
      "learning_rate": 6.6653761775713e-06,
      "loss": 1.0614,
      "step": 10335
    },
    {
      "epoch": 4.00154858691444,
      "grad_norm": 27.162992477416992,
      "learning_rate": 6.664946014539511e-06,
      "loss": 1.3295,
      "step": 10336
    },
    {
      "epoch": 4.001935733643051,
      "grad_norm": 37.118927001953125,
      "learning_rate": 6.664515851507722e-06,
      "loss": 2.5331,
      "step": 10337
    },
    {
      "epoch": 4.002322880371661,
      "grad_norm": 21.179752349853516,
      "learning_rate": 6.6640856884759334e-06,
      "loss": 0.8233,
      "step": 10338
    },
    {
      "epoch": 4.002710027100271,
      "grad_norm": 18.851346969604492,
      "learning_rate": 6.663655525444144e-06,
      "loss": 0.6474,
      "step": 10339
    },
    {
      "epoch": 4.003097173828881,
      "grad_norm": 103.60929107666016,
      "learning_rate": 6.663225362412355e-06,
      "loss": 1.2502,
      "step": 10340
    },
    {
      "epoch": 4.003484320557491,
      "grad_norm": 66.82598876953125,
      "learning_rate": 6.662795199380566e-06,
      "loss": 1.2729,
      "step": 10341
    },
    {
      "epoch": 4.0038714672861015,
      "grad_norm": 19.241914749145508,
      "learning_rate": 6.6623650363487766e-06,
      "loss": 0.6533,
      "step": 10342
    },
    {
      "epoch": 4.004258614014711,
      "grad_norm": 37.67634201049805,
      "learning_rate": 6.661934873316987e-06,
      "loss": 1.4969,
      "step": 10343
    },
    {
      "epoch": 4.004645760743322,
      "grad_norm": 24.83036994934082,
      "learning_rate": 6.6615047102851986e-06,
      "loss": 1.062,
      "step": 10344
    },
    {
      "epoch": 4.005032907471932,
      "grad_norm": 5.843042373657227,
      "learning_rate": 6.66107454725341e-06,
      "loss": 0.2324,
      "step": 10345
    },
    {
      "epoch": 4.005420054200542,
      "grad_norm": 18.249797821044922,
      "learning_rate": 6.6606443842216205e-06,
      "loss": 1.4497,
      "step": 10346
    },
    {
      "epoch": 4.005807200929152,
      "grad_norm": 7.362504482269287,
      "learning_rate": 6.660214221189831e-06,
      "loss": 1.0115,
      "step": 10347
    },
    {
      "epoch": 4.006194347657762,
      "grad_norm": 21.756732940673828,
      "learning_rate": 6.659784058158042e-06,
      "loss": 0.9576,
      "step": 10348
    },
    {
      "epoch": 4.0065814943863725,
      "grad_norm": 30.53158950805664,
      "learning_rate": 6.659353895126254e-06,
      "loss": 0.8166,
      "step": 10349
    },
    {
      "epoch": 4.006968641114983,
      "grad_norm": 25.20648956298828,
      "learning_rate": 6.6589237320944645e-06,
      "loss": 1.2899,
      "step": 10350
    },
    {
      "epoch": 4.007355787843593,
      "grad_norm": 17.194284439086914,
      "learning_rate": 6.658493569062675e-06,
      "loss": 1.4098,
      "step": 10351
    },
    {
      "epoch": 4.007742934572203,
      "grad_norm": 21.597726821899414,
      "learning_rate": 6.658063406030886e-06,
      "loss": 2.6622,
      "step": 10352
    },
    {
      "epoch": 4.008130081300813,
      "grad_norm": 18.657411575317383,
      "learning_rate": 6.657633242999098e-06,
      "loss": 0.7816,
      "step": 10353
    },
    {
      "epoch": 4.008517228029423,
      "grad_norm": 20.730188369750977,
      "learning_rate": 6.6572030799673085e-06,
      "loss": 2.3691,
      "step": 10354
    },
    {
      "epoch": 4.008904374758034,
      "grad_norm": 36.71954345703125,
      "learning_rate": 6.656772916935519e-06,
      "loss": 1.8746,
      "step": 10355
    },
    {
      "epoch": 4.009291521486643,
      "grad_norm": 16.39613914489746,
      "learning_rate": 6.65634275390373e-06,
      "loss": 1.1557,
      "step": 10356
    },
    {
      "epoch": 4.009678668215254,
      "grad_norm": 102.16096496582031,
      "learning_rate": 6.655912590871941e-06,
      "loss": 0.7155,
      "step": 10357
    },
    {
      "epoch": 4.0100658149438635,
      "grad_norm": 27.702665328979492,
      "learning_rate": 6.655482427840152e-06,
      "loss": 1.1604,
      "step": 10358
    },
    {
      "epoch": 4.010452961672474,
      "grad_norm": 19.64728355407715,
      "learning_rate": 6.655052264808363e-06,
      "loss": 1.8829,
      "step": 10359
    },
    {
      "epoch": 4.010840108401084,
      "grad_norm": 90.07774353027344,
      "learning_rate": 6.6546221017765744e-06,
      "loss": 2.1578,
      "step": 10360
    },
    {
      "epoch": 4.011227255129694,
      "grad_norm": 21.395597457885742,
      "learning_rate": 6.654191938744785e-06,
      "loss": 0.8729,
      "step": 10361
    },
    {
      "epoch": 4.011614401858305,
      "grad_norm": 37.63703155517578,
      "learning_rate": 6.6537617757129956e-06,
      "loss": 0.7791,
      "step": 10362
    },
    {
      "epoch": 4.012001548586914,
      "grad_norm": 10.54659652709961,
      "learning_rate": 6.653331612681206e-06,
      "loss": 0.67,
      "step": 10363
    },
    {
      "epoch": 4.012388695315525,
      "grad_norm": 26.164119720458984,
      "learning_rate": 6.652901449649418e-06,
      "loss": 1.4266,
      "step": 10364
    },
    {
      "epoch": 4.012775842044134,
      "grad_norm": 48.38701248168945,
      "learning_rate": 6.652471286617629e-06,
      "loss": 2.7471,
      "step": 10365
    },
    {
      "epoch": 4.013162988772745,
      "grad_norm": 32.39125061035156,
      "learning_rate": 6.6520411235858395e-06,
      "loss": 1.6544,
      "step": 10366
    },
    {
      "epoch": 4.013550135501355,
      "grad_norm": 37.26277542114258,
      "learning_rate": 6.65161096055405e-06,
      "loss": 1.2113,
      "step": 10367
    },
    {
      "epoch": 4.013937282229965,
      "grad_norm": 44.47184371948242,
      "learning_rate": 6.651180797522262e-06,
      "loss": 1.7601,
      "step": 10368
    },
    {
      "epoch": 4.0143244289585756,
      "grad_norm": 88.58058166503906,
      "learning_rate": 6.650750634490473e-06,
      "loss": 1.6658,
      "step": 10369
    },
    {
      "epoch": 4.014711575687185,
      "grad_norm": 25.333538055419922,
      "learning_rate": 6.6503204714586835e-06,
      "loss": 1.3491,
      "step": 10370
    },
    {
      "epoch": 4.015098722415796,
      "grad_norm": 37.94002914428711,
      "learning_rate": 6.649890308426894e-06,
      "loss": 1.3849,
      "step": 10371
    },
    {
      "epoch": 4.015485869144405,
      "grad_norm": 63.592628479003906,
      "learning_rate": 6.6494601453951055e-06,
      "loss": 1.1303,
      "step": 10372
    },
    {
      "epoch": 4.015873015873016,
      "grad_norm": 21.7169132232666,
      "learning_rate": 6.649029982363316e-06,
      "loss": 1.3319,
      "step": 10373
    },
    {
      "epoch": 4.016260162601626,
      "grad_norm": 52.61253356933594,
      "learning_rate": 6.6485998193315275e-06,
      "loss": 0.9718,
      "step": 10374
    },
    {
      "epoch": 4.016647309330236,
      "grad_norm": 19.47239112854004,
      "learning_rate": 6.648169656299738e-06,
      "loss": 0.9944,
      "step": 10375
    },
    {
      "epoch": 4.0170344560588465,
      "grad_norm": 48.851043701171875,
      "learning_rate": 6.6477394932679495e-06,
      "loss": 0.8012,
      "step": 10376
    },
    {
      "epoch": 4.017421602787456,
      "grad_norm": 30.435945510864258,
      "learning_rate": 6.64730933023616e-06,
      "loss": 2.3379,
      "step": 10377
    },
    {
      "epoch": 4.017808749516067,
      "grad_norm": 39.3044319152832,
      "learning_rate": 6.646879167204371e-06,
      "loss": 1.0374,
      "step": 10378
    },
    {
      "epoch": 4.018195896244677,
      "grad_norm": 18.387056350708008,
      "learning_rate": 6.646449004172581e-06,
      "loss": 0.4403,
      "step": 10379
    },
    {
      "epoch": 4.018583042973287,
      "grad_norm": 41.85292053222656,
      "learning_rate": 6.6460188411407934e-06,
      "loss": 1.6893,
      "step": 10380
    },
    {
      "epoch": 4.018970189701897,
      "grad_norm": 14.109975814819336,
      "learning_rate": 6.645588678109004e-06,
      "loss": 0.6181,
      "step": 10381
    },
    {
      "epoch": 4.019357336430507,
      "grad_norm": 22.266759872436523,
      "learning_rate": 6.6451585150772146e-06,
      "loss": 3.2426,
      "step": 10382
    },
    {
      "epoch": 4.019744483159117,
      "grad_norm": 17.35917091369629,
      "learning_rate": 6.644728352045425e-06,
      "loss": 1.0029,
      "step": 10383
    },
    {
      "epoch": 4.020131629887728,
      "grad_norm": 13.067508697509766,
      "learning_rate": 6.644298189013637e-06,
      "loss": 0.4107,
      "step": 10384
    },
    {
      "epoch": 4.0205187766163375,
      "grad_norm": 29.56173324584961,
      "learning_rate": 6.643868025981848e-06,
      "loss": 0.7798,
      "step": 10385
    },
    {
      "epoch": 4.020905923344948,
      "grad_norm": 105.85005950927734,
      "learning_rate": 6.6434378629500585e-06,
      "loss": 2.1563,
      "step": 10386
    },
    {
      "epoch": 4.021293070073558,
      "grad_norm": 41.81902313232422,
      "learning_rate": 6.643007699918269e-06,
      "loss": 1.5567,
      "step": 10387
    },
    {
      "epoch": 4.021680216802168,
      "grad_norm": 24.998592376708984,
      "learning_rate": 6.6425775368864805e-06,
      "loss": 1.4639,
      "step": 10388
    },
    {
      "epoch": 4.022067363530778,
      "grad_norm": 52.7799186706543,
      "learning_rate": 6.642147373854692e-06,
      "loss": 0.6618,
      "step": 10389
    },
    {
      "epoch": 4.022454510259388,
      "grad_norm": 12.540985107421875,
      "learning_rate": 6.6417172108229025e-06,
      "loss": 0.4977,
      "step": 10390
    },
    {
      "epoch": 4.022841656987999,
      "grad_norm": 42.01810073852539,
      "learning_rate": 6.641287047791113e-06,
      "loss": 0.4784,
      "step": 10391
    },
    {
      "epoch": 4.023228803716608,
      "grad_norm": 23.39694595336914,
      "learning_rate": 6.6408568847593245e-06,
      "loss": 1.7792,
      "step": 10392
    },
    {
      "epoch": 4.023615950445219,
      "grad_norm": 22.58444595336914,
      "learning_rate": 6.640426721727535e-06,
      "loss": 1.0806,
      "step": 10393
    },
    {
      "epoch": 4.0240030971738285,
      "grad_norm": 43.000640869140625,
      "learning_rate": 6.639996558695746e-06,
      "loss": 0.8227,
      "step": 10394
    },
    {
      "epoch": 4.024390243902439,
      "grad_norm": 48.80619430541992,
      "learning_rate": 6.639566395663957e-06,
      "loss": 1.8414,
      "step": 10395
    },
    {
      "epoch": 4.02477739063105,
      "grad_norm": 25.572786331176758,
      "learning_rate": 6.6391362326321685e-06,
      "loss": 1.5183,
      "step": 10396
    },
    {
      "epoch": 4.025164537359659,
      "grad_norm": 37.09583282470703,
      "learning_rate": 6.638706069600379e-06,
      "loss": 1.2341,
      "step": 10397
    },
    {
      "epoch": 4.02555168408827,
      "grad_norm": 11.588626861572266,
      "learning_rate": 6.63827590656859e-06,
      "loss": 0.7182,
      "step": 10398
    },
    {
      "epoch": 4.025938830816879,
      "grad_norm": 10.844545364379883,
      "learning_rate": 6.6378457435368e-06,
      "loss": 0.5434,
      "step": 10399
    },
    {
      "epoch": 4.02632597754549,
      "grad_norm": 28.928075790405273,
      "learning_rate": 6.6374155805050124e-06,
      "loss": 2.1014,
      "step": 10400
    },
    {
      "epoch": 4.026713124274099,
      "grad_norm": 46.4918212890625,
      "learning_rate": 6.636985417473223e-06,
      "loss": 2.5293,
      "step": 10401
    },
    {
      "epoch": 4.02710027100271,
      "grad_norm": 49.50349807739258,
      "learning_rate": 6.6365552544414336e-06,
      "loss": 1.5478,
      "step": 10402
    },
    {
      "epoch": 4.0274874177313205,
      "grad_norm": 58.0376091003418,
      "learning_rate": 6.636125091409645e-06,
      "loss": 1.4434,
      "step": 10403
    },
    {
      "epoch": 4.02787456445993,
      "grad_norm": 7.094544887542725,
      "learning_rate": 6.635694928377856e-06,
      "loss": 0.8975,
      "step": 10404
    },
    {
      "epoch": 4.028261711188541,
      "grad_norm": 47.1091423034668,
      "learning_rate": 6.635264765346067e-06,
      "loss": 2.1995,
      "step": 10405
    },
    {
      "epoch": 4.02864885791715,
      "grad_norm": 28.43804931640625,
      "learning_rate": 6.6348346023142775e-06,
      "loss": 1.7148,
      "step": 10406
    },
    {
      "epoch": 4.029036004645761,
      "grad_norm": 20.991626739501953,
      "learning_rate": 6.634404439282489e-06,
      "loss": 1.7579,
      "step": 10407
    },
    {
      "epoch": 4.029423151374371,
      "grad_norm": 26.240816116333008,
      "learning_rate": 6.6339742762506995e-06,
      "loss": 1.4056,
      "step": 10408
    },
    {
      "epoch": 4.029810298102981,
      "grad_norm": 51.03268051147461,
      "learning_rate": 6.63354411321891e-06,
      "loss": 3.6304,
      "step": 10409
    },
    {
      "epoch": 4.030197444831591,
      "grad_norm": 21.09796905517578,
      "learning_rate": 6.6331139501871215e-06,
      "loss": 1.2649,
      "step": 10410
    },
    {
      "epoch": 4.030584591560201,
      "grad_norm": 31.756732940673828,
      "learning_rate": 6.632683787155333e-06,
      "loss": 0.8253,
      "step": 10411
    },
    {
      "epoch": 4.0309717382888115,
      "grad_norm": 9.85543441772461,
      "learning_rate": 6.6322536241235435e-06,
      "loss": 0.3033,
      "step": 10412
    },
    {
      "epoch": 4.031358885017422,
      "grad_norm": 17.06865119934082,
      "learning_rate": 6.631823461091754e-06,
      "loss": 1.4495,
      "step": 10413
    },
    {
      "epoch": 4.031746031746032,
      "grad_norm": 33.745487213134766,
      "learning_rate": 6.631393298059965e-06,
      "loss": 1.0112,
      "step": 10414
    },
    {
      "epoch": 4.032133178474642,
      "grad_norm": 18.94930648803711,
      "learning_rate": 6.630963135028177e-06,
      "loss": 1.2503,
      "step": 10415
    },
    {
      "epoch": 4.032520325203252,
      "grad_norm": 31.393850326538086,
      "learning_rate": 6.6305329719963875e-06,
      "loss": 1.1251,
      "step": 10416
    },
    {
      "epoch": 4.032907471931862,
      "grad_norm": 39.043338775634766,
      "learning_rate": 6.630102808964598e-06,
      "loss": 1.4401,
      "step": 10417
    },
    {
      "epoch": 4.033294618660472,
      "grad_norm": 33.11183547973633,
      "learning_rate": 6.629672645932809e-06,
      "loss": 1.5723,
      "step": 10418
    },
    {
      "epoch": 4.033681765389082,
      "grad_norm": 17.63037872314453,
      "learning_rate": 6.629242482901021e-06,
      "loss": 1.3755,
      "step": 10419
    },
    {
      "epoch": 4.034068912117693,
      "grad_norm": 23.63605308532715,
      "learning_rate": 6.628812319869231e-06,
      "loss": 1.079,
      "step": 10420
    },
    {
      "epoch": 4.0344560588463025,
      "grad_norm": 8.25665283203125,
      "learning_rate": 6.628382156837442e-06,
      "loss": 0.4598,
      "step": 10421
    },
    {
      "epoch": 4.034843205574913,
      "grad_norm": 15.32546329498291,
      "learning_rate": 6.6279519938056526e-06,
      "loss": 1.297,
      "step": 10422
    },
    {
      "epoch": 4.035230352303523,
      "grad_norm": 32.16753005981445,
      "learning_rate": 6.627521830773864e-06,
      "loss": 2.1951,
      "step": 10423
    },
    {
      "epoch": 4.035617499032133,
      "grad_norm": 18.715465545654297,
      "learning_rate": 6.6270916677420745e-06,
      "loss": 1.0428,
      "step": 10424
    },
    {
      "epoch": 4.036004645760744,
      "grad_norm": 18.468961715698242,
      "learning_rate": 6.626661504710286e-06,
      "loss": 0.9037,
      "step": 10425
    },
    {
      "epoch": 4.036391792489353,
      "grad_norm": 14.497517585754395,
      "learning_rate": 6.6262313416784965e-06,
      "loss": 0.7382,
      "step": 10426
    },
    {
      "epoch": 4.036778939217964,
      "grad_norm": 39.87112045288086,
      "learning_rate": 6.625801178646708e-06,
      "loss": 2.6639,
      "step": 10427
    },
    {
      "epoch": 4.0371660859465734,
      "grad_norm": 33.43737030029297,
      "learning_rate": 6.6253710156149185e-06,
      "loss": 2.4536,
      "step": 10428
    },
    {
      "epoch": 4.037553232675184,
      "grad_norm": 25.974916458129883,
      "learning_rate": 6.624940852583129e-06,
      "loss": 2.2867,
      "step": 10429
    },
    {
      "epoch": 4.0379403794037945,
      "grad_norm": 25.600910186767578,
      "learning_rate": 6.62451068955134e-06,
      "loss": 3.7176,
      "step": 10430
    },
    {
      "epoch": 4.038327526132404,
      "grad_norm": 20.07267189025879,
      "learning_rate": 6.624080526519552e-06,
      "loss": 0.6881,
      "step": 10431
    },
    {
      "epoch": 4.038714672861015,
      "grad_norm": 20.841053009033203,
      "learning_rate": 6.6236503634877625e-06,
      "loss": 0.5709,
      "step": 10432
    },
    {
      "epoch": 4.039101819589624,
      "grad_norm": 39.693294525146484,
      "learning_rate": 6.623220200455973e-06,
      "loss": 1.429,
      "step": 10433
    },
    {
      "epoch": 4.039488966318235,
      "grad_norm": 10.94625186920166,
      "learning_rate": 6.622790037424184e-06,
      "loss": 0.3366,
      "step": 10434
    },
    {
      "epoch": 4.039876113046844,
      "grad_norm": 30.611234664916992,
      "learning_rate": 6.622359874392396e-06,
      "loss": 1.3938,
      "step": 10435
    },
    {
      "epoch": 4.040263259775455,
      "grad_norm": 21.47251319885254,
      "learning_rate": 6.6219297113606065e-06,
      "loss": 1.0827,
      "step": 10436
    },
    {
      "epoch": 4.040650406504065,
      "grad_norm": 51.24595642089844,
      "learning_rate": 6.621499548328817e-06,
      "loss": 2.0999,
      "step": 10437
    },
    {
      "epoch": 4.041037553232675,
      "grad_norm": 23.220497131347656,
      "learning_rate": 6.621069385297028e-06,
      "loss": 1.1783,
      "step": 10438
    },
    {
      "epoch": 4.0414246999612855,
      "grad_norm": 20.297258377075195,
      "learning_rate": 6.620639222265239e-06,
      "loss": 1.7913,
      "step": 10439
    },
    {
      "epoch": 4.041811846689895,
      "grad_norm": 35.349884033203125,
      "learning_rate": 6.62020905923345e-06,
      "loss": 1.2279,
      "step": 10440
    },
    {
      "epoch": 4.042198993418506,
      "grad_norm": 7.663334369659424,
      "learning_rate": 6.619778896201661e-06,
      "loss": 0.2839,
      "step": 10441
    },
    {
      "epoch": 4.042586140147116,
      "grad_norm": 35.27507400512695,
      "learning_rate": 6.619348733169872e-06,
      "loss": 0.8603,
      "step": 10442
    },
    {
      "epoch": 4.042973286875726,
      "grad_norm": 28.178544998168945,
      "learning_rate": 6.618918570138083e-06,
      "loss": 0.6572,
      "step": 10443
    },
    {
      "epoch": 4.043360433604336,
      "grad_norm": 61.926326751708984,
      "learning_rate": 6.6184884071062935e-06,
      "loss": 1.6584,
      "step": 10444
    },
    {
      "epoch": 4.043747580332946,
      "grad_norm": 27.620206832885742,
      "learning_rate": 6.618058244074504e-06,
      "loss": 1.0118,
      "step": 10445
    },
    {
      "epoch": 4.044134727061556,
      "grad_norm": 14.309676170349121,
      "learning_rate": 6.617628081042716e-06,
      "loss": 0.4737,
      "step": 10446
    },
    {
      "epoch": 4.044521873790166,
      "grad_norm": 21.47464942932129,
      "learning_rate": 6.617197918010927e-06,
      "loss": 1.5216,
      "step": 10447
    },
    {
      "epoch": 4.0449090205187765,
      "grad_norm": 40.572322845458984,
      "learning_rate": 6.6167677549791375e-06,
      "loss": 0.7218,
      "step": 10448
    },
    {
      "epoch": 4.045296167247387,
      "grad_norm": 23.378162384033203,
      "learning_rate": 6.616337591947348e-06,
      "loss": 0.5596,
      "step": 10449
    },
    {
      "epoch": 4.045683313975997,
      "grad_norm": 20.762462615966797,
      "learning_rate": 6.61590742891556e-06,
      "loss": 0.6575,
      "step": 10450
    },
    {
      "epoch": 4.046070460704607,
      "grad_norm": 13.803275108337402,
      "learning_rate": 6.615477265883771e-06,
      "loss": 1.3843,
      "step": 10451
    },
    {
      "epoch": 4.046457607433217,
      "grad_norm": 34.436859130859375,
      "learning_rate": 6.6150471028519815e-06,
      "loss": 1.6235,
      "step": 10452
    },
    {
      "epoch": 4.046844754161827,
      "grad_norm": 51.51091003417969,
      "learning_rate": 6.614616939820192e-06,
      "loss": 1.5163,
      "step": 10453
    },
    {
      "epoch": 4.047231900890438,
      "grad_norm": 28.438140869140625,
      "learning_rate": 6.6141867767884035e-06,
      "loss": 1.1722,
      "step": 10454
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 18.721254348754883,
      "learning_rate": 6.613756613756615e-06,
      "loss": 1.2195,
      "step": 10455
    },
    {
      "epoch": 4.048006194347658,
      "grad_norm": 21.146440505981445,
      "learning_rate": 6.6133264507248254e-06,
      "loss": 1.7831,
      "step": 10456
    },
    {
      "epoch": 4.048393341076268,
      "grad_norm": 15.81021499633789,
      "learning_rate": 6.612896287693036e-06,
      "loss": 0.7523,
      "step": 10457
    },
    {
      "epoch": 4.048780487804878,
      "grad_norm": 6.668787479400635,
      "learning_rate": 6.6124661246612474e-06,
      "loss": 0.2486,
      "step": 10458
    },
    {
      "epoch": 4.049167634533489,
      "grad_norm": 17.12196159362793,
      "learning_rate": 6.612035961629458e-06,
      "loss": 1.0084,
      "step": 10459
    },
    {
      "epoch": 4.049554781262098,
      "grad_norm": 20.786449432373047,
      "learning_rate": 6.6116057985976686e-06,
      "loss": 1.8438,
      "step": 10460
    },
    {
      "epoch": 4.049941927990709,
      "grad_norm": 20.38715362548828,
      "learning_rate": 6.61117563556588e-06,
      "loss": 1.9809,
      "step": 10461
    },
    {
      "epoch": 4.050329074719318,
      "grad_norm": 27.752071380615234,
      "learning_rate": 6.610745472534091e-06,
      "loss": 2.7549,
      "step": 10462
    },
    {
      "epoch": 4.050716221447929,
      "grad_norm": 11.653989791870117,
      "learning_rate": 6.610315309502302e-06,
      "loss": 0.9091,
      "step": 10463
    },
    {
      "epoch": 4.0511033681765385,
      "grad_norm": 24.001768112182617,
      "learning_rate": 6.6098851464705125e-06,
      "loss": 1.1352,
      "step": 10464
    },
    {
      "epoch": 4.051490514905149,
      "grad_norm": 29.500154495239258,
      "learning_rate": 6.609454983438723e-06,
      "loss": 0.8876,
      "step": 10465
    },
    {
      "epoch": 4.0518776616337595,
      "grad_norm": 15.894253730773926,
      "learning_rate": 6.609024820406935e-06,
      "loss": 1.3367,
      "step": 10466
    },
    {
      "epoch": 4.052264808362369,
      "grad_norm": 16.87305450439453,
      "learning_rate": 6.608594657375146e-06,
      "loss": 1.5686,
      "step": 10467
    },
    {
      "epoch": 4.05265195509098,
      "grad_norm": 36.48143005371094,
      "learning_rate": 6.6081644943433565e-06,
      "loss": 1.148,
      "step": 10468
    },
    {
      "epoch": 4.053039101819589,
      "grad_norm": 26.030820846557617,
      "learning_rate": 6.607734331311567e-06,
      "loss": 0.5742,
      "step": 10469
    },
    {
      "epoch": 4.0534262485482,
      "grad_norm": 15.500965118408203,
      "learning_rate": 6.607304168279779e-06,
      "loss": 0.96,
      "step": 10470
    },
    {
      "epoch": 4.05381339527681,
      "grad_norm": 39.41911697387695,
      "learning_rate": 6.60687400524799e-06,
      "loss": 0.9411,
      "step": 10471
    },
    {
      "epoch": 4.05420054200542,
      "grad_norm": 28.84796714782715,
      "learning_rate": 6.6064438422162005e-06,
      "loss": 1.6697,
      "step": 10472
    },
    {
      "epoch": 4.05458768873403,
      "grad_norm": 10.713770866394043,
      "learning_rate": 6.606013679184411e-06,
      "loss": 0.6972,
      "step": 10473
    },
    {
      "epoch": 4.05497483546264,
      "grad_norm": 28.418838500976562,
      "learning_rate": 6.6055835161526225e-06,
      "loss": 1.0764,
      "step": 10474
    },
    {
      "epoch": 4.0553619821912505,
      "grad_norm": 11.728265762329102,
      "learning_rate": 6.605153353120833e-06,
      "loss": 0.4446,
      "step": 10475
    },
    {
      "epoch": 4.055749128919861,
      "grad_norm": 22.263954162597656,
      "learning_rate": 6.6047231900890444e-06,
      "loss": 0.9508,
      "step": 10476
    },
    {
      "epoch": 4.056136275648471,
      "grad_norm": 14.503880500793457,
      "learning_rate": 6.604293027057255e-06,
      "loss": 0.8835,
      "step": 10477
    },
    {
      "epoch": 4.056523422377081,
      "grad_norm": 35.089134216308594,
      "learning_rate": 6.6038628640254664e-06,
      "loss": 1.5428,
      "step": 10478
    },
    {
      "epoch": 4.056910569105691,
      "grad_norm": 44.12746047973633,
      "learning_rate": 6.603432700993677e-06,
      "loss": 1.0044,
      "step": 10479
    },
    {
      "epoch": 4.057297715834301,
      "grad_norm": 34.97063064575195,
      "learning_rate": 6.6030025379618876e-06,
      "loss": 1.1289,
      "step": 10480
    },
    {
      "epoch": 4.057684862562911,
      "grad_norm": 10.230464935302734,
      "learning_rate": 6.602572374930098e-06,
      "loss": 1.2213,
      "step": 10481
    },
    {
      "epoch": 4.0580720092915215,
      "grad_norm": 16.952699661254883,
      "learning_rate": 6.60214221189831e-06,
      "loss": 0.8429,
      "step": 10482
    },
    {
      "epoch": 4.058459156020132,
      "grad_norm": 56.6427116394043,
      "learning_rate": 6.601712048866521e-06,
      "loss": 0.9729,
      "step": 10483
    },
    {
      "epoch": 4.058846302748742,
      "grad_norm": 13.955362319946289,
      "learning_rate": 6.6012818858347315e-06,
      "loss": 0.7452,
      "step": 10484
    },
    {
      "epoch": 4.059233449477352,
      "grad_norm": 7.603420257568359,
      "learning_rate": 6.600851722802944e-06,
      "loss": 0.343,
      "step": 10485
    },
    {
      "epoch": 4.059620596205962,
      "grad_norm": 67.62556457519531,
      "learning_rate": 6.600421559771154e-06,
      "loss": 2.0363,
      "step": 10486
    },
    {
      "epoch": 4.060007742934572,
      "grad_norm": 41.73958969116211,
      "learning_rate": 6.599991396739365e-06,
      "loss": 1.0863,
      "step": 10487
    },
    {
      "epoch": 4.060394889663183,
      "grad_norm": 36.45382308959961,
      "learning_rate": 6.5995612337075755e-06,
      "loss": 1.2956,
      "step": 10488
    },
    {
      "epoch": 4.060782036391792,
      "grad_norm": 6.515296936035156,
      "learning_rate": 6.599131070675787e-06,
      "loss": 0.2599,
      "step": 10489
    },
    {
      "epoch": 4.061169183120403,
      "grad_norm": 24.84810447692871,
      "learning_rate": 6.5987009076439975e-06,
      "loss": 1.756,
      "step": 10490
    },
    {
      "epoch": 4.0615563298490125,
      "grad_norm": 14.910686492919922,
      "learning_rate": 6.598270744612209e-06,
      "loss": 0.5831,
      "step": 10491
    },
    {
      "epoch": 4.061943476577623,
      "grad_norm": 5.214463233947754,
      "learning_rate": 6.5978405815804195e-06,
      "loss": 0.0942,
      "step": 10492
    },
    {
      "epoch": 4.062330623306233,
      "grad_norm": 19.94829559326172,
      "learning_rate": 6.597410418548631e-06,
      "loss": 1.2052,
      "step": 10493
    },
    {
      "epoch": 4.062717770034843,
      "grad_norm": 27.55438995361328,
      "learning_rate": 6.5969802555168415e-06,
      "loss": 1.3021,
      "step": 10494
    },
    {
      "epoch": 4.063104916763454,
      "grad_norm": 15.493454933166504,
      "learning_rate": 6.596550092485052e-06,
      "loss": 0.7787,
      "step": 10495
    },
    {
      "epoch": 4.063492063492063,
      "grad_norm": 58.0990104675293,
      "learning_rate": 6.596119929453263e-06,
      "loss": 0.8886,
      "step": 10496
    },
    {
      "epoch": 4.063879210220674,
      "grad_norm": 29.27206802368164,
      "learning_rate": 6.595689766421475e-06,
      "loss": 2.0735,
      "step": 10497
    },
    {
      "epoch": 4.064266356949283,
      "grad_norm": 42.661277770996094,
      "learning_rate": 6.5952596033896854e-06,
      "loss": 2.1697,
      "step": 10498
    },
    {
      "epoch": 4.064653503677894,
      "grad_norm": 53.636940002441406,
      "learning_rate": 6.594829440357896e-06,
      "loss": 1.4455,
      "step": 10499
    },
    {
      "epoch": 4.065040650406504,
      "grad_norm": 11.521986961364746,
      "learning_rate": 6.5943992773261066e-06,
      "loss": 0.6117,
      "step": 10500
    },
    {
      "epoch": 4.065427797135114,
      "grad_norm": 18.35468101501465,
      "learning_rate": 6.593969114294319e-06,
      "loss": 1.7005,
      "step": 10501
    },
    {
      "epoch": 4.0658149438637246,
      "grad_norm": 43.10512161254883,
      "learning_rate": 6.593538951262529e-06,
      "loss": 1.7318,
      "step": 10502
    },
    {
      "epoch": 4.066202090592334,
      "grad_norm": 46.17847442626953,
      "learning_rate": 6.59310878823074e-06,
      "loss": 1.5774,
      "step": 10503
    },
    {
      "epoch": 4.066589237320945,
      "grad_norm": 31.822063446044922,
      "learning_rate": 6.5926786251989505e-06,
      "loss": 0.5654,
      "step": 10504
    },
    {
      "epoch": 4.066976384049555,
      "grad_norm": 34.34501266479492,
      "learning_rate": 6.592248462167162e-06,
      "loss": 2.4064,
      "step": 10505
    },
    {
      "epoch": 4.067363530778165,
      "grad_norm": 25.864383697509766,
      "learning_rate": 6.591818299135373e-06,
      "loss": 1.3192,
      "step": 10506
    },
    {
      "epoch": 4.067750677506775,
      "grad_norm": 18.45533561706543,
      "learning_rate": 6.591388136103584e-06,
      "loss": 0.9727,
      "step": 10507
    },
    {
      "epoch": 4.068137824235385,
      "grad_norm": 22.810869216918945,
      "learning_rate": 6.5909579730717945e-06,
      "loss": 1.4722,
      "step": 10508
    },
    {
      "epoch": 4.0685249709639955,
      "grad_norm": 16.669248580932617,
      "learning_rate": 6.590527810040006e-06,
      "loss": 1.3273,
      "step": 10509
    },
    {
      "epoch": 4.068912117692605,
      "grad_norm": 26.669857025146484,
      "learning_rate": 6.5900976470082165e-06,
      "loss": 1.7391,
      "step": 10510
    },
    {
      "epoch": 4.069299264421216,
      "grad_norm": 26.32155990600586,
      "learning_rate": 6.589667483976427e-06,
      "loss": 3.1856,
      "step": 10511
    },
    {
      "epoch": 4.069686411149826,
      "grad_norm": 16.256187438964844,
      "learning_rate": 6.5892373209446385e-06,
      "loss": 0.7618,
      "step": 10512
    },
    {
      "epoch": 4.070073557878436,
      "grad_norm": 31.042295455932617,
      "learning_rate": 6.58880715791285e-06,
      "loss": 1.1163,
      "step": 10513
    },
    {
      "epoch": 4.070460704607046,
      "grad_norm": 33.51019287109375,
      "learning_rate": 6.5883769948810605e-06,
      "loss": 1.04,
      "step": 10514
    },
    {
      "epoch": 4.070847851335656,
      "grad_norm": 55.6784553527832,
      "learning_rate": 6.587946831849271e-06,
      "loss": 1.8315,
      "step": 10515
    },
    {
      "epoch": 4.071234998064266,
      "grad_norm": 13.366325378417969,
      "learning_rate": 6.587516668817482e-06,
      "loss": 0.7668,
      "step": 10516
    },
    {
      "epoch": 4.071622144792877,
      "grad_norm": 39.2994384765625,
      "learning_rate": 6.587086505785694e-06,
      "loss": 0.3174,
      "step": 10517
    },
    {
      "epoch": 4.0720092915214865,
      "grad_norm": 12.845640182495117,
      "learning_rate": 6.5866563427539044e-06,
      "loss": 1.0278,
      "step": 10518
    },
    {
      "epoch": 4.072396438250097,
      "grad_norm": 16.550941467285156,
      "learning_rate": 6.586226179722115e-06,
      "loss": 0.7537,
      "step": 10519
    },
    {
      "epoch": 4.072783584978707,
      "grad_norm": 75.38121795654297,
      "learning_rate": 6.5857960166903256e-06,
      "loss": 1.614,
      "step": 10520
    },
    {
      "epoch": 4.073170731707317,
      "grad_norm": 18.767013549804688,
      "learning_rate": 6.585365853658538e-06,
      "loss": 1.0496,
      "step": 10521
    },
    {
      "epoch": 4.073557878435928,
      "grad_norm": 37.03513717651367,
      "learning_rate": 6.584935690626748e-06,
      "loss": 1.0962,
      "step": 10522
    },
    {
      "epoch": 4.073945025164537,
      "grad_norm": 34.83388137817383,
      "learning_rate": 6.584505527594959e-06,
      "loss": 1.5211,
      "step": 10523
    },
    {
      "epoch": 4.074332171893148,
      "grad_norm": 25.41817283630371,
      "learning_rate": 6.5840753645631695e-06,
      "loss": 1.6286,
      "step": 10524
    },
    {
      "epoch": 4.074719318621757,
      "grad_norm": 37.905662536621094,
      "learning_rate": 6.583645201531381e-06,
      "loss": 1.088,
      "step": 10525
    },
    {
      "epoch": 4.075106465350368,
      "grad_norm": 57.26005935668945,
      "learning_rate": 6.5832150384995915e-06,
      "loss": 0.7773,
      "step": 10526
    },
    {
      "epoch": 4.0754936120789775,
      "grad_norm": 22.951393127441406,
      "learning_rate": 6.582784875467803e-06,
      "loss": 0.4502,
      "step": 10527
    },
    {
      "epoch": 4.075880758807588,
      "grad_norm": 23.736743927001953,
      "learning_rate": 6.582354712436014e-06,
      "loss": 0.8159,
      "step": 10528
    },
    {
      "epoch": 4.0762679055361986,
      "grad_norm": 48.67491912841797,
      "learning_rate": 6.581924549404225e-06,
      "loss": 1.4149,
      "step": 10529
    },
    {
      "epoch": 4.076655052264808,
      "grad_norm": 20.75417137145996,
      "learning_rate": 6.5814943863724355e-06,
      "loss": 1.4901,
      "step": 10530
    },
    {
      "epoch": 4.077042198993419,
      "grad_norm": 27.45579719543457,
      "learning_rate": 6.581064223340646e-06,
      "loss": 0.778,
      "step": 10531
    },
    {
      "epoch": 4.077429345722028,
      "grad_norm": 45.3887939453125,
      "learning_rate": 6.580634060308858e-06,
      "loss": 1.4655,
      "step": 10532
    },
    {
      "epoch": 4.077816492450639,
      "grad_norm": 22.371442794799805,
      "learning_rate": 6.580203897277069e-06,
      "loss": 0.5238,
      "step": 10533
    },
    {
      "epoch": 4.078203639179249,
      "grad_norm": 17.180644989013672,
      "learning_rate": 6.5797737342452795e-06,
      "loss": 0.9221,
      "step": 10534
    },
    {
      "epoch": 4.078590785907859,
      "grad_norm": 16.1374568939209,
      "learning_rate": 6.57934357121349e-06,
      "loss": 1.2975,
      "step": 10535
    },
    {
      "epoch": 4.0789779326364695,
      "grad_norm": 42.5230598449707,
      "learning_rate": 6.578913408181702e-06,
      "loss": 1.2655,
      "step": 10536
    },
    {
      "epoch": 4.079365079365079,
      "grad_norm": 23.885868072509766,
      "learning_rate": 6.578483245149913e-06,
      "loss": 0.7777,
      "step": 10537
    },
    {
      "epoch": 4.07975222609369,
      "grad_norm": 34.724029541015625,
      "learning_rate": 6.5780530821181234e-06,
      "loss": 1.7502,
      "step": 10538
    },
    {
      "epoch": 4.080139372822299,
      "grad_norm": 9.786674499511719,
      "learning_rate": 6.577622919086334e-06,
      "loss": 0.2789,
      "step": 10539
    },
    {
      "epoch": 4.08052651955091,
      "grad_norm": 22.00484275817871,
      "learning_rate": 6.577192756054545e-06,
      "loss": 1.4147,
      "step": 10540
    },
    {
      "epoch": 4.08091366627952,
      "grad_norm": 37.29289245605469,
      "learning_rate": 6.576762593022756e-06,
      "loss": 0.5925,
      "step": 10541
    },
    {
      "epoch": 4.08130081300813,
      "grad_norm": 61.76142501831055,
      "learning_rate": 6.576332429990967e-06,
      "loss": 2.3106,
      "step": 10542
    },
    {
      "epoch": 4.08168795973674,
      "grad_norm": 23.163042068481445,
      "learning_rate": 6.575902266959178e-06,
      "loss": 1.2151,
      "step": 10543
    },
    {
      "epoch": 4.08207510646535,
      "grad_norm": 53.10003662109375,
      "learning_rate": 6.575472103927389e-06,
      "loss": 0.757,
      "step": 10544
    },
    {
      "epoch": 4.0824622531939605,
      "grad_norm": 77.07112121582031,
      "learning_rate": 6.5750419408956e-06,
      "loss": 1.1536,
      "step": 10545
    },
    {
      "epoch": 4.082849399922571,
      "grad_norm": 17.922449111938477,
      "learning_rate": 6.5746117778638105e-06,
      "loss": 1.3768,
      "step": 10546
    },
    {
      "epoch": 4.083236546651181,
      "grad_norm": 25.80972671508789,
      "learning_rate": 6.574181614832021e-06,
      "loss": 2.6103,
      "step": 10547
    },
    {
      "epoch": 4.083623693379791,
      "grad_norm": 108.19904327392578,
      "learning_rate": 6.573751451800233e-06,
      "loss": 2.3678,
      "step": 10548
    },
    {
      "epoch": 4.084010840108401,
      "grad_norm": 29.209484100341797,
      "learning_rate": 6.573321288768444e-06,
      "loss": 3.0236,
      "step": 10549
    },
    {
      "epoch": 4.084397986837011,
      "grad_norm": 6.847677230834961,
      "learning_rate": 6.5728911257366545e-06,
      "loss": 0.2832,
      "step": 10550
    },
    {
      "epoch": 4.084785133565622,
      "grad_norm": 27.799989700317383,
      "learning_rate": 6.572460962704865e-06,
      "loss": 1.1572,
      "step": 10551
    },
    {
      "epoch": 4.085172280294231,
      "grad_norm": 46.27732467651367,
      "learning_rate": 6.572030799673077e-06,
      "loss": 1.811,
      "step": 10552
    },
    {
      "epoch": 4.085559427022842,
      "grad_norm": 19.778759002685547,
      "learning_rate": 6.571600636641288e-06,
      "loss": 0.8291,
      "step": 10553
    },
    {
      "epoch": 4.0859465737514515,
      "grad_norm": 43.312320709228516,
      "learning_rate": 6.5711704736094985e-06,
      "loss": 1.3386,
      "step": 10554
    },
    {
      "epoch": 4.086333720480062,
      "grad_norm": 23.9606876373291,
      "learning_rate": 6.570740310577709e-06,
      "loss": 1.3471,
      "step": 10555
    },
    {
      "epoch": 4.086720867208672,
      "grad_norm": 27.845661163330078,
      "learning_rate": 6.5703101475459204e-06,
      "loss": 0.6144,
      "step": 10556
    },
    {
      "epoch": 4.087108013937282,
      "grad_norm": 36.60721969604492,
      "learning_rate": 6.569879984514132e-06,
      "loss": 0.9607,
      "step": 10557
    },
    {
      "epoch": 4.087495160665893,
      "grad_norm": 12.853418350219727,
      "learning_rate": 6.569449821482342e-06,
      "loss": 0.6927,
      "step": 10558
    },
    {
      "epoch": 4.087882307394502,
      "grad_norm": 33.9538459777832,
      "learning_rate": 6.569019658450553e-06,
      "loss": 0.6793,
      "step": 10559
    },
    {
      "epoch": 4.088269454123113,
      "grad_norm": 95.56690216064453,
      "learning_rate": 6.568589495418764e-06,
      "loss": 1.2122,
      "step": 10560
    },
    {
      "epoch": 4.0886566008517224,
      "grad_norm": 14.669111251831055,
      "learning_rate": 6.568159332386975e-06,
      "loss": 0.8338,
      "step": 10561
    },
    {
      "epoch": 4.089043747580333,
      "grad_norm": 6.6656670570373535,
      "learning_rate": 6.5677291693551855e-06,
      "loss": 1.0212,
      "step": 10562
    },
    {
      "epoch": 4.0894308943089435,
      "grad_norm": 97.41302490234375,
      "learning_rate": 6.567299006323397e-06,
      "loss": 2.4038,
      "step": 10563
    },
    {
      "epoch": 4.089818041037553,
      "grad_norm": 27.623794555664062,
      "learning_rate": 6.566868843291608e-06,
      "loss": 1.509,
      "step": 10564
    },
    {
      "epoch": 4.090205187766164,
      "grad_norm": 22.839771270751953,
      "learning_rate": 6.566438680259819e-06,
      "loss": 1.102,
      "step": 10565
    },
    {
      "epoch": 4.090592334494773,
      "grad_norm": 16.575536727905273,
      "learning_rate": 6.5660085172280295e-06,
      "loss": 0.8074,
      "step": 10566
    },
    {
      "epoch": 4.090979481223384,
      "grad_norm": 12.002748489379883,
      "learning_rate": 6.565578354196242e-06,
      "loss": 0.5367,
      "step": 10567
    },
    {
      "epoch": 4.091366627951994,
      "grad_norm": 8.54088020324707,
      "learning_rate": 6.565148191164452e-06,
      "loss": 0.3326,
      "step": 10568
    },
    {
      "epoch": 4.091753774680604,
      "grad_norm": 47.22233963012695,
      "learning_rate": 6.564718028132663e-06,
      "loss": 1.7581,
      "step": 10569
    },
    {
      "epoch": 4.092140921409214,
      "grad_norm": 28.69434356689453,
      "learning_rate": 6.5642878651008735e-06,
      "loss": 1.7064,
      "step": 10570
    },
    {
      "epoch": 4.092528068137824,
      "grad_norm": 31.247024536132812,
      "learning_rate": 6.563857702069085e-06,
      "loss": 1.8344,
      "step": 10571
    },
    {
      "epoch": 4.0929152148664345,
      "grad_norm": 11.298272132873535,
      "learning_rate": 6.563427539037296e-06,
      "loss": 0.6889,
      "step": 10572
    },
    {
      "epoch": 4.093302361595044,
      "grad_norm": 73.56614685058594,
      "learning_rate": 6.562997376005507e-06,
      "loss": 1.6485,
      "step": 10573
    },
    {
      "epoch": 4.093689508323655,
      "grad_norm": 23.272483825683594,
      "learning_rate": 6.5625672129737175e-06,
      "loss": 1.2174,
      "step": 10574
    },
    {
      "epoch": 4.094076655052265,
      "grad_norm": 27.947547912597656,
      "learning_rate": 6.562137049941929e-06,
      "loss": 0.8459,
      "step": 10575
    },
    {
      "epoch": 4.094463801780875,
      "grad_norm": 7.607845783233643,
      "learning_rate": 6.5617068869101394e-06,
      "loss": 0.5105,
      "step": 10576
    },
    {
      "epoch": 4.094850948509485,
      "grad_norm": 21.74372673034668,
      "learning_rate": 6.56127672387835e-06,
      "loss": 2.253,
      "step": 10577
    },
    {
      "epoch": 4.095238095238095,
      "grad_norm": 37.97816848754883,
      "learning_rate": 6.560846560846561e-06,
      "loss": 1.7237,
      "step": 10578
    },
    {
      "epoch": 4.095625241966705,
      "grad_norm": 34.233882904052734,
      "learning_rate": 6.560416397814773e-06,
      "loss": 1.1187,
      "step": 10579
    },
    {
      "epoch": 4.096012388695316,
      "grad_norm": 19.026403427124023,
      "learning_rate": 6.559986234782983e-06,
      "loss": 1.5408,
      "step": 10580
    },
    {
      "epoch": 4.0963995354239255,
      "grad_norm": 46.55437088012695,
      "learning_rate": 6.559556071751194e-06,
      "loss": 1.6804,
      "step": 10581
    },
    {
      "epoch": 4.096786682152536,
      "grad_norm": 30.348981857299805,
      "learning_rate": 6.5591259087194045e-06,
      "loss": 1.9984,
      "step": 10582
    },
    {
      "epoch": 4.097173828881146,
      "grad_norm": 46.55838394165039,
      "learning_rate": 6.558695745687617e-06,
      "loss": 0.5828,
      "step": 10583
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 33.123661041259766,
      "learning_rate": 6.558265582655827e-06,
      "loss": 1.0443,
      "step": 10584
    },
    {
      "epoch": 4.097948122338366,
      "grad_norm": 23.33457374572754,
      "learning_rate": 6.557835419624038e-06,
      "loss": 1.7606,
      "step": 10585
    },
    {
      "epoch": 4.098335269066976,
      "grad_norm": 18.196401596069336,
      "learning_rate": 6.5574052565922485e-06,
      "loss": 0.8254,
      "step": 10586
    },
    {
      "epoch": 4.098722415795587,
      "grad_norm": 19.708375930786133,
      "learning_rate": 6.556975093560461e-06,
      "loss": 0.3382,
      "step": 10587
    },
    {
      "epoch": 4.0991095625241964,
      "grad_norm": 23.99851417541504,
      "learning_rate": 6.556544930528671e-06,
      "loss": 1.3917,
      "step": 10588
    },
    {
      "epoch": 4.099496709252807,
      "grad_norm": 21.946395874023438,
      "learning_rate": 6.556114767496882e-06,
      "loss": 0.8607,
      "step": 10589
    },
    {
      "epoch": 4.099883855981417,
      "grad_norm": 26.768545150756836,
      "learning_rate": 6.5556846044650925e-06,
      "loss": 1.0859,
      "step": 10590
    },
    {
      "epoch": 4.100271002710027,
      "grad_norm": 14.672090530395508,
      "learning_rate": 6.555254441433304e-06,
      "loss": 1.327,
      "step": 10591
    },
    {
      "epoch": 4.100658149438638,
      "grad_norm": 14.216597557067871,
      "learning_rate": 6.5548242784015145e-06,
      "loss": 1.0438,
      "step": 10592
    },
    {
      "epoch": 4.101045296167247,
      "grad_norm": 8.840967178344727,
      "learning_rate": 6.554394115369726e-06,
      "loss": 0.297,
      "step": 10593
    },
    {
      "epoch": 4.101432442895858,
      "grad_norm": 7.0226640701293945,
      "learning_rate": 6.5539639523379364e-06,
      "loss": 0.4188,
      "step": 10594
    },
    {
      "epoch": 4.101819589624467,
      "grad_norm": 26.825632095336914,
      "learning_rate": 6.553533789306148e-06,
      "loss": 0.9703,
      "step": 10595
    },
    {
      "epoch": 4.102206736353078,
      "grad_norm": 18.219074249267578,
      "learning_rate": 6.5531036262743584e-06,
      "loss": 0.7347,
      "step": 10596
    },
    {
      "epoch": 4.102593883081688,
      "grad_norm": 35.941993713378906,
      "learning_rate": 6.552673463242569e-06,
      "loss": 0.6015,
      "step": 10597
    },
    {
      "epoch": 4.102981029810298,
      "grad_norm": 39.93326187133789,
      "learning_rate": 6.5522433002107796e-06,
      "loss": 1.1458,
      "step": 10598
    },
    {
      "epoch": 4.1033681765389085,
      "grad_norm": 37.18195343017578,
      "learning_rate": 6.551813137178992e-06,
      "loss": 1.5276,
      "step": 10599
    },
    {
      "epoch": 4.103755323267518,
      "grad_norm": 12.986169815063477,
      "learning_rate": 6.551382974147202e-06,
      "loss": 1.3573,
      "step": 10600
    },
    {
      "epoch": 4.104142469996129,
      "grad_norm": 13.116934776306152,
      "learning_rate": 6.550952811115413e-06,
      "loss": 0.8397,
      "step": 10601
    },
    {
      "epoch": 4.104529616724738,
      "grad_norm": 21.565567016601562,
      "learning_rate": 6.5505226480836235e-06,
      "loss": 2.004,
      "step": 10602
    },
    {
      "epoch": 4.104916763453349,
      "grad_norm": 68.94170379638672,
      "learning_rate": 6.550092485051836e-06,
      "loss": 1.6927,
      "step": 10603
    },
    {
      "epoch": 4.105303910181959,
      "grad_norm": 108.48635864257812,
      "learning_rate": 6.549662322020046e-06,
      "loss": 1.0989,
      "step": 10604
    },
    {
      "epoch": 4.105691056910569,
      "grad_norm": 14.667326927185059,
      "learning_rate": 6.549232158988257e-06,
      "loss": 0.963,
      "step": 10605
    },
    {
      "epoch": 4.106078203639179,
      "grad_norm": 37.01152801513672,
      "learning_rate": 6.5488019959564675e-06,
      "loss": 2.271,
      "step": 10606
    },
    {
      "epoch": 4.106465350367789,
      "grad_norm": 35.21889114379883,
      "learning_rate": 6.548371832924679e-06,
      "loss": 1.0823,
      "step": 10607
    },
    {
      "epoch": 4.1068524970963995,
      "grad_norm": 116.18541717529297,
      "learning_rate": 6.54794166989289e-06,
      "loss": 1.5694,
      "step": 10608
    },
    {
      "epoch": 4.10723964382501,
      "grad_norm": 21.5721492767334,
      "learning_rate": 6.547511506861101e-06,
      "loss": 0.905,
      "step": 10609
    },
    {
      "epoch": 4.10762679055362,
      "grad_norm": 13.204559326171875,
      "learning_rate": 6.547081343829312e-06,
      "loss": 0.6397,
      "step": 10610
    },
    {
      "epoch": 4.10801393728223,
      "grad_norm": 19.220285415649414,
      "learning_rate": 6.546651180797523e-06,
      "loss": 1.4861,
      "step": 10611
    },
    {
      "epoch": 4.10840108401084,
      "grad_norm": 1.7132681608200073,
      "learning_rate": 6.5462210177657335e-06,
      "loss": 0.054,
      "step": 10612
    },
    {
      "epoch": 4.10878823073945,
      "grad_norm": 18.762956619262695,
      "learning_rate": 6.545790854733944e-06,
      "loss": 1.2905,
      "step": 10613
    },
    {
      "epoch": 4.109175377468061,
      "grad_norm": 34.34843444824219,
      "learning_rate": 6.545360691702156e-06,
      "loss": 0.7358,
      "step": 10614
    },
    {
      "epoch": 4.1095625241966705,
      "grad_norm": 22.921188354492188,
      "learning_rate": 6.544930528670367e-06,
      "loss": 1.1524,
      "step": 10615
    },
    {
      "epoch": 4.109949670925281,
      "grad_norm": 41.03389358520508,
      "learning_rate": 6.5445003656385774e-06,
      "loss": 1.0662,
      "step": 10616
    },
    {
      "epoch": 4.110336817653891,
      "grad_norm": 32.61367416381836,
      "learning_rate": 6.544070202606788e-06,
      "loss": 1.2697,
      "step": 10617
    },
    {
      "epoch": 4.110723964382501,
      "grad_norm": 20.853940963745117,
      "learning_rate": 6.543640039575e-06,
      "loss": 1.1909,
      "step": 10618
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 13.854769706726074,
      "learning_rate": 6.543209876543211e-06,
      "loss": 0.8293,
      "step": 10619
    },
    {
      "epoch": 4.111498257839721,
      "grad_norm": 42.87307357788086,
      "learning_rate": 6.542779713511421e-06,
      "loss": 0.4816,
      "step": 10620
    },
    {
      "epoch": 4.111885404568332,
      "grad_norm": 32.141151428222656,
      "learning_rate": 6.542349550479632e-06,
      "loss": 1.2217,
      "step": 10621
    },
    {
      "epoch": 4.112272551296941,
      "grad_norm": 19.870065689086914,
      "learning_rate": 6.541919387447843e-06,
      "loss": 0.7758,
      "step": 10622
    },
    {
      "epoch": 4.112659698025552,
      "grad_norm": 14.233181953430176,
      "learning_rate": 6.541489224416055e-06,
      "loss": 0.9556,
      "step": 10623
    },
    {
      "epoch": 4.1130468447541615,
      "grad_norm": 25.434585571289062,
      "learning_rate": 6.541059061384265e-06,
      "loss": 2.232,
      "step": 10624
    },
    {
      "epoch": 4.113433991482772,
      "grad_norm": 32.8648567199707,
      "learning_rate": 6.540628898352476e-06,
      "loss": 1.13,
      "step": 10625
    },
    {
      "epoch": 4.1138211382113825,
      "grad_norm": 31.505451202392578,
      "learning_rate": 6.540198735320687e-06,
      "loss": 1.5036,
      "step": 10626
    },
    {
      "epoch": 4.114208284939992,
      "grad_norm": 37.884979248046875,
      "learning_rate": 6.539768572288898e-06,
      "loss": 1.7241,
      "step": 10627
    },
    {
      "epoch": 4.114595431668603,
      "grad_norm": 11.394989967346191,
      "learning_rate": 6.5393384092571085e-06,
      "loss": 0.8401,
      "step": 10628
    },
    {
      "epoch": 4.114982578397212,
      "grad_norm": 16.498592376708984,
      "learning_rate": 6.53890824622532e-06,
      "loss": 1.3902,
      "step": 10629
    },
    {
      "epoch": 4.115369725125823,
      "grad_norm": 20.74278450012207,
      "learning_rate": 6.538478083193531e-06,
      "loss": 1.6392,
      "step": 10630
    },
    {
      "epoch": 4.115756871854432,
      "grad_norm": 55.9984245300293,
      "learning_rate": 6.538047920161742e-06,
      "loss": 2.5646,
      "step": 10631
    },
    {
      "epoch": 4.116144018583043,
      "grad_norm": 14.74122142791748,
      "learning_rate": 6.5376177571299525e-06,
      "loss": 1.5136,
      "step": 10632
    },
    {
      "epoch": 4.116531165311653,
      "grad_norm": 40.81220245361328,
      "learning_rate": 6.537187594098163e-06,
      "loss": 0.848,
      "step": 10633
    },
    {
      "epoch": 4.116918312040263,
      "grad_norm": 159.1196746826172,
      "learning_rate": 6.536757431066375e-06,
      "loss": 1.1262,
      "step": 10634
    },
    {
      "epoch": 4.1173054587688735,
      "grad_norm": 33.27029800415039,
      "learning_rate": 6.536327268034586e-06,
      "loss": 1.3456,
      "step": 10635
    },
    {
      "epoch": 4.117692605497483,
      "grad_norm": 13.23840618133545,
      "learning_rate": 6.5358971050027964e-06,
      "loss": 0.4267,
      "step": 10636
    },
    {
      "epoch": 4.118079752226094,
      "grad_norm": 16.5783634185791,
      "learning_rate": 6.535466941971007e-06,
      "loss": 0.7102,
      "step": 10637
    },
    {
      "epoch": 4.118466898954704,
      "grad_norm": 13.077364921569824,
      "learning_rate": 6.535036778939219e-06,
      "loss": 0.8892,
      "step": 10638
    },
    {
      "epoch": 4.118854045683314,
      "grad_norm": 61.56575393676758,
      "learning_rate": 6.53460661590743e-06,
      "loss": 1.1615,
      "step": 10639
    },
    {
      "epoch": 4.119241192411924,
      "grad_norm": 38.520877838134766,
      "learning_rate": 6.53417645287564e-06,
      "loss": 2.3243,
      "step": 10640
    },
    {
      "epoch": 4.119628339140534,
      "grad_norm": 71.19309997558594,
      "learning_rate": 6.533746289843851e-06,
      "loss": 1.0909,
      "step": 10641
    },
    {
      "epoch": 4.1200154858691445,
      "grad_norm": 37.90449142456055,
      "learning_rate": 6.533316126812062e-06,
      "loss": 1.8778,
      "step": 10642
    },
    {
      "epoch": 4.120402632597755,
      "grad_norm": 47.814910888671875,
      "learning_rate": 6.532885963780273e-06,
      "loss": 1.7619,
      "step": 10643
    },
    {
      "epoch": 4.120789779326365,
      "grad_norm": 70.77142333984375,
      "learning_rate": 6.532455800748484e-06,
      "loss": 1.4216,
      "step": 10644
    },
    {
      "epoch": 4.121176926054975,
      "grad_norm": 27.76554298400879,
      "learning_rate": 6.532025637716695e-06,
      "loss": 0.5803,
      "step": 10645
    },
    {
      "epoch": 4.121564072783585,
      "grad_norm": 25.287126541137695,
      "learning_rate": 6.531595474684906e-06,
      "loss": 1.1795,
      "step": 10646
    },
    {
      "epoch": 4.121951219512195,
      "grad_norm": 29.3587646484375,
      "learning_rate": 6.531165311653117e-06,
      "loss": 1.3783,
      "step": 10647
    },
    {
      "epoch": 4.122338366240805,
      "grad_norm": 19.291946411132812,
      "learning_rate": 6.5307351486213275e-06,
      "loss": 1.2276,
      "step": 10648
    },
    {
      "epoch": 4.122725512969415,
      "grad_norm": 26.934192657470703,
      "learning_rate": 6.53030498558954e-06,
      "loss": 1.2643,
      "step": 10649
    },
    {
      "epoch": 4.123112659698026,
      "grad_norm": 55.94363021850586,
      "learning_rate": 6.52987482255775e-06,
      "loss": 1.7996,
      "step": 10650
    },
    {
      "epoch": 4.1234998064266355,
      "grad_norm": 47.967437744140625,
      "learning_rate": 6.529444659525961e-06,
      "loss": 3.406,
      "step": 10651
    },
    {
      "epoch": 4.123886953155246,
      "grad_norm": 18.50765037536621,
      "learning_rate": 6.5290144964941715e-06,
      "loss": 1.2395,
      "step": 10652
    },
    {
      "epoch": 4.124274099883856,
      "grad_norm": 11.159906387329102,
      "learning_rate": 6.528584333462383e-06,
      "loss": 0.8241,
      "step": 10653
    },
    {
      "epoch": 4.124661246612466,
      "grad_norm": 11.264472961425781,
      "learning_rate": 6.528154170430594e-06,
      "loss": 0.558,
      "step": 10654
    },
    {
      "epoch": 4.125048393341077,
      "grad_norm": 59.54654312133789,
      "learning_rate": 6.527724007398805e-06,
      "loss": 1.3846,
      "step": 10655
    },
    {
      "epoch": 4.125435540069686,
      "grad_norm": 43.52945327758789,
      "learning_rate": 6.5272938443670154e-06,
      "loss": 1.0241,
      "step": 10656
    },
    {
      "epoch": 4.125822686798297,
      "grad_norm": 66.02452850341797,
      "learning_rate": 6.526863681335227e-06,
      "loss": 2.4011,
      "step": 10657
    },
    {
      "epoch": 4.126209833526906,
      "grad_norm": 14.031084060668945,
      "learning_rate": 6.526433518303437e-06,
      "loss": 1.1809,
      "step": 10658
    },
    {
      "epoch": 4.126596980255517,
      "grad_norm": 32.33119583129883,
      "learning_rate": 6.526003355271649e-06,
      "loss": 0.6609,
      "step": 10659
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 27.116914749145508,
      "learning_rate": 6.525573192239859e-06,
      "loss": 0.9154,
      "step": 10660
    },
    {
      "epoch": 4.127371273712737,
      "grad_norm": 19.113645553588867,
      "learning_rate": 6.525143029208071e-06,
      "loss": 1.0267,
      "step": 10661
    },
    {
      "epoch": 4.1277584204413476,
      "grad_norm": 52.87897872924805,
      "learning_rate": 6.524712866176281e-06,
      "loss": 1.7174,
      "step": 10662
    },
    {
      "epoch": 4.128145567169957,
      "grad_norm": 30.873483657836914,
      "learning_rate": 6.524282703144492e-06,
      "loss": 1.6568,
      "step": 10663
    },
    {
      "epoch": 4.128532713898568,
      "grad_norm": 73.40791320800781,
      "learning_rate": 6.5238525401127025e-06,
      "loss": 2.1938,
      "step": 10664
    },
    {
      "epoch": 4.128919860627177,
      "grad_norm": 17.05743980407715,
      "learning_rate": 6.523422377080915e-06,
      "loss": 1.2895,
      "step": 10665
    },
    {
      "epoch": 4.129307007355788,
      "grad_norm": 48.7813835144043,
      "learning_rate": 6.522992214049125e-06,
      "loss": 1.1022,
      "step": 10666
    },
    {
      "epoch": 4.129694154084398,
      "grad_norm": 17.58833122253418,
      "learning_rate": 6.522562051017336e-06,
      "loss": 0.8688,
      "step": 10667
    },
    {
      "epoch": 4.130081300813008,
      "grad_norm": 16.643314361572266,
      "learning_rate": 6.5221318879855465e-06,
      "loss": 0.9119,
      "step": 10668
    },
    {
      "epoch": 4.1304684475416185,
      "grad_norm": 32.68721008300781,
      "learning_rate": 6.521701724953759e-06,
      "loss": 2.3166,
      "step": 10669
    },
    {
      "epoch": 4.130855594270228,
      "grad_norm": 26.374418258666992,
      "learning_rate": 6.521271561921969e-06,
      "loss": 0.9433,
      "step": 10670
    },
    {
      "epoch": 4.131242740998839,
      "grad_norm": 13.666940689086914,
      "learning_rate": 6.52084139889018e-06,
      "loss": 0.4894,
      "step": 10671
    },
    {
      "epoch": 4.131629887727449,
      "grad_norm": 29.513574600219727,
      "learning_rate": 6.5204112358583905e-06,
      "loss": 1.6927,
      "step": 10672
    },
    {
      "epoch": 4.132017034456059,
      "grad_norm": 6.179163455963135,
      "learning_rate": 6.519981072826602e-06,
      "loss": 0.3519,
      "step": 10673
    },
    {
      "epoch": 4.132404181184669,
      "grad_norm": 56.34666442871094,
      "learning_rate": 6.519550909794813e-06,
      "loss": 0.857,
      "step": 10674
    },
    {
      "epoch": 4.132791327913279,
      "grad_norm": 20.830324172973633,
      "learning_rate": 6.519120746763024e-06,
      "loss": 0.4361,
      "step": 10675
    },
    {
      "epoch": 4.133178474641889,
      "grad_norm": 27.110353469848633,
      "learning_rate": 6.518690583731234e-06,
      "loss": 1.2932,
      "step": 10676
    },
    {
      "epoch": 4.133565621370499,
      "grad_norm": 29.2233829498291,
      "learning_rate": 6.518260420699446e-06,
      "loss": 1.538,
      "step": 10677
    },
    {
      "epoch": 4.1339527680991095,
      "grad_norm": 42.928497314453125,
      "learning_rate": 6.517830257667656e-06,
      "loss": 2.1028,
      "step": 10678
    },
    {
      "epoch": 4.13433991482772,
      "grad_norm": 25.880178451538086,
      "learning_rate": 6.517400094635867e-06,
      "loss": 1.311,
      "step": 10679
    },
    {
      "epoch": 4.13472706155633,
      "grad_norm": 33.4022216796875,
      "learning_rate": 6.516969931604078e-06,
      "loss": 1.4475,
      "step": 10680
    },
    {
      "epoch": 4.13511420828494,
      "grad_norm": 51.39423751831055,
      "learning_rate": 6.51653976857229e-06,
      "loss": 2.7087,
      "step": 10681
    },
    {
      "epoch": 4.13550135501355,
      "grad_norm": 29.742189407348633,
      "learning_rate": 6.5161096055405e-06,
      "loss": 1.2105,
      "step": 10682
    },
    {
      "epoch": 4.13588850174216,
      "grad_norm": 28.063621520996094,
      "learning_rate": 6.515679442508711e-06,
      "loss": 1.308,
      "step": 10683
    },
    {
      "epoch": 4.136275648470771,
      "grad_norm": 9.727276802062988,
      "learning_rate": 6.5152492794769215e-06,
      "loss": 1.0882,
      "step": 10684
    },
    {
      "epoch": 4.13666279519938,
      "grad_norm": 34.92942428588867,
      "learning_rate": 6.514819116445134e-06,
      "loss": 1.5388,
      "step": 10685
    },
    {
      "epoch": 4.137049941927991,
      "grad_norm": 15.275217056274414,
      "learning_rate": 6.514388953413344e-06,
      "loss": 1.0669,
      "step": 10686
    },
    {
      "epoch": 4.1374370886566005,
      "grad_norm": 35.99156951904297,
      "learning_rate": 6.513958790381555e-06,
      "loss": 0.7954,
      "step": 10687
    },
    {
      "epoch": 4.137824235385211,
      "grad_norm": 16.523151397705078,
      "learning_rate": 6.5135286273497655e-06,
      "loss": 1.608,
      "step": 10688
    },
    {
      "epoch": 4.138211382113822,
      "grad_norm": 44.37445831298828,
      "learning_rate": 6.513098464317977e-06,
      "loss": 0.8026,
      "step": 10689
    },
    {
      "epoch": 4.138598528842431,
      "grad_norm": 18.834915161132812,
      "learning_rate": 6.512668301286188e-06,
      "loss": 1.1676,
      "step": 10690
    },
    {
      "epoch": 4.138985675571042,
      "grad_norm": 7.759608745574951,
      "learning_rate": 6.512238138254399e-06,
      "loss": 0.4122,
      "step": 10691
    },
    {
      "epoch": 4.139372822299651,
      "grad_norm": 24.534603118896484,
      "learning_rate": 6.51180797522261e-06,
      "loss": 1.3685,
      "step": 10692
    },
    {
      "epoch": 4.139759969028262,
      "grad_norm": 31.197715759277344,
      "learning_rate": 6.511377812190821e-06,
      "loss": 0.8554,
      "step": 10693
    },
    {
      "epoch": 4.140147115756871,
      "grad_norm": 56.48880386352539,
      "learning_rate": 6.5109476491590314e-06,
      "loss": 2.0695,
      "step": 10694
    },
    {
      "epoch": 4.140534262485482,
      "grad_norm": 52.18336486816406,
      "learning_rate": 6.510517486127243e-06,
      "loss": 1.1859,
      "step": 10695
    },
    {
      "epoch": 4.1409214092140925,
      "grad_norm": 21.79918670654297,
      "learning_rate": 6.510087323095454e-06,
      "loss": 0.6211,
      "step": 10696
    },
    {
      "epoch": 4.141308555942702,
      "grad_norm": 23.216711044311523,
      "learning_rate": 6.509657160063665e-06,
      "loss": 1.4122,
      "step": 10697
    },
    {
      "epoch": 4.141695702671313,
      "grad_norm": 20.016103744506836,
      "learning_rate": 6.509226997031875e-06,
      "loss": 1.0039,
      "step": 10698
    },
    {
      "epoch": 4.142082849399922,
      "grad_norm": 9.188672065734863,
      "learning_rate": 6.508796834000086e-06,
      "loss": 0.6847,
      "step": 10699
    },
    {
      "epoch": 4.142469996128533,
      "grad_norm": 68.50946807861328,
      "learning_rate": 6.508366670968298e-06,
      "loss": 1.0988,
      "step": 10700
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 11.329879760742188,
      "learning_rate": 6.507936507936509e-06,
      "loss": 1.1545,
      "step": 10701
    },
    {
      "epoch": 4.143244289585753,
      "grad_norm": 6.811408042907715,
      "learning_rate": 6.507506344904719e-06,
      "loss": 0.3433,
      "step": 10702
    },
    {
      "epoch": 4.143631436314363,
      "grad_norm": 30.43719482421875,
      "learning_rate": 6.50707618187293e-06,
      "loss": 1.8857,
      "step": 10703
    },
    {
      "epoch": 4.144018583042973,
      "grad_norm": 14.330730438232422,
      "learning_rate": 6.506646018841141e-06,
      "loss": 0.6066,
      "step": 10704
    },
    {
      "epoch": 4.1444057297715835,
      "grad_norm": 37.21757125854492,
      "learning_rate": 6.506215855809353e-06,
      "loss": 0.7368,
      "step": 10705
    },
    {
      "epoch": 4.144792876500194,
      "grad_norm": 10.102713584899902,
      "learning_rate": 6.505785692777563e-06,
      "loss": 0.5931,
      "step": 10706
    },
    {
      "epoch": 4.145180023228804,
      "grad_norm": 64.76840209960938,
      "learning_rate": 6.505355529745774e-06,
      "loss": 1.7302,
      "step": 10707
    },
    {
      "epoch": 4.145567169957414,
      "grad_norm": 19.885826110839844,
      "learning_rate": 6.504925366713985e-06,
      "loss": 0.6025,
      "step": 10708
    },
    {
      "epoch": 4.145954316686024,
      "grad_norm": 26.276281356811523,
      "learning_rate": 6.504495203682196e-06,
      "loss": 1.4617,
      "step": 10709
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 4.35930871963501,
      "learning_rate": 6.504065040650407e-06,
      "loss": 0.2077,
      "step": 10710
    },
    {
      "epoch": 4.146728610143244,
      "grad_norm": 19.884443283081055,
      "learning_rate": 6.503634877618618e-06,
      "loss": 1.0588,
      "step": 10711
    },
    {
      "epoch": 4.147115756871854,
      "grad_norm": 38.914615631103516,
      "learning_rate": 6.503204714586829e-06,
      "loss": 1.0226,
      "step": 10712
    },
    {
      "epoch": 4.147502903600465,
      "grad_norm": 2.768294334411621,
      "learning_rate": 6.50277455155504e-06,
      "loss": 0.0885,
      "step": 10713
    },
    {
      "epoch": 4.1478900503290745,
      "grad_norm": 52.9277229309082,
      "learning_rate": 6.5023443885232504e-06,
      "loss": 1.0246,
      "step": 10714
    },
    {
      "epoch": 4.148277197057685,
      "grad_norm": 13.485631942749023,
      "learning_rate": 6.501914225491461e-06,
      "loss": 0.49,
      "step": 10715
    },
    {
      "epoch": 4.148664343786295,
      "grad_norm": 20.01272201538086,
      "learning_rate": 6.501484062459673e-06,
      "loss": 0.8671,
      "step": 10716
    },
    {
      "epoch": 4.149051490514905,
      "grad_norm": 22.179609298706055,
      "learning_rate": 6.501053899427884e-06,
      "loss": 0.7833,
      "step": 10717
    },
    {
      "epoch": 4.149438637243516,
      "grad_norm": 25.776159286499023,
      "learning_rate": 6.500623736396094e-06,
      "loss": 1.8792,
      "step": 10718
    },
    {
      "epoch": 4.149825783972125,
      "grad_norm": 33.13375473022461,
      "learning_rate": 6.500193573364305e-06,
      "loss": 2.5351,
      "step": 10719
    },
    {
      "epoch": 4.150212930700736,
      "grad_norm": 59.68217849731445,
      "learning_rate": 6.499763410332517e-06,
      "loss": 1.1469,
      "step": 10720
    },
    {
      "epoch": 4.1506000774293454,
      "grad_norm": 14.6611967086792,
      "learning_rate": 6.499333247300728e-06,
      "loss": 1.0021,
      "step": 10721
    },
    {
      "epoch": 4.150987224157956,
      "grad_norm": 18.833337783813477,
      "learning_rate": 6.498903084268938e-06,
      "loss": 1.3139,
      "step": 10722
    },
    {
      "epoch": 4.151374370886566,
      "grad_norm": 17.199098587036133,
      "learning_rate": 6.498472921237149e-06,
      "loss": 0.7184,
      "step": 10723
    },
    {
      "epoch": 4.151761517615176,
      "grad_norm": 32.03858184814453,
      "learning_rate": 6.49804275820536e-06,
      "loss": 1.2194,
      "step": 10724
    },
    {
      "epoch": 4.152148664343787,
      "grad_norm": 19.203041076660156,
      "learning_rate": 6.497612595173571e-06,
      "loss": 0.399,
      "step": 10725
    },
    {
      "epoch": 4.152535811072396,
      "grad_norm": 13.889861106872559,
      "learning_rate": 6.497182432141782e-06,
      "loss": 0.602,
      "step": 10726
    },
    {
      "epoch": 4.152922957801007,
      "grad_norm": 13.225574493408203,
      "learning_rate": 6.496752269109993e-06,
      "loss": 1.0073,
      "step": 10727
    },
    {
      "epoch": 4.153310104529616,
      "grad_norm": 25.27075958251953,
      "learning_rate": 6.496322106078204e-06,
      "loss": 0.9656,
      "step": 10728
    },
    {
      "epoch": 4.153697251258227,
      "grad_norm": 18.270374298095703,
      "learning_rate": 6.495891943046415e-06,
      "loss": 1.2694,
      "step": 10729
    },
    {
      "epoch": 4.154084397986837,
      "grad_norm": 93.54415130615234,
      "learning_rate": 6.4954617800146255e-06,
      "loss": 0.7921,
      "step": 10730
    },
    {
      "epoch": 4.154471544715447,
      "grad_norm": 50.51057052612305,
      "learning_rate": 6.495031616982838e-06,
      "loss": 0.3035,
      "step": 10731
    },
    {
      "epoch": 4.1548586914440575,
      "grad_norm": 21.230504989624023,
      "learning_rate": 6.494601453951048e-06,
      "loss": 1.6254,
      "step": 10732
    },
    {
      "epoch": 4.155245838172667,
      "grad_norm": 40.82045364379883,
      "learning_rate": 6.494171290919259e-06,
      "loss": 0.6365,
      "step": 10733
    },
    {
      "epoch": 4.155632984901278,
      "grad_norm": 107.03336334228516,
      "learning_rate": 6.4937411278874694e-06,
      "loss": 1.1269,
      "step": 10734
    },
    {
      "epoch": 4.156020131629888,
      "grad_norm": 43.958744049072266,
      "learning_rate": 6.493310964855682e-06,
      "loss": 0.7388,
      "step": 10735
    },
    {
      "epoch": 4.156407278358498,
      "grad_norm": 1.708733081817627,
      "learning_rate": 6.492880801823892e-06,
      "loss": 0.0528,
      "step": 10736
    },
    {
      "epoch": 4.156794425087108,
      "grad_norm": 10.933117866516113,
      "learning_rate": 6.492450638792103e-06,
      "loss": 0.7932,
      "step": 10737
    },
    {
      "epoch": 4.157181571815718,
      "grad_norm": 33.58394241333008,
      "learning_rate": 6.492020475760313e-06,
      "loss": 1.8046,
      "step": 10738
    },
    {
      "epoch": 4.157568718544328,
      "grad_norm": 158.3566436767578,
      "learning_rate": 6.491590312728525e-06,
      "loss": 3.2135,
      "step": 10739
    },
    {
      "epoch": 4.157955865272938,
      "grad_norm": 16.558101654052734,
      "learning_rate": 6.491160149696735e-06,
      "loss": 0.9843,
      "step": 10740
    },
    {
      "epoch": 4.1583430120015485,
      "grad_norm": 44.08170700073242,
      "learning_rate": 6.490729986664947e-06,
      "loss": 2.8245,
      "step": 10741
    },
    {
      "epoch": 4.158730158730159,
      "grad_norm": 21.033172607421875,
      "learning_rate": 6.490299823633157e-06,
      "loss": 0.5652,
      "step": 10742
    },
    {
      "epoch": 4.159117305458769,
      "grad_norm": 18.14691162109375,
      "learning_rate": 6.489869660601369e-06,
      "loss": 0.9877,
      "step": 10743
    },
    {
      "epoch": 4.159504452187379,
      "grad_norm": 11.23889446258545,
      "learning_rate": 6.489439497569579e-06,
      "loss": 0.676,
      "step": 10744
    },
    {
      "epoch": 4.159891598915989,
      "grad_norm": 59.990840911865234,
      "learning_rate": 6.48900933453779e-06,
      "loss": 0.9223,
      "step": 10745
    },
    {
      "epoch": 4.160278745644599,
      "grad_norm": 16.54319953918457,
      "learning_rate": 6.488579171506001e-06,
      "loss": 1.198,
      "step": 10746
    },
    {
      "epoch": 4.16066589237321,
      "grad_norm": 38.28017044067383,
      "learning_rate": 6.488149008474213e-06,
      "loss": 1.1391,
      "step": 10747
    },
    {
      "epoch": 4.1610530391018195,
      "grad_norm": 27.55437660217285,
      "learning_rate": 6.487718845442423e-06,
      "loss": 1.6974,
      "step": 10748
    },
    {
      "epoch": 4.16144018583043,
      "grad_norm": 5.268007755279541,
      "learning_rate": 6.487288682410634e-06,
      "loss": 0.2104,
      "step": 10749
    },
    {
      "epoch": 4.16182733255904,
      "grad_norm": 74.94325256347656,
      "learning_rate": 6.4868585193788445e-06,
      "loss": 1.5564,
      "step": 10750
    },
    {
      "epoch": 4.16221447928765,
      "grad_norm": 53.82440948486328,
      "learning_rate": 6.486428356347057e-06,
      "loss": 0.8318,
      "step": 10751
    },
    {
      "epoch": 4.16260162601626,
      "grad_norm": 15.426094055175781,
      "learning_rate": 6.485998193315267e-06,
      "loss": 0.6543,
      "step": 10752
    },
    {
      "epoch": 4.16298877274487,
      "grad_norm": 51.16416549682617,
      "learning_rate": 6.485568030283478e-06,
      "loss": 2.3941,
      "step": 10753
    },
    {
      "epoch": 4.163375919473481,
      "grad_norm": 24.905590057373047,
      "learning_rate": 6.4851378672516884e-06,
      "loss": 1.6319,
      "step": 10754
    },
    {
      "epoch": 4.16376306620209,
      "grad_norm": 27.147497177124023,
      "learning_rate": 6.4847077042199e-06,
      "loss": 1.2332,
      "step": 10755
    },
    {
      "epoch": 4.164150212930701,
      "grad_norm": 14.364601135253906,
      "learning_rate": 6.484277541188111e-06,
      "loss": 1.2919,
      "step": 10756
    },
    {
      "epoch": 4.1645373596593105,
      "grad_norm": 9.609016418457031,
      "learning_rate": 6.483847378156322e-06,
      "loss": 1.1059,
      "step": 10757
    },
    {
      "epoch": 4.164924506387921,
      "grad_norm": 32.717681884765625,
      "learning_rate": 6.483417215124532e-06,
      "loss": 1.5036,
      "step": 10758
    },
    {
      "epoch": 4.1653116531165315,
      "grad_norm": 23.16287612915039,
      "learning_rate": 6.482987052092744e-06,
      "loss": 1.4557,
      "step": 10759
    },
    {
      "epoch": 4.165698799845141,
      "grad_norm": 38.08859634399414,
      "learning_rate": 6.482556889060954e-06,
      "loss": 1.3333,
      "step": 10760
    },
    {
      "epoch": 4.166085946573752,
      "grad_norm": 34.621543884277344,
      "learning_rate": 6.482126726029165e-06,
      "loss": 1.2414,
      "step": 10761
    },
    {
      "epoch": 4.166473093302361,
      "grad_norm": 9.162701606750488,
      "learning_rate": 6.481696562997376e-06,
      "loss": 0.4028,
      "step": 10762
    },
    {
      "epoch": 4.166860240030972,
      "grad_norm": 19.138442993164062,
      "learning_rate": 6.481266399965588e-06,
      "loss": 1.1629,
      "step": 10763
    },
    {
      "epoch": 4.167247386759582,
      "grad_norm": 39.87933349609375,
      "learning_rate": 6.480836236933798e-06,
      "loss": 1.466,
      "step": 10764
    },
    {
      "epoch": 4.167634533488192,
      "grad_norm": 30.348892211914062,
      "learning_rate": 6.480406073902009e-06,
      "loss": 2.0196,
      "step": 10765
    },
    {
      "epoch": 4.168021680216802,
      "grad_norm": 44.16596984863281,
      "learning_rate": 6.4799759108702195e-06,
      "loss": 1.9795,
      "step": 10766
    },
    {
      "epoch": 4.168408826945412,
      "grad_norm": 27.201297760009766,
      "learning_rate": 6.479545747838432e-06,
      "loss": 1.4669,
      "step": 10767
    },
    {
      "epoch": 4.1687959736740225,
      "grad_norm": 37.475440979003906,
      "learning_rate": 6.479115584806642e-06,
      "loss": 3.0137,
      "step": 10768
    },
    {
      "epoch": 4.169183120402632,
      "grad_norm": 52.6117057800293,
      "learning_rate": 6.478685421774853e-06,
      "loss": 1.2183,
      "step": 10769
    },
    {
      "epoch": 4.169570267131243,
      "grad_norm": 28.16297721862793,
      "learning_rate": 6.4782552587430635e-06,
      "loss": 2.2466,
      "step": 10770
    },
    {
      "epoch": 4.169957413859853,
      "grad_norm": 38.82939529418945,
      "learning_rate": 6.477825095711276e-06,
      "loss": 1.4383,
      "step": 10771
    },
    {
      "epoch": 4.170344560588463,
      "grad_norm": 37.884342193603516,
      "learning_rate": 6.477394932679486e-06,
      "loss": 1.1097,
      "step": 10772
    },
    {
      "epoch": 4.170731707317073,
      "grad_norm": 3.2948570251464844,
      "learning_rate": 6.476964769647697e-06,
      "loss": 0.0686,
      "step": 10773
    },
    {
      "epoch": 4.171118854045683,
      "grad_norm": 6.675942420959473,
      "learning_rate": 6.476534606615908e-06,
      "loss": 0.3314,
      "step": 10774
    },
    {
      "epoch": 4.1715060007742935,
      "grad_norm": 18.950090408325195,
      "learning_rate": 6.476104443584119e-06,
      "loss": 1.342,
      "step": 10775
    },
    {
      "epoch": 4.171893147502904,
      "grad_norm": 46.72200012207031,
      "learning_rate": 6.475674280552329e-06,
      "loss": 0.999,
      "step": 10776
    },
    {
      "epoch": 4.172280294231514,
      "grad_norm": 8.44886302947998,
      "learning_rate": 6.475244117520541e-06,
      "loss": 0.4146,
      "step": 10777
    },
    {
      "epoch": 4.172667440960124,
      "grad_norm": 21.77349281311035,
      "learning_rate": 6.474813954488752e-06,
      "loss": 1.8387,
      "step": 10778
    },
    {
      "epoch": 4.173054587688734,
      "grad_norm": 21.295787811279297,
      "learning_rate": 6.474383791456963e-06,
      "loss": 1.9327,
      "step": 10779
    },
    {
      "epoch": 4.173441734417344,
      "grad_norm": 52.97258758544922,
      "learning_rate": 6.473953628425173e-06,
      "loss": 1.2268,
      "step": 10780
    },
    {
      "epoch": 4.173828881145955,
      "grad_norm": 42.9694938659668,
      "learning_rate": 6.473523465393384e-06,
      "loss": 1.0811,
      "step": 10781
    },
    {
      "epoch": 4.174216027874564,
      "grad_norm": 18.72014617919922,
      "learning_rate": 6.473093302361596e-06,
      "loss": 0.4874,
      "step": 10782
    },
    {
      "epoch": 4.174603174603175,
      "grad_norm": 59.09316635131836,
      "learning_rate": 6.472663139329807e-06,
      "loss": 1.0618,
      "step": 10783
    },
    {
      "epoch": 4.1749903213317845,
      "grad_norm": 29.38873863220215,
      "learning_rate": 6.472232976298017e-06,
      "loss": 1.1332,
      "step": 10784
    },
    {
      "epoch": 4.175377468060395,
      "grad_norm": 7.236441612243652,
      "learning_rate": 6.471802813266228e-06,
      "loss": 0.4067,
      "step": 10785
    },
    {
      "epoch": 4.175764614789005,
      "grad_norm": 16.727142333984375,
      "learning_rate": 6.47137265023444e-06,
      "loss": 1.4571,
      "step": 10786
    },
    {
      "epoch": 4.176151761517615,
      "grad_norm": 18.238590240478516,
      "learning_rate": 6.470942487202651e-06,
      "loss": 0.7169,
      "step": 10787
    },
    {
      "epoch": 4.176538908246226,
      "grad_norm": 28.801830291748047,
      "learning_rate": 6.470512324170861e-06,
      "loss": 1.8643,
      "step": 10788
    },
    {
      "epoch": 4.176926054974835,
      "grad_norm": 42.490055084228516,
      "learning_rate": 6.470082161139072e-06,
      "loss": 0.556,
      "step": 10789
    },
    {
      "epoch": 4.177313201703446,
      "grad_norm": 12.033754348754883,
      "learning_rate": 6.469651998107283e-06,
      "loss": 0.5609,
      "step": 10790
    },
    {
      "epoch": 4.177700348432055,
      "grad_norm": 202.59213256835938,
      "learning_rate": 6.469221835075494e-06,
      "loss": 1.2372,
      "step": 10791
    },
    {
      "epoch": 4.178087495160666,
      "grad_norm": 6.694585800170898,
      "learning_rate": 6.468791672043705e-06,
      "loss": 0.1709,
      "step": 10792
    },
    {
      "epoch": 4.178474641889276,
      "grad_norm": 15.13480281829834,
      "learning_rate": 6.468361509011916e-06,
      "loss": 1.01,
      "step": 10793
    },
    {
      "epoch": 4.178861788617886,
      "grad_norm": 69.94330596923828,
      "learning_rate": 6.467931345980127e-06,
      "loss": 1.122,
      "step": 10794
    },
    {
      "epoch": 4.1792489353464966,
      "grad_norm": 44.22563171386719,
      "learning_rate": 6.467501182948338e-06,
      "loss": 2.6071,
      "step": 10795
    },
    {
      "epoch": 4.179636082075106,
      "grad_norm": 16.851505279541016,
      "learning_rate": 6.467071019916548e-06,
      "loss": 0.6702,
      "step": 10796
    },
    {
      "epoch": 4.180023228803717,
      "grad_norm": 9.451981544494629,
      "learning_rate": 6.466640856884759e-06,
      "loss": 0.3433,
      "step": 10797
    },
    {
      "epoch": 4.180410375532327,
      "grad_norm": 44.497833251953125,
      "learning_rate": 6.466210693852971e-06,
      "loss": 1.6836,
      "step": 10798
    },
    {
      "epoch": 4.180797522260937,
      "grad_norm": 35.61052703857422,
      "learning_rate": 6.465780530821182e-06,
      "loss": 1.6566,
      "step": 10799
    },
    {
      "epoch": 4.181184668989547,
      "grad_norm": 61.29608917236328,
      "learning_rate": 6.465350367789392e-06,
      "loss": 1.5834,
      "step": 10800
    },
    {
      "epoch": 4.181571815718157,
      "grad_norm": 9.838628768920898,
      "learning_rate": 6.464920204757603e-06,
      "loss": 0.4383,
      "step": 10801
    },
    {
      "epoch": 4.1819589624467675,
      "grad_norm": 30.485197067260742,
      "learning_rate": 6.464490041725815e-06,
      "loss": 1.1877,
      "step": 10802
    },
    {
      "epoch": 4.182346109175377,
      "grad_norm": 31.80065155029297,
      "learning_rate": 6.464059878694026e-06,
      "loss": 3.8275,
      "step": 10803
    },
    {
      "epoch": 4.182733255903988,
      "grad_norm": 20.453283309936523,
      "learning_rate": 6.463629715662236e-06,
      "loss": 0.4496,
      "step": 10804
    },
    {
      "epoch": 4.183120402632598,
      "grad_norm": 17.838193893432617,
      "learning_rate": 6.463199552630447e-06,
      "loss": 0.5032,
      "step": 10805
    },
    {
      "epoch": 4.183507549361208,
      "grad_norm": 59.84986114501953,
      "learning_rate": 6.462769389598658e-06,
      "loss": 1.2406,
      "step": 10806
    },
    {
      "epoch": 4.183894696089818,
      "grad_norm": 57.53541564941406,
      "learning_rate": 6.46233922656687e-06,
      "loss": 2.6165,
      "step": 10807
    },
    {
      "epoch": 4.184281842818428,
      "grad_norm": 23.78857421875,
      "learning_rate": 6.46190906353508e-06,
      "loss": 1.2139,
      "step": 10808
    },
    {
      "epoch": 4.184668989547038,
      "grad_norm": 27.042943954467773,
      "learning_rate": 6.461478900503291e-06,
      "loss": 2.4493,
      "step": 10809
    },
    {
      "epoch": 4.185056136275649,
      "grad_norm": 83.40083312988281,
      "learning_rate": 6.461048737471502e-06,
      "loss": 1.5031,
      "step": 10810
    },
    {
      "epoch": 4.1854432830042585,
      "grad_norm": 15.370708465576172,
      "learning_rate": 6.460618574439713e-06,
      "loss": 1.2647,
      "step": 10811
    },
    {
      "epoch": 4.185830429732869,
      "grad_norm": 30.21727180480957,
      "learning_rate": 6.4601884114079234e-06,
      "loss": 1.3571,
      "step": 10812
    },
    {
      "epoch": 4.186217576461479,
      "grad_norm": 32.40803146362305,
      "learning_rate": 6.459758248376136e-06,
      "loss": 1.6728,
      "step": 10813
    },
    {
      "epoch": 4.186604723190089,
      "grad_norm": 38.81059265136719,
      "learning_rate": 6.459328085344346e-06,
      "loss": 0.939,
      "step": 10814
    },
    {
      "epoch": 4.186991869918699,
      "grad_norm": 21.259225845336914,
      "learning_rate": 6.458897922312557e-06,
      "loss": 0.3773,
      "step": 10815
    },
    {
      "epoch": 4.187379016647309,
      "grad_norm": 102.84992218017578,
      "learning_rate": 6.458467759280767e-06,
      "loss": 2.0737,
      "step": 10816
    },
    {
      "epoch": 4.18776616337592,
      "grad_norm": 5.646377086639404,
      "learning_rate": 6.45803759624898e-06,
      "loss": 0.2261,
      "step": 10817
    },
    {
      "epoch": 4.188153310104529,
      "grad_norm": 38.83733367919922,
      "learning_rate": 6.45760743321719e-06,
      "loss": 1.5378,
      "step": 10818
    },
    {
      "epoch": 4.18854045683314,
      "grad_norm": 13.886211395263672,
      "learning_rate": 6.457177270185401e-06,
      "loss": 0.7606,
      "step": 10819
    },
    {
      "epoch": 4.1889276035617495,
      "grad_norm": 49.70416259765625,
      "learning_rate": 6.456747107153611e-06,
      "loss": 0.7107,
      "step": 10820
    },
    {
      "epoch": 4.18931475029036,
      "grad_norm": 12.984113693237305,
      "learning_rate": 6.456316944121823e-06,
      "loss": 1.1418,
      "step": 10821
    },
    {
      "epoch": 4.1897018970189706,
      "grad_norm": 18.533245086669922,
      "learning_rate": 6.455886781090034e-06,
      "loss": 0.7187,
      "step": 10822
    },
    {
      "epoch": 4.19008904374758,
      "grad_norm": 47.55297088623047,
      "learning_rate": 6.455456618058245e-06,
      "loss": 1.7621,
      "step": 10823
    },
    {
      "epoch": 4.190476190476191,
      "grad_norm": 11.021732330322266,
      "learning_rate": 6.455026455026455e-06,
      "loss": 0.668,
      "step": 10824
    },
    {
      "epoch": 4.1908633372048,
      "grad_norm": 61.01888656616211,
      "learning_rate": 6.454596291994667e-06,
      "loss": 0.7803,
      "step": 10825
    },
    {
      "epoch": 4.191250483933411,
      "grad_norm": 17.53570556640625,
      "learning_rate": 6.454166128962877e-06,
      "loss": 1.5918,
      "step": 10826
    },
    {
      "epoch": 4.191637630662021,
      "grad_norm": 97.41297912597656,
      "learning_rate": 6.453735965931088e-06,
      "loss": 2.2434,
      "step": 10827
    },
    {
      "epoch": 4.192024777390631,
      "grad_norm": 40.925506591796875,
      "learning_rate": 6.453305802899299e-06,
      "loss": 0.7766,
      "step": 10828
    },
    {
      "epoch": 4.1924119241192415,
      "grad_norm": 22.93451499938965,
      "learning_rate": 6.452875639867511e-06,
      "loss": 1.3034,
      "step": 10829
    },
    {
      "epoch": 4.192799070847851,
      "grad_norm": 6.924659729003906,
      "learning_rate": 6.452445476835721e-06,
      "loss": 0.2559,
      "step": 10830
    },
    {
      "epoch": 4.193186217576462,
      "grad_norm": 32.516658782958984,
      "learning_rate": 6.452015313803932e-06,
      "loss": 1.9216,
      "step": 10831
    },
    {
      "epoch": 4.193573364305071,
      "grad_norm": 14.932848930358887,
      "learning_rate": 6.4515851507721424e-06,
      "loss": 0.7037,
      "step": 10832
    },
    {
      "epoch": 4.193960511033682,
      "grad_norm": 33.11727523803711,
      "learning_rate": 6.451154987740355e-06,
      "loss": 2.4791,
      "step": 10833
    },
    {
      "epoch": 4.194347657762292,
      "grad_norm": 43.53767776489258,
      "learning_rate": 6.450724824708565e-06,
      "loss": 0.7239,
      "step": 10834
    },
    {
      "epoch": 4.194734804490902,
      "grad_norm": 69.45088195800781,
      "learning_rate": 6.450294661676776e-06,
      "loss": 1.1569,
      "step": 10835
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 46.541507720947266,
      "learning_rate": 6.449864498644986e-06,
      "loss": 0.946,
      "step": 10836
    },
    {
      "epoch": 4.195509097948122,
      "grad_norm": 7.840420722961426,
      "learning_rate": 6.449434335613199e-06,
      "loss": 0.3574,
      "step": 10837
    },
    {
      "epoch": 4.1958962446767325,
      "grad_norm": 13.47386646270752,
      "learning_rate": 6.449004172581409e-06,
      "loss": 1.1525,
      "step": 10838
    },
    {
      "epoch": 4.196283391405343,
      "grad_norm": 10.096460342407227,
      "learning_rate": 6.44857400954962e-06,
      "loss": 0.4256,
      "step": 10839
    },
    {
      "epoch": 4.196670538133953,
      "grad_norm": 43.083335876464844,
      "learning_rate": 6.44814384651783e-06,
      "loss": 1.9701,
      "step": 10840
    },
    {
      "epoch": 4.197057684862563,
      "grad_norm": 23.056259155273438,
      "learning_rate": 6.447713683486042e-06,
      "loss": 0.9588,
      "step": 10841
    },
    {
      "epoch": 4.197444831591173,
      "grad_norm": 96.51057434082031,
      "learning_rate": 6.447283520454252e-06,
      "loss": 0.6563,
      "step": 10842
    },
    {
      "epoch": 4.197831978319783,
      "grad_norm": 22.2884464263916,
      "learning_rate": 6.446853357422464e-06,
      "loss": 0.8605,
      "step": 10843
    },
    {
      "epoch": 4.198219125048393,
      "grad_norm": 76.21501159667969,
      "learning_rate": 6.446423194390674e-06,
      "loss": 2.1272,
      "step": 10844
    },
    {
      "epoch": 4.198606271777003,
      "grad_norm": 38.20882797241211,
      "learning_rate": 6.445993031358886e-06,
      "loss": 1.3738,
      "step": 10845
    },
    {
      "epoch": 4.198993418505614,
      "grad_norm": 75.24406433105469,
      "learning_rate": 6.445562868327096e-06,
      "loss": 0.9292,
      "step": 10846
    },
    {
      "epoch": 4.1993805652342235,
      "grad_norm": 51.825660705566406,
      "learning_rate": 6.445132705295307e-06,
      "loss": 1.6215,
      "step": 10847
    },
    {
      "epoch": 4.199767711962834,
      "grad_norm": 10.400596618652344,
      "learning_rate": 6.4447025422635175e-06,
      "loss": 0.3903,
      "step": 10848
    },
    {
      "epoch": 4.200154858691444,
      "grad_norm": 36.097354888916016,
      "learning_rate": 6.44427237923173e-06,
      "loss": 1.6716,
      "step": 10849
    },
    {
      "epoch": 4.200542005420054,
      "grad_norm": 40.32402801513672,
      "learning_rate": 6.44384221619994e-06,
      "loss": 1.1758,
      "step": 10850
    },
    {
      "epoch": 4.200929152148665,
      "grad_norm": 85.17969512939453,
      "learning_rate": 6.443412053168151e-06,
      "loss": 1.1375,
      "step": 10851
    },
    {
      "epoch": 4.201316298877274,
      "grad_norm": 61.03203201293945,
      "learning_rate": 6.4429818901363614e-06,
      "loss": 3.0048,
      "step": 10852
    },
    {
      "epoch": 4.201703445605885,
      "grad_norm": 19.018184661865234,
      "learning_rate": 6.442551727104574e-06,
      "loss": 0.5458,
      "step": 10853
    },
    {
      "epoch": 4.2020905923344944,
      "grad_norm": 81.86875915527344,
      "learning_rate": 6.442121564072784e-06,
      "loss": 1.9337,
      "step": 10854
    },
    {
      "epoch": 4.202477739063105,
      "grad_norm": 32.60318374633789,
      "learning_rate": 6.441691401040995e-06,
      "loss": 2.3819,
      "step": 10855
    },
    {
      "epoch": 4.2028648857917155,
      "grad_norm": 39.206478118896484,
      "learning_rate": 6.441261238009206e-06,
      "loss": 1.0498,
      "step": 10856
    },
    {
      "epoch": 4.203252032520325,
      "grad_norm": 86.69959259033203,
      "learning_rate": 6.440831074977417e-06,
      "loss": 1.1985,
      "step": 10857
    },
    {
      "epoch": 4.203639179248936,
      "grad_norm": 15.602001190185547,
      "learning_rate": 6.440400911945628e-06,
      "loss": 0.8584,
      "step": 10858
    },
    {
      "epoch": 4.204026325977545,
      "grad_norm": 42.38742446899414,
      "learning_rate": 6.439970748913839e-06,
      "loss": 1.6234,
      "step": 10859
    },
    {
      "epoch": 4.204413472706156,
      "grad_norm": 53.073768615722656,
      "learning_rate": 6.43954058588205e-06,
      "loss": 1.138,
      "step": 10860
    },
    {
      "epoch": 4.204800619434765,
      "grad_norm": 24.346778869628906,
      "learning_rate": 6.439110422850261e-06,
      "loss": 2.1614,
      "step": 10861
    },
    {
      "epoch": 4.205187766163376,
      "grad_norm": 24.996797561645508,
      "learning_rate": 6.438680259818471e-06,
      "loss": 1.0358,
      "step": 10862
    },
    {
      "epoch": 4.205574912891986,
      "grad_norm": 25.191265106201172,
      "learning_rate": 6.438250096786682e-06,
      "loss": 1.4197,
      "step": 10863
    },
    {
      "epoch": 4.205962059620596,
      "grad_norm": 60.45693588256836,
      "learning_rate": 6.437819933754894e-06,
      "loss": 2.2404,
      "step": 10864
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 80.61288452148438,
      "learning_rate": 6.437389770723105e-06,
      "loss": 2.6107,
      "step": 10865
    },
    {
      "epoch": 4.206736353077816,
      "grad_norm": 23.757278442382812,
      "learning_rate": 6.436959607691315e-06,
      "loss": 3.0982,
      "step": 10866
    },
    {
      "epoch": 4.207123499806427,
      "grad_norm": 13.351820945739746,
      "learning_rate": 6.436529444659526e-06,
      "loss": 0.5765,
      "step": 10867
    },
    {
      "epoch": 4.207510646535037,
      "grad_norm": 8.851537704467773,
      "learning_rate": 6.436099281627738e-06,
      "loss": 0.3089,
      "step": 10868
    },
    {
      "epoch": 4.207897793263647,
      "grad_norm": 15.131567001342773,
      "learning_rate": 6.435669118595949e-06,
      "loss": 1.0291,
      "step": 10869
    },
    {
      "epoch": 4.208284939992257,
      "grad_norm": 19.948604583740234,
      "learning_rate": 6.435238955564159e-06,
      "loss": 0.5935,
      "step": 10870
    },
    {
      "epoch": 4.208672086720867,
      "grad_norm": 33.1451301574707,
      "learning_rate": 6.43480879253237e-06,
      "loss": 1.4056,
      "step": 10871
    },
    {
      "epoch": 4.209059233449477,
      "grad_norm": 40.13628387451172,
      "learning_rate": 6.434378629500581e-06,
      "loss": 1.1553,
      "step": 10872
    },
    {
      "epoch": 4.209446380178088,
      "grad_norm": 26.69525718688965,
      "learning_rate": 6.433948466468793e-06,
      "loss": 2.3524,
      "step": 10873
    },
    {
      "epoch": 4.2098335269066975,
      "grad_norm": 43.307159423828125,
      "learning_rate": 6.433518303437003e-06,
      "loss": 1.1554,
      "step": 10874
    },
    {
      "epoch": 4.210220673635308,
      "grad_norm": 18.647449493408203,
      "learning_rate": 6.433088140405214e-06,
      "loss": 1.9746,
      "step": 10875
    },
    {
      "epoch": 4.210607820363918,
      "grad_norm": 21.60916519165039,
      "learning_rate": 6.432657977373425e-06,
      "loss": 2.409,
      "step": 10876
    },
    {
      "epoch": 4.210994967092528,
      "grad_norm": 27.479459762573242,
      "learning_rate": 6.432227814341636e-06,
      "loss": 0.9091,
      "step": 10877
    },
    {
      "epoch": 4.211382113821138,
      "grad_norm": 17.13591957092285,
      "learning_rate": 6.431797651309846e-06,
      "loss": 1.2909,
      "step": 10878
    },
    {
      "epoch": 4.211769260549748,
      "grad_norm": 21.83282470703125,
      "learning_rate": 6.431367488278058e-06,
      "loss": 1.7366,
      "step": 10879
    },
    {
      "epoch": 4.212156407278359,
      "grad_norm": 55.109230041503906,
      "learning_rate": 6.430937325246269e-06,
      "loss": 2.3921,
      "step": 10880
    },
    {
      "epoch": 4.2125435540069684,
      "grad_norm": 34.52042770385742,
      "learning_rate": 6.43050716221448e-06,
      "loss": 1.6353,
      "step": 10881
    },
    {
      "epoch": 4.212930700735579,
      "grad_norm": 25.20218849182129,
      "learning_rate": 6.43007699918269e-06,
      "loss": 0.4353,
      "step": 10882
    },
    {
      "epoch": 4.213317847464189,
      "grad_norm": 37.96108627319336,
      "learning_rate": 6.429646836150901e-06,
      "loss": 1.3571,
      "step": 10883
    },
    {
      "epoch": 4.213704994192799,
      "grad_norm": 25.070932388305664,
      "learning_rate": 6.429216673119113e-06,
      "loss": 0.6826,
      "step": 10884
    },
    {
      "epoch": 4.21409214092141,
      "grad_norm": 81.70552062988281,
      "learning_rate": 6.428786510087324e-06,
      "loss": 1.4888,
      "step": 10885
    },
    {
      "epoch": 4.214479287650019,
      "grad_norm": 40.67361831665039,
      "learning_rate": 6.428356347055534e-06,
      "loss": 0.9777,
      "step": 10886
    },
    {
      "epoch": 4.21486643437863,
      "grad_norm": 43.54883575439453,
      "learning_rate": 6.427926184023745e-06,
      "loss": 2.4366,
      "step": 10887
    },
    {
      "epoch": 4.215253581107239,
      "grad_norm": 58.361419677734375,
      "learning_rate": 6.427496020991957e-06,
      "loss": 1.728,
      "step": 10888
    },
    {
      "epoch": 4.21564072783585,
      "grad_norm": 43.74266052246094,
      "learning_rate": 6.427065857960168e-06,
      "loss": 1.1054,
      "step": 10889
    },
    {
      "epoch": 4.21602787456446,
      "grad_norm": 38.779541015625,
      "learning_rate": 6.426635694928378e-06,
      "loss": 1.1894,
      "step": 10890
    },
    {
      "epoch": 4.21641502129307,
      "grad_norm": 25.383745193481445,
      "learning_rate": 6.426205531896589e-06,
      "loss": 2.7924,
      "step": 10891
    },
    {
      "epoch": 4.2168021680216805,
      "grad_norm": 38.05854034423828,
      "learning_rate": 6.4257753688648e-06,
      "loss": 0.8059,
      "step": 10892
    },
    {
      "epoch": 4.21718931475029,
      "grad_norm": 34.103878021240234,
      "learning_rate": 6.425345205833011e-06,
      "loss": 1.4378,
      "step": 10893
    },
    {
      "epoch": 4.217576461478901,
      "grad_norm": 38.93133544921875,
      "learning_rate": 6.424915042801222e-06,
      "loss": 2.8392,
      "step": 10894
    },
    {
      "epoch": 4.21796360820751,
      "grad_norm": 28.5123291015625,
      "learning_rate": 6.424484879769434e-06,
      "loss": 1.5463,
      "step": 10895
    },
    {
      "epoch": 4.218350754936121,
      "grad_norm": 15.221623420715332,
      "learning_rate": 6.424054716737644e-06,
      "loss": 1.2301,
      "step": 10896
    },
    {
      "epoch": 4.218737901664731,
      "grad_norm": 32.51541519165039,
      "learning_rate": 6.423624553705855e-06,
      "loss": 0.6418,
      "step": 10897
    },
    {
      "epoch": 4.219125048393341,
      "grad_norm": 34.45029067993164,
      "learning_rate": 6.423194390674065e-06,
      "loss": 1.2741,
      "step": 10898
    },
    {
      "epoch": 4.219512195121951,
      "grad_norm": 15.889911651611328,
      "learning_rate": 6.422764227642278e-06,
      "loss": 0.4696,
      "step": 10899
    },
    {
      "epoch": 4.219899341850561,
      "grad_norm": 32.58385467529297,
      "learning_rate": 6.422334064610488e-06,
      "loss": 0.5988,
      "step": 10900
    },
    {
      "epoch": 4.2202864885791715,
      "grad_norm": 24.00495719909668,
      "learning_rate": 6.421903901578699e-06,
      "loss": 0.3094,
      "step": 10901
    },
    {
      "epoch": 4.220673635307782,
      "grad_norm": 32.629520416259766,
      "learning_rate": 6.421473738546909e-06,
      "loss": 1.5093,
      "step": 10902
    },
    {
      "epoch": 4.221060782036392,
      "grad_norm": 39.09632873535156,
      "learning_rate": 6.421043575515122e-06,
      "loss": 2.4572,
      "step": 10903
    },
    {
      "epoch": 4.221447928765002,
      "grad_norm": 17.25222396850586,
      "learning_rate": 6.420613412483332e-06,
      "loss": 1.199,
      "step": 10904
    },
    {
      "epoch": 4.221835075493612,
      "grad_norm": 15.627180099487305,
      "learning_rate": 6.420183249451543e-06,
      "loss": 0.667,
      "step": 10905
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 52.38395690917969,
      "learning_rate": 6.419753086419753e-06,
      "loss": 1.9467,
      "step": 10906
    },
    {
      "epoch": 4.222609368950832,
      "grad_norm": 4.76660680770874,
      "learning_rate": 6.419322923387965e-06,
      "loss": 0.1772,
      "step": 10907
    },
    {
      "epoch": 4.2229965156794425,
      "grad_norm": 249.1787109375,
      "learning_rate": 6.418892760356175e-06,
      "loss": 3.0379,
      "step": 10908
    },
    {
      "epoch": 4.223383662408053,
      "grad_norm": 2.4025070667266846,
      "learning_rate": 6.418462597324387e-06,
      "loss": 0.0674,
      "step": 10909
    },
    {
      "epoch": 4.223770809136663,
      "grad_norm": 23.553781509399414,
      "learning_rate": 6.418032434292597e-06,
      "loss": 1.6692,
      "step": 10910
    },
    {
      "epoch": 4.224157955865273,
      "grad_norm": 27.950496673583984,
      "learning_rate": 6.417602271260809e-06,
      "loss": 1.8736,
      "step": 10911
    },
    {
      "epoch": 4.224545102593883,
      "grad_norm": 42.60626983642578,
      "learning_rate": 6.417172108229019e-06,
      "loss": 0.8896,
      "step": 10912
    },
    {
      "epoch": 4.224932249322493,
      "grad_norm": 37.66188430786133,
      "learning_rate": 6.41674194519723e-06,
      "loss": 1.0561,
      "step": 10913
    },
    {
      "epoch": 4.225319396051104,
      "grad_norm": 22.18960189819336,
      "learning_rate": 6.41631178216544e-06,
      "loss": 1.4103,
      "step": 10914
    },
    {
      "epoch": 4.225706542779713,
      "grad_norm": 16.258134841918945,
      "learning_rate": 6.415881619133653e-06,
      "loss": 0.8714,
      "step": 10915
    },
    {
      "epoch": 4.226093689508324,
      "grad_norm": 25.659120559692383,
      "learning_rate": 6.415451456101863e-06,
      "loss": 0.8851,
      "step": 10916
    },
    {
      "epoch": 4.2264808362369335,
      "grad_norm": 28.340660095214844,
      "learning_rate": 6.415021293070074e-06,
      "loss": 1.5529,
      "step": 10917
    },
    {
      "epoch": 4.226867982965544,
      "grad_norm": 3.790543794631958,
      "learning_rate": 6.414591130038284e-06,
      "loss": 0.1658,
      "step": 10918
    },
    {
      "epoch": 4.2272551296941545,
      "grad_norm": 18.47983741760254,
      "learning_rate": 6.414160967006497e-06,
      "loss": 1.2194,
      "step": 10919
    },
    {
      "epoch": 4.227642276422764,
      "grad_norm": 29.883403778076172,
      "learning_rate": 6.413730803974707e-06,
      "loss": 0.5259,
      "step": 10920
    },
    {
      "epoch": 4.228029423151375,
      "grad_norm": 12.934739112854004,
      "learning_rate": 6.413300640942918e-06,
      "loss": 0.6644,
      "step": 10921
    },
    {
      "epoch": 4.228416569879984,
      "grad_norm": 20.480825424194336,
      "learning_rate": 6.412870477911128e-06,
      "loss": 0.7688,
      "step": 10922
    },
    {
      "epoch": 4.228803716608595,
      "grad_norm": 108.36653137207031,
      "learning_rate": 6.41244031487934e-06,
      "loss": 1.4717,
      "step": 10923
    },
    {
      "epoch": 4.229190863337204,
      "grad_norm": 83.78399658203125,
      "learning_rate": 6.412010151847551e-06,
      "loss": 1.6609,
      "step": 10924
    },
    {
      "epoch": 4.229578010065815,
      "grad_norm": 10.57379150390625,
      "learning_rate": 6.411579988815762e-06,
      "loss": 0.6187,
      "step": 10925
    },
    {
      "epoch": 4.229965156794425,
      "grad_norm": 23.156946182250977,
      "learning_rate": 6.411149825783972e-06,
      "loss": 0.4004,
      "step": 10926
    },
    {
      "epoch": 4.230352303523035,
      "grad_norm": 17.24772834777832,
      "learning_rate": 6.410719662752184e-06,
      "loss": 0.7487,
      "step": 10927
    },
    {
      "epoch": 4.2307394502516456,
      "grad_norm": 35.68418502807617,
      "learning_rate": 6.410289499720394e-06,
      "loss": 1.7838,
      "step": 10928
    },
    {
      "epoch": 4.231126596980255,
      "grad_norm": 21.213054656982422,
      "learning_rate": 6.409859336688605e-06,
      "loss": 1.0078,
      "step": 10929
    },
    {
      "epoch": 4.231513743708866,
      "grad_norm": 16.59688377380371,
      "learning_rate": 6.409429173656816e-06,
      "loss": 0.8948,
      "step": 10930
    },
    {
      "epoch": 4.231900890437476,
      "grad_norm": 19.103622436523438,
      "learning_rate": 6.408999010625028e-06,
      "loss": 0.6141,
      "step": 10931
    },
    {
      "epoch": 4.232288037166086,
      "grad_norm": 33.30027770996094,
      "learning_rate": 6.408568847593238e-06,
      "loss": 2.0465,
      "step": 10932
    },
    {
      "epoch": 4.232675183894696,
      "grad_norm": 27.657278060913086,
      "learning_rate": 6.408138684561449e-06,
      "loss": 1.0747,
      "step": 10933
    },
    {
      "epoch": 4.233062330623306,
      "grad_norm": 11.881933212280273,
      "learning_rate": 6.407708521529659e-06,
      "loss": 0.1637,
      "step": 10934
    },
    {
      "epoch": 4.2334494773519165,
      "grad_norm": 17.397083282470703,
      "learning_rate": 6.407278358497872e-06,
      "loss": 0.6788,
      "step": 10935
    },
    {
      "epoch": 4.233836624080526,
      "grad_norm": 28.078914642333984,
      "learning_rate": 6.406848195466082e-06,
      "loss": 1.3078,
      "step": 10936
    },
    {
      "epoch": 4.234223770809137,
      "grad_norm": 61.172000885009766,
      "learning_rate": 6.406418032434293e-06,
      "loss": 0.7354,
      "step": 10937
    },
    {
      "epoch": 4.234610917537747,
      "grad_norm": 193.5126495361328,
      "learning_rate": 6.405987869402504e-06,
      "loss": 1.1993,
      "step": 10938
    },
    {
      "epoch": 4.234998064266357,
      "grad_norm": 33.28135299682617,
      "learning_rate": 6.405557706370716e-06,
      "loss": 0.9871,
      "step": 10939
    },
    {
      "epoch": 4.235385210994967,
      "grad_norm": 41.12767028808594,
      "learning_rate": 6.405127543338926e-06,
      "loss": 0.4654,
      "step": 10940
    },
    {
      "epoch": 4.235772357723577,
      "grad_norm": 11.13131046295166,
      "learning_rate": 6.404697380307137e-06,
      "loss": 0.95,
      "step": 10941
    },
    {
      "epoch": 4.236159504452187,
      "grad_norm": 34.016685485839844,
      "learning_rate": 6.404267217275348e-06,
      "loss": 1.1585,
      "step": 10942
    },
    {
      "epoch": 4.236546651180798,
      "grad_norm": 16.337671279907227,
      "learning_rate": 6.403837054243559e-06,
      "loss": 0.5374,
      "step": 10943
    },
    {
      "epoch": 4.2369337979094075,
      "grad_norm": 13.627791404724121,
      "learning_rate": 6.403406891211769e-06,
      "loss": 0.9635,
      "step": 10944
    },
    {
      "epoch": 4.237320944638018,
      "grad_norm": 21.127168655395508,
      "learning_rate": 6.402976728179981e-06,
      "loss": 1.7803,
      "step": 10945
    },
    {
      "epoch": 4.237708091366628,
      "grad_norm": 19.7287654876709,
      "learning_rate": 6.402546565148192e-06,
      "loss": 1.373,
      "step": 10946
    },
    {
      "epoch": 4.238095238095238,
      "grad_norm": 17.179393768310547,
      "learning_rate": 6.402116402116403e-06,
      "loss": 1.4553,
      "step": 10947
    },
    {
      "epoch": 4.238482384823849,
      "grad_norm": 29.718612670898438,
      "learning_rate": 6.401686239084613e-06,
      "loss": 1.7037,
      "step": 10948
    },
    {
      "epoch": 4.238869531552458,
      "grad_norm": 8.469766616821289,
      "learning_rate": 6.401256076052824e-06,
      "loss": 0.0734,
      "step": 10949
    },
    {
      "epoch": 4.239256678281069,
      "grad_norm": 9.22104263305664,
      "learning_rate": 6.400825913021036e-06,
      "loss": 0.6022,
      "step": 10950
    },
    {
      "epoch": 4.239643825009678,
      "grad_norm": 32.86765670776367,
      "learning_rate": 6.400395749989247e-06,
      "loss": 1.2043,
      "step": 10951
    },
    {
      "epoch": 4.240030971738289,
      "grad_norm": 6.327150821685791,
      "learning_rate": 6.399965586957457e-06,
      "loss": 0.9941,
      "step": 10952
    },
    {
      "epoch": 4.2404181184668985,
      "grad_norm": 99.9637680053711,
      "learning_rate": 6.399535423925668e-06,
      "loss": 1.0854,
      "step": 10953
    },
    {
      "epoch": 4.240805265195509,
      "grad_norm": 9.994560241699219,
      "learning_rate": 6.39910526089388e-06,
      "loss": 0.2736,
      "step": 10954
    },
    {
      "epoch": 4.2411924119241196,
      "grad_norm": 35.022430419921875,
      "learning_rate": 6.398675097862091e-06,
      "loss": 1.23,
      "step": 10955
    },
    {
      "epoch": 4.241579558652729,
      "grad_norm": 46.2214241027832,
      "learning_rate": 6.398244934830301e-06,
      "loss": 1.1733,
      "step": 10956
    },
    {
      "epoch": 4.24196670538134,
      "grad_norm": 18.53240203857422,
      "learning_rate": 6.397814771798512e-06,
      "loss": 0.6366,
      "step": 10957
    },
    {
      "epoch": 4.242353852109949,
      "grad_norm": 43.43576431274414,
      "learning_rate": 6.397384608766723e-06,
      "loss": 1.6006,
      "step": 10958
    },
    {
      "epoch": 4.24274099883856,
      "grad_norm": 44.830284118652344,
      "learning_rate": 6.396954445734934e-06,
      "loss": 1.1453,
      "step": 10959
    },
    {
      "epoch": 4.24312814556717,
      "grad_norm": 68.92298889160156,
      "learning_rate": 6.396524282703145e-06,
      "loss": 1.0931,
      "step": 10960
    },
    {
      "epoch": 4.24351529229578,
      "grad_norm": 110.09489440917969,
      "learning_rate": 6.396094119671356e-06,
      "loss": 2.3899,
      "step": 10961
    },
    {
      "epoch": 4.2439024390243905,
      "grad_norm": 57.86872863769531,
      "learning_rate": 6.395663956639567e-06,
      "loss": 2.7662,
      "step": 10962
    },
    {
      "epoch": 4.244289585753,
      "grad_norm": 22.789623260498047,
      "learning_rate": 6.395233793607778e-06,
      "loss": 0.4258,
      "step": 10963
    },
    {
      "epoch": 4.244676732481611,
      "grad_norm": 25.181665420532227,
      "learning_rate": 6.394803630575988e-06,
      "loss": 1.8622,
      "step": 10964
    },
    {
      "epoch": 4.245063879210221,
      "grad_norm": 57.40576171875,
      "learning_rate": 6.394373467544199e-06,
      "loss": 1.073,
      "step": 10965
    },
    {
      "epoch": 4.245451025938831,
      "grad_norm": 43.447593688964844,
      "learning_rate": 6.393943304512411e-06,
      "loss": 1.482,
      "step": 10966
    },
    {
      "epoch": 4.245838172667441,
      "grad_norm": 11.815140724182129,
      "learning_rate": 6.393513141480622e-06,
      "loss": 0.6275,
      "step": 10967
    },
    {
      "epoch": 4.246225319396051,
      "grad_norm": 8.53173828125,
      "learning_rate": 6.393082978448832e-06,
      "loss": 0.5437,
      "step": 10968
    },
    {
      "epoch": 4.246612466124661,
      "grad_norm": 76.4466552734375,
      "learning_rate": 6.392652815417043e-06,
      "loss": 1.7717,
      "step": 10969
    },
    {
      "epoch": 4.246999612853271,
      "grad_norm": 13.505998611450195,
      "learning_rate": 6.392222652385255e-06,
      "loss": 0.6193,
      "step": 10970
    },
    {
      "epoch": 4.2473867595818815,
      "grad_norm": 20.879640579223633,
      "learning_rate": 6.391792489353466e-06,
      "loss": 0.4991,
      "step": 10971
    },
    {
      "epoch": 4.247773906310492,
      "grad_norm": 28.27606201171875,
      "learning_rate": 6.391362326321676e-06,
      "loss": 1.2225,
      "step": 10972
    },
    {
      "epoch": 4.248161053039102,
      "grad_norm": 21.790754318237305,
      "learning_rate": 6.390932163289887e-06,
      "loss": 1.1772,
      "step": 10973
    },
    {
      "epoch": 4.248548199767712,
      "grad_norm": 37.88959503173828,
      "learning_rate": 6.390502000258098e-06,
      "loss": 1.2869,
      "step": 10974
    },
    {
      "epoch": 4.248935346496322,
      "grad_norm": 12.806538581848145,
      "learning_rate": 6.39007183722631e-06,
      "loss": 0.5867,
      "step": 10975
    },
    {
      "epoch": 4.249322493224932,
      "grad_norm": 27.914827346801758,
      "learning_rate": 6.38964167419452e-06,
      "loss": 1.7956,
      "step": 10976
    },
    {
      "epoch": 4.249709639953543,
      "grad_norm": 43.5687370300293,
      "learning_rate": 6.389211511162732e-06,
      "loss": 1.4386,
      "step": 10977
    },
    {
      "epoch": 4.250096786682152,
      "grad_norm": 23.679168701171875,
      "learning_rate": 6.388781348130942e-06,
      "loss": 0.3846,
      "step": 10978
    },
    {
      "epoch": 4.250483933410763,
      "grad_norm": 17.046344757080078,
      "learning_rate": 6.388351185099153e-06,
      "loss": 1.5126,
      "step": 10979
    },
    {
      "epoch": 4.2508710801393725,
      "grad_norm": 28.352935791015625,
      "learning_rate": 6.387921022067363e-06,
      "loss": 1.0704,
      "step": 10980
    },
    {
      "epoch": 4.251258226867983,
      "grad_norm": 13.938705444335938,
      "learning_rate": 6.387490859035576e-06,
      "loss": 0.9405,
      "step": 10981
    },
    {
      "epoch": 4.251645373596594,
      "grad_norm": 13.391196250915527,
      "learning_rate": 6.387060696003786e-06,
      "loss": 1.6978,
      "step": 10982
    },
    {
      "epoch": 4.252032520325203,
      "grad_norm": 6.726657390594482,
      "learning_rate": 6.386630532971997e-06,
      "loss": 0.974,
      "step": 10983
    },
    {
      "epoch": 4.252419667053814,
      "grad_norm": 10.912055969238281,
      "learning_rate": 6.386200369940207e-06,
      "loss": 0.7385,
      "step": 10984
    },
    {
      "epoch": 4.252806813782423,
      "grad_norm": 15.111573219299316,
      "learning_rate": 6.38577020690842e-06,
      "loss": 1.7978,
      "step": 10985
    },
    {
      "epoch": 4.253193960511034,
      "grad_norm": 17.50765609741211,
      "learning_rate": 6.38534004387663e-06,
      "loss": 0.9752,
      "step": 10986
    },
    {
      "epoch": 4.253581107239643,
      "grad_norm": 52.28587341308594,
      "learning_rate": 6.384909880844841e-06,
      "loss": 1.2605,
      "step": 10987
    },
    {
      "epoch": 4.253968253968254,
      "grad_norm": 52.264190673828125,
      "learning_rate": 6.384479717813051e-06,
      "loss": 0.7285,
      "step": 10988
    },
    {
      "epoch": 4.2543554006968645,
      "grad_norm": 53.05548858642578,
      "learning_rate": 6.384049554781263e-06,
      "loss": 1.1241,
      "step": 10989
    },
    {
      "epoch": 4.254742547425474,
      "grad_norm": 7.727899074554443,
      "learning_rate": 6.383619391749474e-06,
      "loss": 1.0193,
      "step": 10990
    },
    {
      "epoch": 4.255129694154085,
      "grad_norm": 7.320605278015137,
      "learning_rate": 6.383189228717685e-06,
      "loss": 0.9848,
      "step": 10991
    },
    {
      "epoch": 4.255516840882694,
      "grad_norm": 17.63370704650879,
      "learning_rate": 6.382759065685895e-06,
      "loss": 0.636,
      "step": 10992
    },
    {
      "epoch": 4.255903987611305,
      "grad_norm": 13.032929420471191,
      "learning_rate": 6.382328902654107e-06,
      "loss": 0.6865,
      "step": 10993
    },
    {
      "epoch": 4.256291134339915,
      "grad_norm": 22.837478637695312,
      "learning_rate": 6.381898739622317e-06,
      "loss": 0.7051,
      "step": 10994
    },
    {
      "epoch": 4.256678281068525,
      "grad_norm": 28.80657386779785,
      "learning_rate": 6.381468576590528e-06,
      "loss": 1.9815,
      "step": 10995
    },
    {
      "epoch": 4.257065427797135,
      "grad_norm": 10.724778175354004,
      "learning_rate": 6.381038413558739e-06,
      "loss": 0.755,
      "step": 10996
    },
    {
      "epoch": 4.257452574525745,
      "grad_norm": 40.75516891479492,
      "learning_rate": 6.380608250526951e-06,
      "loss": 1.6763,
      "step": 10997
    },
    {
      "epoch": 4.2578397212543555,
      "grad_norm": 10.19853401184082,
      "learning_rate": 6.380178087495161e-06,
      "loss": 0.5631,
      "step": 10998
    },
    {
      "epoch": 4.258226867982966,
      "grad_norm": 7.459163188934326,
      "learning_rate": 6.379747924463372e-06,
      "loss": 0.178,
      "step": 10999
    },
    {
      "epoch": 4.258614014711576,
      "grad_norm": 30.342548370361328,
      "learning_rate": 6.379317761431582e-06,
      "loss": 1.8366,
      "step": 11000
    },
    {
      "epoch": 4.259001161440186,
      "grad_norm": 28.649385452270508,
      "learning_rate": 6.378887598399795e-06,
      "loss": 2.7289,
      "step": 11001
    },
    {
      "epoch": 4.259388308168796,
      "grad_norm": 14.72944450378418,
      "learning_rate": 6.378457435368005e-06,
      "loss": 0.3862,
      "step": 11002
    },
    {
      "epoch": 4.259775454897406,
      "grad_norm": 25.582870483398438,
      "learning_rate": 6.378027272336216e-06,
      "loss": 0.9016,
      "step": 11003
    },
    {
      "epoch": 4.260162601626016,
      "grad_norm": 78.90159606933594,
      "learning_rate": 6.377597109304426e-06,
      "loss": 0.9749,
      "step": 11004
    },
    {
      "epoch": 4.260549748354626,
      "grad_norm": 26.152833938598633,
      "learning_rate": 6.377166946272639e-06,
      "loss": 1.318,
      "step": 11005
    },
    {
      "epoch": 4.260936895083237,
      "grad_norm": 27.584407806396484,
      "learning_rate": 6.376736783240849e-06,
      "loss": 3.0109,
      "step": 11006
    },
    {
      "epoch": 4.2613240418118465,
      "grad_norm": 5.554773807525635,
      "learning_rate": 6.37630662020906e-06,
      "loss": 0.2197,
      "step": 11007
    },
    {
      "epoch": 4.261711188540457,
      "grad_norm": 42.07206726074219,
      "learning_rate": 6.37587645717727e-06,
      "loss": 1.8211,
      "step": 11008
    },
    {
      "epoch": 4.262098335269067,
      "grad_norm": 15.040785789489746,
      "learning_rate": 6.375446294145482e-06,
      "loss": 0.4841,
      "step": 11009
    },
    {
      "epoch": 4.262485481997677,
      "grad_norm": 15.810977935791016,
      "learning_rate": 6.375016131113692e-06,
      "loss": 1.0436,
      "step": 11010
    },
    {
      "epoch": 4.262872628726287,
      "grad_norm": 30.44577407836914,
      "learning_rate": 6.374585968081904e-06,
      "loss": 1.4932,
      "step": 11011
    },
    {
      "epoch": 4.263259775454897,
      "grad_norm": 37.72507858276367,
      "learning_rate": 6.374155805050114e-06,
      "loss": 0.9799,
      "step": 11012
    },
    {
      "epoch": 4.263646922183508,
      "grad_norm": 69.5774154663086,
      "learning_rate": 6.373725642018326e-06,
      "loss": 0.461,
      "step": 11013
    },
    {
      "epoch": 4.2640340689121174,
      "grad_norm": 28.39430046081543,
      "learning_rate": 6.373295478986536e-06,
      "loss": 2.2469,
      "step": 11014
    },
    {
      "epoch": 4.264421215640728,
      "grad_norm": 30.798179626464844,
      "learning_rate": 6.372865315954747e-06,
      "loss": 1.0092,
      "step": 11015
    },
    {
      "epoch": 4.264808362369338,
      "grad_norm": 14.839812278747559,
      "learning_rate": 6.372435152922957e-06,
      "loss": 0.6562,
      "step": 11016
    },
    {
      "epoch": 4.265195509097948,
      "grad_norm": 5.386080741882324,
      "learning_rate": 6.37200498989117e-06,
      "loss": 0.205,
      "step": 11017
    },
    {
      "epoch": 4.265582655826559,
      "grad_norm": 22.607791900634766,
      "learning_rate": 6.37157482685938e-06,
      "loss": 1.4374,
      "step": 11018
    },
    {
      "epoch": 4.265969802555168,
      "grad_norm": 27.01436424255371,
      "learning_rate": 6.371144663827591e-06,
      "loss": 1.4108,
      "step": 11019
    },
    {
      "epoch": 4.266356949283779,
      "grad_norm": 85.53883361816406,
      "learning_rate": 6.370714500795803e-06,
      "loss": 1.9101,
      "step": 11020
    },
    {
      "epoch": 4.266744096012388,
      "grad_norm": 43.43814468383789,
      "learning_rate": 6.370284337764014e-06,
      "loss": 0.4167,
      "step": 11021
    },
    {
      "epoch": 4.267131242740999,
      "grad_norm": 16.500520706176758,
      "learning_rate": 6.369854174732224e-06,
      "loss": 1.5272,
      "step": 11022
    },
    {
      "epoch": 4.267518389469609,
      "grad_norm": 14.888916969299316,
      "learning_rate": 6.369424011700435e-06,
      "loss": 0.6906,
      "step": 11023
    },
    {
      "epoch": 4.267905536198219,
      "grad_norm": 27.674592971801758,
      "learning_rate": 6.368993848668646e-06,
      "loss": 0.1494,
      "step": 11024
    },
    {
      "epoch": 4.2682926829268295,
      "grad_norm": 26.481948852539062,
      "learning_rate": 6.368563685636857e-06,
      "loss": 1.487,
      "step": 11025
    },
    {
      "epoch": 4.268679829655439,
      "grad_norm": 33.55231475830078,
      "learning_rate": 6.368133522605068e-06,
      "loss": 1.2375,
      "step": 11026
    },
    {
      "epoch": 4.26906697638405,
      "grad_norm": 19.454710006713867,
      "learning_rate": 6.367703359573279e-06,
      "loss": 0.517,
      "step": 11027
    },
    {
      "epoch": 4.269454123112659,
      "grad_norm": 21.26791763305664,
      "learning_rate": 6.36727319654149e-06,
      "loss": 0.6458,
      "step": 11028
    },
    {
      "epoch": 4.26984126984127,
      "grad_norm": 28.67617416381836,
      "learning_rate": 6.366843033509701e-06,
      "loss": 0.9231,
      "step": 11029
    },
    {
      "epoch": 4.27022841656988,
      "grad_norm": 28.82870864868164,
      "learning_rate": 6.366412870477911e-06,
      "loss": 2.9786,
      "step": 11030
    },
    {
      "epoch": 4.27061556329849,
      "grad_norm": 60.85640335083008,
      "learning_rate": 6.365982707446122e-06,
      "loss": 1.1838,
      "step": 11031
    },
    {
      "epoch": 4.2710027100271,
      "grad_norm": 28.047481536865234,
      "learning_rate": 6.365552544414334e-06,
      "loss": 1.9428,
      "step": 11032
    },
    {
      "epoch": 4.27138985675571,
      "grad_norm": 24.08918571472168,
      "learning_rate": 6.365122381382545e-06,
      "loss": 0.8327,
      "step": 11033
    },
    {
      "epoch": 4.2717770034843205,
      "grad_norm": 57.08564376831055,
      "learning_rate": 6.364692218350755e-06,
      "loss": 0.6351,
      "step": 11034
    },
    {
      "epoch": 4.272164150212931,
      "grad_norm": 64.51348876953125,
      "learning_rate": 6.364262055318966e-06,
      "loss": 1.9512,
      "step": 11035
    },
    {
      "epoch": 4.272551296941541,
      "grad_norm": 31.571910858154297,
      "learning_rate": 6.363831892287178e-06,
      "loss": 1.3512,
      "step": 11036
    },
    {
      "epoch": 4.272938443670151,
      "grad_norm": 67.17137908935547,
      "learning_rate": 6.363401729255389e-06,
      "loss": 2.2021,
      "step": 11037
    },
    {
      "epoch": 4.273325590398761,
      "grad_norm": 21.05585289001465,
      "learning_rate": 6.362971566223599e-06,
      "loss": 1.2571,
      "step": 11038
    },
    {
      "epoch": 4.273712737127371,
      "grad_norm": 15.590513229370117,
      "learning_rate": 6.36254140319181e-06,
      "loss": 0.8412,
      "step": 11039
    },
    {
      "epoch": 4.274099883855982,
      "grad_norm": 30.49500274658203,
      "learning_rate": 6.362111240160021e-06,
      "loss": 2.1807,
      "step": 11040
    },
    {
      "epoch": 4.2744870305845915,
      "grad_norm": 26.10503387451172,
      "learning_rate": 6.361681077128233e-06,
      "loss": 0.8677,
      "step": 11041
    },
    {
      "epoch": 4.274874177313202,
      "grad_norm": 58.312923431396484,
      "learning_rate": 6.361250914096443e-06,
      "loss": 2.2058,
      "step": 11042
    },
    {
      "epoch": 4.275261324041812,
      "grad_norm": 23.035932540893555,
      "learning_rate": 6.360820751064654e-06,
      "loss": 2.4928,
      "step": 11043
    },
    {
      "epoch": 4.275648470770422,
      "grad_norm": 13.658116340637207,
      "learning_rate": 6.360390588032865e-06,
      "loss": 1.5443,
      "step": 11044
    },
    {
      "epoch": 4.276035617499032,
      "grad_norm": 33.8514518737793,
      "learning_rate": 6.359960425001076e-06,
      "loss": 0.972,
      "step": 11045
    },
    {
      "epoch": 4.276422764227642,
      "grad_norm": 15.904190063476562,
      "learning_rate": 6.359530261969286e-06,
      "loss": 1.5043,
      "step": 11046
    },
    {
      "epoch": 4.276809910956253,
      "grad_norm": 2.6058809757232666,
      "learning_rate": 6.359100098937498e-06,
      "loss": 0.1217,
      "step": 11047
    },
    {
      "epoch": 4.277197057684862,
      "grad_norm": 4.452722072601318,
      "learning_rate": 6.358669935905709e-06,
      "loss": 0.1588,
      "step": 11048
    },
    {
      "epoch": 4.277584204413473,
      "grad_norm": 11.150016784667969,
      "learning_rate": 6.35823977287392e-06,
      "loss": 0.7151,
      "step": 11049
    },
    {
      "epoch": 4.2779713511420825,
      "grad_norm": 2.5541491508483887,
      "learning_rate": 6.35780960984213e-06,
      "loss": 0.122,
      "step": 11050
    },
    {
      "epoch": 4.278358497870693,
      "grad_norm": 67.06938171386719,
      "learning_rate": 6.357379446810341e-06,
      "loss": 0.4734,
      "step": 11051
    },
    {
      "epoch": 4.2787456445993035,
      "grad_norm": 6.70587682723999,
      "learning_rate": 6.356949283778553e-06,
      "loss": 0.0921,
      "step": 11052
    },
    {
      "epoch": 4.279132791327913,
      "grad_norm": 18.98834228515625,
      "learning_rate": 6.356519120746764e-06,
      "loss": 0.7729,
      "step": 11053
    },
    {
      "epoch": 4.279519938056524,
      "grad_norm": 32.22737121582031,
      "learning_rate": 6.356088957714974e-06,
      "loss": 1.8951,
      "step": 11054
    },
    {
      "epoch": 4.279907084785133,
      "grad_norm": 39.284210205078125,
      "learning_rate": 6.355658794683185e-06,
      "loss": 0.8502,
      "step": 11055
    },
    {
      "epoch": 4.280294231513744,
      "grad_norm": 41.024288177490234,
      "learning_rate": 6.355228631651397e-06,
      "loss": 1.2096,
      "step": 11056
    },
    {
      "epoch": 4.280681378242354,
      "grad_norm": 46.4556884765625,
      "learning_rate": 6.354798468619608e-06,
      "loss": 1.2293,
      "step": 11057
    },
    {
      "epoch": 4.281068524970964,
      "grad_norm": 54.39955520629883,
      "learning_rate": 6.354368305587818e-06,
      "loss": 0.7118,
      "step": 11058
    },
    {
      "epoch": 4.281455671699574,
      "grad_norm": 27.60944938659668,
      "learning_rate": 6.35393814255603e-06,
      "loss": 1.3034,
      "step": 11059
    },
    {
      "epoch": 4.281842818428184,
      "grad_norm": 10.837437629699707,
      "learning_rate": 6.35350797952424e-06,
      "loss": 0.6628,
      "step": 11060
    },
    {
      "epoch": 4.2822299651567945,
      "grad_norm": 35.968360900878906,
      "learning_rate": 6.353077816492451e-06,
      "loss": 1.1147,
      "step": 11061
    },
    {
      "epoch": 4.282617111885404,
      "grad_norm": 21.019622802734375,
      "learning_rate": 6.352647653460662e-06,
      "loss": 2.1729,
      "step": 11062
    },
    {
      "epoch": 4.283004258614015,
      "grad_norm": 6.823390007019043,
      "learning_rate": 6.352217490428874e-06,
      "loss": 0.8402,
      "step": 11063
    },
    {
      "epoch": 4.283391405342625,
      "grad_norm": 25.582578659057617,
      "learning_rate": 6.351787327397084e-06,
      "loss": 1.7089,
      "step": 11064
    },
    {
      "epoch": 4.283778552071235,
      "grad_norm": 11.501407623291016,
      "learning_rate": 6.351357164365295e-06,
      "loss": 0.555,
      "step": 11065
    },
    {
      "epoch": 4.284165698799845,
      "grad_norm": 17.448955535888672,
      "learning_rate": 6.350927001333505e-06,
      "loss": 0.8145,
      "step": 11066
    },
    {
      "epoch": 4.284552845528455,
      "grad_norm": 63.865562438964844,
      "learning_rate": 6.3504968383017176e-06,
      "loss": 1.2349,
      "step": 11067
    },
    {
      "epoch": 4.2849399922570655,
      "grad_norm": 37.95570755004883,
      "learning_rate": 6.350066675269928e-06,
      "loss": 0.354,
      "step": 11068
    },
    {
      "epoch": 4.285327138985676,
      "grad_norm": 33.836971282958984,
      "learning_rate": 6.349636512238139e-06,
      "loss": 1.5705,
      "step": 11069
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 21.89960479736328,
      "learning_rate": 6.349206349206349e-06,
      "loss": 2.8156,
      "step": 11070
    },
    {
      "epoch": 4.286101432442896,
      "grad_norm": 10.028435707092285,
      "learning_rate": 6.3487761861745615e-06,
      "loss": 0.4844,
      "step": 11071
    },
    {
      "epoch": 4.286488579171506,
      "grad_norm": 43.617183685302734,
      "learning_rate": 6.348346023142772e-06,
      "loss": 1.4354,
      "step": 11072
    },
    {
      "epoch": 4.286875725900116,
      "grad_norm": 47.947303771972656,
      "learning_rate": 6.347915860110983e-06,
      "loss": 0.3178,
      "step": 11073
    },
    {
      "epoch": 4.287262872628727,
      "grad_norm": 19.71891212463379,
      "learning_rate": 6.347485697079193e-06,
      "loss": 1.1961,
      "step": 11074
    },
    {
      "epoch": 4.287650019357336,
      "grad_norm": 60.35173416137695,
      "learning_rate": 6.347055534047405e-06,
      "loss": 1.7931,
      "step": 11075
    },
    {
      "epoch": 4.288037166085947,
      "grad_norm": 52.69297409057617,
      "learning_rate": 6.346625371015615e-06,
      "loss": 2.0351,
      "step": 11076
    },
    {
      "epoch": 4.2884243128145565,
      "grad_norm": 61.5107421875,
      "learning_rate": 6.346195207983827e-06,
      "loss": 1.3526,
      "step": 11077
    },
    {
      "epoch": 4.288811459543167,
      "grad_norm": 21.873126983642578,
      "learning_rate": 6.345765044952037e-06,
      "loss": 1.7783,
      "step": 11078
    },
    {
      "epoch": 4.289198606271777,
      "grad_norm": 12.49953842163086,
      "learning_rate": 6.345334881920249e-06,
      "loss": 0.6115,
      "step": 11079
    },
    {
      "epoch": 4.289585753000387,
      "grad_norm": 17.706071853637695,
      "learning_rate": 6.344904718888459e-06,
      "loss": 2.993,
      "step": 11080
    },
    {
      "epoch": 4.289972899728998,
      "grad_norm": 8.22217845916748,
      "learning_rate": 6.34447455585667e-06,
      "loss": 0.3554,
      "step": 11081
    },
    {
      "epoch": 4.290360046457607,
      "grad_norm": 12.841621398925781,
      "learning_rate": 6.34404439282488e-06,
      "loss": 0.5335,
      "step": 11082
    },
    {
      "epoch": 4.290747193186218,
      "grad_norm": 50.7646484375,
      "learning_rate": 6.343614229793093e-06,
      "loss": 1.8989,
      "step": 11083
    },
    {
      "epoch": 4.291134339914827,
      "grad_norm": 9.73289966583252,
      "learning_rate": 6.343184066761303e-06,
      "loss": 0.5893,
      "step": 11084
    },
    {
      "epoch": 4.291521486643438,
      "grad_norm": 10.874235153198242,
      "learning_rate": 6.342753903729514e-06,
      "loss": 0.6484,
      "step": 11085
    },
    {
      "epoch": 4.291908633372048,
      "grad_norm": 27.913618087768555,
      "learning_rate": 6.342323740697724e-06,
      "loss": 1.4551,
      "step": 11086
    },
    {
      "epoch": 4.292295780100658,
      "grad_norm": 116.40730285644531,
      "learning_rate": 6.3418935776659366e-06,
      "loss": 0.8625,
      "step": 11087
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 17.28363037109375,
      "learning_rate": 6.341463414634147e-06,
      "loss": 1.9331,
      "step": 11088
    },
    {
      "epoch": 4.293070073557878,
      "grad_norm": 157.90306091308594,
      "learning_rate": 6.341033251602358e-06,
      "loss": 1.7501,
      "step": 11089
    },
    {
      "epoch": 4.293457220286489,
      "grad_norm": 20.52397918701172,
      "learning_rate": 6.340603088570568e-06,
      "loss": 1.562,
      "step": 11090
    },
    {
      "epoch": 4.293844367015098,
      "grad_norm": 9.893752098083496,
      "learning_rate": 6.34017292553878e-06,
      "loss": 0.1277,
      "step": 11091
    },
    {
      "epoch": 4.294231513743709,
      "grad_norm": 8.732270240783691,
      "learning_rate": 6.339742762506991e-06,
      "loss": 0.582,
      "step": 11092
    },
    {
      "epoch": 4.294618660472319,
      "grad_norm": 23.744524002075195,
      "learning_rate": 6.339312599475202e-06,
      "loss": 1.7422,
      "step": 11093
    },
    {
      "epoch": 4.295005807200929,
      "grad_norm": 86.04573822021484,
      "learning_rate": 6.338882436443412e-06,
      "loss": 1.5994,
      "step": 11094
    },
    {
      "epoch": 4.2953929539295395,
      "grad_norm": 49.20360565185547,
      "learning_rate": 6.338452273411624e-06,
      "loss": 0.6772,
      "step": 11095
    },
    {
      "epoch": 4.295780100658149,
      "grad_norm": 3.337336540222168,
      "learning_rate": 6.338022110379834e-06,
      "loss": 0.1335,
      "step": 11096
    },
    {
      "epoch": 4.29616724738676,
      "grad_norm": 52.107791900634766,
      "learning_rate": 6.337591947348045e-06,
      "loss": 0.7775,
      "step": 11097
    },
    {
      "epoch": 4.29655439411537,
      "grad_norm": 30.248432159423828,
      "learning_rate": 6.337161784316256e-06,
      "loss": 1.0083,
      "step": 11098
    },
    {
      "epoch": 4.29694154084398,
      "grad_norm": 15.884591102600098,
      "learning_rate": 6.336731621284468e-06,
      "loss": 1.1598,
      "step": 11099
    },
    {
      "epoch": 4.29732868757259,
      "grad_norm": 44.10295867919922,
      "learning_rate": 6.336301458252678e-06,
      "loss": 0.575,
      "step": 11100
    },
    {
      "epoch": 4.2977158343012,
      "grad_norm": 76.48114776611328,
      "learning_rate": 6.335871295220889e-06,
      "loss": 1.825,
      "step": 11101
    },
    {
      "epoch": 4.29810298102981,
      "grad_norm": 18.947818756103516,
      "learning_rate": 6.335441132189101e-06,
      "loss": 1.1206,
      "step": 11102
    },
    {
      "epoch": 4.29849012775842,
      "grad_norm": 36.79535675048828,
      "learning_rate": 6.335010969157312e-06,
      "loss": 1.7105,
      "step": 11103
    },
    {
      "epoch": 4.2988772744870305,
      "grad_norm": 45.66439437866211,
      "learning_rate": 6.334580806125522e-06,
      "loss": 1.5569,
      "step": 11104
    },
    {
      "epoch": 4.299264421215641,
      "grad_norm": 30.04446792602539,
      "learning_rate": 6.334150643093733e-06,
      "loss": 1.7053,
      "step": 11105
    },
    {
      "epoch": 4.299651567944251,
      "grad_norm": 32.301326751708984,
      "learning_rate": 6.333720480061944e-06,
      "loss": 1.3226,
      "step": 11106
    },
    {
      "epoch": 4.300038714672861,
      "grad_norm": 17.332603454589844,
      "learning_rate": 6.3332903170301556e-06,
      "loss": 0.7593,
      "step": 11107
    },
    {
      "epoch": 4.300425861401471,
      "grad_norm": 10.446497917175293,
      "learning_rate": 6.332860153998366e-06,
      "loss": 0.3779,
      "step": 11108
    },
    {
      "epoch": 4.300813008130081,
      "grad_norm": 17.189590454101562,
      "learning_rate": 6.332429990966577e-06,
      "loss": 1.1329,
      "step": 11109
    },
    {
      "epoch": 4.301200154858692,
      "grad_norm": 28.517961502075195,
      "learning_rate": 6.331999827934788e-06,
      "loss": 0.7625,
      "step": 11110
    },
    {
      "epoch": 4.301587301587301,
      "grad_norm": 65.87872314453125,
      "learning_rate": 6.331569664902999e-06,
      "loss": 3.6491,
      "step": 11111
    },
    {
      "epoch": 4.301974448315912,
      "grad_norm": 31.26498031616211,
      "learning_rate": 6.331139501871209e-06,
      "loss": 0.9318,
      "step": 11112
    },
    {
      "epoch": 4.3023615950445215,
      "grad_norm": 52.220314025878906,
      "learning_rate": 6.330709338839421e-06,
      "loss": 2.6694,
      "step": 11113
    },
    {
      "epoch": 4.302748741773132,
      "grad_norm": 24.960588455200195,
      "learning_rate": 6.330279175807632e-06,
      "loss": 1.1747,
      "step": 11114
    },
    {
      "epoch": 4.303135888501743,
      "grad_norm": 1.4940885305404663,
      "learning_rate": 6.329849012775843e-06,
      "loss": 0.0455,
      "step": 11115
    },
    {
      "epoch": 4.303523035230352,
      "grad_norm": 2.664806604385376,
      "learning_rate": 6.329418849744053e-06,
      "loss": 0.0765,
      "step": 11116
    },
    {
      "epoch": 4.303910181958963,
      "grad_norm": 28.300451278686523,
      "learning_rate": 6.328988686712264e-06,
      "loss": 0.8136,
      "step": 11117
    },
    {
      "epoch": 4.304297328687572,
      "grad_norm": 26.548704147338867,
      "learning_rate": 6.328558523680476e-06,
      "loss": 1.1534,
      "step": 11118
    },
    {
      "epoch": 4.304684475416183,
      "grad_norm": 23.579769134521484,
      "learning_rate": 6.328128360648687e-06,
      "loss": 1.3146,
      "step": 11119
    },
    {
      "epoch": 4.305071622144792,
      "grad_norm": 54.448883056640625,
      "learning_rate": 6.327698197616897e-06,
      "loss": 1.1438,
      "step": 11120
    },
    {
      "epoch": 4.305458768873403,
      "grad_norm": 8.056846618652344,
      "learning_rate": 6.327268034585108e-06,
      "loss": 0.4335,
      "step": 11121
    },
    {
      "epoch": 4.3058459156020135,
      "grad_norm": 29.93140983581543,
      "learning_rate": 6.32683787155332e-06,
      "loss": 0.8941,
      "step": 11122
    },
    {
      "epoch": 4.306233062330623,
      "grad_norm": 111.9439697265625,
      "learning_rate": 6.326407708521531e-06,
      "loss": 0.87,
      "step": 11123
    },
    {
      "epoch": 4.306620209059234,
      "grad_norm": 15.912211418151855,
      "learning_rate": 6.325977545489741e-06,
      "loss": 0.7063,
      "step": 11124
    },
    {
      "epoch": 4.307007355787843,
      "grad_norm": 40.50998306274414,
      "learning_rate": 6.325547382457952e-06,
      "loss": 1.5683,
      "step": 11125
    },
    {
      "epoch": 4.307394502516454,
      "grad_norm": 46.285888671875,
      "learning_rate": 6.325117219426163e-06,
      "loss": 2.6799,
      "step": 11126
    },
    {
      "epoch": 4.307781649245064,
      "grad_norm": 42.255821228027344,
      "learning_rate": 6.324687056394374e-06,
      "loss": 2.1062,
      "step": 11127
    },
    {
      "epoch": 4.308168795973674,
      "grad_norm": 33.276607513427734,
      "learning_rate": 6.324256893362585e-06,
      "loss": 1.665,
      "step": 11128
    },
    {
      "epoch": 4.308555942702284,
      "grad_norm": 53.408546447753906,
      "learning_rate": 6.323826730330796e-06,
      "loss": 1.4111,
      "step": 11129
    },
    {
      "epoch": 4.308943089430894,
      "grad_norm": 85.1344985961914,
      "learning_rate": 6.323396567299007e-06,
      "loss": 1.6014,
      "step": 11130
    },
    {
      "epoch": 4.3093302361595045,
      "grad_norm": 39.239036560058594,
      "learning_rate": 6.322966404267218e-06,
      "loss": 1.3442,
      "step": 11131
    },
    {
      "epoch": 4.309717382888115,
      "grad_norm": 30.66185760498047,
      "learning_rate": 6.322536241235428e-06,
      "loss": 0.4917,
      "step": 11132
    },
    {
      "epoch": 4.310104529616725,
      "grad_norm": 40.40378952026367,
      "learning_rate": 6.322106078203639e-06,
      "loss": 1.9334,
      "step": 11133
    },
    {
      "epoch": 4.310491676345335,
      "grad_norm": 23.160470962524414,
      "learning_rate": 6.321675915171851e-06,
      "loss": 1.3632,
      "step": 11134
    },
    {
      "epoch": 4.310878823073945,
      "grad_norm": 37.13705825805664,
      "learning_rate": 6.321245752140062e-06,
      "loss": 2.0936,
      "step": 11135
    },
    {
      "epoch": 4.311265969802555,
      "grad_norm": 19.05957794189453,
      "learning_rate": 6.320815589108272e-06,
      "loss": 0.718,
      "step": 11136
    },
    {
      "epoch": 4.311653116531165,
      "grad_norm": 63.757999420166016,
      "learning_rate": 6.320385426076483e-06,
      "loss": 2.2975,
      "step": 11137
    },
    {
      "epoch": 4.312040263259775,
      "grad_norm": 62.05204772949219,
      "learning_rate": 6.319955263044695e-06,
      "loss": 2.0991,
      "step": 11138
    },
    {
      "epoch": 4.312427409988386,
      "grad_norm": 55.919898986816406,
      "learning_rate": 6.319525100012906e-06,
      "loss": 1.6188,
      "step": 11139
    },
    {
      "epoch": 4.3128145567169955,
      "grad_norm": 23.41162109375,
      "learning_rate": 6.319094936981116e-06,
      "loss": 1.6679,
      "step": 11140
    },
    {
      "epoch": 4.313201703445606,
      "grad_norm": 22.522111892700195,
      "learning_rate": 6.318664773949327e-06,
      "loss": 0.5659,
      "step": 11141
    },
    {
      "epoch": 4.313588850174216,
      "grad_norm": 37.91476821899414,
      "learning_rate": 6.318234610917538e-06,
      "loss": 1.6542,
      "step": 11142
    },
    {
      "epoch": 4.313975996902826,
      "grad_norm": 19.7916316986084,
      "learning_rate": 6.31780444788575e-06,
      "loss": 0.7215,
      "step": 11143
    },
    {
      "epoch": 4.314363143631437,
      "grad_norm": 54.75337219238281,
      "learning_rate": 6.31737428485396e-06,
      "loss": 1.6422,
      "step": 11144
    },
    {
      "epoch": 4.314750290360046,
      "grad_norm": 29.76640510559082,
      "learning_rate": 6.3169441218221716e-06,
      "loss": 0.2952,
      "step": 11145
    },
    {
      "epoch": 4.315137437088657,
      "grad_norm": 43.73296356201172,
      "learning_rate": 6.316513958790382e-06,
      "loss": 0.8319,
      "step": 11146
    },
    {
      "epoch": 4.3155245838172664,
      "grad_norm": 11.113850593566895,
      "learning_rate": 6.316083795758593e-06,
      "loss": 0.6434,
      "step": 11147
    },
    {
      "epoch": 4.315911730545877,
      "grad_norm": 66.08843231201172,
      "learning_rate": 6.315653632726803e-06,
      "loss": 0.7424,
      "step": 11148
    },
    {
      "epoch": 4.3162988772744875,
      "grad_norm": 31.26044273376465,
      "learning_rate": 6.3152234696950155e-06,
      "loss": 0.3825,
      "step": 11149
    },
    {
      "epoch": 4.316686024003097,
      "grad_norm": 29.10846710205078,
      "learning_rate": 6.314793306663226e-06,
      "loss": 1.7977,
      "step": 11150
    },
    {
      "epoch": 4.317073170731708,
      "grad_norm": 33.56928253173828,
      "learning_rate": 6.314363143631437e-06,
      "loss": 1.0986,
      "step": 11151
    },
    {
      "epoch": 4.317460317460317,
      "grad_norm": 35.668479919433594,
      "learning_rate": 6.313932980599647e-06,
      "loss": 1.3153,
      "step": 11152
    },
    {
      "epoch": 4.317847464188928,
      "grad_norm": 20.43206024169922,
      "learning_rate": 6.3135028175678595e-06,
      "loss": 1.3877,
      "step": 11153
    },
    {
      "epoch": 4.318234610917537,
      "grad_norm": 54.97715377807617,
      "learning_rate": 6.31307265453607e-06,
      "loss": 1.1901,
      "step": 11154
    },
    {
      "epoch": 4.318621757646148,
      "grad_norm": 51.23301315307617,
      "learning_rate": 6.312642491504281e-06,
      "loss": 2.1231,
      "step": 11155
    },
    {
      "epoch": 4.319008904374758,
      "grad_norm": 19.69663429260254,
      "learning_rate": 6.312212328472491e-06,
      "loss": 0.3035,
      "step": 11156
    },
    {
      "epoch": 4.319396051103368,
      "grad_norm": 36.95982360839844,
      "learning_rate": 6.311782165440703e-06,
      "loss": 1.6803,
      "step": 11157
    },
    {
      "epoch": 4.3197831978319785,
      "grad_norm": 42.71078872680664,
      "learning_rate": 6.311352002408914e-06,
      "loss": 2.1519,
      "step": 11158
    },
    {
      "epoch": 4.320170344560588,
      "grad_norm": 25.22430419921875,
      "learning_rate": 6.310921839377125e-06,
      "loss": 3.338,
      "step": 11159
    },
    {
      "epoch": 4.320557491289199,
      "grad_norm": 56.782264709472656,
      "learning_rate": 6.310491676345335e-06,
      "loss": 0.6852,
      "step": 11160
    },
    {
      "epoch": 4.320944638017809,
      "grad_norm": 15.729944229125977,
      "learning_rate": 6.310061513313547e-06,
      "loss": 1.4865,
      "step": 11161
    },
    {
      "epoch": 4.321331784746419,
      "grad_norm": 100.51927185058594,
      "learning_rate": 6.309631350281757e-06,
      "loss": 3.1546,
      "step": 11162
    },
    {
      "epoch": 4.321718931475029,
      "grad_norm": 12.853816032409668,
      "learning_rate": 6.309201187249968e-06,
      "loss": 0.4116,
      "step": 11163
    },
    {
      "epoch": 4.322106078203639,
      "grad_norm": 35.14016342163086,
      "learning_rate": 6.308771024218179e-06,
      "loss": 2.1224,
      "step": 11164
    },
    {
      "epoch": 4.322493224932249,
      "grad_norm": 16.450403213500977,
      "learning_rate": 6.3083408611863906e-06,
      "loss": 0.6453,
      "step": 11165
    },
    {
      "epoch": 4.32288037166086,
      "grad_norm": 15.139491081237793,
      "learning_rate": 6.307910698154601e-06,
      "loss": 0.7905,
      "step": 11166
    },
    {
      "epoch": 4.3232675183894695,
      "grad_norm": 86.4210205078125,
      "learning_rate": 6.307480535122812e-06,
      "loss": 0.8302,
      "step": 11167
    },
    {
      "epoch": 4.32365466511808,
      "grad_norm": 11.59327507019043,
      "learning_rate": 6.307050372091022e-06,
      "loss": 0.758,
      "step": 11168
    },
    {
      "epoch": 4.32404181184669,
      "grad_norm": 11.192963600158691,
      "learning_rate": 6.3066202090592345e-06,
      "loss": 0.3778,
      "step": 11169
    },
    {
      "epoch": 4.3244289585753,
      "grad_norm": 11.968158721923828,
      "learning_rate": 6.306190046027445e-06,
      "loss": 0.5974,
      "step": 11170
    },
    {
      "epoch": 4.32481610530391,
      "grad_norm": 40.49673080444336,
      "learning_rate": 6.305759882995656e-06,
      "loss": 1.5982,
      "step": 11171
    },
    {
      "epoch": 4.32520325203252,
      "grad_norm": 7.968055248260498,
      "learning_rate": 6.305329719963866e-06,
      "loss": 0.3691,
      "step": 11172
    },
    {
      "epoch": 4.325590398761131,
      "grad_norm": 43.18326187133789,
      "learning_rate": 6.3048995569320785e-06,
      "loss": 0.8518,
      "step": 11173
    },
    {
      "epoch": 4.3259775454897405,
      "grad_norm": 16.316381454467773,
      "learning_rate": 6.304469393900289e-06,
      "loss": 1.9727,
      "step": 11174
    },
    {
      "epoch": 4.326364692218351,
      "grad_norm": 65.04717254638672,
      "learning_rate": 6.3040392308685e-06,
      "loss": 2.3632,
      "step": 11175
    },
    {
      "epoch": 4.326751838946961,
      "grad_norm": 16.14714241027832,
      "learning_rate": 6.30360906783671e-06,
      "loss": 0.5811,
      "step": 11176
    },
    {
      "epoch": 4.327138985675571,
      "grad_norm": 28.56730842590332,
      "learning_rate": 6.303178904804922e-06,
      "loss": 1.8924,
      "step": 11177
    },
    {
      "epoch": 4.327526132404181,
      "grad_norm": 11.144458770751953,
      "learning_rate": 6.302748741773132e-06,
      "loss": 0.5373,
      "step": 11178
    },
    {
      "epoch": 4.327913279132791,
      "grad_norm": 12.224526405334473,
      "learning_rate": 6.302318578741344e-06,
      "loss": 0.7698,
      "step": 11179
    },
    {
      "epoch": 4.328300425861402,
      "grad_norm": 90.23575592041016,
      "learning_rate": 6.301888415709554e-06,
      "loss": 1.5429,
      "step": 11180
    },
    {
      "epoch": 4.328687572590011,
      "grad_norm": 4.685013771057129,
      "learning_rate": 6.301458252677766e-06,
      "loss": 0.2557,
      "step": 11181
    },
    {
      "epoch": 4.329074719318622,
      "grad_norm": 5.722337245941162,
      "learning_rate": 6.301028089645976e-06,
      "loss": 0.9334,
      "step": 11182
    },
    {
      "epoch": 4.3294618660472315,
      "grad_norm": 25.444074630737305,
      "learning_rate": 6.300597926614187e-06,
      "loss": 1.492,
      "step": 11183
    },
    {
      "epoch": 4.329849012775842,
      "grad_norm": 57.5927619934082,
      "learning_rate": 6.300167763582399e-06,
      "loss": 1.9951,
      "step": 11184
    },
    {
      "epoch": 4.3302361595044525,
      "grad_norm": 61.464324951171875,
      "learning_rate": 6.2997376005506096e-06,
      "loss": 1.2029,
      "step": 11185
    },
    {
      "epoch": 4.330623306233062,
      "grad_norm": 2.8372840881347656,
      "learning_rate": 6.29930743751882e-06,
      "loss": 0.1387,
      "step": 11186
    },
    {
      "epoch": 4.331010452961673,
      "grad_norm": 26.397972106933594,
      "learning_rate": 6.298877274487031e-06,
      "loss": 0.8641,
      "step": 11187
    },
    {
      "epoch": 4.331397599690282,
      "grad_norm": 5.438799858093262,
      "learning_rate": 6.298447111455243e-06,
      "loss": 0.2116,
      "step": 11188
    },
    {
      "epoch": 4.331784746418893,
      "grad_norm": 11.491976737976074,
      "learning_rate": 6.2980169484234535e-06,
      "loss": 0.5727,
      "step": 11189
    },
    {
      "epoch": 4.332171893147503,
      "grad_norm": 73.90260314941406,
      "learning_rate": 6.297586785391664e-06,
      "loss": 0.9628,
      "step": 11190
    },
    {
      "epoch": 4.332559039876113,
      "grad_norm": 23.986299514770508,
      "learning_rate": 6.297156622359875e-06,
      "loss": 1.6282,
      "step": 11191
    },
    {
      "epoch": 4.332946186604723,
      "grad_norm": 39.780426025390625,
      "learning_rate": 6.296726459328086e-06,
      "loss": 1.6532,
      "step": 11192
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 22.41455078125,
      "learning_rate": 6.296296296296297e-06,
      "loss": 0.6548,
      "step": 11193
    },
    {
      "epoch": 4.3337204800619435,
      "grad_norm": 157.1042022705078,
      "learning_rate": 6.295866133264508e-06,
      "loss": 1.3464,
      "step": 11194
    },
    {
      "epoch": 4.334107626790553,
      "grad_norm": 14.00814437866211,
      "learning_rate": 6.295435970232719e-06,
      "loss": 0.4035,
      "step": 11195
    },
    {
      "epoch": 4.334494773519164,
      "grad_norm": 4.156771183013916,
      "learning_rate": 6.29500580720093e-06,
      "loss": 0.1559,
      "step": 11196
    },
    {
      "epoch": 4.334881920247774,
      "grad_norm": 4.573921203613281,
      "learning_rate": 6.294575644169141e-06,
      "loss": 0.1939,
      "step": 11197
    },
    {
      "epoch": 4.335269066976384,
      "grad_norm": 19.339183807373047,
      "learning_rate": 6.294145481137351e-06,
      "loss": 1.7598,
      "step": 11198
    },
    {
      "epoch": 4.335656213704994,
      "grad_norm": 24.069992065429688,
      "learning_rate": 6.293715318105562e-06,
      "loss": 1.5158,
      "step": 11199
    },
    {
      "epoch": 4.336043360433604,
      "grad_norm": 14.138663291931152,
      "learning_rate": 6.293285155073774e-06,
      "loss": 1.1522,
      "step": 11200
    },
    {
      "epoch": 4.3364305071622145,
      "grad_norm": 11.527789115905762,
      "learning_rate": 6.292854992041985e-06,
      "loss": 0.2924,
      "step": 11201
    },
    {
      "epoch": 4.336817653890825,
      "grad_norm": 59.214481353759766,
      "learning_rate": 6.292424829010195e-06,
      "loss": 2.1475,
      "step": 11202
    },
    {
      "epoch": 4.337204800619435,
      "grad_norm": 24.49384880065918,
      "learning_rate": 6.291994665978406e-06,
      "loss": 1.6634,
      "step": 11203
    },
    {
      "epoch": 4.337591947348045,
      "grad_norm": 93.31602478027344,
      "learning_rate": 6.291564502946618e-06,
      "loss": 0.7934,
      "step": 11204
    },
    {
      "epoch": 4.337979094076655,
      "grad_norm": 29.253400802612305,
      "learning_rate": 6.2911343399148286e-06,
      "loss": 1.5186,
      "step": 11205
    },
    {
      "epoch": 4.338366240805265,
      "grad_norm": 21.828773498535156,
      "learning_rate": 6.290704176883039e-06,
      "loss": 1.4307,
      "step": 11206
    },
    {
      "epoch": 4.338753387533876,
      "grad_norm": 24.341400146484375,
      "learning_rate": 6.29027401385125e-06,
      "loss": 3.2252,
      "step": 11207
    },
    {
      "epoch": 4.339140534262485,
      "grad_norm": 41.88215637207031,
      "learning_rate": 6.289843850819461e-06,
      "loss": 0.8321,
      "step": 11208
    },
    {
      "epoch": 4.339527680991096,
      "grad_norm": 94.65846252441406,
      "learning_rate": 6.2894136877876725e-06,
      "loss": 1.973,
      "step": 11209
    },
    {
      "epoch": 4.3399148277197055,
      "grad_norm": 10.759172439575195,
      "learning_rate": 6.288983524755883e-06,
      "loss": 0.5881,
      "step": 11210
    },
    {
      "epoch": 4.340301974448316,
      "grad_norm": 8.729438781738281,
      "learning_rate": 6.288553361724094e-06,
      "loss": 0.2973,
      "step": 11211
    },
    {
      "epoch": 4.340689121176926,
      "grad_norm": 70.64967346191406,
      "learning_rate": 6.288123198692305e-06,
      "loss": 1.1324,
      "step": 11212
    },
    {
      "epoch": 4.341076267905536,
      "grad_norm": 26.100830078125,
      "learning_rate": 6.287693035660516e-06,
      "loss": 1.2784,
      "step": 11213
    },
    {
      "epoch": 4.341463414634147,
      "grad_norm": 22.712757110595703,
      "learning_rate": 6.287262872628726e-06,
      "loss": 1.3178,
      "step": 11214
    },
    {
      "epoch": 4.341850561362756,
      "grad_norm": 16.519912719726562,
      "learning_rate": 6.286832709596938e-06,
      "loss": 1.3887,
      "step": 11215
    },
    {
      "epoch": 4.342237708091367,
      "grad_norm": 34.4339714050293,
      "learning_rate": 6.286402546565149e-06,
      "loss": 1.573,
      "step": 11216
    },
    {
      "epoch": 4.342624854819976,
      "grad_norm": 36.43645095825195,
      "learning_rate": 6.28597238353336e-06,
      "loss": 1.8429,
      "step": 11217
    },
    {
      "epoch": 4.343012001548587,
      "grad_norm": 32.162471771240234,
      "learning_rate": 6.28554222050157e-06,
      "loss": 1.1738,
      "step": 11218
    },
    {
      "epoch": 4.343399148277197,
      "grad_norm": 46.040523529052734,
      "learning_rate": 6.285112057469781e-06,
      "loss": 0.7526,
      "step": 11219
    },
    {
      "epoch": 4.343786295005807,
      "grad_norm": 31.930683135986328,
      "learning_rate": 6.284681894437993e-06,
      "loss": 1.1099,
      "step": 11220
    },
    {
      "epoch": 4.3441734417344176,
      "grad_norm": 28.080602645874023,
      "learning_rate": 6.284251731406204e-06,
      "loss": 0.6093,
      "step": 11221
    },
    {
      "epoch": 4.344560588463027,
      "grad_norm": 25.26300621032715,
      "learning_rate": 6.283821568374414e-06,
      "loss": 0.8942,
      "step": 11222
    },
    {
      "epoch": 4.344947735191638,
      "grad_norm": 26.020503997802734,
      "learning_rate": 6.283391405342625e-06,
      "loss": 0.9833,
      "step": 11223
    },
    {
      "epoch": 4.345334881920248,
      "grad_norm": 38.90877151489258,
      "learning_rate": 6.282961242310837e-06,
      "loss": 1.5019,
      "step": 11224
    },
    {
      "epoch": 4.345722028648858,
      "grad_norm": 66.53152465820312,
      "learning_rate": 6.2825310792790476e-06,
      "loss": 2.9471,
      "step": 11225
    },
    {
      "epoch": 4.346109175377468,
      "grad_norm": 27.897014617919922,
      "learning_rate": 6.282100916247258e-06,
      "loss": 1.1457,
      "step": 11226
    },
    {
      "epoch": 4.346496322106078,
      "grad_norm": 28.41950225830078,
      "learning_rate": 6.2816707532154695e-06,
      "loss": 1.4529,
      "step": 11227
    },
    {
      "epoch": 4.3468834688346885,
      "grad_norm": 105.83021545410156,
      "learning_rate": 6.28124059018368e-06,
      "loss": 3.4862,
      "step": 11228
    },
    {
      "epoch": 4.347270615563298,
      "grad_norm": 14.420201301574707,
      "learning_rate": 6.280810427151891e-06,
      "loss": 0.5419,
      "step": 11229
    },
    {
      "epoch": 4.347657762291909,
      "grad_norm": 39.96290969848633,
      "learning_rate": 6.280380264120102e-06,
      "loss": 0.7323,
      "step": 11230
    },
    {
      "epoch": 4.348044909020519,
      "grad_norm": 28.8017520904541,
      "learning_rate": 6.2799501010883135e-06,
      "loss": 1.0887,
      "step": 11231
    },
    {
      "epoch": 4.348432055749129,
      "grad_norm": 1.667415738105774,
      "learning_rate": 6.279519938056524e-06,
      "loss": 0.0526,
      "step": 11232
    },
    {
      "epoch": 4.348819202477739,
      "grad_norm": 16.760204315185547,
      "learning_rate": 6.279089775024735e-06,
      "loss": 1.0733,
      "step": 11233
    },
    {
      "epoch": 4.349206349206349,
      "grad_norm": 25.56796646118164,
      "learning_rate": 6.278659611992945e-06,
      "loss": 0.5394,
      "step": 11234
    },
    {
      "epoch": 4.349593495934959,
      "grad_norm": 37.42943572998047,
      "learning_rate": 6.2782294489611575e-06,
      "loss": 0.6202,
      "step": 11235
    },
    {
      "epoch": 4.34998064266357,
      "grad_norm": 65.4003677368164,
      "learning_rate": 6.277799285929368e-06,
      "loss": 1.171,
      "step": 11236
    },
    {
      "epoch": 4.3503677893921795,
      "grad_norm": 28.917892456054688,
      "learning_rate": 6.277369122897579e-06,
      "loss": 1.0724,
      "step": 11237
    },
    {
      "epoch": 4.35075493612079,
      "grad_norm": 32.05062484741211,
      "learning_rate": 6.276938959865789e-06,
      "loss": 0.7981,
      "step": 11238
    },
    {
      "epoch": 4.3511420828494,
      "grad_norm": 25.801374435424805,
      "learning_rate": 6.2765087968340015e-06,
      "loss": 2.2089,
      "step": 11239
    },
    {
      "epoch": 4.35152922957801,
      "grad_norm": 16.690317153930664,
      "learning_rate": 6.276078633802212e-06,
      "loss": 0.7588,
      "step": 11240
    },
    {
      "epoch": 4.351916376306621,
      "grad_norm": 1.399043083190918,
      "learning_rate": 6.275648470770423e-06,
      "loss": 0.0434,
      "step": 11241
    },
    {
      "epoch": 4.35230352303523,
      "grad_norm": 47.96805191040039,
      "learning_rate": 6.275218307738633e-06,
      "loss": 1.8585,
      "step": 11242
    },
    {
      "epoch": 4.352690669763841,
      "grad_norm": 155.64010620117188,
      "learning_rate": 6.2747881447068446e-06,
      "loss": 1.5761,
      "step": 11243
    },
    {
      "epoch": 4.35307781649245,
      "grad_norm": 28.94316864013672,
      "learning_rate": 6.274357981675055e-06,
      "loss": 2.9734,
      "step": 11244
    },
    {
      "epoch": 4.353464963221061,
      "grad_norm": 5.716665267944336,
      "learning_rate": 6.2739278186432666e-06,
      "loss": 0.1623,
      "step": 11245
    },
    {
      "epoch": 4.3538521099496705,
      "grad_norm": 9.36498737335205,
      "learning_rate": 6.273497655611477e-06,
      "loss": 0.6625,
      "step": 11246
    },
    {
      "epoch": 4.354239256678281,
      "grad_norm": 82.4190902709961,
      "learning_rate": 6.2730674925796885e-06,
      "loss": 2.7669,
      "step": 11247
    },
    {
      "epoch": 4.3546264034068916,
      "grad_norm": 18.41118812561035,
      "learning_rate": 6.272637329547899e-06,
      "loss": 0.434,
      "step": 11248
    },
    {
      "epoch": 4.355013550135501,
      "grad_norm": 117.10867309570312,
      "learning_rate": 6.27220716651611e-06,
      "loss": 0.649,
      "step": 11249
    },
    {
      "epoch": 4.355400696864112,
      "grad_norm": 58.77344512939453,
      "learning_rate": 6.27177700348432e-06,
      "loss": 1.609,
      "step": 11250
    },
    {
      "epoch": 4.355787843592721,
      "grad_norm": 6.98621940612793,
      "learning_rate": 6.2713468404525325e-06,
      "loss": 0.4241,
      "step": 11251
    },
    {
      "epoch": 4.356174990321332,
      "grad_norm": 6.132652759552002,
      "learning_rate": 6.270916677420743e-06,
      "loss": 0.1908,
      "step": 11252
    },
    {
      "epoch": 4.356562137049942,
      "grad_norm": 11.048398971557617,
      "learning_rate": 6.270486514388954e-06,
      "loss": 0.8674,
      "step": 11253
    },
    {
      "epoch": 4.356949283778552,
      "grad_norm": 30.747148513793945,
      "learning_rate": 6.270056351357164e-06,
      "loss": 1.4607,
      "step": 11254
    },
    {
      "epoch": 4.3573364305071625,
      "grad_norm": 76.94965362548828,
      "learning_rate": 6.2696261883253765e-06,
      "loss": 1.045,
      "step": 11255
    },
    {
      "epoch": 4.357723577235772,
      "grad_norm": 14.254973411560059,
      "learning_rate": 6.269196025293587e-06,
      "loss": 0.5542,
      "step": 11256
    },
    {
      "epoch": 4.358110723964383,
      "grad_norm": 17.703960418701172,
      "learning_rate": 6.268765862261798e-06,
      "loss": 3.4801,
      "step": 11257
    },
    {
      "epoch": 4.358497870692993,
      "grad_norm": 47.32518005371094,
      "learning_rate": 6.268335699230008e-06,
      "loss": 2.1187,
      "step": 11258
    },
    {
      "epoch": 4.358885017421603,
      "grad_norm": 12.042879104614258,
      "learning_rate": 6.26790553619822e-06,
      "loss": 0.6209,
      "step": 11259
    },
    {
      "epoch": 4.359272164150213,
      "grad_norm": 29.74054527282715,
      "learning_rate": 6.267475373166431e-06,
      "loss": 1.7996,
      "step": 11260
    },
    {
      "epoch": 4.359659310878823,
      "grad_norm": 41.7530403137207,
      "learning_rate": 6.267045210134642e-06,
      "loss": 1.1225,
      "step": 11261
    },
    {
      "epoch": 4.360046457607433,
      "grad_norm": 88.59959411621094,
      "learning_rate": 6.266615047102852e-06,
      "loss": 3.1911,
      "step": 11262
    },
    {
      "epoch": 4.360433604336043,
      "grad_norm": 19.289409637451172,
      "learning_rate": 6.2661848840710636e-06,
      "loss": 2.0652,
      "step": 11263
    },
    {
      "epoch": 4.3608207510646535,
      "grad_norm": 34.286197662353516,
      "learning_rate": 6.265754721039274e-06,
      "loss": 1.3912,
      "step": 11264
    },
    {
      "epoch": 4.361207897793264,
      "grad_norm": 34.055450439453125,
      "learning_rate": 6.265324558007485e-06,
      "loss": 1.1559,
      "step": 11265
    },
    {
      "epoch": 4.361595044521874,
      "grad_norm": 65.3673095703125,
      "learning_rate": 6.264894394975697e-06,
      "loss": 1.3444,
      "step": 11266
    },
    {
      "epoch": 4.361982191250484,
      "grad_norm": 13.434419631958008,
      "learning_rate": 6.2644642319439075e-06,
      "loss": 0.209,
      "step": 11267
    },
    {
      "epoch": 4.362369337979094,
      "grad_norm": 29.174196243286133,
      "learning_rate": 6.264034068912118e-06,
      "loss": 0.748,
      "step": 11268
    },
    {
      "epoch": 4.362756484707704,
      "grad_norm": 39.22677230834961,
      "learning_rate": 6.263603905880329e-06,
      "loss": 1.5365,
      "step": 11269
    },
    {
      "epoch": 4.363143631436314,
      "grad_norm": 20.87662124633789,
      "learning_rate": 6.263173742848541e-06,
      "loss": 0.3775,
      "step": 11270
    },
    {
      "epoch": 4.363530778164924,
      "grad_norm": 20.210887908935547,
      "learning_rate": 6.2627435798167515e-06,
      "loss": 0.7083,
      "step": 11271
    },
    {
      "epoch": 4.363917924893535,
      "grad_norm": 48.79254150390625,
      "learning_rate": 6.262313416784962e-06,
      "loss": 2.7382,
      "step": 11272
    },
    {
      "epoch": 4.3643050716221445,
      "grad_norm": 57.669647216796875,
      "learning_rate": 6.261883253753173e-06,
      "loss": 3.4451,
      "step": 11273
    },
    {
      "epoch": 4.364692218350755,
      "grad_norm": 31.342819213867188,
      "learning_rate": 6.261453090721384e-06,
      "loss": 1.6202,
      "step": 11274
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 38.73265838623047,
      "learning_rate": 6.2610229276895955e-06,
      "loss": 1.6429,
      "step": 11275
    },
    {
      "epoch": 4.365466511807975,
      "grad_norm": 23.185335159301758,
      "learning_rate": 6.260592764657806e-06,
      "loss": 1.1976,
      "step": 11276
    },
    {
      "epoch": 4.365853658536586,
      "grad_norm": 41.60212326049805,
      "learning_rate": 6.260162601626017e-06,
      "loss": 1.0183,
      "step": 11277
    },
    {
      "epoch": 4.366240805265195,
      "grad_norm": 57.85181427001953,
      "learning_rate": 6.259732438594228e-06,
      "loss": 4.5555,
      "step": 11278
    },
    {
      "epoch": 4.366627951993806,
      "grad_norm": 50.100929260253906,
      "learning_rate": 6.259302275562439e-06,
      "loss": 1.2016,
      "step": 11279
    },
    {
      "epoch": 4.3670150987224154,
      "grad_norm": 31.254716873168945,
      "learning_rate": 6.258872112530649e-06,
      "loss": 1.1739,
      "step": 11280
    },
    {
      "epoch": 4.367402245451026,
      "grad_norm": 47.62406539916992,
      "learning_rate": 6.258441949498861e-06,
      "loss": 2.1893,
      "step": 11281
    },
    {
      "epoch": 4.3677893921796365,
      "grad_norm": 16.115602493286133,
      "learning_rate": 6.258011786467072e-06,
      "loss": 0.9319,
      "step": 11282
    },
    {
      "epoch": 4.368176538908246,
      "grad_norm": 20.687854766845703,
      "learning_rate": 6.2575816234352826e-06,
      "loss": 1.0664,
      "step": 11283
    },
    {
      "epoch": 4.368563685636857,
      "grad_norm": 24.762235641479492,
      "learning_rate": 6.257151460403493e-06,
      "loss": 1.679,
      "step": 11284
    },
    {
      "epoch": 4.368950832365466,
      "grad_norm": 27.113422393798828,
      "learning_rate": 6.256721297371704e-06,
      "loss": 0.8588,
      "step": 11285
    },
    {
      "epoch": 4.369337979094077,
      "grad_norm": 2.8869736194610596,
      "learning_rate": 6.256291134339916e-06,
      "loss": 0.1468,
      "step": 11286
    },
    {
      "epoch": 4.369725125822686,
      "grad_norm": 31.114736557006836,
      "learning_rate": 6.2558609713081265e-06,
      "loss": 2.0034,
      "step": 11287
    },
    {
      "epoch": 4.370112272551297,
      "grad_norm": 30.05001449584961,
      "learning_rate": 6.255430808276337e-06,
      "loss": 2.007,
      "step": 11288
    },
    {
      "epoch": 4.370499419279907,
      "grad_norm": 26.68056869506836,
      "learning_rate": 6.255000645244548e-06,
      "loss": 1.1373,
      "step": 11289
    },
    {
      "epoch": 4.370886566008517,
      "grad_norm": 29.23110580444336,
      "learning_rate": 6.25457048221276e-06,
      "loss": 1.0518,
      "step": 11290
    },
    {
      "epoch": 4.3712737127371275,
      "grad_norm": 25.896516799926758,
      "learning_rate": 6.2541403191809705e-06,
      "loss": 2.175,
      "step": 11291
    },
    {
      "epoch": 4.371660859465737,
      "grad_norm": 36.341827392578125,
      "learning_rate": 6.253710156149181e-06,
      "loss": 1.2832,
      "step": 11292
    },
    {
      "epoch": 4.372048006194348,
      "grad_norm": 28.485265731811523,
      "learning_rate": 6.253279993117392e-06,
      "loss": 3.1439,
      "step": 11293
    },
    {
      "epoch": 4.372435152922958,
      "grad_norm": 28.327320098876953,
      "learning_rate": 6.252849830085603e-06,
      "loss": 0.6477,
      "step": 11294
    },
    {
      "epoch": 4.372822299651568,
      "grad_norm": 1.495357632637024,
      "learning_rate": 6.252419667053814e-06,
      "loss": 0.044,
      "step": 11295
    },
    {
      "epoch": 4.373209446380178,
      "grad_norm": 127.67611694335938,
      "learning_rate": 6.251989504022025e-06,
      "loss": 1.4914,
      "step": 11296
    },
    {
      "epoch": 4.373596593108788,
      "grad_norm": 17.002330780029297,
      "learning_rate": 6.251559340990236e-06,
      "loss": 0.7367,
      "step": 11297
    },
    {
      "epoch": 4.373983739837398,
      "grad_norm": 13.826485633850098,
      "learning_rate": 6.251129177958447e-06,
      "loss": 1.2948,
      "step": 11298
    },
    {
      "epoch": 4.374370886566009,
      "grad_norm": 51.0649299621582,
      "learning_rate": 6.250699014926658e-06,
      "loss": 1.9925,
      "step": 11299
    },
    {
      "epoch": 4.3747580332946185,
      "grad_norm": 38.9735107421875,
      "learning_rate": 6.250268851894868e-06,
      "loss": 0.6183,
      "step": 11300
    },
    {
      "epoch": 4.375145180023229,
      "grad_norm": 38.80363082885742,
      "learning_rate": 6.249838688863079e-06,
      "loss": 1.7347,
      "step": 11301
    },
    {
      "epoch": 4.375532326751839,
      "grad_norm": 30.496931076049805,
      "learning_rate": 6.249408525831291e-06,
      "loss": 2.3114,
      "step": 11302
    },
    {
      "epoch": 4.375919473480449,
      "grad_norm": 30.244571685791016,
      "learning_rate": 6.2489783627995016e-06,
      "loss": 1.8681,
      "step": 11303
    },
    {
      "epoch": 4.376306620209059,
      "grad_norm": 148.73361206054688,
      "learning_rate": 6.248548199767712e-06,
      "loss": 1.6901,
      "step": 11304
    },
    {
      "epoch": 4.376693766937669,
      "grad_norm": 6.9348626136779785,
      "learning_rate": 6.248118036735923e-06,
      "loss": 0.991,
      "step": 11305
    },
    {
      "epoch": 4.37708091366628,
      "grad_norm": 8.830842971801758,
      "learning_rate": 6.247687873704135e-06,
      "loss": 1.062,
      "step": 11306
    },
    {
      "epoch": 4.3774680603948894,
      "grad_norm": 73.53071594238281,
      "learning_rate": 6.2472577106723455e-06,
      "loss": 2.2041,
      "step": 11307
    },
    {
      "epoch": 4.3778552071235,
      "grad_norm": 56.587398529052734,
      "learning_rate": 6.246827547640556e-06,
      "loss": 1.1945,
      "step": 11308
    },
    {
      "epoch": 4.37824235385211,
      "grad_norm": 18.804439544677734,
      "learning_rate": 6.2463973846087675e-06,
      "loss": 1.4668,
      "step": 11309
    },
    {
      "epoch": 4.37862950058072,
      "grad_norm": 30.595897674560547,
      "learning_rate": 6.245967221576978e-06,
      "loss": 1.4223,
      "step": 11310
    },
    {
      "epoch": 4.379016647309331,
      "grad_norm": 25.142866134643555,
      "learning_rate": 6.2455370585451895e-06,
      "loss": 1.8123,
      "step": 11311
    },
    {
      "epoch": 4.37940379403794,
      "grad_norm": 59.73628616333008,
      "learning_rate": 6.2451068955134e-06,
      "loss": 1.0232,
      "step": 11312
    },
    {
      "epoch": 4.379790940766551,
      "grad_norm": 35.52509307861328,
      "learning_rate": 6.2446767324816115e-06,
      "loss": 1.1564,
      "step": 11313
    },
    {
      "epoch": 4.38017808749516,
      "grad_norm": 12.042811393737793,
      "learning_rate": 6.244246569449822e-06,
      "loss": 0.7195,
      "step": 11314
    },
    {
      "epoch": 4.380565234223771,
      "grad_norm": 23.47654914855957,
      "learning_rate": 6.243816406418033e-06,
      "loss": 1.3754,
      "step": 11315
    },
    {
      "epoch": 4.380952380952381,
      "grad_norm": 27.266124725341797,
      "learning_rate": 6.243386243386243e-06,
      "loss": 1.1607,
      "step": 11316
    },
    {
      "epoch": 4.381339527680991,
      "grad_norm": 21.008716583251953,
      "learning_rate": 6.2429560803544555e-06,
      "loss": 1.4934,
      "step": 11317
    },
    {
      "epoch": 4.3817266744096015,
      "grad_norm": 14.949326515197754,
      "learning_rate": 6.242525917322666e-06,
      "loss": 0.3193,
      "step": 11318
    },
    {
      "epoch": 4.382113821138211,
      "grad_norm": 13.912396430969238,
      "learning_rate": 6.242095754290877e-06,
      "loss": 0.6231,
      "step": 11319
    },
    {
      "epoch": 4.382500967866822,
      "grad_norm": 1.2929352521896362,
      "learning_rate": 6.241665591259087e-06,
      "loss": 0.0366,
      "step": 11320
    },
    {
      "epoch": 4.382888114595431,
      "grad_norm": 32.57523727416992,
      "learning_rate": 6.2412354282272994e-06,
      "loss": 0.4976,
      "step": 11321
    },
    {
      "epoch": 4.383275261324042,
      "grad_norm": 13.940999984741211,
      "learning_rate": 6.24080526519551e-06,
      "loss": 0.5091,
      "step": 11322
    },
    {
      "epoch": 4.383662408052652,
      "grad_norm": 65.13627624511719,
      "learning_rate": 6.2403751021637206e-06,
      "loss": 2.0273,
      "step": 11323
    },
    {
      "epoch": 4.384049554781262,
      "grad_norm": 37.036659240722656,
      "learning_rate": 6.239944939131931e-06,
      "loss": 0.4421,
      "step": 11324
    },
    {
      "epoch": 4.384436701509872,
      "grad_norm": 6.622003078460693,
      "learning_rate": 6.2395147761001425e-06,
      "loss": 0.284,
      "step": 11325
    },
    {
      "epoch": 4.384823848238482,
      "grad_norm": 21.34078598022461,
      "learning_rate": 6.239084613068354e-06,
      "loss": 1.5956,
      "step": 11326
    },
    {
      "epoch": 4.3852109949670925,
      "grad_norm": 42.09283447265625,
      "learning_rate": 6.2386544500365645e-06,
      "loss": 0.9743,
      "step": 11327
    },
    {
      "epoch": 4.385598141695703,
      "grad_norm": 18.353395462036133,
      "learning_rate": 6.238224287004775e-06,
      "loss": 1.3316,
      "step": 11328
    },
    {
      "epoch": 4.385985288424313,
      "grad_norm": 34.473201751708984,
      "learning_rate": 6.2377941239729865e-06,
      "loss": 1.6457,
      "step": 11329
    },
    {
      "epoch": 4.386372435152923,
      "grad_norm": 9.63245964050293,
      "learning_rate": 6.237363960941197e-06,
      "loss": 0.5886,
      "step": 11330
    },
    {
      "epoch": 4.386759581881533,
      "grad_norm": 26.276243209838867,
      "learning_rate": 6.236933797909408e-06,
      "loss": 3.7899,
      "step": 11331
    },
    {
      "epoch": 4.387146728610143,
      "grad_norm": 25.158254623413086,
      "learning_rate": 6.236503634877619e-06,
      "loss": 0.8996,
      "step": 11332
    },
    {
      "epoch": 4.387533875338754,
      "grad_norm": 43.704532623291016,
      "learning_rate": 6.2360734718458305e-06,
      "loss": 1.6744,
      "step": 11333
    },
    {
      "epoch": 4.3879210220673635,
      "grad_norm": 26.557945251464844,
      "learning_rate": 6.235643308814041e-06,
      "loss": 2.1095,
      "step": 11334
    },
    {
      "epoch": 4.388308168795974,
      "grad_norm": 10.538471221923828,
      "learning_rate": 6.235213145782252e-06,
      "loss": 0.1913,
      "step": 11335
    },
    {
      "epoch": 4.388695315524584,
      "grad_norm": 18.858638763427734,
      "learning_rate": 6.234782982750462e-06,
      "loss": 1.4594,
      "step": 11336
    },
    {
      "epoch": 4.389082462253194,
      "grad_norm": 33.275291442871094,
      "learning_rate": 6.2343528197186745e-06,
      "loss": 1.4532,
      "step": 11337
    },
    {
      "epoch": 4.389469608981804,
      "grad_norm": 32.629146575927734,
      "learning_rate": 6.233922656686885e-06,
      "loss": 0.6679,
      "step": 11338
    },
    {
      "epoch": 4.389856755710414,
      "grad_norm": 58.194190979003906,
      "learning_rate": 6.233492493655096e-06,
      "loss": 0.969,
      "step": 11339
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 23.025819778442383,
      "learning_rate": 6.233062330623306e-06,
      "loss": 1.4748,
      "step": 11340
    },
    {
      "epoch": 4.390631049167634,
      "grad_norm": 27.499496459960938,
      "learning_rate": 6.232632167591518e-06,
      "loss": 2.0439,
      "step": 11341
    },
    {
      "epoch": 4.391018195896245,
      "grad_norm": 7.2498674392700195,
      "learning_rate": 6.232202004559729e-06,
      "loss": 0.4539,
      "step": 11342
    },
    {
      "epoch": 4.3914053426248545,
      "grad_norm": 33.61885070800781,
      "learning_rate": 6.2317718415279396e-06,
      "loss": 1.0009,
      "step": 11343
    },
    {
      "epoch": 4.391792489353465,
      "grad_norm": 55.81181716918945,
      "learning_rate": 6.23134167849615e-06,
      "loss": 2.2379,
      "step": 11344
    },
    {
      "epoch": 4.3921796360820755,
      "grad_norm": 69.37376403808594,
      "learning_rate": 6.2309115154643615e-06,
      "loss": 1.1273,
      "step": 11345
    },
    {
      "epoch": 4.392566782810685,
      "grad_norm": 33.50563049316406,
      "learning_rate": 6.230481352432572e-06,
      "loss": 1.2704,
      "step": 11346
    },
    {
      "epoch": 4.392953929539296,
      "grad_norm": 25.35920524597168,
      "learning_rate": 6.2300511894007835e-06,
      "loss": 1.3605,
      "step": 11347
    },
    {
      "epoch": 4.393341076267905,
      "grad_norm": 27.184728622436523,
      "learning_rate": 6.229621026368995e-06,
      "loss": 2.2799,
      "step": 11348
    },
    {
      "epoch": 4.393728222996516,
      "grad_norm": 12.141678810119629,
      "learning_rate": 6.2291908633372055e-06,
      "loss": 0.6855,
      "step": 11349
    },
    {
      "epoch": 4.394115369725126,
      "grad_norm": 61.09941482543945,
      "learning_rate": 6.228760700305416e-06,
      "loss": 1.2334,
      "step": 11350
    },
    {
      "epoch": 4.394502516453736,
      "grad_norm": 72.85433959960938,
      "learning_rate": 6.228330537273627e-06,
      "loss": 2.554,
      "step": 11351
    },
    {
      "epoch": 4.394889663182346,
      "grad_norm": 5.9434027671813965,
      "learning_rate": 6.227900374241839e-06,
      "loss": 0.3077,
      "step": 11352
    },
    {
      "epoch": 4.395276809910956,
      "grad_norm": 13.535093307495117,
      "learning_rate": 6.2274702112100495e-06,
      "loss": 0.434,
      "step": 11353
    },
    {
      "epoch": 4.3956639566395665,
      "grad_norm": 8.247026443481445,
      "learning_rate": 6.22704004817826e-06,
      "loss": 0.3131,
      "step": 11354
    },
    {
      "epoch": 4.396051103368176,
      "grad_norm": 43.721435546875,
      "learning_rate": 6.226609885146471e-06,
      "loss": 1.7384,
      "step": 11355
    },
    {
      "epoch": 4.396438250096787,
      "grad_norm": 2.6347010135650635,
      "learning_rate": 6.226179722114683e-06,
      "loss": 0.0864,
      "step": 11356
    },
    {
      "epoch": 4.396825396825397,
      "grad_norm": 22.772119522094727,
      "learning_rate": 6.2257495590828935e-06,
      "loss": 0.7633,
      "step": 11357
    },
    {
      "epoch": 4.397212543554007,
      "grad_norm": 7.489337921142578,
      "learning_rate": 6.225319396051104e-06,
      "loss": 0.4526,
      "step": 11358
    },
    {
      "epoch": 4.397599690282617,
      "grad_norm": 14.146852493286133,
      "learning_rate": 6.224889233019315e-06,
      "loss": 1.2931,
      "step": 11359
    },
    {
      "epoch": 4.397986837011227,
      "grad_norm": 34.726409912109375,
      "learning_rate": 6.224459069987526e-06,
      "loss": 0.8624,
      "step": 11360
    },
    {
      "epoch": 4.3983739837398375,
      "grad_norm": 41.13619613647461,
      "learning_rate": 6.2240289069557366e-06,
      "loss": 1.6174,
      "step": 11361
    },
    {
      "epoch": 4.398761130468447,
      "grad_norm": 55.74501037597656,
      "learning_rate": 6.223598743923948e-06,
      "loss": 1.0568,
      "step": 11362
    },
    {
      "epoch": 4.399148277197058,
      "grad_norm": 26.917823791503906,
      "learning_rate": 6.2231685808921586e-06,
      "loss": 0.8701,
      "step": 11363
    },
    {
      "epoch": 4.399535423925668,
      "grad_norm": 37.003440856933594,
      "learning_rate": 6.22273841786037e-06,
      "loss": 1.4912,
      "step": 11364
    },
    {
      "epoch": 4.399922570654278,
      "grad_norm": 38.44749450683594,
      "learning_rate": 6.2223082548285805e-06,
      "loss": 1.3535,
      "step": 11365
    },
    {
      "epoch": 4.400309717382888,
      "grad_norm": 36.43691635131836,
      "learning_rate": 6.221878091796791e-06,
      "loss": 0.6766,
      "step": 11366
    },
    {
      "epoch": 4.400696864111498,
      "grad_norm": 51.315921783447266,
      "learning_rate": 6.221447928765002e-06,
      "loss": 1.6534,
      "step": 11367
    },
    {
      "epoch": 4.401084010840108,
      "grad_norm": 10.726371765136719,
      "learning_rate": 6.221017765733214e-06,
      "loss": 0.6406,
      "step": 11368
    },
    {
      "epoch": 4.401471157568719,
      "grad_norm": 88.72146606445312,
      "learning_rate": 6.2205876027014245e-06,
      "loss": 1.233,
      "step": 11369
    },
    {
      "epoch": 4.4018583042973285,
      "grad_norm": 137.53883361816406,
      "learning_rate": 6.220157439669635e-06,
      "loss": 3.182,
      "step": 11370
    },
    {
      "epoch": 4.402245451025939,
      "grad_norm": 22.303184509277344,
      "learning_rate": 6.219727276637846e-06,
      "loss": 0.8738,
      "step": 11371
    },
    {
      "epoch": 4.402632597754549,
      "grad_norm": 32.97408676147461,
      "learning_rate": 6.219297113606058e-06,
      "loss": 1.1244,
      "step": 11372
    },
    {
      "epoch": 4.403019744483159,
      "grad_norm": 12.974532127380371,
      "learning_rate": 6.2188669505742685e-06,
      "loss": 0.2041,
      "step": 11373
    },
    {
      "epoch": 4.40340689121177,
      "grad_norm": 18.080739974975586,
      "learning_rate": 6.218436787542479e-06,
      "loss": 0.7709,
      "step": 11374
    },
    {
      "epoch": 4.403794037940379,
      "grad_norm": 54.57441711425781,
      "learning_rate": 6.21800662451069e-06,
      "loss": 1.9745,
      "step": 11375
    },
    {
      "epoch": 4.40418118466899,
      "grad_norm": 44.567474365234375,
      "learning_rate": 6.217576461478901e-06,
      "loss": 1.3782,
      "step": 11376
    },
    {
      "epoch": 4.404568331397599,
      "grad_norm": 23.791425704956055,
      "learning_rate": 6.2171462984471124e-06,
      "loss": 1.2764,
      "step": 11377
    },
    {
      "epoch": 4.40495547812621,
      "grad_norm": 59.097599029541016,
      "learning_rate": 6.216716135415323e-06,
      "loss": 2.2188,
      "step": 11378
    },
    {
      "epoch": 4.4053426248548195,
      "grad_norm": 67.74563598632812,
      "learning_rate": 6.216285972383534e-06,
      "loss": 1.3692,
      "step": 11379
    },
    {
      "epoch": 4.40572977158343,
      "grad_norm": 19.44661521911621,
      "learning_rate": 6.215855809351745e-06,
      "loss": 0.8509,
      "step": 11380
    },
    {
      "epoch": 4.4061169183120406,
      "grad_norm": 140.20712280273438,
      "learning_rate": 6.2154256463199556e-06,
      "loss": 1.8791,
      "step": 11381
    },
    {
      "epoch": 4.40650406504065,
      "grad_norm": 25.582250595092773,
      "learning_rate": 6.214995483288166e-06,
      "loss": 1.3907,
      "step": 11382
    },
    {
      "epoch": 4.406891211769261,
      "grad_norm": 1.3553416728973389,
      "learning_rate": 6.2145653202563776e-06,
      "loss": 0.0387,
      "step": 11383
    },
    {
      "epoch": 4.40727835849787,
      "grad_norm": 49.38805389404297,
      "learning_rate": 6.214135157224589e-06,
      "loss": 2.3641,
      "step": 11384
    },
    {
      "epoch": 4.407665505226481,
      "grad_norm": 63.51194763183594,
      "learning_rate": 6.2137049941927995e-06,
      "loss": 1.5858,
      "step": 11385
    },
    {
      "epoch": 4.408052651955091,
      "grad_norm": 21.90382194519043,
      "learning_rate": 6.21327483116101e-06,
      "loss": 3.1213,
      "step": 11386
    },
    {
      "epoch": 4.408439798683701,
      "grad_norm": 21.649837493896484,
      "learning_rate": 6.212844668129221e-06,
      "loss": 0.9911,
      "step": 11387
    },
    {
      "epoch": 4.4088269454123115,
      "grad_norm": 18.245872497558594,
      "learning_rate": 6.212414505097433e-06,
      "loss": 0.7225,
      "step": 11388
    },
    {
      "epoch": 4.409214092140921,
      "grad_norm": 28.376602172851562,
      "learning_rate": 6.2119843420656435e-06,
      "loss": 1.8085,
      "step": 11389
    },
    {
      "epoch": 4.409601238869532,
      "grad_norm": 20.589399337768555,
      "learning_rate": 6.211554179033854e-06,
      "loss": 0.9851,
      "step": 11390
    },
    {
      "epoch": 4.409988385598142,
      "grad_norm": 13.284384727478027,
      "learning_rate": 6.2111240160020655e-06,
      "loss": 0.6503,
      "step": 11391
    },
    {
      "epoch": 4.410375532326752,
      "grad_norm": 133.4479217529297,
      "learning_rate": 6.210693852970277e-06,
      "loss": 1.4945,
      "step": 11392
    },
    {
      "epoch": 4.410762679055362,
      "grad_norm": 29.453506469726562,
      "learning_rate": 6.2102636899384875e-06,
      "loss": 1.6282,
      "step": 11393
    },
    {
      "epoch": 4.411149825783972,
      "grad_norm": 21.449132919311523,
      "learning_rate": 6.209833526906698e-06,
      "loss": 0.5621,
      "step": 11394
    },
    {
      "epoch": 4.411536972512582,
      "grad_norm": 35.794464111328125,
      "learning_rate": 6.2094033638749095e-06,
      "loss": 1.3705,
      "step": 11395
    },
    {
      "epoch": 4.411924119241192,
      "grad_norm": 29.595848083496094,
      "learning_rate": 6.20897320084312e-06,
      "loss": 2.4627,
      "step": 11396
    },
    {
      "epoch": 4.4123112659698025,
      "grad_norm": 31.472679138183594,
      "learning_rate": 6.208543037811331e-06,
      "loss": 1.2786,
      "step": 11397
    },
    {
      "epoch": 4.412698412698413,
      "grad_norm": 18.226459503173828,
      "learning_rate": 6.208112874779542e-06,
      "loss": 0.7192,
      "step": 11398
    },
    {
      "epoch": 4.413085559427023,
      "grad_norm": 1.2973178625106812,
      "learning_rate": 6.2076827117477534e-06,
      "loss": 0.0386,
      "step": 11399
    },
    {
      "epoch": 4.413472706155633,
      "grad_norm": 12.759326934814453,
      "learning_rate": 6.207252548715964e-06,
      "loss": 1.7951,
      "step": 11400
    },
    {
      "epoch": 4.413859852884243,
      "grad_norm": 34.26478958129883,
      "learning_rate": 6.2068223856841746e-06,
      "loss": 1.5752,
      "step": 11401
    },
    {
      "epoch": 4.414246999612853,
      "grad_norm": 6.8819804191589355,
      "learning_rate": 6.206392222652385e-06,
      "loss": 0.2582,
      "step": 11402
    },
    {
      "epoch": 4.414634146341464,
      "grad_norm": 48.98815155029297,
      "learning_rate": 6.205962059620597e-06,
      "loss": 1.6154,
      "step": 11403
    },
    {
      "epoch": 4.415021293070073,
      "grad_norm": 27.399072647094727,
      "learning_rate": 6.205531896588808e-06,
      "loss": 1.6431,
      "step": 11404
    },
    {
      "epoch": 4.415408439798684,
      "grad_norm": 20.36993980407715,
      "learning_rate": 6.2051017335570185e-06,
      "loss": 1.5084,
      "step": 11405
    },
    {
      "epoch": 4.4157955865272935,
      "grad_norm": 27.27158546447754,
      "learning_rate": 6.204671570525229e-06,
      "loss": 2.3967,
      "step": 11406
    },
    {
      "epoch": 4.416182733255904,
      "grad_norm": 10.153377532958984,
      "learning_rate": 6.204241407493441e-06,
      "loss": 0.613,
      "step": 11407
    },
    {
      "epoch": 4.416569879984515,
      "grad_norm": 98.59461975097656,
      "learning_rate": 6.203811244461652e-06,
      "loss": 1.7439,
      "step": 11408
    },
    {
      "epoch": 4.416957026713124,
      "grad_norm": 29.641002655029297,
      "learning_rate": 6.2033810814298625e-06,
      "loss": 0.3158,
      "step": 11409
    },
    {
      "epoch": 4.417344173441735,
      "grad_norm": 148.29177856445312,
      "learning_rate": 6.202950918398073e-06,
      "loss": 3.2745,
      "step": 11410
    },
    {
      "epoch": 4.417731320170344,
      "grad_norm": 33.93695068359375,
      "learning_rate": 6.2025207553662845e-06,
      "loss": 1.6082,
      "step": 11411
    },
    {
      "epoch": 4.418118466898955,
      "grad_norm": 27.188739776611328,
      "learning_rate": 6.202090592334495e-06,
      "loss": 0.583,
      "step": 11412
    },
    {
      "epoch": 4.418505613627564,
      "grad_norm": 75.62993621826172,
      "learning_rate": 6.2016604293027065e-06,
      "loss": 1.8817,
      "step": 11413
    },
    {
      "epoch": 4.418892760356175,
      "grad_norm": 47.57314682006836,
      "learning_rate": 6.201230266270917e-06,
      "loss": 1.6518,
      "step": 11414
    },
    {
      "epoch": 4.4192799070847855,
      "grad_norm": 46.813453674316406,
      "learning_rate": 6.2008001032391285e-06,
      "loss": 1.0626,
      "step": 11415
    },
    {
      "epoch": 4.419667053813395,
      "grad_norm": 24.890684127807617,
      "learning_rate": 6.200369940207339e-06,
      "loss": 0.6768,
      "step": 11416
    },
    {
      "epoch": 4.420054200542006,
      "grad_norm": 28.566904067993164,
      "learning_rate": 6.19993977717555e-06,
      "loss": 1.4874,
      "step": 11417
    },
    {
      "epoch": 4.420441347270615,
      "grad_norm": 38.8682861328125,
      "learning_rate": 6.19950961414376e-06,
      "loss": 1.57,
      "step": 11418
    },
    {
      "epoch": 4.420828493999226,
      "grad_norm": 62.14494323730469,
      "learning_rate": 6.1990794511119724e-06,
      "loss": 0.3117,
      "step": 11419
    },
    {
      "epoch": 4.421215640727836,
      "grad_norm": 88.00501251220703,
      "learning_rate": 6.198649288080183e-06,
      "loss": 2.4681,
      "step": 11420
    },
    {
      "epoch": 4.421602787456446,
      "grad_norm": 62.40571975708008,
      "learning_rate": 6.1982191250483936e-06,
      "loss": 0.4264,
      "step": 11421
    },
    {
      "epoch": 4.421989934185056,
      "grad_norm": 41.64763641357422,
      "learning_rate": 6.197788962016604e-06,
      "loss": 0.4973,
      "step": 11422
    },
    {
      "epoch": 4.422377080913666,
      "grad_norm": 102.01387023925781,
      "learning_rate": 6.197358798984816e-06,
      "loss": 1.7882,
      "step": 11423
    },
    {
      "epoch": 4.4227642276422765,
      "grad_norm": 40.01667022705078,
      "learning_rate": 6.196928635953027e-06,
      "loss": 1.1394,
      "step": 11424
    },
    {
      "epoch": 4.423151374370887,
      "grad_norm": 48.881187438964844,
      "learning_rate": 6.1964984729212375e-06,
      "loss": 0.5631,
      "step": 11425
    },
    {
      "epoch": 4.423538521099497,
      "grad_norm": 29.92091178894043,
      "learning_rate": 6.196068309889448e-06,
      "loss": 1.4144,
      "step": 11426
    },
    {
      "epoch": 4.423925667828107,
      "grad_norm": 1.264250636100769,
      "learning_rate": 6.1956381468576595e-06,
      "loss": 0.0388,
      "step": 11427
    },
    {
      "epoch": 4.424312814556717,
      "grad_norm": 21.586214065551758,
      "learning_rate": 6.195207983825871e-06,
      "loss": 2.9561,
      "step": 11428
    },
    {
      "epoch": 4.424699961285327,
      "grad_norm": 231.02439880371094,
      "learning_rate": 6.1947778207940815e-06,
      "loss": 2.9542,
      "step": 11429
    },
    {
      "epoch": 4.425087108013937,
      "grad_norm": 9.871501922607422,
      "learning_rate": 6.194347657762293e-06,
      "loss": 0.7454,
      "step": 11430
    },
    {
      "epoch": 4.425474254742547,
      "grad_norm": 30.408445358276367,
      "learning_rate": 6.1939174947305035e-06,
      "loss": 1.3094,
      "step": 11431
    },
    {
      "epoch": 4.425861401471158,
      "grad_norm": 23.62192726135254,
      "learning_rate": 6.193487331698714e-06,
      "loss": 0.6754,
      "step": 11432
    },
    {
      "epoch": 4.4262485481997675,
      "grad_norm": 12.339699745178223,
      "learning_rate": 6.193057168666925e-06,
      "loss": 0.2707,
      "step": 11433
    },
    {
      "epoch": 4.426635694928378,
      "grad_norm": 31.414146423339844,
      "learning_rate": 6.192627005635137e-06,
      "loss": 1.7184,
      "step": 11434
    },
    {
      "epoch": 4.427022841656988,
      "grad_norm": 5.5318098068237305,
      "learning_rate": 6.1921968426033475e-06,
      "loss": 0.086,
      "step": 11435
    },
    {
      "epoch": 4.427409988385598,
      "grad_norm": 33.87201690673828,
      "learning_rate": 6.191766679571558e-06,
      "loss": 1.4169,
      "step": 11436
    },
    {
      "epoch": 4.427797135114209,
      "grad_norm": 46.46297836303711,
      "learning_rate": 6.191336516539769e-06,
      "loss": 2.176,
      "step": 11437
    },
    {
      "epoch": 4.428184281842818,
      "grad_norm": 7.839267253875732,
      "learning_rate": 6.190906353507981e-06,
      "loss": 0.2905,
      "step": 11438
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 35.782920837402344,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 3.0624,
      "step": 11439
    },
    {
      "epoch": 4.4289585753000384,
      "grad_norm": 50.43064880371094,
      "learning_rate": 6.190046027444402e-06,
      "loss": 1.43,
      "step": 11440
    },
    {
      "epoch": 4.429345722028649,
      "grad_norm": 22.932415008544922,
      "learning_rate": 6.1896158644126126e-06,
      "loss": 0.4749,
      "step": 11441
    },
    {
      "epoch": 4.4297328687572595,
      "grad_norm": 17.065271377563477,
      "learning_rate": 6.189185701380824e-06,
      "loss": 0.6861,
      "step": 11442
    },
    {
      "epoch": 4.430120015485869,
      "grad_norm": 38.79743957519531,
      "learning_rate": 6.188755538349035e-06,
      "loss": 0.3469,
      "step": 11443
    },
    {
      "epoch": 4.43050716221448,
      "grad_norm": 19.767913818359375,
      "learning_rate": 6.188325375317246e-06,
      "loss": 1.0097,
      "step": 11444
    },
    {
      "epoch": 4.430894308943089,
      "grad_norm": 4.100295543670654,
      "learning_rate": 6.1878952122854565e-06,
      "loss": 0.0704,
      "step": 11445
    },
    {
      "epoch": 4.4312814556717,
      "grad_norm": 52.110687255859375,
      "learning_rate": 6.187465049253668e-06,
      "loss": 0.7703,
      "step": 11446
    },
    {
      "epoch": 4.431668602400309,
      "grad_norm": 43.822853088378906,
      "learning_rate": 6.1870348862218785e-06,
      "loss": 1.3838,
      "step": 11447
    },
    {
      "epoch": 4.43205574912892,
      "grad_norm": 34.48427963256836,
      "learning_rate": 6.186604723190089e-06,
      "loss": 2.0196,
      "step": 11448
    },
    {
      "epoch": 4.43244289585753,
      "grad_norm": 71.92141723632812,
      "learning_rate": 6.1861745601583005e-06,
      "loss": 1.809,
      "step": 11449
    },
    {
      "epoch": 4.43283004258614,
      "grad_norm": 40.94147872924805,
      "learning_rate": 6.185744397126512e-06,
      "loss": 0.5005,
      "step": 11450
    },
    {
      "epoch": 4.4332171893147505,
      "grad_norm": 21.914106369018555,
      "learning_rate": 6.1853142340947225e-06,
      "loss": 1.1847,
      "step": 11451
    },
    {
      "epoch": 4.43360433604336,
      "grad_norm": 45.928741455078125,
      "learning_rate": 6.184884071062933e-06,
      "loss": 1.3183,
      "step": 11452
    },
    {
      "epoch": 4.433991482771971,
      "grad_norm": 4.654817581176758,
      "learning_rate": 6.184453908031144e-06,
      "loss": 0.0506,
      "step": 11453
    },
    {
      "epoch": 4.43437862950058,
      "grad_norm": 45.019683837890625,
      "learning_rate": 6.184023744999356e-06,
      "loss": 0.5507,
      "step": 11454
    },
    {
      "epoch": 4.434765776229191,
      "grad_norm": 103.14754486083984,
      "learning_rate": 6.1835935819675665e-06,
      "loss": 1.0905,
      "step": 11455
    },
    {
      "epoch": 4.435152922957801,
      "grad_norm": 21.426687240600586,
      "learning_rate": 6.183163418935777e-06,
      "loss": 2.5101,
      "step": 11456
    },
    {
      "epoch": 4.435540069686411,
      "grad_norm": 7.702766418457031,
      "learning_rate": 6.182733255903988e-06,
      "loss": 0.3573,
      "step": 11457
    },
    {
      "epoch": 4.435927216415021,
      "grad_norm": 28.703584671020508,
      "learning_rate": 6.182303092872199e-06,
      "loss": 2.6637,
      "step": 11458
    },
    {
      "epoch": 4.436314363143631,
      "grad_norm": 16.84595489501953,
      "learning_rate": 6.1818729298404104e-06,
      "loss": 1.2493,
      "step": 11459
    },
    {
      "epoch": 4.4367015098722415,
      "grad_norm": 4.680911064147949,
      "learning_rate": 6.181442766808621e-06,
      "loss": 0.2596,
      "step": 11460
    },
    {
      "epoch": 4.437088656600852,
      "grad_norm": 57.175750732421875,
      "learning_rate": 6.1810126037768316e-06,
      "loss": 2.0012,
      "step": 11461
    },
    {
      "epoch": 4.437475803329462,
      "grad_norm": 41.73708724975586,
      "learning_rate": 6.180582440745043e-06,
      "loss": 1.9063,
      "step": 11462
    },
    {
      "epoch": 4.437862950058072,
      "grad_norm": 59.82694625854492,
      "learning_rate": 6.1801522777132535e-06,
      "loss": 0.793,
      "step": 11463
    },
    {
      "epoch": 4.438250096786682,
      "grad_norm": 27.061668395996094,
      "learning_rate": 6.179722114681465e-06,
      "loss": 2.3441,
      "step": 11464
    },
    {
      "epoch": 4.438637243515292,
      "grad_norm": 58.903873443603516,
      "learning_rate": 6.1792919516496755e-06,
      "loss": 1.6768,
      "step": 11465
    },
    {
      "epoch": 4.439024390243903,
      "grad_norm": 21.78314781188965,
      "learning_rate": 6.178861788617887e-06,
      "loss": 0.5965,
      "step": 11466
    },
    {
      "epoch": 4.4394115369725125,
      "grad_norm": 24.149564743041992,
      "learning_rate": 6.1784316255860975e-06,
      "loss": 1.3467,
      "step": 11467
    },
    {
      "epoch": 4.439798683701123,
      "grad_norm": 18.95477294921875,
      "learning_rate": 6.178001462554308e-06,
      "loss": 0.3177,
      "step": 11468
    },
    {
      "epoch": 4.440185830429733,
      "grad_norm": 44.03506851196289,
      "learning_rate": 6.177571299522519e-06,
      "loss": 0.716,
      "step": 11469
    },
    {
      "epoch": 4.440572977158343,
      "grad_norm": 74.75836181640625,
      "learning_rate": 6.177141136490731e-06,
      "loss": 1.056,
      "step": 11470
    },
    {
      "epoch": 4.440960123886953,
      "grad_norm": 10.428380012512207,
      "learning_rate": 6.1767109734589415e-06,
      "loss": 0.5841,
      "step": 11471
    },
    {
      "epoch": 4.441347270615563,
      "grad_norm": 44.08885955810547,
      "learning_rate": 6.176280810427152e-06,
      "loss": 0.7878,
      "step": 11472
    },
    {
      "epoch": 4.441734417344174,
      "grad_norm": 47.33476257324219,
      "learning_rate": 6.1758506473953635e-06,
      "loss": 1.7462,
      "step": 11473
    },
    {
      "epoch": 4.442121564072783,
      "grad_norm": 12.310710906982422,
      "learning_rate": 6.175420484363575e-06,
      "loss": 1.3244,
      "step": 11474
    },
    {
      "epoch": 4.442508710801394,
      "grad_norm": 84.59903717041016,
      "learning_rate": 6.1749903213317855e-06,
      "loss": 2.0893,
      "step": 11475
    },
    {
      "epoch": 4.4428958575300035,
      "grad_norm": 11.01416301727295,
      "learning_rate": 6.174560158299996e-06,
      "loss": 0.34,
      "step": 11476
    },
    {
      "epoch": 4.443283004258614,
      "grad_norm": 24.231685638427734,
      "learning_rate": 6.1741299952682074e-06,
      "loss": 2.1221,
      "step": 11477
    },
    {
      "epoch": 4.4436701509872245,
      "grad_norm": 18.276010513305664,
      "learning_rate": 6.173699832236418e-06,
      "loss": 2.1486,
      "step": 11478
    },
    {
      "epoch": 4.444057297715834,
      "grad_norm": 35.71115493774414,
      "learning_rate": 6.173269669204629e-06,
      "loss": 1.8773,
      "step": 11479
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 102.7442855834961,
      "learning_rate": 6.17283950617284e-06,
      "loss": 2.1075,
      "step": 11480
    },
    {
      "epoch": 4.444831591173054,
      "grad_norm": 21.148794174194336,
      "learning_rate": 6.172409343141051e-06,
      "loss": 1.9864,
      "step": 11481
    },
    {
      "epoch": 4.445218737901665,
      "grad_norm": 31.591482162475586,
      "learning_rate": 6.171979180109262e-06,
      "loss": 0.3217,
      "step": 11482
    },
    {
      "epoch": 4.445605884630275,
      "grad_norm": 86.51930236816406,
      "learning_rate": 6.1715490170774725e-06,
      "loss": 1.3737,
      "step": 11483
    },
    {
      "epoch": 4.445993031358885,
      "grad_norm": 19.943317413330078,
      "learning_rate": 6.171118854045683e-06,
      "loss": 0.5938,
      "step": 11484
    },
    {
      "epoch": 4.446380178087495,
      "grad_norm": 47.268524169921875,
      "learning_rate": 6.170688691013895e-06,
      "loss": 0.945,
      "step": 11485
    },
    {
      "epoch": 4.446767324816105,
      "grad_norm": 37.159515380859375,
      "learning_rate": 6.170258527982106e-06,
      "loss": 3.1685,
      "step": 11486
    },
    {
      "epoch": 4.4471544715447155,
      "grad_norm": 62.284767150878906,
      "learning_rate": 6.1698283649503165e-06,
      "loss": 1.4531,
      "step": 11487
    },
    {
      "epoch": 4.447541618273325,
      "grad_norm": 23.63474464416504,
      "learning_rate": 6.169398201918527e-06,
      "loss": 0.7983,
      "step": 11488
    },
    {
      "epoch": 4.447928765001936,
      "grad_norm": 100.77650451660156,
      "learning_rate": 6.168968038886739e-06,
      "loss": 2.0601,
      "step": 11489
    },
    {
      "epoch": 4.448315911730546,
      "grad_norm": 2.777195930480957,
      "learning_rate": 6.16853787585495e-06,
      "loss": 0.1393,
      "step": 11490
    },
    {
      "epoch": 4.448703058459156,
      "grad_norm": 10.373592376708984,
      "learning_rate": 6.1681077128231605e-06,
      "loss": 0.2218,
      "step": 11491
    },
    {
      "epoch": 4.449090205187766,
      "grad_norm": 117.42274475097656,
      "learning_rate": 6.167677549791371e-06,
      "loss": 0.6304,
      "step": 11492
    },
    {
      "epoch": 4.449477351916376,
      "grad_norm": 19.917179107666016,
      "learning_rate": 6.1672473867595825e-06,
      "loss": 0.4613,
      "step": 11493
    },
    {
      "epoch": 4.4498644986449865,
      "grad_norm": 13.275434494018555,
      "learning_rate": 6.166817223727793e-06,
      "loss": 0.5668,
      "step": 11494
    },
    {
      "epoch": 4.450251645373597,
      "grad_norm": 13.07990837097168,
      "learning_rate": 6.1663870606960045e-06,
      "loss": 0.7068,
      "step": 11495
    },
    {
      "epoch": 4.450638792102207,
      "grad_norm": 6.36833381652832,
      "learning_rate": 6.165956897664215e-06,
      "loss": 0.2215,
      "step": 11496
    },
    {
      "epoch": 4.451025938830817,
      "grad_norm": 12.672182083129883,
      "learning_rate": 6.1655267346324264e-06,
      "loss": 0.3412,
      "step": 11497
    },
    {
      "epoch": 4.451413085559427,
      "grad_norm": 28.916521072387695,
      "learning_rate": 6.165096571600637e-06,
      "loss": 1.2956,
      "step": 11498
    },
    {
      "epoch": 4.451800232288037,
      "grad_norm": 37.30107879638672,
      "learning_rate": 6.1646664085688476e-06,
      "loss": 1.529,
      "step": 11499
    },
    {
      "epoch": 4.452187379016648,
      "grad_norm": 14.103378295898438,
      "learning_rate": 6.164236245537059e-06,
      "loss": 1.2709,
      "step": 11500
    },
    {
      "epoch": 4.452574525745257,
      "grad_norm": 58.4990348815918,
      "learning_rate": 6.16380608250527e-06,
      "loss": 1.8328,
      "step": 11501
    },
    {
      "epoch": 4.452961672473868,
      "grad_norm": 71.95777893066406,
      "learning_rate": 6.163375919473481e-06,
      "loss": 0.875,
      "step": 11502
    },
    {
      "epoch": 4.4533488192024775,
      "grad_norm": 56.42610168457031,
      "learning_rate": 6.1629457564416915e-06,
      "loss": 1.6423,
      "step": 11503
    },
    {
      "epoch": 4.453735965931088,
      "grad_norm": 30.607351303100586,
      "learning_rate": 6.162515593409902e-06,
      "loss": 2.5399,
      "step": 11504
    },
    {
      "epoch": 4.454123112659698,
      "grad_norm": 62.05722427368164,
      "learning_rate": 6.162085430378114e-06,
      "loss": 1.9808,
      "step": 11505
    },
    {
      "epoch": 4.454510259388308,
      "grad_norm": 21.09207534790039,
      "learning_rate": 6.161655267346325e-06,
      "loss": 0.9459,
      "step": 11506
    },
    {
      "epoch": 4.454897406116919,
      "grad_norm": 22.674863815307617,
      "learning_rate": 6.1612251043145355e-06,
      "loss": 0.6773,
      "step": 11507
    },
    {
      "epoch": 4.455284552845528,
      "grad_norm": 58.997806549072266,
      "learning_rate": 6.160794941282746e-06,
      "loss": 1.9132,
      "step": 11508
    },
    {
      "epoch": 4.455671699574139,
      "grad_norm": 12.09936809539795,
      "learning_rate": 6.1603647782509575e-06,
      "loss": 0.5751,
      "step": 11509
    },
    {
      "epoch": 4.456058846302748,
      "grad_norm": 7.4639434814453125,
      "learning_rate": 6.159934615219169e-06,
      "loss": 0.21,
      "step": 11510
    },
    {
      "epoch": 4.456445993031359,
      "grad_norm": 27.41029930114746,
      "learning_rate": 6.1595044521873795e-06,
      "loss": 1.8809,
      "step": 11511
    },
    {
      "epoch": 4.456833139759969,
      "grad_norm": 58.9848747253418,
      "learning_rate": 6.159074289155591e-06,
      "loss": 1.9642,
      "step": 11512
    },
    {
      "epoch": 4.457220286488579,
      "grad_norm": 20.76221466064453,
      "learning_rate": 6.1586441261238015e-06,
      "loss": 2.9696,
      "step": 11513
    },
    {
      "epoch": 4.4576074332171896,
      "grad_norm": 10.860024452209473,
      "learning_rate": 6.158213963092012e-06,
      "loss": 0.5088,
      "step": 11514
    },
    {
      "epoch": 4.457994579945799,
      "grad_norm": 8.85363483428955,
      "learning_rate": 6.1577838000602234e-06,
      "loss": 0.4861,
      "step": 11515
    },
    {
      "epoch": 4.45838172667441,
      "grad_norm": 44.84803009033203,
      "learning_rate": 6.157353637028435e-06,
      "loss": 1.0054,
      "step": 11516
    },
    {
      "epoch": 4.45876887340302,
      "grad_norm": 13.814692497253418,
      "learning_rate": 6.1569234739966454e-06,
      "loss": 0.5592,
      "step": 11517
    },
    {
      "epoch": 4.45915602013163,
      "grad_norm": 9.012677192687988,
      "learning_rate": 6.156493310964856e-06,
      "loss": 0.6944,
      "step": 11518
    },
    {
      "epoch": 4.45954316686024,
      "grad_norm": 35.781063079833984,
      "learning_rate": 6.1560631479330666e-06,
      "loss": 0.8348,
      "step": 11519
    },
    {
      "epoch": 4.45993031358885,
      "grad_norm": 22.7501163482666,
      "learning_rate": 6.155632984901279e-06,
      "loss": 1.2523,
      "step": 11520
    },
    {
      "epoch": 4.4603174603174605,
      "grad_norm": 93.16716003417969,
      "learning_rate": 6.155202821869489e-06,
      "loss": 2.9957,
      "step": 11521
    },
    {
      "epoch": 4.46070460704607,
      "grad_norm": 17.052291870117188,
      "learning_rate": 6.1547726588377e-06,
      "loss": 0.9185,
      "step": 11522
    },
    {
      "epoch": 4.461091753774681,
      "grad_norm": 15.147928237915039,
      "learning_rate": 6.1543424958059105e-06,
      "loss": 1.1003,
      "step": 11523
    },
    {
      "epoch": 4.461478900503291,
      "grad_norm": 17.067733764648438,
      "learning_rate": 6.153912332774122e-06,
      "loss": 0.8124,
      "step": 11524
    },
    {
      "epoch": 4.461866047231901,
      "grad_norm": 32.792083740234375,
      "learning_rate": 6.153482169742333e-06,
      "loss": 1.5368,
      "step": 11525
    },
    {
      "epoch": 4.462253193960511,
      "grad_norm": 28.262218475341797,
      "learning_rate": 6.153052006710544e-06,
      "loss": 1.062,
      "step": 11526
    },
    {
      "epoch": 4.462640340689121,
      "grad_norm": 22.57380485534668,
      "learning_rate": 6.1526218436787545e-06,
      "loss": 0.7232,
      "step": 11527
    },
    {
      "epoch": 4.463027487417731,
      "grad_norm": 46.903297424316406,
      "learning_rate": 6.152191680646966e-06,
      "loss": 2.7196,
      "step": 11528
    },
    {
      "epoch": 4.463414634146342,
      "grad_norm": 3.6434524059295654,
      "learning_rate": 6.1517615176151765e-06,
      "loss": 0.0548,
      "step": 11529
    },
    {
      "epoch": 4.4638017808749515,
      "grad_norm": 10.726823806762695,
      "learning_rate": 6.151331354583387e-06,
      "loss": 0.5748,
      "step": 11530
    },
    {
      "epoch": 4.464188927603562,
      "grad_norm": 48.88773727416992,
      "learning_rate": 6.1509011915515985e-06,
      "loss": 1.6798,
      "step": 11531
    },
    {
      "epoch": 4.464576074332172,
      "grad_norm": 16.988683700561523,
      "learning_rate": 6.15047102851981e-06,
      "loss": 0.8966,
      "step": 11532
    },
    {
      "epoch": 4.464963221060782,
      "grad_norm": 44.89128112792969,
      "learning_rate": 6.1500408654880205e-06,
      "loss": 1.4184,
      "step": 11533
    },
    {
      "epoch": 4.465350367789393,
      "grad_norm": 6.068639278411865,
      "learning_rate": 6.149610702456231e-06,
      "loss": 0.2073,
      "step": 11534
    },
    {
      "epoch": 4.465737514518002,
      "grad_norm": 16.13604736328125,
      "learning_rate": 6.149180539424442e-06,
      "loss": 0.7808,
      "step": 11535
    },
    {
      "epoch": 4.466124661246613,
      "grad_norm": 104.38404083251953,
      "learning_rate": 6.148750376392654e-06,
      "loss": 2.3331,
      "step": 11536
    },
    {
      "epoch": 4.466511807975222,
      "grad_norm": 43.15414810180664,
      "learning_rate": 6.1483202133608644e-06,
      "loss": 0.5033,
      "step": 11537
    },
    {
      "epoch": 4.466898954703833,
      "grad_norm": 21.7911319732666,
      "learning_rate": 6.147890050329075e-06,
      "loss": 0.7858,
      "step": 11538
    },
    {
      "epoch": 4.4672861014324425,
      "grad_norm": 20.505596160888672,
      "learning_rate": 6.1474598872972856e-06,
      "loss": 0.6356,
      "step": 11539
    },
    {
      "epoch": 4.467673248161053,
      "grad_norm": 112.52552032470703,
      "learning_rate": 6.147029724265498e-06,
      "loss": 2.3921,
      "step": 11540
    },
    {
      "epoch": 4.4680603948896636,
      "grad_norm": 8.774406433105469,
      "learning_rate": 6.146599561233708e-06,
      "loss": 0.5463,
      "step": 11541
    },
    {
      "epoch": 4.468447541618273,
      "grad_norm": 16.732650756835938,
      "learning_rate": 6.146169398201919e-06,
      "loss": 0.6394,
      "step": 11542
    },
    {
      "epoch": 4.468834688346884,
      "grad_norm": 21.967330932617188,
      "learning_rate": 6.1457392351701295e-06,
      "loss": 1.8808,
      "step": 11543
    },
    {
      "epoch": 4.469221835075493,
      "grad_norm": 24.517070770263672,
      "learning_rate": 6.145309072138341e-06,
      "loss": 0.7377,
      "step": 11544
    },
    {
      "epoch": 4.469608981804104,
      "grad_norm": 20.595998764038086,
      "learning_rate": 6.1448789091065515e-06,
      "loss": 1.4281,
      "step": 11545
    },
    {
      "epoch": 4.469996128532713,
      "grad_norm": 95.03064727783203,
      "learning_rate": 6.144448746074763e-06,
      "loss": 1.369,
      "step": 11546
    },
    {
      "epoch": 4.470383275261324,
      "grad_norm": 55.667091369628906,
      "learning_rate": 6.1440185830429735e-06,
      "loss": 2.4947,
      "step": 11547
    },
    {
      "epoch": 4.4707704219899345,
      "grad_norm": 30.53546714782715,
      "learning_rate": 6.143588420011185e-06,
      "loss": 0.7738,
      "step": 11548
    },
    {
      "epoch": 4.471157568718544,
      "grad_norm": 25.982576370239258,
      "learning_rate": 6.1431582569793955e-06,
      "loss": 0.6652,
      "step": 11549
    },
    {
      "epoch": 4.471544715447155,
      "grad_norm": 6.345634937286377,
      "learning_rate": 6.142728093947606e-06,
      "loss": 0.7946,
      "step": 11550
    },
    {
      "epoch": 4.471931862175764,
      "grad_norm": 28.593477249145508,
      "learning_rate": 6.1422979309158175e-06,
      "loss": 1.6556,
      "step": 11551
    },
    {
      "epoch": 4.472319008904375,
      "grad_norm": 68.01273345947266,
      "learning_rate": 6.141867767884029e-06,
      "loss": 1.2088,
      "step": 11552
    },
    {
      "epoch": 4.472706155632985,
      "grad_norm": 6.6410417556762695,
      "learning_rate": 6.1414376048522395e-06,
      "loss": 0.3274,
      "step": 11553
    },
    {
      "epoch": 4.473093302361595,
      "grad_norm": 42.99383544921875,
      "learning_rate": 6.14100744182045e-06,
      "loss": 1.4673,
      "step": 11554
    },
    {
      "epoch": 4.473480449090205,
      "grad_norm": 22.00086784362793,
      "learning_rate": 6.140577278788662e-06,
      "loss": 1.3337,
      "step": 11555
    },
    {
      "epoch": 4.473867595818815,
      "grad_norm": 23.427310943603516,
      "learning_rate": 6.140147115756873e-06,
      "loss": 1.5301,
      "step": 11556
    },
    {
      "epoch": 4.4742547425474255,
      "grad_norm": 100.44903564453125,
      "learning_rate": 6.1397169527250834e-06,
      "loss": 1.2841,
      "step": 11557
    },
    {
      "epoch": 4.474641889276036,
      "grad_norm": 5.573631763458252,
      "learning_rate": 6.139286789693294e-06,
      "loss": 0.2144,
      "step": 11558
    },
    {
      "epoch": 4.475029036004646,
      "grad_norm": 98.81072998046875,
      "learning_rate": 6.138856626661505e-06,
      "loss": 1.0806,
      "step": 11559
    },
    {
      "epoch": 4.475416182733256,
      "grad_norm": 10.570950508117676,
      "learning_rate": 6.138426463629716e-06,
      "loss": 0.2071,
      "step": 11560
    },
    {
      "epoch": 4.475803329461866,
      "grad_norm": 24.5546932220459,
      "learning_rate": 6.137996300597927e-06,
      "loss": 1.6172,
      "step": 11561
    },
    {
      "epoch": 4.476190476190476,
      "grad_norm": 131.3622589111328,
      "learning_rate": 6.137566137566138e-06,
      "loss": 0.9412,
      "step": 11562
    },
    {
      "epoch": 4.476577622919086,
      "grad_norm": 19.995878219604492,
      "learning_rate": 6.137135974534349e-06,
      "loss": 2.0252,
      "step": 11563
    },
    {
      "epoch": 4.476964769647696,
      "grad_norm": 30.273452758789062,
      "learning_rate": 6.13670581150256e-06,
      "loss": 3.2147,
      "step": 11564
    },
    {
      "epoch": 4.477351916376307,
      "grad_norm": 17.55352783203125,
      "learning_rate": 6.1362756484707705e-06,
      "loss": 1.8183,
      "step": 11565
    },
    {
      "epoch": 4.4777390631049165,
      "grad_norm": 20.65329360961914,
      "learning_rate": 6.135845485438981e-06,
      "loss": 0.72,
      "step": 11566
    },
    {
      "epoch": 4.478126209833527,
      "grad_norm": 18.720911026000977,
      "learning_rate": 6.135415322407193e-06,
      "loss": 1.616,
      "step": 11567
    },
    {
      "epoch": 4.478513356562137,
      "grad_norm": 46.020408630371094,
      "learning_rate": 6.134985159375404e-06,
      "loss": 1.3382,
      "step": 11568
    },
    {
      "epoch": 4.478900503290747,
      "grad_norm": 24.99878692626953,
      "learning_rate": 6.1345549963436145e-06,
      "loss": 1.7435,
      "step": 11569
    },
    {
      "epoch": 4.479287650019358,
      "grad_norm": 57.05166244506836,
      "learning_rate": 6.134124833311825e-06,
      "loss": 1.9451,
      "step": 11570
    },
    {
      "epoch": 4.479674796747967,
      "grad_norm": 17.491106033325195,
      "learning_rate": 6.133694670280037e-06,
      "loss": 1.5941,
      "step": 11571
    },
    {
      "epoch": 4.480061943476578,
      "grad_norm": 14.122191429138184,
      "learning_rate": 6.133264507248248e-06,
      "loss": 1.3574,
      "step": 11572
    },
    {
      "epoch": 4.4804490902051874,
      "grad_norm": 19.635679244995117,
      "learning_rate": 6.1328343442164585e-06,
      "loss": 2.9135,
      "step": 11573
    },
    {
      "epoch": 4.480836236933798,
      "grad_norm": 59.43247604370117,
      "learning_rate": 6.132404181184669e-06,
      "loss": 1.1033,
      "step": 11574
    },
    {
      "epoch": 4.4812233836624085,
      "grad_norm": 41.071800231933594,
      "learning_rate": 6.1319740181528804e-06,
      "loss": 2.186,
      "step": 11575
    },
    {
      "epoch": 4.481610530391018,
      "grad_norm": 12.445154190063477,
      "learning_rate": 6.131543855121092e-06,
      "loss": 0.5572,
      "step": 11576
    },
    {
      "epoch": 4.481997677119629,
      "grad_norm": 12.199165344238281,
      "learning_rate": 6.1311136920893024e-06,
      "loss": 0.6189,
      "step": 11577
    },
    {
      "epoch": 4.482384823848238,
      "grad_norm": 20.006793975830078,
      "learning_rate": 6.130683529057513e-06,
      "loss": 0.9314,
      "step": 11578
    },
    {
      "epoch": 4.482771970576849,
      "grad_norm": 19.66861343383789,
      "learning_rate": 6.130253366025724e-06,
      "loss": 0.9679,
      "step": 11579
    },
    {
      "epoch": 4.483159117305458,
      "grad_norm": 26.42713165283203,
      "learning_rate": 6.129823202993935e-06,
      "loss": 2.0236,
      "step": 11580
    },
    {
      "epoch": 4.483546264034069,
      "grad_norm": 21.117048263549805,
      "learning_rate": 6.1293930399621455e-06,
      "loss": 1.067,
      "step": 11581
    },
    {
      "epoch": 4.483933410762679,
      "grad_norm": 24.451393127441406,
      "learning_rate": 6.128962876930357e-06,
      "loss": 1.3166,
      "step": 11582
    },
    {
      "epoch": 4.484320557491289,
      "grad_norm": 6.741427421569824,
      "learning_rate": 6.128532713898568e-06,
      "loss": 0.3064,
      "step": 11583
    },
    {
      "epoch": 4.4847077042198995,
      "grad_norm": 71.1805419921875,
      "learning_rate": 6.128102550866779e-06,
      "loss": 2.3417,
      "step": 11584
    },
    {
      "epoch": 4.485094850948509,
      "grad_norm": 27.04491424560547,
      "learning_rate": 6.1276723878349895e-06,
      "loss": 2.7665,
      "step": 11585
    },
    {
      "epoch": 4.48548199767712,
      "grad_norm": 38.72299575805664,
      "learning_rate": 6.1272422248032e-06,
      "loss": 2.5245,
      "step": 11586
    },
    {
      "epoch": 4.48586914440573,
      "grad_norm": 44.74666213989258,
      "learning_rate": 6.126812061771412e-06,
      "loss": 1.4812,
      "step": 11587
    },
    {
      "epoch": 4.48625629113434,
      "grad_norm": 32.000911712646484,
      "learning_rate": 6.126381898739623e-06,
      "loss": 0.3644,
      "step": 11588
    },
    {
      "epoch": 4.48664343786295,
      "grad_norm": 19.486726760864258,
      "learning_rate": 6.1259517357078335e-06,
      "loss": 1.5038,
      "step": 11589
    },
    {
      "epoch": 4.48703058459156,
      "grad_norm": 96.10798645019531,
      "learning_rate": 6.125521572676044e-06,
      "loss": 1.3327,
      "step": 11590
    },
    {
      "epoch": 4.48741773132017,
      "grad_norm": 39.86166763305664,
      "learning_rate": 6.125091409644256e-06,
      "loss": 3.4525,
      "step": 11591
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 37.33614730834961,
      "learning_rate": 6.124661246612467e-06,
      "loss": 2.0525,
      "step": 11592
    },
    {
      "epoch": 4.4881920247773905,
      "grad_norm": 23.753032684326172,
      "learning_rate": 6.1242310835806775e-06,
      "loss": 1.3134,
      "step": 11593
    },
    {
      "epoch": 4.488579171506001,
      "grad_norm": 111.75501251220703,
      "learning_rate": 6.123800920548889e-06,
      "loss": 2.1994,
      "step": 11594
    },
    {
      "epoch": 4.488966318234611,
      "grad_norm": 8.846120834350586,
      "learning_rate": 6.1233707575170994e-06,
      "loss": 0.2778,
      "step": 11595
    },
    {
      "epoch": 4.489353464963221,
      "grad_norm": 53.4002571105957,
      "learning_rate": 6.12294059448531e-06,
      "loss": 2.721,
      "step": 11596
    },
    {
      "epoch": 4.489740611691831,
      "grad_norm": 39.04454803466797,
      "learning_rate": 6.122510431453521e-06,
      "loss": 0.2034,
      "step": 11597
    },
    {
      "epoch": 4.490127758420441,
      "grad_norm": 7.637265682220459,
      "learning_rate": 6.122080268421733e-06,
      "loss": 0.9698,
      "step": 11598
    },
    {
      "epoch": 4.490514905149052,
      "grad_norm": 41.095542907714844,
      "learning_rate": 6.121650105389943e-06,
      "loss": 0.7189,
      "step": 11599
    },
    {
      "epoch": 4.4909020518776614,
      "grad_norm": 79.48846435546875,
      "learning_rate": 6.121219942358154e-06,
      "loss": 3.3805,
      "step": 11600
    },
    {
      "epoch": 4.491289198606272,
      "grad_norm": 12.489350318908691,
      "learning_rate": 6.1207897793263645e-06,
      "loss": 0.5631,
      "step": 11601
    },
    {
      "epoch": 4.491676345334882,
      "grad_norm": 54.76112747192383,
      "learning_rate": 6.120359616294577e-06,
      "loss": 2.0606,
      "step": 11602
    },
    {
      "epoch": 4.492063492063492,
      "grad_norm": 27.89410400390625,
      "learning_rate": 6.119929453262787e-06,
      "loss": 2.2148,
      "step": 11603
    },
    {
      "epoch": 4.492450638792103,
      "grad_norm": 34.93745803833008,
      "learning_rate": 6.119499290230998e-06,
      "loss": 0.7973,
      "step": 11604
    },
    {
      "epoch": 4.492837785520712,
      "grad_norm": 25.911428451538086,
      "learning_rate": 6.1190691271992085e-06,
      "loss": 1.5513,
      "step": 11605
    },
    {
      "epoch": 4.493224932249323,
      "grad_norm": 36.987056732177734,
      "learning_rate": 6.118638964167421e-06,
      "loss": 0.731,
      "step": 11606
    },
    {
      "epoch": 4.493612078977932,
      "grad_norm": 18.372283935546875,
      "learning_rate": 6.118208801135631e-06,
      "loss": 0.8756,
      "step": 11607
    },
    {
      "epoch": 4.493999225706543,
      "grad_norm": 29.75079917907715,
      "learning_rate": 6.117778638103842e-06,
      "loss": 1.309,
      "step": 11608
    },
    {
      "epoch": 4.494386372435153,
      "grad_norm": 7.300601005554199,
      "learning_rate": 6.1173484750720525e-06,
      "loss": 0.3899,
      "step": 11609
    },
    {
      "epoch": 4.494773519163763,
      "grad_norm": 23.32546615600586,
      "learning_rate": 6.116918312040264e-06,
      "loss": 3.1145,
      "step": 11610
    },
    {
      "epoch": 4.4951606658923735,
      "grad_norm": 47.8210334777832,
      "learning_rate": 6.1164881490084745e-06,
      "loss": 2.2309,
      "step": 11611
    },
    {
      "epoch": 4.495547812620983,
      "grad_norm": 16.194738388061523,
      "learning_rate": 6.116057985976686e-06,
      "loss": 0.8696,
      "step": 11612
    },
    {
      "epoch": 4.495934959349594,
      "grad_norm": 34.94415283203125,
      "learning_rate": 6.1156278229448965e-06,
      "loss": 1.0362,
      "step": 11613
    },
    {
      "epoch": 4.496322106078203,
      "grad_norm": 35.543827056884766,
      "learning_rate": 6.115197659913108e-06,
      "loss": 1.3571,
      "step": 11614
    },
    {
      "epoch": 4.496709252806814,
      "grad_norm": 28.671947479248047,
      "learning_rate": 6.1147674968813184e-06,
      "loss": 2.3309,
      "step": 11615
    },
    {
      "epoch": 4.497096399535424,
      "grad_norm": 116.05890655517578,
      "learning_rate": 6.114337333849529e-06,
      "loss": 2.9296,
      "step": 11616
    },
    {
      "epoch": 4.497483546264034,
      "grad_norm": 59.487178802490234,
      "learning_rate": 6.1139071708177396e-06,
      "loss": 1.3561,
      "step": 11617
    },
    {
      "epoch": 4.497870692992644,
      "grad_norm": 44.25345230102539,
      "learning_rate": 6.113477007785952e-06,
      "loss": 1.2796,
      "step": 11618
    },
    {
      "epoch": 4.498257839721254,
      "grad_norm": 35.07772445678711,
      "learning_rate": 6.113046844754162e-06,
      "loss": 0.9482,
      "step": 11619
    },
    {
      "epoch": 4.4986449864498645,
      "grad_norm": 20.25782012939453,
      "learning_rate": 6.112616681722373e-06,
      "loss": 0.5873,
      "step": 11620
    },
    {
      "epoch": 4.499032133178475,
      "grad_norm": 1.252820372581482,
      "learning_rate": 6.1121865186905835e-06,
      "loss": 0.0372,
      "step": 11621
    },
    {
      "epoch": 4.499419279907085,
      "grad_norm": 20.53318214416504,
      "learning_rate": 6.111756355658796e-06,
      "loss": 1.0033,
      "step": 11622
    },
    {
      "epoch": 4.499806426635695,
      "grad_norm": 30.67104148864746,
      "learning_rate": 6.111326192627006e-06,
      "loss": 1.2509,
      "step": 11623
    },
    {
      "epoch": 4.500193573364305,
      "grad_norm": 25.17572021484375,
      "learning_rate": 6.110896029595217e-06,
      "loss": 0.5626,
      "step": 11624
    },
    {
      "epoch": 4.500580720092915,
      "grad_norm": 32.1794548034668,
      "learning_rate": 6.1104658665634275e-06,
      "loss": 1.9019,
      "step": 11625
    },
    {
      "epoch": 4.500967866821526,
      "grad_norm": 13.08604621887207,
      "learning_rate": 6.110035703531639e-06,
      "loss": 0.3945,
      "step": 11626
    },
    {
      "epoch": 4.5013550135501355,
      "grad_norm": 27.66823387145996,
      "learning_rate": 6.10960554049985e-06,
      "loss": 2.6317,
      "step": 11627
    },
    {
      "epoch": 4.501742160278746,
      "grad_norm": 7.315390110015869,
      "learning_rate": 6.109175377468061e-06,
      "loss": 0.4643,
      "step": 11628
    },
    {
      "epoch": 4.502129307007356,
      "grad_norm": 35.16007614135742,
      "learning_rate": 6.1087452144362715e-06,
      "loss": 1.2351,
      "step": 11629
    },
    {
      "epoch": 4.502516453735966,
      "grad_norm": 19.18678092956543,
      "learning_rate": 6.108315051404483e-06,
      "loss": 1.5278,
      "step": 11630
    },
    {
      "epoch": 4.502903600464576,
      "grad_norm": 36.15535354614258,
      "learning_rate": 6.1078848883726935e-06,
      "loss": 1.4592,
      "step": 11631
    },
    {
      "epoch": 4.503290747193186,
      "grad_norm": 38.425819396972656,
      "learning_rate": 6.107454725340904e-06,
      "loss": 0.8841,
      "step": 11632
    },
    {
      "epoch": 4.503677893921797,
      "grad_norm": 17.79536247253418,
      "learning_rate": 6.1070245623091154e-06,
      "loss": 1.1172,
      "step": 11633
    },
    {
      "epoch": 4.504065040650406,
      "grad_norm": 19.76451873779297,
      "learning_rate": 6.106594399277327e-06,
      "loss": 1.413,
      "step": 11634
    },
    {
      "epoch": 4.504452187379017,
      "grad_norm": 37.00536346435547,
      "learning_rate": 6.1061642362455374e-06,
      "loss": 1.1979,
      "step": 11635
    },
    {
      "epoch": 4.5048393341076265,
      "grad_norm": 41.48017883300781,
      "learning_rate": 6.105734073213748e-06,
      "loss": 1.6868,
      "step": 11636
    },
    {
      "epoch": 4.505226480836237,
      "grad_norm": 27.491823196411133,
      "learning_rate": 6.10530391018196e-06,
      "loss": 1.2825,
      "step": 11637
    },
    {
      "epoch": 4.505613627564847,
      "grad_norm": 13.029352188110352,
      "learning_rate": 6.104873747150171e-06,
      "loss": 1.0492,
      "step": 11638
    },
    {
      "epoch": 4.506000774293457,
      "grad_norm": 15.20716381072998,
      "learning_rate": 6.104443584118381e-06,
      "loss": 2.0602,
      "step": 11639
    },
    {
      "epoch": 4.506387921022068,
      "grad_norm": 24.2087459564209,
      "learning_rate": 6.104013421086592e-06,
      "loss": 1.5319,
      "step": 11640
    },
    {
      "epoch": 4.506775067750677,
      "grad_norm": 42.8261833190918,
      "learning_rate": 6.103583258054803e-06,
      "loss": 1.1151,
      "step": 11641
    },
    {
      "epoch": 4.507162214479288,
      "grad_norm": 40.37773132324219,
      "learning_rate": 6.103153095023015e-06,
      "loss": 0.6621,
      "step": 11642
    },
    {
      "epoch": 4.507549361207898,
      "grad_norm": 33.62209701538086,
      "learning_rate": 6.102722931991225e-06,
      "loss": 1.374,
      "step": 11643
    },
    {
      "epoch": 4.507936507936508,
      "grad_norm": 33.17489242553711,
      "learning_rate": 6.102292768959436e-06,
      "loss": 1.4102,
      "step": 11644
    },
    {
      "epoch": 4.508323654665118,
      "grad_norm": 83.20313262939453,
      "learning_rate": 6.101862605927647e-06,
      "loss": 2.2504,
      "step": 11645
    },
    {
      "epoch": 4.508710801393728,
      "grad_norm": 95.64576721191406,
      "learning_rate": 6.101432442895858e-06,
      "loss": 0.2964,
      "step": 11646
    },
    {
      "epoch": 4.5090979481223386,
      "grad_norm": 22.612001419067383,
      "learning_rate": 6.1010022798640685e-06,
      "loss": 1.1011,
      "step": 11647
    },
    {
      "epoch": 4.509485094850948,
      "grad_norm": 87.84963989257812,
      "learning_rate": 6.10057211683228e-06,
      "loss": 2.1977,
      "step": 11648
    },
    {
      "epoch": 4.509872241579559,
      "grad_norm": 13.619181632995605,
      "learning_rate": 6.100141953800491e-06,
      "loss": 0.8427,
      "step": 11649
    },
    {
      "epoch": 4.510259388308169,
      "grad_norm": 15.020689010620117,
      "learning_rate": 6.099711790768702e-06,
      "loss": 0.5117,
      "step": 11650
    },
    {
      "epoch": 4.510646535036779,
      "grad_norm": 50.51920700073242,
      "learning_rate": 6.0992816277369125e-06,
      "loss": 1.1833,
      "step": 11651
    },
    {
      "epoch": 4.511033681765389,
      "grad_norm": 14.067859649658203,
      "learning_rate": 6.098851464705123e-06,
      "loss": 1.5277,
      "step": 11652
    },
    {
      "epoch": 4.511420828493999,
      "grad_norm": 34.81776809692383,
      "learning_rate": 6.098421301673335e-06,
      "loss": 1.13,
      "step": 11653
    },
    {
      "epoch": 4.5118079752226095,
      "grad_norm": 15.807197570800781,
      "learning_rate": 6.097991138641546e-06,
      "loss": 0.6666,
      "step": 11654
    },
    {
      "epoch": 4.512195121951219,
      "grad_norm": 23.05440330505371,
      "learning_rate": 6.0975609756097564e-06,
      "loss": 1.3245,
      "step": 11655
    },
    {
      "epoch": 4.51258226867983,
      "grad_norm": 37.82666015625,
      "learning_rate": 6.097130812577967e-06,
      "loss": 0.9095,
      "step": 11656
    },
    {
      "epoch": 4.51296941540844,
      "grad_norm": 13.43283748626709,
      "learning_rate": 6.096700649546179e-06,
      "loss": 0.6148,
      "step": 11657
    },
    {
      "epoch": 4.51335656213705,
      "grad_norm": 8.272836685180664,
      "learning_rate": 6.09627048651439e-06,
      "loss": 0.6225,
      "step": 11658
    },
    {
      "epoch": 4.51374370886566,
      "grad_norm": 32.564109802246094,
      "learning_rate": 6.0958403234826e-06,
      "loss": 1.7661,
      "step": 11659
    },
    {
      "epoch": 4.51413085559427,
      "grad_norm": 20.648456573486328,
      "learning_rate": 6.095410160450811e-06,
      "loss": 0.4864,
      "step": 11660
    },
    {
      "epoch": 4.51451800232288,
      "grad_norm": 24.075620651245117,
      "learning_rate": 6.094979997419022e-06,
      "loss": 1.836,
      "step": 11661
    },
    {
      "epoch": 4.514905149051491,
      "grad_norm": 10.067204475402832,
      "learning_rate": 6.094549834387233e-06,
      "loss": 0.4016,
      "step": 11662
    },
    {
      "epoch": 4.5152922957801005,
      "grad_norm": 60.73823928833008,
      "learning_rate": 6.094119671355444e-06,
      "loss": 1.3652,
      "step": 11663
    },
    {
      "epoch": 4.515679442508711,
      "grad_norm": 26.39322280883789,
      "learning_rate": 6.093689508323655e-06,
      "loss": 0.9739,
      "step": 11664
    },
    {
      "epoch": 4.516066589237321,
      "grad_norm": 24.69813346862793,
      "learning_rate": 6.093259345291866e-06,
      "loss": 1.0165,
      "step": 11665
    },
    {
      "epoch": 4.516453735965931,
      "grad_norm": 34.35422134399414,
      "learning_rate": 6.092829182260077e-06,
      "loss": 0.7381,
      "step": 11666
    },
    {
      "epoch": 4.516840882694542,
      "grad_norm": 13.373615264892578,
      "learning_rate": 6.0923990192282875e-06,
      "loss": 0.4771,
      "step": 11667
    },
    {
      "epoch": 4.517228029423151,
      "grad_norm": 11.980936050415039,
      "learning_rate": 6.091968856196498e-06,
      "loss": 0.6577,
      "step": 11668
    },
    {
      "epoch": 4.517615176151762,
      "grad_norm": 3.9768564701080322,
      "learning_rate": 6.09153869316471e-06,
      "loss": 0.2291,
      "step": 11669
    },
    {
      "epoch": 4.518002322880371,
      "grad_norm": 26.842060089111328,
      "learning_rate": 6.091108530132921e-06,
      "loss": 1.5204,
      "step": 11670
    },
    {
      "epoch": 4.518389469608982,
      "grad_norm": 23.67298698425293,
      "learning_rate": 6.0906783671011315e-06,
      "loss": 1.819,
      "step": 11671
    },
    {
      "epoch": 4.5187766163375915,
      "grad_norm": 52.351966857910156,
      "learning_rate": 6.090248204069342e-06,
      "loss": 1.7331,
      "step": 11672
    },
    {
      "epoch": 4.519163763066202,
      "grad_norm": 6.720490455627441,
      "learning_rate": 6.089818041037554e-06,
      "loss": 0.2872,
      "step": 11673
    },
    {
      "epoch": 4.5195509097948126,
      "grad_norm": 17.108362197875977,
      "learning_rate": 6.089387878005765e-06,
      "loss": 0.5301,
      "step": 11674
    },
    {
      "epoch": 4.519938056523422,
      "grad_norm": 100.58497619628906,
      "learning_rate": 6.0889577149739754e-06,
      "loss": 1.5178,
      "step": 11675
    },
    {
      "epoch": 4.520325203252033,
      "grad_norm": 8.517749786376953,
      "learning_rate": 6.088527551942186e-06,
      "loss": 0.7372,
      "step": 11676
    },
    {
      "epoch": 4.520712349980642,
      "grad_norm": 64.30630493164062,
      "learning_rate": 6.088097388910397e-06,
      "loss": 0.964,
      "step": 11677
    },
    {
      "epoch": 4.521099496709253,
      "grad_norm": 5.922402381896973,
      "learning_rate": 6.087667225878609e-06,
      "loss": 0.2971,
      "step": 11678
    },
    {
      "epoch": 4.521486643437863,
      "grad_norm": 29.899423599243164,
      "learning_rate": 6.087237062846819e-06,
      "loss": 0.5992,
      "step": 11679
    },
    {
      "epoch": 4.521873790166473,
      "grad_norm": 72.78986358642578,
      "learning_rate": 6.086806899815031e-06,
      "loss": 1.0237,
      "step": 11680
    },
    {
      "epoch": 4.5222609368950835,
      "grad_norm": 78.68892669677734,
      "learning_rate": 6.086376736783241e-06,
      "loss": 0.6842,
      "step": 11681
    },
    {
      "epoch": 4.522648083623693,
      "grad_norm": 10.656091690063477,
      "learning_rate": 6.085946573751452e-06,
      "loss": 0.5014,
      "step": 11682
    },
    {
      "epoch": 4.523035230352304,
      "grad_norm": 17.070104598999023,
      "learning_rate": 6.0855164107196625e-06,
      "loss": 0.7032,
      "step": 11683
    },
    {
      "epoch": 4.523422377080914,
      "grad_norm": 17.21055793762207,
      "learning_rate": 6.085086247687875e-06,
      "loss": 1.5582,
      "step": 11684
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 24.29117202758789,
      "learning_rate": 6.084656084656085e-06,
      "loss": 2.7906,
      "step": 11685
    },
    {
      "epoch": 4.524196670538134,
      "grad_norm": 43.23691177368164,
      "learning_rate": 6.084225921624296e-06,
      "loss": 2.2327,
      "step": 11686
    },
    {
      "epoch": 4.524583817266744,
      "grad_norm": 17.977876663208008,
      "learning_rate": 6.0837957585925065e-06,
      "loss": 0.4585,
      "step": 11687
    },
    {
      "epoch": 4.524970963995354,
      "grad_norm": 9.59631633758545,
      "learning_rate": 6.083365595560719e-06,
      "loss": 0.3286,
      "step": 11688
    },
    {
      "epoch": 4.525358110723964,
      "grad_norm": 29.621742248535156,
      "learning_rate": 6.082935432528929e-06,
      "loss": 1.553,
      "step": 11689
    },
    {
      "epoch": 4.5257452574525745,
      "grad_norm": 59.71752166748047,
      "learning_rate": 6.08250526949714e-06,
      "loss": 1.1519,
      "step": 11690
    },
    {
      "epoch": 4.526132404181185,
      "grad_norm": 22.771995544433594,
      "learning_rate": 6.0820751064653505e-06,
      "loss": 0.676,
      "step": 11691
    },
    {
      "epoch": 4.526519550909795,
      "grad_norm": 7.481228351593018,
      "learning_rate": 6.081644943433562e-06,
      "loss": 0.3219,
      "step": 11692
    },
    {
      "epoch": 4.526906697638405,
      "grad_norm": 17.72159767150879,
      "learning_rate": 6.081214780401773e-06,
      "loss": 0.6192,
      "step": 11693
    },
    {
      "epoch": 4.527293844367015,
      "grad_norm": 21.26029396057129,
      "learning_rate": 6.080784617369984e-06,
      "loss": 1.4513,
      "step": 11694
    },
    {
      "epoch": 4.527680991095625,
      "grad_norm": 59.09272766113281,
      "learning_rate": 6.0803544543381944e-06,
      "loss": 1.8853,
      "step": 11695
    },
    {
      "epoch": 4.528068137824235,
      "grad_norm": 18.16705322265625,
      "learning_rate": 6.079924291306406e-06,
      "loss": 1.0435,
      "step": 11696
    },
    {
      "epoch": 4.528455284552845,
      "grad_norm": 11.778770446777344,
      "learning_rate": 6.079494128274616e-06,
      "loss": 0.7304,
      "step": 11697
    },
    {
      "epoch": 4.528842431281456,
      "grad_norm": 27.110475540161133,
      "learning_rate": 6.079063965242827e-06,
      "loss": 0.7839,
      "step": 11698
    },
    {
      "epoch": 4.5292295780100655,
      "grad_norm": 11.422314643859863,
      "learning_rate": 6.078633802211038e-06,
      "loss": 1.1813,
      "step": 11699
    },
    {
      "epoch": 4.529616724738676,
      "grad_norm": 20.896055221557617,
      "learning_rate": 6.07820363917925e-06,
      "loss": 1.3167,
      "step": 11700
    },
    {
      "epoch": 4.530003871467287,
      "grad_norm": 33.58587646484375,
      "learning_rate": 6.07777347614746e-06,
      "loss": 0.7726,
      "step": 11701
    },
    {
      "epoch": 4.530391018195896,
      "grad_norm": 103.41041564941406,
      "learning_rate": 6.077343313115671e-06,
      "loss": 0.8789,
      "step": 11702
    },
    {
      "epoch": 4.530778164924507,
      "grad_norm": 30.729230880737305,
      "learning_rate": 6.0769131500838815e-06,
      "loss": 1.7894,
      "step": 11703
    },
    {
      "epoch": 4.531165311653116,
      "grad_norm": 110.03583526611328,
      "learning_rate": 6.076482987052094e-06,
      "loss": 2.5373,
      "step": 11704
    },
    {
      "epoch": 4.531552458381727,
      "grad_norm": 16.923200607299805,
      "learning_rate": 6.076052824020304e-06,
      "loss": 1.2231,
      "step": 11705
    },
    {
      "epoch": 4.5319396051103364,
      "grad_norm": 29.472684860229492,
      "learning_rate": 6.075622660988515e-06,
      "loss": 0.5909,
      "step": 11706
    },
    {
      "epoch": 4.532326751838947,
      "grad_norm": 29.30854034423828,
      "learning_rate": 6.0751924979567255e-06,
      "loss": 1.2846,
      "step": 11707
    },
    {
      "epoch": 4.5327138985675575,
      "grad_norm": 17.886423110961914,
      "learning_rate": 6.074762334924938e-06,
      "loss": 1.3393,
      "step": 11708
    },
    {
      "epoch": 4.533101045296167,
      "grad_norm": 15.640024185180664,
      "learning_rate": 6.074332171893148e-06,
      "loss": 0.2642,
      "step": 11709
    },
    {
      "epoch": 4.533488192024778,
      "grad_norm": 6.602471828460693,
      "learning_rate": 6.073902008861359e-06,
      "loss": 0.3122,
      "step": 11710
    },
    {
      "epoch": 4.533875338753387,
      "grad_norm": 59.19755554199219,
      "learning_rate": 6.0734718458295695e-06,
      "loss": 0.7136,
      "step": 11711
    },
    {
      "epoch": 4.534262485481998,
      "grad_norm": 45.929107666015625,
      "learning_rate": 6.073041682797781e-06,
      "loss": 1.0226,
      "step": 11712
    },
    {
      "epoch": 4.534649632210607,
      "grad_norm": 9.415163040161133,
      "learning_rate": 6.0726115197659914e-06,
      "loss": 0.4724,
      "step": 11713
    },
    {
      "epoch": 4.535036778939218,
      "grad_norm": 9.977259635925293,
      "learning_rate": 6.072181356734203e-06,
      "loss": 0.9473,
      "step": 11714
    },
    {
      "epoch": 4.535423925667828,
      "grad_norm": 39.53621292114258,
      "learning_rate": 6.0717511937024134e-06,
      "loss": 1.2232,
      "step": 11715
    },
    {
      "epoch": 4.535811072396438,
      "grad_norm": 20.600418090820312,
      "learning_rate": 6.071321030670625e-06,
      "loss": 1.9604,
      "step": 11716
    },
    {
      "epoch": 4.5361982191250485,
      "grad_norm": 27.2952880859375,
      "learning_rate": 6.070890867638835e-06,
      "loss": 1.4292,
      "step": 11717
    },
    {
      "epoch": 4.536585365853659,
      "grad_norm": 14.630192756652832,
      "learning_rate": 6.070460704607046e-06,
      "loss": 0.3728,
      "step": 11718
    },
    {
      "epoch": 4.536972512582269,
      "grad_norm": 36.963104248046875,
      "learning_rate": 6.070030541575258e-06,
      "loss": 1.6309,
      "step": 11719
    },
    {
      "epoch": 4.537359659310879,
      "grad_norm": 11.171996116638184,
      "learning_rate": 6.069600378543469e-06,
      "loss": 0.4506,
      "step": 11720
    },
    {
      "epoch": 4.537746806039489,
      "grad_norm": 29.35108757019043,
      "learning_rate": 6.069170215511679e-06,
      "loss": 1.1792,
      "step": 11721
    },
    {
      "epoch": 4.538133952768099,
      "grad_norm": 58.25231170654297,
      "learning_rate": 6.06874005247989e-06,
      "loss": 1.037,
      "step": 11722
    },
    {
      "epoch": 4.538521099496709,
      "grad_norm": 106.95657348632812,
      "learning_rate": 6.068309889448102e-06,
      "loss": 3.4633,
      "step": 11723
    },
    {
      "epoch": 4.538908246225319,
      "grad_norm": 1.558543086051941,
      "learning_rate": 6.067879726416313e-06,
      "loss": 0.0475,
      "step": 11724
    },
    {
      "epoch": 4.53929539295393,
      "grad_norm": 23.366174697875977,
      "learning_rate": 6.067449563384523e-06,
      "loss": 2.1463,
      "step": 11725
    },
    {
      "epoch": 4.5396825396825395,
      "grad_norm": 23.400951385498047,
      "learning_rate": 6.067019400352734e-06,
      "loss": 0.93,
      "step": 11726
    },
    {
      "epoch": 4.54006968641115,
      "grad_norm": 35.757781982421875,
      "learning_rate": 6.066589237320945e-06,
      "loss": 0.6821,
      "step": 11727
    },
    {
      "epoch": 4.54045683313976,
      "grad_norm": 28.909992218017578,
      "learning_rate": 6.066159074289156e-06,
      "loss": 1.4466,
      "step": 11728
    },
    {
      "epoch": 4.54084397986837,
      "grad_norm": 42.858943939208984,
      "learning_rate": 6.065728911257367e-06,
      "loss": 1.749,
      "step": 11729
    },
    {
      "epoch": 4.54123112659698,
      "grad_norm": 27.294431686401367,
      "learning_rate": 6.065298748225578e-06,
      "loss": 1.3237,
      "step": 11730
    },
    {
      "epoch": 4.54161827332559,
      "grad_norm": 14.467010498046875,
      "learning_rate": 6.064868585193789e-06,
      "loss": 0.8778,
      "step": 11731
    },
    {
      "epoch": 4.542005420054201,
      "grad_norm": 44.98908233642578,
      "learning_rate": 6.064438422162e-06,
      "loss": 1.0711,
      "step": 11732
    },
    {
      "epoch": 4.5423925667828104,
      "grad_norm": 41.16021728515625,
      "learning_rate": 6.0640082591302104e-06,
      "loss": 1.7642,
      "step": 11733
    },
    {
      "epoch": 4.542779713511421,
      "grad_norm": 75.75726318359375,
      "learning_rate": 6.063578096098421e-06,
      "loss": 0.9183,
      "step": 11734
    },
    {
      "epoch": 4.5431668602400315,
      "grad_norm": 76.79142761230469,
      "learning_rate": 6.063147933066633e-06,
      "loss": 2.0256,
      "step": 11735
    },
    {
      "epoch": 4.543554006968641,
      "grad_norm": 5.202484130859375,
      "learning_rate": 6.062717770034844e-06,
      "loss": 0.3431,
      "step": 11736
    },
    {
      "epoch": 4.543941153697252,
      "grad_norm": 16.0026912689209,
      "learning_rate": 6.062287607003054e-06,
      "loss": 1.787,
      "step": 11737
    },
    {
      "epoch": 4.544328300425861,
      "grad_norm": 79.14901733398438,
      "learning_rate": 6.061857443971265e-06,
      "loss": 1.3451,
      "step": 11738
    },
    {
      "epoch": 4.544715447154472,
      "grad_norm": 93.93363952636719,
      "learning_rate": 6.061427280939477e-06,
      "loss": 0.7089,
      "step": 11739
    },
    {
      "epoch": 4.545102593883081,
      "grad_norm": 59.2564582824707,
      "learning_rate": 6.060997117907688e-06,
      "loss": 1.133,
      "step": 11740
    },
    {
      "epoch": 4.545489740611692,
      "grad_norm": 15.869497299194336,
      "learning_rate": 6.060566954875898e-06,
      "loss": 1.8669,
      "step": 11741
    },
    {
      "epoch": 4.545876887340302,
      "grad_norm": 10.434114456176758,
      "learning_rate": 6.060136791844109e-06,
      "loss": 1.1601,
      "step": 11742
    },
    {
      "epoch": 4.546264034068912,
      "grad_norm": 62.26800537109375,
      "learning_rate": 6.05970662881232e-06,
      "loss": 2.1216,
      "step": 11743
    },
    {
      "epoch": 4.5466511807975225,
      "grad_norm": 13.669217109680176,
      "learning_rate": 6.059276465780532e-06,
      "loss": 1.1169,
      "step": 11744
    },
    {
      "epoch": 4.547038327526132,
      "grad_norm": 30.778520584106445,
      "learning_rate": 6.058846302748742e-06,
      "loss": 1.6092,
      "step": 11745
    },
    {
      "epoch": 4.547425474254743,
      "grad_norm": 33.05579376220703,
      "learning_rate": 6.058416139716953e-06,
      "loss": 1.2856,
      "step": 11746
    },
    {
      "epoch": 4.547812620983352,
      "grad_norm": 58.553836822509766,
      "learning_rate": 6.057985976685164e-06,
      "loss": 2.446,
      "step": 11747
    },
    {
      "epoch": 4.548199767711963,
      "grad_norm": 8.99980640411377,
      "learning_rate": 6.057555813653375e-06,
      "loss": 0.4339,
      "step": 11748
    },
    {
      "epoch": 4.548586914440573,
      "grad_norm": 31.03619384765625,
      "learning_rate": 6.0571256506215855e-06,
      "loss": 1.1687,
      "step": 11749
    },
    {
      "epoch": 4.548974061169183,
      "grad_norm": 31.039100646972656,
      "learning_rate": 6.056695487589797e-06,
      "loss": 0.5507,
      "step": 11750
    },
    {
      "epoch": 4.549361207897793,
      "grad_norm": 6.72320556640625,
      "learning_rate": 6.056265324558008e-06,
      "loss": 0.3239,
      "step": 11751
    },
    {
      "epoch": 4.549748354626403,
      "grad_norm": 19.370380401611328,
      "learning_rate": 6.055835161526219e-06,
      "loss": 0.8867,
      "step": 11752
    },
    {
      "epoch": 4.5501355013550135,
      "grad_norm": 19.786598205566406,
      "learning_rate": 6.0554049984944294e-06,
      "loss": 0.8851,
      "step": 11753
    },
    {
      "epoch": 4.550522648083624,
      "grad_norm": 45.35076904296875,
      "learning_rate": 6.05497483546264e-06,
      "loss": 2.5382,
      "step": 11754
    },
    {
      "epoch": 4.550909794812234,
      "grad_norm": 55.7065315246582,
      "learning_rate": 6.054544672430852e-06,
      "loss": 1.8499,
      "step": 11755
    },
    {
      "epoch": 4.551296941540844,
      "grad_norm": 21.103267669677734,
      "learning_rate": 6.054114509399063e-06,
      "loss": 1.1077,
      "step": 11756
    },
    {
      "epoch": 4.551684088269454,
      "grad_norm": 46.26240158081055,
      "learning_rate": 6.053684346367273e-06,
      "loss": 2.5055,
      "step": 11757
    },
    {
      "epoch": 4.552071234998064,
      "grad_norm": 5.074800491333008,
      "learning_rate": 6.053254183335484e-06,
      "loss": 0.2567,
      "step": 11758
    },
    {
      "epoch": 4.552458381726675,
      "grad_norm": 52.03636932373047,
      "learning_rate": 6.052824020303696e-06,
      "loss": 2.1616,
      "step": 11759
    },
    {
      "epoch": 4.5528455284552845,
      "grad_norm": 19.18631362915039,
      "learning_rate": 6.052393857271907e-06,
      "loss": 1.2716,
      "step": 11760
    },
    {
      "epoch": 4.553232675183895,
      "grad_norm": 46.091064453125,
      "learning_rate": 6.051963694240117e-06,
      "loss": 0.6218,
      "step": 11761
    },
    {
      "epoch": 4.553619821912505,
      "grad_norm": 53.43000411987305,
      "learning_rate": 6.051533531208329e-06,
      "loss": 2.6237,
      "step": 11762
    },
    {
      "epoch": 4.554006968641115,
      "grad_norm": 54.016048431396484,
      "learning_rate": 6.051103368176539e-06,
      "loss": 2.0167,
      "step": 11763
    },
    {
      "epoch": 4.554394115369725,
      "grad_norm": 95.1241455078125,
      "learning_rate": 6.05067320514475e-06,
      "loss": 0.6139,
      "step": 11764
    },
    {
      "epoch": 4.554781262098335,
      "grad_norm": 33.41529846191406,
      "learning_rate": 6.050243042112961e-06,
      "loss": 1.3759,
      "step": 11765
    },
    {
      "epoch": 4.555168408826946,
      "grad_norm": 19.654624938964844,
      "learning_rate": 6.049812879081173e-06,
      "loss": 2.2638,
      "step": 11766
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 19.528444290161133,
      "learning_rate": 6.049382716049383e-06,
      "loss": 1.7078,
      "step": 11767
    },
    {
      "epoch": 4.555942702284166,
      "grad_norm": 1.2205239534378052,
      "learning_rate": 6.048952553017594e-06,
      "loss": 0.0381,
      "step": 11768
    },
    {
      "epoch": 4.5563298490127755,
      "grad_norm": 19.61189842224121,
      "learning_rate": 6.0485223899858045e-06,
      "loss": 1.7807,
      "step": 11769
    },
    {
      "epoch": 4.556716995741386,
      "grad_norm": 26.333637237548828,
      "learning_rate": 6.048092226954017e-06,
      "loss": 1.7315,
      "step": 11770
    },
    {
      "epoch": 4.5571041424699965,
      "grad_norm": 26.325157165527344,
      "learning_rate": 6.047662063922227e-06,
      "loss": 2.473,
      "step": 11771
    },
    {
      "epoch": 4.557491289198606,
      "grad_norm": 9.073711395263672,
      "learning_rate": 6.047231900890438e-06,
      "loss": 1.0188,
      "step": 11772
    },
    {
      "epoch": 4.557878435927217,
      "grad_norm": 5.881339073181152,
      "learning_rate": 6.0468017378586484e-06,
      "loss": 0.0724,
      "step": 11773
    },
    {
      "epoch": 4.558265582655826,
      "grad_norm": 54.70304489135742,
      "learning_rate": 6.046371574826861e-06,
      "loss": 0.8664,
      "step": 11774
    },
    {
      "epoch": 4.558652729384437,
      "grad_norm": 20.58522605895996,
      "learning_rate": 6.045941411795071e-06,
      "loss": 0.5624,
      "step": 11775
    },
    {
      "epoch": 4.559039876113047,
      "grad_norm": 48.23287582397461,
      "learning_rate": 6.045511248763282e-06,
      "loss": 1.0763,
      "step": 11776
    },
    {
      "epoch": 4.559427022841657,
      "grad_norm": 57.38492965698242,
      "learning_rate": 6.045081085731492e-06,
      "loss": 1.1542,
      "step": 11777
    },
    {
      "epoch": 4.559814169570267,
      "grad_norm": 13.895528793334961,
      "learning_rate": 6.044650922699704e-06,
      "loss": 0.6549,
      "step": 11778
    },
    {
      "epoch": 4.560201316298877,
      "grad_norm": 45.13263702392578,
      "learning_rate": 6.044220759667914e-06,
      "loss": 1.8526,
      "step": 11779
    },
    {
      "epoch": 4.5605884630274875,
      "grad_norm": 58.916748046875,
      "learning_rate": 6.043790596636126e-06,
      "loss": 0.7796,
      "step": 11780
    },
    {
      "epoch": 4.560975609756097,
      "grad_norm": 61.148582458496094,
      "learning_rate": 6.043360433604336e-06,
      "loss": 2.0519,
      "step": 11781
    },
    {
      "epoch": 4.561362756484708,
      "grad_norm": 8.920744895935059,
      "learning_rate": 6.042930270572548e-06,
      "loss": 0.5823,
      "step": 11782
    },
    {
      "epoch": 4.561749903213318,
      "grad_norm": 14.420215606689453,
      "learning_rate": 6.042500107540758e-06,
      "loss": 0.6443,
      "step": 11783
    },
    {
      "epoch": 4.562137049941928,
      "grad_norm": 69.13214111328125,
      "learning_rate": 6.042069944508969e-06,
      "loss": 2.1867,
      "step": 11784
    },
    {
      "epoch": 4.562524196670538,
      "grad_norm": 25.02790641784668,
      "learning_rate": 6.0416397814771795e-06,
      "loss": 1.2934,
      "step": 11785
    },
    {
      "epoch": 4.562911343399148,
      "grad_norm": 8.900357246398926,
      "learning_rate": 6.041209618445392e-06,
      "loss": 0.4532,
      "step": 11786
    },
    {
      "epoch": 4.5632984901277585,
      "grad_norm": 23.980342864990234,
      "learning_rate": 6.040779455413602e-06,
      "loss": 0.8225,
      "step": 11787
    },
    {
      "epoch": 4.563685636856368,
      "grad_norm": 60.06252670288086,
      "learning_rate": 6.040349292381813e-06,
      "loss": 3.5684,
      "step": 11788
    },
    {
      "epoch": 4.564072783584979,
      "grad_norm": 21.035287857055664,
      "learning_rate": 6.0399191293500235e-06,
      "loss": 1.5351,
      "step": 11789
    },
    {
      "epoch": 4.564459930313589,
      "grad_norm": 11.752182960510254,
      "learning_rate": 6.039488966318236e-06,
      "loss": 0.5706,
      "step": 11790
    },
    {
      "epoch": 4.564847077042199,
      "grad_norm": 114.6978988647461,
      "learning_rate": 6.039058803286446e-06,
      "loss": 1.3162,
      "step": 11791
    },
    {
      "epoch": 4.565234223770809,
      "grad_norm": 38.458961486816406,
      "learning_rate": 6.038628640254657e-06,
      "loss": 1.1032,
      "step": 11792
    },
    {
      "epoch": 4.56562137049942,
      "grad_norm": 21.143253326416016,
      "learning_rate": 6.0381984772228674e-06,
      "loss": 1.1809,
      "step": 11793
    },
    {
      "epoch": 4.566008517228029,
      "grad_norm": 62.06050491333008,
      "learning_rate": 6.037768314191079e-06,
      "loss": 0.2631,
      "step": 11794
    },
    {
      "epoch": 4.56639566395664,
      "grad_norm": 24.18217658996582,
      "learning_rate": 6.03733815115929e-06,
      "loss": 0.8904,
      "step": 11795
    },
    {
      "epoch": 4.5667828106852495,
      "grad_norm": 33.09786605834961,
      "learning_rate": 6.036907988127501e-06,
      "loss": 0.9376,
      "step": 11796
    },
    {
      "epoch": 4.56716995741386,
      "grad_norm": 9.303829193115234,
      "learning_rate": 6.036477825095711e-06,
      "loss": 0.2357,
      "step": 11797
    },
    {
      "epoch": 4.56755710414247,
      "grad_norm": 38.084625244140625,
      "learning_rate": 6.036047662063923e-06,
      "loss": 1.6514,
      "step": 11798
    },
    {
      "epoch": 4.56794425087108,
      "grad_norm": 13.646265983581543,
      "learning_rate": 6.035617499032133e-06,
      "loss": 0.2895,
      "step": 11799
    },
    {
      "epoch": 4.568331397599691,
      "grad_norm": 38.79840850830078,
      "learning_rate": 6.035187336000344e-06,
      "loss": 1.6022,
      "step": 11800
    },
    {
      "epoch": 4.5687185443283,
      "grad_norm": 13.478160858154297,
      "learning_rate": 6.034757172968556e-06,
      "loss": 0.5905,
      "step": 11801
    },
    {
      "epoch": 4.569105691056911,
      "grad_norm": 17.700342178344727,
      "learning_rate": 6.034327009936767e-06,
      "loss": 0.8807,
      "step": 11802
    },
    {
      "epoch": 4.56949283778552,
      "grad_norm": 39.526939392089844,
      "learning_rate": 6.033896846904977e-06,
      "loss": 0.7171,
      "step": 11803
    },
    {
      "epoch": 4.569879984514131,
      "grad_norm": 32.01771545410156,
      "learning_rate": 6.033466683873188e-06,
      "loss": 2.1567,
      "step": 11804
    },
    {
      "epoch": 4.5702671312427405,
      "grad_norm": 7.642843246459961,
      "learning_rate": 6.0330365208414e-06,
      "loss": 0.2165,
      "step": 11805
    },
    {
      "epoch": 4.570654277971351,
      "grad_norm": 26.347864151000977,
      "learning_rate": 6.032606357809611e-06,
      "loss": 0.9186,
      "step": 11806
    },
    {
      "epoch": 4.5710414246999616,
      "grad_norm": 45.38311004638672,
      "learning_rate": 6.032176194777821e-06,
      "loss": 1.0691,
      "step": 11807
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 11.322407722473145,
      "learning_rate": 6.031746031746032e-06,
      "loss": 0.5803,
      "step": 11808
    },
    {
      "epoch": 4.571815718157182,
      "grad_norm": 25.889997482299805,
      "learning_rate": 6.031315868714243e-06,
      "loss": 1.1916,
      "step": 11809
    },
    {
      "epoch": 4.572202864885792,
      "grad_norm": 20.105173110961914,
      "learning_rate": 6.030885705682455e-06,
      "loss": 0.8675,
      "step": 11810
    },
    {
      "epoch": 4.572590011614402,
      "grad_norm": 48.282901763916016,
      "learning_rate": 6.030455542650665e-06,
      "loss": 2.0872,
      "step": 11811
    },
    {
      "epoch": 4.572977158343012,
      "grad_norm": 6.762080192565918,
      "learning_rate": 6.030025379618876e-06,
      "loss": 0.4604,
      "step": 11812
    },
    {
      "epoch": 4.573364305071622,
      "grad_norm": 17.533021926879883,
      "learning_rate": 6.029595216587087e-06,
      "loss": 1.3985,
      "step": 11813
    },
    {
      "epoch": 4.5737514518002325,
      "grad_norm": 20.813997268676758,
      "learning_rate": 6.029165053555298e-06,
      "loss": 0.9635,
      "step": 11814
    },
    {
      "epoch": 4.574138598528842,
      "grad_norm": 40.8139533996582,
      "learning_rate": 6.028734890523508e-06,
      "loss": 2.9963,
      "step": 11815
    },
    {
      "epoch": 4.574525745257453,
      "grad_norm": 48.381996154785156,
      "learning_rate": 6.02830472749172e-06,
      "loss": 1.2942,
      "step": 11816
    },
    {
      "epoch": 4.574912891986063,
      "grad_norm": 8.568747520446777,
      "learning_rate": 6.027874564459931e-06,
      "loss": 0.6362,
      "step": 11817
    },
    {
      "epoch": 4.575300038714673,
      "grad_norm": 29.279808044433594,
      "learning_rate": 6.027444401428142e-06,
      "loss": 2.0547,
      "step": 11818
    },
    {
      "epoch": 4.575687185443283,
      "grad_norm": 13.657754898071289,
      "learning_rate": 6.027014238396352e-06,
      "loss": 0.4607,
      "step": 11819
    },
    {
      "epoch": 4.576074332171893,
      "grad_norm": 22.390426635742188,
      "learning_rate": 6.026584075364563e-06,
      "loss": 1.6554,
      "step": 11820
    },
    {
      "epoch": 4.576461478900503,
      "grad_norm": 23.190837860107422,
      "learning_rate": 6.026153912332775e-06,
      "loss": 0.5208,
      "step": 11821
    },
    {
      "epoch": 4.576848625629113,
      "grad_norm": 31.49864387512207,
      "learning_rate": 6.025723749300986e-06,
      "loss": 1.2109,
      "step": 11822
    },
    {
      "epoch": 4.5772357723577235,
      "grad_norm": 20.814468383789062,
      "learning_rate": 6.025293586269196e-06,
      "loss": 1.2683,
      "step": 11823
    },
    {
      "epoch": 4.577622919086334,
      "grad_norm": 63.05908966064453,
      "learning_rate": 6.024863423237407e-06,
      "loss": 1.7122,
      "step": 11824
    },
    {
      "epoch": 4.578010065814944,
      "grad_norm": 5.727126121520996,
      "learning_rate": 6.024433260205619e-06,
      "loss": 0.1907,
      "step": 11825
    },
    {
      "epoch": 4.578397212543554,
      "grad_norm": 26.217031478881836,
      "learning_rate": 6.02400309717383e-06,
      "loss": 1.1049,
      "step": 11826
    },
    {
      "epoch": 4.578784359272164,
      "grad_norm": 25.459003448486328,
      "learning_rate": 6.02357293414204e-06,
      "loss": 1.181,
      "step": 11827
    },
    {
      "epoch": 4.579171506000774,
      "grad_norm": 12.517508506774902,
      "learning_rate": 6.023142771110251e-06,
      "loss": 1.156,
      "step": 11828
    },
    {
      "epoch": 4.579558652729385,
      "grad_norm": 18.997661590576172,
      "learning_rate": 6.022712608078462e-06,
      "loss": 0.2389,
      "step": 11829
    },
    {
      "epoch": 4.579945799457994,
      "grad_norm": 29.15947151184082,
      "learning_rate": 6.022282445046673e-06,
      "loss": 1.9176,
      "step": 11830
    },
    {
      "epoch": 4.580332946186605,
      "grad_norm": 71.9890365600586,
      "learning_rate": 6.021852282014884e-06,
      "loss": 0.8671,
      "step": 11831
    },
    {
      "epoch": 4.5807200929152145,
      "grad_norm": 6.191417694091797,
      "learning_rate": 6.021422118983095e-06,
      "loss": 0.2367,
      "step": 11832
    },
    {
      "epoch": 4.581107239643825,
      "grad_norm": 15.41966438293457,
      "learning_rate": 6.020991955951306e-06,
      "loss": 1.68,
      "step": 11833
    },
    {
      "epoch": 4.581494386372436,
      "grad_norm": 10.901021003723145,
      "learning_rate": 6.020561792919517e-06,
      "loss": 0.4814,
      "step": 11834
    },
    {
      "epoch": 4.581881533101045,
      "grad_norm": 10.907671928405762,
      "learning_rate": 6.020131629887727e-06,
      "loss": 0.4731,
      "step": 11835
    },
    {
      "epoch": 4.582268679829656,
      "grad_norm": 22.817319869995117,
      "learning_rate": 6.019701466855938e-06,
      "loss": 1.4766,
      "step": 11836
    },
    {
      "epoch": 4.582655826558265,
      "grad_norm": 17.905576705932617,
      "learning_rate": 6.01927130382415e-06,
      "loss": 1.3619,
      "step": 11837
    },
    {
      "epoch": 4.583042973286876,
      "grad_norm": 12.055606842041016,
      "learning_rate": 6.018841140792361e-06,
      "loss": 0.7346,
      "step": 11838
    },
    {
      "epoch": 4.583430120015485,
      "grad_norm": 32.05799102783203,
      "learning_rate": 6.018410977760571e-06,
      "loss": 1.9493,
      "step": 11839
    },
    {
      "epoch": 4.583817266744096,
      "grad_norm": 41.70209503173828,
      "learning_rate": 6.017980814728782e-06,
      "loss": 2.0581,
      "step": 11840
    },
    {
      "epoch": 4.5842044134727065,
      "grad_norm": 19.015417098999023,
      "learning_rate": 6.017550651696994e-06,
      "loss": 0.603,
      "step": 11841
    },
    {
      "epoch": 4.584591560201316,
      "grad_norm": 26.90122413635254,
      "learning_rate": 6.017120488665205e-06,
      "loss": 1.2997,
      "step": 11842
    },
    {
      "epoch": 4.584978706929927,
      "grad_norm": 25.46586036682129,
      "learning_rate": 6.016690325633415e-06,
      "loss": 1.647,
      "step": 11843
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 23.653940200805664,
      "learning_rate": 6.016260162601627e-06,
      "loss": 1.03,
      "step": 11844
    },
    {
      "epoch": 4.585753000387147,
      "grad_norm": 62.821746826171875,
      "learning_rate": 6.015829999569837e-06,
      "loss": 0.474,
      "step": 11845
    },
    {
      "epoch": 4.586140147115757,
      "grad_norm": 7.827042579650879,
      "learning_rate": 6.015399836538049e-06,
      "loss": 0.4056,
      "step": 11846
    },
    {
      "epoch": 4.586527293844367,
      "grad_norm": 59.69792938232422,
      "learning_rate": 6.014969673506259e-06,
      "loss": 0.7412,
      "step": 11847
    },
    {
      "epoch": 4.586914440572977,
      "grad_norm": 27.678470611572266,
      "learning_rate": 6.014539510474471e-06,
      "loss": 1.3112,
      "step": 11848
    },
    {
      "epoch": 4.587301587301587,
      "grad_norm": 13.971240043640137,
      "learning_rate": 6.014109347442681e-06,
      "loss": 0.2056,
      "step": 11849
    },
    {
      "epoch": 4.5876887340301975,
      "grad_norm": 11.85319709777832,
      "learning_rate": 6.013679184410892e-06,
      "loss": 0.805,
      "step": 11850
    },
    {
      "epoch": 4.588075880758808,
      "grad_norm": 63.739295959472656,
      "learning_rate": 6.0132490213791024e-06,
      "loss": 0.9796,
      "step": 11851
    },
    {
      "epoch": 4.588463027487418,
      "grad_norm": 18.56175994873047,
      "learning_rate": 6.012818858347315e-06,
      "loss": 1.4357,
      "step": 11852
    },
    {
      "epoch": 4.588850174216028,
      "grad_norm": 91.05381774902344,
      "learning_rate": 6.012388695315525e-06,
      "loss": 1.3999,
      "step": 11853
    },
    {
      "epoch": 4.589237320944638,
      "grad_norm": 27.560474395751953,
      "learning_rate": 6.011958532283736e-06,
      "loss": 1.1397,
      "step": 11854
    },
    {
      "epoch": 4.589624467673248,
      "grad_norm": 20.483333587646484,
      "learning_rate": 6.011528369251946e-06,
      "loss": 1.259,
      "step": 11855
    },
    {
      "epoch": 4.590011614401858,
      "grad_norm": 77.91255950927734,
      "learning_rate": 6.011098206220159e-06,
      "loss": 1.3784,
      "step": 11856
    },
    {
      "epoch": 4.590398761130468,
      "grad_norm": 13.887178421020508,
      "learning_rate": 6.010668043188369e-06,
      "loss": 0.5082,
      "step": 11857
    },
    {
      "epoch": 4.590785907859079,
      "grad_norm": 30.141218185424805,
      "learning_rate": 6.01023788015658e-06,
      "loss": 1.3739,
      "step": 11858
    },
    {
      "epoch": 4.5911730545876885,
      "grad_norm": 12.011726379394531,
      "learning_rate": 6.00980771712479e-06,
      "loss": 0.585,
      "step": 11859
    },
    {
      "epoch": 4.591560201316299,
      "grad_norm": 24.2122745513916,
      "learning_rate": 6.009377554093002e-06,
      "loss": 1.7736,
      "step": 11860
    },
    {
      "epoch": 4.591947348044909,
      "grad_norm": 18.329517364501953,
      "learning_rate": 6.008947391061213e-06,
      "loss": 1.4656,
      "step": 11861
    },
    {
      "epoch": 4.592334494773519,
      "grad_norm": 33.37223434448242,
      "learning_rate": 6.008517228029424e-06,
      "loss": 1.7292,
      "step": 11862
    },
    {
      "epoch": 4.59272164150213,
      "grad_norm": 36.443138122558594,
      "learning_rate": 6.008087064997634e-06,
      "loss": 1.3516,
      "step": 11863
    },
    {
      "epoch": 4.593108788230739,
      "grad_norm": 24.62886619567871,
      "learning_rate": 6.007656901965846e-06,
      "loss": 1.0892,
      "step": 11864
    },
    {
      "epoch": 4.59349593495935,
      "grad_norm": 28.74647331237793,
      "learning_rate": 6.007226738934056e-06,
      "loss": 1.6813,
      "step": 11865
    },
    {
      "epoch": 4.5938830816879594,
      "grad_norm": 1.5128099918365479,
      "learning_rate": 6.006796575902267e-06,
      "loss": 0.0415,
      "step": 11866
    },
    {
      "epoch": 4.59427022841657,
      "grad_norm": 29.52098274230957,
      "learning_rate": 6.006366412870478e-06,
      "loss": 0.5808,
      "step": 11867
    },
    {
      "epoch": 4.5946573751451805,
      "grad_norm": 21.829750061035156,
      "learning_rate": 6.00593624983869e-06,
      "loss": 0.8685,
      "step": 11868
    },
    {
      "epoch": 4.59504452187379,
      "grad_norm": 20.53628921508789,
      "learning_rate": 6.0055060868069e-06,
      "loss": 1.2223,
      "step": 11869
    },
    {
      "epoch": 4.595431668602401,
      "grad_norm": 1.2398178577423096,
      "learning_rate": 6.005075923775111e-06,
      "loss": 0.0348,
      "step": 11870
    },
    {
      "epoch": 4.59581881533101,
      "grad_norm": 34.355228424072266,
      "learning_rate": 6.0046457607433214e-06,
      "loss": 1.0654,
      "step": 11871
    },
    {
      "epoch": 4.596205962059621,
      "grad_norm": 54.94117736816406,
      "learning_rate": 6.004215597711534e-06,
      "loss": 2.4663,
      "step": 11872
    },
    {
      "epoch": 4.59659310878823,
      "grad_norm": 25.998939514160156,
      "learning_rate": 6.003785434679744e-06,
      "loss": 1.0575,
      "step": 11873
    },
    {
      "epoch": 4.596980255516841,
      "grad_norm": 60.790164947509766,
      "learning_rate": 6.003355271647955e-06,
      "loss": 2.5097,
      "step": 11874
    },
    {
      "epoch": 4.597367402245451,
      "grad_norm": 24.3287410736084,
      "learning_rate": 6.002925108616165e-06,
      "loss": 1.0322,
      "step": 11875
    },
    {
      "epoch": 4.597754548974061,
      "grad_norm": 31.606645584106445,
      "learning_rate": 6.002494945584378e-06,
      "loss": 1.3447,
      "step": 11876
    },
    {
      "epoch": 4.5981416957026715,
      "grad_norm": 20.097997665405273,
      "learning_rate": 6.002064782552588e-06,
      "loss": 1.1821,
      "step": 11877
    },
    {
      "epoch": 4.598528842431281,
      "grad_norm": 20.8033504486084,
      "learning_rate": 6.001634619520799e-06,
      "loss": 1.5733,
      "step": 11878
    },
    {
      "epoch": 4.598915989159892,
      "grad_norm": 61.84596252441406,
      "learning_rate": 6.001204456489009e-06,
      "loss": 2.061,
      "step": 11879
    },
    {
      "epoch": 4.599303135888501,
      "grad_norm": 29.27332878112793,
      "learning_rate": 6.000774293457221e-06,
      "loss": 0.8923,
      "step": 11880
    },
    {
      "epoch": 4.599690282617112,
      "grad_norm": 5.404479503631592,
      "learning_rate": 6.000344130425431e-06,
      "loss": 0.1712,
      "step": 11881
    },
    {
      "epoch": 4.600077429345722,
      "grad_norm": 81.56640625,
      "learning_rate": 5.999913967393643e-06,
      "loss": 0.8812,
      "step": 11882
    },
    {
      "epoch": 4.600464576074332,
      "grad_norm": 5.909420490264893,
      "learning_rate": 5.999483804361854e-06,
      "loss": 0.1543,
      "step": 11883
    },
    {
      "epoch": 4.600851722802942,
      "grad_norm": 32.179908752441406,
      "learning_rate": 5.999053641330065e-06,
      "loss": 0.9099,
      "step": 11884
    },
    {
      "epoch": 4.601238869531553,
      "grad_norm": 64.92822265625,
      "learning_rate": 5.998623478298275e-06,
      "loss": 1.3088,
      "step": 11885
    },
    {
      "epoch": 4.6016260162601625,
      "grad_norm": 17.87748908996582,
      "learning_rate": 5.998193315266486e-06,
      "loss": 0.7118,
      "step": 11886
    },
    {
      "epoch": 4.602013162988773,
      "grad_norm": 20.776321411132812,
      "learning_rate": 5.997763152234698e-06,
      "loss": 0.8751,
      "step": 11887
    },
    {
      "epoch": 4.602400309717383,
      "grad_norm": 5.266936779022217,
      "learning_rate": 5.997332989202909e-06,
      "loss": 0.2485,
      "step": 11888
    },
    {
      "epoch": 4.602787456445993,
      "grad_norm": 4.857852935791016,
      "learning_rate": 5.996902826171119e-06,
      "loss": 0.259,
      "step": 11889
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 64.59149169921875,
      "learning_rate": 5.99647266313933e-06,
      "loss": 1.1421,
      "step": 11890
    },
    {
      "epoch": 4.603561749903213,
      "grad_norm": 24.264095306396484,
      "learning_rate": 5.996042500107542e-06,
      "loss": 2.0049,
      "step": 11891
    },
    {
      "epoch": 4.603948896631824,
      "grad_norm": 58.93436813354492,
      "learning_rate": 5.995612337075753e-06,
      "loss": 2.0468,
      "step": 11892
    },
    {
      "epoch": 4.6043360433604335,
      "grad_norm": 17.940784454345703,
      "learning_rate": 5.995182174043963e-06,
      "loss": 1.0958,
      "step": 11893
    },
    {
      "epoch": 4.604723190089044,
      "grad_norm": 34.50220489501953,
      "learning_rate": 5.994752011012174e-06,
      "loss": 0.9598,
      "step": 11894
    },
    {
      "epoch": 4.605110336817654,
      "grad_norm": 53.70930480957031,
      "learning_rate": 5.994321847980385e-06,
      "loss": 1.6412,
      "step": 11895
    },
    {
      "epoch": 4.605497483546264,
      "grad_norm": 24.910934448242188,
      "learning_rate": 5.993891684948596e-06,
      "loss": 1.0676,
      "step": 11896
    },
    {
      "epoch": 4.605884630274874,
      "grad_norm": 54.27892303466797,
      "learning_rate": 5.993461521916807e-06,
      "loss": 3.094,
      "step": 11897
    },
    {
      "epoch": 4.606271777003484,
      "grad_norm": 5.490017414093018,
      "learning_rate": 5.993031358885018e-06,
      "loss": 0.2825,
      "step": 11898
    },
    {
      "epoch": 4.606658923732095,
      "grad_norm": 38.29547119140625,
      "learning_rate": 5.992601195853229e-06,
      "loss": 1.0122,
      "step": 11899
    },
    {
      "epoch": 4.607046070460704,
      "grad_norm": 13.846495628356934,
      "learning_rate": 5.99217103282144e-06,
      "loss": 0.5169,
      "step": 11900
    },
    {
      "epoch": 4.607433217189315,
      "grad_norm": 42.44799041748047,
      "learning_rate": 5.99174086978965e-06,
      "loss": 1.8957,
      "step": 11901
    },
    {
      "epoch": 4.607820363917925,
      "grad_norm": 29.245851516723633,
      "learning_rate": 5.991310706757861e-06,
      "loss": 1.0438,
      "step": 11902
    },
    {
      "epoch": 4.608207510646535,
      "grad_norm": 13.919960021972656,
      "learning_rate": 5.990880543726073e-06,
      "loss": 1.0281,
      "step": 11903
    },
    {
      "epoch": 4.6085946573751455,
      "grad_norm": 11.317358016967773,
      "learning_rate": 5.990450380694284e-06,
      "loss": 0.6305,
      "step": 11904
    },
    {
      "epoch": 4.608981804103755,
      "grad_norm": 7.961062908172607,
      "learning_rate": 5.990020217662494e-06,
      "loss": 0.502,
      "step": 11905
    },
    {
      "epoch": 4.609368950832366,
      "grad_norm": 29.583471298217773,
      "learning_rate": 5.989590054630705e-06,
      "loss": 0.9911,
      "step": 11906
    },
    {
      "epoch": 4.609756097560975,
      "grad_norm": 26.281780242919922,
      "learning_rate": 5.989159891598917e-06,
      "loss": 1.2522,
      "step": 11907
    },
    {
      "epoch": 4.610143244289586,
      "grad_norm": 27.30879020690918,
      "learning_rate": 5.988729728567128e-06,
      "loss": 1.935,
      "step": 11908
    },
    {
      "epoch": 4.610530391018196,
      "grad_norm": 44.70631790161133,
      "learning_rate": 5.988299565535338e-06,
      "loss": 1.9077,
      "step": 11909
    },
    {
      "epoch": 4.610917537746806,
      "grad_norm": 20.296653747558594,
      "learning_rate": 5.987869402503549e-06,
      "loss": 0.8789,
      "step": 11910
    },
    {
      "epoch": 4.611304684475416,
      "grad_norm": 62.70525360107422,
      "learning_rate": 5.98743923947176e-06,
      "loss": 2.1435,
      "step": 11911
    },
    {
      "epoch": 4.611691831204026,
      "grad_norm": 72.03005981445312,
      "learning_rate": 5.987009076439972e-06,
      "loss": 1.1306,
      "step": 11912
    },
    {
      "epoch": 4.6120789779326365,
      "grad_norm": 65.20599365234375,
      "learning_rate": 5.986578913408182e-06,
      "loss": 1.3096,
      "step": 11913
    },
    {
      "epoch": 4.612466124661246,
      "grad_norm": 32.117393493652344,
      "learning_rate": 5.986148750376393e-06,
      "loss": 0.9875,
      "step": 11914
    },
    {
      "epoch": 4.612853271389857,
      "grad_norm": 41.67057800292969,
      "learning_rate": 5.985718587344604e-06,
      "loss": 2.0041,
      "step": 11915
    },
    {
      "epoch": 4.613240418118467,
      "grad_norm": 34.56648254394531,
      "learning_rate": 5.985288424312815e-06,
      "loss": 2.2613,
      "step": 11916
    },
    {
      "epoch": 4.613627564847077,
      "grad_norm": 29.790103912353516,
      "learning_rate": 5.984858261281025e-06,
      "loss": 1.7175,
      "step": 11917
    },
    {
      "epoch": 4.614014711575687,
      "grad_norm": 14.970290184020996,
      "learning_rate": 5.984428098249237e-06,
      "loss": 0.398,
      "step": 11918
    },
    {
      "epoch": 4.614401858304297,
      "grad_norm": 23.102197647094727,
      "learning_rate": 5.983997935217448e-06,
      "loss": 1.9511,
      "step": 11919
    },
    {
      "epoch": 4.6147890050329075,
      "grad_norm": 19.24897003173828,
      "learning_rate": 5.983567772185659e-06,
      "loss": 0.6423,
      "step": 11920
    },
    {
      "epoch": 4.615176151761518,
      "grad_norm": 23.471338272094727,
      "learning_rate": 5.983137609153869e-06,
      "loss": 1.5663,
      "step": 11921
    },
    {
      "epoch": 4.615563298490128,
      "grad_norm": 30.5810489654541,
      "learning_rate": 5.98270744612208e-06,
      "loss": 1.0617,
      "step": 11922
    },
    {
      "epoch": 4.615950445218738,
      "grad_norm": 18.759990692138672,
      "learning_rate": 5.982277283090292e-06,
      "loss": 0.5329,
      "step": 11923
    },
    {
      "epoch": 4.616337591947348,
      "grad_norm": 20.975923538208008,
      "learning_rate": 5.981847120058503e-06,
      "loss": 0.9641,
      "step": 11924
    },
    {
      "epoch": 4.616724738675958,
      "grad_norm": 24.224695205688477,
      "learning_rate": 5.981416957026713e-06,
      "loss": 1.1231,
      "step": 11925
    },
    {
      "epoch": 4.617111885404569,
      "grad_norm": 46.32662582397461,
      "learning_rate": 5.980986793994925e-06,
      "loss": 1.3862,
      "step": 11926
    },
    {
      "epoch": 4.617499032133178,
      "grad_norm": 11.105149269104004,
      "learning_rate": 5.980556630963136e-06,
      "loss": 1.3021,
      "step": 11927
    },
    {
      "epoch": 4.617886178861789,
      "grad_norm": 15.135208129882812,
      "learning_rate": 5.980126467931347e-06,
      "loss": 0.5375,
      "step": 11928
    },
    {
      "epoch": 4.6182733255903985,
      "grad_norm": 29.028751373291016,
      "learning_rate": 5.979696304899557e-06,
      "loss": 0.8025,
      "step": 11929
    },
    {
      "epoch": 4.618660472319009,
      "grad_norm": 9.096297264099121,
      "learning_rate": 5.979266141867769e-06,
      "loss": 0.5478,
      "step": 11930
    },
    {
      "epoch": 4.619047619047619,
      "grad_norm": 50.7407341003418,
      "learning_rate": 5.978835978835979e-06,
      "loss": 1.559,
      "step": 11931
    },
    {
      "epoch": 4.619434765776229,
      "grad_norm": 25.393644332885742,
      "learning_rate": 5.97840581580419e-06,
      "loss": 1.2981,
      "step": 11932
    },
    {
      "epoch": 4.61982191250484,
      "grad_norm": 83.66421508789062,
      "learning_rate": 5.977975652772401e-06,
      "loss": 0.882,
      "step": 11933
    },
    {
      "epoch": 4.620209059233449,
      "grad_norm": 6.346232891082764,
      "learning_rate": 5.977545489740613e-06,
      "loss": 0.2045,
      "step": 11934
    },
    {
      "epoch": 4.62059620596206,
      "grad_norm": 11.971793174743652,
      "learning_rate": 5.977115326708823e-06,
      "loss": 0.3624,
      "step": 11935
    },
    {
      "epoch": 4.620983352690669,
      "grad_norm": 54.35997772216797,
      "learning_rate": 5.976685163677034e-06,
      "loss": 0.7693,
      "step": 11936
    },
    {
      "epoch": 4.62137049941928,
      "grad_norm": 180.85963439941406,
      "learning_rate": 5.976255000645244e-06,
      "loss": 1.1128,
      "step": 11937
    },
    {
      "epoch": 4.62175764614789,
      "grad_norm": 25.47309112548828,
      "learning_rate": 5.975824837613457e-06,
      "loss": 2.5039,
      "step": 11938
    },
    {
      "epoch": 4.6221447928765,
      "grad_norm": 8.766772270202637,
      "learning_rate": 5.975394674581667e-06,
      "loss": 0.927,
      "step": 11939
    },
    {
      "epoch": 4.6225319396051106,
      "grad_norm": 17.64548683166504,
      "learning_rate": 5.974964511549878e-06,
      "loss": 0.3184,
      "step": 11940
    },
    {
      "epoch": 4.62291908633372,
      "grad_norm": 32.48246765136719,
      "learning_rate": 5.974534348518088e-06,
      "loss": 0.7591,
      "step": 11941
    },
    {
      "epoch": 4.623306233062331,
      "grad_norm": 42.52973556518555,
      "learning_rate": 5.974104185486301e-06,
      "loss": 1.2641,
      "step": 11942
    },
    {
      "epoch": 4.623693379790941,
      "grad_norm": 12.747697830200195,
      "learning_rate": 5.973674022454511e-06,
      "loss": 0.9257,
      "step": 11943
    },
    {
      "epoch": 4.624080526519551,
      "grad_norm": 7.13575553894043,
      "learning_rate": 5.973243859422722e-06,
      "loss": 0.2511,
      "step": 11944
    },
    {
      "epoch": 4.624467673248161,
      "grad_norm": 9.567395210266113,
      "learning_rate": 5.972813696390932e-06,
      "loss": 0.5777,
      "step": 11945
    },
    {
      "epoch": 4.624854819976771,
      "grad_norm": 23.993240356445312,
      "learning_rate": 5.972383533359144e-06,
      "loss": 1.8305,
      "step": 11946
    },
    {
      "epoch": 4.6252419667053815,
      "grad_norm": 10.001296997070312,
      "learning_rate": 5.971953370327354e-06,
      "loss": 0.6197,
      "step": 11947
    },
    {
      "epoch": 4.625629113433991,
      "grad_norm": 82.24492645263672,
      "learning_rate": 5.971523207295566e-06,
      "loss": 1.286,
      "step": 11948
    },
    {
      "epoch": 4.626016260162602,
      "grad_norm": 10.095888137817383,
      "learning_rate": 5.971093044263776e-06,
      "loss": 0.2662,
      "step": 11949
    },
    {
      "epoch": 4.626403406891212,
      "grad_norm": 8.504651069641113,
      "learning_rate": 5.970662881231988e-06,
      "loss": 0.5775,
      "step": 11950
    },
    {
      "epoch": 4.626790553619822,
      "grad_norm": 9.53763198852539,
      "learning_rate": 5.970232718200198e-06,
      "loss": 0.4263,
      "step": 11951
    },
    {
      "epoch": 4.627177700348432,
      "grad_norm": 75.42012786865234,
      "learning_rate": 5.969802555168409e-06,
      "loss": 1.4502,
      "step": 11952
    },
    {
      "epoch": 4.627564847077042,
      "grad_norm": 39.527801513671875,
      "learning_rate": 5.969372392136619e-06,
      "loss": 1.2039,
      "step": 11953
    },
    {
      "epoch": 4.627951993805652,
      "grad_norm": 30.58831024169922,
      "learning_rate": 5.968942229104832e-06,
      "loss": 0.7043,
      "step": 11954
    },
    {
      "epoch": 4.628339140534262,
      "grad_norm": 20.803373336791992,
      "learning_rate": 5.968512066073042e-06,
      "loss": 1.8247,
      "step": 11955
    },
    {
      "epoch": 4.6287262872628725,
      "grad_norm": 17.112337112426758,
      "learning_rate": 5.968081903041253e-06,
      "loss": 0.6091,
      "step": 11956
    },
    {
      "epoch": 4.629113433991483,
      "grad_norm": 73.5808334350586,
      "learning_rate": 5.967651740009463e-06,
      "loss": 0.2031,
      "step": 11957
    },
    {
      "epoch": 4.629500580720093,
      "grad_norm": 40.08883285522461,
      "learning_rate": 5.967221576977676e-06,
      "loss": 0.9821,
      "step": 11958
    },
    {
      "epoch": 4.629887727448703,
      "grad_norm": 6.558535099029541,
      "learning_rate": 5.966791413945886e-06,
      "loss": 0.3858,
      "step": 11959
    },
    {
      "epoch": 4.630274874177314,
      "grad_norm": 8.941535949707031,
      "learning_rate": 5.966361250914097e-06,
      "loss": 0.1965,
      "step": 11960
    },
    {
      "epoch": 4.630662020905923,
      "grad_norm": 31.302583694458008,
      "learning_rate": 5.965931087882307e-06,
      "loss": 1.7258,
      "step": 11961
    },
    {
      "epoch": 4.631049167634534,
      "grad_norm": 67.7548828125,
      "learning_rate": 5.965500924850519e-06,
      "loss": 1.8455,
      "step": 11962
    },
    {
      "epoch": 4.631436314363143,
      "grad_norm": 34.005428314208984,
      "learning_rate": 5.96507076181873e-06,
      "loss": 0.8153,
      "step": 11963
    },
    {
      "epoch": 4.631823461091754,
      "grad_norm": 25.386323928833008,
      "learning_rate": 5.964640598786941e-06,
      "loss": 3.8438,
      "step": 11964
    },
    {
      "epoch": 4.6322106078203635,
      "grad_norm": 31.580150604248047,
      "learning_rate": 5.964210435755152e-06,
      "loss": 0.9229,
      "step": 11965
    },
    {
      "epoch": 4.632597754548974,
      "grad_norm": 23.245824813842773,
      "learning_rate": 5.963780272723363e-06,
      "loss": 1.9639,
      "step": 11966
    },
    {
      "epoch": 4.6329849012775846,
      "grad_norm": 98.7281723022461,
      "learning_rate": 5.963350109691573e-06,
      "loss": 2.0884,
      "step": 11967
    },
    {
      "epoch": 4.633372048006194,
      "grad_norm": 15.76762866973877,
      "learning_rate": 5.962919946659784e-06,
      "loss": 1.2369,
      "step": 11968
    },
    {
      "epoch": 4.633759194734805,
      "grad_norm": 47.29539489746094,
      "learning_rate": 5.962489783627996e-06,
      "loss": 1.8208,
      "step": 11969
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 28.106046676635742,
      "learning_rate": 5.962059620596207e-06,
      "loss": 0.4343,
      "step": 11970
    },
    {
      "epoch": 4.634533488192025,
      "grad_norm": 48.4151725769043,
      "learning_rate": 5.961629457564417e-06,
      "loss": 0.5625,
      "step": 11971
    },
    {
      "epoch": 4.634920634920634,
      "grad_norm": 56.283973693847656,
      "learning_rate": 5.961199294532628e-06,
      "loss": 1.0636,
      "step": 11972
    },
    {
      "epoch": 4.635307781649245,
      "grad_norm": 25.62149429321289,
      "learning_rate": 5.96076913150084e-06,
      "loss": 4.1877,
      "step": 11973
    },
    {
      "epoch": 4.6356949283778555,
      "grad_norm": 129.97128295898438,
      "learning_rate": 5.960338968469051e-06,
      "loss": 3.2182,
      "step": 11974
    },
    {
      "epoch": 4.636082075106465,
      "grad_norm": 25.898313522338867,
      "learning_rate": 5.959908805437261e-06,
      "loss": 1.3929,
      "step": 11975
    },
    {
      "epoch": 4.636469221835076,
      "grad_norm": 35.23798751831055,
      "learning_rate": 5.959478642405472e-06,
      "loss": 3.0376,
      "step": 11976
    },
    {
      "epoch": 4.636856368563686,
      "grad_norm": 66.83314514160156,
      "learning_rate": 5.959048479373683e-06,
      "loss": 2.9382,
      "step": 11977
    },
    {
      "epoch": 4.637243515292296,
      "grad_norm": 24.82806396484375,
      "learning_rate": 5.958618316341895e-06,
      "loss": 0.8227,
      "step": 11978
    },
    {
      "epoch": 4.637630662020906,
      "grad_norm": 65.20950317382812,
      "learning_rate": 5.958188153310105e-06,
      "loss": 1.4825,
      "step": 11979
    },
    {
      "epoch": 4.638017808749516,
      "grad_norm": 5.620428562164307,
      "learning_rate": 5.957757990278316e-06,
      "loss": 0.2812,
      "step": 11980
    },
    {
      "epoch": 4.638404955478126,
      "grad_norm": 45.70320129394531,
      "learning_rate": 5.957327827246527e-06,
      "loss": 1.0286,
      "step": 11981
    },
    {
      "epoch": 4.638792102206736,
      "grad_norm": 51.93843460083008,
      "learning_rate": 5.956897664214738e-06,
      "loss": 1.4346,
      "step": 11982
    },
    {
      "epoch": 4.6391792489353465,
      "grad_norm": 88.15780639648438,
      "learning_rate": 5.956467501182948e-06,
      "loss": 0.5916,
      "step": 11983
    },
    {
      "epoch": 4.639566395663957,
      "grad_norm": 31.64314079284668,
      "learning_rate": 5.95603733815116e-06,
      "loss": 0.9153,
      "step": 11984
    },
    {
      "epoch": 4.639953542392567,
      "grad_norm": 57.338958740234375,
      "learning_rate": 5.955607175119371e-06,
      "loss": 0.8031,
      "step": 11985
    },
    {
      "epoch": 4.640340689121177,
      "grad_norm": 26.70020294189453,
      "learning_rate": 5.955177012087582e-06,
      "loss": 1.3896,
      "step": 11986
    },
    {
      "epoch": 4.640727835849787,
      "grad_norm": 48.55762481689453,
      "learning_rate": 5.954746849055792e-06,
      "loss": 0.7406,
      "step": 11987
    },
    {
      "epoch": 4.641114982578397,
      "grad_norm": 32.12601089477539,
      "learning_rate": 5.954316686024003e-06,
      "loss": 1.7948,
      "step": 11988
    },
    {
      "epoch": 4.641502129307007,
      "grad_norm": 30.49363136291504,
      "learning_rate": 5.953886522992215e-06,
      "loss": 1.7157,
      "step": 11989
    },
    {
      "epoch": 4.641889276035617,
      "grad_norm": 25.624441146850586,
      "learning_rate": 5.953456359960426e-06,
      "loss": 1.7127,
      "step": 11990
    },
    {
      "epoch": 4.642276422764228,
      "grad_norm": 12.359110832214355,
      "learning_rate": 5.953026196928636e-06,
      "loss": 0.6466,
      "step": 11991
    },
    {
      "epoch": 4.6426635694928375,
      "grad_norm": 17.503129959106445,
      "learning_rate": 5.952596033896847e-06,
      "loss": 1.008,
      "step": 11992
    },
    {
      "epoch": 4.643050716221448,
      "grad_norm": 36.14725112915039,
      "learning_rate": 5.952165870865059e-06,
      "loss": 1.3317,
      "step": 11993
    },
    {
      "epoch": 4.643437862950059,
      "grad_norm": 46.23777389526367,
      "learning_rate": 5.95173570783327e-06,
      "loss": 2.4213,
      "step": 11994
    },
    {
      "epoch": 4.643825009678668,
      "grad_norm": 41.85678482055664,
      "learning_rate": 5.95130554480148e-06,
      "loss": 0.6253,
      "step": 11995
    },
    {
      "epoch": 4.644212156407279,
      "grad_norm": 36.651763916015625,
      "learning_rate": 5.950875381769691e-06,
      "loss": 0.7684,
      "step": 11996
    },
    {
      "epoch": 4.644599303135888,
      "grad_norm": 9.678594589233398,
      "learning_rate": 5.950445218737902e-06,
      "loss": 0.6836,
      "step": 11997
    },
    {
      "epoch": 4.644986449864499,
      "grad_norm": 8.80131721496582,
      "learning_rate": 5.950015055706113e-06,
      "loss": 0.3591,
      "step": 11998
    },
    {
      "epoch": 4.6453735965931084,
      "grad_norm": 38.480220794677734,
      "learning_rate": 5.949584892674324e-06,
      "loss": 1.2258,
      "step": 11999
    },
    {
      "epoch": 4.645760743321719,
      "grad_norm": 12.892834663391113,
      "learning_rate": 5.949154729642535e-06,
      "loss": 0.2471,
      "step": 12000
    },
    {
      "epoch": 4.6461478900503295,
      "grad_norm": 6.406428337097168,
      "learning_rate": 5.948724566610746e-06,
      "loss": 0.3688,
      "step": 12001
    },
    {
      "epoch": 4.646535036778939,
      "grad_norm": 15.088776588439941,
      "learning_rate": 5.948294403578957e-06,
      "loss": 1.3328,
      "step": 12002
    },
    {
      "epoch": 4.64692218350755,
      "grad_norm": 13.980270385742188,
      "learning_rate": 5.947864240547167e-06,
      "loss": 0.6452,
      "step": 12003
    },
    {
      "epoch": 4.647309330236159,
      "grad_norm": 20.242630004882812,
      "learning_rate": 5.947434077515378e-06,
      "loss": 1.2945,
      "step": 12004
    },
    {
      "epoch": 4.64769647696477,
      "grad_norm": 17.818023681640625,
      "learning_rate": 5.94700391448359e-06,
      "loss": 0.6864,
      "step": 12005
    },
    {
      "epoch": 4.648083623693379,
      "grad_norm": 33.47552490234375,
      "learning_rate": 5.946573751451801e-06,
      "loss": 2.2663,
      "step": 12006
    },
    {
      "epoch": 4.64847077042199,
      "grad_norm": 35.5875129699707,
      "learning_rate": 5.946143588420011e-06,
      "loss": 1.281,
      "step": 12007
    },
    {
      "epoch": 4.6488579171506,
      "grad_norm": 4.004700660705566,
      "learning_rate": 5.9457134253882236e-06,
      "loss": 0.1634,
      "step": 12008
    },
    {
      "epoch": 4.64924506387921,
      "grad_norm": 20.836915969848633,
      "learning_rate": 5.945283262356434e-06,
      "loss": 1.2378,
      "step": 12009
    },
    {
      "epoch": 4.6496322106078205,
      "grad_norm": 15.920069694519043,
      "learning_rate": 5.944853099324645e-06,
      "loss": 1.3197,
      "step": 12010
    },
    {
      "epoch": 4.65001935733643,
      "grad_norm": 3.051574945449829,
      "learning_rate": 5.944422936292855e-06,
      "loss": 0.1412,
      "step": 12011
    },
    {
      "epoch": 4.650406504065041,
      "grad_norm": 93.53187561035156,
      "learning_rate": 5.943992773261067e-06,
      "loss": 1.5712,
      "step": 12012
    },
    {
      "epoch": 4.650793650793651,
      "grad_norm": 10.627071380615234,
      "learning_rate": 5.943562610229277e-06,
      "loss": 0.3376,
      "step": 12013
    },
    {
      "epoch": 4.651180797522261,
      "grad_norm": 73.10449981689453,
      "learning_rate": 5.943132447197489e-06,
      "loss": 0.9223,
      "step": 12014
    },
    {
      "epoch": 4.651567944250871,
      "grad_norm": 25.0656795501709,
      "learning_rate": 5.942702284165699e-06,
      "loss": 0.5732,
      "step": 12015
    },
    {
      "epoch": 4.651955090979481,
      "grad_norm": 72.7437515258789,
      "learning_rate": 5.942272121133911e-06,
      "loss": 2.3802,
      "step": 12016
    },
    {
      "epoch": 4.652342237708091,
      "grad_norm": 12.422818183898926,
      "learning_rate": 5.941841958102121e-06,
      "loss": 0.6523,
      "step": 12017
    },
    {
      "epoch": 4.652729384436702,
      "grad_norm": 10.904014587402344,
      "learning_rate": 5.941411795070332e-06,
      "loss": 0.5631,
      "step": 12018
    },
    {
      "epoch": 4.6531165311653115,
      "grad_norm": 4.059078693389893,
      "learning_rate": 5.940981632038542e-06,
      "loss": 0.2235,
      "step": 12019
    },
    {
      "epoch": 4.653503677893922,
      "grad_norm": 7.075841426849365,
      "learning_rate": 5.940551469006755e-06,
      "loss": 0.2939,
      "step": 12020
    },
    {
      "epoch": 4.653890824622532,
      "grad_norm": 11.365846633911133,
      "learning_rate": 5.940121305974965e-06,
      "loss": 0.648,
      "step": 12021
    },
    {
      "epoch": 4.654277971351142,
      "grad_norm": 35.01666259765625,
      "learning_rate": 5.939691142943176e-06,
      "loss": 2.2993,
      "step": 12022
    },
    {
      "epoch": 4.654665118079752,
      "grad_norm": 20.9263973236084,
      "learning_rate": 5.939260979911386e-06,
      "loss": 0.5763,
      "step": 12023
    },
    {
      "epoch": 4.655052264808362,
      "grad_norm": 25.624744415283203,
      "learning_rate": 5.938830816879599e-06,
      "loss": 2.3542,
      "step": 12024
    },
    {
      "epoch": 4.655439411536973,
      "grad_norm": 49.62059783935547,
      "learning_rate": 5.938400653847809e-06,
      "loss": 2.0265,
      "step": 12025
    },
    {
      "epoch": 4.6558265582655824,
      "grad_norm": 26.713029861450195,
      "learning_rate": 5.93797049081602e-06,
      "loss": 1.5392,
      "step": 12026
    },
    {
      "epoch": 4.656213704994193,
      "grad_norm": 14.153035163879395,
      "learning_rate": 5.93754032778423e-06,
      "loss": 0.4966,
      "step": 12027
    },
    {
      "epoch": 4.656600851722803,
      "grad_norm": 49.82433319091797,
      "learning_rate": 5.937110164752442e-06,
      "loss": 1.4258,
      "step": 12028
    },
    {
      "epoch": 4.656987998451413,
      "grad_norm": 24.20208740234375,
      "learning_rate": 5.936680001720653e-06,
      "loss": 1.0895,
      "step": 12029
    },
    {
      "epoch": 4.657375145180024,
      "grad_norm": 30.355541229248047,
      "learning_rate": 5.936249838688864e-06,
      "loss": 1.1544,
      "step": 12030
    },
    {
      "epoch": 4.657762291908633,
      "grad_norm": 101.92493438720703,
      "learning_rate": 5.935819675657074e-06,
      "loss": 1.5174,
      "step": 12031
    },
    {
      "epoch": 4.658149438637244,
      "grad_norm": 91.23040008544922,
      "learning_rate": 5.935389512625286e-06,
      "loss": 0.9571,
      "step": 12032
    },
    {
      "epoch": 4.658536585365853,
      "grad_norm": 32.46855926513672,
      "learning_rate": 5.934959349593496e-06,
      "loss": 1.497,
      "step": 12033
    },
    {
      "epoch": 4.658923732094464,
      "grad_norm": 46.681270599365234,
      "learning_rate": 5.934529186561707e-06,
      "loss": 1.7142,
      "step": 12034
    },
    {
      "epoch": 4.659310878823074,
      "grad_norm": 33.97531509399414,
      "learning_rate": 5.934099023529918e-06,
      "loss": 3.4357,
      "step": 12035
    },
    {
      "epoch": 4.659698025551684,
      "grad_norm": 31.970930099487305,
      "learning_rate": 5.93366886049813e-06,
      "loss": 1.0228,
      "step": 12036
    },
    {
      "epoch": 4.6600851722802945,
      "grad_norm": 45.083984375,
      "learning_rate": 5.93323869746634e-06,
      "loss": 1.3557,
      "step": 12037
    },
    {
      "epoch": 4.660472319008904,
      "grad_norm": 27.315465927124023,
      "learning_rate": 5.932808534434551e-06,
      "loss": 0.8915,
      "step": 12038
    },
    {
      "epoch": 4.660859465737515,
      "grad_norm": 7.557075023651123,
      "learning_rate": 5.932378371402761e-06,
      "loss": 0.1871,
      "step": 12039
    },
    {
      "epoch": 4.661246612466124,
      "grad_norm": 46.026554107666016,
      "learning_rate": 5.931948208370974e-06,
      "loss": 1.0662,
      "step": 12040
    },
    {
      "epoch": 4.661633759194735,
      "grad_norm": 98.81705474853516,
      "learning_rate": 5.931518045339184e-06,
      "loss": 0.8165,
      "step": 12041
    },
    {
      "epoch": 4.662020905923345,
      "grad_norm": 14.330205917358398,
      "learning_rate": 5.931087882307395e-06,
      "loss": 0.1605,
      "step": 12042
    },
    {
      "epoch": 4.662408052651955,
      "grad_norm": 4.588691234588623,
      "learning_rate": 5.930657719275605e-06,
      "loss": 0.218,
      "step": 12043
    },
    {
      "epoch": 4.662795199380565,
      "grad_norm": 18.85546112060547,
      "learning_rate": 5.930227556243818e-06,
      "loss": 0.8299,
      "step": 12044
    },
    {
      "epoch": 4.663182346109175,
      "grad_norm": 21.363712310791016,
      "learning_rate": 5.929797393212028e-06,
      "loss": 2.9034,
      "step": 12045
    },
    {
      "epoch": 4.6635694928377855,
      "grad_norm": 110.98352813720703,
      "learning_rate": 5.929367230180239e-06,
      "loss": 2.7489,
      "step": 12046
    },
    {
      "epoch": 4.663956639566395,
      "grad_norm": 17.711502075195312,
      "learning_rate": 5.92893706714845e-06,
      "loss": 0.9568,
      "step": 12047
    },
    {
      "epoch": 4.664343786295006,
      "grad_norm": 66.26351928710938,
      "learning_rate": 5.928506904116661e-06,
      "loss": 0.3714,
      "step": 12048
    },
    {
      "epoch": 4.664730933023616,
      "grad_norm": 12.686226844787598,
      "learning_rate": 5.928076741084871e-06,
      "loss": 0.3944,
      "step": 12049
    },
    {
      "epoch": 4.665118079752226,
      "grad_norm": 14.573907852172852,
      "learning_rate": 5.927646578053083e-06,
      "loss": 0.639,
      "step": 12050
    },
    {
      "epoch": 4.665505226480836,
      "grad_norm": 17.09691047668457,
      "learning_rate": 5.927216415021294e-06,
      "loss": 0.9851,
      "step": 12051
    },
    {
      "epoch": 4.665892373209447,
      "grad_norm": 35.4959831237793,
      "learning_rate": 5.926786251989505e-06,
      "loss": 0.677,
      "step": 12052
    },
    {
      "epoch": 4.6662795199380565,
      "grad_norm": 6.885425090789795,
      "learning_rate": 5.926356088957715e-06,
      "loss": 0.2092,
      "step": 12053
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 11.172749519348145,
      "learning_rate": 5.925925925925926e-06,
      "loss": 0.5842,
      "step": 12054
    },
    {
      "epoch": 4.667053813395277,
      "grad_norm": 28.836084365844727,
      "learning_rate": 5.925495762894138e-06,
      "loss": 1.4283,
      "step": 12055
    },
    {
      "epoch": 4.667440960123887,
      "grad_norm": 22.25922966003418,
      "learning_rate": 5.925065599862349e-06,
      "loss": 1.7195,
      "step": 12056
    },
    {
      "epoch": 4.667828106852497,
      "grad_norm": 23.302841186523438,
      "learning_rate": 5.924635436830559e-06,
      "loss": 1.5355,
      "step": 12057
    },
    {
      "epoch": 4.668215253581107,
      "grad_norm": 11.811046600341797,
      "learning_rate": 5.92420527379877e-06,
      "loss": 0.3345,
      "step": 12058
    },
    {
      "epoch": 4.668602400309718,
      "grad_norm": 60.75938034057617,
      "learning_rate": 5.923775110766982e-06,
      "loss": 1.0738,
      "step": 12059
    },
    {
      "epoch": 4.668989547038327,
      "grad_norm": 3.7883760929107666,
      "learning_rate": 5.923344947735193e-06,
      "loss": 0.1979,
      "step": 12060
    },
    {
      "epoch": 4.669376693766938,
      "grad_norm": 5.732057571411133,
      "learning_rate": 5.922914784703403e-06,
      "loss": 0.3156,
      "step": 12061
    },
    {
      "epoch": 4.6697638404955475,
      "grad_norm": 50.61166763305664,
      "learning_rate": 5.922484621671614e-06,
      "loss": 0.7059,
      "step": 12062
    },
    {
      "epoch": 4.670150987224158,
      "grad_norm": 15.888562202453613,
      "learning_rate": 5.922054458639825e-06,
      "loss": 1.7721,
      "step": 12063
    },
    {
      "epoch": 4.670538133952768,
      "grad_norm": 10.877004623413086,
      "learning_rate": 5.921624295608036e-06,
      "loss": 0.6054,
      "step": 12064
    },
    {
      "epoch": 4.670925280681378,
      "grad_norm": 56.368194580078125,
      "learning_rate": 5.921194132576247e-06,
      "loss": 1.9672,
      "step": 12065
    },
    {
      "epoch": 4.671312427409989,
      "grad_norm": 36.496437072753906,
      "learning_rate": 5.920763969544458e-06,
      "loss": 1.5329,
      "step": 12066
    },
    {
      "epoch": 4.671699574138598,
      "grad_norm": 8.3086519241333,
      "learning_rate": 5.920333806512669e-06,
      "loss": 0.3295,
      "step": 12067
    },
    {
      "epoch": 4.672086720867209,
      "grad_norm": 17.301437377929688,
      "learning_rate": 5.91990364348088e-06,
      "loss": 1.0492,
      "step": 12068
    },
    {
      "epoch": 4.672473867595819,
      "grad_norm": 14.558762550354004,
      "learning_rate": 5.91947348044909e-06,
      "loss": 0.6899,
      "step": 12069
    },
    {
      "epoch": 4.672861014324429,
      "grad_norm": 12.274089813232422,
      "learning_rate": 5.919043317417301e-06,
      "loss": 0.878,
      "step": 12070
    },
    {
      "epoch": 4.673248161053039,
      "grad_norm": 17.702037811279297,
      "learning_rate": 5.918613154385513e-06,
      "loss": 1.2876,
      "step": 12071
    },
    {
      "epoch": 4.673635307781649,
      "grad_norm": 44.058895111083984,
      "learning_rate": 5.918182991353724e-06,
      "loss": 1.2851,
      "step": 12072
    },
    {
      "epoch": 4.6740224545102595,
      "grad_norm": 20.666410446166992,
      "learning_rate": 5.917752828321934e-06,
      "loss": 0.6234,
      "step": 12073
    },
    {
      "epoch": 4.674409601238869,
      "grad_norm": 5.2236409187316895,
      "learning_rate": 5.917322665290145e-06,
      "loss": 0.1727,
      "step": 12074
    },
    {
      "epoch": 4.67479674796748,
      "grad_norm": 171.34547424316406,
      "learning_rate": 5.916892502258357e-06,
      "loss": 2.2376,
      "step": 12075
    },
    {
      "epoch": 4.67518389469609,
      "grad_norm": 10.085844993591309,
      "learning_rate": 5.916462339226568e-06,
      "loss": 0.5292,
      "step": 12076
    },
    {
      "epoch": 4.6755710414247,
      "grad_norm": 45.15915298461914,
      "learning_rate": 5.916032176194778e-06,
      "loss": 1.0335,
      "step": 12077
    },
    {
      "epoch": 4.67595818815331,
      "grad_norm": 33.704837799072266,
      "learning_rate": 5.915602013162989e-06,
      "loss": 1.627,
      "step": 12078
    },
    {
      "epoch": 4.67634533488192,
      "grad_norm": 8.339665412902832,
      "learning_rate": 5.9151718501312e-06,
      "loss": 0.3857,
      "step": 12079
    },
    {
      "epoch": 4.6767324816105305,
      "grad_norm": 35.47608184814453,
      "learning_rate": 5.914741687099412e-06,
      "loss": 1.9,
      "step": 12080
    },
    {
      "epoch": 4.67711962833914,
      "grad_norm": 24.185728073120117,
      "learning_rate": 5.914311524067622e-06,
      "loss": 1.103,
      "step": 12081
    },
    {
      "epoch": 4.677506775067751,
      "grad_norm": 84.33260345458984,
      "learning_rate": 5.913881361035833e-06,
      "loss": 1.0473,
      "step": 12082
    },
    {
      "epoch": 4.677893921796361,
      "grad_norm": 34.40166091918945,
      "learning_rate": 5.913451198004044e-06,
      "loss": 2.5062,
      "step": 12083
    },
    {
      "epoch": 4.678281068524971,
      "grad_norm": 37.05588912963867,
      "learning_rate": 5.913021034972255e-06,
      "loss": 1.6237,
      "step": 12084
    },
    {
      "epoch": 4.678668215253581,
      "grad_norm": 20.51653289794922,
      "learning_rate": 5.912590871940465e-06,
      "loss": 1.1552,
      "step": 12085
    },
    {
      "epoch": 4.679055361982192,
      "grad_norm": 44.646202087402344,
      "learning_rate": 5.912160708908677e-06,
      "loss": 2.2249,
      "step": 12086
    },
    {
      "epoch": 4.679442508710801,
      "grad_norm": 14.872536659240723,
      "learning_rate": 5.911730545876888e-06,
      "loss": 1.3221,
      "step": 12087
    },
    {
      "epoch": 4.679829655439412,
      "grad_norm": 38.2418098449707,
      "learning_rate": 5.911300382845099e-06,
      "loss": 1.1723,
      "step": 12088
    },
    {
      "epoch": 4.6802168021680215,
      "grad_norm": 3.6501688957214355,
      "learning_rate": 5.910870219813309e-06,
      "loss": 0.1295,
      "step": 12089
    },
    {
      "epoch": 4.680603948896632,
      "grad_norm": 39.83771896362305,
      "learning_rate": 5.9104400567815215e-06,
      "loss": 0.8534,
      "step": 12090
    },
    {
      "epoch": 4.680991095625242,
      "grad_norm": 54.223262786865234,
      "learning_rate": 5.910009893749732e-06,
      "loss": 1.9102,
      "step": 12091
    },
    {
      "epoch": 4.681378242353852,
      "grad_norm": 25.889570236206055,
      "learning_rate": 5.909579730717943e-06,
      "loss": 1.1185,
      "step": 12092
    },
    {
      "epoch": 4.681765389082463,
      "grad_norm": 34.307804107666016,
      "learning_rate": 5.909149567686153e-06,
      "loss": 1.2189,
      "step": 12093
    },
    {
      "epoch": 4.682152535811072,
      "grad_norm": 50.965675354003906,
      "learning_rate": 5.908719404654365e-06,
      "loss": 1.4956,
      "step": 12094
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 32.5361442565918,
      "learning_rate": 5.908289241622576e-06,
      "loss": 2.868,
      "step": 12095
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 19.82280158996582,
      "learning_rate": 5.907859078590787e-06,
      "loss": 1.1674,
      "step": 12096
    },
    {
      "epoch": 4.683313975996903,
      "grad_norm": 9.721341133117676,
      "learning_rate": 5.907428915558997e-06,
      "loss": 0.3158,
      "step": 12097
    },
    {
      "epoch": 4.6837011227255125,
      "grad_norm": 48.55305099487305,
      "learning_rate": 5.906998752527209e-06,
      "loss": 2.9224,
      "step": 12098
    },
    {
      "epoch": 4.684088269454123,
      "grad_norm": 78.97132110595703,
      "learning_rate": 5.906568589495419e-06,
      "loss": 1.6771,
      "step": 12099
    },
    {
      "epoch": 4.6844754161827336,
      "grad_norm": 41.381561279296875,
      "learning_rate": 5.90613842646363e-06,
      "loss": 4.1198,
      "step": 12100
    },
    {
      "epoch": 4.684862562911343,
      "grad_norm": 32.9544563293457,
      "learning_rate": 5.905708263431841e-06,
      "loss": 0.588,
      "step": 12101
    },
    {
      "epoch": 4.685249709639954,
      "grad_norm": 22.428314208984375,
      "learning_rate": 5.905278100400053e-06,
      "loss": 0.8258,
      "step": 12102
    },
    {
      "epoch": 4.685636856368563,
      "grad_norm": 27.54440689086914,
      "learning_rate": 5.904847937368263e-06,
      "loss": 2.143,
      "step": 12103
    },
    {
      "epoch": 4.686024003097174,
      "grad_norm": 42.455841064453125,
      "learning_rate": 5.904417774336474e-06,
      "loss": 1.9505,
      "step": 12104
    },
    {
      "epoch": 4.686411149825784,
      "grad_norm": 46.6917839050293,
      "learning_rate": 5.903987611304684e-06,
      "loss": 1.0547,
      "step": 12105
    },
    {
      "epoch": 4.686798296554394,
      "grad_norm": 60.94120788574219,
      "learning_rate": 5.9035574482728966e-06,
      "loss": 0.941,
      "step": 12106
    },
    {
      "epoch": 4.6871854432830045,
      "grad_norm": 21.714210510253906,
      "learning_rate": 5.903127285241107e-06,
      "loss": 1.1076,
      "step": 12107
    },
    {
      "epoch": 4.687572590011614,
      "grad_norm": 25.082300186157227,
      "learning_rate": 5.902697122209318e-06,
      "loss": 2.2071,
      "step": 12108
    },
    {
      "epoch": 4.687959736740225,
      "grad_norm": 3.475879430770874,
      "learning_rate": 5.902266959177528e-06,
      "loss": 0.0465,
      "step": 12109
    },
    {
      "epoch": 4.688346883468835,
      "grad_norm": 117.736083984375,
      "learning_rate": 5.9018367961457405e-06,
      "loss": 1.025,
      "step": 12110
    },
    {
      "epoch": 4.688734030197445,
      "grad_norm": 12.358012199401855,
      "learning_rate": 5.901406633113951e-06,
      "loss": 0.3723,
      "step": 12111
    },
    {
      "epoch": 4.689121176926055,
      "grad_norm": 5.095921993255615,
      "learning_rate": 5.900976470082162e-06,
      "loss": 0.1671,
      "step": 12112
    },
    {
      "epoch": 4.689508323654665,
      "grad_norm": 48.16730499267578,
      "learning_rate": 5.900546307050372e-06,
      "loss": 1.9286,
      "step": 12113
    },
    {
      "epoch": 4.689895470383275,
      "grad_norm": 26.728666305541992,
      "learning_rate": 5.900116144018584e-06,
      "loss": 1.276,
      "step": 12114
    },
    {
      "epoch": 4.690282617111885,
      "grad_norm": 28.01504135131836,
      "learning_rate": 5.899685980986794e-06,
      "loss": 0.9567,
      "step": 12115
    },
    {
      "epoch": 4.6906697638404955,
      "grad_norm": 9.74178409576416,
      "learning_rate": 5.899255817955006e-06,
      "loss": 0.661,
      "step": 12116
    },
    {
      "epoch": 4.691056910569106,
      "grad_norm": 43.77337646484375,
      "learning_rate": 5.898825654923216e-06,
      "loss": 1.3924,
      "step": 12117
    },
    {
      "epoch": 4.691444057297716,
      "grad_norm": 58.283721923828125,
      "learning_rate": 5.898395491891428e-06,
      "loss": 1.6531,
      "step": 12118
    },
    {
      "epoch": 4.691831204026326,
      "grad_norm": 39.87918472290039,
      "learning_rate": 5.897965328859638e-06,
      "loss": 1.35,
      "step": 12119
    },
    {
      "epoch": 4.692218350754936,
      "grad_norm": 26.714200973510742,
      "learning_rate": 5.897535165827849e-06,
      "loss": 3.2344,
      "step": 12120
    },
    {
      "epoch": 4.692605497483546,
      "grad_norm": 30.307233810424805,
      "learning_rate": 5.897105002796059e-06,
      "loss": 1.7751,
      "step": 12121
    },
    {
      "epoch": 4.692992644212157,
      "grad_norm": 98.34194946289062,
      "learning_rate": 5.896674839764272e-06,
      "loss": 2.4701,
      "step": 12122
    },
    {
      "epoch": 4.693379790940766,
      "grad_norm": 90.99585723876953,
      "learning_rate": 5.896244676732482e-06,
      "loss": 2.6044,
      "step": 12123
    },
    {
      "epoch": 4.693766937669377,
      "grad_norm": 31.782711029052734,
      "learning_rate": 5.895814513700693e-06,
      "loss": 1.0408,
      "step": 12124
    },
    {
      "epoch": 4.6941540843979865,
      "grad_norm": 29.321353912353516,
      "learning_rate": 5.895384350668903e-06,
      "loss": 1.7573,
      "step": 12125
    },
    {
      "epoch": 4.694541231126597,
      "grad_norm": 23.74706268310547,
      "learning_rate": 5.8949541876371156e-06,
      "loss": 1.3603,
      "step": 12126
    },
    {
      "epoch": 4.694928377855208,
      "grad_norm": 24.538949966430664,
      "learning_rate": 5.894524024605326e-06,
      "loss": 0.5248,
      "step": 12127
    },
    {
      "epoch": 4.695315524583817,
      "grad_norm": 22.038469314575195,
      "learning_rate": 5.894093861573537e-06,
      "loss": 2.0874,
      "step": 12128
    },
    {
      "epoch": 4.695702671312428,
      "grad_norm": 27.702898025512695,
      "learning_rate": 5.893663698541748e-06,
      "loss": 1.2683,
      "step": 12129
    },
    {
      "epoch": 4.696089818041037,
      "grad_norm": 26.115726470947266,
      "learning_rate": 5.893233535509959e-06,
      "loss": 0.9258,
      "step": 12130
    },
    {
      "epoch": 4.696476964769648,
      "grad_norm": 7.124314308166504,
      "learning_rate": 5.89280337247817e-06,
      "loss": 0.6971,
      "step": 12131
    },
    {
      "epoch": 4.696864111498257,
      "grad_norm": 99.7149887084961,
      "learning_rate": 5.892373209446381e-06,
      "loss": 1.8749,
      "step": 12132
    },
    {
      "epoch": 4.697251258226868,
      "grad_norm": 44.46023941040039,
      "learning_rate": 5.891943046414592e-06,
      "loss": 1.6393,
      "step": 12133
    },
    {
      "epoch": 4.6976384049554785,
      "grad_norm": 7.805644989013672,
      "learning_rate": 5.891512883382803e-06,
      "loss": 0.3766,
      "step": 12134
    },
    {
      "epoch": 4.698025551684088,
      "grad_norm": 24.20924949645996,
      "learning_rate": 5.891082720351013e-06,
      "loss": 0.6803,
      "step": 12135
    },
    {
      "epoch": 4.698412698412699,
      "grad_norm": 17.966039657592773,
      "learning_rate": 5.890652557319224e-06,
      "loss": 1.6039,
      "step": 12136
    },
    {
      "epoch": 4.698799845141308,
      "grad_norm": 34.95213317871094,
      "learning_rate": 5.890222394287436e-06,
      "loss": 1.8769,
      "step": 12137
    },
    {
      "epoch": 4.699186991869919,
      "grad_norm": 30.803794860839844,
      "learning_rate": 5.889792231255647e-06,
      "loss": 1.4609,
      "step": 12138
    },
    {
      "epoch": 4.699574138598528,
      "grad_norm": 57.315521240234375,
      "learning_rate": 5.889362068223857e-06,
      "loss": 1.3598,
      "step": 12139
    },
    {
      "epoch": 4.699961285327139,
      "grad_norm": 17.212526321411133,
      "learning_rate": 5.888931905192068e-06,
      "loss": 0.4562,
      "step": 12140
    },
    {
      "epoch": 4.700348432055749,
      "grad_norm": 18.957944869995117,
      "learning_rate": 5.88850174216028e-06,
      "loss": 0.349,
      "step": 12141
    },
    {
      "epoch": 4.700735578784359,
      "grad_norm": 46.41599655151367,
      "learning_rate": 5.888071579128491e-06,
      "loss": 1.1083,
      "step": 12142
    },
    {
      "epoch": 4.7011227255129695,
      "grad_norm": 13.616414070129395,
      "learning_rate": 5.887641416096701e-06,
      "loss": 0.3422,
      "step": 12143
    },
    {
      "epoch": 4.70150987224158,
      "grad_norm": 24.410144805908203,
      "learning_rate": 5.887211253064912e-06,
      "loss": 1.112,
      "step": 12144
    },
    {
      "epoch": 4.70189701897019,
      "grad_norm": 41.855552673339844,
      "learning_rate": 5.886781090033123e-06,
      "loss": 0.5683,
      "step": 12145
    },
    {
      "epoch": 4.7022841656988,
      "grad_norm": 26.248125076293945,
      "learning_rate": 5.8863509270013346e-06,
      "loss": 3.0584,
      "step": 12146
    },
    {
      "epoch": 4.70267131242741,
      "grad_norm": 14.814020156860352,
      "learning_rate": 5.885920763969545e-06,
      "loss": 0.7262,
      "step": 12147
    },
    {
      "epoch": 4.70305845915602,
      "grad_norm": 13.976326942443848,
      "learning_rate": 5.885490600937756e-06,
      "loss": 0.6376,
      "step": 12148
    },
    {
      "epoch": 4.70344560588463,
      "grad_norm": 10.178932189941406,
      "learning_rate": 5.885060437905967e-06,
      "loss": 0.434,
      "step": 12149
    },
    {
      "epoch": 4.70383275261324,
      "grad_norm": 66.53266906738281,
      "learning_rate": 5.884630274874178e-06,
      "loss": 2.8356,
      "step": 12150
    },
    {
      "epoch": 4.704219899341851,
      "grad_norm": 8.339868545532227,
      "learning_rate": 5.884200111842388e-06,
      "loss": 0.3039,
      "step": 12151
    },
    {
      "epoch": 4.7046070460704605,
      "grad_norm": 21.853981018066406,
      "learning_rate": 5.8837699488106e-06,
      "loss": 1.1218,
      "step": 12152
    },
    {
      "epoch": 4.704994192799071,
      "grad_norm": 20.13991928100586,
      "learning_rate": 5.883339785778811e-06,
      "loss": 0.7749,
      "step": 12153
    },
    {
      "epoch": 4.705381339527681,
      "grad_norm": 20.04361915588379,
      "learning_rate": 5.882909622747022e-06,
      "loss": 1.2539,
      "step": 12154
    },
    {
      "epoch": 4.705768486256291,
      "grad_norm": 58.76516342163086,
      "learning_rate": 5.882479459715232e-06,
      "loss": 0.606,
      "step": 12155
    },
    {
      "epoch": 4.706155632984901,
      "grad_norm": 15.05434799194336,
      "learning_rate": 5.882049296683443e-06,
      "loss": 0.6481,
      "step": 12156
    },
    {
      "epoch": 4.706542779713511,
      "grad_norm": 16.731185913085938,
      "learning_rate": 5.881619133651655e-06,
      "loss": 3.5435,
      "step": 12157
    },
    {
      "epoch": 4.706929926442122,
      "grad_norm": 37.00921630859375,
      "learning_rate": 5.881188970619866e-06,
      "loss": 2.2479,
      "step": 12158
    },
    {
      "epoch": 4.7073170731707314,
      "grad_norm": 8.858746528625488,
      "learning_rate": 5.880758807588076e-06,
      "loss": 0.3499,
      "step": 12159
    },
    {
      "epoch": 4.707704219899342,
      "grad_norm": 8.642557144165039,
      "learning_rate": 5.880328644556287e-06,
      "loss": 0.517,
      "step": 12160
    },
    {
      "epoch": 4.7080913666279525,
      "grad_norm": 40.789852142333984,
      "learning_rate": 5.879898481524499e-06,
      "loss": 1.3992,
      "step": 12161
    },
    {
      "epoch": 4.708478513356562,
      "grad_norm": 33.47347640991211,
      "learning_rate": 5.87946831849271e-06,
      "loss": 1.2385,
      "step": 12162
    },
    {
      "epoch": 4.708865660085173,
      "grad_norm": 14.311232566833496,
      "learning_rate": 5.87903815546092e-06,
      "loss": 0.615,
      "step": 12163
    },
    {
      "epoch": 4.709252806813782,
      "grad_norm": 70.22345733642578,
      "learning_rate": 5.878607992429131e-06,
      "loss": 1.385,
      "step": 12164
    },
    {
      "epoch": 4.709639953542393,
      "grad_norm": 28.856216430664062,
      "learning_rate": 5.878177829397342e-06,
      "loss": 1.4472,
      "step": 12165
    },
    {
      "epoch": 4.710027100271002,
      "grad_norm": 19.19001579284668,
      "learning_rate": 5.877747666365553e-06,
      "loss": 0.3962,
      "step": 12166
    },
    {
      "epoch": 4.710414246999613,
      "grad_norm": 20.54969596862793,
      "learning_rate": 5.877317503333764e-06,
      "loss": 0.5796,
      "step": 12167
    },
    {
      "epoch": 4.710801393728223,
      "grad_norm": 12.186841011047363,
      "learning_rate": 5.876887340301975e-06,
      "loss": 0.2877,
      "step": 12168
    },
    {
      "epoch": 4.711188540456833,
      "grad_norm": 29.481660842895508,
      "learning_rate": 5.876457177270186e-06,
      "loss": 0.7711,
      "step": 12169
    },
    {
      "epoch": 4.7115756871854435,
      "grad_norm": 34.923892974853516,
      "learning_rate": 5.876027014238397e-06,
      "loss": 1.7315,
      "step": 12170
    },
    {
      "epoch": 4.711962833914053,
      "grad_norm": 42.705108642578125,
      "learning_rate": 5.875596851206607e-06,
      "loss": 1.4057,
      "step": 12171
    },
    {
      "epoch": 4.712349980642664,
      "grad_norm": 12.252791404724121,
      "learning_rate": 5.8751666881748195e-06,
      "loss": 0.5775,
      "step": 12172
    },
    {
      "epoch": 4.712737127371273,
      "grad_norm": 10.16969108581543,
      "learning_rate": 5.87473652514303e-06,
      "loss": 0.6463,
      "step": 12173
    },
    {
      "epoch": 4.713124274099884,
      "grad_norm": 41.77544021606445,
      "learning_rate": 5.874306362111241e-06,
      "loss": 1.2476,
      "step": 12174
    },
    {
      "epoch": 4.713511420828494,
      "grad_norm": 20.85807228088379,
      "learning_rate": 5.873876199079451e-06,
      "loss": 0.3156,
      "step": 12175
    },
    {
      "epoch": 4.713898567557104,
      "grad_norm": 23.89316749572754,
      "learning_rate": 5.8734460360476635e-06,
      "loss": 1.4089,
      "step": 12176
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 88.4396743774414,
      "learning_rate": 5.873015873015874e-06,
      "loss": 1.0693,
      "step": 12177
    },
    {
      "epoch": 4.714672861014325,
      "grad_norm": 25.85293960571289,
      "learning_rate": 5.872585709984085e-06,
      "loss": 1.0234,
      "step": 12178
    },
    {
      "epoch": 4.7150600077429345,
      "grad_norm": 93.76335144042969,
      "learning_rate": 5.872155546952295e-06,
      "loss": 0.9275,
      "step": 12179
    },
    {
      "epoch": 4.715447154471545,
      "grad_norm": 35.01577377319336,
      "learning_rate": 5.871725383920507e-06,
      "loss": 1.6422,
      "step": 12180
    },
    {
      "epoch": 4.715834301200155,
      "grad_norm": 24.965320587158203,
      "learning_rate": 5.871295220888717e-06,
      "loss": 0.6555,
      "step": 12181
    },
    {
      "epoch": 4.716221447928765,
      "grad_norm": 15.251067161560059,
      "learning_rate": 5.870865057856929e-06,
      "loss": 0.6679,
      "step": 12182
    },
    {
      "epoch": 4.716608594657375,
      "grad_norm": 24.901535034179688,
      "learning_rate": 5.870434894825139e-06,
      "loss": 1.173,
      "step": 12183
    },
    {
      "epoch": 4.716995741385985,
      "grad_norm": 32.39414978027344,
      "learning_rate": 5.8700047317933506e-06,
      "loss": 2.0276,
      "step": 12184
    },
    {
      "epoch": 4.717382888114596,
      "grad_norm": 22.084239959716797,
      "learning_rate": 5.869574568761561e-06,
      "loss": 0.5282,
      "step": 12185
    },
    {
      "epoch": 4.7177700348432055,
      "grad_norm": 44.74279022216797,
      "learning_rate": 5.869144405729772e-06,
      "loss": 1.6787,
      "step": 12186
    },
    {
      "epoch": 4.718157181571816,
      "grad_norm": 9.633800506591797,
      "learning_rate": 5.868714242697982e-06,
      "loss": 0.5019,
      "step": 12187
    },
    {
      "epoch": 4.718544328300426,
      "grad_norm": 93.69918060302734,
      "learning_rate": 5.8682840796661945e-06,
      "loss": 1.3247,
      "step": 12188
    },
    {
      "epoch": 4.718931475029036,
      "grad_norm": 74.169921875,
      "learning_rate": 5.867853916634405e-06,
      "loss": 1.6073,
      "step": 12189
    },
    {
      "epoch": 4.719318621757646,
      "grad_norm": 31.844810485839844,
      "learning_rate": 5.867423753602616e-06,
      "loss": 3.2023,
      "step": 12190
    },
    {
      "epoch": 4.719705768486256,
      "grad_norm": 57.095123291015625,
      "learning_rate": 5.866993590570826e-06,
      "loss": 1.3162,
      "step": 12191
    },
    {
      "epoch": 4.720092915214867,
      "grad_norm": 34.23782730102539,
      "learning_rate": 5.8665634275390385e-06,
      "loss": 1.1063,
      "step": 12192
    },
    {
      "epoch": 4.720480061943476,
      "grad_norm": 25.943132400512695,
      "learning_rate": 5.866133264507249e-06,
      "loss": 1.4682,
      "step": 12193
    },
    {
      "epoch": 4.720867208672087,
      "grad_norm": 39.62075424194336,
      "learning_rate": 5.86570310147546e-06,
      "loss": 1.0307,
      "step": 12194
    },
    {
      "epoch": 4.7212543554006965,
      "grad_norm": 27.708126068115234,
      "learning_rate": 5.86527293844367e-06,
      "loss": 0.9094,
      "step": 12195
    },
    {
      "epoch": 4.721641502129307,
      "grad_norm": 19.84572410583496,
      "learning_rate": 5.864842775411882e-06,
      "loss": 1.8799,
      "step": 12196
    },
    {
      "epoch": 4.7220286488579175,
      "grad_norm": 12.279001235961914,
      "learning_rate": 5.864412612380093e-06,
      "loss": 1.0659,
      "step": 12197
    },
    {
      "epoch": 4.722415795586527,
      "grad_norm": 8.595364570617676,
      "learning_rate": 5.863982449348304e-06,
      "loss": 0.4364,
      "step": 12198
    },
    {
      "epoch": 4.722802942315138,
      "grad_norm": 33.84670639038086,
      "learning_rate": 5.863552286316514e-06,
      "loss": 0.5768,
      "step": 12199
    },
    {
      "epoch": 4.723190089043747,
      "grad_norm": 43.70040512084961,
      "learning_rate": 5.863122123284726e-06,
      "loss": 0.5787,
      "step": 12200
    },
    {
      "epoch": 4.723577235772358,
      "grad_norm": 26.3942813873291,
      "learning_rate": 5.862691960252936e-06,
      "loss": 0.4562,
      "step": 12201
    },
    {
      "epoch": 4.723964382500968,
      "grad_norm": 30.46923828125,
      "learning_rate": 5.862261797221147e-06,
      "loss": 0.5645,
      "step": 12202
    },
    {
      "epoch": 4.724351529229578,
      "grad_norm": 8.393232345581055,
      "learning_rate": 5.861831634189358e-06,
      "loss": 0.4368,
      "step": 12203
    },
    {
      "epoch": 4.724738675958188,
      "grad_norm": 32.18718338012695,
      "learning_rate": 5.8614014711575696e-06,
      "loss": 0.2918,
      "step": 12204
    },
    {
      "epoch": 4.725125822686798,
      "grad_norm": 47.12248229980469,
      "learning_rate": 5.86097130812578e-06,
      "loss": 1.1771,
      "step": 12205
    },
    {
      "epoch": 4.7255129694154085,
      "grad_norm": 18.745559692382812,
      "learning_rate": 5.860541145093991e-06,
      "loss": 0.7335,
      "step": 12206
    },
    {
      "epoch": 4.725900116144018,
      "grad_norm": 31.687503814697266,
      "learning_rate": 5.860110982062201e-06,
      "loss": 1.219,
      "step": 12207
    },
    {
      "epoch": 4.726287262872629,
      "grad_norm": 9.714803695678711,
      "learning_rate": 5.8596808190304135e-06,
      "loss": 0.382,
      "step": 12208
    },
    {
      "epoch": 4.726674409601239,
      "grad_norm": 15.190875053405762,
      "learning_rate": 5.859250655998624e-06,
      "loss": 1.3148,
      "step": 12209
    },
    {
      "epoch": 4.727061556329849,
      "grad_norm": 10.698810577392578,
      "learning_rate": 5.858820492966835e-06,
      "loss": 0.4298,
      "step": 12210
    },
    {
      "epoch": 4.727448703058459,
      "grad_norm": 33.68743896484375,
      "learning_rate": 5.858390329935046e-06,
      "loss": 0.9596,
      "step": 12211
    },
    {
      "epoch": 4.727835849787069,
      "grad_norm": 37.139949798583984,
      "learning_rate": 5.8579601669032575e-06,
      "loss": 0.2105,
      "step": 12212
    },
    {
      "epoch": 4.7282229965156795,
      "grad_norm": 16.341829299926758,
      "learning_rate": 5.857530003871468e-06,
      "loss": 1.5208,
      "step": 12213
    },
    {
      "epoch": 4.72861014324429,
      "grad_norm": 5.435942649841309,
      "learning_rate": 5.857099840839679e-06,
      "loss": 0.1814,
      "step": 12214
    },
    {
      "epoch": 4.7289972899729,
      "grad_norm": 23.01828384399414,
      "learning_rate": 5.85666967780789e-06,
      "loss": 1.472,
      "step": 12215
    },
    {
      "epoch": 4.72938443670151,
      "grad_norm": 62.23194122314453,
      "learning_rate": 5.856239514776101e-06,
      "loss": 2.891,
      "step": 12216
    },
    {
      "epoch": 4.72977158343012,
      "grad_norm": 42.35730743408203,
      "learning_rate": 5.855809351744311e-06,
      "loss": 1.7666,
      "step": 12217
    },
    {
      "epoch": 4.73015873015873,
      "grad_norm": 13.976645469665527,
      "learning_rate": 5.855379188712523e-06,
      "loss": 0.4617,
      "step": 12218
    },
    {
      "epoch": 4.730545876887341,
      "grad_norm": 86.23467254638672,
      "learning_rate": 5.854949025680734e-06,
      "loss": 2.3604,
      "step": 12219
    },
    {
      "epoch": 4.73093302361595,
      "grad_norm": 33.72953796386719,
      "learning_rate": 5.854518862648945e-06,
      "loss": 1.4109,
      "step": 12220
    },
    {
      "epoch": 4.731320170344561,
      "grad_norm": 49.04432678222656,
      "learning_rate": 5.854088699617155e-06,
      "loss": 2.3264,
      "step": 12221
    },
    {
      "epoch": 4.7317073170731705,
      "grad_norm": 21.03675079345703,
      "learning_rate": 5.853658536585366e-06,
      "loss": 0.8362,
      "step": 12222
    },
    {
      "epoch": 4.732094463801781,
      "grad_norm": 44.74602508544922,
      "learning_rate": 5.853228373553578e-06,
      "loss": 1.58,
      "step": 12223
    },
    {
      "epoch": 4.732481610530391,
      "grad_norm": 20.58297348022461,
      "learning_rate": 5.8527982105217886e-06,
      "loss": 0.9305,
      "step": 12224
    },
    {
      "epoch": 4.732868757259001,
      "grad_norm": 9.888084411621094,
      "learning_rate": 5.852368047489999e-06,
      "loss": 0.5164,
      "step": 12225
    },
    {
      "epoch": 4.733255903987612,
      "grad_norm": 8.539009094238281,
      "learning_rate": 5.85193788445821e-06,
      "loss": 0.5122,
      "step": 12226
    },
    {
      "epoch": 4.733643050716221,
      "grad_norm": 57.29767990112305,
      "learning_rate": 5.851507721426421e-06,
      "loss": 2.169,
      "step": 12227
    },
    {
      "epoch": 4.734030197444832,
      "grad_norm": 18.02682113647461,
      "learning_rate": 5.8510775583946325e-06,
      "loss": 0.8266,
      "step": 12228
    },
    {
      "epoch": 4.734417344173441,
      "grad_norm": 29.2232666015625,
      "learning_rate": 5.850647395362843e-06,
      "loss": 0.7244,
      "step": 12229
    },
    {
      "epoch": 4.734804490902052,
      "grad_norm": 82.02281188964844,
      "learning_rate": 5.850217232331054e-06,
      "loss": 1.9359,
      "step": 12230
    },
    {
      "epoch": 4.7351916376306615,
      "grad_norm": 2.606977701187134,
      "learning_rate": 5.849787069299265e-06,
      "loss": 0.0533,
      "step": 12231
    },
    {
      "epoch": 4.735578784359272,
      "grad_norm": 22.78328514099121,
      "learning_rate": 5.849356906267476e-06,
      "loss": 0.4966,
      "step": 12232
    },
    {
      "epoch": 4.7359659310878826,
      "grad_norm": 14.156195640563965,
      "learning_rate": 5.848926743235687e-06,
      "loss": 1.0519,
      "step": 12233
    },
    {
      "epoch": 4.736353077816492,
      "grad_norm": 18.092912673950195,
      "learning_rate": 5.848496580203898e-06,
      "loss": 0.7204,
      "step": 12234
    },
    {
      "epoch": 4.736740224545103,
      "grad_norm": 46.2351188659668,
      "learning_rate": 5.848066417172109e-06,
      "loss": 1.3817,
      "step": 12235
    },
    {
      "epoch": 4.737127371273713,
      "grad_norm": 18.894113540649414,
      "learning_rate": 5.84763625414032e-06,
      "loss": 0.7648,
      "step": 12236
    },
    {
      "epoch": 4.737514518002323,
      "grad_norm": 10.454015731811523,
      "learning_rate": 5.84720609110853e-06,
      "loss": 0.9175,
      "step": 12237
    },
    {
      "epoch": 4.737901664730933,
      "grad_norm": 85.57428741455078,
      "learning_rate": 5.846775928076741e-06,
      "loss": 1.3629,
      "step": 12238
    },
    {
      "epoch": 4.738288811459543,
      "grad_norm": 23.841087341308594,
      "learning_rate": 5.846345765044953e-06,
      "loss": 1.9602,
      "step": 12239
    },
    {
      "epoch": 4.7386759581881535,
      "grad_norm": 46.09770202636719,
      "learning_rate": 5.845915602013164e-06,
      "loss": 1.0854,
      "step": 12240
    },
    {
      "epoch": 4.739063104916763,
      "grad_norm": 18.204071044921875,
      "learning_rate": 5.845485438981374e-06,
      "loss": 1.9087,
      "step": 12241
    },
    {
      "epoch": 4.739450251645374,
      "grad_norm": 37.46310806274414,
      "learning_rate": 5.845055275949585e-06,
      "loss": 1.1162,
      "step": 12242
    },
    {
      "epoch": 4.739837398373984,
      "grad_norm": 15.497673988342285,
      "learning_rate": 5.844625112917797e-06,
      "loss": 0.7209,
      "step": 12243
    },
    {
      "epoch": 4.740224545102594,
      "grad_norm": 6.168574810028076,
      "learning_rate": 5.8441949498860076e-06,
      "loss": 0.1846,
      "step": 12244
    },
    {
      "epoch": 4.740611691831204,
      "grad_norm": 36.65812683105469,
      "learning_rate": 5.843764786854218e-06,
      "loss": 2.1468,
      "step": 12245
    },
    {
      "epoch": 4.740998838559814,
      "grad_norm": 84.63224792480469,
      "learning_rate": 5.843334623822429e-06,
      "loss": 2.1542,
      "step": 12246
    },
    {
      "epoch": 4.741385985288424,
      "grad_norm": 15.205940246582031,
      "learning_rate": 5.84290446079064e-06,
      "loss": 0.7384,
      "step": 12247
    },
    {
      "epoch": 4.741773132017034,
      "grad_norm": 36.40930938720703,
      "learning_rate": 5.8424742977588515e-06,
      "loss": 2.0586,
      "step": 12248
    },
    {
      "epoch": 4.7421602787456445,
      "grad_norm": 27.754499435424805,
      "learning_rate": 5.842044134727062e-06,
      "loss": 1.7369,
      "step": 12249
    },
    {
      "epoch": 4.742547425474255,
      "grad_norm": 30.674741744995117,
      "learning_rate": 5.841613971695273e-06,
      "loss": 1.2645,
      "step": 12250
    },
    {
      "epoch": 4.742934572202865,
      "grad_norm": 14.008384704589844,
      "learning_rate": 5.841183808663484e-06,
      "loss": 0.7665,
      "step": 12251
    },
    {
      "epoch": 4.743321718931475,
      "grad_norm": 63.5233154296875,
      "learning_rate": 5.840753645631695e-06,
      "loss": 1.5157,
      "step": 12252
    },
    {
      "epoch": 4.743708865660086,
      "grad_norm": 19.712364196777344,
      "learning_rate": 5.840323482599905e-06,
      "loss": 0.4381,
      "step": 12253
    },
    {
      "epoch": 4.744096012388695,
      "grad_norm": 135.92999267578125,
      "learning_rate": 5.8398933195681175e-06,
      "loss": 1.671,
      "step": 12254
    },
    {
      "epoch": 4.744483159117306,
      "grad_norm": 34.18655776977539,
      "learning_rate": 5.839463156536328e-06,
      "loss": 1.1804,
      "step": 12255
    },
    {
      "epoch": 4.744870305845915,
      "grad_norm": 28.745563507080078,
      "learning_rate": 5.839032993504539e-06,
      "loss": 0.7342,
      "step": 12256
    },
    {
      "epoch": 4.745257452574526,
      "grad_norm": 43.59299087524414,
      "learning_rate": 5.838602830472749e-06,
      "loss": 2.4237,
      "step": 12257
    },
    {
      "epoch": 4.7456445993031355,
      "grad_norm": 11.659250259399414,
      "learning_rate": 5.8381726674409615e-06,
      "loss": 0.7652,
      "step": 12258
    },
    {
      "epoch": 4.746031746031746,
      "grad_norm": 44.7029914855957,
      "learning_rate": 5.837742504409172e-06,
      "loss": 0.7329,
      "step": 12259
    },
    {
      "epoch": 4.746418892760357,
      "grad_norm": 32.310115814208984,
      "learning_rate": 5.837312341377383e-06,
      "loss": 1.5298,
      "step": 12260
    },
    {
      "epoch": 4.746806039488966,
      "grad_norm": 55.51231384277344,
      "learning_rate": 5.836882178345593e-06,
      "loss": 1.3058,
      "step": 12261
    },
    {
      "epoch": 4.747193186217577,
      "grad_norm": 18.930585861206055,
      "learning_rate": 5.8364520153138046e-06,
      "loss": 0.9066,
      "step": 12262
    },
    {
      "epoch": 4.747580332946186,
      "grad_norm": 23.554502487182617,
      "learning_rate": 5.836021852282015e-06,
      "loss": 1.5597,
      "step": 12263
    },
    {
      "epoch": 4.747967479674797,
      "grad_norm": 148.08322143554688,
      "learning_rate": 5.8355916892502266e-06,
      "loss": 3.0015,
      "step": 12264
    },
    {
      "epoch": 4.748354626403406,
      "grad_norm": 18.49988555908203,
      "learning_rate": 5.835161526218437e-06,
      "loss": 0.9609,
      "step": 12265
    },
    {
      "epoch": 4.748741773132017,
      "grad_norm": 56.327980041503906,
      "learning_rate": 5.8347313631866485e-06,
      "loss": 1.8355,
      "step": 12266
    },
    {
      "epoch": 4.7491289198606275,
      "grad_norm": 20.892131805419922,
      "learning_rate": 5.834301200154859e-06,
      "loss": 0.9682,
      "step": 12267
    },
    {
      "epoch": 4.749516066589237,
      "grad_norm": 11.069241523742676,
      "learning_rate": 5.83387103712307e-06,
      "loss": 0.6603,
      "step": 12268
    },
    {
      "epoch": 4.749903213317848,
      "grad_norm": 11.254735946655273,
      "learning_rate": 5.833440874091281e-06,
      "loss": 0.5933,
      "step": 12269
    },
    {
      "epoch": 4.750290360046458,
      "grad_norm": 30.095224380493164,
      "learning_rate": 5.8330107110594925e-06,
      "loss": 2.3301,
      "step": 12270
    },
    {
      "epoch": 4.750677506775068,
      "grad_norm": 22.30364990234375,
      "learning_rate": 5.832580548027703e-06,
      "loss": 1.4427,
      "step": 12271
    },
    {
      "epoch": 4.751064653503678,
      "grad_norm": 32.21426773071289,
      "learning_rate": 5.832150384995914e-06,
      "loss": 1.0214,
      "step": 12272
    },
    {
      "epoch": 4.751451800232288,
      "grad_norm": 14.159156799316406,
      "learning_rate": 5.831720221964124e-06,
      "loss": 0.818,
      "step": 12273
    },
    {
      "epoch": 4.751838946960898,
      "grad_norm": 12.079779624938965,
      "learning_rate": 5.8312900589323365e-06,
      "loss": 0.7606,
      "step": 12274
    },
    {
      "epoch": 4.752226093689508,
      "grad_norm": 23.78461456298828,
      "learning_rate": 5.830859895900547e-06,
      "loss": 1.1265,
      "step": 12275
    },
    {
      "epoch": 4.7526132404181185,
      "grad_norm": 17.582988739013672,
      "learning_rate": 5.830429732868758e-06,
      "loss": 0.4438,
      "step": 12276
    },
    {
      "epoch": 4.753000387146729,
      "grad_norm": 9.959346771240234,
      "learning_rate": 5.829999569836968e-06,
      "loss": 0.5812,
      "step": 12277
    },
    {
      "epoch": 4.753387533875339,
      "grad_norm": 34.93787384033203,
      "learning_rate": 5.82956940680518e-06,
      "loss": 1.9146,
      "step": 12278
    },
    {
      "epoch": 4.753774680603949,
      "grad_norm": 23.849140167236328,
      "learning_rate": 5.829139243773391e-06,
      "loss": 0.7457,
      "step": 12279
    },
    {
      "epoch": 4.754161827332559,
      "grad_norm": 28.230274200439453,
      "learning_rate": 5.828709080741602e-06,
      "loss": 1.5701,
      "step": 12280
    },
    {
      "epoch": 4.754548974061169,
      "grad_norm": 48.098106384277344,
      "learning_rate": 5.828278917709812e-06,
      "loss": 1.49,
      "step": 12281
    },
    {
      "epoch": 4.754936120789779,
      "grad_norm": 34.84767150878906,
      "learning_rate": 5.8278487546780236e-06,
      "loss": 1.0772,
      "step": 12282
    },
    {
      "epoch": 4.755323267518389,
      "grad_norm": 23.91739273071289,
      "learning_rate": 5.827418591646234e-06,
      "loss": 1.5431,
      "step": 12283
    },
    {
      "epoch": 4.755710414247,
      "grad_norm": 28.7041015625,
      "learning_rate": 5.8269884286144456e-06,
      "loss": 1.091,
      "step": 12284
    },
    {
      "epoch": 4.7560975609756095,
      "grad_norm": 10.161243438720703,
      "learning_rate": 5.826558265582656e-06,
      "loss": 0.4565,
      "step": 12285
    },
    {
      "epoch": 4.75648470770422,
      "grad_norm": 17.42059326171875,
      "learning_rate": 5.8261281025508675e-06,
      "loss": 1.1903,
      "step": 12286
    },
    {
      "epoch": 4.75687185443283,
      "grad_norm": 25.823387145996094,
      "learning_rate": 5.825697939519078e-06,
      "loss": 0.6885,
      "step": 12287
    },
    {
      "epoch": 4.75725900116144,
      "grad_norm": 19.53162956237793,
      "learning_rate": 5.825267776487289e-06,
      "loss": 1.5111,
      "step": 12288
    },
    {
      "epoch": 4.757646147890051,
      "grad_norm": 28.137680053710938,
      "learning_rate": 5.824837613455499e-06,
      "loss": 1.3296,
      "step": 12289
    },
    {
      "epoch": 4.75803329461866,
      "grad_norm": 18.220458984375,
      "learning_rate": 5.8244074504237115e-06,
      "loss": 1.7436,
      "step": 12290
    },
    {
      "epoch": 4.758420441347271,
      "grad_norm": 44.552059173583984,
      "learning_rate": 5.823977287391922e-06,
      "loss": 1.1573,
      "step": 12291
    },
    {
      "epoch": 4.7588075880758804,
      "grad_norm": 6.808825492858887,
      "learning_rate": 5.823547124360133e-06,
      "loss": 0.3205,
      "step": 12292
    },
    {
      "epoch": 4.759194734804491,
      "grad_norm": 12.740335464477539,
      "learning_rate": 5.823116961328343e-06,
      "loss": 0.6452,
      "step": 12293
    },
    {
      "epoch": 4.7595818815331015,
      "grad_norm": 19.060611724853516,
      "learning_rate": 5.8226867982965555e-06,
      "loss": 0.6275,
      "step": 12294
    },
    {
      "epoch": 4.759969028261711,
      "grad_norm": 35.228721618652344,
      "learning_rate": 5.822256635264766e-06,
      "loss": 1.2928,
      "step": 12295
    },
    {
      "epoch": 4.760356174990322,
      "grad_norm": 31.790971755981445,
      "learning_rate": 5.821826472232977e-06,
      "loss": 2.1449,
      "step": 12296
    },
    {
      "epoch": 4.760743321718931,
      "grad_norm": 7.745275974273682,
      "learning_rate": 5.821396309201188e-06,
      "loss": 0.244,
      "step": 12297
    },
    {
      "epoch": 4.761130468447542,
      "grad_norm": 21.32866668701172,
      "learning_rate": 5.820966146169399e-06,
      "loss": 0.7542,
      "step": 12298
    },
    {
      "epoch": 4.761517615176151,
      "grad_norm": 48.52365493774414,
      "learning_rate": 5.820535983137609e-06,
      "loss": 1.4145,
      "step": 12299
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 29.82744789123535,
      "learning_rate": 5.820105820105821e-06,
      "loss": 0.8108,
      "step": 12300
    },
    {
      "epoch": 4.762291908633372,
      "grad_norm": 17.46570587158203,
      "learning_rate": 5.819675657074032e-06,
      "loss": 0.6379,
      "step": 12301
    },
    {
      "epoch": 4.762679055361982,
      "grad_norm": 69.4395523071289,
      "learning_rate": 5.8192454940422426e-06,
      "loss": 1.1983,
      "step": 12302
    },
    {
      "epoch": 4.7630662020905925,
      "grad_norm": 6.385669231414795,
      "learning_rate": 5.818815331010453e-06,
      "loss": 0.0737,
      "step": 12303
    },
    {
      "epoch": 4.763453348819202,
      "grad_norm": 15.012286186218262,
      "learning_rate": 5.818385167978664e-06,
      "loss": 0.571,
      "step": 12304
    },
    {
      "epoch": 4.763840495547813,
      "grad_norm": 12.742637634277344,
      "learning_rate": 5.817955004946876e-06,
      "loss": 0.6953,
      "step": 12305
    },
    {
      "epoch": 4.764227642276423,
      "grad_norm": 47.387516021728516,
      "learning_rate": 5.8175248419150865e-06,
      "loss": 1.2234,
      "step": 12306
    },
    {
      "epoch": 4.764614789005033,
      "grad_norm": 42.39886474609375,
      "learning_rate": 5.817094678883297e-06,
      "loss": 0.9642,
      "step": 12307
    },
    {
      "epoch": 4.765001935733643,
      "grad_norm": 29.04159164428711,
      "learning_rate": 5.816664515851508e-06,
      "loss": 0.6963,
      "step": 12308
    },
    {
      "epoch": 4.765389082462253,
      "grad_norm": 23.391199111938477,
      "learning_rate": 5.81623435281972e-06,
      "loss": 0.8381,
      "step": 12309
    },
    {
      "epoch": 4.765776229190863,
      "grad_norm": 43.74837875366211,
      "learning_rate": 5.8158041897879305e-06,
      "loss": 2.2608,
      "step": 12310
    },
    {
      "epoch": 4.766163375919474,
      "grad_norm": 15.599227905273438,
      "learning_rate": 5.815374026756141e-06,
      "loss": 1.4357,
      "step": 12311
    },
    {
      "epoch": 4.7665505226480835,
      "grad_norm": 23.396041870117188,
      "learning_rate": 5.814943863724352e-06,
      "loss": 0.5634,
      "step": 12312
    },
    {
      "epoch": 4.766937669376694,
      "grad_norm": 31.53920555114746,
      "learning_rate": 5.814513700692563e-06,
      "loss": 0.9855,
      "step": 12313
    },
    {
      "epoch": 4.767324816105304,
      "grad_norm": 55.755516052246094,
      "learning_rate": 5.814083537660774e-06,
      "loss": 0.4808,
      "step": 12314
    },
    {
      "epoch": 4.767711962833914,
      "grad_norm": 7.176899433135986,
      "learning_rate": 5.813653374628985e-06,
      "loss": 0.3842,
      "step": 12315
    },
    {
      "epoch": 4.768099109562524,
      "grad_norm": 53.95833969116211,
      "learning_rate": 5.813223211597196e-06,
      "loss": 1.6325,
      "step": 12316
    },
    {
      "epoch": 4.768486256291134,
      "grad_norm": 30.059476852416992,
      "learning_rate": 5.812793048565407e-06,
      "loss": 0.9447,
      "step": 12317
    },
    {
      "epoch": 4.768873403019745,
      "grad_norm": 20.424341201782227,
      "learning_rate": 5.812362885533618e-06,
      "loss": 0.5562,
      "step": 12318
    },
    {
      "epoch": 4.7692605497483544,
      "grad_norm": 41.463050842285156,
      "learning_rate": 5.811932722501828e-06,
      "loss": 0.7648,
      "step": 12319
    },
    {
      "epoch": 4.769647696476965,
      "grad_norm": 7.743200778961182,
      "learning_rate": 5.81150255947004e-06,
      "loss": 0.5083,
      "step": 12320
    },
    {
      "epoch": 4.770034843205575,
      "grad_norm": 20.137800216674805,
      "learning_rate": 5.811072396438251e-06,
      "loss": 1.5099,
      "step": 12321
    },
    {
      "epoch": 4.770421989934185,
      "grad_norm": 48.32889175415039,
      "learning_rate": 5.8106422334064616e-06,
      "loss": 1.5491,
      "step": 12322
    },
    {
      "epoch": 4.770809136662795,
      "grad_norm": 62.54763412475586,
      "learning_rate": 5.810212070374672e-06,
      "loss": 1.35,
      "step": 12323
    },
    {
      "epoch": 4.771196283391405,
      "grad_norm": 59.4660758972168,
      "learning_rate": 5.809781907342883e-06,
      "loss": 0.7675,
      "step": 12324
    },
    {
      "epoch": 4.771583430120016,
      "grad_norm": 41.839820861816406,
      "learning_rate": 5.809351744311095e-06,
      "loss": 0.3253,
      "step": 12325
    },
    {
      "epoch": 4.771970576848625,
      "grad_norm": 17.652658462524414,
      "learning_rate": 5.8089215812793055e-06,
      "loss": 1.3418,
      "step": 12326
    },
    {
      "epoch": 4.772357723577236,
      "grad_norm": 13.625750541687012,
      "learning_rate": 5.808491418247516e-06,
      "loss": 0.5951,
      "step": 12327
    },
    {
      "epoch": 4.772744870305846,
      "grad_norm": 21.76413345336914,
      "learning_rate": 5.808061255215727e-06,
      "loss": 0.9642,
      "step": 12328
    },
    {
      "epoch": 4.773132017034456,
      "grad_norm": 9.70728588104248,
      "learning_rate": 5.807631092183938e-06,
      "loss": 1.1438,
      "step": 12329
    },
    {
      "epoch": 4.7735191637630665,
      "grad_norm": 23.56672477722168,
      "learning_rate": 5.8072009291521495e-06,
      "loss": 0.9114,
      "step": 12330
    },
    {
      "epoch": 4.773906310491676,
      "grad_norm": 6.691347599029541,
      "learning_rate": 5.80677076612036e-06,
      "loss": 0.2822,
      "step": 12331
    },
    {
      "epoch": 4.774293457220287,
      "grad_norm": 10.006503105163574,
      "learning_rate": 5.806340603088571e-06,
      "loss": 0.2474,
      "step": 12332
    },
    {
      "epoch": 4.774680603948896,
      "grad_norm": 17.035064697265625,
      "learning_rate": 5.805910440056782e-06,
      "loss": 0.6861,
      "step": 12333
    },
    {
      "epoch": 4.775067750677507,
      "grad_norm": 32.23598098754883,
      "learning_rate": 5.805480277024993e-06,
      "loss": 2.4639,
      "step": 12334
    },
    {
      "epoch": 4.775454897406117,
      "grad_norm": 32.968597412109375,
      "learning_rate": 5.805050113993203e-06,
      "loss": 1.5755,
      "step": 12335
    },
    {
      "epoch": 4.775842044134727,
      "grad_norm": 36.8624153137207,
      "learning_rate": 5.8046199509614155e-06,
      "loss": 0.9562,
      "step": 12336
    },
    {
      "epoch": 4.776229190863337,
      "grad_norm": 16.0532169342041,
      "learning_rate": 5.804189787929626e-06,
      "loss": 1.0784,
      "step": 12337
    },
    {
      "epoch": 4.776616337591947,
      "grad_norm": 109.0899887084961,
      "learning_rate": 5.803759624897837e-06,
      "loss": 1.4152,
      "step": 12338
    },
    {
      "epoch": 4.7770034843205575,
      "grad_norm": 36.92984390258789,
      "learning_rate": 5.803329461866047e-06,
      "loss": 1.7465,
      "step": 12339
    },
    {
      "epoch": 4.777390631049167,
      "grad_norm": 21.25794219970703,
      "learning_rate": 5.8028992988342594e-06,
      "loss": 0.5841,
      "step": 12340
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 34.61265563964844,
      "learning_rate": 5.80246913580247e-06,
      "loss": 1.2942,
      "step": 12341
    },
    {
      "epoch": 4.778164924506388,
      "grad_norm": 12.589522361755371,
      "learning_rate": 5.8020389727706806e-06,
      "loss": 0.4111,
      "step": 12342
    },
    {
      "epoch": 4.778552071234998,
      "grad_norm": 128.809814453125,
      "learning_rate": 5.801608809738891e-06,
      "loss": 1.3978,
      "step": 12343
    },
    {
      "epoch": 4.778939217963608,
      "grad_norm": 33.038211822509766,
      "learning_rate": 5.8011786467071026e-06,
      "loss": 1.2814,
      "step": 12344
    },
    {
      "epoch": 4.779326364692219,
      "grad_norm": 35.57160186767578,
      "learning_rate": 5.800748483675314e-06,
      "loss": 0.3873,
      "step": 12345
    },
    {
      "epoch": 4.7797135114208285,
      "grad_norm": 55.09097671508789,
      "learning_rate": 5.8003183206435245e-06,
      "loss": 2.1824,
      "step": 12346
    },
    {
      "epoch": 4.780100658149439,
      "grad_norm": 18.45015525817871,
      "learning_rate": 5.799888157611735e-06,
      "loss": 1.257,
      "step": 12347
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 72.3243637084961,
      "learning_rate": 5.7994579945799465e-06,
      "loss": 1.8321,
      "step": 12348
    },
    {
      "epoch": 4.780874951606659,
      "grad_norm": 19.484926223754883,
      "learning_rate": 5.799027831548157e-06,
      "loss": 0.7911,
      "step": 12349
    },
    {
      "epoch": 4.781262098335269,
      "grad_norm": 32.35090255737305,
      "learning_rate": 5.798597668516368e-06,
      "loss": 1.6241,
      "step": 12350
    },
    {
      "epoch": 4.781649245063879,
      "grad_norm": 59.96393585205078,
      "learning_rate": 5.798167505484579e-06,
      "loss": 1.521,
      "step": 12351
    },
    {
      "epoch": 4.78203639179249,
      "grad_norm": 34.48893356323242,
      "learning_rate": 5.7977373424527905e-06,
      "loss": 0.6299,
      "step": 12352
    },
    {
      "epoch": 4.782423538521099,
      "grad_norm": 31.253555297851562,
      "learning_rate": 5.797307179421001e-06,
      "loss": 0.6769,
      "step": 12353
    },
    {
      "epoch": 4.78281068524971,
      "grad_norm": 39.276145935058594,
      "learning_rate": 5.796877016389212e-06,
      "loss": 1.9678,
      "step": 12354
    },
    {
      "epoch": 4.7831978319783195,
      "grad_norm": 14.96908187866211,
      "learning_rate": 5.796446853357422e-06,
      "loss": 0.7249,
      "step": 12355
    },
    {
      "epoch": 4.78358497870693,
      "grad_norm": 79.831787109375,
      "learning_rate": 5.7960166903256345e-06,
      "loss": 1.9827,
      "step": 12356
    },
    {
      "epoch": 4.78397212543554,
      "grad_norm": 28.583696365356445,
      "learning_rate": 5.795586527293845e-06,
      "loss": 2.2458,
      "step": 12357
    },
    {
      "epoch": 4.78435927216415,
      "grad_norm": 10.705391883850098,
      "learning_rate": 5.795156364262056e-06,
      "loss": 0.3975,
      "step": 12358
    },
    {
      "epoch": 4.784746418892761,
      "grad_norm": 39.42339324951172,
      "learning_rate": 5.794726201230266e-06,
      "loss": 2.6623,
      "step": 12359
    },
    {
      "epoch": 4.78513356562137,
      "grad_norm": 6.204575061798096,
      "learning_rate": 5.7942960381984784e-06,
      "loss": 0.3225,
      "step": 12360
    },
    {
      "epoch": 4.785520712349981,
      "grad_norm": 49.75220489501953,
      "learning_rate": 5.793865875166689e-06,
      "loss": 0.8646,
      "step": 12361
    },
    {
      "epoch": 4.78590785907859,
      "grad_norm": 90.5428237915039,
      "learning_rate": 5.7934357121348996e-06,
      "loss": 1.3921,
      "step": 12362
    },
    {
      "epoch": 4.786295005807201,
      "grad_norm": 39.164794921875,
      "learning_rate": 5.79300554910311e-06,
      "loss": 0.9299,
      "step": 12363
    },
    {
      "epoch": 4.786682152535811,
      "grad_norm": 45.57681655883789,
      "learning_rate": 5.7925753860713215e-06,
      "loss": 1.6506,
      "step": 12364
    },
    {
      "epoch": 4.787069299264421,
      "grad_norm": 14.779196739196777,
      "learning_rate": 5.792145223039532e-06,
      "loss": 1.6865,
      "step": 12365
    },
    {
      "epoch": 4.7874564459930316,
      "grad_norm": 36.733184814453125,
      "learning_rate": 5.7917150600077435e-06,
      "loss": 1.3218,
      "step": 12366
    },
    {
      "epoch": 4.787843592721641,
      "grad_norm": 1.9936068058013916,
      "learning_rate": 5.791284896975954e-06,
      "loss": 0.0517,
      "step": 12367
    },
    {
      "epoch": 4.788230739450252,
      "grad_norm": 21.96294593811035,
      "learning_rate": 5.7908547339441655e-06,
      "loss": 1.3883,
      "step": 12368
    },
    {
      "epoch": 4.788617886178862,
      "grad_norm": 53.5675048828125,
      "learning_rate": 5.790424570912376e-06,
      "loss": 1.4724,
      "step": 12369
    },
    {
      "epoch": 4.789005032907472,
      "grad_norm": 22.434009552001953,
      "learning_rate": 5.789994407880587e-06,
      "loss": 0.7964,
      "step": 12370
    },
    {
      "epoch": 4.789392179636082,
      "grad_norm": 76.10237121582031,
      "learning_rate": 5.789564244848797e-06,
      "loss": 2.2122,
      "step": 12371
    },
    {
      "epoch": 4.789779326364692,
      "grad_norm": 61.75279235839844,
      "learning_rate": 5.7891340818170095e-06,
      "loss": 1.3602,
      "step": 12372
    },
    {
      "epoch": 4.7901664730933025,
      "grad_norm": 37.757755279541016,
      "learning_rate": 5.78870391878522e-06,
      "loss": 0.7725,
      "step": 12373
    },
    {
      "epoch": 4.790553619821912,
      "grad_norm": 34.56319046020508,
      "learning_rate": 5.788273755753431e-06,
      "loss": 1.5258,
      "step": 12374
    },
    {
      "epoch": 4.790940766550523,
      "grad_norm": 5.664976596832275,
      "learning_rate": 5.787843592721641e-06,
      "loss": 0.2733,
      "step": 12375
    },
    {
      "epoch": 4.791327913279133,
      "grad_norm": 24.747982025146484,
      "learning_rate": 5.7874134296898535e-06,
      "loss": 1.3831,
      "step": 12376
    },
    {
      "epoch": 4.791715060007743,
      "grad_norm": 40.24190139770508,
      "learning_rate": 5.786983266658064e-06,
      "loss": 0.9818,
      "step": 12377
    },
    {
      "epoch": 4.792102206736353,
      "grad_norm": 16.020544052124023,
      "learning_rate": 5.786553103626275e-06,
      "loss": 1.3103,
      "step": 12378
    },
    {
      "epoch": 4.792489353464963,
      "grad_norm": 37.73851776123047,
      "learning_rate": 5.786122940594486e-06,
      "loss": 1.0173,
      "step": 12379
    },
    {
      "epoch": 4.792876500193573,
      "grad_norm": 18.135086059570312,
      "learning_rate": 5.785692777562697e-06,
      "loss": 0.4916,
      "step": 12380
    },
    {
      "epoch": 4.793263646922184,
      "grad_norm": 15.271759033203125,
      "learning_rate": 5.785262614530908e-06,
      "loss": 0.3183,
      "step": 12381
    },
    {
      "epoch": 4.7936507936507935,
      "grad_norm": 78.93415832519531,
      "learning_rate": 5.7848324514991186e-06,
      "loss": 2.5268,
      "step": 12382
    },
    {
      "epoch": 4.794037940379404,
      "grad_norm": 33.924652099609375,
      "learning_rate": 5.78440228846733e-06,
      "loss": 1.2393,
      "step": 12383
    },
    {
      "epoch": 4.794425087108014,
      "grad_norm": 16.65691375732422,
      "learning_rate": 5.7839721254355405e-06,
      "loss": 0.5852,
      "step": 12384
    },
    {
      "epoch": 4.794812233836624,
      "grad_norm": 29.39948844909668,
      "learning_rate": 5.783541962403751e-06,
      "loss": 0.7388,
      "step": 12385
    },
    {
      "epoch": 4.795199380565235,
      "grad_norm": 34.555824279785156,
      "learning_rate": 5.783111799371962e-06,
      "loss": 2.0385,
      "step": 12386
    },
    {
      "epoch": 4.795586527293844,
      "grad_norm": 1.372818112373352,
      "learning_rate": 5.782681636340174e-06,
      "loss": 0.0441,
      "step": 12387
    },
    {
      "epoch": 4.795973674022455,
      "grad_norm": 18.611434936523438,
      "learning_rate": 5.7822514733083845e-06,
      "loss": 0.5202,
      "step": 12388
    },
    {
      "epoch": 4.796360820751064,
      "grad_norm": 43.360294342041016,
      "learning_rate": 5.781821310276595e-06,
      "loss": 0.8921,
      "step": 12389
    },
    {
      "epoch": 4.796747967479675,
      "grad_norm": 18.028581619262695,
      "learning_rate": 5.781391147244806e-06,
      "loss": 0.5128,
      "step": 12390
    },
    {
      "epoch": 4.7971351142082845,
      "grad_norm": 74.68489837646484,
      "learning_rate": 5.780960984213018e-06,
      "loss": 2.028,
      "step": 12391
    },
    {
      "epoch": 4.797522260936895,
      "grad_norm": 30.837106704711914,
      "learning_rate": 5.7805308211812285e-06,
      "loss": 1.049,
      "step": 12392
    },
    {
      "epoch": 4.7979094076655056,
      "grad_norm": 32.78687286376953,
      "learning_rate": 5.780100658149439e-06,
      "loss": 1.5888,
      "step": 12393
    },
    {
      "epoch": 4.798296554394115,
      "grad_norm": 1.0736082792282104,
      "learning_rate": 5.77967049511765e-06,
      "loss": 0.0325,
      "step": 12394
    },
    {
      "epoch": 4.798683701122726,
      "grad_norm": 12.785934448242188,
      "learning_rate": 5.779240332085861e-06,
      "loss": 0.3268,
      "step": 12395
    },
    {
      "epoch": 4.799070847851335,
      "grad_norm": 17.356243133544922,
      "learning_rate": 5.7788101690540725e-06,
      "loss": 0.7141,
      "step": 12396
    },
    {
      "epoch": 4.799457994579946,
      "grad_norm": 51.51691818237305,
      "learning_rate": 5.778380006022283e-06,
      "loss": 0.7494,
      "step": 12397
    },
    {
      "epoch": 4.799845141308556,
      "grad_norm": 12.720759391784668,
      "learning_rate": 5.777949842990494e-06,
      "loss": 0.3499,
      "step": 12398
    },
    {
      "epoch": 4.800232288037166,
      "grad_norm": 6.469544410705566,
      "learning_rate": 5.777519679958705e-06,
      "loss": 0.2854,
      "step": 12399
    },
    {
      "epoch": 4.8006194347657765,
      "grad_norm": 15.49625015258789,
      "learning_rate": 5.7770895169269156e-06,
      "loss": 1.2609,
      "step": 12400
    },
    {
      "epoch": 4.801006581494386,
      "grad_norm": 36.788169860839844,
      "learning_rate": 5.776659353895126e-06,
      "loss": 0.9271,
      "step": 12401
    },
    {
      "epoch": 4.801393728222997,
      "grad_norm": 35.187442779541016,
      "learning_rate": 5.7762291908633376e-06,
      "loss": 1.1467,
      "step": 12402
    },
    {
      "epoch": 4.801780874951607,
      "grad_norm": 63.2286376953125,
      "learning_rate": 5.775799027831549e-06,
      "loss": 1.3239,
      "step": 12403
    },
    {
      "epoch": 4.802168021680217,
      "grad_norm": 79.28899383544922,
      "learning_rate": 5.7753688647997595e-06,
      "loss": 1.502,
      "step": 12404
    },
    {
      "epoch": 4.802555168408827,
      "grad_norm": 29.471643447875977,
      "learning_rate": 5.77493870176797e-06,
      "loss": 0.8043,
      "step": 12405
    },
    {
      "epoch": 4.802942315137437,
      "grad_norm": 128.6178741455078,
      "learning_rate": 5.774508538736181e-06,
      "loss": 2.0226,
      "step": 12406
    },
    {
      "epoch": 4.803329461866047,
      "grad_norm": 54.31635284423828,
      "learning_rate": 5.774078375704393e-06,
      "loss": 2.1124,
      "step": 12407
    },
    {
      "epoch": 4.803716608594657,
      "grad_norm": 20.162485122680664,
      "learning_rate": 5.7736482126726035e-06,
      "loss": 1.3741,
      "step": 12408
    },
    {
      "epoch": 4.8041037553232675,
      "grad_norm": 98.14427185058594,
      "learning_rate": 5.773218049640814e-06,
      "loss": 1.0058,
      "step": 12409
    },
    {
      "epoch": 4.804490902051878,
      "grad_norm": 51.00007629394531,
      "learning_rate": 5.772787886609025e-06,
      "loss": 0.8306,
      "step": 12410
    },
    {
      "epoch": 4.804878048780488,
      "grad_norm": 60.801849365234375,
      "learning_rate": 5.772357723577237e-06,
      "loss": 1.904,
      "step": 12411
    },
    {
      "epoch": 4.805265195509098,
      "grad_norm": 8.861527442932129,
      "learning_rate": 5.7719275605454475e-06,
      "loss": 0.2857,
      "step": 12412
    },
    {
      "epoch": 4.805652342237708,
      "grad_norm": 27.514629364013672,
      "learning_rate": 5.771497397513658e-06,
      "loss": 0.7172,
      "step": 12413
    },
    {
      "epoch": 4.806039488966318,
      "grad_norm": 18.848310470581055,
      "learning_rate": 5.771067234481869e-06,
      "loss": 0.1309,
      "step": 12414
    },
    {
      "epoch": 4.806426635694928,
      "grad_norm": 7.0031657218933105,
      "learning_rate": 5.77063707145008e-06,
      "loss": 0.3661,
      "step": 12415
    },
    {
      "epoch": 4.806813782423538,
      "grad_norm": 36.355918884277344,
      "learning_rate": 5.770206908418291e-06,
      "loss": 0.9322,
      "step": 12416
    },
    {
      "epoch": 4.807200929152149,
      "grad_norm": 62.036766052246094,
      "learning_rate": 5.769776745386502e-06,
      "loss": 0.6163,
      "step": 12417
    },
    {
      "epoch": 4.8075880758807585,
      "grad_norm": 67.34613800048828,
      "learning_rate": 5.7693465823547134e-06,
      "loss": 1.2883,
      "step": 12418
    },
    {
      "epoch": 4.807975222609369,
      "grad_norm": 93.2397232055664,
      "learning_rate": 5.768916419322924e-06,
      "loss": 1.0605,
      "step": 12419
    },
    {
      "epoch": 4.80836236933798,
      "grad_norm": 23.547931671142578,
      "learning_rate": 5.7684862562911346e-06,
      "loss": 1.4089,
      "step": 12420
    },
    {
      "epoch": 4.808749516066589,
      "grad_norm": 27.063602447509766,
      "learning_rate": 5.768056093259345e-06,
      "loss": 0.6278,
      "step": 12421
    },
    {
      "epoch": 4.8091366627952,
      "grad_norm": 23.301071166992188,
      "learning_rate": 5.767625930227557e-06,
      "loss": 2.0343,
      "step": 12422
    },
    {
      "epoch": 4.809523809523809,
      "grad_norm": 63.55046081542969,
      "learning_rate": 5.767195767195768e-06,
      "loss": 2.0634,
      "step": 12423
    },
    {
      "epoch": 4.80991095625242,
      "grad_norm": 19.038616180419922,
      "learning_rate": 5.7667656041639785e-06,
      "loss": 1.5642,
      "step": 12424
    },
    {
      "epoch": 4.8102981029810294,
      "grad_norm": 36.808448791503906,
      "learning_rate": 5.766335441132189e-06,
      "loss": 1.4633,
      "step": 12425
    },
    {
      "epoch": 4.81068524970964,
      "grad_norm": 31.59425163269043,
      "learning_rate": 5.765905278100401e-06,
      "loss": 0.551,
      "step": 12426
    },
    {
      "epoch": 4.8110723964382505,
      "grad_norm": 59.21149826049805,
      "learning_rate": 5.765475115068612e-06,
      "loss": 1.1267,
      "step": 12427
    },
    {
      "epoch": 4.81145954316686,
      "grad_norm": 25.373411178588867,
      "learning_rate": 5.7650449520368225e-06,
      "loss": 1.4277,
      "step": 12428
    },
    {
      "epoch": 4.811846689895471,
      "grad_norm": 26.781251907348633,
      "learning_rate": 5.764614789005033e-06,
      "loss": 1.2636,
      "step": 12429
    },
    {
      "epoch": 4.81223383662408,
      "grad_norm": 14.378311157226562,
      "learning_rate": 5.7641846259732445e-06,
      "loss": 0.4977,
      "step": 12430
    },
    {
      "epoch": 4.812620983352691,
      "grad_norm": 33.21049118041992,
      "learning_rate": 5.763754462941455e-06,
      "loss": 1.5351,
      "step": 12431
    },
    {
      "epoch": 4.8130081300813,
      "grad_norm": 34.935096740722656,
      "learning_rate": 5.7633242999096665e-06,
      "loss": 0.7592,
      "step": 12432
    },
    {
      "epoch": 4.813395276809911,
      "grad_norm": 40.96639633178711,
      "learning_rate": 5.762894136877877e-06,
      "loss": 1.4111,
      "step": 12433
    },
    {
      "epoch": 4.813782423538521,
      "grad_norm": 10.886564254760742,
      "learning_rate": 5.7624639738460885e-06,
      "loss": 0.4027,
      "step": 12434
    },
    {
      "epoch": 4.814169570267131,
      "grad_norm": 34.901100158691406,
      "learning_rate": 5.762033810814299e-06,
      "loss": 1.338,
      "step": 12435
    },
    {
      "epoch": 4.8145567169957415,
      "grad_norm": 86.3604736328125,
      "learning_rate": 5.76160364778251e-06,
      "loss": 3.0819,
      "step": 12436
    },
    {
      "epoch": 4.814943863724352,
      "grad_norm": 31.22892951965332,
      "learning_rate": 5.76117348475072e-06,
      "loss": 1.3265,
      "step": 12437
    },
    {
      "epoch": 4.815331010452962,
      "grad_norm": 73.21822357177734,
      "learning_rate": 5.7607433217189324e-06,
      "loss": 0.7387,
      "step": 12438
    },
    {
      "epoch": 4.815718157181572,
      "grad_norm": 26.161296844482422,
      "learning_rate": 5.760313158687143e-06,
      "loss": 1.0304,
      "step": 12439
    },
    {
      "epoch": 4.816105303910182,
      "grad_norm": 13.032356262207031,
      "learning_rate": 5.7598829956553536e-06,
      "loss": 0.939,
      "step": 12440
    },
    {
      "epoch": 4.816492450638792,
      "grad_norm": 96.0869140625,
      "learning_rate": 5.759452832623564e-06,
      "loss": 0.8192,
      "step": 12441
    },
    {
      "epoch": 4.816879597367402,
      "grad_norm": 39.983882904052734,
      "learning_rate": 5.759022669591776e-06,
      "loss": 1.3972,
      "step": 12442
    },
    {
      "epoch": 4.817266744096012,
      "grad_norm": 17.890844345092773,
      "learning_rate": 5.758592506559987e-06,
      "loss": 0.8241,
      "step": 12443
    },
    {
      "epoch": 4.817653890824623,
      "grad_norm": 66.07723999023438,
      "learning_rate": 5.7581623435281975e-06,
      "loss": 2.9436,
      "step": 12444
    },
    {
      "epoch": 4.8180410375532325,
      "grad_norm": 39.09418869018555,
      "learning_rate": 5.757732180496408e-06,
      "loss": 0.8755,
      "step": 12445
    },
    {
      "epoch": 4.818428184281843,
      "grad_norm": 34.09236526489258,
      "learning_rate": 5.7573020174646195e-06,
      "loss": 1.4892,
      "step": 12446
    },
    {
      "epoch": 4.818815331010453,
      "grad_norm": 49.054771423339844,
      "learning_rate": 5.756871854432831e-06,
      "loss": 1.8224,
      "step": 12447
    },
    {
      "epoch": 4.819202477739063,
      "grad_norm": 31.41172218322754,
      "learning_rate": 5.7564416914010415e-06,
      "loss": 1.1264,
      "step": 12448
    },
    {
      "epoch": 4.819589624467673,
      "grad_norm": 61.332096099853516,
      "learning_rate": 5.756011528369252e-06,
      "loss": 2.1698,
      "step": 12449
    },
    {
      "epoch": 4.819976771196283,
      "grad_norm": 17.08115577697754,
      "learning_rate": 5.7555813653374635e-06,
      "loss": 0.4602,
      "step": 12450
    },
    {
      "epoch": 4.820363917924894,
      "grad_norm": 25.700885772705078,
      "learning_rate": 5.755151202305674e-06,
      "loss": 1.2002,
      "step": 12451
    },
    {
      "epoch": 4.8207510646535034,
      "grad_norm": 30.540390014648438,
      "learning_rate": 5.754721039273885e-06,
      "loss": 2.2807,
      "step": 12452
    },
    {
      "epoch": 4.821138211382114,
      "grad_norm": 23.442047119140625,
      "learning_rate": 5.754290876242096e-06,
      "loss": 1.447,
      "step": 12453
    },
    {
      "epoch": 4.821525358110724,
      "grad_norm": 23.710668563842773,
      "learning_rate": 5.7538607132103075e-06,
      "loss": 1.1885,
      "step": 12454
    },
    {
      "epoch": 4.821912504839334,
      "grad_norm": 13.128026008605957,
      "learning_rate": 5.753430550178518e-06,
      "loss": 2.0579,
      "step": 12455
    },
    {
      "epoch": 4.822299651567945,
      "grad_norm": 46.89585494995117,
      "learning_rate": 5.753000387146729e-06,
      "loss": 1.9086,
      "step": 12456
    },
    {
      "epoch": 4.822686798296554,
      "grad_norm": 69.26770782470703,
      "learning_rate": 5.752570224114939e-06,
      "loss": 2.4563,
      "step": 12457
    },
    {
      "epoch": 4.823073945025165,
      "grad_norm": 24.259737014770508,
      "learning_rate": 5.7521400610831514e-06,
      "loss": 1.1713,
      "step": 12458
    },
    {
      "epoch": 4.823461091753774,
      "grad_norm": 19.065330505371094,
      "learning_rate": 5.751709898051362e-06,
      "loss": 2.328,
      "step": 12459
    },
    {
      "epoch": 4.823848238482385,
      "grad_norm": 44.80103302001953,
      "learning_rate": 5.7512797350195726e-06,
      "loss": 0.7946,
      "step": 12460
    },
    {
      "epoch": 4.824235385210995,
      "grad_norm": 24.431119918823242,
      "learning_rate": 5.750849571987784e-06,
      "loss": 0.8011,
      "step": 12461
    },
    {
      "epoch": 4.824622531939605,
      "grad_norm": 47.541690826416016,
      "learning_rate": 5.750419408955995e-06,
      "loss": 1.8707,
      "step": 12462
    },
    {
      "epoch": 4.8250096786682155,
      "grad_norm": 11.09014892578125,
      "learning_rate": 5.749989245924206e-06,
      "loss": 0.2898,
      "step": 12463
    },
    {
      "epoch": 4.825396825396825,
      "grad_norm": 16.26702880859375,
      "learning_rate": 5.7495590828924165e-06,
      "loss": 0.4627,
      "step": 12464
    },
    {
      "epoch": 4.825783972125436,
      "grad_norm": 82.03435516357422,
      "learning_rate": 5.749128919860628e-06,
      "loss": 1.3544,
      "step": 12465
    },
    {
      "epoch": 4.826171118854045,
      "grad_norm": 54.80504608154297,
      "learning_rate": 5.7486987568288385e-06,
      "loss": 0.9619,
      "step": 12466
    },
    {
      "epoch": 4.826558265582656,
      "grad_norm": 11.479682922363281,
      "learning_rate": 5.748268593797049e-06,
      "loss": 0.2425,
      "step": 12467
    },
    {
      "epoch": 4.826945412311266,
      "grad_norm": 25.10968017578125,
      "learning_rate": 5.7478384307652605e-06,
      "loss": 1.6818,
      "step": 12468
    },
    {
      "epoch": 4.827332559039876,
      "grad_norm": 153.69656372070312,
      "learning_rate": 5.747408267733472e-06,
      "loss": 0.9847,
      "step": 12469
    },
    {
      "epoch": 4.827719705768486,
      "grad_norm": 38.66229248046875,
      "learning_rate": 5.7469781047016825e-06,
      "loss": 2.1607,
      "step": 12470
    },
    {
      "epoch": 4.828106852497096,
      "grad_norm": 45.21638488769531,
      "learning_rate": 5.746547941669893e-06,
      "loss": 1.4985,
      "step": 12471
    },
    {
      "epoch": 4.8284939992257065,
      "grad_norm": 77.12193298339844,
      "learning_rate": 5.746117778638104e-06,
      "loss": 0.556,
      "step": 12472
    },
    {
      "epoch": 4.828881145954317,
      "grad_norm": 23.10972785949707,
      "learning_rate": 5.745687615606316e-06,
      "loss": 0.3393,
      "step": 12473
    },
    {
      "epoch": 4.829268292682927,
      "grad_norm": 69.63192749023438,
      "learning_rate": 5.7452574525745265e-06,
      "loss": 0.8982,
      "step": 12474
    },
    {
      "epoch": 4.829655439411537,
      "grad_norm": 5.02837610244751,
      "learning_rate": 5.744827289542737e-06,
      "loss": 0.2755,
      "step": 12475
    },
    {
      "epoch": 4.830042586140147,
      "grad_norm": 37.82200241088867,
      "learning_rate": 5.744397126510948e-06,
      "loss": 1.8097,
      "step": 12476
    },
    {
      "epoch": 4.830429732868757,
      "grad_norm": 37.32607650756836,
      "learning_rate": 5.74396696347916e-06,
      "loss": 0.8479,
      "step": 12477
    },
    {
      "epoch": 4.830816879597368,
      "grad_norm": 46.93894958496094,
      "learning_rate": 5.7435368004473704e-06,
      "loss": 0.6646,
      "step": 12478
    },
    {
      "epoch": 4.8312040263259775,
      "grad_norm": 32.55186462402344,
      "learning_rate": 5.743106637415581e-06,
      "loss": 2.0742,
      "step": 12479
    },
    {
      "epoch": 4.831591173054588,
      "grad_norm": 42.164146423339844,
      "learning_rate": 5.7426764743837916e-06,
      "loss": 2.6957,
      "step": 12480
    },
    {
      "epoch": 4.831978319783198,
      "grad_norm": 36.717689514160156,
      "learning_rate": 5.742246311352003e-06,
      "loss": 3.1599,
      "step": 12481
    },
    {
      "epoch": 4.832365466511808,
      "grad_norm": 7.4943013191223145,
      "learning_rate": 5.7418161483202135e-06,
      "loss": 0.3812,
      "step": 12482
    },
    {
      "epoch": 4.832752613240418,
      "grad_norm": 24.39757537841797,
      "learning_rate": 5.741385985288425e-06,
      "loss": 2.6298,
      "step": 12483
    },
    {
      "epoch": 4.833139759969028,
      "grad_norm": 23.873722076416016,
      "learning_rate": 5.7409558222566355e-06,
      "loss": 2.9542,
      "step": 12484
    },
    {
      "epoch": 4.833526906697639,
      "grad_norm": 13.059205055236816,
      "learning_rate": 5.740525659224847e-06,
      "loss": 0.6052,
      "step": 12485
    },
    {
      "epoch": 4.833914053426248,
      "grad_norm": 10.451869010925293,
      "learning_rate": 5.7400954961930575e-06,
      "loss": 0.3309,
      "step": 12486
    },
    {
      "epoch": 4.834301200154859,
      "grad_norm": 30.966882705688477,
      "learning_rate": 5.739665333161268e-06,
      "loss": 0.2751,
      "step": 12487
    },
    {
      "epoch": 4.8346883468834685,
      "grad_norm": 89.6893539428711,
      "learning_rate": 5.739235170129479e-06,
      "loss": 1.9286,
      "step": 12488
    },
    {
      "epoch": 4.835075493612079,
      "grad_norm": 9.32201862335205,
      "learning_rate": 5.738805007097691e-06,
      "loss": 0.2738,
      "step": 12489
    },
    {
      "epoch": 4.835462640340689,
      "grad_norm": 46.893497467041016,
      "learning_rate": 5.7383748440659015e-06,
      "loss": 0.5872,
      "step": 12490
    },
    {
      "epoch": 4.835849787069299,
      "grad_norm": 21.84235382080078,
      "learning_rate": 5.737944681034112e-06,
      "loss": 0.5306,
      "step": 12491
    },
    {
      "epoch": 4.83623693379791,
      "grad_norm": 83.21488189697266,
      "learning_rate": 5.737514518002323e-06,
      "loss": 2.1108,
      "step": 12492
    },
    {
      "epoch": 4.836624080526519,
      "grad_norm": 24.142040252685547,
      "learning_rate": 5.737084354970535e-06,
      "loss": 1.0893,
      "step": 12493
    },
    {
      "epoch": 4.83701122725513,
      "grad_norm": 23.76966667175293,
      "learning_rate": 5.7366541919387455e-06,
      "loss": 2.1282,
      "step": 12494
    },
    {
      "epoch": 4.83739837398374,
      "grad_norm": 21.76978302001953,
      "learning_rate": 5.736224028906956e-06,
      "loss": 1.5216,
      "step": 12495
    },
    {
      "epoch": 4.83778552071235,
      "grad_norm": 27.314327239990234,
      "learning_rate": 5.735793865875167e-06,
      "loss": 2.3416,
      "step": 12496
    },
    {
      "epoch": 4.83817266744096,
      "grad_norm": 37.76499938964844,
      "learning_rate": 5.735363702843378e-06,
      "loss": 1.7632,
      "step": 12497
    },
    {
      "epoch": 4.83855981416957,
      "grad_norm": 14.161469459533691,
      "learning_rate": 5.7349335398115894e-06,
      "loss": 0.4505,
      "step": 12498
    },
    {
      "epoch": 4.8389469608981805,
      "grad_norm": 10.236227035522461,
      "learning_rate": 5.7345033767798e-06,
      "loss": 0.2658,
      "step": 12499
    },
    {
      "epoch": 4.83933410762679,
      "grad_norm": 25.479869842529297,
      "learning_rate": 5.734073213748011e-06,
      "loss": 0.675,
      "step": 12500
    },
    {
      "epoch": 4.839721254355401,
      "grad_norm": 15.089033126831055,
      "learning_rate": 5.733643050716222e-06,
      "loss": 0.6687,
      "step": 12501
    },
    {
      "epoch": 4.840108401084011,
      "grad_norm": 1.8139249086380005,
      "learning_rate": 5.7332128876844325e-06,
      "loss": 0.0425,
      "step": 12502
    },
    {
      "epoch": 4.840495547812621,
      "grad_norm": 82.26229095458984,
      "learning_rate": 5.732782724652643e-06,
      "loss": 1.0753,
      "step": 12503
    },
    {
      "epoch": 4.840882694541231,
      "grad_norm": 43.36300277709961,
      "learning_rate": 5.732352561620855e-06,
      "loss": 2.5532,
      "step": 12504
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 21.532760620117188,
      "learning_rate": 5.731922398589066e-06,
      "loss": 1.2967,
      "step": 12505
    },
    {
      "epoch": 4.8416569879984515,
      "grad_norm": 62.20518112182617,
      "learning_rate": 5.7314922355572765e-06,
      "loss": 2.1566,
      "step": 12506
    },
    {
      "epoch": 4.842044134727061,
      "grad_norm": 35.22381591796875,
      "learning_rate": 5.731062072525487e-06,
      "loss": 1.3335,
      "step": 12507
    },
    {
      "epoch": 4.842431281455672,
      "grad_norm": 40.7467155456543,
      "learning_rate": 5.730631909493699e-06,
      "loss": 1.4682,
      "step": 12508
    },
    {
      "epoch": 4.842818428184282,
      "grad_norm": 16.720483779907227,
      "learning_rate": 5.73020174646191e-06,
      "loss": 0.4754,
      "step": 12509
    },
    {
      "epoch": 4.843205574912892,
      "grad_norm": 21.49241828918457,
      "learning_rate": 5.7297715834301205e-06,
      "loss": 0.6776,
      "step": 12510
    },
    {
      "epoch": 4.843592721641502,
      "grad_norm": 19.738285064697266,
      "learning_rate": 5.729341420398331e-06,
      "loss": 0.5719,
      "step": 12511
    },
    {
      "epoch": 4.843979868370113,
      "grad_norm": 4.5269880294799805,
      "learning_rate": 5.7289112573665425e-06,
      "loss": 0.2469,
      "step": 12512
    },
    {
      "epoch": 4.844367015098722,
      "grad_norm": 18.69438934326172,
      "learning_rate": 5.728481094334754e-06,
      "loss": 0.8317,
      "step": 12513
    },
    {
      "epoch": 4.844754161827333,
      "grad_norm": 83.7393569946289,
      "learning_rate": 5.7280509313029645e-06,
      "loss": 1.5555,
      "step": 12514
    },
    {
      "epoch": 4.8451413085559425,
      "grad_norm": 62.96780776977539,
      "learning_rate": 5.727620768271175e-06,
      "loss": 0.5829,
      "step": 12515
    },
    {
      "epoch": 4.845528455284553,
      "grad_norm": 13.034883499145508,
      "learning_rate": 5.7271906052393864e-06,
      "loss": 1.9241,
      "step": 12516
    },
    {
      "epoch": 4.845915602013163,
      "grad_norm": 40.46455764770508,
      "learning_rate": 5.726760442207597e-06,
      "loss": 1.3005,
      "step": 12517
    },
    {
      "epoch": 4.846302748741773,
      "grad_norm": 13.48100471496582,
      "learning_rate": 5.7263302791758076e-06,
      "loss": 1.6841,
      "step": 12518
    },
    {
      "epoch": 4.846689895470384,
      "grad_norm": 13.976052284240723,
      "learning_rate": 5.725900116144019e-06,
      "loss": 1.3852,
      "step": 12519
    },
    {
      "epoch": 4.847077042198993,
      "grad_norm": 11.714082717895508,
      "learning_rate": 5.72546995311223e-06,
      "loss": 0.6063,
      "step": 12520
    },
    {
      "epoch": 4.847464188927604,
      "grad_norm": 61.98925018310547,
      "learning_rate": 5.725039790080441e-06,
      "loss": 1.3686,
      "step": 12521
    },
    {
      "epoch": 4.847851335656213,
      "grad_norm": 49.9254150390625,
      "learning_rate": 5.7246096270486515e-06,
      "loss": 0.7791,
      "step": 12522
    },
    {
      "epoch": 4.848238482384824,
      "grad_norm": 41.787010192871094,
      "learning_rate": 5.724179464016862e-06,
      "loss": 2.1864,
      "step": 12523
    },
    {
      "epoch": 4.8486256291134335,
      "grad_norm": 7.793286323547363,
      "learning_rate": 5.723749300985074e-06,
      "loss": 0.4833,
      "step": 12524
    },
    {
      "epoch": 4.849012775842044,
      "grad_norm": 47.58050537109375,
      "learning_rate": 5.723319137953285e-06,
      "loss": 1.1746,
      "step": 12525
    },
    {
      "epoch": 4.8493999225706546,
      "grad_norm": 7.219261169433594,
      "learning_rate": 5.7228889749214955e-06,
      "loss": 0.7557,
      "step": 12526
    },
    {
      "epoch": 4.849787069299264,
      "grad_norm": 23.374980926513672,
      "learning_rate": 5.722458811889706e-06,
      "loss": 0.6376,
      "step": 12527
    },
    {
      "epoch": 4.850174216027875,
      "grad_norm": 20.878162384033203,
      "learning_rate": 5.722028648857918e-06,
      "loss": 0.6085,
      "step": 12528
    },
    {
      "epoch": 4.850561362756485,
      "grad_norm": 28.13613510131836,
      "learning_rate": 5.721598485826129e-06,
      "loss": 0.8556,
      "step": 12529
    },
    {
      "epoch": 4.850948509485095,
      "grad_norm": 48.44757843017578,
      "learning_rate": 5.7211683227943395e-06,
      "loss": 1.4579,
      "step": 12530
    },
    {
      "epoch": 4.851335656213705,
      "grad_norm": 23.38117027282715,
      "learning_rate": 5.72073815976255e-06,
      "loss": 0.8354,
      "step": 12531
    },
    {
      "epoch": 4.851722802942315,
      "grad_norm": 38.338138580322266,
      "learning_rate": 5.7203079967307615e-06,
      "loss": 0.6094,
      "step": 12532
    },
    {
      "epoch": 4.8521099496709255,
      "grad_norm": 5.391458034515381,
      "learning_rate": 5.719877833698972e-06,
      "loss": 0.1929,
      "step": 12533
    },
    {
      "epoch": 4.852497096399535,
      "grad_norm": 13.24930191040039,
      "learning_rate": 5.7194476706671835e-06,
      "loss": 0.5408,
      "step": 12534
    },
    {
      "epoch": 4.852884243128146,
      "grad_norm": 17.721403121948242,
      "learning_rate": 5.719017507635394e-06,
      "loss": 0.7165,
      "step": 12535
    },
    {
      "epoch": 4.853271389856756,
      "grad_norm": 41.46470260620117,
      "learning_rate": 5.7185873446036054e-06,
      "loss": 2.8922,
      "step": 12536
    },
    {
      "epoch": 4.853658536585366,
      "grad_norm": 9.954460144042969,
      "learning_rate": 5.718157181571816e-06,
      "loss": 0.4834,
      "step": 12537
    },
    {
      "epoch": 4.854045683313976,
      "grad_norm": 47.835845947265625,
      "learning_rate": 5.7177270185400266e-06,
      "loss": 1.826,
      "step": 12538
    },
    {
      "epoch": 4.854432830042586,
      "grad_norm": 12.681256294250488,
      "learning_rate": 5.717296855508237e-06,
      "loss": 0.6041,
      "step": 12539
    },
    {
      "epoch": 4.854819976771196,
      "grad_norm": 19.158588409423828,
      "learning_rate": 5.716866692476449e-06,
      "loss": 0.6195,
      "step": 12540
    },
    {
      "epoch": 4.855207123499806,
      "grad_norm": 142.54339599609375,
      "learning_rate": 5.71643652944466e-06,
      "loss": 1.7292,
      "step": 12541
    },
    {
      "epoch": 4.8555942702284165,
      "grad_norm": 38.91197204589844,
      "learning_rate": 5.7160063664128705e-06,
      "loss": 0.8239,
      "step": 12542
    },
    {
      "epoch": 4.855981416957027,
      "grad_norm": 35.66181182861328,
      "learning_rate": 5.715576203381083e-06,
      "loss": 2.7786,
      "step": 12543
    },
    {
      "epoch": 4.856368563685637,
      "grad_norm": 46.3010368347168,
      "learning_rate": 5.715146040349293e-06,
      "loss": 0.3935,
      "step": 12544
    },
    {
      "epoch": 4.856755710414247,
      "grad_norm": 66.0143814086914,
      "learning_rate": 5.714715877317504e-06,
      "loss": 0.952,
      "step": 12545
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 30.759685516357422,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.6187,
      "step": 12546
    },
    {
      "epoch": 4.857530003871467,
      "grad_norm": 51.230003356933594,
      "learning_rate": 5.713855551253926e-06,
      "loss": 1.1502,
      "step": 12547
    },
    {
      "epoch": 4.857917150600078,
      "grad_norm": 7.228639602661133,
      "learning_rate": 5.7134253882221365e-06,
      "loss": 0.2644,
      "step": 12548
    },
    {
      "epoch": 4.858304297328687,
      "grad_norm": 18.872974395751953,
      "learning_rate": 5.712995225190348e-06,
      "loss": 1.7082,
      "step": 12549
    },
    {
      "epoch": 4.858691444057298,
      "grad_norm": 83.85454559326172,
      "learning_rate": 5.7125650621585585e-06,
      "loss": 1.1722,
      "step": 12550
    },
    {
      "epoch": 4.8590785907859075,
      "grad_norm": 19.099130630493164,
      "learning_rate": 5.71213489912677e-06,
      "loss": 0.9213,
      "step": 12551
    },
    {
      "epoch": 4.859465737514518,
      "grad_norm": 119.50785827636719,
      "learning_rate": 5.7117047360949805e-06,
      "loss": 0.4493,
      "step": 12552
    },
    {
      "epoch": 4.859852884243129,
      "grad_norm": 29.982717514038086,
      "learning_rate": 5.711274573063191e-06,
      "loss": 1.0261,
      "step": 12553
    },
    {
      "epoch": 4.860240030971738,
      "grad_norm": 25.84817886352539,
      "learning_rate": 5.710844410031402e-06,
      "loss": 1.2412,
      "step": 12554
    },
    {
      "epoch": 4.860627177700349,
      "grad_norm": 66.91233825683594,
      "learning_rate": 5.710414246999614e-06,
      "loss": 0.8056,
      "step": 12555
    },
    {
      "epoch": 4.861014324428958,
      "grad_norm": 29.392501831054688,
      "learning_rate": 5.7099840839678244e-06,
      "loss": 1.4878,
      "step": 12556
    },
    {
      "epoch": 4.861401471157569,
      "grad_norm": 49.46519470214844,
      "learning_rate": 5.709553920936035e-06,
      "loss": 1.376,
      "step": 12557
    },
    {
      "epoch": 4.861788617886178,
      "grad_norm": 12.80677318572998,
      "learning_rate": 5.7091237579042456e-06,
      "loss": 0.9567,
      "step": 12558
    },
    {
      "epoch": 4.862175764614789,
      "grad_norm": 50.122737884521484,
      "learning_rate": 5.708693594872458e-06,
      "loss": 1.1387,
      "step": 12559
    },
    {
      "epoch": 4.8625629113433995,
      "grad_norm": 0.8884328603744507,
      "learning_rate": 5.708263431840668e-06,
      "loss": 0.0276,
      "step": 12560
    },
    {
      "epoch": 4.862950058072009,
      "grad_norm": 35.09355545043945,
      "learning_rate": 5.707833268808879e-06,
      "loss": 1.6904,
      "step": 12561
    },
    {
      "epoch": 4.86333720480062,
      "grad_norm": 15.82169246673584,
      "learning_rate": 5.7074031057770895e-06,
      "loss": 0.4426,
      "step": 12562
    },
    {
      "epoch": 4.863724351529229,
      "grad_norm": 24.61711883544922,
      "learning_rate": 5.706972942745301e-06,
      "loss": 0.8109,
      "step": 12563
    },
    {
      "epoch": 4.86411149825784,
      "grad_norm": 18.907304763793945,
      "learning_rate": 5.706542779713512e-06,
      "loss": 1.4478,
      "step": 12564
    },
    {
      "epoch": 4.86449864498645,
      "grad_norm": 11.695353507995605,
      "learning_rate": 5.706112616681723e-06,
      "loss": 0.2879,
      "step": 12565
    },
    {
      "epoch": 4.86488579171506,
      "grad_norm": 15.867364883422852,
      "learning_rate": 5.7056824536499335e-06,
      "loss": 0.4999,
      "step": 12566
    },
    {
      "epoch": 4.86527293844367,
      "grad_norm": 57.67207336425781,
      "learning_rate": 5.705252290618145e-06,
      "loss": 0.5023,
      "step": 12567
    },
    {
      "epoch": 4.86566008517228,
      "grad_norm": 8.773740768432617,
      "learning_rate": 5.7048221275863555e-06,
      "loss": 0.306,
      "step": 12568
    },
    {
      "epoch": 4.8660472319008905,
      "grad_norm": 22.655120849609375,
      "learning_rate": 5.704391964554566e-06,
      "loss": 0.647,
      "step": 12569
    },
    {
      "epoch": 4.866434378629501,
      "grad_norm": 68.72604370117188,
      "learning_rate": 5.7039618015227775e-06,
      "loss": 1.272,
      "step": 12570
    },
    {
      "epoch": 4.866821525358111,
      "grad_norm": 56.65346145629883,
      "learning_rate": 5.703531638490989e-06,
      "loss": 1.7543,
      "step": 12571
    },
    {
      "epoch": 4.867208672086721,
      "grad_norm": 31.08297348022461,
      "learning_rate": 5.7031014754591995e-06,
      "loss": 0.6353,
      "step": 12572
    },
    {
      "epoch": 4.867595818815331,
      "grad_norm": 13.880928039550781,
      "learning_rate": 5.70267131242741e-06,
      "loss": 1.1553,
      "step": 12573
    },
    {
      "epoch": 4.867982965543941,
      "grad_norm": 33.797603607177734,
      "learning_rate": 5.702241149395621e-06,
      "loss": 0.6551,
      "step": 12574
    },
    {
      "epoch": 4.868370112272551,
      "grad_norm": 24.013368606567383,
      "learning_rate": 5.701810986363833e-06,
      "loss": 1.2022,
      "step": 12575
    },
    {
      "epoch": 4.868757259001161,
      "grad_norm": 229.20921325683594,
      "learning_rate": 5.7013808233320434e-06,
      "loss": 1.2219,
      "step": 12576
    },
    {
      "epoch": 4.869144405729772,
      "grad_norm": 26.127729415893555,
      "learning_rate": 5.700950660300254e-06,
      "loss": 1.0198,
      "step": 12577
    },
    {
      "epoch": 4.8695315524583815,
      "grad_norm": 80.81269836425781,
      "learning_rate": 5.7005204972684646e-06,
      "loss": 1.9304,
      "step": 12578
    },
    {
      "epoch": 4.869918699186992,
      "grad_norm": 16.009613037109375,
      "learning_rate": 5.700090334236677e-06,
      "loss": 1.1911,
      "step": 12579
    },
    {
      "epoch": 4.870305845915602,
      "grad_norm": 43.17491912841797,
      "learning_rate": 5.699660171204887e-06,
      "loss": 2.9896,
      "step": 12580
    },
    {
      "epoch": 4.870692992644212,
      "grad_norm": 9.00390625,
      "learning_rate": 5.699230008173098e-06,
      "loss": 0.4013,
      "step": 12581
    },
    {
      "epoch": 4.871080139372822,
      "grad_norm": 25.050968170166016,
      "learning_rate": 5.698799845141309e-06,
      "loss": 1.3624,
      "step": 12582
    },
    {
      "epoch": 4.871467286101432,
      "grad_norm": 14.495770454406738,
      "learning_rate": 5.69836968210952e-06,
      "loss": 0.9166,
      "step": 12583
    },
    {
      "epoch": 4.871854432830043,
      "grad_norm": 29.75502586364746,
      "learning_rate": 5.6979395190777305e-06,
      "loss": 0.6249,
      "step": 12584
    },
    {
      "epoch": 4.8722415795586524,
      "grad_norm": 17.488283157348633,
      "learning_rate": 5.697509356045942e-06,
      "loss": 0.5693,
      "step": 12585
    },
    {
      "epoch": 4.872628726287263,
      "grad_norm": 25.964679718017578,
      "learning_rate": 5.697079193014153e-06,
      "loss": 1.6309,
      "step": 12586
    },
    {
      "epoch": 4.8730158730158735,
      "grad_norm": 36.18635940551758,
      "learning_rate": 5.696649029982364e-06,
      "loss": 0.8672,
      "step": 12587
    },
    {
      "epoch": 4.873403019744483,
      "grad_norm": 49.30988693237305,
      "learning_rate": 5.6962188669505745e-06,
      "loss": 2.8777,
      "step": 12588
    },
    {
      "epoch": 4.873790166473094,
      "grad_norm": 88.38863372802734,
      "learning_rate": 5.695788703918785e-06,
      "loss": 2.0305,
      "step": 12589
    },
    {
      "epoch": 4.874177313201703,
      "grad_norm": 49.90822219848633,
      "learning_rate": 5.695358540886997e-06,
      "loss": 4.3011,
      "step": 12590
    },
    {
      "epoch": 4.874564459930314,
      "grad_norm": 17.66368865966797,
      "learning_rate": 5.694928377855208e-06,
      "loss": 1.1509,
      "step": 12591
    },
    {
      "epoch": 4.874951606658923,
      "grad_norm": 25.90961456298828,
      "learning_rate": 5.6944982148234185e-06,
      "loss": 0.6319,
      "step": 12592
    },
    {
      "epoch": 4.875338753387534,
      "grad_norm": 3.520096778869629,
      "learning_rate": 5.694068051791629e-06,
      "loss": 0.1764,
      "step": 12593
    },
    {
      "epoch": 4.875725900116144,
      "grad_norm": 71.07119750976562,
      "learning_rate": 5.693637888759841e-06,
      "loss": 1.0919,
      "step": 12594
    },
    {
      "epoch": 4.876113046844754,
      "grad_norm": 8.270998001098633,
      "learning_rate": 5.693207725728052e-06,
      "loss": 0.3544,
      "step": 12595
    },
    {
      "epoch": 4.8765001935733645,
      "grad_norm": 120.93077850341797,
      "learning_rate": 5.6927775626962624e-06,
      "loss": 2.8306,
      "step": 12596
    },
    {
      "epoch": 4.876887340301974,
      "grad_norm": 17.574155807495117,
      "learning_rate": 5.692347399664473e-06,
      "loss": 0.587,
      "step": 12597
    },
    {
      "epoch": 4.877274487030585,
      "grad_norm": 3.4728517532348633,
      "learning_rate": 5.691917236632684e-06,
      "loss": 0.0336,
      "step": 12598
    },
    {
      "epoch": 4.877661633759194,
      "grad_norm": 4.722323417663574,
      "learning_rate": 5.691487073600895e-06,
      "loss": 0.2403,
      "step": 12599
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 6.324079990386963,
      "learning_rate": 5.691056910569106e-06,
      "loss": 0.3738,
      "step": 12600
    },
    {
      "epoch": 4.878435927216415,
      "grad_norm": 57.47643280029297,
      "learning_rate": 5.690626747537317e-06,
      "loss": 1.6401,
      "step": 12601
    },
    {
      "epoch": 4.878823073945025,
      "grad_norm": 88.28369140625,
      "learning_rate": 5.690196584505528e-06,
      "loss": 0.9017,
      "step": 12602
    },
    {
      "epoch": 4.879210220673635,
      "grad_norm": 21.400455474853516,
      "learning_rate": 5.689766421473739e-06,
      "loss": 2.5573,
      "step": 12603
    },
    {
      "epoch": 4.879597367402246,
      "grad_norm": 14.721651077270508,
      "learning_rate": 5.6893362584419495e-06,
      "loss": 1.4012,
      "step": 12604
    },
    {
      "epoch": 4.8799845141308555,
      "grad_norm": 11.012696266174316,
      "learning_rate": 5.68890609541016e-06,
      "loss": 0.5222,
      "step": 12605
    },
    {
      "epoch": 4.880371660859466,
      "grad_norm": 33.21863555908203,
      "learning_rate": 5.688475932378372e-06,
      "loss": 0.9403,
      "step": 12606
    },
    {
      "epoch": 4.880758807588076,
      "grad_norm": 46.98252487182617,
      "learning_rate": 5.688045769346583e-06,
      "loss": 1.9739,
      "step": 12607
    },
    {
      "epoch": 4.881145954316686,
      "grad_norm": 78.11064147949219,
      "learning_rate": 5.6876156063147935e-06,
      "loss": 1.1893,
      "step": 12608
    },
    {
      "epoch": 4.881533101045296,
      "grad_norm": 24.6927490234375,
      "learning_rate": 5.687185443283004e-06,
      "loss": 1.511,
      "step": 12609
    },
    {
      "epoch": 4.881920247773906,
      "grad_norm": 17.2669620513916,
      "learning_rate": 5.686755280251216e-06,
      "loss": 0.6898,
      "step": 12610
    },
    {
      "epoch": 4.882307394502517,
      "grad_norm": 97.7496566772461,
      "learning_rate": 5.686325117219427e-06,
      "loss": 2.5678,
      "step": 12611
    },
    {
      "epoch": 4.8826945412311265,
      "grad_norm": 5.693286895751953,
      "learning_rate": 5.6858949541876375e-06,
      "loss": 0.251,
      "step": 12612
    },
    {
      "epoch": 4.883081687959737,
      "grad_norm": 9.078924179077148,
      "learning_rate": 5.685464791155848e-06,
      "loss": 0.4803,
      "step": 12613
    },
    {
      "epoch": 4.883468834688347,
      "grad_norm": 27.14005470275879,
      "learning_rate": 5.6850346281240594e-06,
      "loss": 0.7821,
      "step": 12614
    },
    {
      "epoch": 4.883855981416957,
      "grad_norm": 5.8584980964660645,
      "learning_rate": 5.684604465092271e-06,
      "loss": 0.2983,
      "step": 12615
    },
    {
      "epoch": 4.884243128145567,
      "grad_norm": 192.58372497558594,
      "learning_rate": 5.6841743020604814e-06,
      "loss": 1.3503,
      "step": 12616
    },
    {
      "epoch": 4.884630274874177,
      "grad_norm": 29.60891342163086,
      "learning_rate": 5.683744139028692e-06,
      "loss": 1.4761,
      "step": 12617
    },
    {
      "epoch": 4.885017421602788,
      "grad_norm": 43.50300598144531,
      "learning_rate": 5.683313975996903e-06,
      "loss": 1.3251,
      "step": 12618
    },
    {
      "epoch": 4.885404568331397,
      "grad_norm": 22.780628204345703,
      "learning_rate": 5.682883812965114e-06,
      "loss": 1.3393,
      "step": 12619
    },
    {
      "epoch": 4.885791715060008,
      "grad_norm": 28.324785232543945,
      "learning_rate": 5.6824536499333245e-06,
      "loss": 0.3551,
      "step": 12620
    },
    {
      "epoch": 4.886178861788618,
      "grad_norm": 7.624671459197998,
      "learning_rate": 5.682023486901536e-06,
      "loss": 0.2788,
      "step": 12621
    },
    {
      "epoch": 4.886566008517228,
      "grad_norm": 31.10127830505371,
      "learning_rate": 5.681593323869747e-06,
      "loss": 0.787,
      "step": 12622
    },
    {
      "epoch": 4.8869531552458385,
      "grad_norm": 41.61846923828125,
      "learning_rate": 5.681163160837958e-06,
      "loss": 2.386,
      "step": 12623
    },
    {
      "epoch": 4.887340301974448,
      "grad_norm": 54.827178955078125,
      "learning_rate": 5.6807329978061685e-06,
      "loss": 0.681,
      "step": 12624
    },
    {
      "epoch": 4.887727448703059,
      "grad_norm": 6.3616943359375,
      "learning_rate": 5.680302834774381e-06,
      "loss": 0.2194,
      "step": 12625
    },
    {
      "epoch": 4.888114595431668,
      "grad_norm": 66.05491638183594,
      "learning_rate": 5.679872671742591e-06,
      "loss": 0.987,
      "step": 12626
    },
    {
      "epoch": 4.888501742160279,
      "grad_norm": 26.935380935668945,
      "learning_rate": 5.679442508710802e-06,
      "loss": 0.461,
      "step": 12627
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 11.03026294708252,
      "learning_rate": 5.6790123456790125e-06,
      "loss": 0.715,
      "step": 12628
    },
    {
      "epoch": 4.889276035617499,
      "grad_norm": 75.24052429199219,
      "learning_rate": 5.678582182647224e-06,
      "loss": 0.6894,
      "step": 12629
    },
    {
      "epoch": 4.889663182346109,
      "grad_norm": 76.64582061767578,
      "learning_rate": 5.678152019615435e-06,
      "loss": 1.1273,
      "step": 12630
    },
    {
      "epoch": 4.890050329074719,
      "grad_norm": 2.6164700984954834,
      "learning_rate": 5.677721856583646e-06,
      "loss": 0.116,
      "step": 12631
    },
    {
      "epoch": 4.8904374758033295,
      "grad_norm": 25.289426803588867,
      "learning_rate": 5.6772916935518565e-06,
      "loss": 0.4058,
      "step": 12632
    },
    {
      "epoch": 4.890824622531939,
      "grad_norm": 17.152690887451172,
      "learning_rate": 5.676861530520068e-06,
      "loss": 1.3797,
      "step": 12633
    },
    {
      "epoch": 4.89121176926055,
      "grad_norm": 3.304508924484253,
      "learning_rate": 5.6764313674882784e-06,
      "loss": 0.1667,
      "step": 12634
    },
    {
      "epoch": 4.89159891598916,
      "grad_norm": 81.5141830444336,
      "learning_rate": 5.676001204456489e-06,
      "loss": 1.1247,
      "step": 12635
    },
    {
      "epoch": 4.89198606271777,
      "grad_norm": 53.073631286621094,
      "learning_rate": 5.6755710414247004e-06,
      "loss": 1.7669,
      "step": 12636
    },
    {
      "epoch": 4.89237320944638,
      "grad_norm": 14.496901512145996,
      "learning_rate": 5.675140878392912e-06,
      "loss": 0.919,
      "step": 12637
    },
    {
      "epoch": 4.89276035617499,
      "grad_norm": 27.034547805786133,
      "learning_rate": 5.674710715361122e-06,
      "loss": 1.7409,
      "step": 12638
    },
    {
      "epoch": 4.8931475029036005,
      "grad_norm": 6.278404235839844,
      "learning_rate": 5.674280552329333e-06,
      "loss": 0.3647,
      "step": 12639
    },
    {
      "epoch": 4.893534649632211,
      "grad_norm": 26.090312957763672,
      "learning_rate": 5.6738503892975435e-06,
      "loss": 0.5176,
      "step": 12640
    },
    {
      "epoch": 4.893921796360821,
      "grad_norm": 16.191970825195312,
      "learning_rate": 5.673420226265756e-06,
      "loss": 1.4312,
      "step": 12641
    },
    {
      "epoch": 4.894308943089431,
      "grad_norm": 93.34661102294922,
      "learning_rate": 5.672990063233966e-06,
      "loss": 1.2415,
      "step": 12642
    },
    {
      "epoch": 4.894696089818041,
      "grad_norm": 12.362457275390625,
      "learning_rate": 5.672559900202177e-06,
      "loss": 1.3765,
      "step": 12643
    },
    {
      "epoch": 4.895083236546651,
      "grad_norm": 86.609619140625,
      "learning_rate": 5.6721297371703875e-06,
      "loss": 3.7143,
      "step": 12644
    },
    {
      "epoch": 4.895470383275262,
      "grad_norm": 26.841859817504883,
      "learning_rate": 5.6716995741386e-06,
      "loss": 0.8991,
      "step": 12645
    },
    {
      "epoch": 4.895857530003871,
      "grad_norm": 6.768195152282715,
      "learning_rate": 5.67126941110681e-06,
      "loss": 0.1879,
      "step": 12646
    },
    {
      "epoch": 4.896244676732482,
      "grad_norm": 56.01664352416992,
      "learning_rate": 5.670839248075021e-06,
      "loss": 0.9236,
      "step": 12647
    },
    {
      "epoch": 4.8966318234610915,
      "grad_norm": 79.82615661621094,
      "learning_rate": 5.6704090850432315e-06,
      "loss": 3.1013,
      "step": 12648
    },
    {
      "epoch": 4.897018970189702,
      "grad_norm": 112.25098419189453,
      "learning_rate": 5.669978922011443e-06,
      "loss": 0.7258,
      "step": 12649
    },
    {
      "epoch": 4.897406116918312,
      "grad_norm": 16.037010192871094,
      "learning_rate": 5.6695487589796535e-06,
      "loss": 1.1406,
      "step": 12650
    },
    {
      "epoch": 4.897793263646922,
      "grad_norm": 38.848106384277344,
      "learning_rate": 5.669118595947865e-06,
      "loss": 0.9685,
      "step": 12651
    },
    {
      "epoch": 4.898180410375533,
      "grad_norm": 6.968995571136475,
      "learning_rate": 5.6686884329160755e-06,
      "loss": 0.4065,
      "step": 12652
    },
    {
      "epoch": 4.898567557104142,
      "grad_norm": 13.208630561828613,
      "learning_rate": 5.668258269884287e-06,
      "loss": 1.1364,
      "step": 12653
    },
    {
      "epoch": 4.898954703832753,
      "grad_norm": 32.95865249633789,
      "learning_rate": 5.6678281068524974e-06,
      "loss": 1.5259,
      "step": 12654
    },
    {
      "epoch": 4.899341850561362,
      "grad_norm": 17.97823143005371,
      "learning_rate": 5.667397943820708e-06,
      "loss": 1.2991,
      "step": 12655
    },
    {
      "epoch": 4.899728997289973,
      "grad_norm": 48.32929992675781,
      "learning_rate": 5.6669677807889186e-06,
      "loss": 2.241,
      "step": 12656
    },
    {
      "epoch": 4.900116144018583,
      "grad_norm": 24.47112464904785,
      "learning_rate": 5.666537617757131e-06,
      "loss": 1.0623,
      "step": 12657
    },
    {
      "epoch": 4.900503290747193,
      "grad_norm": 54.81578826904297,
      "learning_rate": 5.666107454725341e-06,
      "loss": 1.2037,
      "step": 12658
    },
    {
      "epoch": 4.9008904374758036,
      "grad_norm": 38.14301300048828,
      "learning_rate": 5.665677291693552e-06,
      "loss": 1.4702,
      "step": 12659
    },
    {
      "epoch": 4.901277584204413,
      "grad_norm": 19.431209564208984,
      "learning_rate": 5.6652471286617625e-06,
      "loss": 0.4155,
      "step": 12660
    },
    {
      "epoch": 4.901664730933024,
      "grad_norm": 42.8960075378418,
      "learning_rate": 5.664816965629975e-06,
      "loss": 0.7672,
      "step": 12661
    },
    {
      "epoch": 4.902051877661634,
      "grad_norm": 28.371902465820312,
      "learning_rate": 5.664386802598185e-06,
      "loss": 1.5817,
      "step": 12662
    },
    {
      "epoch": 4.902439024390244,
      "grad_norm": 114.98282623291016,
      "learning_rate": 5.663956639566396e-06,
      "loss": 1.3378,
      "step": 12663
    },
    {
      "epoch": 4.902826171118854,
      "grad_norm": 28.66115379333496,
      "learning_rate": 5.663526476534607e-06,
      "loss": 1.4017,
      "step": 12664
    },
    {
      "epoch": 4.903213317847464,
      "grad_norm": 43.03017807006836,
      "learning_rate": 5.663096313502818e-06,
      "loss": 1.3832,
      "step": 12665
    },
    {
      "epoch": 4.9036004645760745,
      "grad_norm": 79.2908706665039,
      "learning_rate": 5.662666150471029e-06,
      "loss": 1.0708,
      "step": 12666
    },
    {
      "epoch": 4.903987611304684,
      "grad_norm": 41.61177444458008,
      "learning_rate": 5.66223598743924e-06,
      "loss": 1.042,
      "step": 12667
    },
    {
      "epoch": 4.904374758033295,
      "grad_norm": 20.081483840942383,
      "learning_rate": 5.661805824407451e-06,
      "loss": 0.7153,
      "step": 12668
    },
    {
      "epoch": 4.904761904761905,
      "grad_norm": 20.953716278076172,
      "learning_rate": 5.661375661375662e-06,
      "loss": 0.7984,
      "step": 12669
    },
    {
      "epoch": 4.905149051490515,
      "grad_norm": 64.00006103515625,
      "learning_rate": 5.6609454983438725e-06,
      "loss": 1.1739,
      "step": 12670
    },
    {
      "epoch": 4.905536198219125,
      "grad_norm": 25.1825008392334,
      "learning_rate": 5.660515335312083e-06,
      "loss": 1.5358,
      "step": 12671
    },
    {
      "epoch": 4.905923344947735,
      "grad_norm": 55.462867736816406,
      "learning_rate": 5.660085172280295e-06,
      "loss": 0.7604,
      "step": 12672
    },
    {
      "epoch": 4.906310491676345,
      "grad_norm": 18.802188873291016,
      "learning_rate": 5.659655009248506e-06,
      "loss": 1.3606,
      "step": 12673
    },
    {
      "epoch": 4.906697638404955,
      "grad_norm": 68.56732177734375,
      "learning_rate": 5.6592248462167164e-06,
      "loss": 1.4482,
      "step": 12674
    },
    {
      "epoch": 4.9070847851335655,
      "grad_norm": 24.321657180786133,
      "learning_rate": 5.658794683184927e-06,
      "loss": 4.1163,
      "step": 12675
    },
    {
      "epoch": 4.907471931862176,
      "grad_norm": 33.99240493774414,
      "learning_rate": 5.658364520153139e-06,
      "loss": 1.0226,
      "step": 12676
    },
    {
      "epoch": 4.907859078590786,
      "grad_norm": 9.940974235534668,
      "learning_rate": 5.65793435712135e-06,
      "loss": 0.5618,
      "step": 12677
    },
    {
      "epoch": 4.908246225319396,
      "grad_norm": 30.789331436157227,
      "learning_rate": 5.65750419408956e-06,
      "loss": 1.5394,
      "step": 12678
    },
    {
      "epoch": 4.908633372048007,
      "grad_norm": 5.160893440246582,
      "learning_rate": 5.657074031057771e-06,
      "loss": 0.1957,
      "step": 12679
    },
    {
      "epoch": 4.909020518776616,
      "grad_norm": 129.8030242919922,
      "learning_rate": 5.656643868025982e-06,
      "loss": 1.058,
      "step": 12680
    },
    {
      "epoch": 4.909407665505227,
      "grad_norm": 32.96869659423828,
      "learning_rate": 5.656213704994194e-06,
      "loss": 0.8165,
      "step": 12681
    },
    {
      "epoch": 4.909794812233836,
      "grad_norm": 74.31271362304688,
      "learning_rate": 5.655783541962404e-06,
      "loss": 1.4987,
      "step": 12682
    },
    {
      "epoch": 4.910181958962447,
      "grad_norm": 40.66849899291992,
      "learning_rate": 5.655353378930615e-06,
      "loss": 0.6053,
      "step": 12683
    },
    {
      "epoch": 4.9105691056910565,
      "grad_norm": 64.57674407958984,
      "learning_rate": 5.654923215898826e-06,
      "loss": 0.827,
      "step": 12684
    },
    {
      "epoch": 4.910956252419667,
      "grad_norm": 46.01572036743164,
      "learning_rate": 5.654493052867037e-06,
      "loss": 3.7961,
      "step": 12685
    },
    {
      "epoch": 4.9113433991482776,
      "grad_norm": 33.82564926147461,
      "learning_rate": 5.6540628898352475e-06,
      "loss": 0.5004,
      "step": 12686
    },
    {
      "epoch": 4.911730545876887,
      "grad_norm": 46.32682418823242,
      "learning_rate": 5.653632726803459e-06,
      "loss": 1.1168,
      "step": 12687
    },
    {
      "epoch": 4.912117692605498,
      "grad_norm": 50.09412384033203,
      "learning_rate": 5.65320256377167e-06,
      "loss": 2.2791,
      "step": 12688
    },
    {
      "epoch": 4.912504839334107,
      "grad_norm": 8.470869064331055,
      "learning_rate": 5.652772400739881e-06,
      "loss": 0.43,
      "step": 12689
    },
    {
      "epoch": 4.912891986062718,
      "grad_norm": 84.18661499023438,
      "learning_rate": 5.6523422377080915e-06,
      "loss": 0.984,
      "step": 12690
    },
    {
      "epoch": 4.913279132791327,
      "grad_norm": 23.258378982543945,
      "learning_rate": 5.651912074676302e-06,
      "loss": 0.8573,
      "step": 12691
    },
    {
      "epoch": 4.913666279519938,
      "grad_norm": 42.96460723876953,
      "learning_rate": 5.651481911644514e-06,
      "loss": 1.9032,
      "step": 12692
    },
    {
      "epoch": 4.9140534262485485,
      "grad_norm": 7.514835834503174,
      "learning_rate": 5.651051748612725e-06,
      "loss": 0.3638,
      "step": 12693
    },
    {
      "epoch": 4.914440572977158,
      "grad_norm": 42.06791687011719,
      "learning_rate": 5.6506215855809354e-06,
      "loss": 0.5483,
      "step": 12694
    },
    {
      "epoch": 4.914827719705769,
      "grad_norm": 36.499908447265625,
      "learning_rate": 5.650191422549146e-06,
      "loss": 0.9804,
      "step": 12695
    },
    {
      "epoch": 4.915214866434379,
      "grad_norm": 50.70920181274414,
      "learning_rate": 5.649761259517358e-06,
      "loss": 1.1781,
      "step": 12696
    },
    {
      "epoch": 4.915602013162989,
      "grad_norm": 51.027191162109375,
      "learning_rate": 5.649331096485569e-06,
      "loss": 1.9375,
      "step": 12697
    },
    {
      "epoch": 4.915989159891599,
      "grad_norm": 56.420101165771484,
      "learning_rate": 5.648900933453779e-06,
      "loss": 1.4941,
      "step": 12698
    },
    {
      "epoch": 4.916376306620209,
      "grad_norm": 85.21713256835938,
      "learning_rate": 5.64847077042199e-06,
      "loss": 0.9894,
      "step": 12699
    },
    {
      "epoch": 4.916763453348819,
      "grad_norm": 38.06985092163086,
      "learning_rate": 5.648040607390201e-06,
      "loss": 1.1528,
      "step": 12700
    },
    {
      "epoch": 4.917150600077429,
      "grad_norm": 27.608877182006836,
      "learning_rate": 5.647610444358412e-06,
      "loss": 0.4709,
      "step": 12701
    },
    {
      "epoch": 4.9175377468060395,
      "grad_norm": 66.63957214355469,
      "learning_rate": 5.647180281326623e-06,
      "loss": 2.0627,
      "step": 12702
    },
    {
      "epoch": 4.91792489353465,
      "grad_norm": 40.48028564453125,
      "learning_rate": 5.646750118294834e-06,
      "loss": 3.4027,
      "step": 12703
    },
    {
      "epoch": 4.91831204026326,
      "grad_norm": 26.852121353149414,
      "learning_rate": 5.646319955263045e-06,
      "loss": 1.6266,
      "step": 12704
    },
    {
      "epoch": 4.91869918699187,
      "grad_norm": 53.845584869384766,
      "learning_rate": 5.645889792231256e-06,
      "loss": 1.6643,
      "step": 12705
    },
    {
      "epoch": 4.91908633372048,
      "grad_norm": 18.15450096130371,
      "learning_rate": 5.6454596291994665e-06,
      "loss": 1.9544,
      "step": 12706
    },
    {
      "epoch": 4.91947348044909,
      "grad_norm": 25.016300201416016,
      "learning_rate": 5.645029466167679e-06,
      "loss": 0.3647,
      "step": 12707
    },
    {
      "epoch": 4.9198606271777,
      "grad_norm": 19.76585578918457,
      "learning_rate": 5.644599303135889e-06,
      "loss": 1.4804,
      "step": 12708
    },
    {
      "epoch": 4.92024777390631,
      "grad_norm": 65.73553466796875,
      "learning_rate": 5.6441691401041e-06,
      "loss": 3.5211,
      "step": 12709
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 33.378997802734375,
      "learning_rate": 5.6437389770723105e-06,
      "loss": 0.649,
      "step": 12710
    },
    {
      "epoch": 4.9210220673635305,
      "grad_norm": 27.186870574951172,
      "learning_rate": 5.643308814040523e-06,
      "loss": 0.8152,
      "step": 12711
    },
    {
      "epoch": 4.921409214092141,
      "grad_norm": 19.29619026184082,
      "learning_rate": 5.642878651008733e-06,
      "loss": 0.5757,
      "step": 12712
    },
    {
      "epoch": 4.921796360820752,
      "grad_norm": 42.478790283203125,
      "learning_rate": 5.642448487976944e-06,
      "loss": 0.3795,
      "step": 12713
    },
    {
      "epoch": 4.922183507549361,
      "grad_norm": 28.04924964904785,
      "learning_rate": 5.6420183249451544e-06,
      "loss": 1.4271,
      "step": 12714
    },
    {
      "epoch": 4.922570654277972,
      "grad_norm": 5.50578498840332,
      "learning_rate": 5.641588161913366e-06,
      "loss": 0.1801,
      "step": 12715
    },
    {
      "epoch": 4.922957801006581,
      "grad_norm": 71.42839813232422,
      "learning_rate": 5.641157998881576e-06,
      "loss": 1.8015,
      "step": 12716
    },
    {
      "epoch": 4.923344947735192,
      "grad_norm": 10.745810508728027,
      "learning_rate": 5.640727835849788e-06,
      "loss": 0.5689,
      "step": 12717
    },
    {
      "epoch": 4.9237320944638014,
      "grad_norm": 20.376625061035156,
      "learning_rate": 5.640297672817998e-06,
      "loss": 0.894,
      "step": 12718
    },
    {
      "epoch": 4.924119241192412,
      "grad_norm": 68.39710235595703,
      "learning_rate": 5.63986750978621e-06,
      "loss": 2.5072,
      "step": 12719
    },
    {
      "epoch": 4.9245063879210225,
      "grad_norm": 49.31922149658203,
      "learning_rate": 5.63943734675442e-06,
      "loss": 1.5718,
      "step": 12720
    },
    {
      "epoch": 4.924893534649632,
      "grad_norm": 9.298139572143555,
      "learning_rate": 5.639007183722631e-06,
      "loss": 0.484,
      "step": 12721
    },
    {
      "epoch": 4.925280681378243,
      "grad_norm": 7.176650047302246,
      "learning_rate": 5.6385770206908415e-06,
      "loss": 0.3622,
      "step": 12722
    },
    {
      "epoch": 4.925667828106852,
      "grad_norm": 11.662961959838867,
      "learning_rate": 5.638146857659054e-06,
      "loss": 0.4992,
      "step": 12723
    },
    {
      "epoch": 4.926054974835463,
      "grad_norm": 52.36745834350586,
      "learning_rate": 5.637716694627264e-06,
      "loss": 0.6028,
      "step": 12724
    },
    {
      "epoch": 4.926442121564072,
      "grad_norm": 83.56307220458984,
      "learning_rate": 5.637286531595475e-06,
      "loss": 2.8703,
      "step": 12725
    },
    {
      "epoch": 4.926829268292683,
      "grad_norm": 17.974306106567383,
      "learning_rate": 5.6368563685636855e-06,
      "loss": 0.482,
      "step": 12726
    },
    {
      "epoch": 4.927216415021293,
      "grad_norm": 72.84539794921875,
      "learning_rate": 5.636426205531898e-06,
      "loss": 0.9976,
      "step": 12727
    },
    {
      "epoch": 4.927603561749903,
      "grad_norm": 25.009578704833984,
      "learning_rate": 5.635996042500108e-06,
      "loss": 1.147,
      "step": 12728
    },
    {
      "epoch": 4.9279907084785135,
      "grad_norm": 7.975183486938477,
      "learning_rate": 5.635565879468319e-06,
      "loss": 0.2337,
      "step": 12729
    },
    {
      "epoch": 4.928377855207123,
      "grad_norm": 8.212702751159668,
      "learning_rate": 5.6351357164365295e-06,
      "loss": 0.3902,
      "step": 12730
    },
    {
      "epoch": 4.928765001935734,
      "grad_norm": 21.080228805541992,
      "learning_rate": 5.634705553404741e-06,
      "loss": 0.6855,
      "step": 12731
    },
    {
      "epoch": 4.929152148664344,
      "grad_norm": 109.03578186035156,
      "learning_rate": 5.634275390372952e-06,
      "loss": 1.903,
      "step": 12732
    },
    {
      "epoch": 4.929539295392954,
      "grad_norm": 16.471696853637695,
      "learning_rate": 5.633845227341163e-06,
      "loss": 0.479,
      "step": 12733
    },
    {
      "epoch": 4.929926442121564,
      "grad_norm": 34.86863327026367,
      "learning_rate": 5.6334150643093734e-06,
      "loss": 0.7641,
      "step": 12734
    },
    {
      "epoch": 4.930313588850174,
      "grad_norm": 7.492657661437988,
      "learning_rate": 5.632984901277585e-06,
      "loss": 0.3604,
      "step": 12735
    },
    {
      "epoch": 4.930700735578784,
      "grad_norm": 5.292661190032959,
      "learning_rate": 5.632554738245795e-06,
      "loss": 0.1815,
      "step": 12736
    },
    {
      "epoch": 4.931087882307395,
      "grad_norm": 66.31151580810547,
      "learning_rate": 5.632124575214006e-06,
      "loss": 0.9824,
      "step": 12737
    },
    {
      "epoch": 4.9314750290360045,
      "grad_norm": 29.914936065673828,
      "learning_rate": 5.631694412182217e-06,
      "loss": 1.8735,
      "step": 12738
    },
    {
      "epoch": 4.931862175764615,
      "grad_norm": 34.83546447753906,
      "learning_rate": 5.631264249150429e-06,
      "loss": 2.3186,
      "step": 12739
    },
    {
      "epoch": 4.932249322493225,
      "grad_norm": 29.481847763061523,
      "learning_rate": 5.630834086118639e-06,
      "loss": 2.7965,
      "step": 12740
    },
    {
      "epoch": 4.932636469221835,
      "grad_norm": 61.06161117553711,
      "learning_rate": 5.63040392308685e-06,
      "loss": 0.8494,
      "step": 12741
    },
    {
      "epoch": 4.933023615950445,
      "grad_norm": 26.3106632232666,
      "learning_rate": 5.6299737600550605e-06,
      "loss": 0.9221,
      "step": 12742
    },
    {
      "epoch": 4.933410762679055,
      "grad_norm": 18.93631935119629,
      "learning_rate": 5.629543597023273e-06,
      "loss": 1.2929,
      "step": 12743
    },
    {
      "epoch": 4.933797909407666,
      "grad_norm": 39.348506927490234,
      "learning_rate": 5.629113433991483e-06,
      "loss": 0.9279,
      "step": 12744
    },
    {
      "epoch": 4.9341850561362754,
      "grad_norm": 84.01707458496094,
      "learning_rate": 5.628683270959694e-06,
      "loss": 2.8629,
      "step": 12745
    },
    {
      "epoch": 4.934572202864886,
      "grad_norm": 36.92819595336914,
      "learning_rate": 5.628253107927905e-06,
      "loss": 0.4418,
      "step": 12746
    },
    {
      "epoch": 4.934959349593496,
      "grad_norm": 51.92308807373047,
      "learning_rate": 5.627822944896117e-06,
      "loss": 3.1052,
      "step": 12747
    },
    {
      "epoch": 4.935346496322106,
      "grad_norm": 81.25456237792969,
      "learning_rate": 5.627392781864327e-06,
      "loss": 1.9736,
      "step": 12748
    },
    {
      "epoch": 4.935733643050717,
      "grad_norm": 25.48288345336914,
      "learning_rate": 5.626962618832538e-06,
      "loss": 0.89,
      "step": 12749
    },
    {
      "epoch": 4.936120789779326,
      "grad_norm": 31.502283096313477,
      "learning_rate": 5.626532455800749e-06,
      "loss": 0.6229,
      "step": 12750
    },
    {
      "epoch": 4.936507936507937,
      "grad_norm": 35.74083709716797,
      "learning_rate": 5.62610229276896e-06,
      "loss": 1.1659,
      "step": 12751
    },
    {
      "epoch": 4.936895083236546,
      "grad_norm": 45.91056823730469,
      "learning_rate": 5.6256721297371704e-06,
      "loss": 0.5843,
      "step": 12752
    },
    {
      "epoch": 4.937282229965157,
      "grad_norm": 51.864959716796875,
      "learning_rate": 5.625241966705382e-06,
      "loss": 0.6069,
      "step": 12753
    },
    {
      "epoch": 4.937669376693767,
      "grad_norm": 41.58525085449219,
      "learning_rate": 5.624811803673593e-06,
      "loss": 1.9253,
      "step": 12754
    },
    {
      "epoch": 4.938056523422377,
      "grad_norm": 19.05158042907715,
      "learning_rate": 5.624381640641804e-06,
      "loss": 1.1239,
      "step": 12755
    },
    {
      "epoch": 4.9384436701509875,
      "grad_norm": 138.03883361816406,
      "learning_rate": 5.623951477610014e-06,
      "loss": 1.1318,
      "step": 12756
    },
    {
      "epoch": 4.938830816879597,
      "grad_norm": 24.770877838134766,
      "learning_rate": 5.623521314578225e-06,
      "loss": 0.7162,
      "step": 12757
    },
    {
      "epoch": 4.939217963608208,
      "grad_norm": 26.696979522705078,
      "learning_rate": 5.623091151546437e-06,
      "loss": 1.5472,
      "step": 12758
    },
    {
      "epoch": 4.939605110336817,
      "grad_norm": 88.99864959716797,
      "learning_rate": 5.622660988514648e-06,
      "loss": 0.721,
      "step": 12759
    },
    {
      "epoch": 4.939992257065428,
      "grad_norm": 25.125102996826172,
      "learning_rate": 5.622230825482858e-06,
      "loss": 1.2867,
      "step": 12760
    },
    {
      "epoch": 4.940379403794038,
      "grad_norm": 67.30921936035156,
      "learning_rate": 5.621800662451069e-06,
      "loss": 1.7324,
      "step": 12761
    },
    {
      "epoch": 4.940766550522648,
      "grad_norm": 40.377845764160156,
      "learning_rate": 5.621370499419281e-06,
      "loss": 0.9851,
      "step": 12762
    },
    {
      "epoch": 4.941153697251258,
      "grad_norm": 13.962233543395996,
      "learning_rate": 5.620940336387492e-06,
      "loss": 0.7239,
      "step": 12763
    },
    {
      "epoch": 4.941540843979868,
      "grad_norm": 23.15782928466797,
      "learning_rate": 5.620510173355702e-06,
      "loss": 0.7032,
      "step": 12764
    },
    {
      "epoch": 4.9419279907084785,
      "grad_norm": 3.067399740219116,
      "learning_rate": 5.620080010323913e-06,
      "loss": 0.1439,
      "step": 12765
    },
    {
      "epoch": 4.942315137437088,
      "grad_norm": 15.745274543762207,
      "learning_rate": 5.619649847292124e-06,
      "loss": 1.1778,
      "step": 12766
    },
    {
      "epoch": 4.942702284165699,
      "grad_norm": 36.8845100402832,
      "learning_rate": 5.619219684260335e-06,
      "loss": 1.6485,
      "step": 12767
    },
    {
      "epoch": 4.943089430894309,
      "grad_norm": 25.26726722717285,
      "learning_rate": 5.618789521228546e-06,
      "loss": 2.4658,
      "step": 12768
    },
    {
      "epoch": 4.943476577622919,
      "grad_norm": 22.801420211791992,
      "learning_rate": 5.618359358196757e-06,
      "loss": 0.6302,
      "step": 12769
    },
    {
      "epoch": 4.943863724351529,
      "grad_norm": 81.62080383300781,
      "learning_rate": 5.617929195164968e-06,
      "loss": 1.9497,
      "step": 12770
    },
    {
      "epoch": 4.94425087108014,
      "grad_norm": 19.928335189819336,
      "learning_rate": 5.617499032133179e-06,
      "loss": 1.3867,
      "step": 12771
    },
    {
      "epoch": 4.9446380178087495,
      "grad_norm": 23.145450592041016,
      "learning_rate": 5.6170688691013894e-06,
      "loss": 1.6242,
      "step": 12772
    },
    {
      "epoch": 4.94502516453736,
      "grad_norm": 40.36774826049805,
      "learning_rate": 5.6166387060696e-06,
      "loss": 1.3256,
      "step": 12773
    },
    {
      "epoch": 4.94541231126597,
      "grad_norm": 23.4080810546875,
      "learning_rate": 5.616208543037812e-06,
      "loss": 1.1214,
      "step": 12774
    },
    {
      "epoch": 4.94579945799458,
      "grad_norm": 54.69173049926758,
      "learning_rate": 5.615778380006023e-06,
      "loss": 1.6721,
      "step": 12775
    },
    {
      "epoch": 4.94618660472319,
      "grad_norm": 8.154743194580078,
      "learning_rate": 5.615348216974233e-06,
      "loss": 0.4564,
      "step": 12776
    },
    {
      "epoch": 4.9465737514518,
      "grad_norm": 58.932640075683594,
      "learning_rate": 5.614918053942444e-06,
      "loss": 1.2098,
      "step": 12777
    },
    {
      "epoch": 4.946960898180411,
      "grad_norm": 61.51239013671875,
      "learning_rate": 5.614487890910656e-06,
      "loss": 1.1786,
      "step": 12778
    },
    {
      "epoch": 4.94734804490902,
      "grad_norm": 30.271596908569336,
      "learning_rate": 5.614057727878867e-06,
      "loss": 1.0148,
      "step": 12779
    },
    {
      "epoch": 4.947735191637631,
      "grad_norm": 32.65439987182617,
      "learning_rate": 5.613627564847077e-06,
      "loss": 3.0335,
      "step": 12780
    },
    {
      "epoch": 4.9481223383662405,
      "grad_norm": 49.02067947387695,
      "learning_rate": 5.613197401815288e-06,
      "loss": 2.4798,
      "step": 12781
    },
    {
      "epoch": 4.948509485094851,
      "grad_norm": 86.66339111328125,
      "learning_rate": 5.612767238783499e-06,
      "loss": 1.6928,
      "step": 12782
    },
    {
      "epoch": 4.948896631823461,
      "grad_norm": 24.349756240844727,
      "learning_rate": 5.612337075751711e-06,
      "loss": 0.9351,
      "step": 12783
    },
    {
      "epoch": 4.949283778552071,
      "grad_norm": 9.296996116638184,
      "learning_rate": 5.611906912719921e-06,
      "loss": 0.3117,
      "step": 12784
    },
    {
      "epoch": 4.949670925280682,
      "grad_norm": 36.56243133544922,
      "learning_rate": 5.611476749688132e-06,
      "loss": 2.337,
      "step": 12785
    },
    {
      "epoch": 4.950058072009291,
      "grad_norm": 17.4717960357666,
      "learning_rate": 5.611046586656343e-06,
      "loss": 1.1419,
      "step": 12786
    },
    {
      "epoch": 4.950445218737902,
      "grad_norm": 15.075321197509766,
      "learning_rate": 5.610616423624554e-06,
      "loss": 0.4367,
      "step": 12787
    },
    {
      "epoch": 4.950832365466512,
      "grad_norm": 14.016112327575684,
      "learning_rate": 5.6101862605927645e-06,
      "loss": 0.6452,
      "step": 12788
    },
    {
      "epoch": 4.951219512195122,
      "grad_norm": 4.888169288635254,
      "learning_rate": 5.609756097560977e-06,
      "loss": 0.2177,
      "step": 12789
    },
    {
      "epoch": 4.951606658923732,
      "grad_norm": 20.884109497070312,
      "learning_rate": 5.609325934529187e-06,
      "loss": 1.432,
      "step": 12790
    },
    {
      "epoch": 4.951993805652342,
      "grad_norm": 26.795568466186523,
      "learning_rate": 5.608895771497398e-06,
      "loss": 1.8489,
      "step": 12791
    },
    {
      "epoch": 4.9523809523809526,
      "grad_norm": 27.254146575927734,
      "learning_rate": 5.6084656084656084e-06,
      "loss": 0.7161,
      "step": 12792
    },
    {
      "epoch": 4.952768099109562,
      "grad_norm": 15.159241676330566,
      "learning_rate": 5.608035445433821e-06,
      "loss": 0.107,
      "step": 12793
    },
    {
      "epoch": 4.953155245838173,
      "grad_norm": 114.89691162109375,
      "learning_rate": 5.607605282402031e-06,
      "loss": 0.638,
      "step": 12794
    },
    {
      "epoch": 4.953542392566783,
      "grad_norm": 58.42898941040039,
      "learning_rate": 5.607175119370242e-06,
      "loss": 1.2387,
      "step": 12795
    },
    {
      "epoch": 4.953929539295393,
      "grad_norm": 32.46675109863281,
      "learning_rate": 5.606744956338452e-06,
      "loss": 1.0544,
      "step": 12796
    },
    {
      "epoch": 4.954316686024003,
      "grad_norm": 37.934661865234375,
      "learning_rate": 5.606314793306664e-06,
      "loss": 2.0082,
      "step": 12797
    },
    {
      "epoch": 4.954703832752613,
      "grad_norm": 33.79077911376953,
      "learning_rate": 5.605884630274875e-06,
      "loss": 3.048,
      "step": 12798
    },
    {
      "epoch": 4.9550909794812235,
      "grad_norm": 66.84876251220703,
      "learning_rate": 5.605454467243086e-06,
      "loss": 1.8162,
      "step": 12799
    },
    {
      "epoch": 4.955478126209833,
      "grad_norm": 27.376420974731445,
      "learning_rate": 5.605024304211296e-06,
      "loss": 2.2282,
      "step": 12800
    },
    {
      "epoch": 4.955865272938444,
      "grad_norm": 72.21644592285156,
      "learning_rate": 5.604594141179508e-06,
      "loss": 2.4206,
      "step": 12801
    },
    {
      "epoch": 4.956252419667054,
      "grad_norm": 68.65910339355469,
      "learning_rate": 5.604163978147718e-06,
      "loss": 3.3881,
      "step": 12802
    },
    {
      "epoch": 4.956639566395664,
      "grad_norm": 9.588908195495605,
      "learning_rate": 5.603733815115929e-06,
      "loss": 0.5058,
      "step": 12803
    },
    {
      "epoch": 4.957026713124274,
      "grad_norm": 64.340087890625,
      "learning_rate": 5.60330365208414e-06,
      "loss": 1.3584,
      "step": 12804
    },
    {
      "epoch": 4.957413859852885,
      "grad_norm": 31.110958099365234,
      "learning_rate": 5.602873489052352e-06,
      "loss": 1.4165,
      "step": 12805
    },
    {
      "epoch": 4.957801006581494,
      "grad_norm": 59.46518325805664,
      "learning_rate": 5.602443326020562e-06,
      "loss": 0.6892,
      "step": 12806
    },
    {
      "epoch": 4.958188153310105,
      "grad_norm": 107.15930938720703,
      "learning_rate": 5.602013162988773e-06,
      "loss": 0.5273,
      "step": 12807
    },
    {
      "epoch": 4.9585753000387145,
      "grad_norm": 27.7197322845459,
      "learning_rate": 5.6015829999569835e-06,
      "loss": 2.4631,
      "step": 12808
    },
    {
      "epoch": 4.958962446767325,
      "grad_norm": 42.94179153442383,
      "learning_rate": 5.601152836925196e-06,
      "loss": 1.6052,
      "step": 12809
    },
    {
      "epoch": 4.959349593495935,
      "grad_norm": 42.433876037597656,
      "learning_rate": 5.600722673893406e-06,
      "loss": 1.6611,
      "step": 12810
    },
    {
      "epoch": 4.959736740224545,
      "grad_norm": 15.187748908996582,
      "learning_rate": 5.600292510861617e-06,
      "loss": 1.3274,
      "step": 12811
    },
    {
      "epoch": 4.960123886953156,
      "grad_norm": 11.682854652404785,
      "learning_rate": 5.5998623478298274e-06,
      "loss": 1.1449,
      "step": 12812
    },
    {
      "epoch": 4.960511033681765,
      "grad_norm": 39.50310134887695,
      "learning_rate": 5.59943218479804e-06,
      "loss": 1.1913,
      "step": 12813
    },
    {
      "epoch": 4.960898180410376,
      "grad_norm": 10.800460815429688,
      "learning_rate": 5.59900202176625e-06,
      "loss": 0.3979,
      "step": 12814
    },
    {
      "epoch": 4.961285327138985,
      "grad_norm": 16.184782028198242,
      "learning_rate": 5.598571858734461e-06,
      "loss": 0.2977,
      "step": 12815
    },
    {
      "epoch": 4.961672473867596,
      "grad_norm": 36.27135467529297,
      "learning_rate": 5.598141695702671e-06,
      "loss": 0.7027,
      "step": 12816
    },
    {
      "epoch": 4.9620596205962055,
      "grad_norm": 17.52155876159668,
      "learning_rate": 5.597711532670883e-06,
      "loss": 0.6817,
      "step": 12817
    },
    {
      "epoch": 4.962446767324816,
      "grad_norm": 36.635887145996094,
      "learning_rate": 5.597281369639093e-06,
      "loss": 0.6667,
      "step": 12818
    },
    {
      "epoch": 4.9628339140534266,
      "grad_norm": 12.717644691467285,
      "learning_rate": 5.596851206607305e-06,
      "loss": 0.4684,
      "step": 12819
    },
    {
      "epoch": 4.963221060782036,
      "grad_norm": 23.493480682373047,
      "learning_rate": 5.596421043575515e-06,
      "loss": 0.7643,
      "step": 12820
    },
    {
      "epoch": 4.963608207510647,
      "grad_norm": 18.929119110107422,
      "learning_rate": 5.595990880543727e-06,
      "loss": 0.4726,
      "step": 12821
    },
    {
      "epoch": 4.963995354239256,
      "grad_norm": 5.7144975662231445,
      "learning_rate": 5.595560717511937e-06,
      "loss": 0.2347,
      "step": 12822
    },
    {
      "epoch": 4.964382500967867,
      "grad_norm": 21.36774253845215,
      "learning_rate": 5.595130554480148e-06,
      "loss": 1.7836,
      "step": 12823
    },
    {
      "epoch": 4.964769647696477,
      "grad_norm": 4.315324783325195,
      "learning_rate": 5.5947003914483585e-06,
      "loss": 0.1609,
      "step": 12824
    },
    {
      "epoch": 4.965156794425087,
      "grad_norm": 51.17826843261719,
      "learning_rate": 5.594270228416571e-06,
      "loss": 2.6619,
      "step": 12825
    },
    {
      "epoch": 4.9655439411536975,
      "grad_norm": 25.947738647460938,
      "learning_rate": 5.593840065384781e-06,
      "loss": 0.7987,
      "step": 12826
    },
    {
      "epoch": 4.965931087882307,
      "grad_norm": 7.130546569824219,
      "learning_rate": 5.593409902352992e-06,
      "loss": 0.3697,
      "step": 12827
    },
    {
      "epoch": 4.966318234610918,
      "grad_norm": 100.58507537841797,
      "learning_rate": 5.5929797393212025e-06,
      "loss": 0.7571,
      "step": 12828
    },
    {
      "epoch": 4.966705381339528,
      "grad_norm": 26.018413543701172,
      "learning_rate": 5.592549576289415e-06,
      "loss": 1.621,
      "step": 12829
    },
    {
      "epoch": 4.967092528068138,
      "grad_norm": 19.176015853881836,
      "learning_rate": 5.592119413257625e-06,
      "loss": 1.5912,
      "step": 12830
    },
    {
      "epoch": 4.967479674796748,
      "grad_norm": 42.26395797729492,
      "learning_rate": 5.591689250225836e-06,
      "loss": 1.0136,
      "step": 12831
    },
    {
      "epoch": 4.967866821525358,
      "grad_norm": 32.27484893798828,
      "learning_rate": 5.591259087194047e-06,
      "loss": 1.522,
      "step": 12832
    },
    {
      "epoch": 4.968253968253968,
      "grad_norm": 18.654024124145508,
      "learning_rate": 5.590828924162258e-06,
      "loss": 1.2706,
      "step": 12833
    },
    {
      "epoch": 4.968641114982578,
      "grad_norm": 37.59346389770508,
      "learning_rate": 5.590398761130469e-06,
      "loss": 1.199,
      "step": 12834
    },
    {
      "epoch": 4.9690282617111885,
      "grad_norm": 10.467734336853027,
      "learning_rate": 5.58996859809868e-06,
      "loss": 1.3462,
      "step": 12835
    },
    {
      "epoch": 4.969415408439799,
      "grad_norm": 20.34497833251953,
      "learning_rate": 5.589538435066891e-06,
      "loss": 2.5307,
      "step": 12836
    },
    {
      "epoch": 4.969802555168409,
      "grad_norm": 14.510403633117676,
      "learning_rate": 5.589108272035102e-06,
      "loss": 0.5163,
      "step": 12837
    },
    {
      "epoch": 4.970189701897019,
      "grad_norm": 45.41133499145508,
      "learning_rate": 5.588678109003312e-06,
      "loss": 1.9769,
      "step": 12838
    },
    {
      "epoch": 4.970576848625629,
      "grad_norm": 7.438778877258301,
      "learning_rate": 5.588247945971523e-06,
      "loss": 0.4975,
      "step": 12839
    },
    {
      "epoch": 4.970963995354239,
      "grad_norm": 51.191925048828125,
      "learning_rate": 5.587817782939735e-06,
      "loss": 1.7382,
      "step": 12840
    },
    {
      "epoch": 4.97135114208285,
      "grad_norm": 18.125410079956055,
      "learning_rate": 5.587387619907946e-06,
      "loss": 1.2456,
      "step": 12841
    },
    {
      "epoch": 4.971738288811459,
      "grad_norm": 34.824378967285156,
      "learning_rate": 5.586957456876156e-06,
      "loss": 1.2636,
      "step": 12842
    },
    {
      "epoch": 4.97212543554007,
      "grad_norm": 18.11801528930664,
      "learning_rate": 5.586527293844367e-06,
      "loss": 0.7939,
      "step": 12843
    },
    {
      "epoch": 4.9725125822686795,
      "grad_norm": 26.530946731567383,
      "learning_rate": 5.586097130812579e-06,
      "loss": 1.3206,
      "step": 12844
    },
    {
      "epoch": 4.97289972899729,
      "grad_norm": 15.39411449432373,
      "learning_rate": 5.58566696778079e-06,
      "loss": 0.7053,
      "step": 12845
    },
    {
      "epoch": 4.973286875725901,
      "grad_norm": 15.3857421875,
      "learning_rate": 5.585236804749e-06,
      "loss": 1.3669,
      "step": 12846
    },
    {
      "epoch": 4.97367402245451,
      "grad_norm": 76.20702362060547,
      "learning_rate": 5.584806641717211e-06,
      "loss": 0.7475,
      "step": 12847
    },
    {
      "epoch": 4.974061169183121,
      "grad_norm": 37.22661590576172,
      "learning_rate": 5.584376478685422e-06,
      "loss": 0.8561,
      "step": 12848
    },
    {
      "epoch": 4.97444831591173,
      "grad_norm": 11.859295845031738,
      "learning_rate": 5.583946315653634e-06,
      "loss": 0.2962,
      "step": 12849
    },
    {
      "epoch": 4.974835462640341,
      "grad_norm": 90.00789642333984,
      "learning_rate": 5.583516152621844e-06,
      "loss": 2.6038,
      "step": 12850
    },
    {
      "epoch": 4.97522260936895,
      "grad_norm": 41.3537712097168,
      "learning_rate": 5.583085989590055e-06,
      "loss": 1.9098,
      "step": 12851
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 4.970862865447998,
      "learning_rate": 5.582655826558266e-06,
      "loss": 0.2315,
      "step": 12852
    },
    {
      "epoch": 4.9759969028261715,
      "grad_norm": 13.529546737670898,
      "learning_rate": 5.582225663526477e-06,
      "loss": 0.3281,
      "step": 12853
    },
    {
      "epoch": 4.976384049554781,
      "grad_norm": 141.01663208007812,
      "learning_rate": 5.581795500494687e-06,
      "loss": 2.6752,
      "step": 12854
    },
    {
      "epoch": 4.976771196283392,
      "grad_norm": 13.80823802947998,
      "learning_rate": 5.581365337462899e-06,
      "loss": 0.5862,
      "step": 12855
    },
    {
      "epoch": 4.977158343012001,
      "grad_norm": 36.07625198364258,
      "learning_rate": 5.58093517443111e-06,
      "loss": 1.0445,
      "step": 12856
    },
    {
      "epoch": 4.977545489740612,
      "grad_norm": 102.02806091308594,
      "learning_rate": 5.580505011399321e-06,
      "loss": 3.0222,
      "step": 12857
    },
    {
      "epoch": 4.977932636469221,
      "grad_norm": 51.53959655761719,
      "learning_rate": 5.580074848367531e-06,
      "loss": 1.56,
      "step": 12858
    },
    {
      "epoch": 4.978319783197832,
      "grad_norm": 7.335179328918457,
      "learning_rate": 5.579644685335742e-06,
      "loss": 0.4469,
      "step": 12859
    },
    {
      "epoch": 4.978706929926442,
      "grad_norm": 14.301985740661621,
      "learning_rate": 5.579214522303954e-06,
      "loss": 0.3254,
      "step": 12860
    },
    {
      "epoch": 4.979094076655052,
      "grad_norm": 41.07462692260742,
      "learning_rate": 5.578784359272165e-06,
      "loss": 1.819,
      "step": 12861
    },
    {
      "epoch": 4.9794812233836625,
      "grad_norm": 22.652103424072266,
      "learning_rate": 5.578354196240375e-06,
      "loss": 0.5801,
      "step": 12862
    },
    {
      "epoch": 4.979868370112273,
      "grad_norm": 13.942115783691406,
      "learning_rate": 5.577924033208586e-06,
      "loss": 0.7159,
      "step": 12863
    },
    {
      "epoch": 4.980255516840883,
      "grad_norm": 33.907936096191406,
      "learning_rate": 5.577493870176798e-06,
      "loss": 1.5658,
      "step": 12864
    },
    {
      "epoch": 4.980642663569493,
      "grad_norm": 95.34522247314453,
      "learning_rate": 5.577063707145009e-06,
      "loss": 0.5905,
      "step": 12865
    },
    {
      "epoch": 4.981029810298103,
      "grad_norm": 73.50804901123047,
      "learning_rate": 5.576633544113219e-06,
      "loss": 1.2999,
      "step": 12866
    },
    {
      "epoch": 4.981416957026713,
      "grad_norm": 55.99991989135742,
      "learning_rate": 5.57620338108143e-06,
      "loss": 0.8691,
      "step": 12867
    },
    {
      "epoch": 4.981804103755323,
      "grad_norm": 21.6881103515625,
      "learning_rate": 5.575773218049641e-06,
      "loss": 2.2253,
      "step": 12868
    },
    {
      "epoch": 4.982191250483933,
      "grad_norm": 19.458362579345703,
      "learning_rate": 5.575343055017852e-06,
      "loss": 0.5766,
      "step": 12869
    },
    {
      "epoch": 4.982578397212544,
      "grad_norm": 34.20672607421875,
      "learning_rate": 5.574912891986063e-06,
      "loss": 0.6457,
      "step": 12870
    },
    {
      "epoch": 4.9829655439411535,
      "grad_norm": 4.899438858032227,
      "learning_rate": 5.574482728954275e-06,
      "loss": 0.2795,
      "step": 12871
    },
    {
      "epoch": 4.983352690669764,
      "grad_norm": 29.718994140625,
      "learning_rate": 5.574052565922485e-06,
      "loss": 2.3432,
      "step": 12872
    },
    {
      "epoch": 4.983739837398374,
      "grad_norm": 6.195619583129883,
      "learning_rate": 5.573622402890696e-06,
      "loss": 0.215,
      "step": 12873
    },
    {
      "epoch": 4.984126984126984,
      "grad_norm": 30.192026138305664,
      "learning_rate": 5.573192239858906e-06,
      "loss": 0.528,
      "step": 12874
    },
    {
      "epoch": 4.984514130855594,
      "grad_norm": 40.35380172729492,
      "learning_rate": 5.572762076827119e-06,
      "loss": 0.7,
      "step": 12875
    },
    {
      "epoch": 4.984901277584204,
      "grad_norm": 57.074180603027344,
      "learning_rate": 5.572331913795329e-06,
      "loss": 2.2472,
      "step": 12876
    },
    {
      "epoch": 4.985288424312815,
      "grad_norm": 83.09823608398438,
      "learning_rate": 5.57190175076354e-06,
      "loss": 0.3924,
      "step": 12877
    },
    {
      "epoch": 4.9856755710414244,
      "grad_norm": 33.18018341064453,
      "learning_rate": 5.57147158773175e-06,
      "loss": 1.3377,
      "step": 12878
    },
    {
      "epoch": 4.986062717770035,
      "grad_norm": 5.601704120635986,
      "learning_rate": 5.571041424699963e-06,
      "loss": 0.2819,
      "step": 12879
    },
    {
      "epoch": 4.9864498644986455,
      "grad_norm": 11.702383995056152,
      "learning_rate": 5.570611261668173e-06,
      "loss": 0.3967,
      "step": 12880
    },
    {
      "epoch": 4.986837011227255,
      "grad_norm": 31.915531158447266,
      "learning_rate": 5.570181098636384e-06,
      "loss": 3.9291,
      "step": 12881
    },
    {
      "epoch": 4.987224157955866,
      "grad_norm": 3.468698501586914,
      "learning_rate": 5.569750935604594e-06,
      "loss": 0.1587,
      "step": 12882
    },
    {
      "epoch": 4.987611304684475,
      "grad_norm": 26.101696014404297,
      "learning_rate": 5.569320772572806e-06,
      "loss": 0.4242,
      "step": 12883
    },
    {
      "epoch": 4.987998451413086,
      "grad_norm": 63.67475128173828,
      "learning_rate": 5.568890609541016e-06,
      "loss": 0.7411,
      "step": 12884
    },
    {
      "epoch": 4.988385598141695,
      "grad_norm": 40.13416290283203,
      "learning_rate": 5.568460446509228e-06,
      "loss": 2.5871,
      "step": 12885
    },
    {
      "epoch": 4.988772744870306,
      "grad_norm": 55.931270599365234,
      "learning_rate": 5.568030283477438e-06,
      "loss": 2.2506,
      "step": 12886
    },
    {
      "epoch": 4.989159891598916,
      "grad_norm": 46.274757385253906,
      "learning_rate": 5.56760012044565e-06,
      "loss": 0.8393,
      "step": 12887
    },
    {
      "epoch": 4.989547038327526,
      "grad_norm": 26.205472946166992,
      "learning_rate": 5.56716995741386e-06,
      "loss": 0.4192,
      "step": 12888
    },
    {
      "epoch": 4.9899341850561365,
      "grad_norm": 68.27175903320312,
      "learning_rate": 5.566739794382071e-06,
      "loss": 1.0087,
      "step": 12889
    },
    {
      "epoch": 4.990321331784746,
      "grad_norm": 32.300174713134766,
      "learning_rate": 5.5663096313502814e-06,
      "loss": 0.5553,
      "step": 12890
    },
    {
      "epoch": 4.990708478513357,
      "grad_norm": 41.59076690673828,
      "learning_rate": 5.565879468318494e-06,
      "loss": 3.0386,
      "step": 12891
    },
    {
      "epoch": 4.991095625241966,
      "grad_norm": 24.02722930908203,
      "learning_rate": 5.565449305286704e-06,
      "loss": 0.4458,
      "step": 12892
    },
    {
      "epoch": 4.991482771970577,
      "grad_norm": 35.63145065307617,
      "learning_rate": 5.565019142254915e-06,
      "loss": 0.562,
      "step": 12893
    },
    {
      "epoch": 4.991869918699187,
      "grad_norm": 77.67009735107422,
      "learning_rate": 5.564588979223125e-06,
      "loss": 0.961,
      "step": 12894
    },
    {
      "epoch": 4.992257065427797,
      "grad_norm": 35.97311019897461,
      "learning_rate": 5.564158816191338e-06,
      "loss": 1.684,
      "step": 12895
    },
    {
      "epoch": 4.992644212156407,
      "grad_norm": 30.5330753326416,
      "learning_rate": 5.563728653159548e-06,
      "loss": 1.5947,
      "step": 12896
    },
    {
      "epoch": 4.993031358885017,
      "grad_norm": 13.42824649810791,
      "learning_rate": 5.563298490127759e-06,
      "loss": 0.6794,
      "step": 12897
    },
    {
      "epoch": 4.9934185056136275,
      "grad_norm": 41.49879455566406,
      "learning_rate": 5.562868327095969e-06,
      "loss": 0.8132,
      "step": 12898
    },
    {
      "epoch": 4.993805652342238,
      "grad_norm": 30.522802352905273,
      "learning_rate": 5.562438164064181e-06,
      "loss": 1.2822,
      "step": 12899
    },
    {
      "epoch": 4.994192799070848,
      "grad_norm": 17.200979232788086,
      "learning_rate": 5.562008001032392e-06,
      "loss": 1.1423,
      "step": 12900
    },
    {
      "epoch": 4.994579945799458,
      "grad_norm": 24.07847023010254,
      "learning_rate": 5.561577838000603e-06,
      "loss": 0.731,
      "step": 12901
    },
    {
      "epoch": 4.994967092528068,
      "grad_norm": 34.68620300292969,
      "learning_rate": 5.561147674968813e-06,
      "loss": 1.0652,
      "step": 12902
    },
    {
      "epoch": 4.995354239256678,
      "grad_norm": 64.42825317382812,
      "learning_rate": 5.560717511937025e-06,
      "loss": 2.1518,
      "step": 12903
    },
    {
      "epoch": 4.995741385985289,
      "grad_norm": 2.7094388008117676,
      "learning_rate": 5.560287348905235e-06,
      "loss": 0.1261,
      "step": 12904
    },
    {
      "epoch": 4.9961285327138985,
      "grad_norm": 39.423614501953125,
      "learning_rate": 5.559857185873446e-06,
      "loss": 0.8048,
      "step": 12905
    },
    {
      "epoch": 4.996515679442509,
      "grad_norm": 16.62358283996582,
      "learning_rate": 5.559427022841657e-06,
      "loss": 1.0558,
      "step": 12906
    },
    {
      "epoch": 4.996902826171119,
      "grad_norm": 13.477965354919434,
      "learning_rate": 5.558996859809869e-06,
      "loss": 1.1658,
      "step": 12907
    },
    {
      "epoch": 4.997289972899729,
      "grad_norm": 11.176342010498047,
      "learning_rate": 5.558566696778079e-06,
      "loss": 0.1858,
      "step": 12908
    },
    {
      "epoch": 4.997677119628339,
      "grad_norm": 35.57714080810547,
      "learning_rate": 5.55813653374629e-06,
      "loss": 0.7182,
      "step": 12909
    },
    {
      "epoch": 4.998064266356949,
      "grad_norm": 48.01472854614258,
      "learning_rate": 5.5577063707145004e-06,
      "loss": 1.401,
      "step": 12910
    },
    {
      "epoch": 4.99845141308556,
      "grad_norm": 18.720643997192383,
      "learning_rate": 5.557276207682713e-06,
      "loss": 1.0349,
      "step": 12911
    },
    {
      "epoch": 4.998838559814169,
      "grad_norm": 36.331050872802734,
      "learning_rate": 5.556846044650923e-06,
      "loss": 1.5668,
      "step": 12912
    },
    {
      "epoch": 4.99922570654278,
      "grad_norm": 53.56138610839844,
      "learning_rate": 5.556415881619134e-06,
      "loss": 1.7127,
      "step": 12913
    },
    {
      "epoch": 4.9996128532713895,
      "grad_norm": 23.642534255981445,
      "learning_rate": 5.555985718587345e-06,
      "loss": 1.2174,
      "step": 12914
    },
    {
      "epoch": 5.0,
      "grad_norm": 83.25331115722656,
      "learning_rate": 5.555555555555557e-06,
      "loss": 1.7334,
      "step": 12915
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.5462264150943397,
      "eval_f1": 0.5360866426145637,
      "eval_loss": 1.4503304958343506,
      "eval_runtime": 384.3421,
      "eval_samples_per_second": 2.758,
      "eval_steps_per_second": 1.379,
      "step": 12915
    },
    {
      "epoch": 5.0003871467286105,
      "grad_norm": 7.6319756507873535,
      "learning_rate": 5.555125392523767e-06,
      "loss": 0.3834,
      "step": 12916
    },
    {
      "epoch": 5.00077429345722,
      "grad_norm": 53.437503814697266,
      "learning_rate": 5.554695229491978e-06,
      "loss": 1.2773,
      "step": 12917
    },
    {
      "epoch": 5.001161440185831,
      "grad_norm": 52.5224494934082,
      "learning_rate": 5.554265066460189e-06,
      "loss": 2.0304,
      "step": 12918
    },
    {
      "epoch": 5.00154858691444,
      "grad_norm": 36.81407928466797,
      "learning_rate": 5.5538349034284e-06,
      "loss": 1.0605,
      "step": 12919
    },
    {
      "epoch": 5.001935733643051,
      "grad_norm": 17.765714645385742,
      "learning_rate": 5.55340474039661e-06,
      "loss": 0.3128,
      "step": 12920
    },
    {
      "epoch": 5.002322880371661,
      "grad_norm": 3.6844913959503174,
      "learning_rate": 5.552974577364822e-06,
      "loss": 0.2005,
      "step": 12921
    },
    {
      "epoch": 5.002710027100271,
      "grad_norm": 48.2400016784668,
      "learning_rate": 5.552544414333033e-06,
      "loss": 1.6938,
      "step": 12922
    },
    {
      "epoch": 5.003097173828881,
      "grad_norm": 10.8226957321167,
      "learning_rate": 5.552114251301244e-06,
      "loss": 0.6243,
      "step": 12923
    },
    {
      "epoch": 5.003484320557491,
      "grad_norm": 1.5397963523864746,
      "learning_rate": 5.551684088269454e-06,
      "loss": 0.0412,
      "step": 12924
    },
    {
      "epoch": 5.0038714672861015,
      "grad_norm": 16.810012817382812,
      "learning_rate": 5.551253925237665e-06,
      "loss": 0.8813,
      "step": 12925
    },
    {
      "epoch": 5.004258614014711,
      "grad_norm": 23.996381759643555,
      "learning_rate": 5.550823762205877e-06,
      "loss": 0.9353,
      "step": 12926
    },
    {
      "epoch": 5.004645760743322,
      "grad_norm": 26.99599266052246,
      "learning_rate": 5.550393599174088e-06,
      "loss": 0.5979,
      "step": 12927
    },
    {
      "epoch": 5.005032907471932,
      "grad_norm": 13.257650375366211,
      "learning_rate": 5.549963436142298e-06,
      "loss": 0.4649,
      "step": 12928
    },
    {
      "epoch": 5.005420054200542,
      "grad_norm": 53.936378479003906,
      "learning_rate": 5.549533273110509e-06,
      "loss": 3.5709,
      "step": 12929
    },
    {
      "epoch": 5.005807200929152,
      "grad_norm": 4.509700775146484,
      "learning_rate": 5.549103110078721e-06,
      "loss": 0.2689,
      "step": 12930
    },
    {
      "epoch": 5.006194347657762,
      "grad_norm": 25.252546310424805,
      "learning_rate": 5.548672947046932e-06,
      "loss": 0.8384,
      "step": 12931
    },
    {
      "epoch": 5.0065814943863725,
      "grad_norm": 5.148603439331055,
      "learning_rate": 5.548242784015142e-06,
      "loss": 0.1686,
      "step": 12932
    },
    {
      "epoch": 5.006968641114983,
      "grad_norm": 51.97568130493164,
      "learning_rate": 5.547812620983353e-06,
      "loss": 1.6917,
      "step": 12933
    },
    {
      "epoch": 5.007355787843593,
      "grad_norm": 12.315673828125,
      "learning_rate": 5.547382457951564e-06,
      "loss": 1.2375,
      "step": 12934
    },
    {
      "epoch": 5.007742934572203,
      "grad_norm": 26.660097122192383,
      "learning_rate": 5.546952294919775e-06,
      "loss": 1.0716,
      "step": 12935
    },
    {
      "epoch": 5.008130081300813,
      "grad_norm": 3.587852716445923,
      "learning_rate": 5.546522131887986e-06,
      "loss": 0.1707,
      "step": 12936
    },
    {
      "epoch": 5.008517228029423,
      "grad_norm": 58.13772201538086,
      "learning_rate": 5.546091968856197e-06,
      "loss": 1.401,
      "step": 12937
    },
    {
      "epoch": 5.008904374758034,
      "grad_norm": 13.493705749511719,
      "learning_rate": 5.545661805824408e-06,
      "loss": 1.3893,
      "step": 12938
    },
    {
      "epoch": 5.009291521486643,
      "grad_norm": 89.39695739746094,
      "learning_rate": 5.545231642792619e-06,
      "loss": 1.7306,
      "step": 12939
    },
    {
      "epoch": 5.009678668215254,
      "grad_norm": 38.5281982421875,
      "learning_rate": 5.544801479760829e-06,
      "loss": 0.8037,
      "step": 12940
    },
    {
      "epoch": 5.0100658149438635,
      "grad_norm": 21.256793975830078,
      "learning_rate": 5.54437131672904e-06,
      "loss": 0.3836,
      "step": 12941
    },
    {
      "epoch": 5.010452961672474,
      "grad_norm": 36.5595817565918,
      "learning_rate": 5.543941153697252e-06,
      "loss": 1.6914,
      "step": 12942
    },
    {
      "epoch": 5.010840108401084,
      "grad_norm": 36.12629318237305,
      "learning_rate": 5.543510990665463e-06,
      "loss": 1.6083,
      "step": 12943
    },
    {
      "epoch": 5.011227255129694,
      "grad_norm": 55.84190368652344,
      "learning_rate": 5.543080827633673e-06,
      "loss": 1.793,
      "step": 12944
    },
    {
      "epoch": 5.011614401858305,
      "grad_norm": 15.28809642791748,
      "learning_rate": 5.542650664601884e-06,
      "loss": 0.351,
      "step": 12945
    },
    {
      "epoch": 5.012001548586914,
      "grad_norm": 7.373706817626953,
      "learning_rate": 5.542220501570096e-06,
      "loss": 0.3755,
      "step": 12946
    },
    {
      "epoch": 5.012388695315525,
      "grad_norm": 13.992024421691895,
      "learning_rate": 5.541790338538307e-06,
      "loss": 0.2999,
      "step": 12947
    },
    {
      "epoch": 5.012775842044134,
      "grad_norm": 31.917484283447266,
      "learning_rate": 5.541360175506517e-06,
      "loss": 0.6535,
      "step": 12948
    },
    {
      "epoch": 5.013162988772745,
      "grad_norm": 10.140575408935547,
      "learning_rate": 5.540930012474728e-06,
      "loss": 0.5069,
      "step": 12949
    },
    {
      "epoch": 5.013550135501355,
      "grad_norm": 27.652360916137695,
      "learning_rate": 5.540499849442939e-06,
      "loss": 1.3129,
      "step": 12950
    },
    {
      "epoch": 5.013937282229965,
      "grad_norm": 13.16171646118164,
      "learning_rate": 5.540069686411151e-06,
      "loss": 1.0683,
      "step": 12951
    },
    {
      "epoch": 5.0143244289585756,
      "grad_norm": 6.289307117462158,
      "learning_rate": 5.539639523379361e-06,
      "loss": 0.2991,
      "step": 12952
    },
    {
      "epoch": 5.014711575687185,
      "grad_norm": 61.727413177490234,
      "learning_rate": 5.539209360347573e-06,
      "loss": 0.6481,
      "step": 12953
    },
    {
      "epoch": 5.015098722415796,
      "grad_norm": 8.696544647216797,
      "learning_rate": 5.538779197315783e-06,
      "loss": 0.3996,
      "step": 12954
    },
    {
      "epoch": 5.015485869144405,
      "grad_norm": 41.27149963378906,
      "learning_rate": 5.538349034283994e-06,
      "loss": 1.3146,
      "step": 12955
    },
    {
      "epoch": 5.015873015873016,
      "grad_norm": 58.101959228515625,
      "learning_rate": 5.537918871252204e-06,
      "loss": 1.6532,
      "step": 12956
    },
    {
      "epoch": 5.016260162601626,
      "grad_norm": 16.083921432495117,
      "learning_rate": 5.537488708220417e-06,
      "loss": 0.909,
      "step": 12957
    },
    {
      "epoch": 5.016647309330236,
      "grad_norm": 85.29560852050781,
      "learning_rate": 5.537058545188627e-06,
      "loss": 1.8353,
      "step": 12958
    },
    {
      "epoch": 5.0170344560588465,
      "grad_norm": 41.81814956665039,
      "learning_rate": 5.536628382156838e-06,
      "loss": 1.649,
      "step": 12959
    },
    {
      "epoch": 5.017421602787456,
      "grad_norm": 51.38304138183594,
      "learning_rate": 5.536198219125048e-06,
      "loss": 1.2878,
      "step": 12960
    },
    {
      "epoch": 5.017808749516067,
      "grad_norm": 21.366634368896484,
      "learning_rate": 5.535768056093261e-06,
      "loss": 1.9083,
      "step": 12961
    },
    {
      "epoch": 5.018195896244677,
      "grad_norm": 7.439443588256836,
      "learning_rate": 5.535337893061471e-06,
      "loss": 0.3744,
      "step": 12962
    },
    {
      "epoch": 5.018583042973287,
      "grad_norm": 4.933579444885254,
      "learning_rate": 5.534907730029682e-06,
      "loss": 0.2302,
      "step": 12963
    },
    {
      "epoch": 5.018970189701897,
      "grad_norm": 27.77619171142578,
      "learning_rate": 5.534477566997892e-06,
      "loss": 0.457,
      "step": 12964
    },
    {
      "epoch": 5.019357336430507,
      "grad_norm": 32.202186584472656,
      "learning_rate": 5.534047403966104e-06,
      "loss": 0.3495,
      "step": 12965
    },
    {
      "epoch": 5.019744483159117,
      "grad_norm": 38.02597427368164,
      "learning_rate": 5.533617240934315e-06,
      "loss": 0.5232,
      "step": 12966
    },
    {
      "epoch": 5.020131629887728,
      "grad_norm": 99.69352722167969,
      "learning_rate": 5.533187077902526e-06,
      "loss": 3.862,
      "step": 12967
    },
    {
      "epoch": 5.0205187766163375,
      "grad_norm": 26.167510986328125,
      "learning_rate": 5.532756914870736e-06,
      "loss": 1.3501,
      "step": 12968
    },
    {
      "epoch": 5.020905923344948,
      "grad_norm": 72.22540283203125,
      "learning_rate": 5.532326751838948e-06,
      "loss": 2.3508,
      "step": 12969
    },
    {
      "epoch": 5.021293070073558,
      "grad_norm": 45.988243103027344,
      "learning_rate": 5.531896588807158e-06,
      "loss": 3.2099,
      "step": 12970
    },
    {
      "epoch": 5.021680216802168,
      "grad_norm": 24.642953872680664,
      "learning_rate": 5.531466425775369e-06,
      "loss": 1.8737,
      "step": 12971
    },
    {
      "epoch": 5.022067363530778,
      "grad_norm": 52.00336456298828,
      "learning_rate": 5.53103626274358e-06,
      "loss": 2.0048,
      "step": 12972
    },
    {
      "epoch": 5.022454510259388,
      "grad_norm": 57.2873649597168,
      "learning_rate": 5.530606099711792e-06,
      "loss": 1.1188,
      "step": 12973
    },
    {
      "epoch": 5.022841656987999,
      "grad_norm": 16.434099197387695,
      "learning_rate": 5.530175936680002e-06,
      "loss": 1.3888,
      "step": 12974
    },
    {
      "epoch": 5.023228803716608,
      "grad_norm": 18.008220672607422,
      "learning_rate": 5.529745773648213e-06,
      "loss": 1.1675,
      "step": 12975
    },
    {
      "epoch": 5.023615950445219,
      "grad_norm": 45.2918586730957,
      "learning_rate": 5.529315610616423e-06,
      "loss": 2.5273,
      "step": 12976
    },
    {
      "epoch": 5.0240030971738285,
      "grad_norm": 115.7940444946289,
      "learning_rate": 5.528885447584636e-06,
      "loss": 1.0934,
      "step": 12977
    },
    {
      "epoch": 5.024390243902439,
      "grad_norm": 18.4931640625,
      "learning_rate": 5.528455284552846e-06,
      "loss": 0.7499,
      "step": 12978
    },
    {
      "epoch": 5.02477739063105,
      "grad_norm": 27.874784469604492,
      "learning_rate": 5.528025121521057e-06,
      "loss": 1.7315,
      "step": 12979
    },
    {
      "epoch": 5.025164537359659,
      "grad_norm": 5.169574737548828,
      "learning_rate": 5.527594958489267e-06,
      "loss": 0.1932,
      "step": 12980
    },
    {
      "epoch": 5.02555168408827,
      "grad_norm": 16.821090698242188,
      "learning_rate": 5.52716479545748e-06,
      "loss": 1.5116,
      "step": 12981
    },
    {
      "epoch": 5.025938830816879,
      "grad_norm": 24.67776107788086,
      "learning_rate": 5.52673463242569e-06,
      "loss": 2.0691,
      "step": 12982
    },
    {
      "epoch": 5.02632597754549,
      "grad_norm": 23.29840850830078,
      "learning_rate": 5.526304469393901e-06,
      "loss": 0.8583,
      "step": 12983
    },
    {
      "epoch": 5.026713124274099,
      "grad_norm": 9.510809898376465,
      "learning_rate": 5.525874306362111e-06,
      "loss": 0.2618,
      "step": 12984
    },
    {
      "epoch": 5.02710027100271,
      "grad_norm": 8.601888656616211,
      "learning_rate": 5.525444143330323e-06,
      "loss": 0.5837,
      "step": 12985
    },
    {
      "epoch": 5.0274874177313205,
      "grad_norm": 52.01118850708008,
      "learning_rate": 5.525013980298533e-06,
      "loss": 0.9026,
      "step": 12986
    },
    {
      "epoch": 5.02787456445993,
      "grad_norm": 91.37712860107422,
      "learning_rate": 5.524583817266745e-06,
      "loss": 1.4581,
      "step": 12987
    },
    {
      "epoch": 5.028261711188541,
      "grad_norm": 4.827037334442139,
      "learning_rate": 5.524153654234955e-06,
      "loss": 0.2723,
      "step": 12988
    },
    {
      "epoch": 5.02864885791715,
      "grad_norm": 16.134368896484375,
      "learning_rate": 5.523723491203167e-06,
      "loss": 0.097,
      "step": 12989
    },
    {
      "epoch": 5.029036004645761,
      "grad_norm": 12.006749153137207,
      "learning_rate": 5.523293328171377e-06,
      "loss": 0.3782,
      "step": 12990
    },
    {
      "epoch": 5.029423151374371,
      "grad_norm": 21.99358558654785,
      "learning_rate": 5.522863165139588e-06,
      "loss": 1.4872,
      "step": 12991
    },
    {
      "epoch": 5.029810298102981,
      "grad_norm": 53.85633850097656,
      "learning_rate": 5.522433002107798e-06,
      "loss": 0.6274,
      "step": 12992
    },
    {
      "epoch": 5.030197444831591,
      "grad_norm": 29.795833587646484,
      "learning_rate": 5.522002839076011e-06,
      "loss": 1.8005,
      "step": 12993
    },
    {
      "epoch": 5.030584591560201,
      "grad_norm": 38.15062713623047,
      "learning_rate": 5.521572676044221e-06,
      "loss": 1.668,
      "step": 12994
    },
    {
      "epoch": 5.0309717382888115,
      "grad_norm": 17.336849212646484,
      "learning_rate": 5.521142513012432e-06,
      "loss": 0.8418,
      "step": 12995
    },
    {
      "epoch": 5.031358885017422,
      "grad_norm": 7.324192047119141,
      "learning_rate": 5.520712349980643e-06,
      "loss": 0.2456,
      "step": 12996
    },
    {
      "epoch": 5.031746031746032,
      "grad_norm": 6.80111837387085,
      "learning_rate": 5.520282186948855e-06,
      "loss": 0.2967,
      "step": 12997
    },
    {
      "epoch": 5.032133178474642,
      "grad_norm": 28.299957275390625,
      "learning_rate": 5.519852023917065e-06,
      "loss": 1.2454,
      "step": 12998
    },
    {
      "epoch": 5.032520325203252,
      "grad_norm": 53.39978790283203,
      "learning_rate": 5.519421860885276e-06,
      "loss": 1.3565,
      "step": 12999
    },
    {
      "epoch": 5.032907471931862,
      "grad_norm": 59.66866683959961,
      "learning_rate": 5.518991697853487e-06,
      "loss": 2.0523,
      "step": 13000
    },
    {
      "epoch": 5.033294618660472,
      "grad_norm": 23.54994773864746,
      "learning_rate": 5.518561534821698e-06,
      "loss": 0.5628,
      "step": 13001
    },
    {
      "epoch": 5.033681765389082,
      "grad_norm": 17.69306182861328,
      "learning_rate": 5.518131371789909e-06,
      "loss": 0.7931,
      "step": 13002
    },
    {
      "epoch": 5.034068912117693,
      "grad_norm": 4.339017868041992,
      "learning_rate": 5.51770120875812e-06,
      "loss": 0.1848,
      "step": 13003
    },
    {
      "epoch": 5.0344560588463025,
      "grad_norm": 25.1378173828125,
      "learning_rate": 5.517271045726331e-06,
      "loss": 1.8138,
      "step": 13004
    },
    {
      "epoch": 5.034843205574913,
      "grad_norm": 36.996543884277344,
      "learning_rate": 5.516840882694542e-06,
      "loss": 1.4228,
      "step": 13005
    },
    {
      "epoch": 5.035230352303523,
      "grad_norm": 69.13170623779297,
      "learning_rate": 5.516410719662752e-06,
      "loss": 0.92,
      "step": 13006
    },
    {
      "epoch": 5.035617499032133,
      "grad_norm": 5.585458755493164,
      "learning_rate": 5.515980556630963e-06,
      "loss": 0.2788,
      "step": 13007
    },
    {
      "epoch": 5.036004645760744,
      "grad_norm": 37.87504577636719,
      "learning_rate": 5.515550393599175e-06,
      "loss": 1.4151,
      "step": 13008
    },
    {
      "epoch": 5.036391792489353,
      "grad_norm": 9.65068531036377,
      "learning_rate": 5.515120230567386e-06,
      "loss": 0.1889,
      "step": 13009
    },
    {
      "epoch": 5.036778939217964,
      "grad_norm": 47.268333435058594,
      "learning_rate": 5.514690067535596e-06,
      "loss": 0.8168,
      "step": 13010
    },
    {
      "epoch": 5.0371660859465734,
      "grad_norm": 65.33490753173828,
      "learning_rate": 5.514259904503807e-06,
      "loss": 1.9427,
      "step": 13011
    },
    {
      "epoch": 5.037553232675184,
      "grad_norm": 85.24801635742188,
      "learning_rate": 5.513829741472019e-06,
      "loss": 1.5704,
      "step": 13012
    },
    {
      "epoch": 5.0379403794037945,
      "grad_norm": 9.948219299316406,
      "learning_rate": 5.51339957844023e-06,
      "loss": 0.6187,
      "step": 13013
    },
    {
      "epoch": 5.038327526132404,
      "grad_norm": 8.878190994262695,
      "learning_rate": 5.51296941540844e-06,
      "loss": 0.2645,
      "step": 13014
    },
    {
      "epoch": 5.038714672861015,
      "grad_norm": 33.2506103515625,
      "learning_rate": 5.512539252376651e-06,
      "loss": 0.504,
      "step": 13015
    },
    {
      "epoch": 5.039101819589624,
      "grad_norm": 7.706027030944824,
      "learning_rate": 5.512109089344862e-06,
      "loss": 0.4548,
      "step": 13016
    },
    {
      "epoch": 5.039488966318235,
      "grad_norm": 26.068254470825195,
      "learning_rate": 5.511678926313074e-06,
      "loss": 0.7391,
      "step": 13017
    },
    {
      "epoch": 5.039876113046844,
      "grad_norm": 13.256117820739746,
      "learning_rate": 5.511248763281284e-06,
      "loss": 0.371,
      "step": 13018
    },
    {
      "epoch": 5.040263259775455,
      "grad_norm": 17.721355438232422,
      "learning_rate": 5.510818600249495e-06,
      "loss": 0.3863,
      "step": 13019
    },
    {
      "epoch": 5.040650406504065,
      "grad_norm": 81.54121398925781,
      "learning_rate": 5.510388437217706e-06,
      "loss": 0.99,
      "step": 13020
    },
    {
      "epoch": 5.041037553232675,
      "grad_norm": 1.2123888731002808,
      "learning_rate": 5.509958274185917e-06,
      "loss": 0.0289,
      "step": 13021
    },
    {
      "epoch": 5.0414246999612855,
      "grad_norm": 55.73872375488281,
      "learning_rate": 5.509528111154127e-06,
      "loss": 1.2612,
      "step": 13022
    },
    {
      "epoch": 5.041811846689895,
      "grad_norm": 45.26186752319336,
      "learning_rate": 5.509097948122339e-06,
      "loss": 1.3264,
      "step": 13023
    },
    {
      "epoch": 5.042198993418506,
      "grad_norm": 23.546302795410156,
      "learning_rate": 5.50866778509055e-06,
      "loss": 0.6653,
      "step": 13024
    },
    {
      "epoch": 5.042586140147116,
      "grad_norm": 6.197774410247803,
      "learning_rate": 5.508237622058761e-06,
      "loss": 0.2823,
      "step": 13025
    },
    {
      "epoch": 5.042973286875726,
      "grad_norm": 142.82095336914062,
      "learning_rate": 5.507807459026971e-06,
      "loss": 2.2638,
      "step": 13026
    },
    {
      "epoch": 5.043360433604336,
      "grad_norm": 7.7286505699157715,
      "learning_rate": 5.507377295995182e-06,
      "loss": 0.212,
      "step": 13027
    },
    {
      "epoch": 5.043747580332946,
      "grad_norm": 77.59892272949219,
      "learning_rate": 5.506947132963394e-06,
      "loss": 0.5514,
      "step": 13028
    },
    {
      "epoch": 5.044134727061556,
      "grad_norm": 9.69556713104248,
      "learning_rate": 5.506516969931605e-06,
      "loss": 0.1518,
      "step": 13029
    },
    {
      "epoch": 5.044521873790166,
      "grad_norm": 10.065437316894531,
      "learning_rate": 5.506086806899815e-06,
      "loss": 0.3783,
      "step": 13030
    },
    {
      "epoch": 5.0449090205187765,
      "grad_norm": 21.12200355529785,
      "learning_rate": 5.505656643868026e-06,
      "loss": 2.0398,
      "step": 13031
    },
    {
      "epoch": 5.045296167247387,
      "grad_norm": 16.816946029663086,
      "learning_rate": 5.505226480836237e-06,
      "loss": 0.7354,
      "step": 13032
    },
    {
      "epoch": 5.045683313975997,
      "grad_norm": 15.813041687011719,
      "learning_rate": 5.504796317804449e-06,
      "loss": 0.335,
      "step": 13033
    },
    {
      "epoch": 5.046070460704607,
      "grad_norm": 26.782947540283203,
      "learning_rate": 5.504366154772659e-06,
      "loss": 1.1689,
      "step": 13034
    },
    {
      "epoch": 5.046457607433217,
      "grad_norm": 13.01876163482666,
      "learning_rate": 5.503935991740871e-06,
      "loss": 0.4674,
      "step": 13035
    },
    {
      "epoch": 5.046844754161827,
      "grad_norm": 22.25310707092285,
      "learning_rate": 5.503505828709081e-06,
      "loss": 0.9962,
      "step": 13036
    },
    {
      "epoch": 5.047231900890438,
      "grad_norm": 63.979888916015625,
      "learning_rate": 5.503075665677292e-06,
      "loss": 2.0473,
      "step": 13037
    },
    {
      "epoch": 5.0476190476190474,
      "grad_norm": 63.949378967285156,
      "learning_rate": 5.502645502645503e-06,
      "loss": 1.255,
      "step": 13038
    },
    {
      "epoch": 5.048006194347658,
      "grad_norm": 14.218405723571777,
      "learning_rate": 5.502215339613715e-06,
      "loss": 0.2425,
      "step": 13039
    },
    {
      "epoch": 5.048393341076268,
      "grad_norm": 20.648183822631836,
      "learning_rate": 5.501785176581925e-06,
      "loss": 0.43,
      "step": 13040
    },
    {
      "epoch": 5.048780487804878,
      "grad_norm": 26.72316551208496,
      "learning_rate": 5.501355013550136e-06,
      "loss": 1.7073,
      "step": 13041
    },
    {
      "epoch": 5.049167634533489,
      "grad_norm": 46.80756759643555,
      "learning_rate": 5.500924850518346e-06,
      "loss": 1.4244,
      "step": 13042
    },
    {
      "epoch": 5.049554781262098,
      "grad_norm": 14.1342134475708,
      "learning_rate": 5.500494687486559e-06,
      "loss": 0.6734,
      "step": 13043
    },
    {
      "epoch": 5.049941927990709,
      "grad_norm": 16.100900650024414,
      "learning_rate": 5.500064524454769e-06,
      "loss": 0.7203,
      "step": 13044
    },
    {
      "epoch": 5.050329074719318,
      "grad_norm": 20.817420959472656,
      "learning_rate": 5.49963436142298e-06,
      "loss": 3.4574,
      "step": 13045
    },
    {
      "epoch": 5.050716221447929,
      "grad_norm": 11.695840835571289,
      "learning_rate": 5.49920419839119e-06,
      "loss": 0.3263,
      "step": 13046
    },
    {
      "epoch": 5.0511033681765385,
      "grad_norm": 15.083574295043945,
      "learning_rate": 5.498774035359402e-06,
      "loss": 0.9735,
      "step": 13047
    },
    {
      "epoch": 5.051490514905149,
      "grad_norm": 4.6268839836120605,
      "learning_rate": 5.498343872327613e-06,
      "loss": 0.1776,
      "step": 13048
    },
    {
      "epoch": 5.0518776616337595,
      "grad_norm": 40.87261199951172,
      "learning_rate": 5.497913709295824e-06,
      "loss": 2.2696,
      "step": 13049
    },
    {
      "epoch": 5.052264808362369,
      "grad_norm": 12.98192310333252,
      "learning_rate": 5.497483546264034e-06,
      "loss": 0.2401,
      "step": 13050
    },
    {
      "epoch": 5.05265195509098,
      "grad_norm": 27.91478729248047,
      "learning_rate": 5.497053383232246e-06,
      "loss": 1.4175,
      "step": 13051
    },
    {
      "epoch": 5.053039101819589,
      "grad_norm": 40.33308410644531,
      "learning_rate": 5.496623220200456e-06,
      "loss": 0.7182,
      "step": 13052
    },
    {
      "epoch": 5.0534262485482,
      "grad_norm": 81.24104309082031,
      "learning_rate": 5.496193057168668e-06,
      "loss": 0.7724,
      "step": 13053
    },
    {
      "epoch": 5.05381339527681,
      "grad_norm": 5.673349857330322,
      "learning_rate": 5.495762894136878e-06,
      "loss": 0.3137,
      "step": 13054
    },
    {
      "epoch": 5.05420054200542,
      "grad_norm": 4.527501106262207,
      "learning_rate": 5.49533273110509e-06,
      "loss": 0.1484,
      "step": 13055
    },
    {
      "epoch": 5.05458768873403,
      "grad_norm": 6.3785247802734375,
      "learning_rate": 5.4949025680733e-06,
      "loss": 0.2255,
      "step": 13056
    },
    {
      "epoch": 5.05497483546264,
      "grad_norm": 85.07138061523438,
      "learning_rate": 5.494472405041511e-06,
      "loss": 2.5329,
      "step": 13057
    },
    {
      "epoch": 5.0553619821912505,
      "grad_norm": 88.66605377197266,
      "learning_rate": 5.494042242009721e-06,
      "loss": 2.3268,
      "step": 13058
    },
    {
      "epoch": 5.055749128919861,
      "grad_norm": 33.495487213134766,
      "learning_rate": 5.493612078977934e-06,
      "loss": 2.1622,
      "step": 13059
    },
    {
      "epoch": 5.056136275648471,
      "grad_norm": 36.1076774597168,
      "learning_rate": 5.493181915946144e-06,
      "loss": 2.895,
      "step": 13060
    },
    {
      "epoch": 5.056523422377081,
      "grad_norm": 74.75204467773438,
      "learning_rate": 5.492751752914355e-06,
      "loss": 1.8649,
      "step": 13061
    },
    {
      "epoch": 5.056910569105691,
      "grad_norm": 20.01412582397461,
      "learning_rate": 5.492321589882565e-06,
      "loss": 0.7001,
      "step": 13062
    },
    {
      "epoch": 5.057297715834301,
      "grad_norm": 6.740518093109131,
      "learning_rate": 5.491891426850778e-06,
      "loss": 0.1966,
      "step": 13063
    },
    {
      "epoch": 5.057684862562911,
      "grad_norm": 207.30015563964844,
      "learning_rate": 5.491461263818988e-06,
      "loss": 1.3851,
      "step": 13064
    },
    {
      "epoch": 5.0580720092915215,
      "grad_norm": 13.111309051513672,
      "learning_rate": 5.491031100787199e-06,
      "loss": 0.53,
      "step": 13065
    },
    {
      "epoch": 5.058459156020132,
      "grad_norm": 33.456321716308594,
      "learning_rate": 5.490600937755409e-06,
      "loss": 2.3946,
      "step": 13066
    },
    {
      "epoch": 5.058846302748742,
      "grad_norm": 6.80954122543335,
      "learning_rate": 5.490170774723621e-06,
      "loss": 0.3499,
      "step": 13067
    },
    {
      "epoch": 5.059233449477352,
      "grad_norm": 29.84920310974121,
      "learning_rate": 5.489740611691831e-06,
      "loss": 0.8684,
      "step": 13068
    },
    {
      "epoch": 5.059620596205962,
      "grad_norm": 86.50737762451172,
      "learning_rate": 5.489310448660043e-06,
      "loss": 1.4019,
      "step": 13069
    },
    {
      "epoch": 5.060007742934572,
      "grad_norm": 6.565937519073486,
      "learning_rate": 5.488880285628253e-06,
      "loss": 0.3296,
      "step": 13070
    },
    {
      "epoch": 5.060394889663183,
      "grad_norm": 69.7131576538086,
      "learning_rate": 5.488450122596465e-06,
      "loss": 0.776,
      "step": 13071
    },
    {
      "epoch": 5.060782036391792,
      "grad_norm": 21.208967208862305,
      "learning_rate": 5.488019959564675e-06,
      "loss": 0.7927,
      "step": 13072
    },
    {
      "epoch": 5.061169183120403,
      "grad_norm": 38.01247024536133,
      "learning_rate": 5.487589796532886e-06,
      "loss": 2.3701,
      "step": 13073
    },
    {
      "epoch": 5.0615563298490125,
      "grad_norm": 48.923248291015625,
      "learning_rate": 5.487159633501097e-06,
      "loss": 1.8807,
      "step": 13074
    },
    {
      "epoch": 5.061943476577623,
      "grad_norm": 2.285675287246704,
      "learning_rate": 5.486729470469309e-06,
      "loss": 0.1005,
      "step": 13075
    },
    {
      "epoch": 5.062330623306233,
      "grad_norm": 69.3829574584961,
      "learning_rate": 5.486299307437519e-06,
      "loss": 0.8534,
      "step": 13076
    },
    {
      "epoch": 5.062717770034843,
      "grad_norm": 54.43845748901367,
      "learning_rate": 5.48586914440573e-06,
      "loss": 2.3952,
      "step": 13077
    },
    {
      "epoch": 5.063104916763454,
      "grad_norm": 33.709171295166016,
      "learning_rate": 5.485438981373942e-06,
      "loss": 1.6458,
      "step": 13078
    },
    {
      "epoch": 5.063492063492063,
      "grad_norm": 4.590773582458496,
      "learning_rate": 5.485008818342153e-06,
      "loss": 0.217,
      "step": 13079
    },
    {
      "epoch": 5.063879210220674,
      "grad_norm": 6.534657001495361,
      "learning_rate": 5.484578655310363e-06,
      "loss": 0.306,
      "step": 13080
    },
    {
      "epoch": 5.064266356949283,
      "grad_norm": 27.24382209777832,
      "learning_rate": 5.484148492278574e-06,
      "loss": 1.3773,
      "step": 13081
    },
    {
      "epoch": 5.064653503677894,
      "grad_norm": 48.57780456542969,
      "learning_rate": 5.483718329246785e-06,
      "loss": 0.9401,
      "step": 13082
    },
    {
      "epoch": 5.065040650406504,
      "grad_norm": 23.49137306213379,
      "learning_rate": 5.483288166214996e-06,
      "loss": 0.4127,
      "step": 13083
    },
    {
      "epoch": 5.065427797135114,
      "grad_norm": 66.99899291992188,
      "learning_rate": 5.482858003183207e-06,
      "loss": 1.4953,
      "step": 13084
    },
    {
      "epoch": 5.0658149438637246,
      "grad_norm": 14.181882858276367,
      "learning_rate": 5.482427840151418e-06,
      "loss": 0.2912,
      "step": 13085
    },
    {
      "epoch": 5.066202090592334,
      "grad_norm": 26.351295471191406,
      "learning_rate": 5.481997677119629e-06,
      "loss": 0.7704,
      "step": 13086
    },
    {
      "epoch": 5.066589237320945,
      "grad_norm": 2.236769437789917,
      "learning_rate": 5.48156751408784e-06,
      "loss": 0.0882,
      "step": 13087
    },
    {
      "epoch": 5.066976384049555,
      "grad_norm": 67.04317474365234,
      "learning_rate": 5.48113735105605e-06,
      "loss": 3.1027,
      "step": 13088
    },
    {
      "epoch": 5.067363530778165,
      "grad_norm": 170.5276336669922,
      "learning_rate": 5.480707188024262e-06,
      "loss": 1.4179,
      "step": 13089
    },
    {
      "epoch": 5.067750677506775,
      "grad_norm": 19.15732192993164,
      "learning_rate": 5.480277024992473e-06,
      "loss": 1.5257,
      "step": 13090
    },
    {
      "epoch": 5.068137824235385,
      "grad_norm": 37.26459884643555,
      "learning_rate": 5.479846861960684e-06,
      "loss": 1.1097,
      "step": 13091
    },
    {
      "epoch": 5.0685249709639955,
      "grad_norm": 30.316957473754883,
      "learning_rate": 5.479416698928894e-06,
      "loss": 1.5368,
      "step": 13092
    },
    {
      "epoch": 5.068912117692605,
      "grad_norm": 69.75477600097656,
      "learning_rate": 5.478986535897105e-06,
      "loss": 1.1336,
      "step": 13093
    },
    {
      "epoch": 5.069299264421216,
      "grad_norm": 2.9942033290863037,
      "learning_rate": 5.478556372865317e-06,
      "loss": 0.1355,
      "step": 13094
    },
    {
      "epoch": 5.069686411149826,
      "grad_norm": 101.63044738769531,
      "learning_rate": 5.478126209833528e-06,
      "loss": 0.7342,
      "step": 13095
    },
    {
      "epoch": 5.070073557878436,
      "grad_norm": 13.468323707580566,
      "learning_rate": 5.477696046801738e-06,
      "loss": 0.6749,
      "step": 13096
    },
    {
      "epoch": 5.070460704607046,
      "grad_norm": 27.07157325744629,
      "learning_rate": 5.477265883769949e-06,
      "loss": 1.1495,
      "step": 13097
    },
    {
      "epoch": 5.070847851335656,
      "grad_norm": 29.394039154052734,
      "learning_rate": 5.47683572073816e-06,
      "loss": 1.652,
      "step": 13098
    },
    {
      "epoch": 5.071234998064266,
      "grad_norm": 7.268619537353516,
      "learning_rate": 5.476405557706372e-06,
      "loss": 0.6199,
      "step": 13099
    },
    {
      "epoch": 5.071622144792877,
      "grad_norm": 49.46152877807617,
      "learning_rate": 5.475975394674582e-06,
      "loss": 0.9003,
      "step": 13100
    },
    {
      "epoch": 5.0720092915214865,
      "grad_norm": 59.7072639465332,
      "learning_rate": 5.475545231642793e-06,
      "loss": 2.4129,
      "step": 13101
    },
    {
      "epoch": 5.072396438250097,
      "grad_norm": 4.187254428863525,
      "learning_rate": 5.475115068611004e-06,
      "loss": 0.1571,
      "step": 13102
    },
    {
      "epoch": 5.072783584978707,
      "grad_norm": 22.48541259765625,
      "learning_rate": 5.474684905579215e-06,
      "loss": 0.3721,
      "step": 13103
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 23.367935180664062,
      "learning_rate": 5.474254742547425e-06,
      "loss": 0.7168,
      "step": 13104
    },
    {
      "epoch": 5.073557878435928,
      "grad_norm": 84.20729064941406,
      "learning_rate": 5.473824579515637e-06,
      "loss": 2.324,
      "step": 13105
    },
    {
      "epoch": 5.073945025164537,
      "grad_norm": 27.8414363861084,
      "learning_rate": 5.473394416483848e-06,
      "loss": 0.6708,
      "step": 13106
    },
    {
      "epoch": 5.074332171893148,
      "grad_norm": 17.642955780029297,
      "learning_rate": 5.472964253452059e-06,
      "loss": 0.7389,
      "step": 13107
    },
    {
      "epoch": 5.074719318621757,
      "grad_norm": 41.05657958984375,
      "learning_rate": 5.472534090420269e-06,
      "loss": 0.8542,
      "step": 13108
    },
    {
      "epoch": 5.075106465350368,
      "grad_norm": 115.37893676757812,
      "learning_rate": 5.47210392738848e-06,
      "loss": 1.5319,
      "step": 13109
    },
    {
      "epoch": 5.0754936120789775,
      "grad_norm": 32.8272590637207,
      "learning_rate": 5.471673764356692e-06,
      "loss": 0.5494,
      "step": 13110
    },
    {
      "epoch": 5.075880758807588,
      "grad_norm": 16.164024353027344,
      "learning_rate": 5.471243601324903e-06,
      "loss": 0.4054,
      "step": 13111
    },
    {
      "epoch": 5.0762679055361986,
      "grad_norm": 51.65338897705078,
      "learning_rate": 5.470813438293113e-06,
      "loss": 0.9198,
      "step": 13112
    },
    {
      "epoch": 5.076655052264808,
      "grad_norm": 11.199427604675293,
      "learning_rate": 5.470383275261324e-06,
      "loss": 0.9494,
      "step": 13113
    },
    {
      "epoch": 5.077042198993419,
      "grad_norm": 24.846460342407227,
      "learning_rate": 5.469953112229536e-06,
      "loss": 0.7982,
      "step": 13114
    },
    {
      "epoch": 5.077429345722028,
      "grad_norm": 40.534461975097656,
      "learning_rate": 5.469522949197747e-06,
      "loss": 1.9692,
      "step": 13115
    },
    {
      "epoch": 5.077816492450639,
      "grad_norm": 6.89235782623291,
      "learning_rate": 5.469092786165957e-06,
      "loss": 0.3607,
      "step": 13116
    },
    {
      "epoch": 5.078203639179249,
      "grad_norm": 25.59872055053711,
      "learning_rate": 5.468662623134169e-06,
      "loss": 0.5781,
      "step": 13117
    },
    {
      "epoch": 5.078590785907859,
      "grad_norm": 16.45878791809082,
      "learning_rate": 5.468232460102379e-06,
      "loss": 0.5664,
      "step": 13118
    },
    {
      "epoch": 5.0789779326364695,
      "grad_norm": 4.064862251281738,
      "learning_rate": 5.46780229707059e-06,
      "loss": 0.1704,
      "step": 13119
    },
    {
      "epoch": 5.079365079365079,
      "grad_norm": 56.85688400268555,
      "learning_rate": 5.467372134038801e-06,
      "loss": 0.7707,
      "step": 13120
    },
    {
      "epoch": 5.07975222609369,
      "grad_norm": 10.462050437927246,
      "learning_rate": 5.466941971007013e-06,
      "loss": 0.3014,
      "step": 13121
    },
    {
      "epoch": 5.080139372822299,
      "grad_norm": 17.395872116088867,
      "learning_rate": 5.466511807975223e-06,
      "loss": 1.5107,
      "step": 13122
    },
    {
      "epoch": 5.08052651955091,
      "grad_norm": 6.861771106719971,
      "learning_rate": 5.466081644943434e-06,
      "loss": 0.2915,
      "step": 13123
    },
    {
      "epoch": 5.08091366627952,
      "grad_norm": 13.941672325134277,
      "learning_rate": 5.465651481911644e-06,
      "loss": 0.2744,
      "step": 13124
    },
    {
      "epoch": 5.08130081300813,
      "grad_norm": 19.34739875793457,
      "learning_rate": 5.4652213188798566e-06,
      "loss": 0.3808,
      "step": 13125
    },
    {
      "epoch": 5.08168795973674,
      "grad_norm": 29.104055404663086,
      "learning_rate": 5.464791155848067e-06,
      "loss": 1.1208,
      "step": 13126
    },
    {
      "epoch": 5.08207510646535,
      "grad_norm": 31.213247299194336,
      "learning_rate": 5.464360992816278e-06,
      "loss": 0.8697,
      "step": 13127
    },
    {
      "epoch": 5.0824622531939605,
      "grad_norm": 19.0543270111084,
      "learning_rate": 5.463930829784488e-06,
      "loss": 0.8402,
      "step": 13128
    },
    {
      "epoch": 5.082849399922571,
      "grad_norm": 12.621805191040039,
      "learning_rate": 5.4635006667527005e-06,
      "loss": 0.7152,
      "step": 13129
    },
    {
      "epoch": 5.083236546651181,
      "grad_norm": 17.09259033203125,
      "learning_rate": 5.463070503720911e-06,
      "loss": 0.7479,
      "step": 13130
    },
    {
      "epoch": 5.083623693379791,
      "grad_norm": 31.955049514770508,
      "learning_rate": 5.462640340689122e-06,
      "loss": 0.7144,
      "step": 13131
    },
    {
      "epoch": 5.084010840108401,
      "grad_norm": 9.86369514465332,
      "learning_rate": 5.462210177657332e-06,
      "loss": 0.4464,
      "step": 13132
    },
    {
      "epoch": 5.084397986837011,
      "grad_norm": 8.22395133972168,
      "learning_rate": 5.461780014625544e-06,
      "loss": 0.4983,
      "step": 13133
    },
    {
      "epoch": 5.084785133565622,
      "grad_norm": 37.65663146972656,
      "learning_rate": 5.461349851593754e-06,
      "loss": 2.2755,
      "step": 13134
    },
    {
      "epoch": 5.085172280294231,
      "grad_norm": 13.43614387512207,
      "learning_rate": 5.460919688561966e-06,
      "loss": 0.4476,
      "step": 13135
    },
    {
      "epoch": 5.085559427022842,
      "grad_norm": 52.28202438354492,
      "learning_rate": 5.460489525530176e-06,
      "loss": 0.4174,
      "step": 13136
    },
    {
      "epoch": 5.0859465737514515,
      "grad_norm": 30.833566665649414,
      "learning_rate": 5.460059362498388e-06,
      "loss": 2.3992,
      "step": 13137
    },
    {
      "epoch": 5.086333720480062,
      "grad_norm": 21.027719497680664,
      "learning_rate": 5.459629199466598e-06,
      "loss": 0.3094,
      "step": 13138
    },
    {
      "epoch": 5.086720867208672,
      "grad_norm": 39.25642776489258,
      "learning_rate": 5.459199036434809e-06,
      "loss": 0.9949,
      "step": 13139
    },
    {
      "epoch": 5.087108013937282,
      "grad_norm": 14.803300857543945,
      "learning_rate": 5.458768873403019e-06,
      "loss": 1.7387,
      "step": 13140
    },
    {
      "epoch": 5.087495160665893,
      "grad_norm": 62.347259521484375,
      "learning_rate": 5.458338710371232e-06,
      "loss": 1.86,
      "step": 13141
    },
    {
      "epoch": 5.087882307394502,
      "grad_norm": 25.571958541870117,
      "learning_rate": 5.457908547339442e-06,
      "loss": 0.4508,
      "step": 13142
    },
    {
      "epoch": 5.088269454123113,
      "grad_norm": 52.60851287841797,
      "learning_rate": 5.457478384307653e-06,
      "loss": 0.4957,
      "step": 13143
    },
    {
      "epoch": 5.0886566008517224,
      "grad_norm": 58.2034797668457,
      "learning_rate": 5.457048221275863e-06,
      "loss": 2.0977,
      "step": 13144
    },
    {
      "epoch": 5.089043747580333,
      "grad_norm": 58.86582565307617,
      "learning_rate": 5.4566180582440756e-06,
      "loss": 1.2221,
      "step": 13145
    },
    {
      "epoch": 5.0894308943089435,
      "grad_norm": 97.71868133544922,
      "learning_rate": 5.456187895212286e-06,
      "loss": 2.7305,
      "step": 13146
    },
    {
      "epoch": 5.089818041037553,
      "grad_norm": 35.28080368041992,
      "learning_rate": 5.455757732180497e-06,
      "loss": 0.4837,
      "step": 13147
    },
    {
      "epoch": 5.090205187766164,
      "grad_norm": 3.820702075958252,
      "learning_rate": 5.455327569148707e-06,
      "loss": 0.122,
      "step": 13148
    },
    {
      "epoch": 5.090592334494773,
      "grad_norm": 39.763710021972656,
      "learning_rate": 5.454897406116919e-06,
      "loss": 1.3372,
      "step": 13149
    },
    {
      "epoch": 5.090979481223384,
      "grad_norm": 7.400811195373535,
      "learning_rate": 5.45446724308513e-06,
      "loss": 0.3447,
      "step": 13150
    },
    {
      "epoch": 5.091366627951994,
      "grad_norm": 26.673503875732422,
      "learning_rate": 5.454037080053341e-06,
      "loss": 0.6169,
      "step": 13151
    },
    {
      "epoch": 5.091753774680604,
      "grad_norm": 8.623331069946289,
      "learning_rate": 5.453606917021551e-06,
      "loss": 0.4286,
      "step": 13152
    },
    {
      "epoch": 5.092140921409214,
      "grad_norm": 31.8648738861084,
      "learning_rate": 5.453176753989763e-06,
      "loss": 1.4417,
      "step": 13153
    },
    {
      "epoch": 5.092528068137824,
      "grad_norm": 43.65097427368164,
      "learning_rate": 5.452746590957973e-06,
      "loss": 0.8372,
      "step": 13154
    },
    {
      "epoch": 5.0929152148664345,
      "grad_norm": 7.1060872077941895,
      "learning_rate": 5.452316427926184e-06,
      "loss": 0.2791,
      "step": 13155
    },
    {
      "epoch": 5.093302361595044,
      "grad_norm": 69.72632598876953,
      "learning_rate": 5.451886264894395e-06,
      "loss": 3.3582,
      "step": 13156
    },
    {
      "epoch": 5.093689508323655,
      "grad_norm": 4.96565580368042,
      "learning_rate": 5.451456101862607e-06,
      "loss": 0.1367,
      "step": 13157
    },
    {
      "epoch": 5.094076655052265,
      "grad_norm": 3.1194679737091064,
      "learning_rate": 5.451025938830817e-06,
      "loss": 0.1183,
      "step": 13158
    },
    {
      "epoch": 5.094463801780875,
      "grad_norm": 105.2586441040039,
      "learning_rate": 5.450595775799028e-06,
      "loss": 0.9939,
      "step": 13159
    },
    {
      "epoch": 5.094850948509485,
      "grad_norm": 36.873069763183594,
      "learning_rate": 5.45016561276724e-06,
      "loss": 2.0164,
      "step": 13160
    },
    {
      "epoch": 5.095238095238095,
      "grad_norm": 10.398191452026367,
      "learning_rate": 5.449735449735451e-06,
      "loss": 0.4569,
      "step": 13161
    },
    {
      "epoch": 5.095625241966705,
      "grad_norm": 10.10421371459961,
      "learning_rate": 5.449305286703661e-06,
      "loss": 0.2283,
      "step": 13162
    },
    {
      "epoch": 5.096012388695316,
      "grad_norm": 42.0353889465332,
      "learning_rate": 5.448875123671872e-06,
      "loss": 0.3474,
      "step": 13163
    },
    {
      "epoch": 5.0963995354239255,
      "grad_norm": 132.06448364257812,
      "learning_rate": 5.448444960640083e-06,
      "loss": 2.4709,
      "step": 13164
    },
    {
      "epoch": 5.096786682152536,
      "grad_norm": 4.599781513214111,
      "learning_rate": 5.4480147976082946e-06,
      "loss": 0.2224,
      "step": 13165
    },
    {
      "epoch": 5.097173828881146,
      "grad_norm": 26.663484573364258,
      "learning_rate": 5.447584634576505e-06,
      "loss": 1.4959,
      "step": 13166
    },
    {
      "epoch": 5.097560975609756,
      "grad_norm": 45.32103729248047,
      "learning_rate": 5.447154471544716e-06,
      "loss": 2.5438,
      "step": 13167
    },
    {
      "epoch": 5.097948122338366,
      "grad_norm": 3.112898111343384,
      "learning_rate": 5.446724308512927e-06,
      "loss": 0.1209,
      "step": 13168
    },
    {
      "epoch": 5.098335269066976,
      "grad_norm": 10.556392669677734,
      "learning_rate": 5.446294145481138e-06,
      "loss": 0.3451,
      "step": 13169
    },
    {
      "epoch": 5.098722415795587,
      "grad_norm": 32.81083679199219,
      "learning_rate": 5.445863982449348e-06,
      "loss": 1.108,
      "step": 13170
    },
    {
      "epoch": 5.0991095625241964,
      "grad_norm": 12.504755020141602,
      "learning_rate": 5.44543381941756e-06,
      "loss": 0.4546,
      "step": 13171
    },
    {
      "epoch": 5.099496709252807,
      "grad_norm": 16.70848846435547,
      "learning_rate": 5.445003656385771e-06,
      "loss": 0.5071,
      "step": 13172
    },
    {
      "epoch": 5.099883855981417,
      "grad_norm": 47.82785415649414,
      "learning_rate": 5.444573493353982e-06,
      "loss": 0.8344,
      "step": 13173
    },
    {
      "epoch": 5.100271002710027,
      "grad_norm": 19.937456130981445,
      "learning_rate": 5.444143330322192e-06,
      "loss": 0.2577,
      "step": 13174
    },
    {
      "epoch": 5.100658149438638,
      "grad_norm": 60.720664978027344,
      "learning_rate": 5.443713167290403e-06,
      "loss": 1.2081,
      "step": 13175
    },
    {
      "epoch": 5.101045296167247,
      "grad_norm": 3.799175262451172,
      "learning_rate": 5.443283004258615e-06,
      "loss": 0.1328,
      "step": 13176
    },
    {
      "epoch": 5.101432442895858,
      "grad_norm": 24.386919021606445,
      "learning_rate": 5.442852841226826e-06,
      "loss": 0.6348,
      "step": 13177
    },
    {
      "epoch": 5.101819589624467,
      "grad_norm": 80.4195785522461,
      "learning_rate": 5.442422678195036e-06,
      "loss": 0.8508,
      "step": 13178
    },
    {
      "epoch": 5.102206736353078,
      "grad_norm": 110.14588928222656,
      "learning_rate": 5.441992515163247e-06,
      "loss": 2.4121,
      "step": 13179
    },
    {
      "epoch": 5.102593883081688,
      "grad_norm": 10.378247261047363,
      "learning_rate": 5.441562352131459e-06,
      "loss": 0.3548,
      "step": 13180
    },
    {
      "epoch": 5.102981029810298,
      "grad_norm": 30.9454288482666,
      "learning_rate": 5.44113218909967e-06,
      "loss": 0.8807,
      "step": 13181
    },
    {
      "epoch": 5.1033681765389085,
      "grad_norm": 51.84809494018555,
      "learning_rate": 5.44070202606788e-06,
      "loss": 1.6289,
      "step": 13182
    },
    {
      "epoch": 5.103755323267518,
      "grad_norm": 36.032806396484375,
      "learning_rate": 5.440271863036091e-06,
      "loss": 2.6529,
      "step": 13183
    },
    {
      "epoch": 5.104142469996129,
      "grad_norm": 82.92552947998047,
      "learning_rate": 5.439841700004302e-06,
      "loss": 0.1627,
      "step": 13184
    },
    {
      "epoch": 5.104529616724738,
      "grad_norm": 172.8685302734375,
      "learning_rate": 5.439411536972513e-06,
      "loss": 2.7274,
      "step": 13185
    },
    {
      "epoch": 5.104916763453349,
      "grad_norm": 90.82131958007812,
      "learning_rate": 5.438981373940724e-06,
      "loss": 0.7638,
      "step": 13186
    },
    {
      "epoch": 5.105303910181959,
      "grad_norm": 11.92101764678955,
      "learning_rate": 5.438551210908935e-06,
      "loss": 0.1877,
      "step": 13187
    },
    {
      "epoch": 5.105691056910569,
      "grad_norm": 42.091712951660156,
      "learning_rate": 5.438121047877146e-06,
      "loss": 0.5274,
      "step": 13188
    },
    {
      "epoch": 5.106078203639179,
      "grad_norm": 23.507596969604492,
      "learning_rate": 5.437690884845357e-06,
      "loss": 2.1916,
      "step": 13189
    },
    {
      "epoch": 5.106465350367789,
      "grad_norm": 34.95667266845703,
      "learning_rate": 5.437260721813567e-06,
      "loss": 3.5075,
      "step": 13190
    },
    {
      "epoch": 5.1068524970963995,
      "grad_norm": 13.710208892822266,
      "learning_rate": 5.436830558781778e-06,
      "loss": 0.4674,
      "step": 13191
    },
    {
      "epoch": 5.10723964382501,
      "grad_norm": 138.2147674560547,
      "learning_rate": 5.43640039574999e-06,
      "loss": 0.5345,
      "step": 13192
    },
    {
      "epoch": 5.10762679055362,
      "grad_norm": 20.497636795043945,
      "learning_rate": 5.435970232718201e-06,
      "loss": 1.4625,
      "step": 13193
    },
    {
      "epoch": 5.10801393728223,
      "grad_norm": 32.82033920288086,
      "learning_rate": 5.435540069686411e-06,
      "loss": 1.0725,
      "step": 13194
    },
    {
      "epoch": 5.10840108401084,
      "grad_norm": 87.74760437011719,
      "learning_rate": 5.435109906654622e-06,
      "loss": 0.8892,
      "step": 13195
    },
    {
      "epoch": 5.10878823073945,
      "grad_norm": 17.58926010131836,
      "learning_rate": 5.434679743622834e-06,
      "loss": 0.3359,
      "step": 13196
    },
    {
      "epoch": 5.109175377468061,
      "grad_norm": 56.784912109375,
      "learning_rate": 5.434249580591045e-06,
      "loss": 1.4187,
      "step": 13197
    },
    {
      "epoch": 5.1095625241966705,
      "grad_norm": 52.988014221191406,
      "learning_rate": 5.433819417559255e-06,
      "loss": 0.6727,
      "step": 13198
    },
    {
      "epoch": 5.109949670925281,
      "grad_norm": 22.53382682800293,
      "learning_rate": 5.433389254527467e-06,
      "loss": 1.4807,
      "step": 13199
    },
    {
      "epoch": 5.110336817653891,
      "grad_norm": 63.04094314575195,
      "learning_rate": 5.432959091495677e-06,
      "loss": 1.2662,
      "step": 13200
    },
    {
      "epoch": 5.110723964382501,
      "grad_norm": 14.860867500305176,
      "learning_rate": 5.432528928463889e-06,
      "loss": 0.6075,
      "step": 13201
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 18.911836624145508,
      "learning_rate": 5.432098765432099e-06,
      "loss": 0.4085,
      "step": 13202
    },
    {
      "epoch": 5.111498257839721,
      "grad_norm": 52.80988693237305,
      "learning_rate": 5.4316686024003106e-06,
      "loss": 1.15,
      "step": 13203
    },
    {
      "epoch": 5.111885404568332,
      "grad_norm": 14.995742797851562,
      "learning_rate": 5.431238439368521e-06,
      "loss": 0.788,
      "step": 13204
    },
    {
      "epoch": 5.112272551296941,
      "grad_norm": 15.688606262207031,
      "learning_rate": 5.430808276336732e-06,
      "loss": 0.4276,
      "step": 13205
    },
    {
      "epoch": 5.112659698025552,
      "grad_norm": 38.459800720214844,
      "learning_rate": 5.430378113304942e-06,
      "loss": 0.6367,
      "step": 13206
    },
    {
      "epoch": 5.1130468447541615,
      "grad_norm": 77.98942565917969,
      "learning_rate": 5.4299479502731545e-06,
      "loss": 1.4004,
      "step": 13207
    },
    {
      "epoch": 5.113433991482772,
      "grad_norm": 108.72689819335938,
      "learning_rate": 5.429517787241365e-06,
      "loss": 1.1049,
      "step": 13208
    },
    {
      "epoch": 5.1138211382113825,
      "grad_norm": 56.371490478515625,
      "learning_rate": 5.429087624209576e-06,
      "loss": 2.2367,
      "step": 13209
    },
    {
      "epoch": 5.114208284939992,
      "grad_norm": 23.103816986083984,
      "learning_rate": 5.428657461177786e-06,
      "loss": 0.2158,
      "step": 13210
    },
    {
      "epoch": 5.114595431668603,
      "grad_norm": 25.435487747192383,
      "learning_rate": 5.4282272981459985e-06,
      "loss": 2.3032,
      "step": 13211
    },
    {
      "epoch": 5.114982578397212,
      "grad_norm": 4.370218753814697,
      "learning_rate": 5.427797135114209e-06,
      "loss": 0.282,
      "step": 13212
    },
    {
      "epoch": 5.115369725125823,
      "grad_norm": 41.3514404296875,
      "learning_rate": 5.42736697208242e-06,
      "loss": 1.8821,
      "step": 13213
    },
    {
      "epoch": 5.115756871854432,
      "grad_norm": 27.82259178161621,
      "learning_rate": 5.42693680905063e-06,
      "loss": 0.5603,
      "step": 13214
    },
    {
      "epoch": 5.116144018583043,
      "grad_norm": 5.9317474365234375,
      "learning_rate": 5.426506646018842e-06,
      "loss": 0.1554,
      "step": 13215
    },
    {
      "epoch": 5.116531165311653,
      "grad_norm": 4.024807453155518,
      "learning_rate": 5.426076482987053e-06,
      "loss": 0.2322,
      "step": 13216
    },
    {
      "epoch": 5.116918312040263,
      "grad_norm": 24.133806228637695,
      "learning_rate": 5.425646319955264e-06,
      "loss": 0.9343,
      "step": 13217
    },
    {
      "epoch": 5.1173054587688735,
      "grad_norm": 8.940634727478027,
      "learning_rate": 5.425216156923474e-06,
      "loss": 0.4283,
      "step": 13218
    },
    {
      "epoch": 5.117692605497483,
      "grad_norm": 149.4799346923828,
      "learning_rate": 5.424785993891686e-06,
      "loss": 3.6839,
      "step": 13219
    },
    {
      "epoch": 5.118079752226094,
      "grad_norm": 53.98563766479492,
      "learning_rate": 5.424355830859896e-06,
      "loss": 1.4892,
      "step": 13220
    },
    {
      "epoch": 5.118466898954704,
      "grad_norm": 44.721824645996094,
      "learning_rate": 5.423925667828107e-06,
      "loss": 1.0145,
      "step": 13221
    },
    {
      "epoch": 5.118854045683314,
      "grad_norm": 5.4083123207092285,
      "learning_rate": 5.423495504796318e-06,
      "loss": 0.1907,
      "step": 13222
    },
    {
      "epoch": 5.119241192411924,
      "grad_norm": 5.073446273803711,
      "learning_rate": 5.4230653417645296e-06,
      "loss": 0.1393,
      "step": 13223
    },
    {
      "epoch": 5.119628339140534,
      "grad_norm": 11.322036743164062,
      "learning_rate": 5.42263517873274e-06,
      "loss": 0.4036,
      "step": 13224
    },
    {
      "epoch": 5.1200154858691445,
      "grad_norm": 42.04670333862305,
      "learning_rate": 5.422205015700951e-06,
      "loss": 1.4795,
      "step": 13225
    },
    {
      "epoch": 5.120402632597755,
      "grad_norm": 13.492053031921387,
      "learning_rate": 5.421774852669161e-06,
      "loss": 0.8682,
      "step": 13226
    },
    {
      "epoch": 5.120789779326365,
      "grad_norm": 25.502185821533203,
      "learning_rate": 5.4213446896373735e-06,
      "loss": 0.6368,
      "step": 13227
    },
    {
      "epoch": 5.121176926054975,
      "grad_norm": 24.42841911315918,
      "learning_rate": 5.420914526605584e-06,
      "loss": 2.9802,
      "step": 13228
    },
    {
      "epoch": 5.121564072783585,
      "grad_norm": 8.0455961227417,
      "learning_rate": 5.420484363573795e-06,
      "loss": 0.3474,
      "step": 13229
    },
    {
      "epoch": 5.121951219512195,
      "grad_norm": 138.79859924316406,
      "learning_rate": 5.420054200542005e-06,
      "loss": 2.9001,
      "step": 13230
    },
    {
      "epoch": 5.122338366240805,
      "grad_norm": 46.39518356323242,
      "learning_rate": 5.4196240375102175e-06,
      "loss": 2.4096,
      "step": 13231
    },
    {
      "epoch": 5.122725512969415,
      "grad_norm": 6.869016647338867,
      "learning_rate": 5.419193874478428e-06,
      "loss": 0.2219,
      "step": 13232
    },
    {
      "epoch": 5.123112659698026,
      "grad_norm": 39.2003059387207,
      "learning_rate": 5.418763711446639e-06,
      "loss": 1.5925,
      "step": 13233
    },
    {
      "epoch": 5.1234998064266355,
      "grad_norm": 69.849365234375,
      "learning_rate": 5.418333548414849e-06,
      "loss": 0.4701,
      "step": 13234
    },
    {
      "epoch": 5.123886953155246,
      "grad_norm": 18.823284149169922,
      "learning_rate": 5.417903385383061e-06,
      "loss": 0.3084,
      "step": 13235
    },
    {
      "epoch": 5.124274099883856,
      "grad_norm": 13.990555763244629,
      "learning_rate": 5.417473222351271e-06,
      "loss": 0.4093,
      "step": 13236
    },
    {
      "epoch": 5.124661246612466,
      "grad_norm": 19.21796989440918,
      "learning_rate": 5.417043059319483e-06,
      "loss": 1.6935,
      "step": 13237
    },
    {
      "epoch": 5.125048393341077,
      "grad_norm": 55.12907409667969,
      "learning_rate": 5.416612896287693e-06,
      "loss": 0.8058,
      "step": 13238
    },
    {
      "epoch": 5.125435540069686,
      "grad_norm": 29.017375946044922,
      "learning_rate": 5.416182733255905e-06,
      "loss": 1.3886,
      "step": 13239
    },
    {
      "epoch": 5.125822686798297,
      "grad_norm": 149.98660278320312,
      "learning_rate": 5.415752570224115e-06,
      "loss": 1.2075,
      "step": 13240
    },
    {
      "epoch": 5.126209833526906,
      "grad_norm": 12.39067268371582,
      "learning_rate": 5.415322407192326e-06,
      "loss": 0.3115,
      "step": 13241
    },
    {
      "epoch": 5.126596980255517,
      "grad_norm": 40.116424560546875,
      "learning_rate": 5.414892244160538e-06,
      "loss": 0.7974,
      "step": 13242
    },
    {
      "epoch": 5.1269841269841265,
      "grad_norm": 17.87041664123535,
      "learning_rate": 5.4144620811287486e-06,
      "loss": 1.9297,
      "step": 13243
    },
    {
      "epoch": 5.127371273712737,
      "grad_norm": 28.190414428710938,
      "learning_rate": 5.414031918096959e-06,
      "loss": 0.4882,
      "step": 13244
    },
    {
      "epoch": 5.1277584204413476,
      "grad_norm": 61.00908660888672,
      "learning_rate": 5.41360175506517e-06,
      "loss": 0.9369,
      "step": 13245
    },
    {
      "epoch": 5.128145567169957,
      "grad_norm": 4.68438196182251,
      "learning_rate": 5.413171592033382e-06,
      "loss": 0.1635,
      "step": 13246
    },
    {
      "epoch": 5.128532713898568,
      "grad_norm": 15.456310272216797,
      "learning_rate": 5.4127414290015925e-06,
      "loss": 1.336,
      "step": 13247
    },
    {
      "epoch": 5.128919860627177,
      "grad_norm": 24.271724700927734,
      "learning_rate": 5.412311265969803e-06,
      "loss": 2.1629,
      "step": 13248
    },
    {
      "epoch": 5.129307007355788,
      "grad_norm": 111.6103744506836,
      "learning_rate": 5.411881102938014e-06,
      "loss": 2.2029,
      "step": 13249
    },
    {
      "epoch": 5.129694154084398,
      "grad_norm": 0.8831021189689636,
      "learning_rate": 5.411450939906225e-06,
      "loss": 0.0247,
      "step": 13250
    },
    {
      "epoch": 5.130081300813008,
      "grad_norm": 80.58882904052734,
      "learning_rate": 5.411020776874436e-06,
      "loss": 2.085,
      "step": 13251
    },
    {
      "epoch": 5.1304684475416185,
      "grad_norm": 37.62694549560547,
      "learning_rate": 5.410590613842647e-06,
      "loss": 1.8441,
      "step": 13252
    },
    {
      "epoch": 5.130855594270228,
      "grad_norm": 7.677853584289551,
      "learning_rate": 5.410160450810858e-06,
      "loss": 0.3734,
      "step": 13253
    },
    {
      "epoch": 5.131242740998839,
      "grad_norm": 141.67735290527344,
      "learning_rate": 5.409730287779069e-06,
      "loss": 2.3431,
      "step": 13254
    },
    {
      "epoch": 5.131629887727449,
      "grad_norm": 143.67820739746094,
      "learning_rate": 5.40930012474728e-06,
      "loss": 0.5529,
      "step": 13255
    },
    {
      "epoch": 5.132017034456059,
      "grad_norm": 0.9990411400794983,
      "learning_rate": 5.40886996171549e-06,
      "loss": 0.0286,
      "step": 13256
    },
    {
      "epoch": 5.132404181184669,
      "grad_norm": 7.024639129638672,
      "learning_rate": 5.408439798683701e-06,
      "loss": 0.3929,
      "step": 13257
    },
    {
      "epoch": 5.132791327913279,
      "grad_norm": 88.8636245727539,
      "learning_rate": 5.408009635651913e-06,
      "loss": 3.4722,
      "step": 13258
    },
    {
      "epoch": 5.133178474641889,
      "grad_norm": 24.597753524780273,
      "learning_rate": 5.407579472620124e-06,
      "loss": 1.8505,
      "step": 13259
    },
    {
      "epoch": 5.133565621370499,
      "grad_norm": 15.441603660583496,
      "learning_rate": 5.407149309588334e-06,
      "loss": 1.3173,
      "step": 13260
    },
    {
      "epoch": 5.1339527680991095,
      "grad_norm": 43.918460845947266,
      "learning_rate": 5.406719146556545e-06,
      "loss": 1.2364,
      "step": 13261
    },
    {
      "epoch": 5.13433991482772,
      "grad_norm": 15.784868240356445,
      "learning_rate": 5.406288983524757e-06,
      "loss": 1.7592,
      "step": 13262
    },
    {
      "epoch": 5.13472706155633,
      "grad_norm": 54.00225067138672,
      "learning_rate": 5.4058588204929676e-06,
      "loss": 0.9903,
      "step": 13263
    },
    {
      "epoch": 5.13511420828494,
      "grad_norm": 36.96796417236328,
      "learning_rate": 5.405428657461178e-06,
      "loss": 0.3114,
      "step": 13264
    },
    {
      "epoch": 5.13550135501355,
      "grad_norm": 32.4207878112793,
      "learning_rate": 5.404998494429389e-06,
      "loss": 3.7383,
      "step": 13265
    },
    {
      "epoch": 5.13588850174216,
      "grad_norm": 5.398869037628174,
      "learning_rate": 5.4045683313976e-06,
      "loss": 0.1862,
      "step": 13266
    },
    {
      "epoch": 5.136275648470771,
      "grad_norm": 28.37201499938965,
      "learning_rate": 5.4041381683658115e-06,
      "loss": 1.3036,
      "step": 13267
    },
    {
      "epoch": 5.13666279519938,
      "grad_norm": 15.215555191040039,
      "learning_rate": 5.403708005334022e-06,
      "loss": 0.3437,
      "step": 13268
    },
    {
      "epoch": 5.137049941927991,
      "grad_norm": 27.655977249145508,
      "learning_rate": 5.403277842302233e-06,
      "loss": 1.0322,
      "step": 13269
    },
    {
      "epoch": 5.1374370886566005,
      "grad_norm": 117.95340728759766,
      "learning_rate": 5.402847679270444e-06,
      "loss": 0.9309,
      "step": 13270
    },
    {
      "epoch": 5.137824235385211,
      "grad_norm": 5.423039436340332,
      "learning_rate": 5.402417516238655e-06,
      "loss": 0.1398,
      "step": 13271
    },
    {
      "epoch": 5.138211382113822,
      "grad_norm": 75.25728607177734,
      "learning_rate": 5.401987353206865e-06,
      "loss": 0.3258,
      "step": 13272
    },
    {
      "epoch": 5.138598528842431,
      "grad_norm": 23.857046127319336,
      "learning_rate": 5.401557190175077e-06,
      "loss": 2.6304,
      "step": 13273
    },
    {
      "epoch": 5.138985675571042,
      "grad_norm": 59.26206588745117,
      "learning_rate": 5.401127027143288e-06,
      "loss": 0.8136,
      "step": 13274
    },
    {
      "epoch": 5.139372822299651,
      "grad_norm": 115.3033218383789,
      "learning_rate": 5.400696864111499e-06,
      "loss": 0.6764,
      "step": 13275
    },
    {
      "epoch": 5.139759969028262,
      "grad_norm": 26.442705154418945,
      "learning_rate": 5.400266701079709e-06,
      "loss": 1.3109,
      "step": 13276
    },
    {
      "epoch": 5.140147115756871,
      "grad_norm": 84.5252914428711,
      "learning_rate": 5.39983653804792e-06,
      "loss": 2.3778,
      "step": 13277
    },
    {
      "epoch": 5.140534262485482,
      "grad_norm": 2.0408060550689697,
      "learning_rate": 5.399406375016132e-06,
      "loss": 0.0854,
      "step": 13278
    },
    {
      "epoch": 5.1409214092140925,
      "grad_norm": 24.96422004699707,
      "learning_rate": 5.398976211984343e-06,
      "loss": 1.3376,
      "step": 13279
    },
    {
      "epoch": 5.141308555942702,
      "grad_norm": 48.506534576416016,
      "learning_rate": 5.398546048952553e-06,
      "loss": 1.3827,
      "step": 13280
    },
    {
      "epoch": 5.141695702671313,
      "grad_norm": 50.64543533325195,
      "learning_rate": 5.398115885920765e-06,
      "loss": 3.069,
      "step": 13281
    },
    {
      "epoch": 5.142082849399922,
      "grad_norm": 44.95285415649414,
      "learning_rate": 5.397685722888976e-06,
      "loss": 1.1286,
      "step": 13282
    },
    {
      "epoch": 5.142469996128533,
      "grad_norm": 21.249488830566406,
      "learning_rate": 5.3972555598571866e-06,
      "loss": 0.4923,
      "step": 13283
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 29.247098922729492,
      "learning_rate": 5.396825396825397e-06,
      "loss": 0.7609,
      "step": 13284
    },
    {
      "epoch": 5.143244289585753,
      "grad_norm": 24.53728675842285,
      "learning_rate": 5.3963952337936085e-06,
      "loss": 1.3569,
      "step": 13285
    },
    {
      "epoch": 5.143631436314363,
      "grad_norm": 9.639570236206055,
      "learning_rate": 5.395965070761819e-06,
      "loss": 0.1798,
      "step": 13286
    },
    {
      "epoch": 5.144018583042973,
      "grad_norm": 25.794858932495117,
      "learning_rate": 5.39553490773003e-06,
      "loss": 0.4421,
      "step": 13287
    },
    {
      "epoch": 5.1444057297715835,
      "grad_norm": 23.615005493164062,
      "learning_rate": 5.395104744698241e-06,
      "loss": 0.3809,
      "step": 13288
    },
    {
      "epoch": 5.144792876500194,
      "grad_norm": 62.318115234375,
      "learning_rate": 5.3946745816664525e-06,
      "loss": 0.6686,
      "step": 13289
    },
    {
      "epoch": 5.145180023228804,
      "grad_norm": 32.89167785644531,
      "learning_rate": 5.394244418634663e-06,
      "loss": 1.8048,
      "step": 13290
    },
    {
      "epoch": 5.145567169957414,
      "grad_norm": 18.106931686401367,
      "learning_rate": 5.393814255602874e-06,
      "loss": 1.2039,
      "step": 13291
    },
    {
      "epoch": 5.145954316686024,
      "grad_norm": 45.77008056640625,
      "learning_rate": 5.393384092571084e-06,
      "loss": 2.8849,
      "step": 13292
    },
    {
      "epoch": 5.146341463414634,
      "grad_norm": 75.97718811035156,
      "learning_rate": 5.3929539295392965e-06,
      "loss": 1.9711,
      "step": 13293
    },
    {
      "epoch": 5.146728610143244,
      "grad_norm": 28.467451095581055,
      "learning_rate": 5.392523766507507e-06,
      "loss": 1.1842,
      "step": 13294
    },
    {
      "epoch": 5.147115756871854,
      "grad_norm": 3.4584059715270996,
      "learning_rate": 5.392093603475718e-06,
      "loss": 0.183,
      "step": 13295
    },
    {
      "epoch": 5.147502903600465,
      "grad_norm": 94.59281158447266,
      "learning_rate": 5.391663440443928e-06,
      "loss": 1.8678,
      "step": 13296
    },
    {
      "epoch": 5.1478900503290745,
      "grad_norm": 112.20035552978516,
      "learning_rate": 5.3912332774121405e-06,
      "loss": 2.4725,
      "step": 13297
    },
    {
      "epoch": 5.148277197057685,
      "grad_norm": 7.43589448928833,
      "learning_rate": 5.390803114380351e-06,
      "loss": 0.3074,
      "step": 13298
    },
    {
      "epoch": 5.148664343786295,
      "grad_norm": 40.99283218383789,
      "learning_rate": 5.390372951348562e-06,
      "loss": 0.7007,
      "step": 13299
    },
    {
      "epoch": 5.149051490514905,
      "grad_norm": 49.37509536743164,
      "learning_rate": 5.389942788316772e-06,
      "loss": 3.4021,
      "step": 13300
    },
    {
      "epoch": 5.149438637243516,
      "grad_norm": 70.58275604248047,
      "learning_rate": 5.389512625284984e-06,
      "loss": 1.3408,
      "step": 13301
    },
    {
      "epoch": 5.149825783972125,
      "grad_norm": 66.83018493652344,
      "learning_rate": 5.389082462253194e-06,
      "loss": 0.418,
      "step": 13302
    },
    {
      "epoch": 5.150212930700736,
      "grad_norm": 6.954215049743652,
      "learning_rate": 5.3886522992214056e-06,
      "loss": 0.4256,
      "step": 13303
    },
    {
      "epoch": 5.1506000774293454,
      "grad_norm": 114.9866943359375,
      "learning_rate": 5.388222136189616e-06,
      "loss": 1.7299,
      "step": 13304
    },
    {
      "epoch": 5.150987224157956,
      "grad_norm": 1.1714072227478027,
      "learning_rate": 5.3877919731578275e-06,
      "loss": 0.03,
      "step": 13305
    },
    {
      "epoch": 5.151374370886566,
      "grad_norm": 12.511384963989258,
      "learning_rate": 5.387361810126038e-06,
      "loss": 0.8964,
      "step": 13306
    },
    {
      "epoch": 5.151761517615176,
      "grad_norm": 123.24456024169922,
      "learning_rate": 5.386931647094249e-06,
      "loss": 2.8712,
      "step": 13307
    },
    {
      "epoch": 5.152148664343787,
      "grad_norm": 94.22221374511719,
      "learning_rate": 5.386501484062459e-06,
      "loss": 0.6887,
      "step": 13308
    },
    {
      "epoch": 5.152535811072396,
      "grad_norm": 27.6961727142334,
      "learning_rate": 5.3860713210306715e-06,
      "loss": 1.6053,
      "step": 13309
    },
    {
      "epoch": 5.152922957801007,
      "grad_norm": 13.1083984375,
      "learning_rate": 5.385641157998882e-06,
      "loss": 0.4453,
      "step": 13310
    },
    {
      "epoch": 5.153310104529616,
      "grad_norm": 6.522459030151367,
      "learning_rate": 5.385210994967093e-06,
      "loss": 0.2427,
      "step": 13311
    },
    {
      "epoch": 5.153697251258227,
      "grad_norm": 16.51807975769043,
      "learning_rate": 5.384780831935303e-06,
      "loss": 1.2939,
      "step": 13312
    },
    {
      "epoch": 5.154084397986837,
      "grad_norm": 6.658791542053223,
      "learning_rate": 5.3843506689035155e-06,
      "loss": 0.3695,
      "step": 13313
    },
    {
      "epoch": 5.154471544715447,
      "grad_norm": 101.28109741210938,
      "learning_rate": 5.383920505871726e-06,
      "loss": 1.6092,
      "step": 13314
    },
    {
      "epoch": 5.1548586914440575,
      "grad_norm": 36.32447052001953,
      "learning_rate": 5.383490342839937e-06,
      "loss": 0.8149,
      "step": 13315
    },
    {
      "epoch": 5.155245838172667,
      "grad_norm": 20.33922576904297,
      "learning_rate": 5.383060179808147e-06,
      "loss": 0.899,
      "step": 13316
    },
    {
      "epoch": 5.155632984901278,
      "grad_norm": 220.5295867919922,
      "learning_rate": 5.382630016776359e-06,
      "loss": 0.5521,
      "step": 13317
    },
    {
      "epoch": 5.156020131629888,
      "grad_norm": 18.793289184570312,
      "learning_rate": 5.38219985374457e-06,
      "loss": 0.6499,
      "step": 13318
    },
    {
      "epoch": 5.156407278358498,
      "grad_norm": 5.37342643737793,
      "learning_rate": 5.381769690712781e-06,
      "loss": 0.1071,
      "step": 13319
    },
    {
      "epoch": 5.156794425087108,
      "grad_norm": 42.424686431884766,
      "learning_rate": 5.381339527680991e-06,
      "loss": 0.9208,
      "step": 13320
    },
    {
      "epoch": 5.157181571815718,
      "grad_norm": 136.03953552246094,
      "learning_rate": 5.3809093646492026e-06,
      "loss": 1.2131,
      "step": 13321
    },
    {
      "epoch": 5.157568718544328,
      "grad_norm": 7.061507225036621,
      "learning_rate": 5.380479201617413e-06,
      "loss": 0.1601,
      "step": 13322
    },
    {
      "epoch": 5.157955865272938,
      "grad_norm": 26.563322067260742,
      "learning_rate": 5.380049038585624e-06,
      "loss": 1.8519,
      "step": 13323
    },
    {
      "epoch": 5.1583430120015485,
      "grad_norm": 21.89763069152832,
      "learning_rate": 5.379618875553836e-06,
      "loss": 0.5492,
      "step": 13324
    },
    {
      "epoch": 5.158730158730159,
      "grad_norm": 6.450747966766357,
      "learning_rate": 5.3791887125220465e-06,
      "loss": 0.3057,
      "step": 13325
    },
    {
      "epoch": 5.159117305458769,
      "grad_norm": 43.74612808227539,
      "learning_rate": 5.378758549490257e-06,
      "loss": 1.3062,
      "step": 13326
    },
    {
      "epoch": 5.159504452187379,
      "grad_norm": 29.138242721557617,
      "learning_rate": 5.378328386458468e-06,
      "loss": 0.3932,
      "step": 13327
    },
    {
      "epoch": 5.159891598915989,
      "grad_norm": 23.279523849487305,
      "learning_rate": 5.37789822342668e-06,
      "loss": 0.6143,
      "step": 13328
    },
    {
      "epoch": 5.160278745644599,
      "grad_norm": 15.425487518310547,
      "learning_rate": 5.3774680603948905e-06,
      "loss": 0.3534,
      "step": 13329
    },
    {
      "epoch": 5.16066589237321,
      "grad_norm": 51.62139129638672,
      "learning_rate": 5.377037897363101e-06,
      "loss": 0.7027,
      "step": 13330
    },
    {
      "epoch": 5.1610530391018195,
      "grad_norm": 29.282764434814453,
      "learning_rate": 5.376607734331312e-06,
      "loss": 0.2975,
      "step": 13331
    },
    {
      "epoch": 5.16144018583043,
      "grad_norm": 29.060321807861328,
      "learning_rate": 5.376177571299523e-06,
      "loss": 1.8396,
      "step": 13332
    },
    {
      "epoch": 5.16182733255904,
      "grad_norm": 34.10405731201172,
      "learning_rate": 5.3757474082677345e-06,
      "loss": 2.0839,
      "step": 13333
    },
    {
      "epoch": 5.16221447928765,
      "grad_norm": 14.63509750366211,
      "learning_rate": 5.375317245235945e-06,
      "loss": 0.3416,
      "step": 13334
    },
    {
      "epoch": 5.16260162601626,
      "grad_norm": 61.06145477294922,
      "learning_rate": 5.374887082204156e-06,
      "loss": 1.1993,
      "step": 13335
    },
    {
      "epoch": 5.16298877274487,
      "grad_norm": 98.61756896972656,
      "learning_rate": 5.374456919172367e-06,
      "loss": 1.9782,
      "step": 13336
    },
    {
      "epoch": 5.163375919473481,
      "grad_norm": 88.46623229980469,
      "learning_rate": 5.374026756140578e-06,
      "loss": 1.9024,
      "step": 13337
    },
    {
      "epoch": 5.16376306620209,
      "grad_norm": 4.773868083953857,
      "learning_rate": 5.373596593108788e-06,
      "loss": 0.2575,
      "step": 13338
    },
    {
      "epoch": 5.164150212930701,
      "grad_norm": 9.500463485717773,
      "learning_rate": 5.373166430077e-06,
      "loss": 0.4219,
      "step": 13339
    },
    {
      "epoch": 5.1645373596593105,
      "grad_norm": 43.973052978515625,
      "learning_rate": 5.372736267045211e-06,
      "loss": 0.7392,
      "step": 13340
    },
    {
      "epoch": 5.164924506387921,
      "grad_norm": 38.430294036865234,
      "learning_rate": 5.3723061040134216e-06,
      "loss": 0.5744,
      "step": 13341
    },
    {
      "epoch": 5.1653116531165315,
      "grad_norm": 12.032938957214355,
      "learning_rate": 5.371875940981632e-06,
      "loss": 0.4525,
      "step": 13342
    },
    {
      "epoch": 5.165698799845141,
      "grad_norm": 27.293678283691406,
      "learning_rate": 5.371445777949843e-06,
      "loss": 0.9518,
      "step": 13343
    },
    {
      "epoch": 5.166085946573752,
      "grad_norm": 97.33035278320312,
      "learning_rate": 5.371015614918055e-06,
      "loss": 0.821,
      "step": 13344
    },
    {
      "epoch": 5.166473093302361,
      "grad_norm": 10.988755226135254,
      "learning_rate": 5.3705854518862655e-06,
      "loss": 0.4967,
      "step": 13345
    },
    {
      "epoch": 5.166860240030972,
      "grad_norm": 18.43368148803711,
      "learning_rate": 5.370155288854476e-06,
      "loss": 0.8436,
      "step": 13346
    },
    {
      "epoch": 5.167247386759582,
      "grad_norm": 24.229219436645508,
      "learning_rate": 5.369725125822687e-06,
      "loss": 1.057,
      "step": 13347
    },
    {
      "epoch": 5.167634533488192,
      "grad_norm": 28.971168518066406,
      "learning_rate": 5.369294962790899e-06,
      "loss": 1.2006,
      "step": 13348
    },
    {
      "epoch": 5.168021680216802,
      "grad_norm": 48.911712646484375,
      "learning_rate": 5.3688647997591095e-06,
      "loss": 1.4748,
      "step": 13349
    },
    {
      "epoch": 5.168408826945412,
      "grad_norm": 51.998085021972656,
      "learning_rate": 5.36843463672732e-06,
      "loss": 0.2395,
      "step": 13350
    },
    {
      "epoch": 5.1687959736740225,
      "grad_norm": 130.66598510742188,
      "learning_rate": 5.368004473695531e-06,
      "loss": 2.8153,
      "step": 13351
    },
    {
      "epoch": 5.169183120402632,
      "grad_norm": 6.964365005493164,
      "learning_rate": 5.367574310663742e-06,
      "loss": 0.9638,
      "step": 13352
    },
    {
      "epoch": 5.169570267131243,
      "grad_norm": 25.972522735595703,
      "learning_rate": 5.367144147631953e-06,
      "loss": 0.4751,
      "step": 13353
    },
    {
      "epoch": 5.169957413859853,
      "grad_norm": 65.82816314697266,
      "learning_rate": 5.366713984600164e-06,
      "loss": 1.8246,
      "step": 13354
    },
    {
      "epoch": 5.170344560588463,
      "grad_norm": 20.553869247436523,
      "learning_rate": 5.366283821568375e-06,
      "loss": 0.8978,
      "step": 13355
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 66.89022064208984,
      "learning_rate": 5.365853658536586e-06,
      "loss": 3.0102,
      "step": 13356
    },
    {
      "epoch": 5.171118854045683,
      "grad_norm": 35.70777130126953,
      "learning_rate": 5.365423495504797e-06,
      "loss": 0.5634,
      "step": 13357
    },
    {
      "epoch": 5.1715060007742935,
      "grad_norm": 119.65704345703125,
      "learning_rate": 5.364993332473007e-06,
      "loss": 2.2931,
      "step": 13358
    },
    {
      "epoch": 5.171893147502904,
      "grad_norm": 107.24518585205078,
      "learning_rate": 5.364563169441218e-06,
      "loss": 2.7852,
      "step": 13359
    },
    {
      "epoch": 5.172280294231514,
      "grad_norm": 4.927428722381592,
      "learning_rate": 5.36413300640943e-06,
      "loss": 0.2441,
      "step": 13360
    },
    {
      "epoch": 5.172667440960124,
      "grad_norm": 8.649070739746094,
      "learning_rate": 5.3637028433776406e-06,
      "loss": 0.4388,
      "step": 13361
    },
    {
      "epoch": 5.173054587688734,
      "grad_norm": 6.553772449493408,
      "learning_rate": 5.363272680345851e-06,
      "loss": 0.3241,
      "step": 13362
    },
    {
      "epoch": 5.173441734417344,
      "grad_norm": 44.57386016845703,
      "learning_rate": 5.362842517314062e-06,
      "loss": 2.9986,
      "step": 13363
    },
    {
      "epoch": 5.173828881145955,
      "grad_norm": 32.040950775146484,
      "learning_rate": 5.362412354282274e-06,
      "loss": 0.5103,
      "step": 13364
    },
    {
      "epoch": 5.174216027874564,
      "grad_norm": 17.626039505004883,
      "learning_rate": 5.3619821912504845e-06,
      "loss": 1.5055,
      "step": 13365
    },
    {
      "epoch": 5.174603174603175,
      "grad_norm": 57.57505798339844,
      "learning_rate": 5.361552028218695e-06,
      "loss": 0.8789,
      "step": 13366
    },
    {
      "epoch": 5.1749903213317845,
      "grad_norm": 19.041101455688477,
      "learning_rate": 5.3611218651869065e-06,
      "loss": 1.3235,
      "step": 13367
    },
    {
      "epoch": 5.175377468060395,
      "grad_norm": 18.04476547241211,
      "learning_rate": 5.360691702155117e-06,
      "loss": 0.5281,
      "step": 13368
    },
    {
      "epoch": 5.175764614789005,
      "grad_norm": 14.861384391784668,
      "learning_rate": 5.3602615391233285e-06,
      "loss": 0.4986,
      "step": 13369
    },
    {
      "epoch": 5.176151761517615,
      "grad_norm": 18.5369930267334,
      "learning_rate": 5.359831376091539e-06,
      "loss": 0.321,
      "step": 13370
    },
    {
      "epoch": 5.176538908246226,
      "grad_norm": 8.140523910522461,
      "learning_rate": 5.3594012130597505e-06,
      "loss": 0.3414,
      "step": 13371
    },
    {
      "epoch": 5.176926054974835,
      "grad_norm": 304.3667907714844,
      "learning_rate": 5.358971050027961e-06,
      "loss": 1.6688,
      "step": 13372
    },
    {
      "epoch": 5.177313201703446,
      "grad_norm": 5.709446430206299,
      "learning_rate": 5.358540886996172e-06,
      "loss": 0.2729,
      "step": 13373
    },
    {
      "epoch": 5.177700348432055,
      "grad_norm": 6.484813690185547,
      "learning_rate": 5.358110723964382e-06,
      "loss": 0.4455,
      "step": 13374
    },
    {
      "epoch": 5.178087495160666,
      "grad_norm": 14.416947364807129,
      "learning_rate": 5.3576805609325945e-06,
      "loss": 0.4161,
      "step": 13375
    },
    {
      "epoch": 5.178474641889276,
      "grad_norm": 80.50701904296875,
      "learning_rate": 5.357250397900805e-06,
      "loss": 0.8284,
      "step": 13376
    },
    {
      "epoch": 5.178861788617886,
      "grad_norm": 50.78328323364258,
      "learning_rate": 5.356820234869016e-06,
      "loss": 2.3466,
      "step": 13377
    },
    {
      "epoch": 5.1792489353464966,
      "grad_norm": 14.508332252502441,
      "learning_rate": 5.356390071837226e-06,
      "loss": 0.5872,
      "step": 13378
    },
    {
      "epoch": 5.179636082075106,
      "grad_norm": 11.00553035736084,
      "learning_rate": 5.3559599088054384e-06,
      "loss": 0.3314,
      "step": 13379
    },
    {
      "epoch": 5.180023228803717,
      "grad_norm": 3.321631908416748,
      "learning_rate": 5.355529745773649e-06,
      "loss": 0.1056,
      "step": 13380
    },
    {
      "epoch": 5.180410375532327,
      "grad_norm": 15.830467224121094,
      "learning_rate": 5.3550995827418596e-06,
      "loss": 0.1886,
      "step": 13381
    },
    {
      "epoch": 5.180797522260937,
      "grad_norm": 43.54902648925781,
      "learning_rate": 5.35466941971007e-06,
      "loss": 1.5981,
      "step": 13382
    },
    {
      "epoch": 5.181184668989547,
      "grad_norm": 22.430994033813477,
      "learning_rate": 5.3542392566782816e-06,
      "loss": 1.3577,
      "step": 13383
    },
    {
      "epoch": 5.181571815718157,
      "grad_norm": 38.55772399902344,
      "learning_rate": 5.353809093646493e-06,
      "loss": 1.8994,
      "step": 13384
    },
    {
      "epoch": 5.1819589624467675,
      "grad_norm": 39.65856170654297,
      "learning_rate": 5.3533789306147035e-06,
      "loss": 1.6215,
      "step": 13385
    },
    {
      "epoch": 5.182346109175377,
      "grad_norm": 27.302156448364258,
      "learning_rate": 5.352948767582914e-06,
      "loss": 1.667,
      "step": 13386
    },
    {
      "epoch": 5.182733255903988,
      "grad_norm": 26.65790367126465,
      "learning_rate": 5.3525186045511255e-06,
      "loss": 0.7717,
      "step": 13387
    },
    {
      "epoch": 5.183120402632598,
      "grad_norm": 19.82150650024414,
      "learning_rate": 5.352088441519336e-06,
      "loss": 0.3134,
      "step": 13388
    },
    {
      "epoch": 5.183507549361208,
      "grad_norm": 46.99308395385742,
      "learning_rate": 5.351658278487547e-06,
      "loss": 3.182,
      "step": 13389
    },
    {
      "epoch": 5.183894696089818,
      "grad_norm": 49.466461181640625,
      "learning_rate": 5.351228115455758e-06,
      "loss": 1.1217,
      "step": 13390
    },
    {
      "epoch": 5.184281842818428,
      "grad_norm": 135.66395568847656,
      "learning_rate": 5.3507979524239695e-06,
      "loss": 2.1645,
      "step": 13391
    },
    {
      "epoch": 5.184668989547038,
      "grad_norm": 28.53710174560547,
      "learning_rate": 5.35036778939218e-06,
      "loss": 1.406,
      "step": 13392
    },
    {
      "epoch": 5.185056136275649,
      "grad_norm": 50.14669418334961,
      "learning_rate": 5.349937626360391e-06,
      "loss": 1.8983,
      "step": 13393
    },
    {
      "epoch": 5.1854432830042585,
      "grad_norm": 6.255145072937012,
      "learning_rate": 5.349507463328601e-06,
      "loss": 0.38,
      "step": 13394
    },
    {
      "epoch": 5.185830429732869,
      "grad_norm": 66.88877868652344,
      "learning_rate": 5.3490773002968135e-06,
      "loss": 1.4467,
      "step": 13395
    },
    {
      "epoch": 5.186217576461479,
      "grad_norm": 52.35615921020508,
      "learning_rate": 5.348647137265024e-06,
      "loss": 0.7588,
      "step": 13396
    },
    {
      "epoch": 5.186604723190089,
      "grad_norm": 37.79454803466797,
      "learning_rate": 5.348216974233235e-06,
      "loss": 1.5241,
      "step": 13397
    },
    {
      "epoch": 5.186991869918699,
      "grad_norm": 26.47919464111328,
      "learning_rate": 5.347786811201445e-06,
      "loss": 1.788,
      "step": 13398
    },
    {
      "epoch": 5.187379016647309,
      "grad_norm": 75.4267807006836,
      "learning_rate": 5.3473566481696574e-06,
      "loss": 1.024,
      "step": 13399
    },
    {
      "epoch": 5.18776616337592,
      "grad_norm": 21.215002059936523,
      "learning_rate": 5.346926485137868e-06,
      "loss": 1.2137,
      "step": 13400
    },
    {
      "epoch": 5.188153310104529,
      "grad_norm": 10.230833053588867,
      "learning_rate": 5.3464963221060786e-06,
      "loss": 0.436,
      "step": 13401
    },
    {
      "epoch": 5.18854045683314,
      "grad_norm": 10.897384643554688,
      "learning_rate": 5.346066159074289e-06,
      "loss": 0.7718,
      "step": 13402
    },
    {
      "epoch": 5.1889276035617495,
      "grad_norm": 63.107730865478516,
      "learning_rate": 5.3456359960425005e-06,
      "loss": 2.5626,
      "step": 13403
    },
    {
      "epoch": 5.18931475029036,
      "grad_norm": 4.595325946807861,
      "learning_rate": 5.345205833010711e-06,
      "loss": 0.1566,
      "step": 13404
    },
    {
      "epoch": 5.1897018970189706,
      "grad_norm": 2.789501428604126,
      "learning_rate": 5.3447756699789225e-06,
      "loss": 0.1258,
      "step": 13405
    },
    {
      "epoch": 5.19008904374758,
      "grad_norm": 4.728544235229492,
      "learning_rate": 5.344345506947134e-06,
      "loss": 0.1512,
      "step": 13406
    },
    {
      "epoch": 5.190476190476191,
      "grad_norm": 5.943058490753174,
      "learning_rate": 5.3439153439153445e-06,
      "loss": 0.2983,
      "step": 13407
    },
    {
      "epoch": 5.1908633372048,
      "grad_norm": 46.55075454711914,
      "learning_rate": 5.343485180883555e-06,
      "loss": 1.171,
      "step": 13408
    },
    {
      "epoch": 5.191250483933411,
      "grad_norm": 12.822583198547363,
      "learning_rate": 5.343055017851766e-06,
      "loss": 0.7949,
      "step": 13409
    },
    {
      "epoch": 5.191637630662021,
      "grad_norm": 18.750953674316406,
      "learning_rate": 5.342624854819978e-06,
      "loss": 1.1015,
      "step": 13410
    },
    {
      "epoch": 5.192024777390631,
      "grad_norm": 22.59333610534668,
      "learning_rate": 5.3421946917881885e-06,
      "loss": 0.8316,
      "step": 13411
    },
    {
      "epoch": 5.1924119241192415,
      "grad_norm": 15.448286056518555,
      "learning_rate": 5.341764528756399e-06,
      "loss": 1.2688,
      "step": 13412
    },
    {
      "epoch": 5.192799070847851,
      "grad_norm": 36.8226318359375,
      "learning_rate": 5.34133436572461e-06,
      "loss": 0.409,
      "step": 13413
    },
    {
      "epoch": 5.193186217576462,
      "grad_norm": 63.59024429321289,
      "learning_rate": 5.340904202692822e-06,
      "loss": 0.6701,
      "step": 13414
    },
    {
      "epoch": 5.193573364305071,
      "grad_norm": 63.43475341796875,
      "learning_rate": 5.3404740396610325e-06,
      "loss": 1.4572,
      "step": 13415
    },
    {
      "epoch": 5.193960511033682,
      "grad_norm": 0.8159573078155518,
      "learning_rate": 5.340043876629243e-06,
      "loss": 0.0243,
      "step": 13416
    },
    {
      "epoch": 5.194347657762292,
      "grad_norm": 27.42875862121582,
      "learning_rate": 5.339613713597454e-06,
      "loss": 0.3076,
      "step": 13417
    },
    {
      "epoch": 5.194734804490902,
      "grad_norm": 77.6319351196289,
      "learning_rate": 5.339183550565665e-06,
      "loss": 1.5779,
      "step": 13418
    },
    {
      "epoch": 5.195121951219512,
      "grad_norm": 37.54887390136719,
      "learning_rate": 5.338753387533876e-06,
      "loss": 0.9067,
      "step": 13419
    },
    {
      "epoch": 5.195509097948122,
      "grad_norm": 85.85941314697266,
      "learning_rate": 5.338323224502087e-06,
      "loss": 2.4185,
      "step": 13420
    },
    {
      "epoch": 5.1958962446767325,
      "grad_norm": 67.0030288696289,
      "learning_rate": 5.3378930614702976e-06,
      "loss": 0.7355,
      "step": 13421
    },
    {
      "epoch": 5.196283391405343,
      "grad_norm": 4.979142189025879,
      "learning_rate": 5.337462898438509e-06,
      "loss": 0.232,
      "step": 13422
    },
    {
      "epoch": 5.196670538133953,
      "grad_norm": 86.15177917480469,
      "learning_rate": 5.3370327354067195e-06,
      "loss": 1.7861,
      "step": 13423
    },
    {
      "epoch": 5.197057684862563,
      "grad_norm": 35.417724609375,
      "learning_rate": 5.33660257237493e-06,
      "loss": 3.1519,
      "step": 13424
    },
    {
      "epoch": 5.197444831591173,
      "grad_norm": 7.1786322593688965,
      "learning_rate": 5.336172409343141e-06,
      "loss": 0.4189,
      "step": 13425
    },
    {
      "epoch": 5.197831978319783,
      "grad_norm": 63.770713806152344,
      "learning_rate": 5.335742246311353e-06,
      "loss": 0.3896,
      "step": 13426
    },
    {
      "epoch": 5.198219125048393,
      "grad_norm": 19.225189208984375,
      "learning_rate": 5.3353120832795635e-06,
      "loss": 1.2972,
      "step": 13427
    },
    {
      "epoch": 5.198606271777003,
      "grad_norm": 5.112397193908691,
      "learning_rate": 5.334881920247774e-06,
      "loss": 0.1875,
      "step": 13428
    },
    {
      "epoch": 5.198993418505614,
      "grad_norm": 52.9530029296875,
      "learning_rate": 5.334451757215985e-06,
      "loss": 0.7214,
      "step": 13429
    },
    {
      "epoch": 5.1993805652342235,
      "grad_norm": 10.237934112548828,
      "learning_rate": 5.334021594184197e-06,
      "loss": 0.2693,
      "step": 13430
    },
    {
      "epoch": 5.199767711962834,
      "grad_norm": 43.71097183227539,
      "learning_rate": 5.3335914311524075e-06,
      "loss": 1.7147,
      "step": 13431
    },
    {
      "epoch": 5.200154858691444,
      "grad_norm": 33.08491516113281,
      "learning_rate": 5.333161268120618e-06,
      "loss": 1.1595,
      "step": 13432
    },
    {
      "epoch": 5.200542005420054,
      "grad_norm": 32.93937301635742,
      "learning_rate": 5.332731105088829e-06,
      "loss": 1.8445,
      "step": 13433
    },
    {
      "epoch": 5.200929152148665,
      "grad_norm": 97.0519790649414,
      "learning_rate": 5.33230094205704e-06,
      "loss": 0.6926,
      "step": 13434
    },
    {
      "epoch": 5.201316298877274,
      "grad_norm": 7.918684005737305,
      "learning_rate": 5.3318707790252515e-06,
      "loss": 0.2941,
      "step": 13435
    },
    {
      "epoch": 5.201703445605885,
      "grad_norm": 6.257390022277832,
      "learning_rate": 5.331440615993462e-06,
      "loss": 0.4037,
      "step": 13436
    },
    {
      "epoch": 5.2020905923344944,
      "grad_norm": 123.10428619384766,
      "learning_rate": 5.331010452961673e-06,
      "loss": 1.4896,
      "step": 13437
    },
    {
      "epoch": 5.202477739063105,
      "grad_norm": 17.511337280273438,
      "learning_rate": 5.330580289929884e-06,
      "loss": 0.2968,
      "step": 13438
    },
    {
      "epoch": 5.2028648857917155,
      "grad_norm": 48.10419464111328,
      "learning_rate": 5.3301501268980946e-06,
      "loss": 0.3614,
      "step": 13439
    },
    {
      "epoch": 5.203252032520325,
      "grad_norm": 56.706787109375,
      "learning_rate": 5.329719963866305e-06,
      "loss": 0.9005,
      "step": 13440
    },
    {
      "epoch": 5.203639179248936,
      "grad_norm": 4.169762134552002,
      "learning_rate": 5.3292898008345166e-06,
      "loss": 0.2145,
      "step": 13441
    },
    {
      "epoch": 5.204026325977545,
      "grad_norm": 30.779165267944336,
      "learning_rate": 5.328859637802728e-06,
      "loss": 0.8556,
      "step": 13442
    },
    {
      "epoch": 5.204413472706156,
      "grad_norm": 41.5069580078125,
      "learning_rate": 5.3284294747709385e-06,
      "loss": 0.5403,
      "step": 13443
    },
    {
      "epoch": 5.204800619434765,
      "grad_norm": 8.330442428588867,
      "learning_rate": 5.327999311739149e-06,
      "loss": 0.3494,
      "step": 13444
    },
    {
      "epoch": 5.205187766163376,
      "grad_norm": 60.843292236328125,
      "learning_rate": 5.32756914870736e-06,
      "loss": 0.5551,
      "step": 13445
    },
    {
      "epoch": 5.205574912891986,
      "grad_norm": 109.38509368896484,
      "learning_rate": 5.327138985675572e-06,
      "loss": 2.3569,
      "step": 13446
    },
    {
      "epoch": 5.205962059620596,
      "grad_norm": 24.679222106933594,
      "learning_rate": 5.3267088226437825e-06,
      "loss": 1.7107,
      "step": 13447
    },
    {
      "epoch": 5.2063492063492065,
      "grad_norm": 8.479578971862793,
      "learning_rate": 5.326278659611993e-06,
      "loss": 0.3259,
      "step": 13448
    },
    {
      "epoch": 5.206736353077816,
      "grad_norm": 27.212003707885742,
      "learning_rate": 5.3258484965802045e-06,
      "loss": 0.6619,
      "step": 13449
    },
    {
      "epoch": 5.207123499806427,
      "grad_norm": 62.38909149169922,
      "learning_rate": 5.325418333548416e-06,
      "loss": 0.9194,
      "step": 13450
    },
    {
      "epoch": 5.207510646535037,
      "grad_norm": 4.9652228355407715,
      "learning_rate": 5.3249881705166265e-06,
      "loss": 0.0463,
      "step": 13451
    },
    {
      "epoch": 5.207897793263647,
      "grad_norm": 26.109285354614258,
      "learning_rate": 5.324558007484837e-06,
      "loss": 1.9127,
      "step": 13452
    },
    {
      "epoch": 5.208284939992257,
      "grad_norm": 368.2105712890625,
      "learning_rate": 5.3241278444530485e-06,
      "loss": 3.3902,
      "step": 13453
    },
    {
      "epoch": 5.208672086720867,
      "grad_norm": 24.715110778808594,
      "learning_rate": 5.323697681421259e-06,
      "loss": 0.558,
      "step": 13454
    },
    {
      "epoch": 5.209059233449477,
      "grad_norm": 16.093992233276367,
      "learning_rate": 5.32326751838947e-06,
      "loss": 1.2744,
      "step": 13455
    },
    {
      "epoch": 5.209446380178088,
      "grad_norm": 6.601431369781494,
      "learning_rate": 5.322837355357681e-06,
      "loss": 0.4252,
      "step": 13456
    },
    {
      "epoch": 5.2098335269066975,
      "grad_norm": 37.03788757324219,
      "learning_rate": 5.3224071923258924e-06,
      "loss": 1.5767,
      "step": 13457
    },
    {
      "epoch": 5.210220673635308,
      "grad_norm": 179.35638427734375,
      "learning_rate": 5.321977029294103e-06,
      "loss": 0.8646,
      "step": 13458
    },
    {
      "epoch": 5.210607820363918,
      "grad_norm": 24.339481353759766,
      "learning_rate": 5.3215468662623136e-06,
      "loss": 2.508,
      "step": 13459
    },
    {
      "epoch": 5.210994967092528,
      "grad_norm": 3.5924108028411865,
      "learning_rate": 5.321116703230524e-06,
      "loss": 0.1323,
      "step": 13460
    },
    {
      "epoch": 5.211382113821138,
      "grad_norm": 77.26950073242188,
      "learning_rate": 5.320686540198736e-06,
      "loss": 0.9425,
      "step": 13461
    },
    {
      "epoch": 5.211769260549748,
      "grad_norm": 19.886022567749023,
      "learning_rate": 5.320256377166947e-06,
      "loss": 1.9327,
      "step": 13462
    },
    {
      "epoch": 5.212156407278359,
      "grad_norm": 9.885110855102539,
      "learning_rate": 5.3198262141351575e-06,
      "loss": 0.4588,
      "step": 13463
    },
    {
      "epoch": 5.2125435540069684,
      "grad_norm": 32.457252502441406,
      "learning_rate": 5.319396051103368e-06,
      "loss": 2.0789,
      "step": 13464
    },
    {
      "epoch": 5.212930700735579,
      "grad_norm": 90.85272216796875,
      "learning_rate": 5.31896588807158e-06,
      "loss": 2.2073,
      "step": 13465
    },
    {
      "epoch": 5.213317847464189,
      "grad_norm": 7.673085689544678,
      "learning_rate": 5.318535725039791e-06,
      "loss": 0.1932,
      "step": 13466
    },
    {
      "epoch": 5.213704994192799,
      "grad_norm": 14.969525337219238,
      "learning_rate": 5.3181055620080015e-06,
      "loss": 0.2488,
      "step": 13467
    },
    {
      "epoch": 5.21409214092141,
      "grad_norm": 26.41762924194336,
      "learning_rate": 5.317675398976212e-06,
      "loss": 0.8363,
      "step": 13468
    },
    {
      "epoch": 5.214479287650019,
      "grad_norm": 53.51018524169922,
      "learning_rate": 5.3172452359444235e-06,
      "loss": 1.5339,
      "step": 13469
    },
    {
      "epoch": 5.21486643437863,
      "grad_norm": 105.56895446777344,
      "learning_rate": 5.316815072912634e-06,
      "loss": 2.7175,
      "step": 13470
    },
    {
      "epoch": 5.215253581107239,
      "grad_norm": 17.665983200073242,
      "learning_rate": 5.3163849098808455e-06,
      "loss": 1.3981,
      "step": 13471
    },
    {
      "epoch": 5.21564072783585,
      "grad_norm": 37.77893829345703,
      "learning_rate": 5.315954746849056e-06,
      "loss": 1.5649,
      "step": 13472
    },
    {
      "epoch": 5.21602787456446,
      "grad_norm": 25.87982177734375,
      "learning_rate": 5.3155245838172675e-06,
      "loss": 1.657,
      "step": 13473
    },
    {
      "epoch": 5.21641502129307,
      "grad_norm": 25.108694076538086,
      "learning_rate": 5.315094420785478e-06,
      "loss": 1.5845,
      "step": 13474
    },
    {
      "epoch": 5.2168021680216805,
      "grad_norm": 60.48488998413086,
      "learning_rate": 5.314664257753689e-06,
      "loss": 0.8617,
      "step": 13475
    },
    {
      "epoch": 5.21718931475029,
      "grad_norm": 9.344276428222656,
      "learning_rate": 5.314234094721899e-06,
      "loss": 0.4034,
      "step": 13476
    },
    {
      "epoch": 5.217576461478901,
      "grad_norm": 12.99886417388916,
      "learning_rate": 5.3138039316901114e-06,
      "loss": 0.6248,
      "step": 13477
    },
    {
      "epoch": 5.21796360820751,
      "grad_norm": 9.268512725830078,
      "learning_rate": 5.313373768658322e-06,
      "loss": 0.059,
      "step": 13478
    },
    {
      "epoch": 5.218350754936121,
      "grad_norm": 61.55483627319336,
      "learning_rate": 5.3129436056265326e-06,
      "loss": 0.8214,
      "step": 13479
    },
    {
      "epoch": 5.218737901664731,
      "grad_norm": 54.7716178894043,
      "learning_rate": 5.312513442594743e-06,
      "loss": 0.3992,
      "step": 13480
    },
    {
      "epoch": 5.219125048393341,
      "grad_norm": 20.720767974853516,
      "learning_rate": 5.312083279562955e-06,
      "loss": 0.3672,
      "step": 13481
    },
    {
      "epoch": 5.219512195121951,
      "grad_norm": 112.67005920410156,
      "learning_rate": 5.311653116531166e-06,
      "loss": 1.5934,
      "step": 13482
    },
    {
      "epoch": 5.219899341850561,
      "grad_norm": 65.0564956665039,
      "learning_rate": 5.3112229534993765e-06,
      "loss": 1.3788,
      "step": 13483
    },
    {
      "epoch": 5.2202864885791715,
      "grad_norm": 72.84349060058594,
      "learning_rate": 5.310792790467587e-06,
      "loss": 1.9188,
      "step": 13484
    },
    {
      "epoch": 5.220673635307782,
      "grad_norm": 76.42985534667969,
      "learning_rate": 5.3103626274357985e-06,
      "loss": 2.7039,
      "step": 13485
    },
    {
      "epoch": 5.221060782036392,
      "grad_norm": 51.117671966552734,
      "learning_rate": 5.30993246440401e-06,
      "loss": 2.3898,
      "step": 13486
    },
    {
      "epoch": 5.221447928765002,
      "grad_norm": 14.29590129852295,
      "learning_rate": 5.3095023013722205e-06,
      "loss": 0.2526,
      "step": 13487
    },
    {
      "epoch": 5.221835075493612,
      "grad_norm": 69.8537368774414,
      "learning_rate": 5.309072138340432e-06,
      "loss": 0.3762,
      "step": 13488
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 76.4351806640625,
      "learning_rate": 5.3086419753086425e-06,
      "loss": 1.7097,
      "step": 13489
    },
    {
      "epoch": 5.222609368950832,
      "grad_norm": 45.27864074707031,
      "learning_rate": 5.308211812276853e-06,
      "loss": 2.1072,
      "step": 13490
    },
    {
      "epoch": 5.2229965156794425,
      "grad_norm": 21.913097381591797,
      "learning_rate": 5.307781649245064e-06,
      "loss": 0.5632,
      "step": 13491
    },
    {
      "epoch": 5.223383662408053,
      "grad_norm": 163.5651092529297,
      "learning_rate": 5.307351486213276e-06,
      "loss": 4.1574,
      "step": 13492
    },
    {
      "epoch": 5.223770809136663,
      "grad_norm": 6.261774063110352,
      "learning_rate": 5.3069213231814865e-06,
      "loss": 0.3495,
      "step": 13493
    },
    {
      "epoch": 5.224157955865273,
      "grad_norm": 29.393741607666016,
      "learning_rate": 5.306491160149697e-06,
      "loss": 2.1292,
      "step": 13494
    },
    {
      "epoch": 5.224545102593883,
      "grad_norm": 37.037818908691406,
      "learning_rate": 5.306060997117908e-06,
      "loss": 1.2821,
      "step": 13495
    },
    {
      "epoch": 5.224932249322493,
      "grad_norm": 74.0390396118164,
      "learning_rate": 5.30563083408612e-06,
      "loss": 1.4815,
      "step": 13496
    },
    {
      "epoch": 5.225319396051104,
      "grad_norm": 72.2639389038086,
      "learning_rate": 5.3052006710543304e-06,
      "loss": 2.8335,
      "step": 13497
    },
    {
      "epoch": 5.225706542779713,
      "grad_norm": 190.2113494873047,
      "learning_rate": 5.304770508022541e-06,
      "loss": 0.6903,
      "step": 13498
    },
    {
      "epoch": 5.226093689508324,
      "grad_norm": 12.013014793395996,
      "learning_rate": 5.3043403449907516e-06,
      "loss": 0.4245,
      "step": 13499
    },
    {
      "epoch": 5.2264808362369335,
      "grad_norm": 54.184356689453125,
      "learning_rate": 5.303910181958963e-06,
      "loss": 2.1401,
      "step": 13500
    },
    {
      "epoch": 5.226867982965544,
      "grad_norm": 27.63278579711914,
      "learning_rate": 5.303480018927174e-06,
      "loss": 1.0937,
      "step": 13501
    },
    {
      "epoch": 5.2272551296941545,
      "grad_norm": 76.92180633544922,
      "learning_rate": 5.303049855895385e-06,
      "loss": 0.8318,
      "step": 13502
    },
    {
      "epoch": 5.227642276422764,
      "grad_norm": 54.90230178833008,
      "learning_rate": 5.3026196928635955e-06,
      "loss": 0.8089,
      "step": 13503
    },
    {
      "epoch": 5.228029423151375,
      "grad_norm": 6.196142673492432,
      "learning_rate": 5.302189529831807e-06,
      "loss": 0.15,
      "step": 13504
    },
    {
      "epoch": 5.228416569879984,
      "grad_norm": 66.6767807006836,
      "learning_rate": 5.3017593668000175e-06,
      "loss": 1.9609,
      "step": 13505
    },
    {
      "epoch": 5.228803716608595,
      "grad_norm": 157.30563354492188,
      "learning_rate": 5.301329203768228e-06,
      "loss": 2.2369,
      "step": 13506
    },
    {
      "epoch": 5.229190863337204,
      "grad_norm": 177.3017578125,
      "learning_rate": 5.3008990407364395e-06,
      "loss": 2.185,
      "step": 13507
    },
    {
      "epoch": 5.229578010065815,
      "grad_norm": 8.378351211547852,
      "learning_rate": 5.300468877704651e-06,
      "loss": 0.9117,
      "step": 13508
    },
    {
      "epoch": 5.229965156794425,
      "grad_norm": 49.6811408996582,
      "learning_rate": 5.3000387146728615e-06,
      "loss": 1.8711,
      "step": 13509
    },
    {
      "epoch": 5.230352303523035,
      "grad_norm": 125.08657836914062,
      "learning_rate": 5.299608551641072e-06,
      "loss": 1.5428,
      "step": 13510
    },
    {
      "epoch": 5.2307394502516456,
      "grad_norm": 32.760093688964844,
      "learning_rate": 5.299178388609283e-06,
      "loss": 1.2028,
      "step": 13511
    },
    {
      "epoch": 5.231126596980255,
      "grad_norm": 8.003532409667969,
      "learning_rate": 5.298748225577495e-06,
      "loss": 0.1531,
      "step": 13512
    },
    {
      "epoch": 5.231513743708866,
      "grad_norm": 24.131513595581055,
      "learning_rate": 5.2983180625457055e-06,
      "loss": 0.8958,
      "step": 13513
    },
    {
      "epoch": 5.231900890437476,
      "grad_norm": 24.020906448364258,
      "learning_rate": 5.297887899513916e-06,
      "loss": 1.6397,
      "step": 13514
    },
    {
      "epoch": 5.232288037166086,
      "grad_norm": 66.13343048095703,
      "learning_rate": 5.297457736482127e-06,
      "loss": 0.3412,
      "step": 13515
    },
    {
      "epoch": 5.232675183894696,
      "grad_norm": 37.048362731933594,
      "learning_rate": 5.297027573450339e-06,
      "loss": 0.615,
      "step": 13516
    },
    {
      "epoch": 5.233062330623306,
      "grad_norm": 46.33266067504883,
      "learning_rate": 5.2965974104185494e-06,
      "loss": 1.9232,
      "step": 13517
    },
    {
      "epoch": 5.2334494773519165,
      "grad_norm": 11.778300285339355,
      "learning_rate": 5.29616724738676e-06,
      "loss": 0.9384,
      "step": 13518
    },
    {
      "epoch": 5.233836624080526,
      "grad_norm": 139.6261444091797,
      "learning_rate": 5.2957370843549706e-06,
      "loss": 1.5166,
      "step": 13519
    },
    {
      "epoch": 5.234223770809137,
      "grad_norm": 26.74677085876465,
      "learning_rate": 5.295306921323182e-06,
      "loss": 0.4979,
      "step": 13520
    },
    {
      "epoch": 5.234610917537747,
      "grad_norm": 9.749455451965332,
      "learning_rate": 5.2948767582913926e-06,
      "loss": 0.396,
      "step": 13521
    },
    {
      "epoch": 5.234998064266357,
      "grad_norm": 19.216426849365234,
      "learning_rate": 5.294446595259604e-06,
      "loss": 0.4673,
      "step": 13522
    },
    {
      "epoch": 5.235385210994967,
      "grad_norm": 23.276817321777344,
      "learning_rate": 5.2940164322278145e-06,
      "loss": 1.4945,
      "step": 13523
    },
    {
      "epoch": 5.235772357723577,
      "grad_norm": 26.150188446044922,
      "learning_rate": 5.293586269196026e-06,
      "loss": 0.8581,
      "step": 13524
    },
    {
      "epoch": 5.236159504452187,
      "grad_norm": 46.45004653930664,
      "learning_rate": 5.2931561061642365e-06,
      "loss": 3.3238,
      "step": 13525
    },
    {
      "epoch": 5.236546651180798,
      "grad_norm": 19.12889862060547,
      "learning_rate": 5.292725943132447e-06,
      "loss": 1.0352,
      "step": 13526
    },
    {
      "epoch": 5.2369337979094075,
      "grad_norm": 15.048346519470215,
      "learning_rate": 5.292295780100658e-06,
      "loss": 1.4475,
      "step": 13527
    },
    {
      "epoch": 5.237320944638018,
      "grad_norm": 127.52366638183594,
      "learning_rate": 5.29186561706887e-06,
      "loss": 0.8696,
      "step": 13528
    },
    {
      "epoch": 5.237708091366628,
      "grad_norm": 58.0111083984375,
      "learning_rate": 5.2914354540370805e-06,
      "loss": 0.4937,
      "step": 13529
    },
    {
      "epoch": 5.238095238095238,
      "grad_norm": 94.68534851074219,
      "learning_rate": 5.291005291005291e-06,
      "loss": 1.5207,
      "step": 13530
    },
    {
      "epoch": 5.238482384823849,
      "grad_norm": 17.867923736572266,
      "learning_rate": 5.290575127973503e-06,
      "loss": 1.7123,
      "step": 13531
    },
    {
      "epoch": 5.238869531552458,
      "grad_norm": 8.78809928894043,
      "learning_rate": 5.290144964941714e-06,
      "loss": 0.2823,
      "step": 13532
    },
    {
      "epoch": 5.239256678281069,
      "grad_norm": 6.122128963470459,
      "learning_rate": 5.2897148019099245e-06,
      "loss": 0.6196,
      "step": 13533
    },
    {
      "epoch": 5.239643825009678,
      "grad_norm": 62.75677490234375,
      "learning_rate": 5.289284638878135e-06,
      "loss": 1.3374,
      "step": 13534
    },
    {
      "epoch": 5.240030971738289,
      "grad_norm": 135.2406463623047,
      "learning_rate": 5.2888544758463464e-06,
      "loss": 2.8817,
      "step": 13535
    },
    {
      "epoch": 5.2404181184668985,
      "grad_norm": 29.22331428527832,
      "learning_rate": 5.288424312814557e-06,
      "loss": 0.7325,
      "step": 13536
    },
    {
      "epoch": 5.240805265195509,
      "grad_norm": 6.5794501304626465,
      "learning_rate": 5.2879941497827684e-06,
      "loss": 0.4023,
      "step": 13537
    },
    {
      "epoch": 5.2411924119241196,
      "grad_norm": 75.81289672851562,
      "learning_rate": 5.287563986750979e-06,
      "loss": 1.3421,
      "step": 13538
    },
    {
      "epoch": 5.241579558652729,
      "grad_norm": 49.65423583984375,
      "learning_rate": 5.28713382371919e-06,
      "loss": 0.8881,
      "step": 13539
    },
    {
      "epoch": 5.24196670538134,
      "grad_norm": 42.69853591918945,
      "learning_rate": 5.286703660687401e-06,
      "loss": 1.2215,
      "step": 13540
    },
    {
      "epoch": 5.242353852109949,
      "grad_norm": 24.596508026123047,
      "learning_rate": 5.2862734976556115e-06,
      "loss": 2.345,
      "step": 13541
    },
    {
      "epoch": 5.24274099883856,
      "grad_norm": 41.28165817260742,
      "learning_rate": 5.285843334623822e-06,
      "loss": 0.4305,
      "step": 13542
    },
    {
      "epoch": 5.24312814556717,
      "grad_norm": 53.58635330200195,
      "learning_rate": 5.285413171592034e-06,
      "loss": 1.0726,
      "step": 13543
    },
    {
      "epoch": 5.24351529229578,
      "grad_norm": 92.01019287109375,
      "learning_rate": 5.284983008560245e-06,
      "loss": 0.7726,
      "step": 13544
    },
    {
      "epoch": 5.2439024390243905,
      "grad_norm": 19.05412483215332,
      "learning_rate": 5.2845528455284555e-06,
      "loss": 1.6203,
      "step": 13545
    },
    {
      "epoch": 5.244289585753,
      "grad_norm": 2.0860397815704346,
      "learning_rate": 5.284122682496666e-06,
      "loss": 0.0884,
      "step": 13546
    },
    {
      "epoch": 5.244676732481611,
      "grad_norm": 32.96331787109375,
      "learning_rate": 5.283692519464878e-06,
      "loss": 0.6665,
      "step": 13547
    },
    {
      "epoch": 5.245063879210221,
      "grad_norm": 16.82838249206543,
      "learning_rate": 5.283262356433089e-06,
      "loss": 1.3972,
      "step": 13548
    },
    {
      "epoch": 5.245451025938831,
      "grad_norm": 50.375999450683594,
      "learning_rate": 5.2828321934012995e-06,
      "loss": 0.7743,
      "step": 13549
    },
    {
      "epoch": 5.245838172667441,
      "grad_norm": 44.17202377319336,
      "learning_rate": 5.28240203036951e-06,
      "loss": 2.0889,
      "step": 13550
    },
    {
      "epoch": 5.246225319396051,
      "grad_norm": 3.022902250289917,
      "learning_rate": 5.2819718673377215e-06,
      "loss": 0.1216,
      "step": 13551
    },
    {
      "epoch": 5.246612466124661,
      "grad_norm": 53.21907043457031,
      "learning_rate": 5.281541704305933e-06,
      "loss": 0.8372,
      "step": 13552
    },
    {
      "epoch": 5.246999612853271,
      "grad_norm": 75.17900848388672,
      "learning_rate": 5.2811115412741435e-06,
      "loss": 0.7681,
      "step": 13553
    },
    {
      "epoch": 5.2473867595818815,
      "grad_norm": 20.56397819519043,
      "learning_rate": 5.280681378242354e-06,
      "loss": 1.9203,
      "step": 13554
    },
    {
      "epoch": 5.247773906310492,
      "grad_norm": 49.10069274902344,
      "learning_rate": 5.2802512152105654e-06,
      "loss": 0.7044,
      "step": 13555
    },
    {
      "epoch": 5.248161053039102,
      "grad_norm": 9.221449851989746,
      "learning_rate": 5.279821052178776e-06,
      "loss": 0.1462,
      "step": 13556
    },
    {
      "epoch": 5.248548199767712,
      "grad_norm": 66.70475769042969,
      "learning_rate": 5.279390889146987e-06,
      "loss": 1.9165,
      "step": 13557
    },
    {
      "epoch": 5.248935346496322,
      "grad_norm": 37.978294372558594,
      "learning_rate": 5.278960726115198e-06,
      "loss": 0.6795,
      "step": 13558
    },
    {
      "epoch": 5.249322493224932,
      "grad_norm": 59.12067794799805,
      "learning_rate": 5.278530563083409e-06,
      "loss": 1.5874,
      "step": 13559
    },
    {
      "epoch": 5.249709639953543,
      "grad_norm": 20.15290069580078,
      "learning_rate": 5.27810040005162e-06,
      "loss": 1.487,
      "step": 13560
    },
    {
      "epoch": 5.250096786682152,
      "grad_norm": 12.385324478149414,
      "learning_rate": 5.2776702370198305e-06,
      "loss": 0.4792,
      "step": 13561
    },
    {
      "epoch": 5.250483933410763,
      "grad_norm": 3.9697744846343994,
      "learning_rate": 5.277240073988041e-06,
      "loss": 0.1783,
      "step": 13562
    },
    {
      "epoch": 5.2508710801393725,
      "grad_norm": 50.73426818847656,
      "learning_rate": 5.276809910956253e-06,
      "loss": 3.5048,
      "step": 13563
    },
    {
      "epoch": 5.251258226867983,
      "grad_norm": 33.82699203491211,
      "learning_rate": 5.276379747924464e-06,
      "loss": 1.7392,
      "step": 13564
    },
    {
      "epoch": 5.251645373596594,
      "grad_norm": 68.52120208740234,
      "learning_rate": 5.2759495848926745e-06,
      "loss": 2.4849,
      "step": 13565
    },
    {
      "epoch": 5.252032520325203,
      "grad_norm": 25.484294891357422,
      "learning_rate": 5.275519421860885e-06,
      "loss": 0.0974,
      "step": 13566
    },
    {
      "epoch": 5.252419667053814,
      "grad_norm": 4.08237361907959,
      "learning_rate": 5.275089258829097e-06,
      "loss": 0.1954,
      "step": 13567
    },
    {
      "epoch": 5.252806813782423,
      "grad_norm": 76.58304595947266,
      "learning_rate": 5.274659095797308e-06,
      "loss": 2.2286,
      "step": 13568
    },
    {
      "epoch": 5.253193960511034,
      "grad_norm": 47.427032470703125,
      "learning_rate": 5.2742289327655185e-06,
      "loss": 0.3863,
      "step": 13569
    },
    {
      "epoch": 5.253581107239643,
      "grad_norm": 10.6262845993042,
      "learning_rate": 5.27379876973373e-06,
      "loss": 0.4272,
      "step": 13570
    },
    {
      "epoch": 5.253968253968254,
      "grad_norm": 14.0717191696167,
      "learning_rate": 5.2733686067019405e-06,
      "loss": 0.7678,
      "step": 13571
    },
    {
      "epoch": 5.2543554006968645,
      "grad_norm": 137.4764862060547,
      "learning_rate": 5.272938443670151e-06,
      "loss": 0.942,
      "step": 13572
    },
    {
      "epoch": 5.254742547425474,
      "grad_norm": 58.19913864135742,
      "learning_rate": 5.2725082806383625e-06,
      "loss": 1.5576,
      "step": 13573
    },
    {
      "epoch": 5.255129694154085,
      "grad_norm": 6.407485008239746,
      "learning_rate": 5.272078117606574e-06,
      "loss": 0.3545,
      "step": 13574
    },
    {
      "epoch": 5.255516840882694,
      "grad_norm": 42.807552337646484,
      "learning_rate": 5.2716479545747844e-06,
      "loss": 1.5228,
      "step": 13575
    },
    {
      "epoch": 5.255903987611305,
      "grad_norm": 58.74840545654297,
      "learning_rate": 5.271217791542995e-06,
      "loss": 0.4321,
      "step": 13576
    },
    {
      "epoch": 5.256291134339915,
      "grad_norm": 78.47268676757812,
      "learning_rate": 5.2707876285112056e-06,
      "loss": 2.3479,
      "step": 13577
    },
    {
      "epoch": 5.256678281068525,
      "grad_norm": 36.589332580566406,
      "learning_rate": 5.270357465479418e-06,
      "loss": 0.7252,
      "step": 13578
    },
    {
      "epoch": 5.257065427797135,
      "grad_norm": 98.02420806884766,
      "learning_rate": 5.269927302447628e-06,
      "loss": 0.7729,
      "step": 13579
    },
    {
      "epoch": 5.257452574525745,
      "grad_norm": 23.290185928344727,
      "learning_rate": 5.269497139415839e-06,
      "loss": 1.2788,
      "step": 13580
    },
    {
      "epoch": 5.2578397212543555,
      "grad_norm": 44.406192779541016,
      "learning_rate": 5.2690669763840495e-06,
      "loss": 1.6827,
      "step": 13581
    },
    {
      "epoch": 5.258226867982966,
      "grad_norm": 27.5078125,
      "learning_rate": 5.268636813352262e-06,
      "loss": 0.8512,
      "step": 13582
    },
    {
      "epoch": 5.258614014711576,
      "grad_norm": 172.32351684570312,
      "learning_rate": 5.268206650320472e-06,
      "loss": 0.7718,
      "step": 13583
    },
    {
      "epoch": 5.259001161440186,
      "grad_norm": 3.9220855236053467,
      "learning_rate": 5.267776487288683e-06,
      "loss": 0.1042,
      "step": 13584
    },
    {
      "epoch": 5.259388308168796,
      "grad_norm": 10.962516784667969,
      "learning_rate": 5.2673463242568935e-06,
      "loss": 0.2047,
      "step": 13585
    },
    {
      "epoch": 5.259775454897406,
      "grad_norm": 11.981846809387207,
      "learning_rate": 5.266916161225105e-06,
      "loss": 0.2514,
      "step": 13586
    },
    {
      "epoch": 5.260162601626016,
      "grad_norm": 35.718406677246094,
      "learning_rate": 5.2664859981933155e-06,
      "loss": 1.5819,
      "step": 13587
    },
    {
      "epoch": 5.260549748354626,
      "grad_norm": 22.970136642456055,
      "learning_rate": 5.266055835161527e-06,
      "loss": 2.0139,
      "step": 13588
    },
    {
      "epoch": 5.260936895083237,
      "grad_norm": 27.297182083129883,
      "learning_rate": 5.2656256721297375e-06,
      "loss": 1.7052,
      "step": 13589
    },
    {
      "epoch": 5.2613240418118465,
      "grad_norm": 66.29450988769531,
      "learning_rate": 5.265195509097949e-06,
      "loss": 1.8461,
      "step": 13590
    },
    {
      "epoch": 5.261711188540457,
      "grad_norm": 23.738019943237305,
      "learning_rate": 5.2647653460661595e-06,
      "loss": 1.972,
      "step": 13591
    },
    {
      "epoch": 5.262098335269067,
      "grad_norm": 47.65178680419922,
      "learning_rate": 5.26433518303437e-06,
      "loss": 1.1038,
      "step": 13592
    },
    {
      "epoch": 5.262485481997677,
      "grad_norm": 9.976452827453613,
      "learning_rate": 5.263905020002581e-06,
      "loss": 0.246,
      "step": 13593
    },
    {
      "epoch": 5.262872628726287,
      "grad_norm": 23.460582733154297,
      "learning_rate": 5.263474856970793e-06,
      "loss": 1.5606,
      "step": 13594
    },
    {
      "epoch": 5.263259775454897,
      "grad_norm": 27.752958297729492,
      "learning_rate": 5.2630446939390034e-06,
      "loss": 1.4494,
      "step": 13595
    },
    {
      "epoch": 5.263646922183508,
      "grad_norm": 33.024539947509766,
      "learning_rate": 5.262614530907214e-06,
      "loss": 0.7229,
      "step": 13596
    },
    {
      "epoch": 5.2640340689121174,
      "grad_norm": 4.937562942504883,
      "learning_rate": 5.2621843678754246e-06,
      "loss": 0.2124,
      "step": 13597
    },
    {
      "epoch": 5.264421215640728,
      "grad_norm": 12.37831974029541,
      "learning_rate": 5.261754204843637e-06,
      "loss": 0.3652,
      "step": 13598
    },
    {
      "epoch": 5.264808362369338,
      "grad_norm": 36.552734375,
      "learning_rate": 5.261324041811847e-06,
      "loss": 0.7691,
      "step": 13599
    },
    {
      "epoch": 5.265195509097948,
      "grad_norm": 39.315330505371094,
      "learning_rate": 5.260893878780058e-06,
      "loss": 0.59,
      "step": 13600
    },
    {
      "epoch": 5.265582655826559,
      "grad_norm": 130.92909240722656,
      "learning_rate": 5.2604637157482685e-06,
      "loss": 4.171,
      "step": 13601
    },
    {
      "epoch": 5.265969802555168,
      "grad_norm": 120.20816040039062,
      "learning_rate": 5.26003355271648e-06,
      "loss": 1.0247,
      "step": 13602
    },
    {
      "epoch": 5.266356949283779,
      "grad_norm": 4.86549711227417,
      "learning_rate": 5.259603389684691e-06,
      "loss": 0.1839,
      "step": 13603
    },
    {
      "epoch": 5.266744096012388,
      "grad_norm": 16.88113021850586,
      "learning_rate": 5.259173226652902e-06,
      "loss": 1.256,
      "step": 13604
    },
    {
      "epoch": 5.267131242740999,
      "grad_norm": 26.05615997314453,
      "learning_rate": 5.2587430636211125e-06,
      "loss": 1.1368,
      "step": 13605
    },
    {
      "epoch": 5.267518389469609,
      "grad_norm": 29.48737144470215,
      "learning_rate": 5.258312900589324e-06,
      "loss": 0.584,
      "step": 13606
    },
    {
      "epoch": 5.267905536198219,
      "grad_norm": 64.6136703491211,
      "learning_rate": 5.2578827375575345e-06,
      "loss": 2.6005,
      "step": 13607
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 72.45665740966797,
      "learning_rate": 5.257452574525745e-06,
      "loss": 0.8955,
      "step": 13608
    },
    {
      "epoch": 5.268679829655439,
      "grad_norm": 25.542022705078125,
      "learning_rate": 5.2570224114939565e-06,
      "loss": 1.261,
      "step": 13609
    },
    {
      "epoch": 5.26906697638405,
      "grad_norm": 21.622753143310547,
      "learning_rate": 5.256592248462168e-06,
      "loss": 0.5822,
      "step": 13610
    },
    {
      "epoch": 5.269454123112659,
      "grad_norm": 3.3334245681762695,
      "learning_rate": 5.2561620854303785e-06,
      "loss": 0.1069,
      "step": 13611
    },
    {
      "epoch": 5.26984126984127,
      "grad_norm": 66.02265167236328,
      "learning_rate": 5.255731922398589e-06,
      "loss": 2.5223,
      "step": 13612
    },
    {
      "epoch": 5.27022841656988,
      "grad_norm": 27.263521194458008,
      "learning_rate": 5.255301759366801e-06,
      "loss": 0.6555,
      "step": 13613
    },
    {
      "epoch": 5.27061556329849,
      "grad_norm": 22.58685874938965,
      "learning_rate": 5.254871596335012e-06,
      "loss": 1.5542,
      "step": 13614
    },
    {
      "epoch": 5.2710027100271,
      "grad_norm": 9.899879455566406,
      "learning_rate": 5.2544414333032224e-06,
      "loss": 0.9095,
      "step": 13615
    },
    {
      "epoch": 5.27138985675571,
      "grad_norm": 9.515061378479004,
      "learning_rate": 5.254011270271433e-06,
      "loss": 0.1935,
      "step": 13616
    },
    {
      "epoch": 5.2717770034843205,
      "grad_norm": 45.7869987487793,
      "learning_rate": 5.253581107239644e-06,
      "loss": 1.5085,
      "step": 13617
    },
    {
      "epoch": 5.272164150212931,
      "grad_norm": 35.412723541259766,
      "learning_rate": 5.253150944207856e-06,
      "loss": 0.8813,
      "step": 13618
    },
    {
      "epoch": 5.272551296941541,
      "grad_norm": 17.408096313476562,
      "learning_rate": 5.252720781176066e-06,
      "loss": 0.4797,
      "step": 13619
    },
    {
      "epoch": 5.272938443670151,
      "grad_norm": 14.596006393432617,
      "learning_rate": 5.252290618144277e-06,
      "loss": 0.3002,
      "step": 13620
    },
    {
      "epoch": 5.273325590398761,
      "grad_norm": 37.88474655151367,
      "learning_rate": 5.251860455112488e-06,
      "loss": 1.2721,
      "step": 13621
    },
    {
      "epoch": 5.273712737127371,
      "grad_norm": 25.24083709716797,
      "learning_rate": 5.251430292080699e-06,
      "loss": 0.707,
      "step": 13622
    },
    {
      "epoch": 5.274099883855982,
      "grad_norm": 3.248197317123413,
      "learning_rate": 5.2510001290489095e-06,
      "loss": 0.1024,
      "step": 13623
    },
    {
      "epoch": 5.2744870305845915,
      "grad_norm": 15.009902000427246,
      "learning_rate": 5.250569966017121e-06,
      "loss": 0.3679,
      "step": 13624
    },
    {
      "epoch": 5.274874177313202,
      "grad_norm": 21.35745620727539,
      "learning_rate": 5.250139802985332e-06,
      "loss": 1.8373,
      "step": 13625
    },
    {
      "epoch": 5.275261324041812,
      "grad_norm": 18.11680030822754,
      "learning_rate": 5.249709639953543e-06,
      "loss": 0.3619,
      "step": 13626
    },
    {
      "epoch": 5.275648470770422,
      "grad_norm": 96.75530242919922,
      "learning_rate": 5.2492794769217535e-06,
      "loss": 2.1295,
      "step": 13627
    },
    {
      "epoch": 5.276035617499032,
      "grad_norm": 7.037982940673828,
      "learning_rate": 5.248849313889964e-06,
      "loss": 0.3129,
      "step": 13628
    },
    {
      "epoch": 5.276422764227642,
      "grad_norm": 66.42311096191406,
      "learning_rate": 5.248419150858176e-06,
      "loss": 0.8874,
      "step": 13629
    },
    {
      "epoch": 5.276809910956253,
      "grad_norm": 19.84235191345215,
      "learning_rate": 5.247988987826387e-06,
      "loss": 0.642,
      "step": 13630
    },
    {
      "epoch": 5.277197057684862,
      "grad_norm": 23.940021514892578,
      "learning_rate": 5.2475588247945975e-06,
      "loss": 1.2933,
      "step": 13631
    },
    {
      "epoch": 5.277584204413473,
      "grad_norm": 26.719179153442383,
      "learning_rate": 5.247128661762808e-06,
      "loss": 0.3124,
      "step": 13632
    },
    {
      "epoch": 5.2779713511420825,
      "grad_norm": 53.71841812133789,
      "learning_rate": 5.24669849873102e-06,
      "loss": 0.748,
      "step": 13633
    },
    {
      "epoch": 5.278358497870693,
      "grad_norm": 36.99001693725586,
      "learning_rate": 5.246268335699231e-06,
      "loss": 0.8207,
      "step": 13634
    },
    {
      "epoch": 5.2787456445993035,
      "grad_norm": 26.799524307250977,
      "learning_rate": 5.2458381726674414e-06,
      "loss": 1.8896,
      "step": 13635
    },
    {
      "epoch": 5.279132791327913,
      "grad_norm": 25.73526382446289,
      "learning_rate": 5.245408009635652e-06,
      "loss": 2.132,
      "step": 13636
    },
    {
      "epoch": 5.279519938056524,
      "grad_norm": 8.413851737976074,
      "learning_rate": 5.244977846603863e-06,
      "loss": 0.2203,
      "step": 13637
    },
    {
      "epoch": 5.279907084785133,
      "grad_norm": 57.5627326965332,
      "learning_rate": 5.244547683572074e-06,
      "loss": 2.1908,
      "step": 13638
    },
    {
      "epoch": 5.280294231513744,
      "grad_norm": 19.0592098236084,
      "learning_rate": 5.244117520540285e-06,
      "loss": 0.3622,
      "step": 13639
    },
    {
      "epoch": 5.280681378242354,
      "grad_norm": 64.96610260009766,
      "learning_rate": 5.243687357508496e-06,
      "loss": 1.9191,
      "step": 13640
    },
    {
      "epoch": 5.281068524970964,
      "grad_norm": 1.8440414667129517,
      "learning_rate": 5.243257194476707e-06,
      "loss": 0.0708,
      "step": 13641
    },
    {
      "epoch": 5.281455671699574,
      "grad_norm": 20.655628204345703,
      "learning_rate": 5.242827031444918e-06,
      "loss": 1.5559,
      "step": 13642
    },
    {
      "epoch": 5.281842818428184,
      "grad_norm": 28.142433166503906,
      "learning_rate": 5.2423968684131285e-06,
      "loss": 2.3405,
      "step": 13643
    },
    {
      "epoch": 5.2822299651567945,
      "grad_norm": 46.19163513183594,
      "learning_rate": 5.241966705381339e-06,
      "loss": 1.4203,
      "step": 13644
    },
    {
      "epoch": 5.282617111885404,
      "grad_norm": 15.079347610473633,
      "learning_rate": 5.241536542349551e-06,
      "loss": 0.4446,
      "step": 13645
    },
    {
      "epoch": 5.283004258614015,
      "grad_norm": 29.5733699798584,
      "learning_rate": 5.241106379317762e-06,
      "loss": 0.5873,
      "step": 13646
    },
    {
      "epoch": 5.283391405342625,
      "grad_norm": 27.0723934173584,
      "learning_rate": 5.2406762162859725e-06,
      "loss": 0.8275,
      "step": 13647
    },
    {
      "epoch": 5.283778552071235,
      "grad_norm": 12.079591751098633,
      "learning_rate": 5.240246053254183e-06,
      "loss": 0.3251,
      "step": 13648
    },
    {
      "epoch": 5.284165698799845,
      "grad_norm": 6.510493755340576,
      "learning_rate": 5.239815890222395e-06,
      "loss": 0.1864,
      "step": 13649
    },
    {
      "epoch": 5.284552845528455,
      "grad_norm": 10.475154876708984,
      "learning_rate": 5.239385727190606e-06,
      "loss": 0.3645,
      "step": 13650
    },
    {
      "epoch": 5.2849399922570655,
      "grad_norm": 3.8780341148376465,
      "learning_rate": 5.2389555641588165e-06,
      "loss": 0.1148,
      "step": 13651
    },
    {
      "epoch": 5.285327138985676,
      "grad_norm": 97.1824722290039,
      "learning_rate": 5.238525401127028e-06,
      "loss": 0.9587,
      "step": 13652
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 31.87212562561035,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 0.574,
      "step": 13653
    },
    {
      "epoch": 5.286101432442896,
      "grad_norm": 30.443809509277344,
      "learning_rate": 5.23766507506345e-06,
      "loss": 1.3646,
      "step": 13654
    },
    {
      "epoch": 5.286488579171506,
      "grad_norm": 132.16571044921875,
      "learning_rate": 5.2372349120316604e-06,
      "loss": 1.426,
      "step": 13655
    },
    {
      "epoch": 5.286875725900116,
      "grad_norm": 125.45694732666016,
      "learning_rate": 5.236804748999872e-06,
      "loss": 0.7152,
      "step": 13656
    },
    {
      "epoch": 5.287262872628727,
      "grad_norm": 5.121857166290283,
      "learning_rate": 5.236374585968082e-06,
      "loss": 0.1209,
      "step": 13657
    },
    {
      "epoch": 5.287650019357336,
      "grad_norm": 11.247665405273438,
      "learning_rate": 5.235944422936293e-06,
      "loss": 0.4991,
      "step": 13658
    },
    {
      "epoch": 5.288037166085947,
      "grad_norm": 21.493501663208008,
      "learning_rate": 5.2355142599045035e-06,
      "loss": 1.6101,
      "step": 13659
    },
    {
      "epoch": 5.2884243128145565,
      "grad_norm": 53.75347137451172,
      "learning_rate": 5.235084096872716e-06,
      "loss": 1.0126,
      "step": 13660
    },
    {
      "epoch": 5.288811459543167,
      "grad_norm": 94.13487243652344,
      "learning_rate": 5.234653933840926e-06,
      "loss": 1.7233,
      "step": 13661
    },
    {
      "epoch": 5.289198606271777,
      "grad_norm": 23.29389190673828,
      "learning_rate": 5.234223770809137e-06,
      "loss": 0.9393,
      "step": 13662
    },
    {
      "epoch": 5.289585753000387,
      "grad_norm": 42.76198959350586,
      "learning_rate": 5.2337936077773475e-06,
      "loss": 1.6347,
      "step": 13663
    },
    {
      "epoch": 5.289972899728998,
      "grad_norm": 30.969221115112305,
      "learning_rate": 5.23336344474556e-06,
      "loss": 1.4833,
      "step": 13664
    },
    {
      "epoch": 5.290360046457607,
      "grad_norm": 81.64307403564453,
      "learning_rate": 5.23293328171377e-06,
      "loss": 1.2075,
      "step": 13665
    },
    {
      "epoch": 5.290747193186218,
      "grad_norm": 28.182086944580078,
      "learning_rate": 5.232503118681981e-06,
      "loss": 0.6722,
      "step": 13666
    },
    {
      "epoch": 5.291134339914827,
      "grad_norm": 89.20841217041016,
      "learning_rate": 5.2320729556501915e-06,
      "loss": 2.0137,
      "step": 13667
    },
    {
      "epoch": 5.291521486643438,
      "grad_norm": 9.501631736755371,
      "learning_rate": 5.231642792618403e-06,
      "loss": 0.677,
      "step": 13668
    },
    {
      "epoch": 5.291908633372048,
      "grad_norm": 31.10942268371582,
      "learning_rate": 5.231212629586614e-06,
      "loss": 1.7968,
      "step": 13669
    },
    {
      "epoch": 5.292295780100658,
      "grad_norm": 43.712520599365234,
      "learning_rate": 5.230782466554825e-06,
      "loss": 2.0021,
      "step": 13670
    },
    {
      "epoch": 5.2926829268292686,
      "grad_norm": 31.961538314819336,
      "learning_rate": 5.2303523035230355e-06,
      "loss": 0.7817,
      "step": 13671
    },
    {
      "epoch": 5.293070073557878,
      "grad_norm": 10.15314769744873,
      "learning_rate": 5.229922140491247e-06,
      "loss": 0.4294,
      "step": 13672
    },
    {
      "epoch": 5.293457220286489,
      "grad_norm": 127.73467254638672,
      "learning_rate": 5.2294919774594574e-06,
      "loss": 1.1609,
      "step": 13673
    },
    {
      "epoch": 5.293844367015098,
      "grad_norm": 80.76460266113281,
      "learning_rate": 5.229061814427668e-06,
      "loss": 0.7006,
      "step": 13674
    },
    {
      "epoch": 5.294231513743709,
      "grad_norm": 12.043755531311035,
      "learning_rate": 5.2286316513958794e-06,
      "loss": 0.6902,
      "step": 13675
    },
    {
      "epoch": 5.294618660472319,
      "grad_norm": 111.53878021240234,
      "learning_rate": 5.228201488364091e-06,
      "loss": 0.919,
      "step": 13676
    },
    {
      "epoch": 5.295005807200929,
      "grad_norm": 6.927179336547852,
      "learning_rate": 5.227771325332301e-06,
      "loss": 0.2241,
      "step": 13677
    },
    {
      "epoch": 5.2953929539295395,
      "grad_norm": 45.385292053222656,
      "learning_rate": 5.227341162300512e-06,
      "loss": 0.4987,
      "step": 13678
    },
    {
      "epoch": 5.295780100658149,
      "grad_norm": 9.36751937866211,
      "learning_rate": 5.2269109992687225e-06,
      "loss": 0.5729,
      "step": 13679
    },
    {
      "epoch": 5.29616724738676,
      "grad_norm": 74.4720687866211,
      "learning_rate": 5.226480836236935e-06,
      "loss": 0.5016,
      "step": 13680
    },
    {
      "epoch": 5.29655439411537,
      "grad_norm": 30.63255500793457,
      "learning_rate": 5.226050673205145e-06,
      "loss": 1.924,
      "step": 13681
    },
    {
      "epoch": 5.29694154084398,
      "grad_norm": 29.75812530517578,
      "learning_rate": 5.225620510173356e-06,
      "loss": 2.0648,
      "step": 13682
    },
    {
      "epoch": 5.29732868757259,
      "grad_norm": 49.71622085571289,
      "learning_rate": 5.2251903471415665e-06,
      "loss": 1.395,
      "step": 13683
    },
    {
      "epoch": 5.2977158343012,
      "grad_norm": 23.669137954711914,
      "learning_rate": 5.224760184109779e-06,
      "loss": 0.8757,
      "step": 13684
    },
    {
      "epoch": 5.29810298102981,
      "grad_norm": 41.34529113769531,
      "learning_rate": 5.224330021077989e-06,
      "loss": 1.8657,
      "step": 13685
    },
    {
      "epoch": 5.29849012775842,
      "grad_norm": 7.913342475891113,
      "learning_rate": 5.2238998580462e-06,
      "loss": 0.2886,
      "step": 13686
    },
    {
      "epoch": 5.2988772744870305,
      "grad_norm": 10.890509605407715,
      "learning_rate": 5.2234696950144105e-06,
      "loss": 0.3484,
      "step": 13687
    },
    {
      "epoch": 5.299264421215641,
      "grad_norm": 46.96791458129883,
      "learning_rate": 5.223039531982622e-06,
      "loss": 0.79,
      "step": 13688
    },
    {
      "epoch": 5.299651567944251,
      "grad_norm": 2.486656904220581,
      "learning_rate": 5.2226093689508325e-06,
      "loss": 0.1042,
      "step": 13689
    },
    {
      "epoch": 5.300038714672861,
      "grad_norm": 28.45871925354004,
      "learning_rate": 5.222179205919044e-06,
      "loss": 1.0114,
      "step": 13690
    },
    {
      "epoch": 5.300425861401471,
      "grad_norm": 0.943780779838562,
      "learning_rate": 5.2217490428872545e-06,
      "loss": 0.0251,
      "step": 13691
    },
    {
      "epoch": 5.300813008130081,
      "grad_norm": 35.50868225097656,
      "learning_rate": 5.221318879855466e-06,
      "loss": 1.7843,
      "step": 13692
    },
    {
      "epoch": 5.301200154858692,
      "grad_norm": 44.16606140136719,
      "learning_rate": 5.2208887168236764e-06,
      "loss": 0.6271,
      "step": 13693
    },
    {
      "epoch": 5.301587301587301,
      "grad_norm": 32.17860412597656,
      "learning_rate": 5.220458553791887e-06,
      "loss": 1.7712,
      "step": 13694
    },
    {
      "epoch": 5.301974448315912,
      "grad_norm": 42.1372184753418,
      "learning_rate": 5.220028390760099e-06,
      "loss": 0.9755,
      "step": 13695
    },
    {
      "epoch": 5.3023615950445215,
      "grad_norm": 28.872894287109375,
      "learning_rate": 5.21959822772831e-06,
      "loss": 0.6184,
      "step": 13696
    },
    {
      "epoch": 5.302748741773132,
      "grad_norm": 31.053926467895508,
      "learning_rate": 5.21916806469652e-06,
      "loss": 1.1118,
      "step": 13697
    },
    {
      "epoch": 5.303135888501743,
      "grad_norm": 64.5101318359375,
      "learning_rate": 5.218737901664731e-06,
      "loss": 0.9265,
      "step": 13698
    },
    {
      "epoch": 5.303523035230352,
      "grad_norm": 58.46323013305664,
      "learning_rate": 5.218307738632943e-06,
      "loss": 2.1327,
      "step": 13699
    },
    {
      "epoch": 5.303910181958963,
      "grad_norm": 49.58075714111328,
      "learning_rate": 5.217877575601154e-06,
      "loss": 1.9462,
      "step": 13700
    },
    {
      "epoch": 5.304297328687572,
      "grad_norm": 39.93809509277344,
      "learning_rate": 5.217447412569364e-06,
      "loss": 1.3351,
      "step": 13701
    },
    {
      "epoch": 5.304684475416183,
      "grad_norm": 72.19670104980469,
      "learning_rate": 5.217017249537575e-06,
      "loss": 1.0552,
      "step": 13702
    },
    {
      "epoch": 5.305071622144792,
      "grad_norm": 63.352779388427734,
      "learning_rate": 5.216587086505786e-06,
      "loss": 2.9603,
      "step": 13703
    },
    {
      "epoch": 5.305458768873403,
      "grad_norm": 115.70043182373047,
      "learning_rate": 5.216156923473997e-06,
      "loss": 0.5067,
      "step": 13704
    },
    {
      "epoch": 5.3058459156020135,
      "grad_norm": 113.09867858886719,
      "learning_rate": 5.215726760442208e-06,
      "loss": 2.8656,
      "step": 13705
    },
    {
      "epoch": 5.306233062330623,
      "grad_norm": 21.8841552734375,
      "learning_rate": 5.215296597410419e-06,
      "loss": 2.0354,
      "step": 13706
    },
    {
      "epoch": 5.306620209059234,
      "grad_norm": 20.717506408691406,
      "learning_rate": 5.21486643437863e-06,
      "loss": 1.1191,
      "step": 13707
    },
    {
      "epoch": 5.307007355787843,
      "grad_norm": 92.46430206298828,
      "learning_rate": 5.214436271346841e-06,
      "loss": 0.9519,
      "step": 13708
    },
    {
      "epoch": 5.307394502516454,
      "grad_norm": 76.873291015625,
      "learning_rate": 5.2140061083150515e-06,
      "loss": 1.4683,
      "step": 13709
    },
    {
      "epoch": 5.307781649245064,
      "grad_norm": 5.124859809875488,
      "learning_rate": 5.213575945283262e-06,
      "loss": 0.2615,
      "step": 13710
    },
    {
      "epoch": 5.308168795973674,
      "grad_norm": 12.038243293762207,
      "learning_rate": 5.213145782251474e-06,
      "loss": 0.2051,
      "step": 13711
    },
    {
      "epoch": 5.308555942702284,
      "grad_norm": 10.557720184326172,
      "learning_rate": 5.212715619219685e-06,
      "loss": 0.2974,
      "step": 13712
    },
    {
      "epoch": 5.308943089430894,
      "grad_norm": 16.11029624938965,
      "learning_rate": 5.2122854561878954e-06,
      "loss": 0.3784,
      "step": 13713
    },
    {
      "epoch": 5.3093302361595045,
      "grad_norm": 9.45495319366455,
      "learning_rate": 5.211855293156106e-06,
      "loss": 0.3799,
      "step": 13714
    },
    {
      "epoch": 5.309717382888115,
      "grad_norm": 22.470552444458008,
      "learning_rate": 5.211425130124318e-06,
      "loss": 1.4154,
      "step": 13715
    },
    {
      "epoch": 5.310104529616725,
      "grad_norm": 52.86726379394531,
      "learning_rate": 5.210994967092529e-06,
      "loss": 0.7811,
      "step": 13716
    },
    {
      "epoch": 5.310491676345335,
      "grad_norm": 30.678787231445312,
      "learning_rate": 5.210564804060739e-06,
      "loss": 1.2717,
      "step": 13717
    },
    {
      "epoch": 5.310878823073945,
      "grad_norm": 29.885696411132812,
      "learning_rate": 5.21013464102895e-06,
      "loss": 1.0701,
      "step": 13718
    },
    {
      "epoch": 5.311265969802555,
      "grad_norm": 6.297903060913086,
      "learning_rate": 5.209704477997161e-06,
      "loss": 0.1606,
      "step": 13719
    },
    {
      "epoch": 5.311653116531165,
      "grad_norm": 17.56540870666504,
      "learning_rate": 5.209274314965373e-06,
      "loss": 1.5301,
      "step": 13720
    },
    {
      "epoch": 5.312040263259775,
      "grad_norm": 39.821128845214844,
      "learning_rate": 5.208844151933583e-06,
      "loss": 1.7076,
      "step": 13721
    },
    {
      "epoch": 5.312427409988386,
      "grad_norm": 23.6876277923584,
      "learning_rate": 5.208413988901794e-06,
      "loss": 1.5408,
      "step": 13722
    },
    {
      "epoch": 5.3128145567169955,
      "grad_norm": 8.240092277526855,
      "learning_rate": 5.207983825870005e-06,
      "loss": 0.2628,
      "step": 13723
    },
    {
      "epoch": 5.313201703445606,
      "grad_norm": 31.316831588745117,
      "learning_rate": 5.207553662838216e-06,
      "loss": 1.8701,
      "step": 13724
    },
    {
      "epoch": 5.313588850174216,
      "grad_norm": 14.672436714172363,
      "learning_rate": 5.2071234998064265e-06,
      "loss": 0.4974,
      "step": 13725
    },
    {
      "epoch": 5.313975996902826,
      "grad_norm": 26.718801498413086,
      "learning_rate": 5.206693336774638e-06,
      "loss": 0.4073,
      "step": 13726
    },
    {
      "epoch": 5.314363143631437,
      "grad_norm": 15.285094261169434,
      "learning_rate": 5.206263173742849e-06,
      "loss": 0.7376,
      "step": 13727
    },
    {
      "epoch": 5.314750290360046,
      "grad_norm": 3.431436061859131,
      "learning_rate": 5.20583301071106e-06,
      "loss": 0.2018,
      "step": 13728
    },
    {
      "epoch": 5.315137437088657,
      "grad_norm": 58.142555236816406,
      "learning_rate": 5.2054028476792705e-06,
      "loss": 2.4154,
      "step": 13729
    },
    {
      "epoch": 5.3155245838172664,
      "grad_norm": 24.645734786987305,
      "learning_rate": 5.204972684647481e-06,
      "loss": 1.6899,
      "step": 13730
    },
    {
      "epoch": 5.315911730545877,
      "grad_norm": 127.01020812988281,
      "learning_rate": 5.204542521615693e-06,
      "loss": 3.0312,
      "step": 13731
    },
    {
      "epoch": 5.3162988772744875,
      "grad_norm": 13.890113830566406,
      "learning_rate": 5.204112358583904e-06,
      "loss": 0.6105,
      "step": 13732
    },
    {
      "epoch": 5.316686024003097,
      "grad_norm": 13.091507911682129,
      "learning_rate": 5.2036821955521144e-06,
      "loss": 0.3954,
      "step": 13733
    },
    {
      "epoch": 5.317073170731708,
      "grad_norm": 50.60475158691406,
      "learning_rate": 5.203252032520326e-06,
      "loss": 3.7485,
      "step": 13734
    },
    {
      "epoch": 5.317460317460317,
      "grad_norm": 78.64418029785156,
      "learning_rate": 5.202821869488537e-06,
      "loss": 1.4591,
      "step": 13735
    },
    {
      "epoch": 5.317847464188928,
      "grad_norm": 16.217233657836914,
      "learning_rate": 5.202391706456748e-06,
      "loss": 0.4607,
      "step": 13736
    },
    {
      "epoch": 5.318234610917537,
      "grad_norm": 31.818342208862305,
      "learning_rate": 5.201961543424958e-06,
      "loss": 1.4543,
      "step": 13737
    },
    {
      "epoch": 5.318621757646148,
      "grad_norm": 5.278947353363037,
      "learning_rate": 5.20153138039317e-06,
      "loss": 0.284,
      "step": 13738
    },
    {
      "epoch": 5.319008904374758,
      "grad_norm": 48.25008773803711,
      "learning_rate": 5.20110121736138e-06,
      "loss": 1.1998,
      "step": 13739
    },
    {
      "epoch": 5.319396051103368,
      "grad_norm": 51.76613998413086,
      "learning_rate": 5.200671054329591e-06,
      "loss": 0.5289,
      "step": 13740
    },
    {
      "epoch": 5.3197831978319785,
      "grad_norm": 47.61702346801758,
      "learning_rate": 5.200240891297802e-06,
      "loss": 1.0192,
      "step": 13741
    },
    {
      "epoch": 5.320170344560588,
      "grad_norm": 70.78964233398438,
      "learning_rate": 5.199810728266014e-06,
      "loss": 0.7082,
      "step": 13742
    },
    {
      "epoch": 5.320557491289199,
      "grad_norm": 66.10442352294922,
      "learning_rate": 5.199380565234224e-06,
      "loss": 1.9538,
      "step": 13743
    },
    {
      "epoch": 5.320944638017809,
      "grad_norm": 53.69835662841797,
      "learning_rate": 5.198950402202435e-06,
      "loss": 0.6698,
      "step": 13744
    },
    {
      "epoch": 5.321331784746419,
      "grad_norm": 3.5205042362213135,
      "learning_rate": 5.1985202391706455e-06,
      "loss": 0.1465,
      "step": 13745
    },
    {
      "epoch": 5.321718931475029,
      "grad_norm": 32.22466278076172,
      "learning_rate": 5.198090076138858e-06,
      "loss": 0.7037,
      "step": 13746
    },
    {
      "epoch": 5.322106078203639,
      "grad_norm": 207.86219787597656,
      "learning_rate": 5.197659913107068e-06,
      "loss": 0.5523,
      "step": 13747
    },
    {
      "epoch": 5.322493224932249,
      "grad_norm": 33.218685150146484,
      "learning_rate": 5.197229750075279e-06,
      "loss": 2.1783,
      "step": 13748
    },
    {
      "epoch": 5.32288037166086,
      "grad_norm": 18.05717658996582,
      "learning_rate": 5.1967995870434895e-06,
      "loss": 0.3227,
      "step": 13749
    },
    {
      "epoch": 5.3232675183894695,
      "grad_norm": 47.54640197753906,
      "learning_rate": 5.196369424011702e-06,
      "loss": 0.4344,
      "step": 13750
    },
    {
      "epoch": 5.32365466511808,
      "grad_norm": 11.131831169128418,
      "learning_rate": 5.195939260979912e-06,
      "loss": 0.4688,
      "step": 13751
    },
    {
      "epoch": 5.32404181184669,
      "grad_norm": 68.6688003540039,
      "learning_rate": 5.195509097948123e-06,
      "loss": 2.3632,
      "step": 13752
    },
    {
      "epoch": 5.3244289585753,
      "grad_norm": 28.753406524658203,
      "learning_rate": 5.1950789349163334e-06,
      "loss": 0.41,
      "step": 13753
    },
    {
      "epoch": 5.32481610530391,
      "grad_norm": 181.62350463867188,
      "learning_rate": 5.194648771884545e-06,
      "loss": 1.7793,
      "step": 13754
    },
    {
      "epoch": 5.32520325203252,
      "grad_norm": 33.27702331542969,
      "learning_rate": 5.194218608852755e-06,
      "loss": 0.6677,
      "step": 13755
    },
    {
      "epoch": 5.325590398761131,
      "grad_norm": 21.780052185058594,
      "learning_rate": 5.193788445820967e-06,
      "loss": 1.4328,
      "step": 13756
    },
    {
      "epoch": 5.3259775454897405,
      "grad_norm": 9.729826927185059,
      "learning_rate": 5.193358282789177e-06,
      "loss": 1.0266,
      "step": 13757
    },
    {
      "epoch": 5.326364692218351,
      "grad_norm": 34.060665130615234,
      "learning_rate": 5.192928119757389e-06,
      "loss": 1.8648,
      "step": 13758
    },
    {
      "epoch": 5.326751838946961,
      "grad_norm": 30.271751403808594,
      "learning_rate": 5.192497956725599e-06,
      "loss": 1.8337,
      "step": 13759
    },
    {
      "epoch": 5.327138985675571,
      "grad_norm": 8.48184871673584,
      "learning_rate": 5.19206779369381e-06,
      "loss": 0.417,
      "step": 13760
    },
    {
      "epoch": 5.327526132404181,
      "grad_norm": 44.89474868774414,
      "learning_rate": 5.1916376306620205e-06,
      "loss": 0.6322,
      "step": 13761
    },
    {
      "epoch": 5.327913279132791,
      "grad_norm": 9.144804954528809,
      "learning_rate": 5.191207467630233e-06,
      "loss": 0.2265,
      "step": 13762
    },
    {
      "epoch": 5.328300425861402,
      "grad_norm": 36.8561897277832,
      "learning_rate": 5.190777304598443e-06,
      "loss": 0.5336,
      "step": 13763
    },
    {
      "epoch": 5.328687572590011,
      "grad_norm": 31.816375732421875,
      "learning_rate": 5.190347141566654e-06,
      "loss": 2.1992,
      "step": 13764
    },
    {
      "epoch": 5.329074719318622,
      "grad_norm": 20.3799991607666,
      "learning_rate": 5.1899169785348645e-06,
      "loss": 1.4155,
      "step": 13765
    },
    {
      "epoch": 5.3294618660472315,
      "grad_norm": 212.98416137695312,
      "learning_rate": 5.189486815503077e-06,
      "loss": 1.3638,
      "step": 13766
    },
    {
      "epoch": 5.329849012775842,
      "grad_norm": 65.46800994873047,
      "learning_rate": 5.189056652471287e-06,
      "loss": 2.7723,
      "step": 13767
    },
    {
      "epoch": 5.3302361595044525,
      "grad_norm": 68.43637084960938,
      "learning_rate": 5.188626489439498e-06,
      "loss": 2.2568,
      "step": 13768
    },
    {
      "epoch": 5.330623306233062,
      "grad_norm": 8.603656768798828,
      "learning_rate": 5.1881963264077085e-06,
      "loss": 0.3881,
      "step": 13769
    },
    {
      "epoch": 5.331010452961673,
      "grad_norm": 41.721797943115234,
      "learning_rate": 5.18776616337592e-06,
      "loss": 0.6016,
      "step": 13770
    },
    {
      "epoch": 5.331397599690282,
      "grad_norm": 51.20488357543945,
      "learning_rate": 5.187336000344131e-06,
      "loss": 1.4457,
      "step": 13771
    },
    {
      "epoch": 5.331784746418893,
      "grad_norm": 3.289232015609741,
      "learning_rate": 5.186905837312342e-06,
      "loss": 0.0999,
      "step": 13772
    },
    {
      "epoch": 5.332171893147503,
      "grad_norm": 138.0779266357422,
      "learning_rate": 5.1864756742805524e-06,
      "loss": 1.3076,
      "step": 13773
    },
    {
      "epoch": 5.332559039876113,
      "grad_norm": 35.72968292236328,
      "learning_rate": 5.186045511248764e-06,
      "loss": 1.389,
      "step": 13774
    },
    {
      "epoch": 5.332946186604723,
      "grad_norm": 32.2161865234375,
      "learning_rate": 5.185615348216974e-06,
      "loss": 1.3437,
      "step": 13775
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 31.230051040649414,
      "learning_rate": 5.185185185185185e-06,
      "loss": 1.4069,
      "step": 13776
    },
    {
      "epoch": 5.3337204800619435,
      "grad_norm": 1.6960022449493408,
      "learning_rate": 5.184755022153397e-06,
      "loss": 0.0367,
      "step": 13777
    },
    {
      "epoch": 5.334107626790553,
      "grad_norm": 3.522521495819092,
      "learning_rate": 5.184324859121608e-06,
      "loss": 0.1327,
      "step": 13778
    },
    {
      "epoch": 5.334494773519164,
      "grad_norm": 212.57168579101562,
      "learning_rate": 5.183894696089818e-06,
      "loss": 1.3345,
      "step": 13779
    },
    {
      "epoch": 5.334881920247774,
      "grad_norm": 23.378671646118164,
      "learning_rate": 5.183464533058029e-06,
      "loss": 1.8247,
      "step": 13780
    },
    {
      "epoch": 5.335269066976384,
      "grad_norm": 146.13623046875,
      "learning_rate": 5.183034370026241e-06,
      "loss": 1.1537,
      "step": 13781
    },
    {
      "epoch": 5.335656213704994,
      "grad_norm": 29.591188430786133,
      "learning_rate": 5.182604206994452e-06,
      "loss": 1.6249,
      "step": 13782
    },
    {
      "epoch": 5.336043360433604,
      "grad_norm": 56.69141387939453,
      "learning_rate": 5.182174043962662e-06,
      "loss": 0.8735,
      "step": 13783
    },
    {
      "epoch": 5.3364305071622145,
      "grad_norm": 50.300567626953125,
      "learning_rate": 5.181743880930873e-06,
      "loss": 1.3667,
      "step": 13784
    },
    {
      "epoch": 5.336817653890825,
      "grad_norm": 7.180405616760254,
      "learning_rate": 5.181313717899084e-06,
      "loss": 0.3613,
      "step": 13785
    },
    {
      "epoch": 5.337204800619435,
      "grad_norm": 8.849043846130371,
      "learning_rate": 5.180883554867296e-06,
      "loss": 0.3695,
      "step": 13786
    },
    {
      "epoch": 5.337591947348045,
      "grad_norm": 24.162792205810547,
      "learning_rate": 5.180453391835506e-06,
      "loss": 1.5692,
      "step": 13787
    },
    {
      "epoch": 5.337979094076655,
      "grad_norm": 56.52573013305664,
      "learning_rate": 5.180023228803717e-06,
      "loss": 1.238,
      "step": 13788
    },
    {
      "epoch": 5.338366240805265,
      "grad_norm": 62.15316390991211,
      "learning_rate": 5.179593065771928e-06,
      "loss": 2.4125,
      "step": 13789
    },
    {
      "epoch": 5.338753387533876,
      "grad_norm": 83.44534301757812,
      "learning_rate": 5.179162902740139e-06,
      "loss": 1.8512,
      "step": 13790
    },
    {
      "epoch": 5.339140534262485,
      "grad_norm": 40.77899932861328,
      "learning_rate": 5.1787327397083494e-06,
      "loss": 3.4466,
      "step": 13791
    },
    {
      "epoch": 5.339527680991096,
      "grad_norm": 66.58106994628906,
      "learning_rate": 5.178302576676561e-06,
      "loss": 2.968,
      "step": 13792
    },
    {
      "epoch": 5.3399148277197055,
      "grad_norm": 50.16368103027344,
      "learning_rate": 5.177872413644772e-06,
      "loss": 0.4681,
      "step": 13793
    },
    {
      "epoch": 5.340301974448316,
      "grad_norm": 6.401741981506348,
      "learning_rate": 5.177442250612983e-06,
      "loss": 0.1863,
      "step": 13794
    },
    {
      "epoch": 5.340689121176926,
      "grad_norm": 7.0663299560546875,
      "learning_rate": 5.177012087581193e-06,
      "loss": 0.1834,
      "step": 13795
    },
    {
      "epoch": 5.341076267905536,
      "grad_norm": 25.314306259155273,
      "learning_rate": 5.176581924549404e-06,
      "loss": 0.3166,
      "step": 13796
    },
    {
      "epoch": 5.341463414634147,
      "grad_norm": 9.902657508850098,
      "learning_rate": 5.176151761517616e-06,
      "loss": 0.2654,
      "step": 13797
    },
    {
      "epoch": 5.341850561362756,
      "grad_norm": 151.55503845214844,
      "learning_rate": 5.175721598485827e-06,
      "loss": 2.4933,
      "step": 13798
    },
    {
      "epoch": 5.342237708091367,
      "grad_norm": 32.1292839050293,
      "learning_rate": 5.175291435454037e-06,
      "loss": 1.5984,
      "step": 13799
    },
    {
      "epoch": 5.342624854819976,
      "grad_norm": 64.16175842285156,
      "learning_rate": 5.174861272422248e-06,
      "loss": 2.0082,
      "step": 13800
    },
    {
      "epoch": 5.343012001548587,
      "grad_norm": 104.59666442871094,
      "learning_rate": 5.174431109390459e-06,
      "loss": 2.3199,
      "step": 13801
    },
    {
      "epoch": 5.343399148277197,
      "grad_norm": 8.55344295501709,
      "learning_rate": 5.174000946358671e-06,
      "loss": 0.3618,
      "step": 13802
    },
    {
      "epoch": 5.343786295005807,
      "grad_norm": 64.18882751464844,
      "learning_rate": 5.173570783326881e-06,
      "loss": 1.2528,
      "step": 13803
    },
    {
      "epoch": 5.3441734417344176,
      "grad_norm": 3.5933427810668945,
      "learning_rate": 5.173140620295092e-06,
      "loss": 0.1537,
      "step": 13804
    },
    {
      "epoch": 5.344560588463027,
      "grad_norm": 27.1746826171875,
      "learning_rate": 5.172710457263303e-06,
      "loss": 1.0237,
      "step": 13805
    },
    {
      "epoch": 5.344947735191638,
      "grad_norm": 28.88854217529297,
      "learning_rate": 5.172280294231514e-06,
      "loss": 1.4091,
      "step": 13806
    },
    {
      "epoch": 5.345334881920248,
      "grad_norm": 146.41278076171875,
      "learning_rate": 5.171850131199725e-06,
      "loss": 1.2313,
      "step": 13807
    },
    {
      "epoch": 5.345722028648858,
      "grad_norm": 39.957122802734375,
      "learning_rate": 5.171419968167936e-06,
      "loss": 1.969,
      "step": 13808
    },
    {
      "epoch": 5.346109175377468,
      "grad_norm": 30.36565589904785,
      "learning_rate": 5.170989805136147e-06,
      "loss": 1.4887,
      "step": 13809
    },
    {
      "epoch": 5.346496322106078,
      "grad_norm": 35.94863510131836,
      "learning_rate": 5.170559642104358e-06,
      "loss": 0.383,
      "step": 13810
    },
    {
      "epoch": 5.3468834688346885,
      "grad_norm": 58.70951843261719,
      "learning_rate": 5.1701294790725684e-06,
      "loss": 1.1842,
      "step": 13811
    },
    {
      "epoch": 5.347270615563298,
      "grad_norm": 75.49758911132812,
      "learning_rate": 5.169699316040779e-06,
      "loss": 1.9105,
      "step": 13812
    },
    {
      "epoch": 5.347657762291909,
      "grad_norm": 30.30082893371582,
      "learning_rate": 5.169269153008991e-06,
      "loss": 0.3902,
      "step": 13813
    },
    {
      "epoch": 5.348044909020519,
      "grad_norm": 98.38550567626953,
      "learning_rate": 5.168838989977202e-06,
      "loss": 3.6441,
      "step": 13814
    },
    {
      "epoch": 5.348432055749129,
      "grad_norm": 34.32997512817383,
      "learning_rate": 5.168408826945412e-06,
      "loss": 0.435,
      "step": 13815
    },
    {
      "epoch": 5.348819202477739,
      "grad_norm": 55.799495697021484,
      "learning_rate": 5.167978663913624e-06,
      "loss": 2.4892,
      "step": 13816
    },
    {
      "epoch": 5.349206349206349,
      "grad_norm": 7.437443733215332,
      "learning_rate": 5.167548500881835e-06,
      "loss": 0.3419,
      "step": 13817
    },
    {
      "epoch": 5.349593495934959,
      "grad_norm": 60.306854248046875,
      "learning_rate": 5.167118337850046e-06,
      "loss": 1.1349,
      "step": 13818
    },
    {
      "epoch": 5.34998064266357,
      "grad_norm": 18.458145141601562,
      "learning_rate": 5.166688174818256e-06,
      "loss": 0.6599,
      "step": 13819
    },
    {
      "epoch": 5.3503677893921795,
      "grad_norm": 59.9853630065918,
      "learning_rate": 5.166258011786468e-06,
      "loss": 1.9016,
      "step": 13820
    },
    {
      "epoch": 5.35075493612079,
      "grad_norm": 31.153127670288086,
      "learning_rate": 5.165827848754678e-06,
      "loss": 0.5446,
      "step": 13821
    },
    {
      "epoch": 5.3511420828494,
      "grad_norm": 14.973730087280273,
      "learning_rate": 5.16539768572289e-06,
      "loss": 0.5667,
      "step": 13822
    },
    {
      "epoch": 5.35152922957801,
      "grad_norm": 8.213757514953613,
      "learning_rate": 5.1649675226911e-06,
      "loss": 0.1548,
      "step": 13823
    },
    {
      "epoch": 5.351916376306621,
      "grad_norm": 5.183905124664307,
      "learning_rate": 5.164537359659312e-06,
      "loss": 0.2123,
      "step": 13824
    },
    {
      "epoch": 5.35230352303523,
      "grad_norm": 6.07446813583374,
      "learning_rate": 5.164107196627522e-06,
      "loss": 0.1501,
      "step": 13825
    },
    {
      "epoch": 5.352690669763841,
      "grad_norm": 61.520294189453125,
      "learning_rate": 5.163677033595733e-06,
      "loss": 0.5799,
      "step": 13826
    },
    {
      "epoch": 5.35307781649245,
      "grad_norm": 3.659160852432251,
      "learning_rate": 5.1632468705639435e-06,
      "loss": 0.1794,
      "step": 13827
    },
    {
      "epoch": 5.353464963221061,
      "grad_norm": 40.94102478027344,
      "learning_rate": 5.162816707532156e-06,
      "loss": 0.7477,
      "step": 13828
    },
    {
      "epoch": 5.3538521099496705,
      "grad_norm": 106.53933715820312,
      "learning_rate": 5.162386544500366e-06,
      "loss": 1.8773,
      "step": 13829
    },
    {
      "epoch": 5.354239256678281,
      "grad_norm": 40.734214782714844,
      "learning_rate": 5.161956381468577e-06,
      "loss": 3.2971,
      "step": 13830
    },
    {
      "epoch": 5.3546264034068916,
      "grad_norm": 22.290409088134766,
      "learning_rate": 5.1615262184367874e-06,
      "loss": 1.4212,
      "step": 13831
    },
    {
      "epoch": 5.355013550135501,
      "grad_norm": 4.8762688636779785,
      "learning_rate": 5.161096055405e-06,
      "loss": 0.225,
      "step": 13832
    },
    {
      "epoch": 5.355400696864112,
      "grad_norm": 5.821259498596191,
      "learning_rate": 5.16066589237321e-06,
      "loss": 0.2634,
      "step": 13833
    },
    {
      "epoch": 5.355787843592721,
      "grad_norm": 38.746002197265625,
      "learning_rate": 5.160235729341421e-06,
      "loss": 1.0598,
      "step": 13834
    },
    {
      "epoch": 5.356174990321332,
      "grad_norm": 7.292273998260498,
      "learning_rate": 5.159805566309631e-06,
      "loss": 0.1503,
      "step": 13835
    },
    {
      "epoch": 5.356562137049942,
      "grad_norm": 37.29642105102539,
      "learning_rate": 5.159375403277843e-06,
      "loss": 3.3166,
      "step": 13836
    },
    {
      "epoch": 5.356949283778552,
      "grad_norm": 70.81195831298828,
      "learning_rate": 5.158945240246053e-06,
      "loss": 1.2703,
      "step": 13837
    },
    {
      "epoch": 5.3573364305071625,
      "grad_norm": 98.53107452392578,
      "learning_rate": 5.158515077214265e-06,
      "loss": 0.7045,
      "step": 13838
    },
    {
      "epoch": 5.357723577235772,
      "grad_norm": 18.415267944335938,
      "learning_rate": 5.158084914182475e-06,
      "loss": 0.2586,
      "step": 13839
    },
    {
      "epoch": 5.358110723964383,
      "grad_norm": 28.31859588623047,
      "learning_rate": 5.157654751150687e-06,
      "loss": 3.3299,
      "step": 13840
    },
    {
      "epoch": 5.358497870692993,
      "grad_norm": 18.0284366607666,
      "learning_rate": 5.157224588118897e-06,
      "loss": 1.9858,
      "step": 13841
    },
    {
      "epoch": 5.358885017421603,
      "grad_norm": 46.510677337646484,
      "learning_rate": 5.156794425087108e-06,
      "loss": 0.5576,
      "step": 13842
    },
    {
      "epoch": 5.359272164150213,
      "grad_norm": 0.7350156307220459,
      "learning_rate": 5.156364262055319e-06,
      "loss": 0.0215,
      "step": 13843
    },
    {
      "epoch": 5.359659310878823,
      "grad_norm": 12.479622840881348,
      "learning_rate": 5.155934099023531e-06,
      "loss": 0.3069,
      "step": 13844
    },
    {
      "epoch": 5.360046457607433,
      "grad_norm": 17.37818145751953,
      "learning_rate": 5.155503935991741e-06,
      "loss": 2.2583,
      "step": 13845
    },
    {
      "epoch": 5.360433604336043,
      "grad_norm": 7.601504802703857,
      "learning_rate": 5.155073772959952e-06,
      "loss": 0.3996,
      "step": 13846
    },
    {
      "epoch": 5.3608207510646535,
      "grad_norm": 87.29153442382812,
      "learning_rate": 5.1546436099281625e-06,
      "loss": 1.6048,
      "step": 13847
    },
    {
      "epoch": 5.361207897793264,
      "grad_norm": 48.33873748779297,
      "learning_rate": 5.154213446896375e-06,
      "loss": 0.3608,
      "step": 13848
    },
    {
      "epoch": 5.361595044521874,
      "grad_norm": 42.692283630371094,
      "learning_rate": 5.153783283864585e-06,
      "loss": 0.2725,
      "step": 13849
    },
    {
      "epoch": 5.361982191250484,
      "grad_norm": 4.548160552978516,
      "learning_rate": 5.153353120832796e-06,
      "loss": 0.2956,
      "step": 13850
    },
    {
      "epoch": 5.362369337979094,
      "grad_norm": 29.990758895874023,
      "learning_rate": 5.1529229578010064e-06,
      "loss": 0.3417,
      "step": 13851
    },
    {
      "epoch": 5.362756484707704,
      "grad_norm": 75.87317657470703,
      "learning_rate": 5.152492794769218e-06,
      "loss": 1.005,
      "step": 13852
    },
    {
      "epoch": 5.363143631436314,
      "grad_norm": 46.60995864868164,
      "learning_rate": 5.152062631737429e-06,
      "loss": 1.3601,
      "step": 13853
    },
    {
      "epoch": 5.363530778164924,
      "grad_norm": 2.479321002960205,
      "learning_rate": 5.15163246870564e-06,
      "loss": 0.0844,
      "step": 13854
    },
    {
      "epoch": 5.363917924893535,
      "grad_norm": 20.715190887451172,
      "learning_rate": 5.15120230567385e-06,
      "loss": 2.1933,
      "step": 13855
    },
    {
      "epoch": 5.3643050716221445,
      "grad_norm": 48.728919982910156,
      "learning_rate": 5.150772142642062e-06,
      "loss": 1.1046,
      "step": 13856
    },
    {
      "epoch": 5.364692218350755,
      "grad_norm": 4.684244632720947,
      "learning_rate": 5.150341979610272e-06,
      "loss": 0.2269,
      "step": 13857
    },
    {
      "epoch": 5.365079365079365,
      "grad_norm": 36.4708137512207,
      "learning_rate": 5.149911816578484e-06,
      "loss": 0.7608,
      "step": 13858
    },
    {
      "epoch": 5.365466511807975,
      "grad_norm": 67.83187103271484,
      "learning_rate": 5.149481653546695e-06,
      "loss": 0.797,
      "step": 13859
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 72.54940795898438,
      "learning_rate": 5.149051490514906e-06,
      "loss": 4.448,
      "step": 13860
    },
    {
      "epoch": 5.366240805265195,
      "grad_norm": 55.19791793823242,
      "learning_rate": 5.148621327483116e-06,
      "loss": 3.5728,
      "step": 13861
    },
    {
      "epoch": 5.366627951993806,
      "grad_norm": 58.56978225708008,
      "learning_rate": 5.148191164451327e-06,
      "loss": 1.2527,
      "step": 13862
    },
    {
      "epoch": 5.3670150987224154,
      "grad_norm": 58.58259201049805,
      "learning_rate": 5.147761001419539e-06,
      "loss": 2.0133,
      "step": 13863
    },
    {
      "epoch": 5.367402245451026,
      "grad_norm": 14.04311752319336,
      "learning_rate": 5.14733083838775e-06,
      "loss": 0.4816,
      "step": 13864
    },
    {
      "epoch": 5.3677893921796365,
      "grad_norm": 32.47418212890625,
      "learning_rate": 5.14690067535596e-06,
      "loss": 2.9025,
      "step": 13865
    },
    {
      "epoch": 5.368176538908246,
      "grad_norm": 53.25886535644531,
      "learning_rate": 5.146470512324171e-06,
      "loss": 1.4672,
      "step": 13866
    },
    {
      "epoch": 5.368563685636857,
      "grad_norm": 6.70681619644165,
      "learning_rate": 5.146040349292382e-06,
      "loss": 0.3536,
      "step": 13867
    },
    {
      "epoch": 5.368950832365466,
      "grad_norm": 18.180830001831055,
      "learning_rate": 5.145610186260594e-06,
      "loss": 1.2793,
      "step": 13868
    },
    {
      "epoch": 5.369337979094077,
      "grad_norm": 3.351724147796631,
      "learning_rate": 5.145180023228804e-06,
      "loss": 0.178,
      "step": 13869
    },
    {
      "epoch": 5.369725125822686,
      "grad_norm": 21.22221565246582,
      "learning_rate": 5.144749860197015e-06,
      "loss": 1.381,
      "step": 13870
    },
    {
      "epoch": 5.370112272551297,
      "grad_norm": 3.4647622108459473,
      "learning_rate": 5.144319697165226e-06,
      "loss": 0.1856,
      "step": 13871
    },
    {
      "epoch": 5.370499419279907,
      "grad_norm": 71.0353012084961,
      "learning_rate": 5.143889534133437e-06,
      "loss": 2.5488,
      "step": 13872
    },
    {
      "epoch": 5.370886566008517,
      "grad_norm": 61.29257583618164,
      "learning_rate": 5.143459371101647e-06,
      "loss": 0.3469,
      "step": 13873
    },
    {
      "epoch": 5.3712737127371275,
      "grad_norm": 70.92681121826172,
      "learning_rate": 5.143029208069859e-06,
      "loss": 0.759,
      "step": 13874
    },
    {
      "epoch": 5.371660859465737,
      "grad_norm": 99.0845947265625,
      "learning_rate": 5.14259904503807e-06,
      "loss": 0.6369,
      "step": 13875
    },
    {
      "epoch": 5.372048006194348,
      "grad_norm": 40.54671859741211,
      "learning_rate": 5.142168882006281e-06,
      "loss": 2.3801,
      "step": 13876
    },
    {
      "epoch": 5.372435152922958,
      "grad_norm": 5.4002509117126465,
      "learning_rate": 5.141738718974491e-06,
      "loss": 0.267,
      "step": 13877
    },
    {
      "epoch": 5.372822299651568,
      "grad_norm": 33.65017318725586,
      "learning_rate": 5.141308555942702e-06,
      "loss": 1.1839,
      "step": 13878
    },
    {
      "epoch": 5.373209446380178,
      "grad_norm": 11.314276695251465,
      "learning_rate": 5.140878392910914e-06,
      "loss": 0.4252,
      "step": 13879
    },
    {
      "epoch": 5.373596593108788,
      "grad_norm": 27.024066925048828,
      "learning_rate": 5.140448229879125e-06,
      "loss": 0.2234,
      "step": 13880
    },
    {
      "epoch": 5.373983739837398,
      "grad_norm": 32.554203033447266,
      "learning_rate": 5.140018066847335e-06,
      "loss": 1.6298,
      "step": 13881
    },
    {
      "epoch": 5.374370886566009,
      "grad_norm": 49.2727165222168,
      "learning_rate": 5.139587903815546e-06,
      "loss": 1.041,
      "step": 13882
    },
    {
      "epoch": 5.3747580332946185,
      "grad_norm": 179.4549560546875,
      "learning_rate": 5.139157740783758e-06,
      "loss": 0.741,
      "step": 13883
    },
    {
      "epoch": 5.375145180023229,
      "grad_norm": 42.339256286621094,
      "learning_rate": 5.138727577751969e-06,
      "loss": 2.1708,
      "step": 13884
    },
    {
      "epoch": 5.375532326751839,
      "grad_norm": 30.565431594848633,
      "learning_rate": 5.138297414720179e-06,
      "loss": 1.2096,
      "step": 13885
    },
    {
      "epoch": 5.375919473480449,
      "grad_norm": 49.289100646972656,
      "learning_rate": 5.13786725168839e-06,
      "loss": 1.5985,
      "step": 13886
    },
    {
      "epoch": 5.376306620209059,
      "grad_norm": 29.813074111938477,
      "learning_rate": 5.137437088656601e-06,
      "loss": 1.3212,
      "step": 13887
    },
    {
      "epoch": 5.376693766937669,
      "grad_norm": 34.60493850708008,
      "learning_rate": 5.137006925624812e-06,
      "loss": 2.6086,
      "step": 13888
    },
    {
      "epoch": 5.37708091366628,
      "grad_norm": 12.07918930053711,
      "learning_rate": 5.136576762593023e-06,
      "loss": 0.9917,
      "step": 13889
    },
    {
      "epoch": 5.3774680603948894,
      "grad_norm": 10.431547164916992,
      "learning_rate": 5.136146599561234e-06,
      "loss": 0.1985,
      "step": 13890
    },
    {
      "epoch": 5.3778552071235,
      "grad_norm": 64.93643188476562,
      "learning_rate": 5.135716436529445e-06,
      "loss": 2.1954,
      "step": 13891
    },
    {
      "epoch": 5.37824235385211,
      "grad_norm": 6.433724880218506,
      "learning_rate": 5.135286273497656e-06,
      "loss": 0.27,
      "step": 13892
    },
    {
      "epoch": 5.37862950058072,
      "grad_norm": 28.26961326599121,
      "learning_rate": 5.134856110465866e-06,
      "loss": 1.3334,
      "step": 13893
    },
    {
      "epoch": 5.379016647309331,
      "grad_norm": 22.988868713378906,
      "learning_rate": 5.134425947434078e-06,
      "loss": 1.4951,
      "step": 13894
    },
    {
      "epoch": 5.37940379403794,
      "grad_norm": 6.722318649291992,
      "learning_rate": 5.133995784402289e-06,
      "loss": 0.3232,
      "step": 13895
    },
    {
      "epoch": 5.379790940766551,
      "grad_norm": 27.250123977661133,
      "learning_rate": 5.1335656213705e-06,
      "loss": 1.2887,
      "step": 13896
    },
    {
      "epoch": 5.38017808749516,
      "grad_norm": 22.815216064453125,
      "learning_rate": 5.13313545833871e-06,
      "loss": 0.2675,
      "step": 13897
    },
    {
      "epoch": 5.380565234223771,
      "grad_norm": 1.9393714666366577,
      "learning_rate": 5.132705295306923e-06,
      "loss": 0.071,
      "step": 13898
    },
    {
      "epoch": 5.380952380952381,
      "grad_norm": 16.95446014404297,
      "learning_rate": 5.132275132275133e-06,
      "loss": 0.5461,
      "step": 13899
    },
    {
      "epoch": 5.381339527680991,
      "grad_norm": 35.826786041259766,
      "learning_rate": 5.131844969243344e-06,
      "loss": 1.6305,
      "step": 13900
    },
    {
      "epoch": 5.3817266744096015,
      "grad_norm": 45.33041763305664,
      "learning_rate": 5.131414806211554e-06,
      "loss": 0.6502,
      "step": 13901
    },
    {
      "epoch": 5.382113821138211,
      "grad_norm": 45.21629333496094,
      "learning_rate": 5.130984643179766e-06,
      "loss": 1.2642,
      "step": 13902
    },
    {
      "epoch": 5.382500967866822,
      "grad_norm": 31.634035110473633,
      "learning_rate": 5.130554480147976e-06,
      "loss": 1.4985,
      "step": 13903
    },
    {
      "epoch": 5.382888114595431,
      "grad_norm": 5.1604790687561035,
      "learning_rate": 5.130124317116188e-06,
      "loss": 0.2008,
      "step": 13904
    },
    {
      "epoch": 5.383275261324042,
      "grad_norm": 7.994565010070801,
      "learning_rate": 5.129694154084398e-06,
      "loss": 0.2251,
      "step": 13905
    },
    {
      "epoch": 5.383662408052652,
      "grad_norm": 16.209239959716797,
      "learning_rate": 5.12926399105261e-06,
      "loss": 0.1705,
      "step": 13906
    },
    {
      "epoch": 5.384049554781262,
      "grad_norm": 3.909680128097534,
      "learning_rate": 5.12883382802082e-06,
      "loss": 0.0955,
      "step": 13907
    },
    {
      "epoch": 5.384436701509872,
      "grad_norm": 36.69404983520508,
      "learning_rate": 5.128403664989031e-06,
      "loss": 2.2935,
      "step": 13908
    },
    {
      "epoch": 5.384823848238482,
      "grad_norm": 30.12453842163086,
      "learning_rate": 5.1279735019572414e-06,
      "loss": 0.6005,
      "step": 13909
    },
    {
      "epoch": 5.3852109949670925,
      "grad_norm": 63.30181121826172,
      "learning_rate": 5.127543338925454e-06,
      "loss": 1.9111,
      "step": 13910
    },
    {
      "epoch": 5.385598141695703,
      "grad_norm": 15.009696006774902,
      "learning_rate": 5.127113175893664e-06,
      "loss": 0.363,
      "step": 13911
    },
    {
      "epoch": 5.385985288424313,
      "grad_norm": 63.324832916259766,
      "learning_rate": 5.126683012861875e-06,
      "loss": 1.2392,
      "step": 13912
    },
    {
      "epoch": 5.386372435152923,
      "grad_norm": 63.89873123168945,
      "learning_rate": 5.126252849830085e-06,
      "loss": 0.5727,
      "step": 13913
    },
    {
      "epoch": 5.386759581881533,
      "grad_norm": 6.810623645782471,
      "learning_rate": 5.125822686798298e-06,
      "loss": 0.316,
      "step": 13914
    },
    {
      "epoch": 5.387146728610143,
      "grad_norm": 3.8363864421844482,
      "learning_rate": 5.125392523766508e-06,
      "loss": 0.1142,
      "step": 13915
    },
    {
      "epoch": 5.387533875338754,
      "grad_norm": 4.540786266326904,
      "learning_rate": 5.124962360734719e-06,
      "loss": 0.2159,
      "step": 13916
    },
    {
      "epoch": 5.3879210220673635,
      "grad_norm": 19.960350036621094,
      "learning_rate": 5.124532197702929e-06,
      "loss": 1.1853,
      "step": 13917
    },
    {
      "epoch": 5.388308168795974,
      "grad_norm": 56.240657806396484,
      "learning_rate": 5.124102034671141e-06,
      "loss": 0.6523,
      "step": 13918
    },
    {
      "epoch": 5.388695315524584,
      "grad_norm": 61.78465270996094,
      "learning_rate": 5.123671871639352e-06,
      "loss": 0.7719,
      "step": 13919
    },
    {
      "epoch": 5.389082462253194,
      "grad_norm": 32.46689987182617,
      "learning_rate": 5.123241708607563e-06,
      "loss": 1.8981,
      "step": 13920
    },
    {
      "epoch": 5.389469608981804,
      "grad_norm": 108.55763244628906,
      "learning_rate": 5.122811545575773e-06,
      "loss": 2.0357,
      "step": 13921
    },
    {
      "epoch": 5.389856755710414,
      "grad_norm": 19.08648681640625,
      "learning_rate": 5.122381382543985e-06,
      "loss": 1.6319,
      "step": 13922
    },
    {
      "epoch": 5.390243902439025,
      "grad_norm": 33.42412567138672,
      "learning_rate": 5.121951219512195e-06,
      "loss": 1.4446,
      "step": 13923
    },
    {
      "epoch": 5.390631049167634,
      "grad_norm": 67.86970520019531,
      "learning_rate": 5.121521056480406e-06,
      "loss": 3.2053,
      "step": 13924
    },
    {
      "epoch": 5.391018195896245,
      "grad_norm": 82.35539245605469,
      "learning_rate": 5.121090893448617e-06,
      "loss": 0.4455,
      "step": 13925
    },
    {
      "epoch": 5.3914053426248545,
      "grad_norm": 31.269908905029297,
      "learning_rate": 5.120660730416829e-06,
      "loss": 1.8558,
      "step": 13926
    },
    {
      "epoch": 5.391792489353465,
      "grad_norm": 141.8293914794922,
      "learning_rate": 5.120230567385039e-06,
      "loss": 0.8303,
      "step": 13927
    },
    {
      "epoch": 5.3921796360820755,
      "grad_norm": 5.539148330688477,
      "learning_rate": 5.11980040435325e-06,
      "loss": 0.2658,
      "step": 13928
    },
    {
      "epoch": 5.392566782810685,
      "grad_norm": 48.795650482177734,
      "learning_rate": 5.1193702413214604e-06,
      "loss": 1.1792,
      "step": 13929
    },
    {
      "epoch": 5.392953929539296,
      "grad_norm": 3.045785665512085,
      "learning_rate": 5.118940078289673e-06,
      "loss": 0.1067,
      "step": 13930
    },
    {
      "epoch": 5.393341076267905,
      "grad_norm": 0.912152886390686,
      "learning_rate": 5.118509915257883e-06,
      "loss": 0.0268,
      "step": 13931
    },
    {
      "epoch": 5.393728222996516,
      "grad_norm": 66.6011962890625,
      "learning_rate": 5.118079752226094e-06,
      "loss": 0.9912,
      "step": 13932
    },
    {
      "epoch": 5.394115369725126,
      "grad_norm": 27.755456924438477,
      "learning_rate": 5.117649589194304e-06,
      "loss": 1.4048,
      "step": 13933
    },
    {
      "epoch": 5.394502516453736,
      "grad_norm": 34.244606018066406,
      "learning_rate": 5.117219426162517e-06,
      "loss": 2.7427,
      "step": 13934
    },
    {
      "epoch": 5.394889663182346,
      "grad_norm": 32.8289909362793,
      "learning_rate": 5.116789263130727e-06,
      "loss": 2.9521,
      "step": 13935
    },
    {
      "epoch": 5.395276809910956,
      "grad_norm": 22.231243133544922,
      "learning_rate": 5.116359100098938e-06,
      "loss": 1.7977,
      "step": 13936
    },
    {
      "epoch": 5.3956639566395665,
      "grad_norm": 44.792030334472656,
      "learning_rate": 5.115928937067148e-06,
      "loss": 1.6984,
      "step": 13937
    },
    {
      "epoch": 5.396051103368176,
      "grad_norm": 56.08823013305664,
      "learning_rate": 5.11549877403536e-06,
      "loss": 0.687,
      "step": 13938
    },
    {
      "epoch": 5.396438250096787,
      "grad_norm": 19.472938537597656,
      "learning_rate": 5.11506861100357e-06,
      "loss": 1.6812,
      "step": 13939
    },
    {
      "epoch": 5.396825396825397,
      "grad_norm": 17.01116180419922,
      "learning_rate": 5.114638447971782e-06,
      "loss": 1.709,
      "step": 13940
    },
    {
      "epoch": 5.397212543554007,
      "grad_norm": 95.34233093261719,
      "learning_rate": 5.114208284939993e-06,
      "loss": 1.8024,
      "step": 13941
    },
    {
      "epoch": 5.397599690282617,
      "grad_norm": 17.772293090820312,
      "learning_rate": 5.113778121908204e-06,
      "loss": 0.2736,
      "step": 13942
    },
    {
      "epoch": 5.397986837011227,
      "grad_norm": 21.654573440551758,
      "learning_rate": 5.113347958876414e-06,
      "loss": 1.835,
      "step": 13943
    },
    {
      "epoch": 5.3983739837398375,
      "grad_norm": 43.682716369628906,
      "learning_rate": 5.112917795844625e-06,
      "loss": 0.9066,
      "step": 13944
    },
    {
      "epoch": 5.398761130468447,
      "grad_norm": 8.149375915527344,
      "learning_rate": 5.112487632812837e-06,
      "loss": 0.2442,
      "step": 13945
    },
    {
      "epoch": 5.399148277197058,
      "grad_norm": 102.49395751953125,
      "learning_rate": 5.112057469781048e-06,
      "loss": 2.0631,
      "step": 13946
    },
    {
      "epoch": 5.399535423925668,
      "grad_norm": 19.041183471679688,
      "learning_rate": 5.111627306749258e-06,
      "loss": 0.3185,
      "step": 13947
    },
    {
      "epoch": 5.399922570654278,
      "grad_norm": 70.01995849609375,
      "learning_rate": 5.111197143717469e-06,
      "loss": 3.3478,
      "step": 13948
    },
    {
      "epoch": 5.400309717382888,
      "grad_norm": 33.52894592285156,
      "learning_rate": 5.110766980685681e-06,
      "loss": 1.4103,
      "step": 13949
    },
    {
      "epoch": 5.400696864111498,
      "grad_norm": 11.051959037780762,
      "learning_rate": 5.110336817653892e-06,
      "loss": 0.2431,
      "step": 13950
    },
    {
      "epoch": 5.401084010840108,
      "grad_norm": 37.88002014160156,
      "learning_rate": 5.109906654622102e-06,
      "loss": 0.5554,
      "step": 13951
    },
    {
      "epoch": 5.401471157568719,
      "grad_norm": 71.9992904663086,
      "learning_rate": 5.109476491590313e-06,
      "loss": 2.2718,
      "step": 13952
    },
    {
      "epoch": 5.4018583042973285,
      "grad_norm": 51.28570556640625,
      "learning_rate": 5.109046328558524e-06,
      "loss": 1.3602,
      "step": 13953
    },
    {
      "epoch": 5.402245451025939,
      "grad_norm": 6.6108293533325195,
      "learning_rate": 5.108616165526735e-06,
      "loss": 0.3146,
      "step": 13954
    },
    {
      "epoch": 5.402632597754549,
      "grad_norm": 18.316402435302734,
      "learning_rate": 5.108186002494946e-06,
      "loss": 0.322,
      "step": 13955
    },
    {
      "epoch": 5.403019744483159,
      "grad_norm": 4.48098087310791,
      "learning_rate": 5.107755839463157e-06,
      "loss": 0.1342,
      "step": 13956
    },
    {
      "epoch": 5.40340689121177,
      "grad_norm": 23.8886775970459,
      "learning_rate": 5.107325676431368e-06,
      "loss": 1.5561,
      "step": 13957
    },
    {
      "epoch": 5.403794037940379,
      "grad_norm": 36.511775970458984,
      "learning_rate": 5.106895513399579e-06,
      "loss": 1.8189,
      "step": 13958
    },
    {
      "epoch": 5.40418118466899,
      "grad_norm": 15.652791023254395,
      "learning_rate": 5.106465350367789e-06,
      "loss": 1.8204,
      "step": 13959
    },
    {
      "epoch": 5.404568331397599,
      "grad_norm": 22.82710838317871,
      "learning_rate": 5.106035187336e-06,
      "loss": 0.2985,
      "step": 13960
    },
    {
      "epoch": 5.40495547812621,
      "grad_norm": 26.594465255737305,
      "learning_rate": 5.105605024304212e-06,
      "loss": 1.5394,
      "step": 13961
    },
    {
      "epoch": 5.4053426248548195,
      "grad_norm": 3.158078193664551,
      "learning_rate": 5.105174861272423e-06,
      "loss": 0.1604,
      "step": 13962
    },
    {
      "epoch": 5.40572977158343,
      "grad_norm": 53.27691650390625,
      "learning_rate": 5.104744698240633e-06,
      "loss": 2.7326,
      "step": 13963
    },
    {
      "epoch": 5.4061169183120406,
      "grad_norm": 38.37116241455078,
      "learning_rate": 5.104314535208844e-06,
      "loss": 1.4226,
      "step": 13964
    },
    {
      "epoch": 5.40650406504065,
      "grad_norm": 43.88095474243164,
      "learning_rate": 5.103884372177056e-06,
      "loss": 0.9882,
      "step": 13965
    },
    {
      "epoch": 5.406891211769261,
      "grad_norm": 118.09538269042969,
      "learning_rate": 5.103454209145267e-06,
      "loss": 0.599,
      "step": 13966
    },
    {
      "epoch": 5.40727835849787,
      "grad_norm": 69.54750061035156,
      "learning_rate": 5.103024046113477e-06,
      "loss": 1.7558,
      "step": 13967
    },
    {
      "epoch": 5.407665505226481,
      "grad_norm": 153.8479766845703,
      "learning_rate": 5.102593883081688e-06,
      "loss": 0.7984,
      "step": 13968
    },
    {
      "epoch": 5.408052651955091,
      "grad_norm": 91.32392883300781,
      "learning_rate": 5.102163720049899e-06,
      "loss": 2.0788,
      "step": 13969
    },
    {
      "epoch": 5.408439798683701,
      "grad_norm": 99.12054443359375,
      "learning_rate": 5.101733557018111e-06,
      "loss": 0.3359,
      "step": 13970
    },
    {
      "epoch": 5.4088269454123115,
      "grad_norm": 65.06865692138672,
      "learning_rate": 5.101303393986321e-06,
      "loss": 1.1861,
      "step": 13971
    },
    {
      "epoch": 5.409214092140921,
      "grad_norm": 125.85189819335938,
      "learning_rate": 5.100873230954532e-06,
      "loss": 1.2636,
      "step": 13972
    },
    {
      "epoch": 5.409601238869532,
      "grad_norm": 67.59773254394531,
      "learning_rate": 5.100443067922743e-06,
      "loss": 2.103,
      "step": 13973
    },
    {
      "epoch": 5.409988385598142,
      "grad_norm": 32.52553176879883,
      "learning_rate": 5.100012904890954e-06,
      "loss": 1.9535,
      "step": 13974
    },
    {
      "epoch": 5.410375532326752,
      "grad_norm": 26.60552215576172,
      "learning_rate": 5.099582741859164e-06,
      "loss": 1.6122,
      "step": 13975
    },
    {
      "epoch": 5.410762679055362,
      "grad_norm": 22.353878021240234,
      "learning_rate": 5.099152578827376e-06,
      "loss": 0.3662,
      "step": 13976
    },
    {
      "epoch": 5.411149825783972,
      "grad_norm": 43.646480560302734,
      "learning_rate": 5.098722415795587e-06,
      "loss": 0.3834,
      "step": 13977
    },
    {
      "epoch": 5.411536972512582,
      "grad_norm": 6.077436923980713,
      "learning_rate": 5.098292252763798e-06,
      "loss": 0.3194,
      "step": 13978
    },
    {
      "epoch": 5.411924119241192,
      "grad_norm": 59.846832275390625,
      "learning_rate": 5.097862089732008e-06,
      "loss": 2.3339,
      "step": 13979
    },
    {
      "epoch": 5.4123112659698025,
      "grad_norm": 92.84505462646484,
      "learning_rate": 5.097431926700219e-06,
      "loss": 1.7203,
      "step": 13980
    },
    {
      "epoch": 5.412698412698413,
      "grad_norm": 4.529000282287598,
      "learning_rate": 5.097001763668431e-06,
      "loss": 0.164,
      "step": 13981
    },
    {
      "epoch": 5.413085559427023,
      "grad_norm": 8.460614204406738,
      "learning_rate": 5.096571600636642e-06,
      "loss": 0.264,
      "step": 13982
    },
    {
      "epoch": 5.413472706155633,
      "grad_norm": 11.569443702697754,
      "learning_rate": 5.096141437604852e-06,
      "loss": 0.2767,
      "step": 13983
    },
    {
      "epoch": 5.413859852884243,
      "grad_norm": 63.77716827392578,
      "learning_rate": 5.095711274573064e-06,
      "loss": 0.7899,
      "step": 13984
    },
    {
      "epoch": 5.414246999612853,
      "grad_norm": 113.4409408569336,
      "learning_rate": 5.095281111541275e-06,
      "loss": 0.5603,
      "step": 13985
    },
    {
      "epoch": 5.414634146341464,
      "grad_norm": 47.869483947753906,
      "learning_rate": 5.094850948509486e-06,
      "loss": 2.5742,
      "step": 13986
    },
    {
      "epoch": 5.415021293070073,
      "grad_norm": 80.5606689453125,
      "learning_rate": 5.094420785477696e-06,
      "loss": 0.5818,
      "step": 13987
    },
    {
      "epoch": 5.415408439798684,
      "grad_norm": 11.180002212524414,
      "learning_rate": 5.093990622445908e-06,
      "loss": 0.2642,
      "step": 13988
    },
    {
      "epoch": 5.4157955865272935,
      "grad_norm": 22.408302307128906,
      "learning_rate": 5.093560459414118e-06,
      "loss": 0.1257,
      "step": 13989
    },
    {
      "epoch": 5.416182733255904,
      "grad_norm": 13.104893684387207,
      "learning_rate": 5.093130296382329e-06,
      "loss": 0.3344,
      "step": 13990
    },
    {
      "epoch": 5.416569879984515,
      "grad_norm": 39.31742477416992,
      "learning_rate": 5.09270013335054e-06,
      "loss": 0.6376,
      "step": 13991
    },
    {
      "epoch": 5.416957026713124,
      "grad_norm": 32.7260627746582,
      "learning_rate": 5.092269970318752e-06,
      "loss": 1.5354,
      "step": 13992
    },
    {
      "epoch": 5.417344173441735,
      "grad_norm": 8.75050163269043,
      "learning_rate": 5.091839807286962e-06,
      "loss": 0.1939,
      "step": 13993
    },
    {
      "epoch": 5.417731320170344,
      "grad_norm": 106.61636352539062,
      "learning_rate": 5.091409644255173e-06,
      "loss": 3.3273,
      "step": 13994
    },
    {
      "epoch": 5.418118466898955,
      "grad_norm": 24.435396194458008,
      "learning_rate": 5.090979481223383e-06,
      "loss": 1.8391,
      "step": 13995
    },
    {
      "epoch": 5.418505613627564,
      "grad_norm": 3.9195499420166016,
      "learning_rate": 5.090549318191596e-06,
      "loss": 0.1026,
      "step": 13996
    },
    {
      "epoch": 5.418892760356175,
      "grad_norm": 20.716079711914062,
      "learning_rate": 5.090119155159806e-06,
      "loss": 0.3231,
      "step": 13997
    },
    {
      "epoch": 5.4192799070847855,
      "grad_norm": 143.03604125976562,
      "learning_rate": 5.089688992128017e-06,
      "loss": 2.4528,
      "step": 13998
    },
    {
      "epoch": 5.419667053813395,
      "grad_norm": 58.613121032714844,
      "learning_rate": 5.089258829096227e-06,
      "loss": 3.6537,
      "step": 13999
    },
    {
      "epoch": 5.420054200542006,
      "grad_norm": 19.57969856262207,
      "learning_rate": 5.08882866606444e-06,
      "loss": 0.7331,
      "step": 14000
    },
    {
      "epoch": 5.420441347270615,
      "grad_norm": 25.030529022216797,
      "learning_rate": 5.08839850303265e-06,
      "loss": 2.2179,
      "step": 14001
    },
    {
      "epoch": 5.420828493999226,
      "grad_norm": 6.006039619445801,
      "learning_rate": 5.087968340000861e-06,
      "loss": 0.1988,
      "step": 14002
    },
    {
      "epoch": 5.421215640727836,
      "grad_norm": 5.314766883850098,
      "learning_rate": 5.087538176969071e-06,
      "loss": 0.1509,
      "step": 14003
    },
    {
      "epoch": 5.421602787456446,
      "grad_norm": 2.3435606956481934,
      "learning_rate": 5.087108013937283e-06,
      "loss": 0.0862,
      "step": 14004
    },
    {
      "epoch": 5.421989934185056,
      "grad_norm": 23.657798767089844,
      "learning_rate": 5.086677850905493e-06,
      "loss": 0.4986,
      "step": 14005
    },
    {
      "epoch": 5.422377080913666,
      "grad_norm": 93.44438934326172,
      "learning_rate": 5.086247687873705e-06,
      "loss": 1.0922,
      "step": 14006
    },
    {
      "epoch": 5.4227642276422765,
      "grad_norm": 17.025447845458984,
      "learning_rate": 5.085817524841915e-06,
      "loss": 1.3917,
      "step": 14007
    },
    {
      "epoch": 5.423151374370887,
      "grad_norm": 33.08060836791992,
      "learning_rate": 5.085387361810127e-06,
      "loss": 3.6724,
      "step": 14008
    },
    {
      "epoch": 5.423538521099497,
      "grad_norm": 89.85179138183594,
      "learning_rate": 5.084957198778337e-06,
      "loss": 0.9893,
      "step": 14009
    },
    {
      "epoch": 5.423925667828107,
      "grad_norm": 39.93083953857422,
      "learning_rate": 5.084527035746548e-06,
      "loss": 0.5653,
      "step": 14010
    },
    {
      "epoch": 5.424312814556717,
      "grad_norm": 29.33432960510254,
      "learning_rate": 5.084096872714758e-06,
      "loss": 1.55,
      "step": 14011
    },
    {
      "epoch": 5.424699961285327,
      "grad_norm": 85.1871337890625,
      "learning_rate": 5.083666709682971e-06,
      "loss": 0.689,
      "step": 14012
    },
    {
      "epoch": 5.425087108013937,
      "grad_norm": 19.051406860351562,
      "learning_rate": 5.083236546651181e-06,
      "loss": 0.7049,
      "step": 14013
    },
    {
      "epoch": 5.425474254742547,
      "grad_norm": 64.45529174804688,
      "learning_rate": 5.082806383619392e-06,
      "loss": 2.0119,
      "step": 14014
    },
    {
      "epoch": 5.425861401471158,
      "grad_norm": 24.87609100341797,
      "learning_rate": 5.082376220587602e-06,
      "loss": 1.3949,
      "step": 14015
    },
    {
      "epoch": 5.4262485481997675,
      "grad_norm": 21.98406410217285,
      "learning_rate": 5.081946057555815e-06,
      "loss": 1.6756,
      "step": 14016
    },
    {
      "epoch": 5.426635694928378,
      "grad_norm": 16.62880516052246,
      "learning_rate": 5.081515894524025e-06,
      "loss": 0.3172,
      "step": 14017
    },
    {
      "epoch": 5.427022841656988,
      "grad_norm": 35.49968338012695,
      "learning_rate": 5.081085731492236e-06,
      "loss": 1.2378,
      "step": 14018
    },
    {
      "epoch": 5.427409988385598,
      "grad_norm": 62.28252410888672,
      "learning_rate": 5.080655568460446e-06,
      "loss": 1.3441,
      "step": 14019
    },
    {
      "epoch": 5.427797135114209,
      "grad_norm": 83.59466552734375,
      "learning_rate": 5.080225405428658e-06,
      "loss": 0.9266,
      "step": 14020
    },
    {
      "epoch": 5.428184281842818,
      "grad_norm": 32.546993255615234,
      "learning_rate": 5.079795242396869e-06,
      "loss": 1.5103,
      "step": 14021
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 84.2585220336914,
      "learning_rate": 5.07936507936508e-06,
      "loss": 1.0538,
      "step": 14022
    },
    {
      "epoch": 5.4289585753000384,
      "grad_norm": 54.129058837890625,
      "learning_rate": 5.078934916333291e-06,
      "loss": 1.0044,
      "step": 14023
    },
    {
      "epoch": 5.429345722028649,
      "grad_norm": 20.524921417236328,
      "learning_rate": 5.078504753301502e-06,
      "loss": 0.6678,
      "step": 14024
    },
    {
      "epoch": 5.4297328687572595,
      "grad_norm": 22.60572624206543,
      "learning_rate": 5.078074590269712e-06,
      "loss": 0.3252,
      "step": 14025
    },
    {
      "epoch": 5.430120015485869,
      "grad_norm": 80.69532775878906,
      "learning_rate": 5.077644427237923e-06,
      "loss": 1.4639,
      "step": 14026
    },
    {
      "epoch": 5.43050716221448,
      "grad_norm": 6.727116107940674,
      "learning_rate": 5.077214264206135e-06,
      "loss": 0.2986,
      "step": 14027
    },
    {
      "epoch": 5.430894308943089,
      "grad_norm": 82.3704833984375,
      "learning_rate": 5.076784101174346e-06,
      "loss": 1.2866,
      "step": 14028
    },
    {
      "epoch": 5.4312814556717,
      "grad_norm": 50.59378433227539,
      "learning_rate": 5.076353938142556e-06,
      "loss": 0.393,
      "step": 14029
    },
    {
      "epoch": 5.431668602400309,
      "grad_norm": 79.80380249023438,
      "learning_rate": 5.075923775110767e-06,
      "loss": 2.969,
      "step": 14030
    },
    {
      "epoch": 5.43205574912892,
      "grad_norm": 57.33973693847656,
      "learning_rate": 5.075493612078979e-06,
      "loss": 0.7495,
      "step": 14031
    },
    {
      "epoch": 5.43244289585753,
      "grad_norm": 44.4019775390625,
      "learning_rate": 5.07506344904719e-06,
      "loss": 0.7708,
      "step": 14032
    },
    {
      "epoch": 5.43283004258614,
      "grad_norm": 89.86437225341797,
      "learning_rate": 5.0746332860154e-06,
      "loss": 2.4891,
      "step": 14033
    },
    {
      "epoch": 5.4332171893147505,
      "grad_norm": 50.73392868041992,
      "learning_rate": 5.074203122983611e-06,
      "loss": 0.967,
      "step": 14034
    },
    {
      "epoch": 5.43360433604336,
      "grad_norm": 30.592992782592773,
      "learning_rate": 5.073772959951822e-06,
      "loss": 2.3366,
      "step": 14035
    },
    {
      "epoch": 5.433991482771971,
      "grad_norm": 65.42961883544922,
      "learning_rate": 5.073342796920034e-06,
      "loss": 1.7792,
      "step": 14036
    },
    {
      "epoch": 5.43437862950058,
      "grad_norm": 4.647179126739502,
      "learning_rate": 5.072912633888244e-06,
      "loss": 0.4294,
      "step": 14037
    },
    {
      "epoch": 5.434765776229191,
      "grad_norm": 15.441211700439453,
      "learning_rate": 5.072482470856455e-06,
      "loss": 1.3821,
      "step": 14038
    },
    {
      "epoch": 5.435152922957801,
      "grad_norm": 90.79753875732422,
      "learning_rate": 5.072052307824666e-06,
      "loss": 1.4323,
      "step": 14039
    },
    {
      "epoch": 5.435540069686411,
      "grad_norm": 33.586666107177734,
      "learning_rate": 5.071622144792877e-06,
      "loss": 1.5406,
      "step": 14040
    },
    {
      "epoch": 5.435927216415021,
      "grad_norm": 79.64962768554688,
      "learning_rate": 5.071191981761087e-06,
      "loss": 0.8049,
      "step": 14041
    },
    {
      "epoch": 5.436314363143631,
      "grad_norm": 59.28602600097656,
      "learning_rate": 5.070761818729299e-06,
      "loss": 1.2925,
      "step": 14042
    },
    {
      "epoch": 5.4367015098722415,
      "grad_norm": 33.89069747924805,
      "learning_rate": 5.07033165569751e-06,
      "loss": 1.3258,
      "step": 14043
    },
    {
      "epoch": 5.437088656600852,
      "grad_norm": 32.345096588134766,
      "learning_rate": 5.069901492665721e-06,
      "loss": 2.0804,
      "step": 14044
    },
    {
      "epoch": 5.437475803329462,
      "grad_norm": 4.318213939666748,
      "learning_rate": 5.069471329633931e-06,
      "loss": 0.1358,
      "step": 14045
    },
    {
      "epoch": 5.437862950058072,
      "grad_norm": 82.0537109375,
      "learning_rate": 5.069041166602142e-06,
      "loss": 0.9575,
      "step": 14046
    },
    {
      "epoch": 5.438250096786682,
      "grad_norm": 50.77726364135742,
      "learning_rate": 5.068611003570354e-06,
      "loss": 1.4115,
      "step": 14047
    },
    {
      "epoch": 5.438637243515292,
      "grad_norm": 32.04771423339844,
      "learning_rate": 5.068180840538565e-06,
      "loss": 0.5257,
      "step": 14048
    },
    {
      "epoch": 5.439024390243903,
      "grad_norm": 21.363880157470703,
      "learning_rate": 5.067750677506775e-06,
      "loss": 0.2639,
      "step": 14049
    },
    {
      "epoch": 5.4394115369725125,
      "grad_norm": 9.404531478881836,
      "learning_rate": 5.067320514474986e-06,
      "loss": 0.3247,
      "step": 14050
    },
    {
      "epoch": 5.439798683701123,
      "grad_norm": 22.532493591308594,
      "learning_rate": 5.066890351443198e-06,
      "loss": 2.2678,
      "step": 14051
    },
    {
      "epoch": 5.440185830429733,
      "grad_norm": 29.01918601989746,
      "learning_rate": 5.066460188411409e-06,
      "loss": 2.8622,
      "step": 14052
    },
    {
      "epoch": 5.440572977158343,
      "grad_norm": 87.58342742919922,
      "learning_rate": 5.066030025379619e-06,
      "loss": 1.809,
      "step": 14053
    },
    {
      "epoch": 5.440960123886953,
      "grad_norm": 7.264939308166504,
      "learning_rate": 5.06559986234783e-06,
      "loss": 0.2953,
      "step": 14054
    },
    {
      "epoch": 5.441347270615563,
      "grad_norm": 39.89384460449219,
      "learning_rate": 5.065169699316041e-06,
      "loss": 1.1175,
      "step": 14055
    },
    {
      "epoch": 5.441734417344174,
      "grad_norm": 73.68535614013672,
      "learning_rate": 5.064739536284252e-06,
      "loss": 1.2973,
      "step": 14056
    },
    {
      "epoch": 5.442121564072783,
      "grad_norm": 30.905059814453125,
      "learning_rate": 5.064309373252463e-06,
      "loss": 3.2446,
      "step": 14057
    },
    {
      "epoch": 5.442508710801394,
      "grad_norm": 25.006689071655273,
      "learning_rate": 5.063879210220674e-06,
      "loss": 1.2218,
      "step": 14058
    },
    {
      "epoch": 5.4428958575300035,
      "grad_norm": 20.98142433166504,
      "learning_rate": 5.063449047188885e-06,
      "loss": 0.6133,
      "step": 14059
    },
    {
      "epoch": 5.443283004258614,
      "grad_norm": 39.71049880981445,
      "learning_rate": 5.063018884157096e-06,
      "loss": 1.7163,
      "step": 14060
    },
    {
      "epoch": 5.4436701509872245,
      "grad_norm": 44.7543830871582,
      "learning_rate": 5.062588721125306e-06,
      "loss": 2.2315,
      "step": 14061
    },
    {
      "epoch": 5.444057297715834,
      "grad_norm": 148.09730529785156,
      "learning_rate": 5.062158558093517e-06,
      "loss": 0.6495,
      "step": 14062
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 39.99993133544922,
      "learning_rate": 5.061728395061729e-06,
      "loss": 0.7163,
      "step": 14063
    },
    {
      "epoch": 5.444831591173054,
      "grad_norm": 16.907379150390625,
      "learning_rate": 5.06129823202994e-06,
      "loss": 0.5031,
      "step": 14064
    },
    {
      "epoch": 5.445218737901665,
      "grad_norm": 3.8915293216705322,
      "learning_rate": 5.06086806899815e-06,
      "loss": 0.1751,
      "step": 14065
    },
    {
      "epoch": 5.445605884630275,
      "grad_norm": 4.876806735992432,
      "learning_rate": 5.0604379059663626e-06,
      "loss": 0.2144,
      "step": 14066
    },
    {
      "epoch": 5.445993031358885,
      "grad_norm": 15.069851875305176,
      "learning_rate": 5.060007742934573e-06,
      "loss": 0.7012,
      "step": 14067
    },
    {
      "epoch": 5.446380178087495,
      "grad_norm": 39.18903732299805,
      "learning_rate": 5.059577579902784e-06,
      "loss": 1.4267,
      "step": 14068
    },
    {
      "epoch": 5.446767324816105,
      "grad_norm": 99.52055358886719,
      "learning_rate": 5.059147416870994e-06,
      "loss": 4.3572,
      "step": 14069
    },
    {
      "epoch": 5.4471544715447155,
      "grad_norm": 23.278182983398438,
      "learning_rate": 5.058717253839206e-06,
      "loss": 0.8836,
      "step": 14070
    },
    {
      "epoch": 5.447541618273325,
      "grad_norm": 36.41954040527344,
      "learning_rate": 5.058287090807416e-06,
      "loss": 0.5141,
      "step": 14071
    },
    {
      "epoch": 5.447928765001936,
      "grad_norm": 80.16497039794922,
      "learning_rate": 5.057856927775628e-06,
      "loss": 2.4643,
      "step": 14072
    },
    {
      "epoch": 5.448315911730546,
      "grad_norm": 68.28723907470703,
      "learning_rate": 5.057426764743838e-06,
      "loss": 0.5097,
      "step": 14073
    },
    {
      "epoch": 5.448703058459156,
      "grad_norm": 33.6667594909668,
      "learning_rate": 5.05699660171205e-06,
      "loss": 0.7084,
      "step": 14074
    },
    {
      "epoch": 5.449090205187766,
      "grad_norm": 90.79946899414062,
      "learning_rate": 5.05656643868026e-06,
      "loss": 2.2418,
      "step": 14075
    },
    {
      "epoch": 5.449477351916376,
      "grad_norm": 48.902565002441406,
      "learning_rate": 5.056136275648471e-06,
      "loss": 1.9519,
      "step": 14076
    },
    {
      "epoch": 5.4498644986449865,
      "grad_norm": 5.338163375854492,
      "learning_rate": 5.055706112616681e-06,
      "loss": 0.1343,
      "step": 14077
    },
    {
      "epoch": 5.450251645373597,
      "grad_norm": 65.25344848632812,
      "learning_rate": 5.055275949584894e-06,
      "loss": 0.7575,
      "step": 14078
    },
    {
      "epoch": 5.450638792102207,
      "grad_norm": 71.82135009765625,
      "learning_rate": 5.054845786553104e-06,
      "loss": 2.2696,
      "step": 14079
    },
    {
      "epoch": 5.451025938830817,
      "grad_norm": 47.44717788696289,
      "learning_rate": 5.054415623521315e-06,
      "loss": 1.7702,
      "step": 14080
    },
    {
      "epoch": 5.451413085559427,
      "grad_norm": 5.9078755378723145,
      "learning_rate": 5.053985460489525e-06,
      "loss": 0.3012,
      "step": 14081
    },
    {
      "epoch": 5.451800232288037,
      "grad_norm": 8.54560375213623,
      "learning_rate": 5.053555297457738e-06,
      "loss": 0.209,
      "step": 14082
    },
    {
      "epoch": 5.452187379016648,
      "grad_norm": 0.6721850633621216,
      "learning_rate": 5.053125134425948e-06,
      "loss": 0.0189,
      "step": 14083
    },
    {
      "epoch": 5.452574525745257,
      "grad_norm": 22.994476318359375,
      "learning_rate": 5.052694971394159e-06,
      "loss": 1.2238,
      "step": 14084
    },
    {
      "epoch": 5.452961672473868,
      "grad_norm": 55.108036041259766,
      "learning_rate": 5.052264808362369e-06,
      "loss": 1.7759,
      "step": 14085
    },
    {
      "epoch": 5.4533488192024775,
      "grad_norm": 59.684017181396484,
      "learning_rate": 5.051834645330581e-06,
      "loss": 1.0294,
      "step": 14086
    },
    {
      "epoch": 5.453735965931088,
      "grad_norm": 43.380741119384766,
      "learning_rate": 5.051404482298792e-06,
      "loss": 1.5765,
      "step": 14087
    },
    {
      "epoch": 5.454123112659698,
      "grad_norm": 76.75035858154297,
      "learning_rate": 5.050974319267003e-06,
      "loss": 1.5136,
      "step": 14088
    },
    {
      "epoch": 5.454510259388308,
      "grad_norm": 56.791725158691406,
      "learning_rate": 5.050544156235213e-06,
      "loss": 0.8479,
      "step": 14089
    },
    {
      "epoch": 5.454897406116919,
      "grad_norm": 54.88615417480469,
      "learning_rate": 5.050113993203425e-06,
      "loss": 1.9231,
      "step": 14090
    },
    {
      "epoch": 5.455284552845528,
      "grad_norm": 105.93750762939453,
      "learning_rate": 5.049683830171635e-06,
      "loss": 1.8453,
      "step": 14091
    },
    {
      "epoch": 5.455671699574139,
      "grad_norm": 31.66617774963379,
      "learning_rate": 5.049253667139846e-06,
      "loss": 1.6835,
      "step": 14092
    },
    {
      "epoch": 5.456058846302748,
      "grad_norm": 453.2159423828125,
      "learning_rate": 5.048823504108057e-06,
      "loss": 1.2394,
      "step": 14093
    },
    {
      "epoch": 5.456445993031359,
      "grad_norm": 36.11166000366211,
      "learning_rate": 5.048393341076269e-06,
      "loss": 2.7908,
      "step": 14094
    },
    {
      "epoch": 5.456833139759969,
      "grad_norm": 49.83100891113281,
      "learning_rate": 5.047963178044479e-06,
      "loss": 1.9884,
      "step": 14095
    },
    {
      "epoch": 5.457220286488579,
      "grad_norm": 99.79975128173828,
      "learning_rate": 5.04753301501269e-06,
      "loss": 1.5362,
      "step": 14096
    },
    {
      "epoch": 5.4576074332171896,
      "grad_norm": 27.414031982421875,
      "learning_rate": 5.0471028519809e-06,
      "loss": 0.3824,
      "step": 14097
    },
    {
      "epoch": 5.457994579945799,
      "grad_norm": 4.26702356338501,
      "learning_rate": 5.046672688949113e-06,
      "loss": 0.2187,
      "step": 14098
    },
    {
      "epoch": 5.45838172667441,
      "grad_norm": 5.139276027679443,
      "learning_rate": 5.046242525917323e-06,
      "loss": 0.1982,
      "step": 14099
    },
    {
      "epoch": 5.45876887340302,
      "grad_norm": 90.45650482177734,
      "learning_rate": 5.045812362885534e-06,
      "loss": 0.6354,
      "step": 14100
    },
    {
      "epoch": 5.45915602013163,
      "grad_norm": 38.000343322753906,
      "learning_rate": 5.045382199853744e-06,
      "loss": 2.7578,
      "step": 14101
    },
    {
      "epoch": 5.45954316686024,
      "grad_norm": 55.92035675048828,
      "learning_rate": 5.044952036821957e-06,
      "loss": 2.7137,
      "step": 14102
    },
    {
      "epoch": 5.45993031358885,
      "grad_norm": 66.00137329101562,
      "learning_rate": 5.044521873790167e-06,
      "loss": 1.6422,
      "step": 14103
    },
    {
      "epoch": 5.4603174603174605,
      "grad_norm": 2.667306900024414,
      "learning_rate": 5.044091710758378e-06,
      "loss": 0.1174,
      "step": 14104
    },
    {
      "epoch": 5.46070460704607,
      "grad_norm": 13.309409141540527,
      "learning_rate": 5.043661547726589e-06,
      "loss": 0.4571,
      "step": 14105
    },
    {
      "epoch": 5.461091753774681,
      "grad_norm": 71.26361846923828,
      "learning_rate": 5.0432313846948e-06,
      "loss": 0.821,
      "step": 14106
    },
    {
      "epoch": 5.461478900503291,
      "grad_norm": 19.070711135864258,
      "learning_rate": 5.04280122166301e-06,
      "loss": 1.0883,
      "step": 14107
    },
    {
      "epoch": 5.461866047231901,
      "grad_norm": 40.186119079589844,
      "learning_rate": 5.042371058631222e-06,
      "loss": 1.0394,
      "step": 14108
    },
    {
      "epoch": 5.462253193960511,
      "grad_norm": 20.450708389282227,
      "learning_rate": 5.041940895599433e-06,
      "loss": 0.9894,
      "step": 14109
    },
    {
      "epoch": 5.462640340689121,
      "grad_norm": 7.294010162353516,
      "learning_rate": 5.041510732567644e-06,
      "loss": 0.3763,
      "step": 14110
    },
    {
      "epoch": 5.463027487417731,
      "grad_norm": 140.78164672851562,
      "learning_rate": 5.041080569535854e-06,
      "loss": 1.5298,
      "step": 14111
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 76.79021453857422,
      "learning_rate": 5.040650406504065e-06,
      "loss": 1.0273,
      "step": 14112
    },
    {
      "epoch": 5.4638017808749515,
      "grad_norm": 57.4622802734375,
      "learning_rate": 5.040220243472277e-06,
      "loss": 1.5359,
      "step": 14113
    },
    {
      "epoch": 5.464188927603562,
      "grad_norm": 15.640249252319336,
      "learning_rate": 5.039790080440488e-06,
      "loss": 0.6266,
      "step": 14114
    },
    {
      "epoch": 5.464576074332172,
      "grad_norm": 61.07109451293945,
      "learning_rate": 5.039359917408698e-06,
      "loss": 2.6194,
      "step": 14115
    },
    {
      "epoch": 5.464963221060782,
      "grad_norm": 44.84636306762695,
      "learning_rate": 5.038929754376909e-06,
      "loss": 1.1408,
      "step": 14116
    },
    {
      "epoch": 5.465350367789393,
      "grad_norm": 4.947056293487549,
      "learning_rate": 5.038499591345121e-06,
      "loss": 0.1658,
      "step": 14117
    },
    {
      "epoch": 5.465737514518002,
      "grad_norm": 6.598569869995117,
      "learning_rate": 5.038069428313332e-06,
      "loss": 0.3876,
      "step": 14118
    },
    {
      "epoch": 5.466124661246613,
      "grad_norm": 52.66877746582031,
      "learning_rate": 5.037639265281542e-06,
      "loss": 0.2581,
      "step": 14119
    },
    {
      "epoch": 5.466511807975222,
      "grad_norm": 5.219354629516602,
      "learning_rate": 5.037209102249753e-06,
      "loss": 0.1942,
      "step": 14120
    },
    {
      "epoch": 5.466898954703833,
      "grad_norm": 67.40164947509766,
      "learning_rate": 5.036778939217964e-06,
      "loss": 0.8707,
      "step": 14121
    },
    {
      "epoch": 5.4672861014324425,
      "grad_norm": 59.6515998840332,
      "learning_rate": 5.036348776186175e-06,
      "loss": 3.3837,
      "step": 14122
    },
    {
      "epoch": 5.467673248161053,
      "grad_norm": 37.03045654296875,
      "learning_rate": 5.035918613154386e-06,
      "loss": 1.1583,
      "step": 14123
    },
    {
      "epoch": 5.4680603948896636,
      "grad_norm": 35.26823806762695,
      "learning_rate": 5.035488450122597e-06,
      "loss": 1.9935,
      "step": 14124
    },
    {
      "epoch": 5.468447541618273,
      "grad_norm": 19.039432525634766,
      "learning_rate": 5.035058287090808e-06,
      "loss": 0.8089,
      "step": 14125
    },
    {
      "epoch": 5.468834688346884,
      "grad_norm": 16.548568725585938,
      "learning_rate": 5.034628124059019e-06,
      "loss": 0.9776,
      "step": 14126
    },
    {
      "epoch": 5.469221835075493,
      "grad_norm": 49.74006271362305,
      "learning_rate": 5.034197961027229e-06,
      "loss": 0.5495,
      "step": 14127
    },
    {
      "epoch": 5.469608981804104,
      "grad_norm": 7.725539684295654,
      "learning_rate": 5.03376779799544e-06,
      "loss": 0.1856,
      "step": 14128
    },
    {
      "epoch": 5.469996128532713,
      "grad_norm": 17.11606788635254,
      "learning_rate": 5.033337634963652e-06,
      "loss": 0.639,
      "step": 14129
    },
    {
      "epoch": 5.470383275261324,
      "grad_norm": 73.1458511352539,
      "learning_rate": 5.032907471931863e-06,
      "loss": 2.4709,
      "step": 14130
    },
    {
      "epoch": 5.4707704219899345,
      "grad_norm": 7.952816009521484,
      "learning_rate": 5.032477308900073e-06,
      "loss": 0.3288,
      "step": 14131
    },
    {
      "epoch": 5.471157568718544,
      "grad_norm": 13.726866722106934,
      "learning_rate": 5.032047145868284e-06,
      "loss": 1.0487,
      "step": 14132
    },
    {
      "epoch": 5.471544715447155,
      "grad_norm": 25.128021240234375,
      "learning_rate": 5.031616982836496e-06,
      "loss": 0.9216,
      "step": 14133
    },
    {
      "epoch": 5.471931862175764,
      "grad_norm": 38.2753791809082,
      "learning_rate": 5.031186819804707e-06,
      "loss": 0.4009,
      "step": 14134
    },
    {
      "epoch": 5.472319008904375,
      "grad_norm": 17.993610382080078,
      "learning_rate": 5.030756656772917e-06,
      "loss": 0.9839,
      "step": 14135
    },
    {
      "epoch": 5.472706155632985,
      "grad_norm": 26.16948127746582,
      "learning_rate": 5.030326493741128e-06,
      "loss": 0.8732,
      "step": 14136
    },
    {
      "epoch": 5.473093302361595,
      "grad_norm": 41.828426361083984,
      "learning_rate": 5.029896330709339e-06,
      "loss": 1.366,
      "step": 14137
    },
    {
      "epoch": 5.473480449090205,
      "grad_norm": 31.30572509765625,
      "learning_rate": 5.029466167677551e-06,
      "loss": 0.4213,
      "step": 14138
    },
    {
      "epoch": 5.473867595818815,
      "grad_norm": 7.95250129699707,
      "learning_rate": 5.029036004645761e-06,
      "loss": 0.424,
      "step": 14139
    },
    {
      "epoch": 5.4742547425474255,
      "grad_norm": 37.04161071777344,
      "learning_rate": 5.028605841613972e-06,
      "loss": 0.6519,
      "step": 14140
    },
    {
      "epoch": 5.474641889276036,
      "grad_norm": 61.47710418701172,
      "learning_rate": 5.028175678582183e-06,
      "loss": 2.3024,
      "step": 14141
    },
    {
      "epoch": 5.475029036004646,
      "grad_norm": 2.102229118347168,
      "learning_rate": 5.027745515550394e-06,
      "loss": 0.0317,
      "step": 14142
    },
    {
      "epoch": 5.475416182733256,
      "grad_norm": 27.047426223754883,
      "learning_rate": 5.027315352518604e-06,
      "loss": 1.6133,
      "step": 14143
    },
    {
      "epoch": 5.475803329461866,
      "grad_norm": 51.1968994140625,
      "learning_rate": 5.026885189486816e-06,
      "loss": 1.5761,
      "step": 14144
    },
    {
      "epoch": 5.476190476190476,
      "grad_norm": 5.695130348205566,
      "learning_rate": 5.026455026455027e-06,
      "loss": 0.3434,
      "step": 14145
    },
    {
      "epoch": 5.476577622919086,
      "grad_norm": 45.68109893798828,
      "learning_rate": 5.026024863423238e-06,
      "loss": 1.2003,
      "step": 14146
    },
    {
      "epoch": 5.476964769647696,
      "grad_norm": 75.0147933959961,
      "learning_rate": 5.025594700391448e-06,
      "loss": 1.0845,
      "step": 14147
    },
    {
      "epoch": 5.477351916376307,
      "grad_norm": 130.84352111816406,
      "learning_rate": 5.0251645373596605e-06,
      "loss": 2.0554,
      "step": 14148
    },
    {
      "epoch": 5.4777390631049165,
      "grad_norm": 118.34125518798828,
      "learning_rate": 5.024734374327871e-06,
      "loss": 2.9851,
      "step": 14149
    },
    {
      "epoch": 5.478126209833527,
      "grad_norm": 20.544811248779297,
      "learning_rate": 5.024304211296082e-06,
      "loss": 1.0317,
      "step": 14150
    },
    {
      "epoch": 5.478513356562137,
      "grad_norm": 25.259449005126953,
      "learning_rate": 5.023874048264292e-06,
      "loss": 1.1796,
      "step": 14151
    },
    {
      "epoch": 5.478900503290747,
      "grad_norm": 53.089439392089844,
      "learning_rate": 5.023443885232504e-06,
      "loss": 0.8768,
      "step": 14152
    },
    {
      "epoch": 5.479287650019358,
      "grad_norm": 28.589218139648438,
      "learning_rate": 5.023013722200715e-06,
      "loss": 0.2368,
      "step": 14153
    },
    {
      "epoch": 5.479674796747967,
      "grad_norm": 37.62370300292969,
      "learning_rate": 5.022583559168926e-06,
      "loss": 1.5363,
      "step": 14154
    },
    {
      "epoch": 5.480061943476578,
      "grad_norm": 5.473175048828125,
      "learning_rate": 5.022153396137136e-06,
      "loss": 0.4425,
      "step": 14155
    },
    {
      "epoch": 5.4804490902051874,
      "grad_norm": 14.105592727661133,
      "learning_rate": 5.021723233105348e-06,
      "loss": 0.508,
      "step": 14156
    },
    {
      "epoch": 5.480836236933798,
      "grad_norm": 64.64740753173828,
      "learning_rate": 5.021293070073558e-06,
      "loss": 1.0249,
      "step": 14157
    },
    {
      "epoch": 5.4812233836624085,
      "grad_norm": 30.152708053588867,
      "learning_rate": 5.020862907041769e-06,
      "loss": 1.8416,
      "step": 14158
    },
    {
      "epoch": 5.481610530391018,
      "grad_norm": 23.02028465270996,
      "learning_rate": 5.02043274400998e-06,
      "loss": 1.3818,
      "step": 14159
    },
    {
      "epoch": 5.481997677119629,
      "grad_norm": 10.107630729675293,
      "learning_rate": 5.020002580978192e-06,
      "loss": 0.3476,
      "step": 14160
    },
    {
      "epoch": 5.482384823848238,
      "grad_norm": 45.155357360839844,
      "learning_rate": 5.019572417946402e-06,
      "loss": 2.2606,
      "step": 14161
    },
    {
      "epoch": 5.482771970576849,
      "grad_norm": 163.75942993164062,
      "learning_rate": 5.019142254914613e-06,
      "loss": 3.1966,
      "step": 14162
    },
    {
      "epoch": 5.483159117305458,
      "grad_norm": 56.6099739074707,
      "learning_rate": 5.018712091882823e-06,
      "loss": 1.6823,
      "step": 14163
    },
    {
      "epoch": 5.483546264034069,
      "grad_norm": 32.849361419677734,
      "learning_rate": 5.0182819288510356e-06,
      "loss": 1.3471,
      "step": 14164
    },
    {
      "epoch": 5.483933410762679,
      "grad_norm": 97.77348327636719,
      "learning_rate": 5.017851765819246e-06,
      "loss": 0.7114,
      "step": 14165
    },
    {
      "epoch": 5.484320557491289,
      "grad_norm": 29.650224685668945,
      "learning_rate": 5.017421602787457e-06,
      "loss": 2.6621,
      "step": 14166
    },
    {
      "epoch": 5.4847077042198995,
      "grad_norm": 48.38605499267578,
      "learning_rate": 5.016991439755667e-06,
      "loss": 1.8282,
      "step": 14167
    },
    {
      "epoch": 5.485094850948509,
      "grad_norm": 37.924583435058594,
      "learning_rate": 5.0165612767238795e-06,
      "loss": 1.8092,
      "step": 14168
    },
    {
      "epoch": 5.48548199767712,
      "grad_norm": 48.2711296081543,
      "learning_rate": 5.01613111369209e-06,
      "loss": 0.948,
      "step": 14169
    },
    {
      "epoch": 5.48586914440573,
      "grad_norm": 47.581546783447266,
      "learning_rate": 5.015700950660301e-06,
      "loss": 4.2793,
      "step": 14170
    },
    {
      "epoch": 5.48625629113434,
      "grad_norm": 5.626546859741211,
      "learning_rate": 5.015270787628511e-06,
      "loss": 0.2716,
      "step": 14171
    },
    {
      "epoch": 5.48664343786295,
      "grad_norm": 8.779845237731934,
      "learning_rate": 5.014840624596723e-06,
      "loss": 0.5084,
      "step": 14172
    },
    {
      "epoch": 5.48703058459156,
      "grad_norm": 36.67384338378906,
      "learning_rate": 5.014410461564933e-06,
      "loss": 1.7653,
      "step": 14173
    },
    {
      "epoch": 5.48741773132017,
      "grad_norm": 81.71282196044922,
      "learning_rate": 5.013980298533145e-06,
      "loss": 0.7806,
      "step": 14174
    },
    {
      "epoch": 5.487804878048781,
      "grad_norm": 69.45154571533203,
      "learning_rate": 5.013550135501355e-06,
      "loss": 0.4819,
      "step": 14175
    },
    {
      "epoch": 5.4881920247773905,
      "grad_norm": 12.141812324523926,
      "learning_rate": 5.013119972469567e-06,
      "loss": 0.4052,
      "step": 14176
    },
    {
      "epoch": 5.488579171506001,
      "grad_norm": 45.210304260253906,
      "learning_rate": 5.012689809437777e-06,
      "loss": 0.4511,
      "step": 14177
    },
    {
      "epoch": 5.488966318234611,
      "grad_norm": 28.600257873535156,
      "learning_rate": 5.012259646405988e-06,
      "loss": 0.9659,
      "step": 14178
    },
    {
      "epoch": 5.489353464963221,
      "grad_norm": 9.002309799194336,
      "learning_rate": 5.011829483374198e-06,
      "loss": 0.4139,
      "step": 14179
    },
    {
      "epoch": 5.489740611691831,
      "grad_norm": 103.98686218261719,
      "learning_rate": 5.011399320342411e-06,
      "loss": 3.3693,
      "step": 14180
    },
    {
      "epoch": 5.490127758420441,
      "grad_norm": 33.146568298339844,
      "learning_rate": 5.010969157310621e-06,
      "loss": 1.3992,
      "step": 14181
    },
    {
      "epoch": 5.490514905149052,
      "grad_norm": 35.76567459106445,
      "learning_rate": 5.010538994278832e-06,
      "loss": 1.0992,
      "step": 14182
    },
    {
      "epoch": 5.4909020518776614,
      "grad_norm": 42.23011016845703,
      "learning_rate": 5.010108831247042e-06,
      "loss": 1.5388,
      "step": 14183
    },
    {
      "epoch": 5.491289198606272,
      "grad_norm": 8.927207946777344,
      "learning_rate": 5.0096786682152546e-06,
      "loss": 0.0965,
      "step": 14184
    },
    {
      "epoch": 5.491676345334882,
      "grad_norm": 17.193944931030273,
      "learning_rate": 5.009248505183465e-06,
      "loss": 1.1512,
      "step": 14185
    },
    {
      "epoch": 5.492063492063492,
      "grad_norm": 79.99280548095703,
      "learning_rate": 5.008818342151676e-06,
      "loss": 1.7393,
      "step": 14186
    },
    {
      "epoch": 5.492450638792103,
      "grad_norm": 40.77299499511719,
      "learning_rate": 5.008388179119887e-06,
      "loss": 1.2131,
      "step": 14187
    },
    {
      "epoch": 5.492837785520712,
      "grad_norm": 24.481426239013672,
      "learning_rate": 5.007958016088098e-06,
      "loss": 0.8671,
      "step": 14188
    },
    {
      "epoch": 5.493224932249323,
      "grad_norm": 31.052217483520508,
      "learning_rate": 5.007527853056309e-06,
      "loss": 0.1515,
      "step": 14189
    },
    {
      "epoch": 5.493612078977932,
      "grad_norm": 32.381919860839844,
      "learning_rate": 5.00709769002452e-06,
      "loss": 2.5874,
      "step": 14190
    },
    {
      "epoch": 5.493999225706543,
      "grad_norm": 59.90074920654297,
      "learning_rate": 5.006667526992731e-06,
      "loss": 1.2542,
      "step": 14191
    },
    {
      "epoch": 5.494386372435153,
      "grad_norm": 89.44498443603516,
      "learning_rate": 5.006237363960942e-06,
      "loss": 1.5173,
      "step": 14192
    },
    {
      "epoch": 5.494773519163763,
      "grad_norm": 59.927730560302734,
      "learning_rate": 5.005807200929152e-06,
      "loss": 0.6623,
      "step": 14193
    },
    {
      "epoch": 5.4951606658923735,
      "grad_norm": 17.394775390625,
      "learning_rate": 5.005377037897363e-06,
      "loss": 0.5624,
      "step": 14194
    },
    {
      "epoch": 5.495547812620983,
      "grad_norm": 37.27558517456055,
      "learning_rate": 5.004946874865575e-06,
      "loss": 1.588,
      "step": 14195
    },
    {
      "epoch": 5.495934959349594,
      "grad_norm": 91.32366180419922,
      "learning_rate": 5.004516711833786e-06,
      "loss": 0.6256,
      "step": 14196
    },
    {
      "epoch": 5.496322106078203,
      "grad_norm": 22.214025497436523,
      "learning_rate": 5.004086548801996e-06,
      "loss": 1.5969,
      "step": 14197
    },
    {
      "epoch": 5.496709252806814,
      "grad_norm": 31.878589630126953,
      "learning_rate": 5.003656385770207e-06,
      "loss": 0.1943,
      "step": 14198
    },
    {
      "epoch": 5.497096399535424,
      "grad_norm": 68.02984619140625,
      "learning_rate": 5.003226222738419e-06,
      "loss": 1.1869,
      "step": 14199
    },
    {
      "epoch": 5.497483546264034,
      "grad_norm": 61.90092468261719,
      "learning_rate": 5.00279605970663e-06,
      "loss": 1.0547,
      "step": 14200
    },
    {
      "epoch": 5.497870692992644,
      "grad_norm": 3.1467041969299316,
      "learning_rate": 5.00236589667484e-06,
      "loss": 0.1532,
      "step": 14201
    },
    {
      "epoch": 5.498257839721254,
      "grad_norm": 43.065086364746094,
      "learning_rate": 5.001935733643051e-06,
      "loss": 0.8184,
      "step": 14202
    },
    {
      "epoch": 5.4986449864498645,
      "grad_norm": 33.33795928955078,
      "learning_rate": 5.001505570611262e-06,
      "loss": 1.6483,
      "step": 14203
    },
    {
      "epoch": 5.499032133178475,
      "grad_norm": 33.74448776245117,
      "learning_rate": 5.0010754075794736e-06,
      "loss": 2.4487,
      "step": 14204
    },
    {
      "epoch": 5.499419279907085,
      "grad_norm": 39.21221160888672,
      "learning_rate": 5.000645244547684e-06,
      "loss": 0.8407,
      "step": 14205
    },
    {
      "epoch": 5.499806426635695,
      "grad_norm": 69.09346008300781,
      "learning_rate": 5.000215081515895e-06,
      "loss": 2.2519,
      "step": 14206
    },
    {
      "epoch": 5.500193573364305,
      "grad_norm": 19.60410499572754,
      "learning_rate": 4.999784918484106e-06,
      "loss": 1.5944,
      "step": 14207
    },
    {
      "epoch": 5.500580720092915,
      "grad_norm": 25.412519454956055,
      "learning_rate": 4.999354755452317e-06,
      "loss": 1.4815,
      "step": 14208
    },
    {
      "epoch": 5.500967866821526,
      "grad_norm": 135.52157592773438,
      "learning_rate": 4.998924592420527e-06,
      "loss": 1.6677,
      "step": 14209
    },
    {
      "epoch": 5.5013550135501355,
      "grad_norm": 66.6864013671875,
      "learning_rate": 4.998494429388739e-06,
      "loss": 0.6506,
      "step": 14210
    },
    {
      "epoch": 5.501742160278746,
      "grad_norm": 10.678315162658691,
      "learning_rate": 4.998064266356949e-06,
      "loss": 0.3351,
      "step": 14211
    },
    {
      "epoch": 5.502129307007356,
      "grad_norm": 43.037071228027344,
      "learning_rate": 4.997634103325161e-06,
      "loss": 0.7397,
      "step": 14212
    },
    {
      "epoch": 5.502516453735966,
      "grad_norm": 58.56725311279297,
      "learning_rate": 4.997203940293371e-06,
      "loss": 1.9834,
      "step": 14213
    },
    {
      "epoch": 5.502903600464576,
      "grad_norm": 47.59722137451172,
      "learning_rate": 4.996773777261583e-06,
      "loss": 1.5791,
      "step": 14214
    },
    {
      "epoch": 5.503290747193186,
      "grad_norm": 27.324634552001953,
      "learning_rate": 4.996343614229793e-06,
      "loss": 1.0949,
      "step": 14215
    },
    {
      "epoch": 5.503677893921797,
      "grad_norm": 79.96630096435547,
      "learning_rate": 4.995913451198005e-06,
      "loss": 0.9225,
      "step": 14216
    },
    {
      "epoch": 5.504065040650406,
      "grad_norm": 30.61745262145996,
      "learning_rate": 4.995483288166215e-06,
      "loss": 0.9824,
      "step": 14217
    },
    {
      "epoch": 5.504452187379017,
      "grad_norm": 65.8216781616211,
      "learning_rate": 4.995053125134427e-06,
      "loss": 1.7215,
      "step": 14218
    },
    {
      "epoch": 5.5048393341076265,
      "grad_norm": 65.57852935791016,
      "learning_rate": 4.994622962102637e-06,
      "loss": 0.6363,
      "step": 14219
    },
    {
      "epoch": 5.505226480836237,
      "grad_norm": 21.236560821533203,
      "learning_rate": 4.994192799070849e-06,
      "loss": 0.533,
      "step": 14220
    },
    {
      "epoch": 5.505613627564847,
      "grad_norm": 9.142675399780273,
      "learning_rate": 4.993762636039059e-06,
      "loss": 0.2048,
      "step": 14221
    },
    {
      "epoch": 5.506000774293457,
      "grad_norm": 42.825531005859375,
      "learning_rate": 4.993332473007271e-06,
      "loss": 1.0233,
      "step": 14222
    },
    {
      "epoch": 5.506387921022068,
      "grad_norm": 16.65580177307129,
      "learning_rate": 4.992902309975481e-06,
      "loss": 1.5039,
      "step": 14223
    },
    {
      "epoch": 5.506775067750677,
      "grad_norm": 9.394936561584473,
      "learning_rate": 4.992472146943692e-06,
      "loss": 0.515,
      "step": 14224
    },
    {
      "epoch": 5.507162214479288,
      "grad_norm": 6.004218101501465,
      "learning_rate": 4.992041983911903e-06,
      "loss": 0.3671,
      "step": 14225
    },
    {
      "epoch": 5.507549361207898,
      "grad_norm": 21.127666473388672,
      "learning_rate": 4.991611820880114e-06,
      "loss": 2.2747,
      "step": 14226
    },
    {
      "epoch": 5.507936507936508,
      "grad_norm": 39.5604133605957,
      "learning_rate": 4.991181657848324e-06,
      "loss": 2.0647,
      "step": 14227
    },
    {
      "epoch": 5.508323654665118,
      "grad_norm": 55.360557556152344,
      "learning_rate": 4.990751494816536e-06,
      "loss": 2.0242,
      "step": 14228
    },
    {
      "epoch": 5.508710801393728,
      "grad_norm": 57.388946533203125,
      "learning_rate": 4.990321331784747e-06,
      "loss": 3.1978,
      "step": 14229
    },
    {
      "epoch": 5.5090979481223386,
      "grad_norm": 128.64817810058594,
      "learning_rate": 4.989891168752958e-06,
      "loss": 1.5808,
      "step": 14230
    },
    {
      "epoch": 5.509485094850948,
      "grad_norm": 34.60658264160156,
      "learning_rate": 4.989461005721169e-06,
      "loss": 2.9187,
      "step": 14231
    },
    {
      "epoch": 5.509872241579559,
      "grad_norm": 34.39393615722656,
      "learning_rate": 4.98903084268938e-06,
      "loss": 0.4231,
      "step": 14232
    },
    {
      "epoch": 5.510259388308169,
      "grad_norm": 15.898479461669922,
      "learning_rate": 4.988600679657591e-06,
      "loss": 1.2222,
      "step": 14233
    },
    {
      "epoch": 5.510646535036779,
      "grad_norm": 45.98685836791992,
      "learning_rate": 4.988170516625802e-06,
      "loss": 2.1363,
      "step": 14234
    },
    {
      "epoch": 5.511033681765389,
      "grad_norm": 52.94501495361328,
      "learning_rate": 4.987740353594013e-06,
      "loss": 2.8193,
      "step": 14235
    },
    {
      "epoch": 5.511420828493999,
      "grad_norm": 4.972152233123779,
      "learning_rate": 4.987310190562224e-06,
      "loss": 0.4287,
      "step": 14236
    },
    {
      "epoch": 5.5118079752226095,
      "grad_norm": 34.75483703613281,
      "learning_rate": 4.986880027530435e-06,
      "loss": 2.1974,
      "step": 14237
    },
    {
      "epoch": 5.512195121951219,
      "grad_norm": 13.789016723632812,
      "learning_rate": 4.986449864498646e-06,
      "loss": 0.9296,
      "step": 14238
    },
    {
      "epoch": 5.51258226867983,
      "grad_norm": 6.663513660430908,
      "learning_rate": 4.986019701466856e-06,
      "loss": 0.1857,
      "step": 14239
    },
    {
      "epoch": 5.51296941540844,
      "grad_norm": 74.8680191040039,
      "learning_rate": 4.985589538435068e-06,
      "loss": 1.8248,
      "step": 14240
    },
    {
      "epoch": 5.51335656213705,
      "grad_norm": 45.975799560546875,
      "learning_rate": 4.985159375403278e-06,
      "loss": 1.6447,
      "step": 14241
    },
    {
      "epoch": 5.51374370886566,
      "grad_norm": 27.077672958374023,
      "learning_rate": 4.984729212371489e-06,
      "loss": 1.8644,
      "step": 14242
    },
    {
      "epoch": 5.51413085559427,
      "grad_norm": 16.133670806884766,
      "learning_rate": 4.9842990493397e-06,
      "loss": 0.5541,
      "step": 14243
    },
    {
      "epoch": 5.51451800232288,
      "grad_norm": 37.8934211730957,
      "learning_rate": 4.983868886307911e-06,
      "loss": 1.9154,
      "step": 14244
    },
    {
      "epoch": 5.514905149051491,
      "grad_norm": 17.6431941986084,
      "learning_rate": 4.983438723276122e-06,
      "loss": 1.4236,
      "step": 14245
    },
    {
      "epoch": 5.5152922957801005,
      "grad_norm": 29.10434913635254,
      "learning_rate": 4.983008560244333e-06,
      "loss": 1.4532,
      "step": 14246
    },
    {
      "epoch": 5.515679442508711,
      "grad_norm": 55.88264465332031,
      "learning_rate": 4.982578397212544e-06,
      "loss": 1.2009,
      "step": 14247
    },
    {
      "epoch": 5.516066589237321,
      "grad_norm": 5.7341437339782715,
      "learning_rate": 4.982148234180755e-06,
      "loss": 0.3652,
      "step": 14248
    },
    {
      "epoch": 5.516453735965931,
      "grad_norm": 34.96524429321289,
      "learning_rate": 4.981718071148966e-06,
      "loss": 1.7235,
      "step": 14249
    },
    {
      "epoch": 5.516840882694542,
      "grad_norm": 44.36201095581055,
      "learning_rate": 4.981287908117177e-06,
      "loss": 1.2646,
      "step": 14250
    },
    {
      "epoch": 5.517228029423151,
      "grad_norm": 26.369529724121094,
      "learning_rate": 4.980857745085388e-06,
      "loss": 0.9784,
      "step": 14251
    },
    {
      "epoch": 5.517615176151762,
      "grad_norm": 17.65070152282715,
      "learning_rate": 4.980427582053599e-06,
      "loss": 0.482,
      "step": 14252
    },
    {
      "epoch": 5.518002322880371,
      "grad_norm": 46.79290771484375,
      "learning_rate": 4.97999741902181e-06,
      "loss": 0.7042,
      "step": 14253
    },
    {
      "epoch": 5.518389469608982,
      "grad_norm": 29.404285430908203,
      "learning_rate": 4.979567255990021e-06,
      "loss": 1.117,
      "step": 14254
    },
    {
      "epoch": 5.5187766163375915,
      "grad_norm": 44.415557861328125,
      "learning_rate": 4.979137092958232e-06,
      "loss": 0.4031,
      "step": 14255
    },
    {
      "epoch": 5.519163763066202,
      "grad_norm": 44.820804595947266,
      "learning_rate": 4.978706929926443e-06,
      "loss": 0.9702,
      "step": 14256
    },
    {
      "epoch": 5.5195509097948126,
      "grad_norm": 48.7369499206543,
      "learning_rate": 4.978276766894653e-06,
      "loss": 0.4127,
      "step": 14257
    },
    {
      "epoch": 5.519938056523422,
      "grad_norm": 21.139488220214844,
      "learning_rate": 4.977846603862865e-06,
      "loss": 0.4384,
      "step": 14258
    },
    {
      "epoch": 5.520325203252033,
      "grad_norm": 21.225889205932617,
      "learning_rate": 4.977416440831075e-06,
      "loss": 1.2453,
      "step": 14259
    },
    {
      "epoch": 5.520712349980642,
      "grad_norm": 82.07032775878906,
      "learning_rate": 4.976986277799286e-06,
      "loss": 0.7635,
      "step": 14260
    },
    {
      "epoch": 5.521099496709253,
      "grad_norm": 22.979597091674805,
      "learning_rate": 4.976556114767497e-06,
      "loss": 1.7684,
      "step": 14261
    },
    {
      "epoch": 5.521486643437863,
      "grad_norm": 5.2325758934021,
      "learning_rate": 4.976125951735708e-06,
      "loss": 0.2464,
      "step": 14262
    },
    {
      "epoch": 5.521873790166473,
      "grad_norm": 22.85163688659668,
      "learning_rate": 4.975695788703919e-06,
      "loss": 0.3477,
      "step": 14263
    },
    {
      "epoch": 5.5222609368950835,
      "grad_norm": 38.14425277709961,
      "learning_rate": 4.97526562567213e-06,
      "loss": 0.845,
      "step": 14264
    },
    {
      "epoch": 5.522648083623693,
      "grad_norm": 43.40879821777344,
      "learning_rate": 4.974835462640341e-06,
      "loss": 2.1142,
      "step": 14265
    },
    {
      "epoch": 5.523035230352304,
      "grad_norm": 29.88835906982422,
      "learning_rate": 4.974405299608552e-06,
      "loss": 0.5266,
      "step": 14266
    },
    {
      "epoch": 5.523422377080914,
      "grad_norm": 1.1074626445770264,
      "learning_rate": 4.973975136576763e-06,
      "loss": 0.0298,
      "step": 14267
    },
    {
      "epoch": 5.523809523809524,
      "grad_norm": 59.427650451660156,
      "learning_rate": 4.973544973544974e-06,
      "loss": 1.4328,
      "step": 14268
    },
    {
      "epoch": 5.524196670538134,
      "grad_norm": 34.779136657714844,
      "learning_rate": 4.973114810513185e-06,
      "loss": 0.8873,
      "step": 14269
    },
    {
      "epoch": 5.524583817266744,
      "grad_norm": 45.52431106567383,
      "learning_rate": 4.9726846474813965e-06,
      "loss": 0.9929,
      "step": 14270
    },
    {
      "epoch": 5.524970963995354,
      "grad_norm": 67.21342468261719,
      "learning_rate": 4.972254484449607e-06,
      "loss": 0.8988,
      "step": 14271
    },
    {
      "epoch": 5.525358110723964,
      "grad_norm": 117.50064849853516,
      "learning_rate": 4.971824321417818e-06,
      "loss": 0.5188,
      "step": 14272
    },
    {
      "epoch": 5.5257452574525745,
      "grad_norm": 66.65425872802734,
      "learning_rate": 4.971394158386029e-06,
      "loss": 2.5163,
      "step": 14273
    },
    {
      "epoch": 5.526132404181185,
      "grad_norm": 20.93601417541504,
      "learning_rate": 4.97096399535424e-06,
      "loss": 0.6993,
      "step": 14274
    },
    {
      "epoch": 5.526519550909795,
      "grad_norm": 39.705387115478516,
      "learning_rate": 4.97053383232245e-06,
      "loss": 1.0657,
      "step": 14275
    },
    {
      "epoch": 5.526906697638405,
      "grad_norm": 43.79487609863281,
      "learning_rate": 4.970103669290662e-06,
      "loss": 0.5246,
      "step": 14276
    },
    {
      "epoch": 5.527293844367015,
      "grad_norm": 19.945159912109375,
      "learning_rate": 4.969673506258872e-06,
      "loss": 0.8524,
      "step": 14277
    },
    {
      "epoch": 5.527680991095625,
      "grad_norm": 131.66624450683594,
      "learning_rate": 4.969243343227084e-06,
      "loss": 2.5806,
      "step": 14278
    },
    {
      "epoch": 5.528068137824235,
      "grad_norm": 7.0926008224487305,
      "learning_rate": 4.968813180195294e-06,
      "loss": 0.324,
      "step": 14279
    },
    {
      "epoch": 5.528455284552845,
      "grad_norm": 10.753829956054688,
      "learning_rate": 4.968383017163506e-06,
      "loss": 0.3064,
      "step": 14280
    },
    {
      "epoch": 5.528842431281456,
      "grad_norm": 97.66060638427734,
      "learning_rate": 4.967952854131716e-06,
      "loss": 1.951,
      "step": 14281
    },
    {
      "epoch": 5.5292295780100655,
      "grad_norm": 18.21621322631836,
      "learning_rate": 4.9675226910999276e-06,
      "loss": 0.2959,
      "step": 14282
    },
    {
      "epoch": 5.529616724738676,
      "grad_norm": 5.490966796875,
      "learning_rate": 4.967092528068138e-06,
      "loss": 0.2411,
      "step": 14283
    },
    {
      "epoch": 5.530003871467287,
      "grad_norm": 72.93955993652344,
      "learning_rate": 4.9666623650363496e-06,
      "loss": 1.7419,
      "step": 14284
    },
    {
      "epoch": 5.530391018195896,
      "grad_norm": 47.53453826904297,
      "learning_rate": 4.96623220200456e-06,
      "loss": 1.3912,
      "step": 14285
    },
    {
      "epoch": 5.530778164924507,
      "grad_norm": 4.8451056480407715,
      "learning_rate": 4.9658020389727715e-06,
      "loss": 0.2896,
      "step": 14286
    },
    {
      "epoch": 5.531165311653116,
      "grad_norm": 3.842874765396118,
      "learning_rate": 4.965371875940982e-06,
      "loss": 0.18,
      "step": 14287
    },
    {
      "epoch": 5.531552458381727,
      "grad_norm": 57.00285339355469,
      "learning_rate": 4.9649417129091935e-06,
      "loss": 1.562,
      "step": 14288
    },
    {
      "epoch": 5.5319396051103364,
      "grad_norm": 36.286476135253906,
      "learning_rate": 4.964511549877404e-06,
      "loss": 0.5529,
      "step": 14289
    },
    {
      "epoch": 5.532326751838947,
      "grad_norm": 30.669710159301758,
      "learning_rate": 4.964081386845615e-06,
      "loss": 1.4455,
      "step": 14290
    },
    {
      "epoch": 5.5327138985675575,
      "grad_norm": 27.23790168762207,
      "learning_rate": 4.963651223813826e-06,
      "loss": 1.152,
      "step": 14291
    },
    {
      "epoch": 5.533101045296167,
      "grad_norm": 58.9804801940918,
      "learning_rate": 4.963221060782037e-06,
      "loss": 0.6272,
      "step": 14292
    },
    {
      "epoch": 5.533488192024778,
      "grad_norm": 6.293341636657715,
      "learning_rate": 4.962790897750247e-06,
      "loss": 0.3334,
      "step": 14293
    },
    {
      "epoch": 5.533875338753387,
      "grad_norm": 12.141195297241211,
      "learning_rate": 4.962360734718459e-06,
      "loss": 0.3413,
      "step": 14294
    },
    {
      "epoch": 5.534262485481998,
      "grad_norm": 4.303830146789551,
      "learning_rate": 4.961930571686669e-06,
      "loss": 0.1491,
      "step": 14295
    },
    {
      "epoch": 5.534649632210607,
      "grad_norm": 29.12111473083496,
      "learning_rate": 4.961500408654881e-06,
      "loss": 2.2891,
      "step": 14296
    },
    {
      "epoch": 5.535036778939218,
      "grad_norm": 44.53888702392578,
      "learning_rate": 4.961070245623091e-06,
      "loss": 2.2574,
      "step": 14297
    },
    {
      "epoch": 5.535423925667828,
      "grad_norm": 18.288436889648438,
      "learning_rate": 4.960640082591303e-06,
      "loss": 0.7323,
      "step": 14298
    },
    {
      "epoch": 5.535811072396438,
      "grad_norm": 37.98400115966797,
      "learning_rate": 4.960209919559513e-06,
      "loss": 2.1243,
      "step": 14299
    },
    {
      "epoch": 5.5361982191250485,
      "grad_norm": 97.8096923828125,
      "learning_rate": 4.959779756527725e-06,
      "loss": 1.3588,
      "step": 14300
    },
    {
      "epoch": 5.536585365853659,
      "grad_norm": 15.38771915435791,
      "learning_rate": 4.959349593495935e-06,
      "loss": 0.323,
      "step": 14301
    },
    {
      "epoch": 5.536972512582269,
      "grad_norm": 19.790340423583984,
      "learning_rate": 4.9589194304641466e-06,
      "loss": 1.171,
      "step": 14302
    },
    {
      "epoch": 5.537359659310879,
      "grad_norm": 95.64714813232422,
      "learning_rate": 4.958489267432357e-06,
      "loss": 1.7202,
      "step": 14303
    },
    {
      "epoch": 5.537746806039489,
      "grad_norm": 47.0438117980957,
      "learning_rate": 4.9580591044005686e-06,
      "loss": 1.1915,
      "step": 14304
    },
    {
      "epoch": 5.538133952768099,
      "grad_norm": 4.59516716003418,
      "learning_rate": 4.957628941368779e-06,
      "loss": 0.2362,
      "step": 14305
    },
    {
      "epoch": 5.538521099496709,
      "grad_norm": 42.93825149536133,
      "learning_rate": 4.9571987783369905e-06,
      "loss": 1.815,
      "step": 14306
    },
    {
      "epoch": 5.538908246225319,
      "grad_norm": 37.73653793334961,
      "learning_rate": 4.956768615305201e-06,
      "loss": 0.9121,
      "step": 14307
    },
    {
      "epoch": 5.53929539295393,
      "grad_norm": 37.939945220947266,
      "learning_rate": 4.956338452273412e-06,
      "loss": 0.7028,
      "step": 14308
    },
    {
      "epoch": 5.5396825396825395,
      "grad_norm": 20.445005416870117,
      "learning_rate": 4.955908289241623e-06,
      "loss": 0.3377,
      "step": 14309
    },
    {
      "epoch": 5.54006968641115,
      "grad_norm": 2.5740082263946533,
      "learning_rate": 4.955478126209834e-06,
      "loss": 0.052,
      "step": 14310
    },
    {
      "epoch": 5.54045683313976,
      "grad_norm": 52.707698822021484,
      "learning_rate": 4.955047963178045e-06,
      "loss": 1.5118,
      "step": 14311
    },
    {
      "epoch": 5.54084397986837,
      "grad_norm": 37.236061096191406,
      "learning_rate": 4.954617800146256e-06,
      "loss": 1.7297,
      "step": 14312
    },
    {
      "epoch": 5.54123112659698,
      "grad_norm": 12.364391326904297,
      "learning_rate": 4.954187637114467e-06,
      "loss": 0.4715,
      "step": 14313
    },
    {
      "epoch": 5.54161827332559,
      "grad_norm": 36.156150817871094,
      "learning_rate": 4.953757474082678e-06,
      "loss": 0.8831,
      "step": 14314
    },
    {
      "epoch": 5.542005420054201,
      "grad_norm": 95.31122589111328,
      "learning_rate": 4.953327311050889e-06,
      "loss": 1.7737,
      "step": 14315
    },
    {
      "epoch": 5.5423925667828104,
      "grad_norm": 56.19541931152344,
      "learning_rate": 4.9528971480191e-06,
      "loss": 3.2263,
      "step": 14316
    },
    {
      "epoch": 5.542779713511421,
      "grad_norm": 65.40404510498047,
      "learning_rate": 4.952466984987311e-06,
      "loss": 0.4185,
      "step": 14317
    },
    {
      "epoch": 5.5431668602400315,
      "grad_norm": 16.1375675201416,
      "learning_rate": 4.952036821955522e-06,
      "loss": 1.4295,
      "step": 14318
    },
    {
      "epoch": 5.543554006968641,
      "grad_norm": 4.428734302520752,
      "learning_rate": 4.951606658923733e-06,
      "loss": 0.2263,
      "step": 14319
    },
    {
      "epoch": 5.543941153697252,
      "grad_norm": 55.44110107421875,
      "learning_rate": 4.951176495891944e-06,
      "loss": 1.4534,
      "step": 14320
    },
    {
      "epoch": 5.544328300425861,
      "grad_norm": 20.27048683166504,
      "learning_rate": 4.950746332860155e-06,
      "loss": 1.5659,
      "step": 14321
    },
    {
      "epoch": 5.544715447154472,
      "grad_norm": 17.387863159179688,
      "learning_rate": 4.9503161698283656e-06,
      "loss": 0.5016,
      "step": 14322
    },
    {
      "epoch": 5.545102593883081,
      "grad_norm": 54.48316955566406,
      "learning_rate": 4.949886006796576e-06,
      "loss": 1.7794,
      "step": 14323
    },
    {
      "epoch": 5.545489740611692,
      "grad_norm": 8.833229064941406,
      "learning_rate": 4.9494558437647875e-06,
      "loss": 0.2181,
      "step": 14324
    },
    {
      "epoch": 5.545876887340302,
      "grad_norm": 34.785301208496094,
      "learning_rate": 4.949025680732998e-06,
      "loss": 1.2759,
      "step": 14325
    },
    {
      "epoch": 5.546264034068912,
      "grad_norm": 21.517709732055664,
      "learning_rate": 4.948595517701209e-06,
      "loss": 1.1332,
      "step": 14326
    },
    {
      "epoch": 5.5466511807975225,
      "grad_norm": 53.17839813232422,
      "learning_rate": 4.94816535466942e-06,
      "loss": 1.3725,
      "step": 14327
    },
    {
      "epoch": 5.547038327526132,
      "grad_norm": 135.59213256835938,
      "learning_rate": 4.947735191637631e-06,
      "loss": 2.0916,
      "step": 14328
    },
    {
      "epoch": 5.547425474254743,
      "grad_norm": 46.96034240722656,
      "learning_rate": 4.947305028605842e-06,
      "loss": 1.0324,
      "step": 14329
    },
    {
      "epoch": 5.547812620983352,
      "grad_norm": 26.697507858276367,
      "learning_rate": 4.946874865574053e-06,
      "loss": 1.3561,
      "step": 14330
    },
    {
      "epoch": 5.548199767711963,
      "grad_norm": 4.71992826461792,
      "learning_rate": 4.946444702542264e-06,
      "loss": 0.1586,
      "step": 14331
    },
    {
      "epoch": 5.548586914440573,
      "grad_norm": 28.977205276489258,
      "learning_rate": 4.946014539510475e-06,
      "loss": 1.8225,
      "step": 14332
    },
    {
      "epoch": 5.548974061169183,
      "grad_norm": 37.56584548950195,
      "learning_rate": 4.945584376478686e-06,
      "loss": 0.7631,
      "step": 14333
    },
    {
      "epoch": 5.549361207897793,
      "grad_norm": 63.75016403198242,
      "learning_rate": 4.945154213446897e-06,
      "loss": 1.1607,
      "step": 14334
    },
    {
      "epoch": 5.549748354626403,
      "grad_norm": 95.59812927246094,
      "learning_rate": 4.944724050415108e-06,
      "loss": 2.0463,
      "step": 14335
    },
    {
      "epoch": 5.5501355013550135,
      "grad_norm": 46.27181625366211,
      "learning_rate": 4.944293887383319e-06,
      "loss": 1.2453,
      "step": 14336
    },
    {
      "epoch": 5.550522648083624,
      "grad_norm": 33.07368469238281,
      "learning_rate": 4.94386372435153e-06,
      "loss": 1.6047,
      "step": 14337
    },
    {
      "epoch": 5.550909794812234,
      "grad_norm": 37.81141662597656,
      "learning_rate": 4.943433561319741e-06,
      "loss": 2.7369,
      "step": 14338
    },
    {
      "epoch": 5.551296941540844,
      "grad_norm": 63.83815383911133,
      "learning_rate": 4.943003398287952e-06,
      "loss": 1.209,
      "step": 14339
    },
    {
      "epoch": 5.551684088269454,
      "grad_norm": 17.562326431274414,
      "learning_rate": 4.942573235256163e-06,
      "loss": 1.7731,
      "step": 14340
    },
    {
      "epoch": 5.552071234998064,
      "grad_norm": 11.645896911621094,
      "learning_rate": 4.942143072224373e-06,
      "loss": 0.3159,
      "step": 14341
    },
    {
      "epoch": 5.552458381726675,
      "grad_norm": 102.93888854980469,
      "learning_rate": 4.9417129091925846e-06,
      "loss": 1.4074,
      "step": 14342
    },
    {
      "epoch": 5.5528455284552845,
      "grad_norm": 40.75463104248047,
      "learning_rate": 4.941282746160795e-06,
      "loss": 1.0971,
      "step": 14343
    },
    {
      "epoch": 5.553232675183895,
      "grad_norm": 68.21315002441406,
      "learning_rate": 4.940852583129006e-06,
      "loss": 1.3297,
      "step": 14344
    },
    {
      "epoch": 5.553619821912505,
      "grad_norm": 27.765140533447266,
      "learning_rate": 4.940422420097217e-06,
      "loss": 1.5755,
      "step": 14345
    },
    {
      "epoch": 5.554006968641115,
      "grad_norm": 39.47697830200195,
      "learning_rate": 4.939992257065428e-06,
      "loss": 1.005,
      "step": 14346
    },
    {
      "epoch": 5.554394115369725,
      "grad_norm": 65.12487030029297,
      "learning_rate": 4.939562094033639e-06,
      "loss": 0.8071,
      "step": 14347
    },
    {
      "epoch": 5.554781262098335,
      "grad_norm": 181.2556610107422,
      "learning_rate": 4.93913193100185e-06,
      "loss": 1.3612,
      "step": 14348
    },
    {
      "epoch": 5.555168408826946,
      "grad_norm": 26.251737594604492,
      "learning_rate": 4.938701767970061e-06,
      "loss": 1.2218,
      "step": 14349
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 10.012340545654297,
      "learning_rate": 4.938271604938272e-06,
      "loss": 0.3639,
      "step": 14350
    },
    {
      "epoch": 5.555942702284166,
      "grad_norm": 46.43231964111328,
      "learning_rate": 4.937841441906483e-06,
      "loss": 0.9442,
      "step": 14351
    },
    {
      "epoch": 5.5563298490127755,
      "grad_norm": 1.216435194015503,
      "learning_rate": 4.9374112788746945e-06,
      "loss": 0.033,
      "step": 14352
    },
    {
      "epoch": 5.556716995741386,
      "grad_norm": 75.00566101074219,
      "learning_rate": 4.936981115842905e-06,
      "loss": 1.2623,
      "step": 14353
    },
    {
      "epoch": 5.5571041424699965,
      "grad_norm": 237.38381958007812,
      "learning_rate": 4.9365509528111165e-06,
      "loss": 1.6862,
      "step": 14354
    },
    {
      "epoch": 5.557491289198606,
      "grad_norm": 17.423660278320312,
      "learning_rate": 4.936120789779327e-06,
      "loss": 0.8124,
      "step": 14355
    },
    {
      "epoch": 5.557878435927217,
      "grad_norm": 2.7158737182617188,
      "learning_rate": 4.935690626747538e-06,
      "loss": 0.1193,
      "step": 14356
    },
    {
      "epoch": 5.558265582655826,
      "grad_norm": 34.0816764831543,
      "learning_rate": 4.935260463715749e-06,
      "loss": 1.7475,
      "step": 14357
    },
    {
      "epoch": 5.558652729384437,
      "grad_norm": 35.53160858154297,
      "learning_rate": 4.93483030068396e-06,
      "loss": 0.9973,
      "step": 14358
    },
    {
      "epoch": 5.559039876113047,
      "grad_norm": 25.39148712158203,
      "learning_rate": 4.93440013765217e-06,
      "loss": 2.9883,
      "step": 14359
    },
    {
      "epoch": 5.559427022841657,
      "grad_norm": 18.268632888793945,
      "learning_rate": 4.9339699746203816e-06,
      "loss": 0.3947,
      "step": 14360
    },
    {
      "epoch": 5.559814169570267,
      "grad_norm": 63.4521369934082,
      "learning_rate": 4.933539811588592e-06,
      "loss": 0.9141,
      "step": 14361
    },
    {
      "epoch": 5.560201316298877,
      "grad_norm": 38.87653732299805,
      "learning_rate": 4.9331096485568036e-06,
      "loss": 0.3423,
      "step": 14362
    },
    {
      "epoch": 5.5605884630274875,
      "grad_norm": 15.021222114562988,
      "learning_rate": 4.932679485525014e-06,
      "loss": 0.3161,
      "step": 14363
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 10.369499206542969,
      "learning_rate": 4.9322493224932255e-06,
      "loss": 0.1702,
      "step": 14364
    },
    {
      "epoch": 5.561362756484708,
      "grad_norm": 19.674222946166992,
      "learning_rate": 4.931819159461436e-06,
      "loss": 0.5504,
      "step": 14365
    },
    {
      "epoch": 5.561749903213318,
      "grad_norm": 258.4390869140625,
      "learning_rate": 4.9313889964296475e-06,
      "loss": 2.6763,
      "step": 14366
    },
    {
      "epoch": 5.562137049941928,
      "grad_norm": 88.13973999023438,
      "learning_rate": 4.930958833397858e-06,
      "loss": 1.6974,
      "step": 14367
    },
    {
      "epoch": 5.562524196670538,
      "grad_norm": 40.3329963684082,
      "learning_rate": 4.9305286703660695e-06,
      "loss": 0.9598,
      "step": 14368
    },
    {
      "epoch": 5.562911343399148,
      "grad_norm": 24.30616569519043,
      "learning_rate": 4.93009850733428e-06,
      "loss": 0.3337,
      "step": 14369
    },
    {
      "epoch": 5.5632984901277585,
      "grad_norm": 5.594727516174316,
      "learning_rate": 4.9296683443024915e-06,
      "loss": 0.1206,
      "step": 14370
    },
    {
      "epoch": 5.563685636856368,
      "grad_norm": 43.11030197143555,
      "learning_rate": 4.929238181270702e-06,
      "loss": 1.6131,
      "step": 14371
    },
    {
      "epoch": 5.564072783584979,
      "grad_norm": 22.326932907104492,
      "learning_rate": 4.9288080182389135e-06,
      "loss": 1.2747,
      "step": 14372
    },
    {
      "epoch": 5.564459930313589,
      "grad_norm": 33.70463562011719,
      "learning_rate": 4.928377855207124e-06,
      "loss": 0.5883,
      "step": 14373
    },
    {
      "epoch": 5.564847077042199,
      "grad_norm": 53.510196685791016,
      "learning_rate": 4.927947692175335e-06,
      "loss": 2.0009,
      "step": 14374
    },
    {
      "epoch": 5.565234223770809,
      "grad_norm": 17.225154876708984,
      "learning_rate": 4.927517529143546e-06,
      "loss": 1.6859,
      "step": 14375
    },
    {
      "epoch": 5.56562137049942,
      "grad_norm": 24.94068145751953,
      "learning_rate": 4.927087366111757e-06,
      "loss": 1.5751,
      "step": 14376
    },
    {
      "epoch": 5.566008517228029,
      "grad_norm": 31.551111221313477,
      "learning_rate": 4.926657203079967e-06,
      "loss": 2.7596,
      "step": 14377
    },
    {
      "epoch": 5.56639566395664,
      "grad_norm": 27.694826126098633,
      "learning_rate": 4.926227040048179e-06,
      "loss": 1.6673,
      "step": 14378
    },
    {
      "epoch": 5.5667828106852495,
      "grad_norm": 53.13749694824219,
      "learning_rate": 4.925796877016389e-06,
      "loss": 0.8828,
      "step": 14379
    },
    {
      "epoch": 5.56716995741386,
      "grad_norm": 85.77879333496094,
      "learning_rate": 4.9253667139846006e-06,
      "loss": 3.5727,
      "step": 14380
    },
    {
      "epoch": 5.56755710414247,
      "grad_norm": 90.10285186767578,
      "learning_rate": 4.924936550952811e-06,
      "loss": 4.3173,
      "step": 14381
    },
    {
      "epoch": 5.56794425087108,
      "grad_norm": 26.507333755493164,
      "learning_rate": 4.9245063879210226e-06,
      "loss": 0.1407,
      "step": 14382
    },
    {
      "epoch": 5.568331397599691,
      "grad_norm": 30.58185577392578,
      "learning_rate": 4.924076224889233e-06,
      "loss": 3.1719,
      "step": 14383
    },
    {
      "epoch": 5.5687185443283,
      "grad_norm": 33.65993881225586,
      "learning_rate": 4.9236460618574445e-06,
      "loss": 1.3115,
      "step": 14384
    },
    {
      "epoch": 5.569105691056911,
      "grad_norm": 22.418067932128906,
      "learning_rate": 4.923215898825655e-06,
      "loss": 0.7557,
      "step": 14385
    },
    {
      "epoch": 5.56949283778552,
      "grad_norm": 170.0854949951172,
      "learning_rate": 4.9227857357938665e-06,
      "loss": 1.0637,
      "step": 14386
    },
    {
      "epoch": 5.569879984514131,
      "grad_norm": 6.823028564453125,
      "learning_rate": 4.922355572762077e-06,
      "loss": 0.2869,
      "step": 14387
    },
    {
      "epoch": 5.5702671312427405,
      "grad_norm": 126.1102066040039,
      "learning_rate": 4.9219254097302885e-06,
      "loss": 0.8105,
      "step": 14388
    },
    {
      "epoch": 5.570654277971351,
      "grad_norm": 43.62684631347656,
      "learning_rate": 4.921495246698499e-06,
      "loss": 2.3699,
      "step": 14389
    },
    {
      "epoch": 5.5710414246999616,
      "grad_norm": 51.504066467285156,
      "learning_rate": 4.9210650836667105e-06,
      "loss": 0.4817,
      "step": 14390
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 105.95682525634766,
      "learning_rate": 4.920634920634921e-06,
      "loss": 2.5324,
      "step": 14391
    },
    {
      "epoch": 5.571815718157182,
      "grad_norm": 21.628173828125,
      "learning_rate": 4.920204757603132e-06,
      "loss": 1.5526,
      "step": 14392
    },
    {
      "epoch": 5.572202864885792,
      "grad_norm": 89.83409118652344,
      "learning_rate": 4.919774594571343e-06,
      "loss": 3.0339,
      "step": 14393
    },
    {
      "epoch": 5.572590011614402,
      "grad_norm": 28.70145606994629,
      "learning_rate": 4.919344431539554e-06,
      "loss": 1.0051,
      "step": 14394
    },
    {
      "epoch": 5.572977158343012,
      "grad_norm": 60.10599899291992,
      "learning_rate": 4.918914268507765e-06,
      "loss": 2.0564,
      "step": 14395
    },
    {
      "epoch": 5.573364305071622,
      "grad_norm": 43.62163162231445,
      "learning_rate": 4.918484105475976e-06,
      "loss": 2.0546,
      "step": 14396
    },
    {
      "epoch": 5.5737514518002325,
      "grad_norm": 46.46541213989258,
      "learning_rate": 4.918053942444187e-06,
      "loss": 1.5381,
      "step": 14397
    },
    {
      "epoch": 5.574138598528842,
      "grad_norm": 43.79096984863281,
      "learning_rate": 4.917623779412398e-06,
      "loss": 1.9621,
      "step": 14398
    },
    {
      "epoch": 5.574525745257453,
      "grad_norm": 88.7519760131836,
      "learning_rate": 4.917193616380609e-06,
      "loss": 1.7382,
      "step": 14399
    },
    {
      "epoch": 5.574912891986063,
      "grad_norm": 23.955350875854492,
      "learning_rate": 4.9167634533488196e-06,
      "loss": 1.15,
      "step": 14400
    },
    {
      "epoch": 5.575300038714673,
      "grad_norm": 4.715015888214111,
      "learning_rate": 4.916333290317031e-06,
      "loss": 0.2031,
      "step": 14401
    },
    {
      "epoch": 5.575687185443283,
      "grad_norm": 38.76298522949219,
      "learning_rate": 4.9159031272852416e-06,
      "loss": 1.1216,
      "step": 14402
    },
    {
      "epoch": 5.576074332171893,
      "grad_norm": 23.214719772338867,
      "learning_rate": 4.915472964253453e-06,
      "loss": 0.2711,
      "step": 14403
    },
    {
      "epoch": 5.576461478900503,
      "grad_norm": 69.8265380859375,
      "learning_rate": 4.9150428012216635e-06,
      "loss": 1.5372,
      "step": 14404
    },
    {
      "epoch": 5.576848625629113,
      "grad_norm": 90.25108337402344,
      "learning_rate": 4.914612638189875e-06,
      "loss": 2.8642,
      "step": 14405
    },
    {
      "epoch": 5.5772357723577235,
      "grad_norm": 43.044857025146484,
      "learning_rate": 4.9141824751580855e-06,
      "loss": 1.2767,
      "step": 14406
    },
    {
      "epoch": 5.577622919086334,
      "grad_norm": 5.11599063873291,
      "learning_rate": 4.913752312126296e-06,
      "loss": 0.2274,
      "step": 14407
    },
    {
      "epoch": 5.578010065814944,
      "grad_norm": 16.690956115722656,
      "learning_rate": 4.9133221490945075e-06,
      "loss": 0.54,
      "step": 14408
    },
    {
      "epoch": 5.578397212543554,
      "grad_norm": 5.662042140960693,
      "learning_rate": 4.912891986062718e-06,
      "loss": 0.3527,
      "step": 14409
    },
    {
      "epoch": 5.578784359272164,
      "grad_norm": 36.903419494628906,
      "learning_rate": 4.912461823030929e-06,
      "loss": 2.6132,
      "step": 14410
    },
    {
      "epoch": 5.579171506000774,
      "grad_norm": 54.594444274902344,
      "learning_rate": 4.91203165999914e-06,
      "loss": 0.9589,
      "step": 14411
    },
    {
      "epoch": 5.579558652729385,
      "grad_norm": 75.6706314086914,
      "learning_rate": 4.911601496967351e-06,
      "loss": 1.0668,
      "step": 14412
    },
    {
      "epoch": 5.579945799457994,
      "grad_norm": 27.490398406982422,
      "learning_rate": 4.911171333935562e-06,
      "loss": 0.2537,
      "step": 14413
    },
    {
      "epoch": 5.580332946186605,
      "grad_norm": 32.0147590637207,
      "learning_rate": 4.910741170903773e-06,
      "loss": 0.9359,
      "step": 14414
    },
    {
      "epoch": 5.5807200929152145,
      "grad_norm": 2.3786370754241943,
      "learning_rate": 4.910311007871984e-06,
      "loss": 0.0953,
      "step": 14415
    },
    {
      "epoch": 5.581107239643825,
      "grad_norm": 0.8283777832984924,
      "learning_rate": 4.909880844840195e-06,
      "loss": 0.0233,
      "step": 14416
    },
    {
      "epoch": 5.581494386372436,
      "grad_norm": 46.2586555480957,
      "learning_rate": 4.909450681808406e-06,
      "loss": 0.6085,
      "step": 14417
    },
    {
      "epoch": 5.581881533101045,
      "grad_norm": 2.148715019226074,
      "learning_rate": 4.909020518776617e-06,
      "loss": 0.0871,
      "step": 14418
    },
    {
      "epoch": 5.582268679829656,
      "grad_norm": 27.96935272216797,
      "learning_rate": 4.908590355744828e-06,
      "loss": 0.5516,
      "step": 14419
    },
    {
      "epoch": 5.582655826558265,
      "grad_norm": 49.82795715332031,
      "learning_rate": 4.9081601927130386e-06,
      "loss": 2.8318,
      "step": 14420
    },
    {
      "epoch": 5.583042973286876,
      "grad_norm": 7.378545761108398,
      "learning_rate": 4.90773002968125e-06,
      "loss": 0.3194,
      "step": 14421
    },
    {
      "epoch": 5.583430120015485,
      "grad_norm": 3.5180718898773193,
      "learning_rate": 4.9072998666494606e-06,
      "loss": 0.1884,
      "step": 14422
    },
    {
      "epoch": 5.583817266744096,
      "grad_norm": 85.1716079711914,
      "learning_rate": 4.906869703617672e-06,
      "loss": 0.7908,
      "step": 14423
    },
    {
      "epoch": 5.5842044134727065,
      "grad_norm": 4.393505573272705,
      "learning_rate": 4.9064395405858825e-06,
      "loss": 0.2693,
      "step": 14424
    },
    {
      "epoch": 5.584591560201316,
      "grad_norm": 19.344348907470703,
      "learning_rate": 4.906009377554093e-06,
      "loss": 2.2101,
      "step": 14425
    },
    {
      "epoch": 5.584978706929927,
      "grad_norm": 35.96479797363281,
      "learning_rate": 4.9055792145223045e-06,
      "loss": 1.9751,
      "step": 14426
    },
    {
      "epoch": 5.585365853658536,
      "grad_norm": 38.854671478271484,
      "learning_rate": 4.905149051490515e-06,
      "loss": 1.136,
      "step": 14427
    },
    {
      "epoch": 5.585753000387147,
      "grad_norm": 19.99300765991211,
      "learning_rate": 4.904718888458726e-06,
      "loss": 2.0243,
      "step": 14428
    },
    {
      "epoch": 5.586140147115757,
      "grad_norm": 51.19778060913086,
      "learning_rate": 4.904288725426937e-06,
      "loss": 1.3361,
      "step": 14429
    },
    {
      "epoch": 5.586527293844367,
      "grad_norm": 130.07192993164062,
      "learning_rate": 4.903858562395148e-06,
      "loss": 0.9439,
      "step": 14430
    },
    {
      "epoch": 5.586914440572977,
      "grad_norm": 72.6250991821289,
      "learning_rate": 4.903428399363359e-06,
      "loss": 0.694,
      "step": 14431
    },
    {
      "epoch": 5.587301587301587,
      "grad_norm": 68.61360931396484,
      "learning_rate": 4.90299823633157e-06,
      "loss": 2.2031,
      "step": 14432
    },
    {
      "epoch": 5.5876887340301975,
      "grad_norm": 13.828466415405273,
      "learning_rate": 4.902568073299781e-06,
      "loss": 0.2668,
      "step": 14433
    },
    {
      "epoch": 5.588075880758808,
      "grad_norm": 2.727125883102417,
      "learning_rate": 4.9021379102679925e-06,
      "loss": 0.1194,
      "step": 14434
    },
    {
      "epoch": 5.588463027487418,
      "grad_norm": 63.96477508544922,
      "learning_rate": 4.901707747236203e-06,
      "loss": 1.9248,
      "step": 14435
    },
    {
      "epoch": 5.588850174216028,
      "grad_norm": 16.391597747802734,
      "learning_rate": 4.9012775842044144e-06,
      "loss": 0.7812,
      "step": 14436
    },
    {
      "epoch": 5.589237320944638,
      "grad_norm": 66.26790618896484,
      "learning_rate": 4.900847421172625e-06,
      "loss": 0.5487,
      "step": 14437
    },
    {
      "epoch": 5.589624467673248,
      "grad_norm": 114.71707153320312,
      "learning_rate": 4.9004172581408364e-06,
      "loss": 1.9595,
      "step": 14438
    },
    {
      "epoch": 5.590011614401858,
      "grad_norm": 24.128816604614258,
      "learning_rate": 4.899987095109047e-06,
      "loss": 1.3089,
      "step": 14439
    },
    {
      "epoch": 5.590398761130468,
      "grad_norm": 4.533959865570068,
      "learning_rate": 4.8995569320772576e-06,
      "loss": 0.1431,
      "step": 14440
    },
    {
      "epoch": 5.590785907859079,
      "grad_norm": 163.21202087402344,
      "learning_rate": 4.899126769045469e-06,
      "loss": 3.1296,
      "step": 14441
    },
    {
      "epoch": 5.5911730545876885,
      "grad_norm": 41.43975830078125,
      "learning_rate": 4.8986966060136796e-06,
      "loss": 0.4784,
      "step": 14442
    },
    {
      "epoch": 5.591560201316299,
      "grad_norm": 65.76991271972656,
      "learning_rate": 4.89826644298189e-06,
      "loss": 1.3584,
      "step": 14443
    },
    {
      "epoch": 5.591947348044909,
      "grad_norm": 5.991824626922607,
      "learning_rate": 4.8978362799501015e-06,
      "loss": 0.1104,
      "step": 14444
    },
    {
      "epoch": 5.592334494773519,
      "grad_norm": 7.820708751678467,
      "learning_rate": 4.897406116918312e-06,
      "loss": 0.2296,
      "step": 14445
    },
    {
      "epoch": 5.59272164150213,
      "grad_norm": 35.66474151611328,
      "learning_rate": 4.8969759538865235e-06,
      "loss": 1.6538,
      "step": 14446
    },
    {
      "epoch": 5.593108788230739,
      "grad_norm": 27.76463508605957,
      "learning_rate": 4.896545790854734e-06,
      "loss": 1.9457,
      "step": 14447
    },
    {
      "epoch": 5.59349593495935,
      "grad_norm": 66.61547088623047,
      "learning_rate": 4.8961156278229455e-06,
      "loss": 0.2475,
      "step": 14448
    },
    {
      "epoch": 5.5938830816879594,
      "grad_norm": 32.62873840332031,
      "learning_rate": 4.895685464791156e-06,
      "loss": 1.5731,
      "step": 14449
    },
    {
      "epoch": 5.59427022841657,
      "grad_norm": 65.35519409179688,
      "learning_rate": 4.8952553017593675e-06,
      "loss": 1.3226,
      "step": 14450
    },
    {
      "epoch": 5.5946573751451805,
      "grad_norm": 37.8310661315918,
      "learning_rate": 4.894825138727578e-06,
      "loss": 1.6995,
      "step": 14451
    },
    {
      "epoch": 5.59504452187379,
      "grad_norm": 38.09794616699219,
      "learning_rate": 4.8943949756957895e-06,
      "loss": 0.4906,
      "step": 14452
    },
    {
      "epoch": 5.595431668602401,
      "grad_norm": 29.484233856201172,
      "learning_rate": 4.893964812664e-06,
      "loss": 1.4793,
      "step": 14453
    },
    {
      "epoch": 5.59581881533101,
      "grad_norm": 134.48953247070312,
      "learning_rate": 4.8935346496322115e-06,
      "loss": 0.5784,
      "step": 14454
    },
    {
      "epoch": 5.596205962059621,
      "grad_norm": 78.18428802490234,
      "learning_rate": 4.893104486600422e-06,
      "loss": 1.9973,
      "step": 14455
    },
    {
      "epoch": 5.59659310878823,
      "grad_norm": 33.270965576171875,
      "learning_rate": 4.8926743235686334e-06,
      "loss": 1.517,
      "step": 14456
    },
    {
      "epoch": 5.596980255516841,
      "grad_norm": 78.93831634521484,
      "learning_rate": 4.892244160536844e-06,
      "loss": 0.6249,
      "step": 14457
    },
    {
      "epoch": 5.597367402245451,
      "grad_norm": 11.598372459411621,
      "learning_rate": 4.891813997505055e-06,
      "loss": 0.397,
      "step": 14458
    },
    {
      "epoch": 5.597754548974061,
      "grad_norm": 113.57034301757812,
      "learning_rate": 4.891383834473266e-06,
      "loss": 0.9322,
      "step": 14459
    },
    {
      "epoch": 5.5981416957026715,
      "grad_norm": 30.853466033935547,
      "learning_rate": 4.8909536714414766e-06,
      "loss": 1.3089,
      "step": 14460
    },
    {
      "epoch": 5.598528842431281,
      "grad_norm": 73.06982421875,
      "learning_rate": 4.890523508409687e-06,
      "loss": 1.776,
      "step": 14461
    },
    {
      "epoch": 5.598915989159892,
      "grad_norm": 10.174320220947266,
      "learning_rate": 4.8900933453778985e-06,
      "loss": 0.532,
      "step": 14462
    },
    {
      "epoch": 5.599303135888501,
      "grad_norm": 57.236446380615234,
      "learning_rate": 4.889663182346109e-06,
      "loss": 4.7131,
      "step": 14463
    },
    {
      "epoch": 5.599690282617112,
      "grad_norm": 63.330726623535156,
      "learning_rate": 4.8892330193143205e-06,
      "loss": 0.6935,
      "step": 14464
    },
    {
      "epoch": 5.600077429345722,
      "grad_norm": 30.77058982849121,
      "learning_rate": 4.888802856282531e-06,
      "loss": 0.3006,
      "step": 14465
    },
    {
      "epoch": 5.600464576074332,
      "grad_norm": 47.67647171020508,
      "learning_rate": 4.8883726932507425e-06,
      "loss": 1.5448,
      "step": 14466
    },
    {
      "epoch": 5.600851722802942,
      "grad_norm": 35.57061004638672,
      "learning_rate": 4.887942530218953e-06,
      "loss": 1.2452,
      "step": 14467
    },
    {
      "epoch": 5.601238869531553,
      "grad_norm": 11.78947925567627,
      "learning_rate": 4.8875123671871645e-06,
      "loss": 0.221,
      "step": 14468
    },
    {
      "epoch": 5.6016260162601625,
      "grad_norm": 139.32327270507812,
      "learning_rate": 4.887082204155375e-06,
      "loss": 1.7521,
      "step": 14469
    },
    {
      "epoch": 5.602013162988773,
      "grad_norm": 29.485729217529297,
      "learning_rate": 4.8866520411235865e-06,
      "loss": 1.3503,
      "step": 14470
    },
    {
      "epoch": 5.602400309717383,
      "grad_norm": 3.159130811691284,
      "learning_rate": 4.886221878091797e-06,
      "loss": 0.0571,
      "step": 14471
    },
    {
      "epoch": 5.602787456445993,
      "grad_norm": 79.91680908203125,
      "learning_rate": 4.8857917150600085e-06,
      "loss": 2.4362,
      "step": 14472
    },
    {
      "epoch": 5.603174603174603,
      "grad_norm": 93.14013671875,
      "learning_rate": 4.885361552028219e-06,
      "loss": 1.4994,
      "step": 14473
    },
    {
      "epoch": 5.603561749903213,
      "grad_norm": 81.87112426757812,
      "learning_rate": 4.8849313889964305e-06,
      "loss": 0.9158,
      "step": 14474
    },
    {
      "epoch": 5.603948896631824,
      "grad_norm": 54.61558151245117,
      "learning_rate": 4.884501225964641e-06,
      "loss": 0.7214,
      "step": 14475
    },
    {
      "epoch": 5.6043360433604335,
      "grad_norm": 6.355631351470947,
      "learning_rate": 4.884071062932852e-06,
      "loss": 0.146,
      "step": 14476
    },
    {
      "epoch": 5.604723190089044,
      "grad_norm": 21.705780029296875,
      "learning_rate": 4.883640899901063e-06,
      "loss": 1.6669,
      "step": 14477
    },
    {
      "epoch": 5.605110336817654,
      "grad_norm": 55.29988098144531,
      "learning_rate": 4.883210736869274e-06,
      "loss": 0.7109,
      "step": 14478
    },
    {
      "epoch": 5.605497483546264,
      "grad_norm": 73.4480972290039,
      "learning_rate": 4.882780573837485e-06,
      "loss": 1.1399,
      "step": 14479
    },
    {
      "epoch": 5.605884630274874,
      "grad_norm": 40.612884521484375,
      "learning_rate": 4.8823504108056956e-06,
      "loss": 1.0424,
      "step": 14480
    },
    {
      "epoch": 5.606271777003484,
      "grad_norm": 47.24541473388672,
      "learning_rate": 4.881920247773907e-06,
      "loss": 3.0805,
      "step": 14481
    },
    {
      "epoch": 5.606658923732095,
      "grad_norm": 26.991235733032227,
      "learning_rate": 4.8814900847421175e-06,
      "loss": 1.179,
      "step": 14482
    },
    {
      "epoch": 5.607046070460704,
      "grad_norm": 45.66044616699219,
      "learning_rate": 4.881059921710329e-06,
      "loss": 1.1559,
      "step": 14483
    },
    {
      "epoch": 5.607433217189315,
      "grad_norm": 32.140586853027344,
      "learning_rate": 4.8806297586785395e-06,
      "loss": 0.9867,
      "step": 14484
    },
    {
      "epoch": 5.607820363917925,
      "grad_norm": 20.583541870117188,
      "learning_rate": 4.880199595646751e-06,
      "loss": 1.6592,
      "step": 14485
    },
    {
      "epoch": 5.608207510646535,
      "grad_norm": 22.625391006469727,
      "learning_rate": 4.8797694326149615e-06,
      "loss": 1.0659,
      "step": 14486
    },
    {
      "epoch": 5.6085946573751455,
      "grad_norm": 4.2904157638549805,
      "learning_rate": 4.879339269583173e-06,
      "loss": 0.1364,
      "step": 14487
    },
    {
      "epoch": 5.608981804103755,
      "grad_norm": 67.15711212158203,
      "learning_rate": 4.8789091065513835e-06,
      "loss": 1.3135,
      "step": 14488
    },
    {
      "epoch": 5.609368950832366,
      "grad_norm": 22.979236602783203,
      "learning_rate": 4.878478943519595e-06,
      "loss": 0.8675,
      "step": 14489
    },
    {
      "epoch": 5.609756097560975,
      "grad_norm": 5.206203460693359,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 0.1725,
      "step": 14490
    },
    {
      "epoch": 5.610143244289586,
      "grad_norm": 94.6104736328125,
      "learning_rate": 4.877618617456016e-06,
      "loss": 2.1218,
      "step": 14491
    },
    {
      "epoch": 5.610530391018196,
      "grad_norm": 91.51522064208984,
      "learning_rate": 4.8771884544242275e-06,
      "loss": 0.2853,
      "step": 14492
    },
    {
      "epoch": 5.610917537746806,
      "grad_norm": 20.94338607788086,
      "learning_rate": 4.876758291392438e-06,
      "loss": 1.0317,
      "step": 14493
    },
    {
      "epoch": 5.611304684475416,
      "grad_norm": 76.48991394042969,
      "learning_rate": 4.876328128360649e-06,
      "loss": 0.9923,
      "step": 14494
    },
    {
      "epoch": 5.611691831204026,
      "grad_norm": 37.09790802001953,
      "learning_rate": 4.87589796532886e-06,
      "loss": 0.3489,
      "step": 14495
    },
    {
      "epoch": 5.6120789779326365,
      "grad_norm": 58.02790069580078,
      "learning_rate": 4.875467802297071e-06,
      "loss": 1.7731,
      "step": 14496
    },
    {
      "epoch": 5.612466124661246,
      "grad_norm": 111.32035827636719,
      "learning_rate": 4.875037639265282e-06,
      "loss": 1.1448,
      "step": 14497
    },
    {
      "epoch": 5.612853271389857,
      "grad_norm": 49.307865142822266,
      "learning_rate": 4.8746074762334926e-06,
      "loss": 2.3438,
      "step": 14498
    },
    {
      "epoch": 5.613240418118467,
      "grad_norm": 21.06876564025879,
      "learning_rate": 4.874177313201704e-06,
      "loss": 0.1864,
      "step": 14499
    },
    {
      "epoch": 5.613627564847077,
      "grad_norm": 19.477027893066406,
      "learning_rate": 4.8737471501699146e-06,
      "loss": 0.2107,
      "step": 14500
    },
    {
      "epoch": 5.614014711575687,
      "grad_norm": 38.76267623901367,
      "learning_rate": 4.873316987138126e-06,
      "loss": 1.8187,
      "step": 14501
    },
    {
      "epoch": 5.614401858304297,
      "grad_norm": 66.32969665527344,
      "learning_rate": 4.8728868241063365e-06,
      "loss": 0.8778,
      "step": 14502
    },
    {
      "epoch": 5.6147890050329075,
      "grad_norm": 21.753890991210938,
      "learning_rate": 4.872456661074548e-06,
      "loss": 0.3574,
      "step": 14503
    },
    {
      "epoch": 5.615176151761518,
      "grad_norm": 56.85071563720703,
      "learning_rate": 4.8720264980427585e-06,
      "loss": 1.4651,
      "step": 14504
    },
    {
      "epoch": 5.615563298490128,
      "grad_norm": 3.2311601638793945,
      "learning_rate": 4.87159633501097e-06,
      "loss": 0.0998,
      "step": 14505
    },
    {
      "epoch": 5.615950445218738,
      "grad_norm": 28.750003814697266,
      "learning_rate": 4.8711661719791805e-06,
      "loss": 0.8735,
      "step": 14506
    },
    {
      "epoch": 5.616337591947348,
      "grad_norm": 34.19023513793945,
      "learning_rate": 4.870736008947392e-06,
      "loss": 1.464,
      "step": 14507
    },
    {
      "epoch": 5.616724738675958,
      "grad_norm": 54.82157516479492,
      "learning_rate": 4.8703058459156025e-06,
      "loss": 2.5393,
      "step": 14508
    },
    {
      "epoch": 5.617111885404569,
      "grad_norm": 6.2733917236328125,
      "learning_rate": 4.869875682883813e-06,
      "loss": 0.5066,
      "step": 14509
    },
    {
      "epoch": 5.617499032133178,
      "grad_norm": 21.33887481689453,
      "learning_rate": 4.8694455198520245e-06,
      "loss": 0.904,
      "step": 14510
    },
    {
      "epoch": 5.617886178861789,
      "grad_norm": 13.140902519226074,
      "learning_rate": 4.869015356820235e-06,
      "loss": 0.6712,
      "step": 14511
    },
    {
      "epoch": 5.6182733255903985,
      "grad_norm": 35.35758590698242,
      "learning_rate": 4.868585193788446e-06,
      "loss": 0.5764,
      "step": 14512
    },
    {
      "epoch": 5.618660472319009,
      "grad_norm": 34.596580505371094,
      "learning_rate": 4.868155030756657e-06,
      "loss": 1.552,
      "step": 14513
    },
    {
      "epoch": 5.619047619047619,
      "grad_norm": 14.121909141540527,
      "learning_rate": 4.867724867724868e-06,
      "loss": 0.3371,
      "step": 14514
    },
    {
      "epoch": 5.619434765776229,
      "grad_norm": 63.31659698486328,
      "learning_rate": 4.867294704693079e-06,
      "loss": 0.3352,
      "step": 14515
    },
    {
      "epoch": 5.61982191250484,
      "grad_norm": 5.08951473236084,
      "learning_rate": 4.8668645416612904e-06,
      "loss": 0.2225,
      "step": 14516
    },
    {
      "epoch": 5.620209059233449,
      "grad_norm": 40.96842956542969,
      "learning_rate": 4.866434378629501e-06,
      "loss": 0.7138,
      "step": 14517
    },
    {
      "epoch": 5.62059620596206,
      "grad_norm": 47.04890441894531,
      "learning_rate": 4.866004215597712e-06,
      "loss": 0.4552,
      "step": 14518
    },
    {
      "epoch": 5.620983352690669,
      "grad_norm": 84.07450866699219,
      "learning_rate": 4.865574052565923e-06,
      "loss": 2.5271,
      "step": 14519
    },
    {
      "epoch": 5.62137049941928,
      "grad_norm": 45.44771957397461,
      "learning_rate": 4.865143889534134e-06,
      "loss": 2.0651,
      "step": 14520
    },
    {
      "epoch": 5.62175764614789,
      "grad_norm": 37.857383728027344,
      "learning_rate": 4.864713726502345e-06,
      "loss": 0.4843,
      "step": 14521
    },
    {
      "epoch": 5.6221447928765,
      "grad_norm": 48.537837982177734,
      "learning_rate": 4.864283563470556e-06,
      "loss": 1.8539,
      "step": 14522
    },
    {
      "epoch": 5.6225319396051106,
      "grad_norm": 64.81586456298828,
      "learning_rate": 4.863853400438767e-06,
      "loss": 2.6667,
      "step": 14523
    },
    {
      "epoch": 5.62291908633372,
      "grad_norm": 26.275293350219727,
      "learning_rate": 4.8634232374069775e-06,
      "loss": 0.5621,
      "step": 14524
    },
    {
      "epoch": 5.623306233062331,
      "grad_norm": 8.023824691772461,
      "learning_rate": 4.862993074375189e-06,
      "loss": 0.243,
      "step": 14525
    },
    {
      "epoch": 5.623693379790941,
      "grad_norm": 104.58997344970703,
      "learning_rate": 4.8625629113433995e-06,
      "loss": 2.2499,
      "step": 14526
    },
    {
      "epoch": 5.624080526519551,
      "grad_norm": 42.24421691894531,
      "learning_rate": 4.86213274831161e-06,
      "loss": 1.4067,
      "step": 14527
    },
    {
      "epoch": 5.624467673248161,
      "grad_norm": 81.05563354492188,
      "learning_rate": 4.8617025852798215e-06,
      "loss": 0.2889,
      "step": 14528
    },
    {
      "epoch": 5.624854819976771,
      "grad_norm": 34.46384811401367,
      "learning_rate": 4.861272422248032e-06,
      "loss": 1.3203,
      "step": 14529
    },
    {
      "epoch": 5.6252419667053815,
      "grad_norm": 14.016029357910156,
      "learning_rate": 4.8608422592162435e-06,
      "loss": 1.2272,
      "step": 14530
    },
    {
      "epoch": 5.625629113433991,
      "grad_norm": 43.248680114746094,
      "learning_rate": 4.860412096184454e-06,
      "loss": 1.2318,
      "step": 14531
    },
    {
      "epoch": 5.626016260162602,
      "grad_norm": 18.080373764038086,
      "learning_rate": 4.8599819331526655e-06,
      "loss": 0.4823,
      "step": 14532
    },
    {
      "epoch": 5.626403406891212,
      "grad_norm": 32.56390380859375,
      "learning_rate": 4.859551770120876e-06,
      "loss": 3.3651,
      "step": 14533
    },
    {
      "epoch": 5.626790553619822,
      "grad_norm": 28.54327964782715,
      "learning_rate": 4.8591216070890874e-06,
      "loss": 1.5568,
      "step": 14534
    },
    {
      "epoch": 5.627177700348432,
      "grad_norm": 43.233402252197266,
      "learning_rate": 4.858691444057298e-06,
      "loss": 3.0044,
      "step": 14535
    },
    {
      "epoch": 5.627564847077042,
      "grad_norm": 24.090232849121094,
      "learning_rate": 4.8582612810255094e-06,
      "loss": 0.2023,
      "step": 14536
    },
    {
      "epoch": 5.627951993805652,
      "grad_norm": 40.45509719848633,
      "learning_rate": 4.85783111799372e-06,
      "loss": 0.4274,
      "step": 14537
    },
    {
      "epoch": 5.628339140534262,
      "grad_norm": 60.72053527832031,
      "learning_rate": 4.857400954961931e-06,
      "loss": 0.4568,
      "step": 14538
    },
    {
      "epoch": 5.6287262872628725,
      "grad_norm": 6.135773658752441,
      "learning_rate": 4.856970791930142e-06,
      "loss": 0.2692,
      "step": 14539
    },
    {
      "epoch": 5.629113433991483,
      "grad_norm": 34.57952880859375,
      "learning_rate": 4.856540628898353e-06,
      "loss": 1.7632,
      "step": 14540
    },
    {
      "epoch": 5.629500580720093,
      "grad_norm": 34.69791030883789,
      "learning_rate": 4.856110465866564e-06,
      "loss": 0.9027,
      "step": 14541
    },
    {
      "epoch": 5.629887727448703,
      "grad_norm": 56.006996154785156,
      "learning_rate": 4.8556803028347745e-06,
      "loss": 1.023,
      "step": 14542
    },
    {
      "epoch": 5.630274874177314,
      "grad_norm": 39.53412628173828,
      "learning_rate": 4.855250139802986e-06,
      "loss": 1.1633,
      "step": 14543
    },
    {
      "epoch": 5.630662020905923,
      "grad_norm": 2.9507384300231934,
      "learning_rate": 4.8548199767711965e-06,
      "loss": 0.1019,
      "step": 14544
    },
    {
      "epoch": 5.631049167634534,
      "grad_norm": 18.10003662109375,
      "learning_rate": 4.854389813739407e-06,
      "loss": 0.4241,
      "step": 14545
    },
    {
      "epoch": 5.631436314363143,
      "grad_norm": 29.496984481811523,
      "learning_rate": 4.8539596507076185e-06,
      "loss": 1.1972,
      "step": 14546
    },
    {
      "epoch": 5.631823461091754,
      "grad_norm": 56.78850173950195,
      "learning_rate": 4.853529487675829e-06,
      "loss": 0.6187,
      "step": 14547
    },
    {
      "epoch": 5.6322106078203635,
      "grad_norm": 1.165802240371704,
      "learning_rate": 4.8530993246440405e-06,
      "loss": 0.0306,
      "step": 14548
    },
    {
      "epoch": 5.632597754548974,
      "grad_norm": 16.04161262512207,
      "learning_rate": 4.852669161612251e-06,
      "loss": 1.1276,
      "step": 14549
    },
    {
      "epoch": 5.6329849012775846,
      "grad_norm": 54.967105865478516,
      "learning_rate": 4.8522389985804625e-06,
      "loss": 2.1085,
      "step": 14550
    },
    {
      "epoch": 5.633372048006194,
      "grad_norm": 26.445804595947266,
      "learning_rate": 4.851808835548673e-06,
      "loss": 0.2205,
      "step": 14551
    },
    {
      "epoch": 5.633759194734805,
      "grad_norm": 16.012611389160156,
      "learning_rate": 4.8513786725168845e-06,
      "loss": 1.0266,
      "step": 14552
    },
    {
      "epoch": 5.634146341463414,
      "grad_norm": 3.1339006423950195,
      "learning_rate": 4.850948509485095e-06,
      "loss": 0.1229,
      "step": 14553
    },
    {
      "epoch": 5.634533488192025,
      "grad_norm": 7.471309661865234,
      "learning_rate": 4.8505183464533064e-06,
      "loss": 0.4468,
      "step": 14554
    },
    {
      "epoch": 5.634920634920634,
      "grad_norm": 8.395185470581055,
      "learning_rate": 4.850088183421517e-06,
      "loss": 0.2162,
      "step": 14555
    },
    {
      "epoch": 5.635307781649245,
      "grad_norm": 23.57670021057129,
      "learning_rate": 4.8496580203897284e-06,
      "loss": 0.4506,
      "step": 14556
    },
    {
      "epoch": 5.6356949283778555,
      "grad_norm": 56.38756561279297,
      "learning_rate": 4.849227857357939e-06,
      "loss": 3.1048,
      "step": 14557
    },
    {
      "epoch": 5.636082075106465,
      "grad_norm": 84.97811126708984,
      "learning_rate": 4.84879769432615e-06,
      "loss": 0.555,
      "step": 14558
    },
    {
      "epoch": 5.636469221835076,
      "grad_norm": 39.76179504394531,
      "learning_rate": 4.848367531294361e-06,
      "loss": 0.5795,
      "step": 14559
    },
    {
      "epoch": 5.636856368563686,
      "grad_norm": 47.61794662475586,
      "learning_rate": 4.8479373682625716e-06,
      "loss": 0.5597,
      "step": 14560
    },
    {
      "epoch": 5.637243515292296,
      "grad_norm": 69.46675872802734,
      "learning_rate": 4.847507205230783e-06,
      "loss": 0.6531,
      "step": 14561
    },
    {
      "epoch": 5.637630662020906,
      "grad_norm": 65.33515167236328,
      "learning_rate": 4.8470770421989935e-06,
      "loss": 3.4738,
      "step": 14562
    },
    {
      "epoch": 5.638017808749516,
      "grad_norm": 89.63719177246094,
      "learning_rate": 4.846646879167205e-06,
      "loss": 1.6318,
      "step": 14563
    },
    {
      "epoch": 5.638404955478126,
      "grad_norm": 59.38709259033203,
      "learning_rate": 4.8462167161354155e-06,
      "loss": 2.9219,
      "step": 14564
    },
    {
      "epoch": 5.638792102206736,
      "grad_norm": 32.50940704345703,
      "learning_rate": 4.845786553103627e-06,
      "loss": 2.7769,
      "step": 14565
    },
    {
      "epoch": 5.6391792489353465,
      "grad_norm": 0.8816835880279541,
      "learning_rate": 4.8453563900718375e-06,
      "loss": 0.0253,
      "step": 14566
    },
    {
      "epoch": 5.639566395663957,
      "grad_norm": 18.19235610961914,
      "learning_rate": 4.844926227040049e-06,
      "loss": 1.7218,
      "step": 14567
    },
    {
      "epoch": 5.639953542392567,
      "grad_norm": 74.93124389648438,
      "learning_rate": 4.8444960640082595e-06,
      "loss": 1.0,
      "step": 14568
    },
    {
      "epoch": 5.640340689121177,
      "grad_norm": 73.50387573242188,
      "learning_rate": 4.844065900976471e-06,
      "loss": 2.442,
      "step": 14569
    },
    {
      "epoch": 5.640727835849787,
      "grad_norm": 3.4427194595336914,
      "learning_rate": 4.8436357379446815e-06,
      "loss": 0.1189,
      "step": 14570
    },
    {
      "epoch": 5.641114982578397,
      "grad_norm": 67.52935028076172,
      "learning_rate": 4.843205574912893e-06,
      "loss": 2.4521,
      "step": 14571
    },
    {
      "epoch": 5.641502129307007,
      "grad_norm": 44.36195755004883,
      "learning_rate": 4.8427754118811035e-06,
      "loss": 2.1549,
      "step": 14572
    },
    {
      "epoch": 5.641889276035617,
      "grad_norm": 34.73823547363281,
      "learning_rate": 4.842345248849314e-06,
      "loss": 1.1534,
      "step": 14573
    },
    {
      "epoch": 5.642276422764228,
      "grad_norm": 0.838611364364624,
      "learning_rate": 4.8419150858175254e-06,
      "loss": 0.0236,
      "step": 14574
    },
    {
      "epoch": 5.6426635694928375,
      "grad_norm": 136.0468292236328,
      "learning_rate": 4.841484922785736e-06,
      "loss": 1.1218,
      "step": 14575
    },
    {
      "epoch": 5.643050716221448,
      "grad_norm": 4.64043664932251,
      "learning_rate": 4.8410547597539474e-06,
      "loss": 0.1795,
      "step": 14576
    },
    {
      "epoch": 5.643437862950059,
      "grad_norm": 157.39955139160156,
      "learning_rate": 4.840624596722158e-06,
      "loss": 2.1141,
      "step": 14577
    },
    {
      "epoch": 5.643825009678668,
      "grad_norm": 9.653705596923828,
      "learning_rate": 4.8401944336903686e-06,
      "loss": 0.4951,
      "step": 14578
    },
    {
      "epoch": 5.644212156407279,
      "grad_norm": 79.80138397216797,
      "learning_rate": 4.83976427065858e-06,
      "loss": 0.7739,
      "step": 14579
    },
    {
      "epoch": 5.644599303135888,
      "grad_norm": 34.218994140625,
      "learning_rate": 4.8393341076267905e-06,
      "loss": 0.6081,
      "step": 14580
    },
    {
      "epoch": 5.644986449864499,
      "grad_norm": 74.03765869140625,
      "learning_rate": 4.838903944595002e-06,
      "loss": 1.9465,
      "step": 14581
    },
    {
      "epoch": 5.6453735965931084,
      "grad_norm": 5.038256645202637,
      "learning_rate": 4.8384737815632125e-06,
      "loss": 0.3643,
      "step": 14582
    },
    {
      "epoch": 5.645760743321719,
      "grad_norm": 29.05423355102539,
      "learning_rate": 4.838043618531424e-06,
      "loss": 1.3141,
      "step": 14583
    },
    {
      "epoch": 5.6461478900503295,
      "grad_norm": 53.197731018066406,
      "learning_rate": 4.8376134554996345e-06,
      "loss": 0.9649,
      "step": 14584
    },
    {
      "epoch": 5.646535036778939,
      "grad_norm": 54.945884704589844,
      "learning_rate": 4.837183292467846e-06,
      "loss": 2.1038,
      "step": 14585
    },
    {
      "epoch": 5.64692218350755,
      "grad_norm": 52.01945114135742,
      "learning_rate": 4.8367531294360565e-06,
      "loss": 2.25,
      "step": 14586
    },
    {
      "epoch": 5.647309330236159,
      "grad_norm": 19.07560157775879,
      "learning_rate": 4.836322966404268e-06,
      "loss": 1.8588,
      "step": 14587
    },
    {
      "epoch": 5.64769647696477,
      "grad_norm": 68.40179443359375,
      "learning_rate": 4.8358928033724785e-06,
      "loss": 1.6962,
      "step": 14588
    },
    {
      "epoch": 5.648083623693379,
      "grad_norm": 67.30000305175781,
      "learning_rate": 4.83546264034069e-06,
      "loss": 1.8328,
      "step": 14589
    },
    {
      "epoch": 5.64847077042199,
      "grad_norm": 6.894538879394531,
      "learning_rate": 4.8350324773089005e-06,
      "loss": 0.2519,
      "step": 14590
    },
    {
      "epoch": 5.6488579171506,
      "grad_norm": 36.272701263427734,
      "learning_rate": 4.834602314277111e-06,
      "loss": 0.4739,
      "step": 14591
    },
    {
      "epoch": 5.64924506387921,
      "grad_norm": 69.59932708740234,
      "learning_rate": 4.8341721512453225e-06,
      "loss": 2.4541,
      "step": 14592
    },
    {
      "epoch": 5.6496322106078205,
      "grad_norm": 76.64872741699219,
      "learning_rate": 4.833741988213533e-06,
      "loss": 1.9283,
      "step": 14593
    },
    {
      "epoch": 5.65001935733643,
      "grad_norm": 44.178192138671875,
      "learning_rate": 4.8333118251817444e-06,
      "loss": 0.2628,
      "step": 14594
    },
    {
      "epoch": 5.650406504065041,
      "grad_norm": 38.43864822387695,
      "learning_rate": 4.832881662149955e-06,
      "loss": 1.3823,
      "step": 14595
    },
    {
      "epoch": 5.650793650793651,
      "grad_norm": 67.62479400634766,
      "learning_rate": 4.832451499118166e-06,
      "loss": 2.4123,
      "step": 14596
    },
    {
      "epoch": 5.651180797522261,
      "grad_norm": 16.9323787689209,
      "learning_rate": 4.832021336086377e-06,
      "loss": 1.0253,
      "step": 14597
    },
    {
      "epoch": 5.651567944250871,
      "grad_norm": 63.96073532104492,
      "learning_rate": 4.831591173054588e-06,
      "loss": 2.3791,
      "step": 14598
    },
    {
      "epoch": 5.651955090979481,
      "grad_norm": 29.167911529541016,
      "learning_rate": 4.831161010022799e-06,
      "loss": 1.7001,
      "step": 14599
    },
    {
      "epoch": 5.652342237708091,
      "grad_norm": 106.12208557128906,
      "learning_rate": 4.83073084699101e-06,
      "loss": 1.8615,
      "step": 14600
    },
    {
      "epoch": 5.652729384436702,
      "grad_norm": 5.457708358764648,
      "learning_rate": 4.830300683959221e-06,
      "loss": 0.1455,
      "step": 14601
    },
    {
      "epoch": 5.6531165311653115,
      "grad_norm": 46.68846893310547,
      "learning_rate": 4.829870520927432e-06,
      "loss": 0.3111,
      "step": 14602
    },
    {
      "epoch": 5.653503677893922,
      "grad_norm": 43.75100326538086,
      "learning_rate": 4.829440357895643e-06,
      "loss": 1.1314,
      "step": 14603
    },
    {
      "epoch": 5.653890824622532,
      "grad_norm": 30.161312103271484,
      "learning_rate": 4.829010194863854e-06,
      "loss": 0.8521,
      "step": 14604
    },
    {
      "epoch": 5.654277971351142,
      "grad_norm": 58.670623779296875,
      "learning_rate": 4.828580031832065e-06,
      "loss": 0.307,
      "step": 14605
    },
    {
      "epoch": 5.654665118079752,
      "grad_norm": 32.03254699707031,
      "learning_rate": 4.8281498688002755e-06,
      "loss": 2.6139,
      "step": 14606
    },
    {
      "epoch": 5.655052264808362,
      "grad_norm": 43.97518539428711,
      "learning_rate": 4.827719705768487e-06,
      "loss": 0.8909,
      "step": 14607
    },
    {
      "epoch": 5.655439411536973,
      "grad_norm": 48.82219314575195,
      "learning_rate": 4.8272895427366975e-06,
      "loss": 2.2273,
      "step": 14608
    },
    {
      "epoch": 5.6558265582655824,
      "grad_norm": 0.5317537784576416,
      "learning_rate": 4.826859379704908e-06,
      "loss": 0.0157,
      "step": 14609
    },
    {
      "epoch": 5.656213704994193,
      "grad_norm": 1.4151391983032227,
      "learning_rate": 4.8264292166731195e-06,
      "loss": 0.0234,
      "step": 14610
    },
    {
      "epoch": 5.656600851722803,
      "grad_norm": 37.29973602294922,
      "learning_rate": 4.82599905364133e-06,
      "loss": 0.904,
      "step": 14611
    },
    {
      "epoch": 5.656987998451413,
      "grad_norm": 7.059274196624756,
      "learning_rate": 4.8255688906095415e-06,
      "loss": 0.1479,
      "step": 14612
    },
    {
      "epoch": 5.657375145180024,
      "grad_norm": 16.46436309814453,
      "learning_rate": 4.825138727577752e-06,
      "loss": 0.1659,
      "step": 14613
    },
    {
      "epoch": 5.657762291908633,
      "grad_norm": 80.5557861328125,
      "learning_rate": 4.8247085645459634e-06,
      "loss": 2.6427,
      "step": 14614
    },
    {
      "epoch": 5.658149438637244,
      "grad_norm": 60.457035064697266,
      "learning_rate": 4.824278401514174e-06,
      "loss": 2.0576,
      "step": 14615
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 16.38861083984375,
      "learning_rate": 4.823848238482385e-06,
      "loss": 1.4965,
      "step": 14616
    },
    {
      "epoch": 5.658923732094464,
      "grad_norm": 2.9014618396759033,
      "learning_rate": 4.823418075450596e-06,
      "loss": 0.1444,
      "step": 14617
    },
    {
      "epoch": 5.659310878823074,
      "grad_norm": 40.94693374633789,
      "learning_rate": 4.822987912418807e-06,
      "loss": 0.4197,
      "step": 14618
    },
    {
      "epoch": 5.659698025551684,
      "grad_norm": 35.832027435302734,
      "learning_rate": 4.822557749387018e-06,
      "loss": 1.1343,
      "step": 14619
    },
    {
      "epoch": 5.6600851722802945,
      "grad_norm": 34.758811950683594,
      "learning_rate": 4.822127586355229e-06,
      "loss": 0.5219,
      "step": 14620
    },
    {
      "epoch": 5.660472319008904,
      "grad_norm": 111.07298278808594,
      "learning_rate": 4.82169742332344e-06,
      "loss": 1.0817,
      "step": 14621
    },
    {
      "epoch": 5.660859465737515,
      "grad_norm": 35.206520080566406,
      "learning_rate": 4.821267260291651e-06,
      "loss": 0.7902,
      "step": 14622
    },
    {
      "epoch": 5.661246612466124,
      "grad_norm": 39.39356231689453,
      "learning_rate": 4.820837097259862e-06,
      "loss": 1.0174,
      "step": 14623
    },
    {
      "epoch": 5.661633759194735,
      "grad_norm": 32.78345489501953,
      "learning_rate": 4.8204069342280725e-06,
      "loss": 0.9145,
      "step": 14624
    },
    {
      "epoch": 5.662020905923345,
      "grad_norm": 5.829682350158691,
      "learning_rate": 4.819976771196284e-06,
      "loss": 0.1403,
      "step": 14625
    },
    {
      "epoch": 5.662408052651955,
      "grad_norm": 101.6373062133789,
      "learning_rate": 4.8195466081644945e-06,
      "loss": 1.1047,
      "step": 14626
    },
    {
      "epoch": 5.662795199380565,
      "grad_norm": 61.30192184448242,
      "learning_rate": 4.819116445132705e-06,
      "loss": 1.0637,
      "step": 14627
    },
    {
      "epoch": 5.663182346109175,
      "grad_norm": 7.189159393310547,
      "learning_rate": 4.8186862821009165e-06,
      "loss": 0.1706,
      "step": 14628
    },
    {
      "epoch": 5.6635694928377855,
      "grad_norm": 60.974853515625,
      "learning_rate": 4.818256119069127e-06,
      "loss": 0.9771,
      "step": 14629
    },
    {
      "epoch": 5.663956639566395,
      "grad_norm": 5.797574520111084,
      "learning_rate": 4.8178259560373385e-06,
      "loss": 0.1873,
      "step": 14630
    },
    {
      "epoch": 5.664343786295006,
      "grad_norm": 48.50048065185547,
      "learning_rate": 4.817395793005549e-06,
      "loss": 1.1844,
      "step": 14631
    },
    {
      "epoch": 5.664730933023616,
      "grad_norm": 9.407965660095215,
      "learning_rate": 4.8169656299737605e-06,
      "loss": 0.2463,
      "step": 14632
    },
    {
      "epoch": 5.665118079752226,
      "grad_norm": 141.59658813476562,
      "learning_rate": 4.816535466941971e-06,
      "loss": 2.2977,
      "step": 14633
    },
    {
      "epoch": 5.665505226480836,
      "grad_norm": 5.652577877044678,
      "learning_rate": 4.8161053039101824e-06,
      "loss": 0.2619,
      "step": 14634
    },
    {
      "epoch": 5.665892373209447,
      "grad_norm": 22.866601943969727,
      "learning_rate": 4.815675140878393e-06,
      "loss": 1.5135,
      "step": 14635
    },
    {
      "epoch": 5.6662795199380565,
      "grad_norm": 25.36748695373535,
      "learning_rate": 4.815244977846604e-06,
      "loss": 0.9714,
      "step": 14636
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 28.48677635192871,
      "learning_rate": 4.814814814814815e-06,
      "loss": 1.7154,
      "step": 14637
    },
    {
      "epoch": 5.667053813395277,
      "grad_norm": 47.759033203125,
      "learning_rate": 4.814384651783026e-06,
      "loss": 0.2232,
      "step": 14638
    },
    {
      "epoch": 5.667440960123887,
      "grad_norm": 72.9565200805664,
      "learning_rate": 4.813954488751237e-06,
      "loss": 0.6923,
      "step": 14639
    },
    {
      "epoch": 5.667828106852497,
      "grad_norm": 17.125349044799805,
      "learning_rate": 4.813524325719448e-06,
      "loss": 1.6367,
      "step": 14640
    },
    {
      "epoch": 5.668215253581107,
      "grad_norm": 65.25037384033203,
      "learning_rate": 4.813094162687659e-06,
      "loss": 2.2649,
      "step": 14641
    },
    {
      "epoch": 5.668602400309718,
      "grad_norm": 113.08335876464844,
      "learning_rate": 4.8126639996558695e-06,
      "loss": 0.859,
      "step": 14642
    },
    {
      "epoch": 5.668989547038327,
      "grad_norm": 39.75004577636719,
      "learning_rate": 4.812233836624081e-06,
      "loss": 1.2464,
      "step": 14643
    },
    {
      "epoch": 5.669376693766938,
      "grad_norm": 67.004638671875,
      "learning_rate": 4.8118036735922915e-06,
      "loss": 1.094,
      "step": 14644
    },
    {
      "epoch": 5.6697638404955475,
      "grad_norm": 91.12471771240234,
      "learning_rate": 4.811373510560503e-06,
      "loss": 1.3685,
      "step": 14645
    },
    {
      "epoch": 5.670150987224158,
      "grad_norm": 84.85847473144531,
      "learning_rate": 4.8109433475287135e-06,
      "loss": 1.7226,
      "step": 14646
    },
    {
      "epoch": 5.670538133952768,
      "grad_norm": 64.56509399414062,
      "learning_rate": 4.810513184496925e-06,
      "loss": 1.3883,
      "step": 14647
    },
    {
      "epoch": 5.670925280681378,
      "grad_norm": 73.67525482177734,
      "learning_rate": 4.8100830214651355e-06,
      "loss": 2.4746,
      "step": 14648
    },
    {
      "epoch": 5.671312427409989,
      "grad_norm": 25.345468521118164,
      "learning_rate": 4.809652858433347e-06,
      "loss": 0.4354,
      "step": 14649
    },
    {
      "epoch": 5.671699574138598,
      "grad_norm": 53.782405853271484,
      "learning_rate": 4.8092226954015575e-06,
      "loss": 0.6909,
      "step": 14650
    },
    {
      "epoch": 5.672086720867209,
      "grad_norm": 39.07181167602539,
      "learning_rate": 4.808792532369769e-06,
      "loss": 0.9309,
      "step": 14651
    },
    {
      "epoch": 5.672473867595819,
      "grad_norm": 10.437606811523438,
      "learning_rate": 4.8083623693379794e-06,
      "loss": 0.3377,
      "step": 14652
    },
    {
      "epoch": 5.672861014324429,
      "grad_norm": 23.69228744506836,
      "learning_rate": 4.807932206306191e-06,
      "loss": 0.3902,
      "step": 14653
    },
    {
      "epoch": 5.673248161053039,
      "grad_norm": 24.7708683013916,
      "learning_rate": 4.8075020432744014e-06,
      "loss": 1.5906,
      "step": 14654
    },
    {
      "epoch": 5.673635307781649,
      "grad_norm": 64.43041229248047,
      "learning_rate": 4.807071880242613e-06,
      "loss": 1.7584,
      "step": 14655
    },
    {
      "epoch": 5.6740224545102595,
      "grad_norm": 122.41912078857422,
      "learning_rate": 4.806641717210823e-06,
      "loss": 1.2371,
      "step": 14656
    },
    {
      "epoch": 5.674409601238869,
      "grad_norm": 71.0667724609375,
      "learning_rate": 4.806211554179034e-06,
      "loss": 0.6676,
      "step": 14657
    },
    {
      "epoch": 5.67479674796748,
      "grad_norm": 58.12222671508789,
      "learning_rate": 4.805781391147245e-06,
      "loss": 1.7569,
      "step": 14658
    },
    {
      "epoch": 5.67518389469609,
      "grad_norm": 95.65875244140625,
      "learning_rate": 4.805351228115456e-06,
      "loss": 1.6387,
      "step": 14659
    },
    {
      "epoch": 5.6755710414247,
      "grad_norm": 35.86556625366211,
      "learning_rate": 4.8049210650836665e-06,
      "loss": 0.8844,
      "step": 14660
    },
    {
      "epoch": 5.67595818815331,
      "grad_norm": 13.448065757751465,
      "learning_rate": 4.804490902051878e-06,
      "loss": 0.6627,
      "step": 14661
    },
    {
      "epoch": 5.67634533488192,
      "grad_norm": 51.18497085571289,
      "learning_rate": 4.8040607390200885e-06,
      "loss": 0.6994,
      "step": 14662
    },
    {
      "epoch": 5.6767324816105305,
      "grad_norm": 36.045204162597656,
      "learning_rate": 4.8036305759883e-06,
      "loss": 2.8289,
      "step": 14663
    },
    {
      "epoch": 5.67711962833914,
      "grad_norm": 3.7174315452575684,
      "learning_rate": 4.8032004129565105e-06,
      "loss": 0.168,
      "step": 14664
    },
    {
      "epoch": 5.677506775067751,
      "grad_norm": 37.07516860961914,
      "learning_rate": 4.802770249924722e-06,
      "loss": 0.9892,
      "step": 14665
    },
    {
      "epoch": 5.677893921796361,
      "grad_norm": 15.004389762878418,
      "learning_rate": 4.8023400868929325e-06,
      "loss": 0.1635,
      "step": 14666
    },
    {
      "epoch": 5.678281068524971,
      "grad_norm": 185.92123413085938,
      "learning_rate": 4.801909923861144e-06,
      "loss": 1.8025,
      "step": 14667
    },
    {
      "epoch": 5.678668215253581,
      "grad_norm": 79.33993530273438,
      "learning_rate": 4.8014797608293545e-06,
      "loss": 0.4899,
      "step": 14668
    },
    {
      "epoch": 5.679055361982192,
      "grad_norm": 80.72022247314453,
      "learning_rate": 4.801049597797566e-06,
      "loss": 1.7453,
      "step": 14669
    },
    {
      "epoch": 5.679442508710801,
      "grad_norm": 9.104537010192871,
      "learning_rate": 4.8006194347657765e-06,
      "loss": 0.5105,
      "step": 14670
    },
    {
      "epoch": 5.679829655439412,
      "grad_norm": 26.312963485717773,
      "learning_rate": 4.800189271733988e-06,
      "loss": 1.4081,
      "step": 14671
    },
    {
      "epoch": 5.6802168021680215,
      "grad_norm": 17.347036361694336,
      "learning_rate": 4.7997591087021984e-06,
      "loss": 1.5431,
      "step": 14672
    },
    {
      "epoch": 5.680603948896632,
      "grad_norm": 24.93270492553711,
      "learning_rate": 4.79932894567041e-06,
      "loss": 0.6614,
      "step": 14673
    },
    {
      "epoch": 5.680991095625242,
      "grad_norm": 78.75345611572266,
      "learning_rate": 4.7988987826386204e-06,
      "loss": 3.4421,
      "step": 14674
    },
    {
      "epoch": 5.681378242353852,
      "grad_norm": 105.18699645996094,
      "learning_rate": 4.798468619606831e-06,
      "loss": 1.3479,
      "step": 14675
    },
    {
      "epoch": 5.681765389082463,
      "grad_norm": 13.094205856323242,
      "learning_rate": 4.798038456575042e-06,
      "loss": 0.3833,
      "step": 14676
    },
    {
      "epoch": 5.682152535811072,
      "grad_norm": 3.9619460105895996,
      "learning_rate": 4.797608293543253e-06,
      "loss": 0.1865,
      "step": 14677
    },
    {
      "epoch": 5.682539682539683,
      "grad_norm": 3.4511818885803223,
      "learning_rate": 4.7971781305114636e-06,
      "loss": 0.095,
      "step": 14678
    },
    {
      "epoch": 5.682926829268292,
      "grad_norm": 38.421329498291016,
      "learning_rate": 4.796747967479675e-06,
      "loss": 2.8741,
      "step": 14679
    },
    {
      "epoch": 5.683313975996903,
      "grad_norm": 5.604306221008301,
      "learning_rate": 4.796317804447886e-06,
      "loss": 0.1228,
      "step": 14680
    },
    {
      "epoch": 5.6837011227255125,
      "grad_norm": 60.485374450683594,
      "learning_rate": 4.795887641416097e-06,
      "loss": 0.8965,
      "step": 14681
    },
    {
      "epoch": 5.684088269454123,
      "grad_norm": 4.245169162750244,
      "learning_rate": 4.795457478384308e-06,
      "loss": 0.1691,
      "step": 14682
    },
    {
      "epoch": 5.6844754161827336,
      "grad_norm": 113.37042236328125,
      "learning_rate": 4.795027315352519e-06,
      "loss": 1.984,
      "step": 14683
    },
    {
      "epoch": 5.684862562911343,
      "grad_norm": 2.6944432258605957,
      "learning_rate": 4.79459715232073e-06,
      "loss": 0.081,
      "step": 14684
    },
    {
      "epoch": 5.685249709639954,
      "grad_norm": 49.87393569946289,
      "learning_rate": 4.794166989288941e-06,
      "loss": 0.9203,
      "step": 14685
    },
    {
      "epoch": 5.685636856368563,
      "grad_norm": 55.116058349609375,
      "learning_rate": 4.793736826257152e-06,
      "loss": 1.7845,
      "step": 14686
    },
    {
      "epoch": 5.686024003097174,
      "grad_norm": 44.97768783569336,
      "learning_rate": 4.793306663225363e-06,
      "loss": 1.2024,
      "step": 14687
    },
    {
      "epoch": 5.686411149825784,
      "grad_norm": 5.624671459197998,
      "learning_rate": 4.792876500193574e-06,
      "loss": 0.3663,
      "step": 14688
    },
    {
      "epoch": 5.686798296554394,
      "grad_norm": 27.57273292541504,
      "learning_rate": 4.792446337161785e-06,
      "loss": 0.7986,
      "step": 14689
    },
    {
      "epoch": 5.6871854432830045,
      "grad_norm": 11.5851469039917,
      "learning_rate": 4.7920161741299955e-06,
      "loss": 0.3788,
      "step": 14690
    },
    {
      "epoch": 5.687572590011614,
      "grad_norm": 1.8346071243286133,
      "learning_rate": 4.791586011098207e-06,
      "loss": 0.0727,
      "step": 14691
    },
    {
      "epoch": 5.687959736740225,
      "grad_norm": 62.120582580566406,
      "learning_rate": 4.7911558480664174e-06,
      "loss": 1.1541,
      "step": 14692
    },
    {
      "epoch": 5.688346883468835,
      "grad_norm": 14.394082069396973,
      "learning_rate": 4.790725685034628e-06,
      "loss": 0.227,
      "step": 14693
    },
    {
      "epoch": 5.688734030197445,
      "grad_norm": 20.48793601989746,
      "learning_rate": 4.7902955220028394e-06,
      "loss": 0.7191,
      "step": 14694
    },
    {
      "epoch": 5.689121176926055,
      "grad_norm": 37.439727783203125,
      "learning_rate": 4.78986535897105e-06,
      "loss": 2.213,
      "step": 14695
    },
    {
      "epoch": 5.689508323654665,
      "grad_norm": 17.189619064331055,
      "learning_rate": 4.789435195939261e-06,
      "loss": 0.3055,
      "step": 14696
    },
    {
      "epoch": 5.689895470383275,
      "grad_norm": 18.55549430847168,
      "learning_rate": 4.789005032907472e-06,
      "loss": 1.7073,
      "step": 14697
    },
    {
      "epoch": 5.690282617111885,
      "grad_norm": 35.38172149658203,
      "learning_rate": 4.788574869875683e-06,
      "loss": 1.3556,
      "step": 14698
    },
    {
      "epoch": 5.6906697638404955,
      "grad_norm": 4.905681133270264,
      "learning_rate": 4.788144706843894e-06,
      "loss": 0.2789,
      "step": 14699
    },
    {
      "epoch": 5.691056910569106,
      "grad_norm": 21.12342071533203,
      "learning_rate": 4.787714543812105e-06,
      "loss": 0.5084,
      "step": 14700
    },
    {
      "epoch": 5.691444057297716,
      "grad_norm": 1.4945518970489502,
      "learning_rate": 4.787284380780316e-06,
      "loss": 0.0594,
      "step": 14701
    },
    {
      "epoch": 5.691831204026326,
      "grad_norm": 22.425682067871094,
      "learning_rate": 4.786854217748527e-06,
      "loss": 1.104,
      "step": 14702
    },
    {
      "epoch": 5.692218350754936,
      "grad_norm": 73.11011505126953,
      "learning_rate": 4.786424054716738e-06,
      "loss": 0.5371,
      "step": 14703
    },
    {
      "epoch": 5.692605497483546,
      "grad_norm": 89.83921813964844,
      "learning_rate": 4.785993891684949e-06,
      "loss": 2.5938,
      "step": 14704
    },
    {
      "epoch": 5.692992644212157,
      "grad_norm": 18.89876937866211,
      "learning_rate": 4.78556372865316e-06,
      "loss": 1.7021,
      "step": 14705
    },
    {
      "epoch": 5.693379790940766,
      "grad_norm": 80.65055084228516,
      "learning_rate": 4.785133565621371e-06,
      "loss": 1.2282,
      "step": 14706
    },
    {
      "epoch": 5.693766937669377,
      "grad_norm": 22.08989715576172,
      "learning_rate": 4.784703402589582e-06,
      "loss": 1.1317,
      "step": 14707
    },
    {
      "epoch": 5.6941540843979865,
      "grad_norm": 19.25089454650879,
      "learning_rate": 4.7842732395577925e-06,
      "loss": 0.2443,
      "step": 14708
    },
    {
      "epoch": 5.694541231126597,
      "grad_norm": 37.985923767089844,
      "learning_rate": 4.783843076526004e-06,
      "loss": 2.248,
      "step": 14709
    },
    {
      "epoch": 5.694928377855208,
      "grad_norm": 235.52493286132812,
      "learning_rate": 4.7834129134942145e-06,
      "loss": 3.4192,
      "step": 14710
    },
    {
      "epoch": 5.695315524583817,
      "grad_norm": 63.27762222290039,
      "learning_rate": 4.782982750462425e-06,
      "loss": 1.8307,
      "step": 14711
    },
    {
      "epoch": 5.695702671312428,
      "grad_norm": 39.70905303955078,
      "learning_rate": 4.7825525874306364e-06,
      "loss": 1.265,
      "step": 14712
    },
    {
      "epoch": 5.696089818041037,
      "grad_norm": 50.45331954956055,
      "learning_rate": 4.782122424398847e-06,
      "loss": 1.1818,
      "step": 14713
    },
    {
      "epoch": 5.696476964769648,
      "grad_norm": 51.02082061767578,
      "learning_rate": 4.7816922613670584e-06,
      "loss": 3.7967,
      "step": 14714
    },
    {
      "epoch": 5.696864111498257,
      "grad_norm": 32.811981201171875,
      "learning_rate": 4.781262098335269e-06,
      "loss": 0.6086,
      "step": 14715
    },
    {
      "epoch": 5.697251258226868,
      "grad_norm": 43.542091369628906,
      "learning_rate": 4.78083193530348e-06,
      "loss": 1.2962,
      "step": 14716
    },
    {
      "epoch": 5.6976384049554785,
      "grad_norm": 33.86239242553711,
      "learning_rate": 4.780401772271691e-06,
      "loss": 0.6115,
      "step": 14717
    },
    {
      "epoch": 5.698025551684088,
      "grad_norm": 49.95064163208008,
      "learning_rate": 4.779971609239902e-06,
      "loss": 1.8156,
      "step": 14718
    },
    {
      "epoch": 5.698412698412699,
      "grad_norm": 37.67545700073242,
      "learning_rate": 4.779541446208113e-06,
      "loss": 1.9817,
      "step": 14719
    },
    {
      "epoch": 5.698799845141308,
      "grad_norm": 67.57525634765625,
      "learning_rate": 4.779111283176324e-06,
      "loss": 2.705,
      "step": 14720
    },
    {
      "epoch": 5.699186991869919,
      "grad_norm": 24.274620056152344,
      "learning_rate": 4.778681120144536e-06,
      "loss": 2.6537,
      "step": 14721
    },
    {
      "epoch": 5.699574138598528,
      "grad_norm": 61.60432815551758,
      "learning_rate": 4.778250957112746e-06,
      "loss": 1.4713,
      "step": 14722
    },
    {
      "epoch": 5.699961285327139,
      "grad_norm": 54.47706985473633,
      "learning_rate": 4.777820794080957e-06,
      "loss": 2.1849,
      "step": 14723
    },
    {
      "epoch": 5.700348432055749,
      "grad_norm": 67.95533752441406,
      "learning_rate": 4.777390631049168e-06,
      "loss": 0.8033,
      "step": 14724
    },
    {
      "epoch": 5.700735578784359,
      "grad_norm": 24.595949172973633,
      "learning_rate": 4.776960468017379e-06,
      "loss": 0.6888,
      "step": 14725
    },
    {
      "epoch": 5.7011227255129695,
      "grad_norm": 44.563297271728516,
      "learning_rate": 4.7765303049855895e-06,
      "loss": 1.1557,
      "step": 14726
    },
    {
      "epoch": 5.70150987224158,
      "grad_norm": 57.08024215698242,
      "learning_rate": 4.776100141953801e-06,
      "loss": 1.3652,
      "step": 14727
    },
    {
      "epoch": 5.70189701897019,
      "grad_norm": 48.70606994628906,
      "learning_rate": 4.7756699789220115e-06,
      "loss": 0.8561,
      "step": 14728
    },
    {
      "epoch": 5.7022841656988,
      "grad_norm": 83.19473266601562,
      "learning_rate": 4.775239815890223e-06,
      "loss": 3.0487,
      "step": 14729
    },
    {
      "epoch": 5.70267131242741,
      "grad_norm": 6.943052291870117,
      "learning_rate": 4.7748096528584335e-06,
      "loss": 0.3973,
      "step": 14730
    },
    {
      "epoch": 5.70305845915602,
      "grad_norm": 31.0303955078125,
      "learning_rate": 4.774379489826645e-06,
      "loss": 1.5437,
      "step": 14731
    },
    {
      "epoch": 5.70344560588463,
      "grad_norm": 18.486026763916016,
      "learning_rate": 4.7739493267948554e-06,
      "loss": 0.4261,
      "step": 14732
    },
    {
      "epoch": 5.70383275261324,
      "grad_norm": 6.7727742195129395,
      "learning_rate": 4.773519163763067e-06,
      "loss": 0.3292,
      "step": 14733
    },
    {
      "epoch": 5.704219899341851,
      "grad_norm": 50.97688293457031,
      "learning_rate": 4.7730890007312774e-06,
      "loss": 1.4029,
      "step": 14734
    },
    {
      "epoch": 5.7046070460704605,
      "grad_norm": 35.958370208740234,
      "learning_rate": 4.772658837699489e-06,
      "loss": 1.2582,
      "step": 14735
    },
    {
      "epoch": 5.704994192799071,
      "grad_norm": 75.11524963378906,
      "learning_rate": 4.772228674667699e-06,
      "loss": 1.1152,
      "step": 14736
    },
    {
      "epoch": 5.705381339527681,
      "grad_norm": 16.03514289855957,
      "learning_rate": 4.771798511635911e-06,
      "loss": 0.2383,
      "step": 14737
    },
    {
      "epoch": 5.705768486256291,
      "grad_norm": 152.34307861328125,
      "learning_rate": 4.771368348604121e-06,
      "loss": 3.7421,
      "step": 14738
    },
    {
      "epoch": 5.706155632984901,
      "grad_norm": 63.380279541015625,
      "learning_rate": 4.770938185572333e-06,
      "loss": 2.511,
      "step": 14739
    },
    {
      "epoch": 5.706542779713511,
      "grad_norm": 116.9862060546875,
      "learning_rate": 4.770508022540543e-06,
      "loss": 1.705,
      "step": 14740
    },
    {
      "epoch": 5.706929926442122,
      "grad_norm": 33.90055465698242,
      "learning_rate": 4.770077859508754e-06,
      "loss": 0.9275,
      "step": 14741
    },
    {
      "epoch": 5.7073170731707314,
      "grad_norm": 27.84505271911621,
      "learning_rate": 4.769647696476965e-06,
      "loss": 1.1211,
      "step": 14742
    },
    {
      "epoch": 5.707704219899342,
      "grad_norm": 37.87189865112305,
      "learning_rate": 4.769217533445176e-06,
      "loss": 0.922,
      "step": 14743
    },
    {
      "epoch": 5.7080913666279525,
      "grad_norm": 0.5149078965187073,
      "learning_rate": 4.7687873704133865e-06,
      "loss": 0.0153,
      "step": 14744
    },
    {
      "epoch": 5.708478513356562,
      "grad_norm": 3.827910900115967,
      "learning_rate": 4.768357207381598e-06,
      "loss": 0.1671,
      "step": 14745
    },
    {
      "epoch": 5.708865660085173,
      "grad_norm": 175.98617553710938,
      "learning_rate": 4.7679270443498085e-06,
      "loss": 0.7803,
      "step": 14746
    },
    {
      "epoch": 5.709252806813782,
      "grad_norm": 26.19346046447754,
      "learning_rate": 4.76749688131802e-06,
      "loss": 3.7255,
      "step": 14747
    },
    {
      "epoch": 5.709639953542393,
      "grad_norm": 28.858272552490234,
      "learning_rate": 4.7670667182862305e-06,
      "loss": 0.9664,
      "step": 14748
    },
    {
      "epoch": 5.710027100271002,
      "grad_norm": 31.168718338012695,
      "learning_rate": 4.766636555254442e-06,
      "loss": 2.3037,
      "step": 14749
    },
    {
      "epoch": 5.710414246999613,
      "grad_norm": 4.247780799865723,
      "learning_rate": 4.7662063922226525e-06,
      "loss": 0.1906,
      "step": 14750
    },
    {
      "epoch": 5.710801393728223,
      "grad_norm": 77.02337646484375,
      "learning_rate": 4.765776229190864e-06,
      "loss": 1.0932,
      "step": 14751
    },
    {
      "epoch": 5.711188540456833,
      "grad_norm": 29.08965492248535,
      "learning_rate": 4.7653460661590744e-06,
      "loss": 0.4045,
      "step": 14752
    },
    {
      "epoch": 5.7115756871854435,
      "grad_norm": 21.23710823059082,
      "learning_rate": 4.764915903127286e-06,
      "loss": 1.2438,
      "step": 14753
    },
    {
      "epoch": 5.711962833914053,
      "grad_norm": 65.98147583007812,
      "learning_rate": 4.764485740095496e-06,
      "loss": 1.2992,
      "step": 14754
    },
    {
      "epoch": 5.712349980642664,
      "grad_norm": 29.0877742767334,
      "learning_rate": 4.764055577063708e-06,
      "loss": 1.5481,
      "step": 14755
    },
    {
      "epoch": 5.712737127371273,
      "grad_norm": 28.775001525878906,
      "learning_rate": 4.763625414031918e-06,
      "loss": 0.9818,
      "step": 14756
    },
    {
      "epoch": 5.713124274099884,
      "grad_norm": 67.11593627929688,
      "learning_rate": 4.76319525100013e-06,
      "loss": 0.915,
      "step": 14757
    },
    {
      "epoch": 5.713511420828494,
      "grad_norm": 39.950931549072266,
      "learning_rate": 4.76276508796834e-06,
      "loss": 2.029,
      "step": 14758
    },
    {
      "epoch": 5.713898567557104,
      "grad_norm": 31.0766658782959,
      "learning_rate": 4.762334924936551e-06,
      "loss": 2.7237,
      "step": 14759
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 8.121929168701172,
      "learning_rate": 4.761904761904762e-06,
      "loss": 0.216,
      "step": 14760
    },
    {
      "epoch": 5.714672861014325,
      "grad_norm": 31.088468551635742,
      "learning_rate": 4.761474598872973e-06,
      "loss": 1.2173,
      "step": 14761
    },
    {
      "epoch": 5.7150600077429345,
      "grad_norm": 30.091224670410156,
      "learning_rate": 4.761044435841184e-06,
      "loss": 0.2796,
      "step": 14762
    },
    {
      "epoch": 5.715447154471545,
      "grad_norm": 19.288408279418945,
      "learning_rate": 4.760614272809395e-06,
      "loss": 0.2487,
      "step": 14763
    },
    {
      "epoch": 5.715834301200155,
      "grad_norm": 50.90996170043945,
      "learning_rate": 4.760184109777606e-06,
      "loss": 1.1322,
      "step": 14764
    },
    {
      "epoch": 5.716221447928765,
      "grad_norm": 147.7987060546875,
      "learning_rate": 4.759753946745817e-06,
      "loss": 0.8847,
      "step": 14765
    },
    {
      "epoch": 5.716608594657375,
      "grad_norm": 55.395992279052734,
      "learning_rate": 4.759323783714028e-06,
      "loss": 1.8334,
      "step": 14766
    },
    {
      "epoch": 5.716995741385985,
      "grad_norm": 31.36332893371582,
      "learning_rate": 4.758893620682239e-06,
      "loss": 0.4139,
      "step": 14767
    },
    {
      "epoch": 5.717382888114596,
      "grad_norm": 44.24380874633789,
      "learning_rate": 4.75846345765045e-06,
      "loss": 1.4896,
      "step": 14768
    },
    {
      "epoch": 5.7177700348432055,
      "grad_norm": 40.571006774902344,
      "learning_rate": 4.758033294618661e-06,
      "loss": 1.5023,
      "step": 14769
    },
    {
      "epoch": 5.718157181571816,
      "grad_norm": 19.50487518310547,
      "learning_rate": 4.757603131586872e-06,
      "loss": 0.8125,
      "step": 14770
    },
    {
      "epoch": 5.718544328300426,
      "grad_norm": 73.7352523803711,
      "learning_rate": 4.757172968555083e-06,
      "loss": 0.9498,
      "step": 14771
    },
    {
      "epoch": 5.718931475029036,
      "grad_norm": 23.745311737060547,
      "learning_rate": 4.756742805523294e-06,
      "loss": 1.7229,
      "step": 14772
    },
    {
      "epoch": 5.719318621757646,
      "grad_norm": 17.287622451782227,
      "learning_rate": 4.756312642491505e-06,
      "loss": 1.1904,
      "step": 14773
    },
    {
      "epoch": 5.719705768486256,
      "grad_norm": 107.78495788574219,
      "learning_rate": 4.755882479459715e-06,
      "loss": 0.9867,
      "step": 14774
    },
    {
      "epoch": 5.720092915214867,
      "grad_norm": 77.1366958618164,
      "learning_rate": 4.755452316427927e-06,
      "loss": 0.8322,
      "step": 14775
    },
    {
      "epoch": 5.720480061943476,
      "grad_norm": 52.9650993347168,
      "learning_rate": 4.755022153396137e-06,
      "loss": 1.1135,
      "step": 14776
    },
    {
      "epoch": 5.720867208672087,
      "grad_norm": 8.091364860534668,
      "learning_rate": 4.754591990364348e-06,
      "loss": 0.4279,
      "step": 14777
    },
    {
      "epoch": 5.7212543554006965,
      "grad_norm": 53.90244674682617,
      "learning_rate": 4.754161827332559e-06,
      "loss": 0.4882,
      "step": 14778
    },
    {
      "epoch": 5.721641502129307,
      "grad_norm": 63.469573974609375,
      "learning_rate": 4.75373166430077e-06,
      "loss": 0.8204,
      "step": 14779
    },
    {
      "epoch": 5.7220286488579175,
      "grad_norm": 52.1799430847168,
      "learning_rate": 4.753301501268981e-06,
      "loss": 2.1345,
      "step": 14780
    },
    {
      "epoch": 5.722415795586527,
      "grad_norm": 135.43807983398438,
      "learning_rate": 4.752871338237192e-06,
      "loss": 0.5096,
      "step": 14781
    },
    {
      "epoch": 5.722802942315138,
      "grad_norm": 48.499298095703125,
      "learning_rate": 4.752441175205403e-06,
      "loss": 0.7885,
      "step": 14782
    },
    {
      "epoch": 5.723190089043747,
      "grad_norm": 6.268040657043457,
      "learning_rate": 4.752011012173614e-06,
      "loss": 0.1322,
      "step": 14783
    },
    {
      "epoch": 5.723577235772358,
      "grad_norm": 31.373361587524414,
      "learning_rate": 4.751580849141825e-06,
      "loss": 1.5616,
      "step": 14784
    },
    {
      "epoch": 5.723964382500968,
      "grad_norm": 4.541751384735107,
      "learning_rate": 4.751150686110036e-06,
      "loss": 0.2296,
      "step": 14785
    },
    {
      "epoch": 5.724351529229578,
      "grad_norm": 68.05211639404297,
      "learning_rate": 4.750720523078247e-06,
      "loss": 0.9155,
      "step": 14786
    },
    {
      "epoch": 5.724738675958188,
      "grad_norm": 40.517215728759766,
      "learning_rate": 4.750290360046458e-06,
      "loss": 0.5328,
      "step": 14787
    },
    {
      "epoch": 5.725125822686798,
      "grad_norm": 26.287309646606445,
      "learning_rate": 4.749860197014669e-06,
      "loss": 0.5634,
      "step": 14788
    },
    {
      "epoch": 5.7255129694154085,
      "grad_norm": 12.081223487854004,
      "learning_rate": 4.74943003398288e-06,
      "loss": 0.5425,
      "step": 14789
    },
    {
      "epoch": 5.725900116144018,
      "grad_norm": 74.40812683105469,
      "learning_rate": 4.748999870951091e-06,
      "loss": 1.7262,
      "step": 14790
    },
    {
      "epoch": 5.726287262872629,
      "grad_norm": 66.3665542602539,
      "learning_rate": 4.748569707919302e-06,
      "loss": 2.6432,
      "step": 14791
    },
    {
      "epoch": 5.726674409601239,
      "grad_norm": 118.28599548339844,
      "learning_rate": 4.7481395448875124e-06,
      "loss": 1.2707,
      "step": 14792
    },
    {
      "epoch": 5.727061556329849,
      "grad_norm": 46.9463005065918,
      "learning_rate": 4.747709381855724e-06,
      "loss": 1.1458,
      "step": 14793
    },
    {
      "epoch": 5.727448703058459,
      "grad_norm": 25.70009422302246,
      "learning_rate": 4.747279218823934e-06,
      "loss": 1.7229,
      "step": 14794
    },
    {
      "epoch": 5.727835849787069,
      "grad_norm": 79.69558715820312,
      "learning_rate": 4.746849055792145e-06,
      "loss": 2.0007,
      "step": 14795
    },
    {
      "epoch": 5.7282229965156795,
      "grad_norm": 55.03704071044922,
      "learning_rate": 4.746418892760356e-06,
      "loss": 0.8279,
      "step": 14796
    },
    {
      "epoch": 5.72861014324429,
      "grad_norm": 70.95559692382812,
      "learning_rate": 4.745988729728567e-06,
      "loss": 1.2151,
      "step": 14797
    },
    {
      "epoch": 5.7289972899729,
      "grad_norm": 6.47360897064209,
      "learning_rate": 4.745558566696778e-06,
      "loss": 0.2798,
      "step": 14798
    },
    {
      "epoch": 5.72938443670151,
      "grad_norm": 15.27772331237793,
      "learning_rate": 4.745128403664989e-06,
      "loss": 0.3975,
      "step": 14799
    },
    {
      "epoch": 5.72977158343012,
      "grad_norm": 4.314396381378174,
      "learning_rate": 4.7446982406332e-06,
      "loss": 0.1213,
      "step": 14800
    },
    {
      "epoch": 5.73015873015873,
      "grad_norm": 2.2840631008148193,
      "learning_rate": 4.744268077601411e-06,
      "loss": 0.1048,
      "step": 14801
    },
    {
      "epoch": 5.730545876887341,
      "grad_norm": 36.94676971435547,
      "learning_rate": 4.743837914569622e-06,
      "loss": 1.7745,
      "step": 14802
    },
    {
      "epoch": 5.73093302361595,
      "grad_norm": 40.10542297363281,
      "learning_rate": 4.743407751537833e-06,
      "loss": 1.7542,
      "step": 14803
    },
    {
      "epoch": 5.731320170344561,
      "grad_norm": 106.26126098632812,
      "learning_rate": 4.742977588506044e-06,
      "loss": 0.8878,
      "step": 14804
    },
    {
      "epoch": 5.7317073170731705,
      "grad_norm": 20.34016990661621,
      "learning_rate": 4.742547425474256e-06,
      "loss": 0.5993,
      "step": 14805
    },
    {
      "epoch": 5.732094463801781,
      "grad_norm": 15.624794006347656,
      "learning_rate": 4.742117262442466e-06,
      "loss": 0.4744,
      "step": 14806
    },
    {
      "epoch": 5.732481610530391,
      "grad_norm": 45.19881057739258,
      "learning_rate": 4.741687099410677e-06,
      "loss": 1.6822,
      "step": 14807
    },
    {
      "epoch": 5.732868757259001,
      "grad_norm": 26.051271438598633,
      "learning_rate": 4.741256936378888e-06,
      "loss": 1.9766,
      "step": 14808
    },
    {
      "epoch": 5.733255903987612,
      "grad_norm": 33.60102844238281,
      "learning_rate": 4.740826773347099e-06,
      "loss": 0.8253,
      "step": 14809
    },
    {
      "epoch": 5.733643050716221,
      "grad_norm": 32.483238220214844,
      "learning_rate": 4.7403966103153094e-06,
      "loss": 3.3545,
      "step": 14810
    },
    {
      "epoch": 5.734030197444832,
      "grad_norm": 66.18797302246094,
      "learning_rate": 4.739966447283521e-06,
      "loss": 1.6044,
      "step": 14811
    },
    {
      "epoch": 5.734417344173441,
      "grad_norm": 22.40859031677246,
      "learning_rate": 4.7395362842517314e-06,
      "loss": 1.9275,
      "step": 14812
    },
    {
      "epoch": 5.734804490902052,
      "grad_norm": 8.385175704956055,
      "learning_rate": 4.739106121219943e-06,
      "loss": 0.2124,
      "step": 14813
    },
    {
      "epoch": 5.7351916376306615,
      "grad_norm": 112.2587661743164,
      "learning_rate": 4.738675958188153e-06,
      "loss": 2.7232,
      "step": 14814
    },
    {
      "epoch": 5.735578784359272,
      "grad_norm": 6.917173385620117,
      "learning_rate": 4.738245795156365e-06,
      "loss": 0.2874,
      "step": 14815
    },
    {
      "epoch": 5.7359659310878826,
      "grad_norm": 62.83986282348633,
      "learning_rate": 4.737815632124575e-06,
      "loss": 2.0019,
      "step": 14816
    },
    {
      "epoch": 5.736353077816492,
      "grad_norm": 168.1752471923828,
      "learning_rate": 4.737385469092787e-06,
      "loss": 0.9659,
      "step": 14817
    },
    {
      "epoch": 5.736740224545103,
      "grad_norm": 52.7586784362793,
      "learning_rate": 4.736955306060997e-06,
      "loss": 0.6336,
      "step": 14818
    },
    {
      "epoch": 5.737127371273713,
      "grad_norm": 28.26341438293457,
      "learning_rate": 4.736525143029209e-06,
      "loss": 1.7537,
      "step": 14819
    },
    {
      "epoch": 5.737514518002323,
      "grad_norm": 58.964473724365234,
      "learning_rate": 4.736094979997419e-06,
      "loss": 1.098,
      "step": 14820
    },
    {
      "epoch": 5.737901664730933,
      "grad_norm": 16.430572509765625,
      "learning_rate": 4.735664816965631e-06,
      "loss": 0.6182,
      "step": 14821
    },
    {
      "epoch": 5.738288811459543,
      "grad_norm": 37.808597564697266,
      "learning_rate": 4.735234653933841e-06,
      "loss": 1.307,
      "step": 14822
    },
    {
      "epoch": 5.7386759581881535,
      "grad_norm": 47.61296081542969,
      "learning_rate": 4.734804490902053e-06,
      "loss": 0.7502,
      "step": 14823
    },
    {
      "epoch": 5.739063104916763,
      "grad_norm": 28.582496643066406,
      "learning_rate": 4.734374327870263e-06,
      "loss": 1.3789,
      "step": 14824
    },
    {
      "epoch": 5.739450251645374,
      "grad_norm": 63.05378341674805,
      "learning_rate": 4.733944164838474e-06,
      "loss": 1.9778,
      "step": 14825
    },
    {
      "epoch": 5.739837398373984,
      "grad_norm": 42.298831939697266,
      "learning_rate": 4.733514001806685e-06,
      "loss": 0.5217,
      "step": 14826
    },
    {
      "epoch": 5.740224545102594,
      "grad_norm": 36.591209411621094,
      "learning_rate": 4.733083838774896e-06,
      "loss": 1.8405,
      "step": 14827
    },
    {
      "epoch": 5.740611691831204,
      "grad_norm": 38.3187141418457,
      "learning_rate": 4.7326536757431065e-06,
      "loss": 0.5754,
      "step": 14828
    },
    {
      "epoch": 5.740998838559814,
      "grad_norm": 34.23126220703125,
      "learning_rate": 4.732223512711318e-06,
      "loss": 1.7569,
      "step": 14829
    },
    {
      "epoch": 5.741385985288424,
      "grad_norm": 50.9613037109375,
      "learning_rate": 4.7317933496795284e-06,
      "loss": 0.599,
      "step": 14830
    },
    {
      "epoch": 5.741773132017034,
      "grad_norm": 22.221845626831055,
      "learning_rate": 4.73136318664774e-06,
      "loss": 0.1841,
      "step": 14831
    },
    {
      "epoch": 5.7421602787456445,
      "grad_norm": 88.74124908447266,
      "learning_rate": 4.7309330236159504e-06,
      "loss": 1.3498,
      "step": 14832
    },
    {
      "epoch": 5.742547425474255,
      "grad_norm": 22.756288528442383,
      "learning_rate": 4.730502860584162e-06,
      "loss": 0.7926,
      "step": 14833
    },
    {
      "epoch": 5.742934572202865,
      "grad_norm": 9.280694007873535,
      "learning_rate": 4.730072697552372e-06,
      "loss": 0.2513,
      "step": 14834
    },
    {
      "epoch": 5.743321718931475,
      "grad_norm": 20.776691436767578,
      "learning_rate": 4.729642534520584e-06,
      "loss": 1.6003,
      "step": 14835
    },
    {
      "epoch": 5.743708865660086,
      "grad_norm": 2.591622829437256,
      "learning_rate": 4.729212371488794e-06,
      "loss": 0.1159,
      "step": 14836
    },
    {
      "epoch": 5.744096012388695,
      "grad_norm": 6.070149898529053,
      "learning_rate": 4.728782208457006e-06,
      "loss": 0.1668,
      "step": 14837
    },
    {
      "epoch": 5.744483159117306,
      "grad_norm": 72.70623779296875,
      "learning_rate": 4.728352045425216e-06,
      "loss": 1.548,
      "step": 14838
    },
    {
      "epoch": 5.744870305845915,
      "grad_norm": 32.557594299316406,
      "learning_rate": 4.727921882393428e-06,
      "loss": 1.7824,
      "step": 14839
    },
    {
      "epoch": 5.745257452574526,
      "grad_norm": 48.86764144897461,
      "learning_rate": 4.727491719361638e-06,
      "loss": 0.6904,
      "step": 14840
    },
    {
      "epoch": 5.7456445993031355,
      "grad_norm": 8.044960021972656,
      "learning_rate": 4.72706155632985e-06,
      "loss": 0.3444,
      "step": 14841
    },
    {
      "epoch": 5.746031746031746,
      "grad_norm": 58.77401351928711,
      "learning_rate": 4.72663139329806e-06,
      "loss": 1.037,
      "step": 14842
    },
    {
      "epoch": 5.746418892760357,
      "grad_norm": 13.505730628967285,
      "learning_rate": 4.726201230266271e-06,
      "loss": 0.6145,
      "step": 14843
    },
    {
      "epoch": 5.746806039488966,
      "grad_norm": 17.90991973876953,
      "learning_rate": 4.725771067234482e-06,
      "loss": 0.2993,
      "step": 14844
    },
    {
      "epoch": 5.747193186217577,
      "grad_norm": 46.95546340942383,
      "learning_rate": 4.725340904202693e-06,
      "loss": 0.9987,
      "step": 14845
    },
    {
      "epoch": 5.747580332946186,
      "grad_norm": 5.19432258605957,
      "learning_rate": 4.724910741170904e-06,
      "loss": 0.2785,
      "step": 14846
    },
    {
      "epoch": 5.747967479674797,
      "grad_norm": 26.85267448425293,
      "learning_rate": 4.724480578139115e-06,
      "loss": 0.8621,
      "step": 14847
    },
    {
      "epoch": 5.748354626403406,
      "grad_norm": 4.080873012542725,
      "learning_rate": 4.724050415107326e-06,
      "loss": 0.1448,
      "step": 14848
    },
    {
      "epoch": 5.748741773132017,
      "grad_norm": 35.261329650878906,
      "learning_rate": 4.723620252075537e-06,
      "loss": 0.9445,
      "step": 14849
    },
    {
      "epoch": 5.7491289198606275,
      "grad_norm": 21.530227661132812,
      "learning_rate": 4.723190089043748e-06,
      "loss": 2.5965,
      "step": 14850
    },
    {
      "epoch": 5.749516066589237,
      "grad_norm": 56.682464599609375,
      "learning_rate": 4.722759926011959e-06,
      "loss": 3.0718,
      "step": 14851
    },
    {
      "epoch": 5.749903213317848,
      "grad_norm": 27.935834884643555,
      "learning_rate": 4.72232976298017e-06,
      "loss": 1.0953,
      "step": 14852
    },
    {
      "epoch": 5.750290360046458,
      "grad_norm": 61.26659393310547,
      "learning_rate": 4.721899599948381e-06,
      "loss": 2.4994,
      "step": 14853
    },
    {
      "epoch": 5.750677506775068,
      "grad_norm": 26.375492095947266,
      "learning_rate": 4.721469436916592e-06,
      "loss": 0.955,
      "step": 14854
    },
    {
      "epoch": 5.751064653503678,
      "grad_norm": 48.094181060791016,
      "learning_rate": 4.721039273884803e-06,
      "loss": 1.5567,
      "step": 14855
    },
    {
      "epoch": 5.751451800232288,
      "grad_norm": 62.34809494018555,
      "learning_rate": 4.720609110853014e-06,
      "loss": 2.352,
      "step": 14856
    },
    {
      "epoch": 5.751838946960898,
      "grad_norm": 36.152366638183594,
      "learning_rate": 4.720178947821225e-06,
      "loss": 0.3488,
      "step": 14857
    },
    {
      "epoch": 5.752226093689508,
      "grad_norm": 17.83734130859375,
      "learning_rate": 4.719748784789435e-06,
      "loss": 1.4977,
      "step": 14858
    },
    {
      "epoch": 5.7526132404181185,
      "grad_norm": 5.790642261505127,
      "learning_rate": 4.719318621757647e-06,
      "loss": 0.0544,
      "step": 14859
    },
    {
      "epoch": 5.753000387146729,
      "grad_norm": 23.94427490234375,
      "learning_rate": 4.718888458725857e-06,
      "loss": 1.1346,
      "step": 14860
    },
    {
      "epoch": 5.753387533875339,
      "grad_norm": 10.382761001586914,
      "learning_rate": 4.718458295694068e-06,
      "loss": 0.3187,
      "step": 14861
    },
    {
      "epoch": 5.753774680603949,
      "grad_norm": 21.988218307495117,
      "learning_rate": 4.718028132662279e-06,
      "loss": 0.8072,
      "step": 14862
    },
    {
      "epoch": 5.754161827332559,
      "grad_norm": 55.78290939331055,
      "learning_rate": 4.71759796963049e-06,
      "loss": 0.2239,
      "step": 14863
    },
    {
      "epoch": 5.754548974061169,
      "grad_norm": 23.049345016479492,
      "learning_rate": 4.717167806598701e-06,
      "loss": 0.5145,
      "step": 14864
    },
    {
      "epoch": 5.754936120789779,
      "grad_norm": 22.906612396240234,
      "learning_rate": 4.716737643566912e-06,
      "loss": 1.2891,
      "step": 14865
    },
    {
      "epoch": 5.755323267518389,
      "grad_norm": 2.162545680999756,
      "learning_rate": 4.716307480535123e-06,
      "loss": 0.0914,
      "step": 14866
    },
    {
      "epoch": 5.755710414247,
      "grad_norm": 31.0930118560791,
      "learning_rate": 4.715877317503334e-06,
      "loss": 0.436,
      "step": 14867
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 10.13991928100586,
      "learning_rate": 4.715447154471545e-06,
      "loss": 0.2648,
      "step": 14868
    },
    {
      "epoch": 5.75648470770422,
      "grad_norm": 29.547794342041016,
      "learning_rate": 4.715016991439756e-06,
      "loss": 0.8134,
      "step": 14869
    },
    {
      "epoch": 5.75687185443283,
      "grad_norm": 2.584326982498169,
      "learning_rate": 4.714586828407967e-06,
      "loss": 0.1123,
      "step": 14870
    },
    {
      "epoch": 5.75725900116144,
      "grad_norm": 47.598663330078125,
      "learning_rate": 4.714156665376178e-06,
      "loss": 0.2448,
      "step": 14871
    },
    {
      "epoch": 5.757646147890051,
      "grad_norm": 24.398042678833008,
      "learning_rate": 4.713726502344389e-06,
      "loss": 0.9931,
      "step": 14872
    },
    {
      "epoch": 5.75803329461866,
      "grad_norm": 41.45362091064453,
      "learning_rate": 4.7132963393126e-06,
      "loss": 1.5231,
      "step": 14873
    },
    {
      "epoch": 5.758420441347271,
      "grad_norm": 74.74627685546875,
      "learning_rate": 4.712866176280811e-06,
      "loss": 0.5199,
      "step": 14874
    },
    {
      "epoch": 5.7588075880758804,
      "grad_norm": 56.220848083496094,
      "learning_rate": 4.712436013249022e-06,
      "loss": 1.792,
      "step": 14875
    },
    {
      "epoch": 5.759194734804491,
      "grad_norm": 29.292224884033203,
      "learning_rate": 4.712005850217232e-06,
      "loss": 0.6624,
      "step": 14876
    },
    {
      "epoch": 5.7595818815331015,
      "grad_norm": 10.997488021850586,
      "learning_rate": 4.711575687185444e-06,
      "loss": 0.4057,
      "step": 14877
    },
    {
      "epoch": 5.759969028261711,
      "grad_norm": 17.069299697875977,
      "learning_rate": 4.711145524153654e-06,
      "loss": 1.0097,
      "step": 14878
    },
    {
      "epoch": 5.760356174990322,
      "grad_norm": 78.58390808105469,
      "learning_rate": 4.710715361121865e-06,
      "loss": 0.4398,
      "step": 14879
    },
    {
      "epoch": 5.760743321718931,
      "grad_norm": 11.834912300109863,
      "learning_rate": 4.710285198090076e-06,
      "loss": 0.5976,
      "step": 14880
    },
    {
      "epoch": 5.761130468447542,
      "grad_norm": 59.596702575683594,
      "learning_rate": 4.709855035058287e-06,
      "loss": 2.0127,
      "step": 14881
    },
    {
      "epoch": 5.761517615176151,
      "grad_norm": 39.67608642578125,
      "learning_rate": 4.709424872026498e-06,
      "loss": 0.6314,
      "step": 14882
    },
    {
      "epoch": 5.761904761904762,
      "grad_norm": 33.46659851074219,
      "learning_rate": 4.708994708994709e-06,
      "loss": 0.807,
      "step": 14883
    },
    {
      "epoch": 5.762291908633372,
      "grad_norm": 19.164833068847656,
      "learning_rate": 4.70856454596292e-06,
      "loss": 0.767,
      "step": 14884
    },
    {
      "epoch": 5.762679055361982,
      "grad_norm": 27.085847854614258,
      "learning_rate": 4.708134382931131e-06,
      "loss": 1.3853,
      "step": 14885
    },
    {
      "epoch": 5.7630662020905925,
      "grad_norm": 21.70225715637207,
      "learning_rate": 4.707704219899342e-06,
      "loss": 2.8848,
      "step": 14886
    },
    {
      "epoch": 5.763453348819202,
      "grad_norm": 78.40933227539062,
      "learning_rate": 4.707274056867554e-06,
      "loss": 1.3124,
      "step": 14887
    },
    {
      "epoch": 5.763840495547813,
      "grad_norm": 22.4987850189209,
      "learning_rate": 4.706843893835764e-06,
      "loss": 1.6926,
      "step": 14888
    },
    {
      "epoch": 5.764227642276423,
      "grad_norm": 40.713134765625,
      "learning_rate": 4.706413730803976e-06,
      "loss": 0.347,
      "step": 14889
    },
    {
      "epoch": 5.764614789005033,
      "grad_norm": 41.725738525390625,
      "learning_rate": 4.705983567772186e-06,
      "loss": 1.6129,
      "step": 14890
    },
    {
      "epoch": 5.765001935733643,
      "grad_norm": 21.430261611938477,
      "learning_rate": 4.705553404740397e-06,
      "loss": 1.306,
      "step": 14891
    },
    {
      "epoch": 5.765389082462253,
      "grad_norm": 19.538536071777344,
      "learning_rate": 4.705123241708608e-06,
      "loss": 0.4878,
      "step": 14892
    },
    {
      "epoch": 5.765776229190863,
      "grad_norm": 18.74299430847168,
      "learning_rate": 4.704693078676819e-06,
      "loss": 1.0585,
      "step": 14893
    },
    {
      "epoch": 5.766163375919474,
      "grad_norm": 82.60124969482422,
      "learning_rate": 4.704262915645029e-06,
      "loss": 0.5999,
      "step": 14894
    },
    {
      "epoch": 5.7665505226480835,
      "grad_norm": 91.38168334960938,
      "learning_rate": 4.703832752613241e-06,
      "loss": 1.9322,
      "step": 14895
    },
    {
      "epoch": 5.766937669376694,
      "grad_norm": 96.23928833007812,
      "learning_rate": 4.703402589581451e-06,
      "loss": 1.6559,
      "step": 14896
    },
    {
      "epoch": 5.767324816105304,
      "grad_norm": 27.5943603515625,
      "learning_rate": 4.702972426549663e-06,
      "loss": 1.1994,
      "step": 14897
    },
    {
      "epoch": 5.767711962833914,
      "grad_norm": 81.76622772216797,
      "learning_rate": 4.702542263517873e-06,
      "loss": 0.7562,
      "step": 14898
    },
    {
      "epoch": 5.768099109562524,
      "grad_norm": 27.40478515625,
      "learning_rate": 4.702112100486085e-06,
      "loss": 1.6056,
      "step": 14899
    },
    {
      "epoch": 5.768486256291134,
      "grad_norm": 20.492591857910156,
      "learning_rate": 4.701681937454295e-06,
      "loss": 0.4571,
      "step": 14900
    },
    {
      "epoch": 5.768873403019745,
      "grad_norm": 93.31673431396484,
      "learning_rate": 4.701251774422507e-06,
      "loss": 0.9377,
      "step": 14901
    },
    {
      "epoch": 5.7692605497483544,
      "grad_norm": 48.6351432800293,
      "learning_rate": 4.700821611390717e-06,
      "loss": 2.3351,
      "step": 14902
    },
    {
      "epoch": 5.769647696476965,
      "grad_norm": 52.147613525390625,
      "learning_rate": 4.700391448358929e-06,
      "loss": 0.8436,
      "step": 14903
    },
    {
      "epoch": 5.770034843205575,
      "grad_norm": 54.46331787109375,
      "learning_rate": 4.699961285327139e-06,
      "loss": 1.6308,
      "step": 14904
    },
    {
      "epoch": 5.770421989934185,
      "grad_norm": 51.99565124511719,
      "learning_rate": 4.699531122295351e-06,
      "loss": 1.3676,
      "step": 14905
    },
    {
      "epoch": 5.770809136662795,
      "grad_norm": 37.79676818847656,
      "learning_rate": 4.699100959263561e-06,
      "loss": 0.342,
      "step": 14906
    },
    {
      "epoch": 5.771196283391405,
      "grad_norm": 25.362245559692383,
      "learning_rate": 4.698670796231773e-06,
      "loss": 0.4058,
      "step": 14907
    },
    {
      "epoch": 5.771583430120016,
      "grad_norm": 110.8286361694336,
      "learning_rate": 4.698240633199983e-06,
      "loss": 1.3562,
      "step": 14908
    },
    {
      "epoch": 5.771970576848625,
      "grad_norm": 151.26547241210938,
      "learning_rate": 4.697810470168194e-06,
      "loss": 0.7503,
      "step": 14909
    },
    {
      "epoch": 5.772357723577236,
      "grad_norm": 5.1522674560546875,
      "learning_rate": 4.697380307136405e-06,
      "loss": 0.2106,
      "step": 14910
    },
    {
      "epoch": 5.772744870305846,
      "grad_norm": 11.915059089660645,
      "learning_rate": 4.696950144104616e-06,
      "loss": 0.3946,
      "step": 14911
    },
    {
      "epoch": 5.773132017034456,
      "grad_norm": 77.84891510009766,
      "learning_rate": 4.696519981072826e-06,
      "loss": 0.9721,
      "step": 14912
    },
    {
      "epoch": 5.7735191637630665,
      "grad_norm": 61.21477127075195,
      "learning_rate": 4.696089818041038e-06,
      "loss": 1.3775,
      "step": 14913
    },
    {
      "epoch": 5.773906310491676,
      "grad_norm": 60.7962532043457,
      "learning_rate": 4.695659655009248e-06,
      "loss": 2.8692,
      "step": 14914
    },
    {
      "epoch": 5.774293457220287,
      "grad_norm": 16.653810501098633,
      "learning_rate": 4.69522949197746e-06,
      "loss": 1.5942,
      "step": 14915
    },
    {
      "epoch": 5.774680603948896,
      "grad_norm": 112.27295684814453,
      "learning_rate": 4.69479932894567e-06,
      "loss": 0.5171,
      "step": 14916
    },
    {
      "epoch": 5.775067750677507,
      "grad_norm": 75.40380859375,
      "learning_rate": 4.694369165913882e-06,
      "loss": 1.6632,
      "step": 14917
    },
    {
      "epoch": 5.775454897406117,
      "grad_norm": 77.62919616699219,
      "learning_rate": 4.693939002882092e-06,
      "loss": 0.5765,
      "step": 14918
    },
    {
      "epoch": 5.775842044134727,
      "grad_norm": 51.63862991333008,
      "learning_rate": 4.693508839850304e-06,
      "loss": 0.9936,
      "step": 14919
    },
    {
      "epoch": 5.776229190863337,
      "grad_norm": 24.37236785888672,
      "learning_rate": 4.693078676818514e-06,
      "loss": 0.967,
      "step": 14920
    },
    {
      "epoch": 5.776616337591947,
      "grad_norm": 3.228388786315918,
      "learning_rate": 4.692648513786726e-06,
      "loss": 0.174,
      "step": 14921
    },
    {
      "epoch": 5.7770034843205575,
      "grad_norm": 31.742286682128906,
      "learning_rate": 4.692218350754936e-06,
      "loss": 0.837,
      "step": 14922
    },
    {
      "epoch": 5.777390631049167,
      "grad_norm": 21.738773345947266,
      "learning_rate": 4.691788187723148e-06,
      "loss": 1.7326,
      "step": 14923
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 40.899322509765625,
      "learning_rate": 4.691358024691358e-06,
      "loss": 1.6698,
      "step": 14924
    },
    {
      "epoch": 5.778164924506388,
      "grad_norm": 30.1944580078125,
      "learning_rate": 4.69092786165957e-06,
      "loss": 1.7273,
      "step": 14925
    },
    {
      "epoch": 5.778552071234998,
      "grad_norm": 76.32477569580078,
      "learning_rate": 4.69049769862778e-06,
      "loss": 0.4066,
      "step": 14926
    },
    {
      "epoch": 5.778939217963608,
      "grad_norm": 112.90123748779297,
      "learning_rate": 4.690067535595991e-06,
      "loss": 1.0595,
      "step": 14927
    },
    {
      "epoch": 5.779326364692219,
      "grad_norm": 29.3879451751709,
      "learning_rate": 4.689637372564202e-06,
      "loss": 0.8924,
      "step": 14928
    },
    {
      "epoch": 5.7797135114208285,
      "grad_norm": 42.158851623535156,
      "learning_rate": 4.689207209532413e-06,
      "loss": 2.0845,
      "step": 14929
    },
    {
      "epoch": 5.780100658149439,
      "grad_norm": 9.028322219848633,
      "learning_rate": 4.688777046500624e-06,
      "loss": 0.302,
      "step": 14930
    },
    {
      "epoch": 5.780487804878049,
      "grad_norm": 4.822821617126465,
      "learning_rate": 4.688346883468835e-06,
      "loss": 0.2158,
      "step": 14931
    },
    {
      "epoch": 5.780874951606659,
      "grad_norm": 100.90289306640625,
      "learning_rate": 4.687916720437046e-06,
      "loss": 2.8551,
      "step": 14932
    },
    {
      "epoch": 5.781262098335269,
      "grad_norm": 54.43424987792969,
      "learning_rate": 4.687486557405257e-06,
      "loss": 0.8798,
      "step": 14933
    },
    {
      "epoch": 5.781649245063879,
      "grad_norm": 38.894535064697266,
      "learning_rate": 4.687056394373468e-06,
      "loss": 1.8018,
      "step": 14934
    },
    {
      "epoch": 5.78203639179249,
      "grad_norm": 37.48383331298828,
      "learning_rate": 4.686626231341679e-06,
      "loss": 0.8007,
      "step": 14935
    },
    {
      "epoch": 5.782423538521099,
      "grad_norm": 27.73544692993164,
      "learning_rate": 4.68619606830989e-06,
      "loss": 0.9782,
      "step": 14936
    },
    {
      "epoch": 5.78281068524971,
      "grad_norm": 106.13975524902344,
      "learning_rate": 4.685765905278101e-06,
      "loss": 1.5741,
      "step": 14937
    },
    {
      "epoch": 5.7831978319783195,
      "grad_norm": 25.86736297607422,
      "learning_rate": 4.685335742246312e-06,
      "loss": 0.6005,
      "step": 14938
    },
    {
      "epoch": 5.78358497870693,
      "grad_norm": 39.6144905090332,
      "learning_rate": 4.684905579214523e-06,
      "loss": 0.8408,
      "step": 14939
    },
    {
      "epoch": 5.78397212543554,
      "grad_norm": 54.70561218261719,
      "learning_rate": 4.684475416182734e-06,
      "loss": 1.3366,
      "step": 14940
    },
    {
      "epoch": 5.78435927216415,
      "grad_norm": 22.076080322265625,
      "learning_rate": 4.684045253150945e-06,
      "loss": 0.2006,
      "step": 14941
    },
    {
      "epoch": 5.784746418892761,
      "grad_norm": 63.80491638183594,
      "learning_rate": 4.683615090119155e-06,
      "loss": 1.3642,
      "step": 14942
    },
    {
      "epoch": 5.78513356562137,
      "grad_norm": 58.62140655517578,
      "learning_rate": 4.683184927087367e-06,
      "loss": 1.0833,
      "step": 14943
    },
    {
      "epoch": 5.785520712349981,
      "grad_norm": 81.82622528076172,
      "learning_rate": 4.682754764055577e-06,
      "loss": 0.6056,
      "step": 14944
    },
    {
      "epoch": 5.78590785907859,
      "grad_norm": 78.25321960449219,
      "learning_rate": 4.682324601023788e-06,
      "loss": 1.0665,
      "step": 14945
    },
    {
      "epoch": 5.786295005807201,
      "grad_norm": 55.28768539428711,
      "learning_rate": 4.681894437991999e-06,
      "loss": 0.6514,
      "step": 14946
    },
    {
      "epoch": 5.786682152535811,
      "grad_norm": 8.147732734680176,
      "learning_rate": 4.68146427496021e-06,
      "loss": 0.4264,
      "step": 14947
    },
    {
      "epoch": 5.787069299264421,
      "grad_norm": 82.26087951660156,
      "learning_rate": 4.681034111928421e-06,
      "loss": 2.1937,
      "step": 14948
    },
    {
      "epoch": 5.7874564459930316,
      "grad_norm": 70.29244232177734,
      "learning_rate": 4.680603948896632e-06,
      "loss": 2.7953,
      "step": 14949
    },
    {
      "epoch": 5.787843592721641,
      "grad_norm": 211.68356323242188,
      "learning_rate": 4.680173785864843e-06,
      "loss": 0.9348,
      "step": 14950
    },
    {
      "epoch": 5.788230739450252,
      "grad_norm": 63.65171432495117,
      "learning_rate": 4.679743622833054e-06,
      "loss": 1.5556,
      "step": 14951
    },
    {
      "epoch": 5.788617886178862,
      "grad_norm": 62.358245849609375,
      "learning_rate": 4.679313459801265e-06,
      "loss": 2.6139,
      "step": 14952
    },
    {
      "epoch": 5.789005032907472,
      "grad_norm": 19.79730987548828,
      "learning_rate": 4.678883296769476e-06,
      "loss": 0.4873,
      "step": 14953
    },
    {
      "epoch": 5.789392179636082,
      "grad_norm": 24.293447494506836,
      "learning_rate": 4.678453133737687e-06,
      "loss": 1.5986,
      "step": 14954
    },
    {
      "epoch": 5.789779326364692,
      "grad_norm": 0.8696364164352417,
      "learning_rate": 4.678022970705898e-06,
      "loss": 0.023,
      "step": 14955
    },
    {
      "epoch": 5.7901664730933025,
      "grad_norm": 22.791452407836914,
      "learning_rate": 4.677592807674109e-06,
      "loss": 3.2387,
      "step": 14956
    },
    {
      "epoch": 5.790553619821912,
      "grad_norm": 8.062090873718262,
      "learning_rate": 4.67716264464232e-06,
      "loss": 0.208,
      "step": 14957
    },
    {
      "epoch": 5.790940766550523,
      "grad_norm": 32.40579605102539,
      "learning_rate": 4.676732481610531e-06,
      "loss": 2.0613,
      "step": 14958
    },
    {
      "epoch": 5.791327913279133,
      "grad_norm": 29.797605514526367,
      "learning_rate": 4.676302318578742e-06,
      "loss": 1.0661,
      "step": 14959
    },
    {
      "epoch": 5.791715060007743,
      "grad_norm": 118.18816375732422,
      "learning_rate": 4.675872155546952e-06,
      "loss": 0.5456,
      "step": 14960
    },
    {
      "epoch": 5.792102206736353,
      "grad_norm": 20.09470558166504,
      "learning_rate": 4.675441992515164e-06,
      "loss": 1.3276,
      "step": 14961
    },
    {
      "epoch": 5.792489353464963,
      "grad_norm": 172.53955078125,
      "learning_rate": 4.675011829483374e-06,
      "loss": 0.5586,
      "step": 14962
    },
    {
      "epoch": 5.792876500193573,
      "grad_norm": 133.01551818847656,
      "learning_rate": 4.674581666451585e-06,
      "loss": 0.7,
      "step": 14963
    },
    {
      "epoch": 5.793263646922184,
      "grad_norm": 73.72238159179688,
      "learning_rate": 4.674151503419796e-06,
      "loss": 1.3832,
      "step": 14964
    },
    {
      "epoch": 5.7936507936507935,
      "grad_norm": 187.93927001953125,
      "learning_rate": 4.673721340388007e-06,
      "loss": 1.8552,
      "step": 14965
    },
    {
      "epoch": 5.794037940379404,
      "grad_norm": 65.11201477050781,
      "learning_rate": 4.673291177356218e-06,
      "loss": 0.6387,
      "step": 14966
    },
    {
      "epoch": 5.794425087108014,
      "grad_norm": 5.994988441467285,
      "learning_rate": 4.672861014324429e-06,
      "loss": 0.1626,
      "step": 14967
    },
    {
      "epoch": 5.794812233836624,
      "grad_norm": 4.148584842681885,
      "learning_rate": 4.67243085129264e-06,
      "loss": 0.1142,
      "step": 14968
    },
    {
      "epoch": 5.795199380565235,
      "grad_norm": 92.24506378173828,
      "learning_rate": 4.672000688260852e-06,
      "loss": 0.8303,
      "step": 14969
    },
    {
      "epoch": 5.795586527293844,
      "grad_norm": 45.68942642211914,
      "learning_rate": 4.671570525229062e-06,
      "loss": 0.4422,
      "step": 14970
    },
    {
      "epoch": 5.795973674022455,
      "grad_norm": 10.091864585876465,
      "learning_rate": 4.671140362197274e-06,
      "loss": 0.2758,
      "step": 14971
    },
    {
      "epoch": 5.796360820751064,
      "grad_norm": 36.06805419921875,
      "learning_rate": 4.670710199165484e-06,
      "loss": 1.2118,
      "step": 14972
    },
    {
      "epoch": 5.796747967479675,
      "grad_norm": 8.388060569763184,
      "learning_rate": 4.670280036133696e-06,
      "loss": 0.4982,
      "step": 14973
    },
    {
      "epoch": 5.7971351142082845,
      "grad_norm": 46.89520263671875,
      "learning_rate": 4.669849873101906e-06,
      "loss": 1.4713,
      "step": 14974
    },
    {
      "epoch": 5.797522260936895,
      "grad_norm": 20.40460968017578,
      "learning_rate": 4.669419710070117e-06,
      "loss": 1.269,
      "step": 14975
    },
    {
      "epoch": 5.7979094076655056,
      "grad_norm": 42.92682647705078,
      "learning_rate": 4.668989547038328e-06,
      "loss": 1.6712,
      "step": 14976
    },
    {
      "epoch": 5.798296554394115,
      "grad_norm": 24.559814453125,
      "learning_rate": 4.668559384006539e-06,
      "loss": 0.6411,
      "step": 14977
    },
    {
      "epoch": 5.798683701122726,
      "grad_norm": 24.43402099609375,
      "learning_rate": 4.668129220974749e-06,
      "loss": 2.8482,
      "step": 14978
    },
    {
      "epoch": 5.799070847851335,
      "grad_norm": 0.6459609866142273,
      "learning_rate": 4.667699057942961e-06,
      "loss": 0.0181,
      "step": 14979
    },
    {
      "epoch": 5.799457994579946,
      "grad_norm": 51.11151123046875,
      "learning_rate": 4.667268894911171e-06,
      "loss": 1.6948,
      "step": 14980
    },
    {
      "epoch": 5.799845141308556,
      "grad_norm": 23.219955444335938,
      "learning_rate": 4.666838731879383e-06,
      "loss": 0.2166,
      "step": 14981
    },
    {
      "epoch": 5.800232288037166,
      "grad_norm": 7.03812837600708,
      "learning_rate": 4.666408568847593e-06,
      "loss": 0.191,
      "step": 14982
    },
    {
      "epoch": 5.8006194347657765,
      "grad_norm": 31.276399612426758,
      "learning_rate": 4.665978405815805e-06,
      "loss": 1.7662,
      "step": 14983
    },
    {
      "epoch": 5.801006581494386,
      "grad_norm": 17.447452545166016,
      "learning_rate": 4.665548242784015e-06,
      "loss": 0.3712,
      "step": 14984
    },
    {
      "epoch": 5.801393728222997,
      "grad_norm": 42.32375717163086,
      "learning_rate": 4.665118079752227e-06,
      "loss": 0.6881,
      "step": 14985
    },
    {
      "epoch": 5.801780874951607,
      "grad_norm": 78.148681640625,
      "learning_rate": 4.664687916720437e-06,
      "loss": 3.2887,
      "step": 14986
    },
    {
      "epoch": 5.802168021680217,
      "grad_norm": 43.17490005493164,
      "learning_rate": 4.664257753688649e-06,
      "loss": 1.0772,
      "step": 14987
    },
    {
      "epoch": 5.802555168408827,
      "grad_norm": 46.90584182739258,
      "learning_rate": 4.663827590656859e-06,
      "loss": 0.5105,
      "step": 14988
    },
    {
      "epoch": 5.802942315137437,
      "grad_norm": 87.29368591308594,
      "learning_rate": 4.663397427625071e-06,
      "loss": 1.7656,
      "step": 14989
    },
    {
      "epoch": 5.803329461866047,
      "grad_norm": 85.21717071533203,
      "learning_rate": 4.662967264593281e-06,
      "loss": 1.8477,
      "step": 14990
    },
    {
      "epoch": 5.803716608594657,
      "grad_norm": 53.42338180541992,
      "learning_rate": 4.662537101561493e-06,
      "loss": 0.3887,
      "step": 14991
    },
    {
      "epoch": 5.8041037553232675,
      "grad_norm": 2.560513496398926,
      "learning_rate": 4.662106938529703e-06,
      "loss": 0.1076,
      "step": 14992
    },
    {
      "epoch": 5.804490902051878,
      "grad_norm": 24.88167953491211,
      "learning_rate": 4.661676775497914e-06,
      "loss": 1.3141,
      "step": 14993
    },
    {
      "epoch": 5.804878048780488,
      "grad_norm": 18.60731315612793,
      "learning_rate": 4.661246612466125e-06,
      "loss": 0.3541,
      "step": 14994
    },
    {
      "epoch": 5.805265195509098,
      "grad_norm": 40.6292610168457,
      "learning_rate": 4.660816449434336e-06,
      "loss": 3.4121,
      "step": 14995
    },
    {
      "epoch": 5.805652342237708,
      "grad_norm": 42.279563903808594,
      "learning_rate": 4.660386286402546e-06,
      "loss": 0.7165,
      "step": 14996
    },
    {
      "epoch": 5.806039488966318,
      "grad_norm": 20.892641067504883,
      "learning_rate": 4.659956123370758e-06,
      "loss": 0.8392,
      "step": 14997
    },
    {
      "epoch": 5.806426635694928,
      "grad_norm": 119.07295989990234,
      "learning_rate": 4.659525960338968e-06,
      "loss": 1.4919,
      "step": 14998
    },
    {
      "epoch": 5.806813782423538,
      "grad_norm": 42.45038604736328,
      "learning_rate": 4.65909579730718e-06,
      "loss": 1.9181,
      "step": 14999
    },
    {
      "epoch": 5.807200929152149,
      "grad_norm": 48.93841552734375,
      "learning_rate": 4.65866563427539e-06,
      "loss": 1.8543,
      "step": 15000
    },
    {
      "epoch": 5.8075880758807585,
      "grad_norm": 6.650043487548828,
      "learning_rate": 4.658235471243602e-06,
      "loss": 0.2775,
      "step": 15001
    },
    {
      "epoch": 5.807975222609369,
      "grad_norm": 6.875773906707764,
      "learning_rate": 4.657805308211812e-06,
      "loss": 0.307,
      "step": 15002
    },
    {
      "epoch": 5.80836236933798,
      "grad_norm": 15.924558639526367,
      "learning_rate": 4.657375145180024e-06,
      "loss": 1.4692,
      "step": 15003
    },
    {
      "epoch": 5.808749516066589,
      "grad_norm": 23.134870529174805,
      "learning_rate": 4.656944982148234e-06,
      "loss": 0.5725,
      "step": 15004
    },
    {
      "epoch": 5.8091366627952,
      "grad_norm": 59.286712646484375,
      "learning_rate": 4.656514819116446e-06,
      "loss": 1.2952,
      "step": 15005
    },
    {
      "epoch": 5.809523809523809,
      "grad_norm": 31.03162384033203,
      "learning_rate": 4.656084656084656e-06,
      "loss": 0.5765,
      "step": 15006
    },
    {
      "epoch": 5.80991095625242,
      "grad_norm": 10.603519439697266,
      "learning_rate": 4.655654493052868e-06,
      "loss": 0.2049,
      "step": 15007
    },
    {
      "epoch": 5.8102981029810294,
      "grad_norm": 196.7367401123047,
      "learning_rate": 4.655224330021078e-06,
      "loss": 1.7703,
      "step": 15008
    },
    {
      "epoch": 5.81068524970964,
      "grad_norm": 98.25853729248047,
      "learning_rate": 4.65479416698929e-06,
      "loss": 0.4787,
      "step": 15009
    },
    {
      "epoch": 5.8110723964382505,
      "grad_norm": 85.92540740966797,
      "learning_rate": 4.6543640039575e-06,
      "loss": 0.5315,
      "step": 15010
    },
    {
      "epoch": 5.81145954316686,
      "grad_norm": 21.260541915893555,
      "learning_rate": 4.653933840925711e-06,
      "loss": 0.066,
      "step": 15011
    },
    {
      "epoch": 5.811846689895471,
      "grad_norm": 83.14286041259766,
      "learning_rate": 4.653503677893922e-06,
      "loss": 0.7046,
      "step": 15012
    },
    {
      "epoch": 5.81223383662408,
      "grad_norm": 23.23412322998047,
      "learning_rate": 4.653073514862133e-06,
      "loss": 0.3055,
      "step": 15013
    },
    {
      "epoch": 5.812620983352691,
      "grad_norm": 5.579524517059326,
      "learning_rate": 4.652643351830344e-06,
      "loss": 0.172,
      "step": 15014
    },
    {
      "epoch": 5.8130081300813,
      "grad_norm": 67.55199432373047,
      "learning_rate": 4.652213188798555e-06,
      "loss": 0.4043,
      "step": 15015
    },
    {
      "epoch": 5.813395276809911,
      "grad_norm": 19.113628387451172,
      "learning_rate": 4.651783025766766e-06,
      "loss": 1.6253,
      "step": 15016
    },
    {
      "epoch": 5.813782423538521,
      "grad_norm": 0.8543604016304016,
      "learning_rate": 4.651352862734977e-06,
      "loss": 0.0218,
      "step": 15017
    },
    {
      "epoch": 5.814169570267131,
      "grad_norm": 111.12290954589844,
      "learning_rate": 4.650922699703188e-06,
      "loss": 0.1662,
      "step": 15018
    },
    {
      "epoch": 5.8145567169957415,
      "grad_norm": 113.4289321899414,
      "learning_rate": 4.650492536671399e-06,
      "loss": 1.6461,
      "step": 15019
    },
    {
      "epoch": 5.814943863724352,
      "grad_norm": 28.52362632751465,
      "learning_rate": 4.65006237363961e-06,
      "loss": 0.751,
      "step": 15020
    },
    {
      "epoch": 5.815331010452962,
      "grad_norm": 57.95915603637695,
      "learning_rate": 4.649632210607821e-06,
      "loss": 1.6951,
      "step": 15021
    },
    {
      "epoch": 5.815718157181572,
      "grad_norm": 31.959684371948242,
      "learning_rate": 4.649202047576032e-06,
      "loss": 0.6142,
      "step": 15022
    },
    {
      "epoch": 5.816105303910182,
      "grad_norm": 48.546783447265625,
      "learning_rate": 4.648771884544243e-06,
      "loss": 2.198,
      "step": 15023
    },
    {
      "epoch": 5.816492450638792,
      "grad_norm": 153.53692626953125,
      "learning_rate": 4.648341721512454e-06,
      "loss": 0.4368,
      "step": 15024
    },
    {
      "epoch": 5.816879597367402,
      "grad_norm": 221.41734313964844,
      "learning_rate": 4.647911558480665e-06,
      "loss": 2.9161,
      "step": 15025
    },
    {
      "epoch": 5.817266744096012,
      "grad_norm": 77.77570343017578,
      "learning_rate": 4.647481395448875e-06,
      "loss": 1.8204,
      "step": 15026
    },
    {
      "epoch": 5.817653890824623,
      "grad_norm": 42.64664840698242,
      "learning_rate": 4.647051232417087e-06,
      "loss": 0.4911,
      "step": 15027
    },
    {
      "epoch": 5.8180410375532325,
      "grad_norm": 28.424131393432617,
      "learning_rate": 4.646621069385297e-06,
      "loss": 2.5045,
      "step": 15028
    },
    {
      "epoch": 5.818428184281843,
      "grad_norm": 77.98843383789062,
      "learning_rate": 4.646190906353508e-06,
      "loss": 0.9669,
      "step": 15029
    },
    {
      "epoch": 5.818815331010453,
      "grad_norm": 48.828468322753906,
      "learning_rate": 4.645760743321719e-06,
      "loss": 0.3404,
      "step": 15030
    },
    {
      "epoch": 5.819202477739063,
      "grad_norm": 49.34779357910156,
      "learning_rate": 4.64533058028993e-06,
      "loss": 1.751,
      "step": 15031
    },
    {
      "epoch": 5.819589624467673,
      "grad_norm": 61.742000579833984,
      "learning_rate": 4.644900417258141e-06,
      "loss": 0.6288,
      "step": 15032
    },
    {
      "epoch": 5.819976771196283,
      "grad_norm": 16.514074325561523,
      "learning_rate": 4.644470254226352e-06,
      "loss": 0.6487,
      "step": 15033
    },
    {
      "epoch": 5.820363917924894,
      "grad_norm": 16.050968170166016,
      "learning_rate": 4.644040091194563e-06,
      "loss": 0.3398,
      "step": 15034
    },
    {
      "epoch": 5.8207510646535034,
      "grad_norm": 17.726024627685547,
      "learning_rate": 4.643609928162774e-06,
      "loss": 0.8045,
      "step": 15035
    },
    {
      "epoch": 5.821138211382114,
      "grad_norm": 86.59732055664062,
      "learning_rate": 4.643179765130985e-06,
      "loss": 3.0331,
      "step": 15036
    },
    {
      "epoch": 5.821525358110724,
      "grad_norm": 40.55430603027344,
      "learning_rate": 4.642749602099196e-06,
      "loss": 0.5094,
      "step": 15037
    },
    {
      "epoch": 5.821912504839334,
      "grad_norm": 22.850780487060547,
      "learning_rate": 4.642319439067407e-06,
      "loss": 0.5084,
      "step": 15038
    },
    {
      "epoch": 5.822299651567945,
      "grad_norm": 101.68672943115234,
      "learning_rate": 4.641889276035618e-06,
      "loss": 0.4636,
      "step": 15039
    },
    {
      "epoch": 5.822686798296554,
      "grad_norm": 1.8163827657699585,
      "learning_rate": 4.641459113003829e-06,
      "loss": 0.0581,
      "step": 15040
    },
    {
      "epoch": 5.823073945025165,
      "grad_norm": 78.31440734863281,
      "learning_rate": 4.64102894997204e-06,
      "loss": 1.4995,
      "step": 15041
    },
    {
      "epoch": 5.823461091753774,
      "grad_norm": 33.94047164916992,
      "learning_rate": 4.640598786940251e-06,
      "loss": 2.7169,
      "step": 15042
    },
    {
      "epoch": 5.823848238482385,
      "grad_norm": 94.09917449951172,
      "learning_rate": 4.640168623908462e-06,
      "loss": 1.3798,
      "step": 15043
    },
    {
      "epoch": 5.824235385210995,
      "grad_norm": 20.978656768798828,
      "learning_rate": 4.639738460876672e-06,
      "loss": 0.218,
      "step": 15044
    },
    {
      "epoch": 5.824622531939605,
      "grad_norm": 13.358182907104492,
      "learning_rate": 4.639308297844884e-06,
      "loss": 0.473,
      "step": 15045
    },
    {
      "epoch": 5.8250096786682155,
      "grad_norm": 27.098665237426758,
      "learning_rate": 4.638878134813094e-06,
      "loss": 1.6205,
      "step": 15046
    },
    {
      "epoch": 5.825396825396825,
      "grad_norm": 43.79506301879883,
      "learning_rate": 4.638447971781305e-06,
      "loss": 0.5021,
      "step": 15047
    },
    {
      "epoch": 5.825783972125436,
      "grad_norm": 54.67950439453125,
      "learning_rate": 4.638017808749516e-06,
      "loss": 1.0444,
      "step": 15048
    },
    {
      "epoch": 5.826171118854045,
      "grad_norm": 42.215518951416016,
      "learning_rate": 4.637587645717727e-06,
      "loss": 1.5699,
      "step": 15049
    },
    {
      "epoch": 5.826558265582656,
      "grad_norm": 0.7084563970565796,
      "learning_rate": 4.637157482685938e-06,
      "loss": 0.0163,
      "step": 15050
    },
    {
      "epoch": 5.826945412311266,
      "grad_norm": 12.98090648651123,
      "learning_rate": 4.63672731965415e-06,
      "loss": 0.4247,
      "step": 15051
    },
    {
      "epoch": 5.827332559039876,
      "grad_norm": 29.94853401184082,
      "learning_rate": 4.63629715662236e-06,
      "loss": 0.9745,
      "step": 15052
    },
    {
      "epoch": 5.827719705768486,
      "grad_norm": 44.67508316040039,
      "learning_rate": 4.635866993590572e-06,
      "loss": 2.7448,
      "step": 15053
    },
    {
      "epoch": 5.828106852497096,
      "grad_norm": 138.89016723632812,
      "learning_rate": 4.635436830558782e-06,
      "loss": 1.0557,
      "step": 15054
    },
    {
      "epoch": 5.8284939992257065,
      "grad_norm": 5.125576496124268,
      "learning_rate": 4.635006667526994e-06,
      "loss": 0.2654,
      "step": 15055
    },
    {
      "epoch": 5.828881145954317,
      "grad_norm": 5.854185581207275,
      "learning_rate": 4.634576504495204e-06,
      "loss": 0.2776,
      "step": 15056
    },
    {
      "epoch": 5.829268292682927,
      "grad_norm": 2.026418685913086,
      "learning_rate": 4.634146341463416e-06,
      "loss": 0.086,
      "step": 15057
    },
    {
      "epoch": 5.829655439411537,
      "grad_norm": 2.076364517211914,
      "learning_rate": 4.633716178431626e-06,
      "loss": 0.0918,
      "step": 15058
    },
    {
      "epoch": 5.830042586140147,
      "grad_norm": 44.70574188232422,
      "learning_rate": 4.633286015399837e-06,
      "loss": 0.2696,
      "step": 15059
    },
    {
      "epoch": 5.830429732868757,
      "grad_norm": 2.5715253353118896,
      "learning_rate": 4.632855852368048e-06,
      "loss": 0.0666,
      "step": 15060
    },
    {
      "epoch": 5.830816879597368,
      "grad_norm": 7.476590633392334,
      "learning_rate": 4.632425689336259e-06,
      "loss": 0.1463,
      "step": 15061
    },
    {
      "epoch": 5.8312040263259775,
      "grad_norm": 34.05352020263672,
      "learning_rate": 4.631995526304469e-06,
      "loss": 1.2278,
      "step": 15062
    },
    {
      "epoch": 5.831591173054588,
      "grad_norm": 4.572112560272217,
      "learning_rate": 4.631565363272681e-06,
      "loss": 0.1559,
      "step": 15063
    },
    {
      "epoch": 5.831978319783198,
      "grad_norm": 38.67576599121094,
      "learning_rate": 4.631135200240891e-06,
      "loss": 0.3334,
      "step": 15064
    },
    {
      "epoch": 5.832365466511808,
      "grad_norm": 48.47701644897461,
      "learning_rate": 4.630705037209103e-06,
      "loss": 0.7161,
      "step": 15065
    },
    {
      "epoch": 5.832752613240418,
      "grad_norm": 45.42753219604492,
      "learning_rate": 4.630274874177313e-06,
      "loss": 0.6674,
      "step": 15066
    },
    {
      "epoch": 5.833139759969028,
      "grad_norm": 44.186180114746094,
      "learning_rate": 4.629844711145525e-06,
      "loss": 1.9087,
      "step": 15067
    },
    {
      "epoch": 5.833526906697639,
      "grad_norm": 70.39900207519531,
      "learning_rate": 4.629414548113735e-06,
      "loss": 2.2969,
      "step": 15068
    },
    {
      "epoch": 5.833914053426248,
      "grad_norm": 77.19178771972656,
      "learning_rate": 4.628984385081947e-06,
      "loss": 4.1167,
      "step": 15069
    },
    {
      "epoch": 5.834301200154859,
      "grad_norm": 34.983985900878906,
      "learning_rate": 4.628554222050157e-06,
      "loss": 0.6951,
      "step": 15070
    },
    {
      "epoch": 5.8346883468834685,
      "grad_norm": 63.00508117675781,
      "learning_rate": 4.628124059018369e-06,
      "loss": 0.3066,
      "step": 15071
    },
    {
      "epoch": 5.835075493612079,
      "grad_norm": 6.0656843185424805,
      "learning_rate": 4.627693895986579e-06,
      "loss": 0.2947,
      "step": 15072
    },
    {
      "epoch": 5.835462640340689,
      "grad_norm": 32.601287841796875,
      "learning_rate": 4.627263732954791e-06,
      "loss": 0.8465,
      "step": 15073
    },
    {
      "epoch": 5.835849787069299,
      "grad_norm": 6.490438938140869,
      "learning_rate": 4.626833569923001e-06,
      "loss": 0.2058,
      "step": 15074
    },
    {
      "epoch": 5.83623693379791,
      "grad_norm": 10.490823745727539,
      "learning_rate": 4.626403406891213e-06,
      "loss": 0.2185,
      "step": 15075
    },
    {
      "epoch": 5.836624080526519,
      "grad_norm": 74.75069427490234,
      "learning_rate": 4.625973243859423e-06,
      "loss": 0.5707,
      "step": 15076
    },
    {
      "epoch": 5.83701122725513,
      "grad_norm": 53.155487060546875,
      "learning_rate": 4.625543080827634e-06,
      "loss": 1.6809,
      "step": 15077
    },
    {
      "epoch": 5.83739837398374,
      "grad_norm": 67.96722412109375,
      "learning_rate": 4.625112917795845e-06,
      "loss": 1.9489,
      "step": 15078
    },
    {
      "epoch": 5.83778552071235,
      "grad_norm": 39.69684600830078,
      "learning_rate": 4.624682754764056e-06,
      "loss": 0.6572,
      "step": 15079
    },
    {
      "epoch": 5.83817266744096,
      "grad_norm": 35.818756103515625,
      "learning_rate": 4.624252591732266e-06,
      "loss": 0.8861,
      "step": 15080
    },
    {
      "epoch": 5.83855981416957,
      "grad_norm": 63.70979690551758,
      "learning_rate": 4.623822428700478e-06,
      "loss": 0.9743,
      "step": 15081
    },
    {
      "epoch": 5.8389469608981805,
      "grad_norm": 28.38772201538086,
      "learning_rate": 4.623392265668688e-06,
      "loss": 0.4262,
      "step": 15082
    },
    {
      "epoch": 5.83933410762679,
      "grad_norm": 55.07756423950195,
      "learning_rate": 4.6229621026369e-06,
      "loss": 1.113,
      "step": 15083
    },
    {
      "epoch": 5.839721254355401,
      "grad_norm": 37.39542770385742,
      "learning_rate": 4.62253193960511e-06,
      "loss": 3.0204,
      "step": 15084
    },
    {
      "epoch": 5.840108401084011,
      "grad_norm": 58.73426818847656,
      "learning_rate": 4.622101776573322e-06,
      "loss": 1.1343,
      "step": 15085
    },
    {
      "epoch": 5.840495547812621,
      "grad_norm": 59.49260711669922,
      "learning_rate": 4.621671613541532e-06,
      "loss": 0.9693,
      "step": 15086
    },
    {
      "epoch": 5.840882694541231,
      "grad_norm": 134.08609008789062,
      "learning_rate": 4.621241450509744e-06,
      "loss": 1.0246,
      "step": 15087
    },
    {
      "epoch": 5.841269841269841,
      "grad_norm": 0.5533060431480408,
      "learning_rate": 4.620811287477954e-06,
      "loss": 0.016,
      "step": 15088
    },
    {
      "epoch": 5.8416569879984515,
      "grad_norm": 116.56996154785156,
      "learning_rate": 4.620381124446166e-06,
      "loss": 2.8454,
      "step": 15089
    },
    {
      "epoch": 5.842044134727061,
      "grad_norm": 19.00464630126953,
      "learning_rate": 4.619950961414376e-06,
      "loss": 0.5318,
      "step": 15090
    },
    {
      "epoch": 5.842431281455672,
      "grad_norm": 20.246225357055664,
      "learning_rate": 4.619520798382588e-06,
      "loss": 1.9706,
      "step": 15091
    },
    {
      "epoch": 5.842818428184282,
      "grad_norm": 73.15802001953125,
      "learning_rate": 4.619090635350798e-06,
      "loss": 1.8115,
      "step": 15092
    },
    {
      "epoch": 5.843205574912892,
      "grad_norm": 36.219932556152344,
      "learning_rate": 4.61866047231901e-06,
      "loss": 0.6377,
      "step": 15093
    },
    {
      "epoch": 5.843592721641502,
      "grad_norm": 17.424776077270508,
      "learning_rate": 4.61823030928722e-06,
      "loss": 1.5797,
      "step": 15094
    },
    {
      "epoch": 5.843979868370113,
      "grad_norm": 48.438232421875,
      "learning_rate": 4.617800146255431e-06,
      "loss": 0.9457,
      "step": 15095
    },
    {
      "epoch": 5.844367015098722,
      "grad_norm": 2.9180006980895996,
      "learning_rate": 4.617369983223642e-06,
      "loss": 0.1544,
      "step": 15096
    },
    {
      "epoch": 5.844754161827333,
      "grad_norm": 133.45364379882812,
      "learning_rate": 4.616939820191853e-06,
      "loss": 2.0963,
      "step": 15097
    },
    {
      "epoch": 5.8451413085559425,
      "grad_norm": 20.89834976196289,
      "learning_rate": 4.616509657160064e-06,
      "loss": 1.6087,
      "step": 15098
    },
    {
      "epoch": 5.845528455284553,
      "grad_norm": 107.21342468261719,
      "learning_rate": 4.616079494128275e-06,
      "loss": 1.0643,
      "step": 15099
    },
    {
      "epoch": 5.845915602013163,
      "grad_norm": 85.94353485107422,
      "learning_rate": 4.615649331096486e-06,
      "loss": 1.7271,
      "step": 15100
    },
    {
      "epoch": 5.846302748741773,
      "grad_norm": 22.96802520751953,
      "learning_rate": 4.615219168064697e-06,
      "loss": 0.4049,
      "step": 15101
    },
    {
      "epoch": 5.846689895470384,
      "grad_norm": 95.6446533203125,
      "learning_rate": 4.614789005032908e-06,
      "loss": 1.3196,
      "step": 15102
    },
    {
      "epoch": 5.847077042198993,
      "grad_norm": 17.627182006835938,
      "learning_rate": 4.614358842001119e-06,
      "loss": 0.2393,
      "step": 15103
    },
    {
      "epoch": 5.847464188927604,
      "grad_norm": 76.84227752685547,
      "learning_rate": 4.61392867896933e-06,
      "loss": 1.7109,
      "step": 15104
    },
    {
      "epoch": 5.847851335656213,
      "grad_norm": 104.27456665039062,
      "learning_rate": 4.613498515937541e-06,
      "loss": 1.9474,
      "step": 15105
    },
    {
      "epoch": 5.848238482384824,
      "grad_norm": 3.768458127975464,
      "learning_rate": 4.613068352905752e-06,
      "loss": 0.1101,
      "step": 15106
    },
    {
      "epoch": 5.8486256291134335,
      "grad_norm": 26.316139221191406,
      "learning_rate": 4.612638189873963e-06,
      "loss": 1.6726,
      "step": 15107
    },
    {
      "epoch": 5.849012775842044,
      "grad_norm": 33.974327087402344,
      "learning_rate": 4.612208026842174e-06,
      "loss": 1.5272,
      "step": 15108
    },
    {
      "epoch": 5.8493999225706546,
      "grad_norm": 93.26688385009766,
      "learning_rate": 4.611777863810385e-06,
      "loss": 1.1986,
      "step": 15109
    },
    {
      "epoch": 5.849787069299264,
      "grad_norm": 30.078447341918945,
      "learning_rate": 4.611347700778595e-06,
      "loss": 0.6141,
      "step": 15110
    },
    {
      "epoch": 5.850174216027875,
      "grad_norm": 94.63362121582031,
      "learning_rate": 4.610917537746807e-06,
      "loss": 1.7497,
      "step": 15111
    },
    {
      "epoch": 5.850561362756485,
      "grad_norm": 36.61772918701172,
      "learning_rate": 4.610487374715017e-06,
      "loss": 0.2823,
      "step": 15112
    },
    {
      "epoch": 5.850948509485095,
      "grad_norm": 31.4396915435791,
      "learning_rate": 4.610057211683228e-06,
      "loss": 1.9999,
      "step": 15113
    },
    {
      "epoch": 5.851335656213705,
      "grad_norm": 1.0839693546295166,
      "learning_rate": 4.609627048651439e-06,
      "loss": 0.0194,
      "step": 15114
    },
    {
      "epoch": 5.851722802942315,
      "grad_norm": 18.785959243774414,
      "learning_rate": 4.60919688561965e-06,
      "loss": 1.6967,
      "step": 15115
    },
    {
      "epoch": 5.8521099496709255,
      "grad_norm": 64.86454772949219,
      "learning_rate": 4.608766722587861e-06,
      "loss": 0.8501,
      "step": 15116
    },
    {
      "epoch": 5.852497096399535,
      "grad_norm": 21.548276901245117,
      "learning_rate": 4.608336559556072e-06,
      "loss": 1.5601,
      "step": 15117
    },
    {
      "epoch": 5.852884243128146,
      "grad_norm": 77.51476287841797,
      "learning_rate": 4.607906396524283e-06,
      "loss": 1.6781,
      "step": 15118
    },
    {
      "epoch": 5.853271389856756,
      "grad_norm": 16.890506744384766,
      "learning_rate": 4.607476233492494e-06,
      "loss": 0.3146,
      "step": 15119
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 46.93342208862305,
      "learning_rate": 4.607046070460705e-06,
      "loss": 1.3212,
      "step": 15120
    },
    {
      "epoch": 5.854045683313976,
      "grad_norm": 68.45045471191406,
      "learning_rate": 4.606615907428916e-06,
      "loss": 1.5004,
      "step": 15121
    },
    {
      "epoch": 5.854432830042586,
      "grad_norm": 101.34266662597656,
      "learning_rate": 4.606185744397127e-06,
      "loss": 0.44,
      "step": 15122
    },
    {
      "epoch": 5.854819976771196,
      "grad_norm": 32.439544677734375,
      "learning_rate": 4.605755581365338e-06,
      "loss": 0.5657,
      "step": 15123
    },
    {
      "epoch": 5.855207123499806,
      "grad_norm": 26.19647979736328,
      "learning_rate": 4.605325418333549e-06,
      "loss": 1.1192,
      "step": 15124
    },
    {
      "epoch": 5.8555942702284165,
      "grad_norm": 180.97006225585938,
      "learning_rate": 4.60489525530176e-06,
      "loss": 1.0148,
      "step": 15125
    },
    {
      "epoch": 5.855981416957027,
      "grad_norm": 22.22258949279785,
      "learning_rate": 4.604465092269971e-06,
      "loss": 1.5869,
      "step": 15126
    },
    {
      "epoch": 5.856368563685637,
      "grad_norm": 5.003754615783691,
      "learning_rate": 4.604034929238182e-06,
      "loss": 0.2482,
      "step": 15127
    },
    {
      "epoch": 5.856755710414247,
      "grad_norm": 73.4866714477539,
      "learning_rate": 4.603604766206392e-06,
      "loss": 0.8561,
      "step": 15128
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 135.58108520507812,
      "learning_rate": 4.603174603174604e-06,
      "loss": 1.8162,
      "step": 15129
    },
    {
      "epoch": 5.857530003871467,
      "grad_norm": 69.50720977783203,
      "learning_rate": 4.602744440142814e-06,
      "loss": 3.4204,
      "step": 15130
    },
    {
      "epoch": 5.857917150600078,
      "grad_norm": 39.70021438598633,
      "learning_rate": 4.602314277111025e-06,
      "loss": 1.0318,
      "step": 15131
    },
    {
      "epoch": 5.858304297328687,
      "grad_norm": 30.394210815429688,
      "learning_rate": 4.601884114079236e-06,
      "loss": 1.557,
      "step": 15132
    },
    {
      "epoch": 5.858691444057298,
      "grad_norm": 5.777876377105713,
      "learning_rate": 4.601453951047448e-06,
      "loss": 0.3198,
      "step": 15133
    },
    {
      "epoch": 5.8590785907859075,
      "grad_norm": 23.61164665222168,
      "learning_rate": 4.601023788015658e-06,
      "loss": 1.2044,
      "step": 15134
    },
    {
      "epoch": 5.859465737514518,
      "grad_norm": 106.04798126220703,
      "learning_rate": 4.60059362498387e-06,
      "loss": 2.0101,
      "step": 15135
    },
    {
      "epoch": 5.859852884243129,
      "grad_norm": 45.81056594848633,
      "learning_rate": 4.60016346195208e-06,
      "loss": 2.8967,
      "step": 15136
    },
    {
      "epoch": 5.860240030971738,
      "grad_norm": 66.86720275878906,
      "learning_rate": 4.599733298920292e-06,
      "loss": 0.5361,
      "step": 15137
    },
    {
      "epoch": 5.860627177700349,
      "grad_norm": 6.827519416809082,
      "learning_rate": 4.599303135888502e-06,
      "loss": 0.2972,
      "step": 15138
    },
    {
      "epoch": 5.861014324428958,
      "grad_norm": 26.835424423217773,
      "learning_rate": 4.598872972856714e-06,
      "loss": 2.0455,
      "step": 15139
    },
    {
      "epoch": 5.861401471157569,
      "grad_norm": 12.92543888092041,
      "learning_rate": 4.598442809824924e-06,
      "loss": 0.8212,
      "step": 15140
    },
    {
      "epoch": 5.861788617886178,
      "grad_norm": 7.46430778503418,
      "learning_rate": 4.598012646793136e-06,
      "loss": 0.2624,
      "step": 15141
    },
    {
      "epoch": 5.862175764614789,
      "grad_norm": 25.588171005249023,
      "learning_rate": 4.597582483761346e-06,
      "loss": 1.8699,
      "step": 15142
    },
    {
      "epoch": 5.8625629113433995,
      "grad_norm": 82.33021545410156,
      "learning_rate": 4.597152320729557e-06,
      "loss": 0.5441,
      "step": 15143
    },
    {
      "epoch": 5.862950058072009,
      "grad_norm": 37.52446746826172,
      "learning_rate": 4.596722157697768e-06,
      "loss": 1.0785,
      "step": 15144
    },
    {
      "epoch": 5.86333720480062,
      "grad_norm": 25.924997329711914,
      "learning_rate": 4.596291994665979e-06,
      "loss": 1.7542,
      "step": 15145
    },
    {
      "epoch": 5.863724351529229,
      "grad_norm": 36.31584167480469,
      "learning_rate": 4.595861831634189e-06,
      "loss": 1.0185,
      "step": 15146
    },
    {
      "epoch": 5.86411149825784,
      "grad_norm": 57.62786102294922,
      "learning_rate": 4.595431668602401e-06,
      "loss": 2.9887,
      "step": 15147
    },
    {
      "epoch": 5.86449864498645,
      "grad_norm": 6.079727649688721,
      "learning_rate": 4.595001505570611e-06,
      "loss": 0.1172,
      "step": 15148
    },
    {
      "epoch": 5.86488579171506,
      "grad_norm": 120.4942855834961,
      "learning_rate": 4.594571342538823e-06,
      "loss": 0.8741,
      "step": 15149
    },
    {
      "epoch": 5.86527293844367,
      "grad_norm": 15.678037643432617,
      "learning_rate": 4.594141179507033e-06,
      "loss": 0.2399,
      "step": 15150
    },
    {
      "epoch": 5.86566008517228,
      "grad_norm": 4.511541843414307,
      "learning_rate": 4.593711016475245e-06,
      "loss": 0.1094,
      "step": 15151
    },
    {
      "epoch": 5.8660472319008905,
      "grad_norm": 87.6231460571289,
      "learning_rate": 4.593280853443455e-06,
      "loss": 1.1807,
      "step": 15152
    },
    {
      "epoch": 5.866434378629501,
      "grad_norm": 55.73808670043945,
      "learning_rate": 4.592850690411667e-06,
      "loss": 1.1139,
      "step": 15153
    },
    {
      "epoch": 5.866821525358111,
      "grad_norm": 33.25760269165039,
      "learning_rate": 4.592420527379877e-06,
      "loss": 0.8811,
      "step": 15154
    },
    {
      "epoch": 5.867208672086721,
      "grad_norm": 6.7680439949035645,
      "learning_rate": 4.591990364348089e-06,
      "loss": 0.2875,
      "step": 15155
    },
    {
      "epoch": 5.867595818815331,
      "grad_norm": 37.026885986328125,
      "learning_rate": 4.591560201316299e-06,
      "loss": 1.5827,
      "step": 15156
    },
    {
      "epoch": 5.867982965543941,
      "grad_norm": 13.624672889709473,
      "learning_rate": 4.591130038284511e-06,
      "loss": 0.6951,
      "step": 15157
    },
    {
      "epoch": 5.868370112272551,
      "grad_norm": 63.39030075073242,
      "learning_rate": 4.590699875252721e-06,
      "loss": 4.0289,
      "step": 15158
    },
    {
      "epoch": 5.868757259001161,
      "grad_norm": 66.10295104980469,
      "learning_rate": 4.590269712220933e-06,
      "loss": 2.5727,
      "step": 15159
    },
    {
      "epoch": 5.869144405729772,
      "grad_norm": 110.04352569580078,
      "learning_rate": 4.589839549189143e-06,
      "loss": 2.1962,
      "step": 15160
    },
    {
      "epoch": 5.8695315524583815,
      "grad_norm": 24.799354553222656,
      "learning_rate": 4.589409386157354e-06,
      "loss": 1.2374,
      "step": 15161
    },
    {
      "epoch": 5.869918699186992,
      "grad_norm": 69.59187316894531,
      "learning_rate": 4.588979223125565e-06,
      "loss": 0.7537,
      "step": 15162
    },
    {
      "epoch": 5.870305845915602,
      "grad_norm": 18.421689987182617,
      "learning_rate": 4.588549060093776e-06,
      "loss": 0.2779,
      "step": 15163
    },
    {
      "epoch": 5.870692992644212,
      "grad_norm": 4.987075328826904,
      "learning_rate": 4.588118897061986e-06,
      "loss": 0.1671,
      "step": 15164
    },
    {
      "epoch": 5.871080139372822,
      "grad_norm": 7.228478908538818,
      "learning_rate": 4.587688734030198e-06,
      "loss": 0.2742,
      "step": 15165
    },
    {
      "epoch": 5.871467286101432,
      "grad_norm": 36.64631271362305,
      "learning_rate": 4.587258570998408e-06,
      "loss": 0.7334,
      "step": 15166
    },
    {
      "epoch": 5.871854432830043,
      "grad_norm": 30.2730655670166,
      "learning_rate": 4.58682840796662e-06,
      "loss": 1.8971,
      "step": 15167
    },
    {
      "epoch": 5.8722415795586524,
      "grad_norm": 100.73683166503906,
      "learning_rate": 4.58639824493483e-06,
      "loss": 1.4776,
      "step": 15168
    },
    {
      "epoch": 5.872628726287263,
      "grad_norm": 11.308968544006348,
      "learning_rate": 4.585968081903042e-06,
      "loss": 0.1571,
      "step": 15169
    },
    {
      "epoch": 5.8730158730158735,
      "grad_norm": 73.69752502441406,
      "learning_rate": 4.585537918871252e-06,
      "loss": 1.5814,
      "step": 15170
    },
    {
      "epoch": 5.873403019744483,
      "grad_norm": 34.68894958496094,
      "learning_rate": 4.585107755839464e-06,
      "loss": 1.3739,
      "step": 15171
    },
    {
      "epoch": 5.873790166473094,
      "grad_norm": 32.306976318359375,
      "learning_rate": 4.584677592807674e-06,
      "loss": 1.0928,
      "step": 15172
    },
    {
      "epoch": 5.874177313201703,
      "grad_norm": 29.979576110839844,
      "learning_rate": 4.584247429775886e-06,
      "loss": 1.4471,
      "step": 15173
    },
    {
      "epoch": 5.874564459930314,
      "grad_norm": 37.84362030029297,
      "learning_rate": 4.583817266744097e-06,
      "loss": 1.3311,
      "step": 15174
    },
    {
      "epoch": 5.874951606658923,
      "grad_norm": 45.8640022277832,
      "learning_rate": 4.583387103712308e-06,
      "loss": 1.4854,
      "step": 15175
    },
    {
      "epoch": 5.875338753387534,
      "grad_norm": 55.48806381225586,
      "learning_rate": 4.582956940680518e-06,
      "loss": 0.745,
      "step": 15176
    },
    {
      "epoch": 5.875725900116144,
      "grad_norm": 72.62171173095703,
      "learning_rate": 4.58252677764873e-06,
      "loss": 1.2979,
      "step": 15177
    },
    {
      "epoch": 5.876113046844754,
      "grad_norm": 15.64953327178955,
      "learning_rate": 4.58209661461694e-06,
      "loss": 0.1608,
      "step": 15178
    },
    {
      "epoch": 5.8765001935733645,
      "grad_norm": 54.0916748046875,
      "learning_rate": 4.581666451585151e-06,
      "loss": 2.1331,
      "step": 15179
    },
    {
      "epoch": 5.876887340301974,
      "grad_norm": 96.97240447998047,
      "learning_rate": 4.581236288553362e-06,
      "loss": 1.3975,
      "step": 15180
    },
    {
      "epoch": 5.877274487030585,
      "grad_norm": 37.15768051147461,
      "learning_rate": 4.580806125521573e-06,
      "loss": 0.4302,
      "step": 15181
    },
    {
      "epoch": 5.877661633759194,
      "grad_norm": 5.174729347229004,
      "learning_rate": 4.580375962489784e-06,
      "loss": 0.2158,
      "step": 15182
    },
    {
      "epoch": 5.878048780487805,
      "grad_norm": 31.034353256225586,
      "learning_rate": 4.579945799457995e-06,
      "loss": 2.8117,
      "step": 15183
    },
    {
      "epoch": 5.878435927216415,
      "grad_norm": 42.17618942260742,
      "learning_rate": 4.579515636426206e-06,
      "loss": 2.4289,
      "step": 15184
    },
    {
      "epoch": 5.878823073945025,
      "grad_norm": 33.2246208190918,
      "learning_rate": 4.579085473394417e-06,
      "loss": 0.6908,
      "step": 15185
    },
    {
      "epoch": 5.879210220673635,
      "grad_norm": 56.4744758605957,
      "learning_rate": 4.578655310362628e-06,
      "loss": 0.9087,
      "step": 15186
    },
    {
      "epoch": 5.879597367402246,
      "grad_norm": 31.77092170715332,
      "learning_rate": 4.578225147330839e-06,
      "loss": 0.6236,
      "step": 15187
    },
    {
      "epoch": 5.8799845141308555,
      "grad_norm": 58.31071090698242,
      "learning_rate": 4.57779498429905e-06,
      "loss": 2.3776,
      "step": 15188
    },
    {
      "epoch": 5.880371660859466,
      "grad_norm": 11.694584846496582,
      "learning_rate": 4.577364821267261e-06,
      "loss": 0.195,
      "step": 15189
    },
    {
      "epoch": 5.880758807588076,
      "grad_norm": 45.476192474365234,
      "learning_rate": 4.576934658235472e-06,
      "loss": 1.072,
      "step": 15190
    },
    {
      "epoch": 5.881145954316686,
      "grad_norm": 32.6490478515625,
      "learning_rate": 4.576504495203683e-06,
      "loss": 0.288,
      "step": 15191
    },
    {
      "epoch": 5.881533101045296,
      "grad_norm": 16.249013900756836,
      "learning_rate": 4.576074332171894e-06,
      "loss": 1.2618,
      "step": 15192
    },
    {
      "epoch": 5.881920247773906,
      "grad_norm": 74.21405792236328,
      "learning_rate": 4.575644169140105e-06,
      "loss": 3.0508,
      "step": 15193
    },
    {
      "epoch": 5.882307394502517,
      "grad_norm": 100.10893249511719,
      "learning_rate": 4.575214006108315e-06,
      "loss": 1.0837,
      "step": 15194
    },
    {
      "epoch": 5.8826945412311265,
      "grad_norm": 12.23733139038086,
      "learning_rate": 4.574783843076527e-06,
      "loss": 0.4747,
      "step": 15195
    },
    {
      "epoch": 5.883081687959737,
      "grad_norm": 52.4066276550293,
      "learning_rate": 4.574353680044737e-06,
      "loss": 1.4318,
      "step": 15196
    },
    {
      "epoch": 5.883468834688347,
      "grad_norm": 36.118141174316406,
      "learning_rate": 4.573923517012948e-06,
      "loss": 0.4517,
      "step": 15197
    },
    {
      "epoch": 5.883855981416957,
      "grad_norm": 24.164310455322266,
      "learning_rate": 4.573493353981159e-06,
      "loss": 0.7774,
      "step": 15198
    },
    {
      "epoch": 5.884243128145567,
      "grad_norm": 43.62172317504883,
      "learning_rate": 4.57306319094937e-06,
      "loss": 1.4012,
      "step": 15199
    },
    {
      "epoch": 5.884630274874177,
      "grad_norm": 52.44449996948242,
      "learning_rate": 4.572633027917581e-06,
      "loss": 1.121,
      "step": 15200
    },
    {
      "epoch": 5.885017421602788,
      "grad_norm": 45.179683685302734,
      "learning_rate": 4.572202864885792e-06,
      "loss": 0.4464,
      "step": 15201
    },
    {
      "epoch": 5.885404568331397,
      "grad_norm": 5.499342441558838,
      "learning_rate": 4.571772701854003e-06,
      "loss": 0.3364,
      "step": 15202
    },
    {
      "epoch": 5.885791715060008,
      "grad_norm": 60.762020111083984,
      "learning_rate": 4.571342538822214e-06,
      "loss": 1.468,
      "step": 15203
    },
    {
      "epoch": 5.886178861788618,
      "grad_norm": 48.197479248046875,
      "learning_rate": 4.570912375790425e-06,
      "loss": 1.9947,
      "step": 15204
    },
    {
      "epoch": 5.886566008517228,
      "grad_norm": 31.852636337280273,
      "learning_rate": 4.570482212758636e-06,
      "loss": 1.5492,
      "step": 15205
    },
    {
      "epoch": 5.8869531552458385,
      "grad_norm": 24.100257873535156,
      "learning_rate": 4.570052049726847e-06,
      "loss": 1.5204,
      "step": 15206
    },
    {
      "epoch": 5.887340301974448,
      "grad_norm": 32.95353317260742,
      "learning_rate": 4.569621886695058e-06,
      "loss": 0.698,
      "step": 15207
    },
    {
      "epoch": 5.887727448703059,
      "grad_norm": 26.962303161621094,
      "learning_rate": 4.569191723663269e-06,
      "loss": 0.3479,
      "step": 15208
    },
    {
      "epoch": 5.888114595431668,
      "grad_norm": 14.092218399047852,
      "learning_rate": 4.56876156063148e-06,
      "loss": 0.2667,
      "step": 15209
    },
    {
      "epoch": 5.888501742160279,
      "grad_norm": 0.6807346343994141,
      "learning_rate": 4.568331397599691e-06,
      "loss": 0.0192,
      "step": 15210
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 5.5617218017578125,
      "learning_rate": 4.567901234567902e-06,
      "loss": 0.2649,
      "step": 15211
    },
    {
      "epoch": 5.889276035617499,
      "grad_norm": 25.686141967773438,
      "learning_rate": 4.567471071536112e-06,
      "loss": 2.3773,
      "step": 15212
    },
    {
      "epoch": 5.889663182346109,
      "grad_norm": 26.3760929107666,
      "learning_rate": 4.567040908504324e-06,
      "loss": 1.4089,
      "step": 15213
    },
    {
      "epoch": 5.890050329074719,
      "grad_norm": 54.14488220214844,
      "learning_rate": 4.566610745472534e-06,
      "loss": 0.3108,
      "step": 15214
    },
    {
      "epoch": 5.8904374758033295,
      "grad_norm": 40.71644973754883,
      "learning_rate": 4.566180582440746e-06,
      "loss": 0.7293,
      "step": 15215
    },
    {
      "epoch": 5.890824622531939,
      "grad_norm": 27.014129638671875,
      "learning_rate": 4.565750419408956e-06,
      "loss": 0.2338,
      "step": 15216
    },
    {
      "epoch": 5.89121176926055,
      "grad_norm": 69.68661499023438,
      "learning_rate": 4.565320256377168e-06,
      "loss": 0.72,
      "step": 15217
    },
    {
      "epoch": 5.89159891598916,
      "grad_norm": 47.75868225097656,
      "learning_rate": 4.564890093345378e-06,
      "loss": 1.7692,
      "step": 15218
    },
    {
      "epoch": 5.89198606271777,
      "grad_norm": 63.281105041503906,
      "learning_rate": 4.56445993031359e-06,
      "loss": 2.1761,
      "step": 15219
    },
    {
      "epoch": 5.89237320944638,
      "grad_norm": 12.095536231994629,
      "learning_rate": 4.5640297672818e-06,
      "loss": 0.1432,
      "step": 15220
    },
    {
      "epoch": 5.89276035617499,
      "grad_norm": 4.513026714324951,
      "learning_rate": 4.563599604250012e-06,
      "loss": 0.1687,
      "step": 15221
    },
    {
      "epoch": 5.8931475029036005,
      "grad_norm": 73.9609603881836,
      "learning_rate": 4.563169441218222e-06,
      "loss": 1.0004,
      "step": 15222
    },
    {
      "epoch": 5.893534649632211,
      "grad_norm": 71.36509704589844,
      "learning_rate": 4.5627392781864336e-06,
      "loss": 1.5998,
      "step": 15223
    },
    {
      "epoch": 5.893921796360821,
      "grad_norm": 76.0006332397461,
      "learning_rate": 4.562309115154644e-06,
      "loss": 3.1799,
      "step": 15224
    },
    {
      "epoch": 5.894308943089431,
      "grad_norm": 36.3302001953125,
      "learning_rate": 4.5618789521228556e-06,
      "loss": 2.7324,
      "step": 15225
    },
    {
      "epoch": 5.894696089818041,
      "grad_norm": 59.610023498535156,
      "learning_rate": 4.561448789091066e-06,
      "loss": 3.7366,
      "step": 15226
    },
    {
      "epoch": 5.895083236546651,
      "grad_norm": 98.1850814819336,
      "learning_rate": 4.561018626059277e-06,
      "loss": 1.7329,
      "step": 15227
    },
    {
      "epoch": 5.895470383275262,
      "grad_norm": 52.51924514770508,
      "learning_rate": 4.560588463027488e-06,
      "loss": 1.8244,
      "step": 15228
    },
    {
      "epoch": 5.895857530003871,
      "grad_norm": 60.00777053833008,
      "learning_rate": 4.560158299995699e-06,
      "loss": 1.3728,
      "step": 15229
    },
    {
      "epoch": 5.896244676732482,
      "grad_norm": 58.20399856567383,
      "learning_rate": 4.559728136963909e-06,
      "loss": 2.8848,
      "step": 15230
    },
    {
      "epoch": 5.8966318234610915,
      "grad_norm": 49.233089447021484,
      "learning_rate": 4.559297973932121e-06,
      "loss": 1.0473,
      "step": 15231
    },
    {
      "epoch": 5.897018970189702,
      "grad_norm": 39.62043762207031,
      "learning_rate": 4.558867810900331e-06,
      "loss": 1.312,
      "step": 15232
    },
    {
      "epoch": 5.897406116918312,
      "grad_norm": 58.24740219116211,
      "learning_rate": 4.558437647868543e-06,
      "loss": 1.5618,
      "step": 15233
    },
    {
      "epoch": 5.897793263646922,
      "grad_norm": 9.884353637695312,
      "learning_rate": 4.558007484836753e-06,
      "loss": 0.3843,
      "step": 15234
    },
    {
      "epoch": 5.898180410375533,
      "grad_norm": 34.026458740234375,
      "learning_rate": 4.557577321804965e-06,
      "loss": 1.6255,
      "step": 15235
    },
    {
      "epoch": 5.898567557104142,
      "grad_norm": 26.794612884521484,
      "learning_rate": 4.557147158773175e-06,
      "loss": 1.9243,
      "step": 15236
    },
    {
      "epoch": 5.898954703832753,
      "grad_norm": 60.882450103759766,
      "learning_rate": 4.556716995741387e-06,
      "loss": 0.9132,
      "step": 15237
    },
    {
      "epoch": 5.899341850561362,
      "grad_norm": 17.931062698364258,
      "learning_rate": 4.556286832709597e-06,
      "loss": 0.2878,
      "step": 15238
    },
    {
      "epoch": 5.899728997289973,
      "grad_norm": 60.63392639160156,
      "learning_rate": 4.555856669677809e-06,
      "loss": 1.9327,
      "step": 15239
    },
    {
      "epoch": 5.900116144018583,
      "grad_norm": 241.0397491455078,
      "learning_rate": 4.555426506646019e-06,
      "loss": 1.5799,
      "step": 15240
    },
    {
      "epoch": 5.900503290747193,
      "grad_norm": 133.6416778564453,
      "learning_rate": 4.554996343614231e-06,
      "loss": 1.1295,
      "step": 15241
    },
    {
      "epoch": 5.9008904374758036,
      "grad_norm": 35.11600875854492,
      "learning_rate": 4.554566180582441e-06,
      "loss": 2.8793,
      "step": 15242
    },
    {
      "epoch": 5.901277584204413,
      "grad_norm": 19.986228942871094,
      "learning_rate": 4.5541360175506526e-06,
      "loss": 1.3477,
      "step": 15243
    },
    {
      "epoch": 5.901664730933024,
      "grad_norm": 34.30937194824219,
      "learning_rate": 4.553705854518863e-06,
      "loss": 3.8288,
      "step": 15244
    },
    {
      "epoch": 5.902051877661634,
      "grad_norm": 57.570396423339844,
      "learning_rate": 4.553275691487074e-06,
      "loss": 0.9564,
      "step": 15245
    },
    {
      "epoch": 5.902439024390244,
      "grad_norm": 33.67228317260742,
      "learning_rate": 4.552845528455285e-06,
      "loss": 1.7217,
      "step": 15246
    },
    {
      "epoch": 5.902826171118854,
      "grad_norm": 5.391868591308594,
      "learning_rate": 4.552415365423496e-06,
      "loss": 0.1775,
      "step": 15247
    },
    {
      "epoch": 5.903213317847464,
      "grad_norm": 38.686397552490234,
      "learning_rate": 4.551985202391706e-06,
      "loss": 0.8396,
      "step": 15248
    },
    {
      "epoch": 5.9036004645760745,
      "grad_norm": 146.43310546875,
      "learning_rate": 4.551555039359918e-06,
      "loss": 1.447,
      "step": 15249
    },
    {
      "epoch": 5.903987611304684,
      "grad_norm": 35.39144515991211,
      "learning_rate": 4.551124876328128e-06,
      "loss": 0.7199,
      "step": 15250
    },
    {
      "epoch": 5.904374758033295,
      "grad_norm": 93.70054626464844,
      "learning_rate": 4.55069471329634e-06,
      "loss": 1.0184,
      "step": 15251
    },
    {
      "epoch": 5.904761904761905,
      "grad_norm": 4.956722736358643,
      "learning_rate": 4.55026455026455e-06,
      "loss": 0.2119,
      "step": 15252
    },
    {
      "epoch": 5.905149051490515,
      "grad_norm": 50.63651657104492,
      "learning_rate": 4.549834387232762e-06,
      "loss": 2.5456,
      "step": 15253
    },
    {
      "epoch": 5.905536198219125,
      "grad_norm": 8.217775344848633,
      "learning_rate": 4.549404224200972e-06,
      "loss": 0.3269,
      "step": 15254
    },
    {
      "epoch": 5.905923344947735,
      "grad_norm": 60.81013107299805,
      "learning_rate": 4.548974061169184e-06,
      "loss": 1.1288,
      "step": 15255
    },
    {
      "epoch": 5.906310491676345,
      "grad_norm": 5.642701148986816,
      "learning_rate": 4.548543898137395e-06,
      "loss": 0.2404,
      "step": 15256
    },
    {
      "epoch": 5.906697638404955,
      "grad_norm": 71.10366821289062,
      "learning_rate": 4.548113735105606e-06,
      "loss": 1.8257,
      "step": 15257
    },
    {
      "epoch": 5.9070847851335655,
      "grad_norm": 39.4312629699707,
      "learning_rate": 4.547683572073817e-06,
      "loss": 0.4991,
      "step": 15258
    },
    {
      "epoch": 5.907471931862176,
      "grad_norm": 17.470434188842773,
      "learning_rate": 4.547253409042028e-06,
      "loss": 1.3025,
      "step": 15259
    },
    {
      "epoch": 5.907859078590786,
      "grad_norm": 46.633426666259766,
      "learning_rate": 4.546823246010238e-06,
      "loss": 1.7297,
      "step": 15260
    },
    {
      "epoch": 5.908246225319396,
      "grad_norm": 117.6251449584961,
      "learning_rate": 4.54639308297845e-06,
      "loss": 0.7072,
      "step": 15261
    },
    {
      "epoch": 5.908633372048007,
      "grad_norm": 16.667787551879883,
      "learning_rate": 4.54596291994666e-06,
      "loss": 0.7367,
      "step": 15262
    },
    {
      "epoch": 5.909020518776616,
      "grad_norm": 6.46171760559082,
      "learning_rate": 4.545532756914871e-06,
      "loss": 0.3564,
      "step": 15263
    },
    {
      "epoch": 5.909407665505227,
      "grad_norm": 3.3294544219970703,
      "learning_rate": 4.545102593883082e-06,
      "loss": 0.1443,
      "step": 15264
    },
    {
      "epoch": 5.909794812233836,
      "grad_norm": 56.42099380493164,
      "learning_rate": 4.544672430851293e-06,
      "loss": 2.6525,
      "step": 15265
    },
    {
      "epoch": 5.910181958962447,
      "grad_norm": 26.290929794311523,
      "learning_rate": 4.544242267819504e-06,
      "loss": 2.4038,
      "step": 15266
    },
    {
      "epoch": 5.9105691056910565,
      "grad_norm": 24.85547637939453,
      "learning_rate": 4.543812104787715e-06,
      "loss": 0.8363,
      "step": 15267
    },
    {
      "epoch": 5.910956252419667,
      "grad_norm": 71.93173217773438,
      "learning_rate": 4.543381941755926e-06,
      "loss": 0.5671,
      "step": 15268
    },
    {
      "epoch": 5.9113433991482776,
      "grad_norm": 53.523399353027344,
      "learning_rate": 4.542951778724137e-06,
      "loss": 0.721,
      "step": 15269
    },
    {
      "epoch": 5.911730545876887,
      "grad_norm": 36.14781951904297,
      "learning_rate": 4.542521615692348e-06,
      "loss": 0.5694,
      "step": 15270
    },
    {
      "epoch": 5.912117692605498,
      "grad_norm": 31.5479793548584,
      "learning_rate": 4.542091452660559e-06,
      "loss": 0.2167,
      "step": 15271
    },
    {
      "epoch": 5.912504839334107,
      "grad_norm": 45.6107177734375,
      "learning_rate": 4.54166128962877e-06,
      "loss": 1.7644,
      "step": 15272
    },
    {
      "epoch": 5.912891986062718,
      "grad_norm": 39.82576370239258,
      "learning_rate": 4.541231126596981e-06,
      "loss": 1.1025,
      "step": 15273
    },
    {
      "epoch": 5.913279132791327,
      "grad_norm": 61.06539535522461,
      "learning_rate": 4.540800963565192e-06,
      "loss": 3.6872,
      "step": 15274
    },
    {
      "epoch": 5.913666279519938,
      "grad_norm": 30.677989959716797,
      "learning_rate": 4.540370800533403e-06,
      "loss": 1.4512,
      "step": 15275
    },
    {
      "epoch": 5.9140534262485485,
      "grad_norm": 10.256524085998535,
      "learning_rate": 4.539940637501614e-06,
      "loss": 0.1974,
      "step": 15276
    },
    {
      "epoch": 5.914440572977158,
      "grad_norm": 5.003480911254883,
      "learning_rate": 4.539510474469825e-06,
      "loss": 0.2885,
      "step": 15277
    },
    {
      "epoch": 5.914827719705769,
      "grad_norm": 69.65090942382812,
      "learning_rate": 4.539080311438035e-06,
      "loss": 1.7674,
      "step": 15278
    },
    {
      "epoch": 5.915214866434379,
      "grad_norm": 12.772940635681152,
      "learning_rate": 4.538650148406247e-06,
      "loss": 0.713,
      "step": 15279
    },
    {
      "epoch": 5.915602013162989,
      "grad_norm": 42.074180603027344,
      "learning_rate": 4.538219985374457e-06,
      "loss": 0.2996,
      "step": 15280
    },
    {
      "epoch": 5.915989159891599,
      "grad_norm": 25.66877555847168,
      "learning_rate": 4.537789822342668e-06,
      "loss": 1.4613,
      "step": 15281
    },
    {
      "epoch": 5.916376306620209,
      "grad_norm": 59.28794479370117,
      "learning_rate": 4.537359659310879e-06,
      "loss": 1.0479,
      "step": 15282
    },
    {
      "epoch": 5.916763453348819,
      "grad_norm": 69.6856918334961,
      "learning_rate": 4.53692949627909e-06,
      "loss": 2.7913,
      "step": 15283
    },
    {
      "epoch": 5.917150600077429,
      "grad_norm": 52.262821197509766,
      "learning_rate": 4.536499333247301e-06,
      "loss": 1.506,
      "step": 15284
    },
    {
      "epoch": 5.9175377468060395,
      "grad_norm": 6.317698955535889,
      "learning_rate": 4.536069170215512e-06,
      "loss": 0.2814,
      "step": 15285
    },
    {
      "epoch": 5.91792489353465,
      "grad_norm": 44.54747009277344,
      "learning_rate": 4.535639007183723e-06,
      "loss": 3.4151,
      "step": 15286
    },
    {
      "epoch": 5.91831204026326,
      "grad_norm": 22.810096740722656,
      "learning_rate": 4.535208844151934e-06,
      "loss": 0.2685,
      "step": 15287
    },
    {
      "epoch": 5.91869918699187,
      "grad_norm": 29.44594955444336,
      "learning_rate": 4.534778681120145e-06,
      "loss": 1.324,
      "step": 15288
    },
    {
      "epoch": 5.91908633372048,
      "grad_norm": 56.8134765625,
      "learning_rate": 4.534348518088356e-06,
      "loss": 0.623,
      "step": 15289
    },
    {
      "epoch": 5.91947348044909,
      "grad_norm": 22.918720245361328,
      "learning_rate": 4.533918355056567e-06,
      "loss": 1.4161,
      "step": 15290
    },
    {
      "epoch": 5.9198606271777,
      "grad_norm": 109.63593292236328,
      "learning_rate": 4.533488192024778e-06,
      "loss": 2.1924,
      "step": 15291
    },
    {
      "epoch": 5.92024777390631,
      "grad_norm": 33.56972885131836,
      "learning_rate": 4.533058028992989e-06,
      "loss": 1.6032,
      "step": 15292
    },
    {
      "epoch": 5.920634920634921,
      "grad_norm": 36.996437072753906,
      "learning_rate": 4.5326278659612e-06,
      "loss": 0.5133,
      "step": 15293
    },
    {
      "epoch": 5.9210220673635305,
      "grad_norm": 84.80159759521484,
      "learning_rate": 4.532197702929411e-06,
      "loss": 0.5772,
      "step": 15294
    },
    {
      "epoch": 5.921409214092141,
      "grad_norm": 7.422123908996582,
      "learning_rate": 4.531767539897622e-06,
      "loss": 0.2048,
      "step": 15295
    },
    {
      "epoch": 5.921796360820752,
      "grad_norm": 3.0024614334106445,
      "learning_rate": 4.531337376865832e-06,
      "loss": 0.1227,
      "step": 15296
    },
    {
      "epoch": 5.922183507549361,
      "grad_norm": 5.808695316314697,
      "learning_rate": 4.530907213834044e-06,
      "loss": 0.3036,
      "step": 15297
    },
    {
      "epoch": 5.922570654277972,
      "grad_norm": 16.128307342529297,
      "learning_rate": 4.530477050802254e-06,
      "loss": 0.4607,
      "step": 15298
    },
    {
      "epoch": 5.922957801006581,
      "grad_norm": 138.39219665527344,
      "learning_rate": 4.530046887770466e-06,
      "loss": 2.3091,
      "step": 15299
    },
    {
      "epoch": 5.923344947735192,
      "grad_norm": 59.68753433227539,
      "learning_rate": 4.529616724738676e-06,
      "loss": 1.4717,
      "step": 15300
    },
    {
      "epoch": 5.9237320944638014,
      "grad_norm": 6.690108299255371,
      "learning_rate": 4.5291865617068876e-06,
      "loss": 0.1864,
      "step": 15301
    },
    {
      "epoch": 5.924119241192412,
      "grad_norm": 34.58673095703125,
      "learning_rate": 4.528756398675098e-06,
      "loss": 2.0633,
      "step": 15302
    },
    {
      "epoch": 5.9245063879210225,
      "grad_norm": 77.93680572509766,
      "learning_rate": 4.5283262356433096e-06,
      "loss": 0.8655,
      "step": 15303
    },
    {
      "epoch": 5.924893534649632,
      "grad_norm": 109.62676239013672,
      "learning_rate": 4.52789607261152e-06,
      "loss": 1.6312,
      "step": 15304
    },
    {
      "epoch": 5.925280681378243,
      "grad_norm": 6.203495502471924,
      "learning_rate": 4.5274659095797315e-06,
      "loss": 0.1619,
      "step": 15305
    },
    {
      "epoch": 5.925667828106852,
      "grad_norm": 45.1260871887207,
      "learning_rate": 4.527035746547942e-06,
      "loss": 1.7735,
      "step": 15306
    },
    {
      "epoch": 5.926054974835463,
      "grad_norm": 60.94202423095703,
      "learning_rate": 4.5266055835161535e-06,
      "loss": 0.9365,
      "step": 15307
    },
    {
      "epoch": 5.926442121564072,
      "grad_norm": 25.63205909729004,
      "learning_rate": 4.526175420484364e-06,
      "loss": 0.7801,
      "step": 15308
    },
    {
      "epoch": 5.926829268292683,
      "grad_norm": 28.641403198242188,
      "learning_rate": 4.5257452574525755e-06,
      "loss": 1.1231,
      "step": 15309
    },
    {
      "epoch": 5.927216415021293,
      "grad_norm": 24.8853702545166,
      "learning_rate": 4.525315094420786e-06,
      "loss": 0.928,
      "step": 15310
    },
    {
      "epoch": 5.927603561749903,
      "grad_norm": 67.4123764038086,
      "learning_rate": 4.524884931388997e-06,
      "loss": 1.9568,
      "step": 15311
    },
    {
      "epoch": 5.9279907084785135,
      "grad_norm": 3.4251325130462646,
      "learning_rate": 4.524454768357208e-06,
      "loss": 0.1012,
      "step": 15312
    },
    {
      "epoch": 5.928377855207123,
      "grad_norm": 7.204189777374268,
      "learning_rate": 4.524024605325419e-06,
      "loss": 0.1511,
      "step": 15313
    },
    {
      "epoch": 5.928765001935734,
      "grad_norm": 60.9356575012207,
      "learning_rate": 4.523594442293629e-06,
      "loss": 1.2907,
      "step": 15314
    },
    {
      "epoch": 5.929152148664344,
      "grad_norm": 6.982901573181152,
      "learning_rate": 4.523164279261841e-06,
      "loss": 0.3147,
      "step": 15315
    },
    {
      "epoch": 5.929539295392954,
      "grad_norm": 79.64322662353516,
      "learning_rate": 4.522734116230051e-06,
      "loss": 1.2462,
      "step": 15316
    },
    {
      "epoch": 5.929926442121564,
      "grad_norm": 18.552717208862305,
      "learning_rate": 4.522303953198263e-06,
      "loss": 0.9686,
      "step": 15317
    },
    {
      "epoch": 5.930313588850174,
      "grad_norm": 49.62961959838867,
      "learning_rate": 4.521873790166473e-06,
      "loss": 0.4752,
      "step": 15318
    },
    {
      "epoch": 5.930700735578784,
      "grad_norm": 143.4729766845703,
      "learning_rate": 4.521443627134685e-06,
      "loss": 0.8709,
      "step": 15319
    },
    {
      "epoch": 5.931087882307395,
      "grad_norm": 26.88014793395996,
      "learning_rate": 4.521013464102895e-06,
      "loss": 1.5648,
      "step": 15320
    },
    {
      "epoch": 5.9314750290360045,
      "grad_norm": 4.431222438812256,
      "learning_rate": 4.5205833010711066e-06,
      "loss": 0.2963,
      "step": 15321
    },
    {
      "epoch": 5.931862175764615,
      "grad_norm": 45.02202606201172,
      "learning_rate": 4.520153138039317e-06,
      "loss": 2.1345,
      "step": 15322
    },
    {
      "epoch": 5.932249322493225,
      "grad_norm": 40.47221374511719,
      "learning_rate": 4.5197229750075286e-06,
      "loss": 1.6096,
      "step": 15323
    },
    {
      "epoch": 5.932636469221835,
      "grad_norm": 54.6901741027832,
      "learning_rate": 4.519292811975739e-06,
      "loss": 1.3084,
      "step": 15324
    },
    {
      "epoch": 5.933023615950445,
      "grad_norm": 79.88023376464844,
      "learning_rate": 4.5188626489439505e-06,
      "loss": 0.9563,
      "step": 15325
    },
    {
      "epoch": 5.933410762679055,
      "grad_norm": 49.75674819946289,
      "learning_rate": 4.518432485912161e-06,
      "loss": 0.5975,
      "step": 15326
    },
    {
      "epoch": 5.933797909407666,
      "grad_norm": 18.179737091064453,
      "learning_rate": 4.5180023228803725e-06,
      "loss": 0.9687,
      "step": 15327
    },
    {
      "epoch": 5.9341850561362754,
      "grad_norm": 64.83567810058594,
      "learning_rate": 4.517572159848583e-06,
      "loss": 4.2826,
      "step": 15328
    },
    {
      "epoch": 5.934572202864886,
      "grad_norm": 20.008333206176758,
      "learning_rate": 4.517141996816794e-06,
      "loss": 0.3135,
      "step": 15329
    },
    {
      "epoch": 5.934959349593496,
      "grad_norm": 42.371891021728516,
      "learning_rate": 4.516711833785005e-06,
      "loss": 0.5346,
      "step": 15330
    },
    {
      "epoch": 5.935346496322106,
      "grad_norm": 10.874188423156738,
      "learning_rate": 4.516281670753216e-06,
      "loss": 0.2639,
      "step": 15331
    },
    {
      "epoch": 5.935733643050717,
      "grad_norm": 86.86722564697266,
      "learning_rate": 4.515851507721426e-06,
      "loss": 0.9165,
      "step": 15332
    },
    {
      "epoch": 5.936120789779326,
      "grad_norm": 180.41363525390625,
      "learning_rate": 4.515421344689638e-06,
      "loss": 1.7983,
      "step": 15333
    },
    {
      "epoch": 5.936507936507937,
      "grad_norm": 23.44666862487793,
      "learning_rate": 4.514991181657848e-06,
      "loss": 0.1827,
      "step": 15334
    },
    {
      "epoch": 5.936895083236546,
      "grad_norm": 68.54784393310547,
      "learning_rate": 4.51456101862606e-06,
      "loss": 1.0866,
      "step": 15335
    },
    {
      "epoch": 5.937282229965157,
      "grad_norm": 0.7804188132286072,
      "learning_rate": 4.51413085559427e-06,
      "loss": 0.0174,
      "step": 15336
    },
    {
      "epoch": 5.937669376693767,
      "grad_norm": 32.171871185302734,
      "learning_rate": 4.513700692562482e-06,
      "loss": 1.1612,
      "step": 15337
    },
    {
      "epoch": 5.938056523422377,
      "grad_norm": 6.128636837005615,
      "learning_rate": 4.513270529530693e-06,
      "loss": 0.2492,
      "step": 15338
    },
    {
      "epoch": 5.9384436701509875,
      "grad_norm": 8.605002403259277,
      "learning_rate": 4.512840366498904e-06,
      "loss": 0.1135,
      "step": 15339
    },
    {
      "epoch": 5.938830816879597,
      "grad_norm": 361.4053039550781,
      "learning_rate": 4.512410203467115e-06,
      "loss": 3.0681,
      "step": 15340
    },
    {
      "epoch": 5.939217963608208,
      "grad_norm": 71.60553741455078,
      "learning_rate": 4.5119800404353256e-06,
      "loss": 1.4637,
      "step": 15341
    },
    {
      "epoch": 5.939605110336817,
      "grad_norm": 135.00027465820312,
      "learning_rate": 4.511549877403536e-06,
      "loss": 1.9003,
      "step": 15342
    },
    {
      "epoch": 5.939992257065428,
      "grad_norm": 40.556209564208984,
      "learning_rate": 4.5111197143717476e-06,
      "loss": 1.1617,
      "step": 15343
    },
    {
      "epoch": 5.940379403794038,
      "grad_norm": 4.972743034362793,
      "learning_rate": 4.510689551339958e-06,
      "loss": 0.3775,
      "step": 15344
    },
    {
      "epoch": 5.940766550522648,
      "grad_norm": 33.412384033203125,
      "learning_rate": 4.5102593883081695e-06,
      "loss": 0.349,
      "step": 15345
    },
    {
      "epoch": 5.941153697251258,
      "grad_norm": 60.77016830444336,
      "learning_rate": 4.50982922527638e-06,
      "loss": 0.3818,
      "step": 15346
    },
    {
      "epoch": 5.941540843979868,
      "grad_norm": 34.47350311279297,
      "learning_rate": 4.509399062244591e-06,
      "loss": 1.4807,
      "step": 15347
    },
    {
      "epoch": 5.9419279907084785,
      "grad_norm": 119.91551208496094,
      "learning_rate": 4.508968899212802e-06,
      "loss": 0.9251,
      "step": 15348
    },
    {
      "epoch": 5.942315137437088,
      "grad_norm": 57.102542877197266,
      "learning_rate": 4.508538736181013e-06,
      "loss": 1.6039,
      "step": 15349
    },
    {
      "epoch": 5.942702284165699,
      "grad_norm": 18.097097396850586,
      "learning_rate": 4.508108573149224e-06,
      "loss": 0.3834,
      "step": 15350
    },
    {
      "epoch": 5.943089430894309,
      "grad_norm": 100.2560806274414,
      "learning_rate": 4.507678410117435e-06,
      "loss": 0.939,
      "step": 15351
    },
    {
      "epoch": 5.943476577622919,
      "grad_norm": 48.38040542602539,
      "learning_rate": 4.507248247085646e-06,
      "loss": 0.8974,
      "step": 15352
    },
    {
      "epoch": 5.943863724351529,
      "grad_norm": 108.9499740600586,
      "learning_rate": 4.506818084053857e-06,
      "loss": 0.1734,
      "step": 15353
    },
    {
      "epoch": 5.94425087108014,
      "grad_norm": 78.24229431152344,
      "learning_rate": 4.506387921022068e-06,
      "loss": 2.0925,
      "step": 15354
    },
    {
      "epoch": 5.9446380178087495,
      "grad_norm": 31.517715454101562,
      "learning_rate": 4.505957757990279e-06,
      "loss": 1.4038,
      "step": 15355
    },
    {
      "epoch": 5.94502516453736,
      "grad_norm": 47.66562271118164,
      "learning_rate": 4.50552759495849e-06,
      "loss": 3.1292,
      "step": 15356
    },
    {
      "epoch": 5.94541231126597,
      "grad_norm": 40.760597229003906,
      "learning_rate": 4.505097431926701e-06,
      "loss": 0.3274,
      "step": 15357
    },
    {
      "epoch": 5.94579945799458,
      "grad_norm": 78.92457580566406,
      "learning_rate": 4.504667268894912e-06,
      "loss": 1.1006,
      "step": 15358
    },
    {
      "epoch": 5.94618660472319,
      "grad_norm": 94.78643035888672,
      "learning_rate": 4.504237105863123e-06,
      "loss": 1.5535,
      "step": 15359
    },
    {
      "epoch": 5.9465737514518,
      "grad_norm": 2.388032913208008,
      "learning_rate": 4.503806942831333e-06,
      "loss": 0.0936,
      "step": 15360
    },
    {
      "epoch": 5.946960898180411,
      "grad_norm": 75.9713134765625,
      "learning_rate": 4.5033767797995446e-06,
      "loss": 1.893,
      "step": 15361
    },
    {
      "epoch": 5.94734804490902,
      "grad_norm": 36.057613372802734,
      "learning_rate": 4.502946616767755e-06,
      "loss": 1.341,
      "step": 15362
    },
    {
      "epoch": 5.947735191637631,
      "grad_norm": 29.92222785949707,
      "learning_rate": 4.5025164537359666e-06,
      "loss": 1.3138,
      "step": 15363
    },
    {
      "epoch": 5.9481223383662405,
      "grad_norm": 45.47874450683594,
      "learning_rate": 4.502086290704177e-06,
      "loss": 0.5496,
      "step": 15364
    },
    {
      "epoch": 5.948509485094851,
      "grad_norm": 6.942070960998535,
      "learning_rate": 4.501656127672388e-06,
      "loss": 0.3007,
      "step": 15365
    },
    {
      "epoch": 5.948896631823461,
      "grad_norm": 5.300240993499756,
      "learning_rate": 4.501225964640599e-06,
      "loss": 0.2268,
      "step": 15366
    },
    {
      "epoch": 5.949283778552071,
      "grad_norm": 16.432384490966797,
      "learning_rate": 4.50079580160881e-06,
      "loss": 0.5535,
      "step": 15367
    },
    {
      "epoch": 5.949670925280682,
      "grad_norm": 31.226673126220703,
      "learning_rate": 4.500365638577021e-06,
      "loss": 1.2011,
      "step": 15368
    },
    {
      "epoch": 5.950058072009291,
      "grad_norm": 61.63416290283203,
      "learning_rate": 4.499935475545232e-06,
      "loss": 2.2013,
      "step": 15369
    },
    {
      "epoch": 5.950445218737902,
      "grad_norm": 19.85933494567871,
      "learning_rate": 4.499505312513443e-06,
      "loss": 0.337,
      "step": 15370
    },
    {
      "epoch": 5.950832365466512,
      "grad_norm": 4.740946292877197,
      "learning_rate": 4.499075149481654e-06,
      "loss": 0.1204,
      "step": 15371
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 25.694772720336914,
      "learning_rate": 4.498644986449865e-06,
      "loss": 1.5534,
      "step": 15372
    },
    {
      "epoch": 5.951606658923732,
      "grad_norm": 14.646687507629395,
      "learning_rate": 4.498214823418076e-06,
      "loss": 0.4618,
      "step": 15373
    },
    {
      "epoch": 5.951993805652342,
      "grad_norm": 53.78346633911133,
      "learning_rate": 4.497784660386287e-06,
      "loss": 2.2438,
      "step": 15374
    },
    {
      "epoch": 5.9523809523809526,
      "grad_norm": 38.105186462402344,
      "learning_rate": 4.497354497354498e-06,
      "loss": 1.1407,
      "step": 15375
    },
    {
      "epoch": 5.952768099109562,
      "grad_norm": 92.96403503417969,
      "learning_rate": 4.496924334322709e-06,
      "loss": 0.788,
      "step": 15376
    },
    {
      "epoch": 5.953155245838173,
      "grad_norm": 3.6706597805023193,
      "learning_rate": 4.49649417129092e-06,
      "loss": 0.1484,
      "step": 15377
    },
    {
      "epoch": 5.953542392566783,
      "grad_norm": 18.398530960083008,
      "learning_rate": 4.49606400825913e-06,
      "loss": 1.6951,
      "step": 15378
    },
    {
      "epoch": 5.953929539295393,
      "grad_norm": 55.62180709838867,
      "learning_rate": 4.495633845227342e-06,
      "loss": 1.4671,
      "step": 15379
    },
    {
      "epoch": 5.954316686024003,
      "grad_norm": 58.341060638427734,
      "learning_rate": 4.495203682195552e-06,
      "loss": 1.6503,
      "step": 15380
    },
    {
      "epoch": 5.954703832752613,
      "grad_norm": 4.242222785949707,
      "learning_rate": 4.4947735191637636e-06,
      "loss": 0.19,
      "step": 15381
    },
    {
      "epoch": 5.9550909794812235,
      "grad_norm": 11.908989906311035,
      "learning_rate": 4.494343356131974e-06,
      "loss": 0.1922,
      "step": 15382
    },
    {
      "epoch": 5.955478126209833,
      "grad_norm": 153.5428924560547,
      "learning_rate": 4.4939131931001855e-06,
      "loss": 2.7018,
      "step": 15383
    },
    {
      "epoch": 5.955865272938444,
      "grad_norm": 66.7516860961914,
      "learning_rate": 4.493483030068396e-06,
      "loss": 2.0769,
      "step": 15384
    },
    {
      "epoch": 5.956252419667054,
      "grad_norm": 86.99727630615234,
      "learning_rate": 4.4930528670366075e-06,
      "loss": 0.9014,
      "step": 15385
    },
    {
      "epoch": 5.956639566395664,
      "grad_norm": 44.4795036315918,
      "learning_rate": 4.492622704004818e-06,
      "loss": 1.0254,
      "step": 15386
    },
    {
      "epoch": 5.957026713124274,
      "grad_norm": 80.90345764160156,
      "learning_rate": 4.4921925409730295e-06,
      "loss": 1.0053,
      "step": 15387
    },
    {
      "epoch": 5.957413859852885,
      "grad_norm": 42.79198455810547,
      "learning_rate": 4.49176237794124e-06,
      "loss": 0.4282,
      "step": 15388
    },
    {
      "epoch": 5.957801006581494,
      "grad_norm": 6.807878494262695,
      "learning_rate": 4.4913322149094515e-06,
      "loss": 0.2279,
      "step": 15389
    },
    {
      "epoch": 5.958188153310105,
      "grad_norm": 167.6632080078125,
      "learning_rate": 4.490902051877662e-06,
      "loss": 0.8031,
      "step": 15390
    },
    {
      "epoch": 5.9585753000387145,
      "grad_norm": 101.23225402832031,
      "learning_rate": 4.4904718888458735e-06,
      "loss": 2.1088,
      "step": 15391
    },
    {
      "epoch": 5.958962446767325,
      "grad_norm": 27.088153839111328,
      "learning_rate": 4.490041725814084e-06,
      "loss": 0.3104,
      "step": 15392
    },
    {
      "epoch": 5.959349593495935,
      "grad_norm": 30.018400192260742,
      "learning_rate": 4.489611562782295e-06,
      "loss": 3.3422,
      "step": 15393
    },
    {
      "epoch": 5.959736740224545,
      "grad_norm": 29.536151885986328,
      "learning_rate": 4.489181399750506e-06,
      "loss": 0.4238,
      "step": 15394
    },
    {
      "epoch": 5.960123886953156,
      "grad_norm": 3.3540279865264893,
      "learning_rate": 4.488751236718717e-06,
      "loss": 0.1387,
      "step": 15395
    },
    {
      "epoch": 5.960511033681765,
      "grad_norm": 3.8563241958618164,
      "learning_rate": 4.488321073686927e-06,
      "loss": 0.1598,
      "step": 15396
    },
    {
      "epoch": 5.960898180410376,
      "grad_norm": 40.144004821777344,
      "learning_rate": 4.487890910655139e-06,
      "loss": 0.8603,
      "step": 15397
    },
    {
      "epoch": 5.961285327138985,
      "grad_norm": 16.72154998779297,
      "learning_rate": 4.487460747623349e-06,
      "loss": 0.2285,
      "step": 15398
    },
    {
      "epoch": 5.961672473867596,
      "grad_norm": 36.475433349609375,
      "learning_rate": 4.487030584591561e-06,
      "loss": 1.5727,
      "step": 15399
    },
    {
      "epoch": 5.9620596205962055,
      "grad_norm": 66.27655792236328,
      "learning_rate": 4.486600421559771e-06,
      "loss": 1.0285,
      "step": 15400
    },
    {
      "epoch": 5.962446767324816,
      "grad_norm": 12.770975112915039,
      "learning_rate": 4.4861702585279826e-06,
      "loss": 0.2822,
      "step": 15401
    },
    {
      "epoch": 5.9628339140534266,
      "grad_norm": 74.31904602050781,
      "learning_rate": 4.485740095496193e-06,
      "loss": 2.1085,
      "step": 15402
    },
    {
      "epoch": 5.963221060782036,
      "grad_norm": 27.413681030273438,
      "learning_rate": 4.4853099324644045e-06,
      "loss": 0.2557,
      "step": 15403
    },
    {
      "epoch": 5.963608207510647,
      "grad_norm": 40.51851272583008,
      "learning_rate": 4.484879769432615e-06,
      "loss": 2.7215,
      "step": 15404
    },
    {
      "epoch": 5.963995354239256,
      "grad_norm": 29.661222457885742,
      "learning_rate": 4.4844496064008265e-06,
      "loss": 0.4745,
      "step": 15405
    },
    {
      "epoch": 5.964382500967867,
      "grad_norm": 5.480353832244873,
      "learning_rate": 4.484019443369037e-06,
      "loss": 0.2479,
      "step": 15406
    },
    {
      "epoch": 5.964769647696477,
      "grad_norm": 38.7316780090332,
      "learning_rate": 4.4835892803372485e-06,
      "loss": 2.0058,
      "step": 15407
    },
    {
      "epoch": 5.965156794425087,
      "grad_norm": 18.601226806640625,
      "learning_rate": 4.483159117305459e-06,
      "loss": 2.0786,
      "step": 15408
    },
    {
      "epoch": 5.9655439411536975,
      "grad_norm": 5.190310478210449,
      "learning_rate": 4.4827289542736705e-06,
      "loss": 0.2994,
      "step": 15409
    },
    {
      "epoch": 5.965931087882307,
      "grad_norm": 12.323014259338379,
      "learning_rate": 4.482298791241881e-06,
      "loss": 0.2904,
      "step": 15410
    },
    {
      "epoch": 5.966318234610918,
      "grad_norm": 25.814510345458984,
      "learning_rate": 4.481868628210092e-06,
      "loss": 0.3116,
      "step": 15411
    },
    {
      "epoch": 5.966705381339528,
      "grad_norm": 90.28912353515625,
      "learning_rate": 4.481438465178303e-06,
      "loss": 0.477,
      "step": 15412
    },
    {
      "epoch": 5.967092528068138,
      "grad_norm": 7.222379684448242,
      "learning_rate": 4.481008302146514e-06,
      "loss": 0.213,
      "step": 15413
    },
    {
      "epoch": 5.967479674796748,
      "grad_norm": 148.4930419921875,
      "learning_rate": 4.480578139114724e-06,
      "loss": 1.6158,
      "step": 15414
    },
    {
      "epoch": 5.967866821525358,
      "grad_norm": 5.141565322875977,
      "learning_rate": 4.480147976082936e-06,
      "loss": 0.1148,
      "step": 15415
    },
    {
      "epoch": 5.968253968253968,
      "grad_norm": 3.441164970397949,
      "learning_rate": 4.479717813051146e-06,
      "loss": 0.1787,
      "step": 15416
    },
    {
      "epoch": 5.968641114982578,
      "grad_norm": 115.06766510009766,
      "learning_rate": 4.479287650019358e-06,
      "loss": 1.0027,
      "step": 15417
    },
    {
      "epoch": 5.9690282617111885,
      "grad_norm": 123.92909240722656,
      "learning_rate": 4.478857486987568e-06,
      "loss": 1.7919,
      "step": 15418
    },
    {
      "epoch": 5.969415408439799,
      "grad_norm": 25.457883834838867,
      "learning_rate": 4.4784273239557796e-06,
      "loss": 0.4404,
      "step": 15419
    },
    {
      "epoch": 5.969802555168409,
      "grad_norm": 35.376434326171875,
      "learning_rate": 4.47799716092399e-06,
      "loss": 0.4736,
      "step": 15420
    },
    {
      "epoch": 5.970189701897019,
      "grad_norm": 53.99488067626953,
      "learning_rate": 4.4775669978922016e-06,
      "loss": 0.509,
      "step": 15421
    },
    {
      "epoch": 5.970576848625629,
      "grad_norm": 28.15509605407715,
      "learning_rate": 4.477136834860413e-06,
      "loss": 0.979,
      "step": 15422
    },
    {
      "epoch": 5.970963995354239,
      "grad_norm": 34.552730560302734,
      "learning_rate": 4.4767066718286235e-06,
      "loss": 2.3867,
      "step": 15423
    },
    {
      "epoch": 5.97135114208285,
      "grad_norm": 137.16734313964844,
      "learning_rate": 4.476276508796835e-06,
      "loss": 2.3078,
      "step": 15424
    },
    {
      "epoch": 5.971738288811459,
      "grad_norm": 129.42730712890625,
      "learning_rate": 4.4758463457650455e-06,
      "loss": 2.0633,
      "step": 15425
    },
    {
      "epoch": 5.97212543554007,
      "grad_norm": 22.394813537597656,
      "learning_rate": 4.475416182733256e-06,
      "loss": 0.8353,
      "step": 15426
    },
    {
      "epoch": 5.9725125822686795,
      "grad_norm": 33.92475891113281,
      "learning_rate": 4.4749860197014675e-06,
      "loss": 0.4614,
      "step": 15427
    },
    {
      "epoch": 5.97289972899729,
      "grad_norm": 27.441984176635742,
      "learning_rate": 4.474555856669678e-06,
      "loss": 0.1518,
      "step": 15428
    },
    {
      "epoch": 5.973286875725901,
      "grad_norm": 40.7187614440918,
      "learning_rate": 4.474125693637889e-06,
      "loss": 1.222,
      "step": 15429
    },
    {
      "epoch": 5.97367402245451,
      "grad_norm": 153.6007537841797,
      "learning_rate": 4.4736955306061e-06,
      "loss": 1.3047,
      "step": 15430
    },
    {
      "epoch": 5.974061169183121,
      "grad_norm": 51.312747955322266,
      "learning_rate": 4.473265367574311e-06,
      "loss": 1.3488,
      "step": 15431
    },
    {
      "epoch": 5.97444831591173,
      "grad_norm": 25.01887321472168,
      "learning_rate": 4.472835204542522e-06,
      "loss": 1.4411,
      "step": 15432
    },
    {
      "epoch": 5.974835462640341,
      "grad_norm": 33.4461784362793,
      "learning_rate": 4.472405041510733e-06,
      "loss": 1.1297,
      "step": 15433
    },
    {
      "epoch": 5.97522260936895,
      "grad_norm": 41.32004928588867,
      "learning_rate": 4.471974878478944e-06,
      "loss": 0.627,
      "step": 15434
    },
    {
      "epoch": 5.975609756097561,
      "grad_norm": 17.996747970581055,
      "learning_rate": 4.471544715447155e-06,
      "loss": 0.3578,
      "step": 15435
    },
    {
      "epoch": 5.9759969028261715,
      "grad_norm": 37.450538635253906,
      "learning_rate": 4.471114552415366e-06,
      "loss": 2.3057,
      "step": 15436
    },
    {
      "epoch": 5.976384049554781,
      "grad_norm": 27.594003677368164,
      "learning_rate": 4.470684389383577e-06,
      "loss": 0.4688,
      "step": 15437
    },
    {
      "epoch": 5.976771196283392,
      "grad_norm": 65.82208251953125,
      "learning_rate": 4.470254226351788e-06,
      "loss": 0.8724,
      "step": 15438
    },
    {
      "epoch": 5.977158343012001,
      "grad_norm": 4.373324394226074,
      "learning_rate": 4.4698240633199986e-06,
      "loss": 0.1541,
      "step": 15439
    },
    {
      "epoch": 5.977545489740612,
      "grad_norm": 59.18203353881836,
      "learning_rate": 4.46939390028821e-06,
      "loss": 1.4998,
      "step": 15440
    },
    {
      "epoch": 5.977932636469221,
      "grad_norm": 72.43743896484375,
      "learning_rate": 4.4689637372564206e-06,
      "loss": 0.6577,
      "step": 15441
    },
    {
      "epoch": 5.978319783197832,
      "grad_norm": 19.065061569213867,
      "learning_rate": 4.468533574224632e-06,
      "loss": 1.6525,
      "step": 15442
    },
    {
      "epoch": 5.978706929926442,
      "grad_norm": 3.983903169631958,
      "learning_rate": 4.4681034111928425e-06,
      "loss": 0.1563,
      "step": 15443
    },
    {
      "epoch": 5.979094076655052,
      "grad_norm": 3.2846338748931885,
      "learning_rate": 4.467673248161053e-06,
      "loss": 0.113,
      "step": 15444
    },
    {
      "epoch": 5.9794812233836625,
      "grad_norm": 55.55491256713867,
      "learning_rate": 4.4672430851292645e-06,
      "loss": 0.4352,
      "step": 15445
    },
    {
      "epoch": 5.979868370112273,
      "grad_norm": 132.2755889892578,
      "learning_rate": 4.466812922097475e-06,
      "loss": 1.1357,
      "step": 15446
    },
    {
      "epoch": 5.980255516840883,
      "grad_norm": 25.461318969726562,
      "learning_rate": 4.466382759065686e-06,
      "loss": 0.5343,
      "step": 15447
    },
    {
      "epoch": 5.980642663569493,
      "grad_norm": 141.21141052246094,
      "learning_rate": 4.465952596033897e-06,
      "loss": 2.2492,
      "step": 15448
    },
    {
      "epoch": 5.981029810298103,
      "grad_norm": 30.342424392700195,
      "learning_rate": 4.465522433002108e-06,
      "loss": 1.4291,
      "step": 15449
    },
    {
      "epoch": 5.981416957026713,
      "grad_norm": 63.35980224609375,
      "learning_rate": 4.465092269970319e-06,
      "loss": 2.9796,
      "step": 15450
    },
    {
      "epoch": 5.981804103755323,
      "grad_norm": 18.7012882232666,
      "learning_rate": 4.46466210693853e-06,
      "loss": 0.317,
      "step": 15451
    },
    {
      "epoch": 5.982191250483933,
      "grad_norm": 159.26193237304688,
      "learning_rate": 4.464231943906741e-06,
      "loss": 2.0253,
      "step": 15452
    },
    {
      "epoch": 5.982578397212544,
      "grad_norm": 135.77806091308594,
      "learning_rate": 4.463801780874952e-06,
      "loss": 0.852,
      "step": 15453
    },
    {
      "epoch": 5.9829655439411535,
      "grad_norm": 26.424030303955078,
      "learning_rate": 4.463371617843163e-06,
      "loss": 0.5678,
      "step": 15454
    },
    {
      "epoch": 5.983352690669764,
      "grad_norm": 89.4345474243164,
      "learning_rate": 4.462941454811374e-06,
      "loss": 0.7535,
      "step": 15455
    },
    {
      "epoch": 5.983739837398374,
      "grad_norm": 175.33839416503906,
      "learning_rate": 4.462511291779585e-06,
      "loss": 2.0147,
      "step": 15456
    },
    {
      "epoch": 5.984126984126984,
      "grad_norm": 99.64598083496094,
      "learning_rate": 4.462081128747796e-06,
      "loss": 1.2565,
      "step": 15457
    },
    {
      "epoch": 5.984514130855594,
      "grad_norm": 49.96561050415039,
      "learning_rate": 4.461650965716007e-06,
      "loss": 0.6064,
      "step": 15458
    },
    {
      "epoch": 5.984901277584204,
      "grad_norm": 65.38002014160156,
      "learning_rate": 4.4612208026842176e-06,
      "loss": 1.5306,
      "step": 15459
    },
    {
      "epoch": 5.985288424312815,
      "grad_norm": 7.508880138397217,
      "learning_rate": 4.460790639652429e-06,
      "loss": 0.1589,
      "step": 15460
    },
    {
      "epoch": 5.9856755710414244,
      "grad_norm": 85.114501953125,
      "learning_rate": 4.4603604766206396e-06,
      "loss": 0.901,
      "step": 15461
    },
    {
      "epoch": 5.986062717770035,
      "grad_norm": 6.002455234527588,
      "learning_rate": 4.45993031358885e-06,
      "loss": 0.1315,
      "step": 15462
    },
    {
      "epoch": 5.9864498644986455,
      "grad_norm": 31.051156997680664,
      "learning_rate": 4.4595001505570615e-06,
      "loss": 0.7471,
      "step": 15463
    },
    {
      "epoch": 5.986837011227255,
      "grad_norm": 54.37303924560547,
      "learning_rate": 4.459069987525272e-06,
      "loss": 1.1357,
      "step": 15464
    },
    {
      "epoch": 5.987224157955866,
      "grad_norm": 4.649227619171143,
      "learning_rate": 4.4586398244934835e-06,
      "loss": 0.0976,
      "step": 15465
    },
    {
      "epoch": 5.987611304684475,
      "grad_norm": 13.162537574768066,
      "learning_rate": 4.458209661461694e-06,
      "loss": 0.239,
      "step": 15466
    },
    {
      "epoch": 5.987998451413086,
      "grad_norm": 2.0787694454193115,
      "learning_rate": 4.4577794984299055e-06,
      "loss": 0.0816,
      "step": 15467
    },
    {
      "epoch": 5.988385598141695,
      "grad_norm": 26.617412567138672,
      "learning_rate": 4.457349335398116e-06,
      "loss": 0.8315,
      "step": 15468
    },
    {
      "epoch": 5.988772744870306,
      "grad_norm": 130.912109375,
      "learning_rate": 4.4569191723663275e-06,
      "loss": 0.7218,
      "step": 15469
    },
    {
      "epoch": 5.989159891598916,
      "grad_norm": 62.06239318847656,
      "learning_rate": 4.456489009334538e-06,
      "loss": 1.7991,
      "step": 15470
    },
    {
      "epoch": 5.989547038327526,
      "grad_norm": 3.7138359546661377,
      "learning_rate": 4.4560588463027495e-06,
      "loss": 0.1402,
      "step": 15471
    },
    {
      "epoch": 5.9899341850561365,
      "grad_norm": 71.865478515625,
      "learning_rate": 4.45562868327096e-06,
      "loss": 1.3159,
      "step": 15472
    },
    {
      "epoch": 5.990321331784746,
      "grad_norm": 72.40721130371094,
      "learning_rate": 4.4551985202391715e-06,
      "loss": 1.4916,
      "step": 15473
    },
    {
      "epoch": 5.990708478513357,
      "grad_norm": 116.3349838256836,
      "learning_rate": 4.454768357207382e-06,
      "loss": 1.2769,
      "step": 15474
    },
    {
      "epoch": 5.991095625241966,
      "grad_norm": 91.0790023803711,
      "learning_rate": 4.4543381941755934e-06,
      "loss": 0.8297,
      "step": 15475
    },
    {
      "epoch": 5.991482771970577,
      "grad_norm": 51.5300407409668,
      "learning_rate": 4.453908031143804e-06,
      "loss": 0.0955,
      "step": 15476
    },
    {
      "epoch": 5.991869918699187,
      "grad_norm": 50.394290924072266,
      "learning_rate": 4.453477868112015e-06,
      "loss": 2.0673,
      "step": 15477
    },
    {
      "epoch": 5.992257065427797,
      "grad_norm": 19.425373077392578,
      "learning_rate": 4.453047705080226e-06,
      "loss": 0.0487,
      "step": 15478
    },
    {
      "epoch": 5.992644212156407,
      "grad_norm": 75.826171875,
      "learning_rate": 4.4526175420484366e-06,
      "loss": 1.4958,
      "step": 15479
    },
    {
      "epoch": 5.993031358885017,
      "grad_norm": 10.856790542602539,
      "learning_rate": 4.452187379016647e-06,
      "loss": 0.3296,
      "step": 15480
    },
    {
      "epoch": 5.9934185056136275,
      "grad_norm": 30.53468132019043,
      "learning_rate": 4.4517572159848586e-06,
      "loss": 1.7291,
      "step": 15481
    },
    {
      "epoch": 5.993805652342238,
      "grad_norm": 42.6000862121582,
      "learning_rate": 4.451327052953069e-06,
      "loss": 1.1108,
      "step": 15482
    },
    {
      "epoch": 5.994192799070848,
      "grad_norm": 28.670385360717773,
      "learning_rate": 4.4508968899212805e-06,
      "loss": 1.4546,
      "step": 15483
    },
    {
      "epoch": 5.994579945799458,
      "grad_norm": 58.06396484375,
      "learning_rate": 4.450466726889491e-06,
      "loss": 1.9733,
      "step": 15484
    },
    {
      "epoch": 5.994967092528068,
      "grad_norm": 161.91372680664062,
      "learning_rate": 4.4500365638577025e-06,
      "loss": 1.3897,
      "step": 15485
    },
    {
      "epoch": 5.995354239256678,
      "grad_norm": 22.346466064453125,
      "learning_rate": 4.449606400825913e-06,
      "loss": 2.4783,
      "step": 15486
    },
    {
      "epoch": 5.995741385985289,
      "grad_norm": 0.520858645439148,
      "learning_rate": 4.4491762377941245e-06,
      "loss": 0.0152,
      "step": 15487
    },
    {
      "epoch": 5.9961285327138985,
      "grad_norm": 102.87969970703125,
      "learning_rate": 4.448746074762335e-06,
      "loss": 0.5376,
      "step": 15488
    },
    {
      "epoch": 5.996515679442509,
      "grad_norm": 7.993885517120361,
      "learning_rate": 4.4483159117305465e-06,
      "loss": 0.2574,
      "step": 15489
    },
    {
      "epoch": 5.996902826171119,
      "grad_norm": 39.482234954833984,
      "learning_rate": 4.447885748698757e-06,
      "loss": 1.327,
      "step": 15490
    },
    {
      "epoch": 5.997289972899729,
      "grad_norm": 37.425018310546875,
      "learning_rate": 4.4474555856669685e-06,
      "loss": 0.8583,
      "step": 15491
    },
    {
      "epoch": 5.997677119628339,
      "grad_norm": 42.2076416015625,
      "learning_rate": 4.447025422635179e-06,
      "loss": 3.6846,
      "step": 15492
    },
    {
      "epoch": 5.998064266356949,
      "grad_norm": 94.70507049560547,
      "learning_rate": 4.4465952596033905e-06,
      "loss": 3.943,
      "step": 15493
    },
    {
      "epoch": 5.99845141308556,
      "grad_norm": 13.326870918273926,
      "learning_rate": 4.446165096571601e-06,
      "loss": 0.2879,
      "step": 15494
    },
    {
      "epoch": 5.998838559814169,
      "grad_norm": 51.85302734375,
      "learning_rate": 4.445734933539812e-06,
      "loss": 2.8205,
      "step": 15495
    },
    {
      "epoch": 5.99922570654278,
      "grad_norm": 66.68136596679688,
      "learning_rate": 4.445304770508023e-06,
      "loss": 1.7761,
      "step": 15496
    },
    {
      "epoch": 5.9996128532713895,
      "grad_norm": 3.486114263534546,
      "learning_rate": 4.444874607476234e-06,
      "loss": 0.0853,
      "step": 15497
    },
    {
      "epoch": 6.0,
      "grad_norm": 11.862382888793945,
      "learning_rate": 4.444444444444444e-06,
      "loss": 0.2902,
      "step": 15498
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.5311320754716982,
      "eval_f1": 0.514562916117783,
      "eval_loss": 1.8248498439788818,
      "eval_runtime": 383.0655,
      "eval_samples_per_second": 2.767,
      "eval_steps_per_second": 1.384,
      "step": 15498
    },
    {
      "epoch": 6.0003871467286105,
      "grad_norm": 47.63709259033203,
      "learning_rate": 4.4440142814126556e-06,
      "loss": 2.4424,
      "step": 15499
    },
    {
      "epoch": 6.00077429345722,
      "grad_norm": 23.92387580871582,
      "learning_rate": 4.443584118380866e-06,
      "loss": 1.8397,
      "step": 15500
    },
    {
      "epoch": 6.001161440185831,
      "grad_norm": 22.675548553466797,
      "learning_rate": 4.4431539553490775e-06,
      "loss": 0.934,
      "step": 15501
    },
    {
      "epoch": 6.00154858691444,
      "grad_norm": 173.197509765625,
      "learning_rate": 4.442723792317288e-06,
      "loss": 0.6983,
      "step": 15502
    },
    {
      "epoch": 6.001935733643051,
      "grad_norm": 191.81573486328125,
      "learning_rate": 4.4422936292854995e-06,
      "loss": 1.5655,
      "step": 15503
    },
    {
      "epoch": 6.002322880371661,
      "grad_norm": 16.471994400024414,
      "learning_rate": 4.441863466253711e-06,
      "loss": 0.1084,
      "step": 15504
    },
    {
      "epoch": 6.002710027100271,
      "grad_norm": 0.9831888675689697,
      "learning_rate": 4.4414333032219215e-06,
      "loss": 0.0193,
      "step": 15505
    },
    {
      "epoch": 6.003097173828881,
      "grad_norm": 6.215763568878174,
      "learning_rate": 4.441003140190133e-06,
      "loss": 0.2379,
      "step": 15506
    },
    {
      "epoch": 6.003484320557491,
      "grad_norm": 53.956581115722656,
      "learning_rate": 4.4405729771583435e-06,
      "loss": 0.7533,
      "step": 15507
    },
    {
      "epoch": 6.0038714672861015,
      "grad_norm": 35.29750061035156,
      "learning_rate": 4.440142814126555e-06,
      "loss": 1.3724,
      "step": 15508
    },
    {
      "epoch": 6.004258614014711,
      "grad_norm": 78.34259796142578,
      "learning_rate": 4.4397126510947655e-06,
      "loss": 0.9242,
      "step": 15509
    },
    {
      "epoch": 6.004645760743322,
      "grad_norm": 30.35041618347168,
      "learning_rate": 4.439282488062976e-06,
      "loss": 0.6166,
      "step": 15510
    },
    {
      "epoch": 6.005032907471932,
      "grad_norm": 25.364028930664062,
      "learning_rate": 4.4388523250311875e-06,
      "loss": 1.7582,
      "step": 15511
    },
    {
      "epoch": 6.005420054200542,
      "grad_norm": 3.799217700958252,
      "learning_rate": 4.438422161999398e-06,
      "loss": 0.1335,
      "step": 15512
    },
    {
      "epoch": 6.005807200929152,
      "grad_norm": 6.738455772399902,
      "learning_rate": 4.437991998967609e-06,
      "loss": 0.3993,
      "step": 15513
    },
    {
      "epoch": 6.006194347657762,
      "grad_norm": 136.6097869873047,
      "learning_rate": 4.43756183593582e-06,
      "loss": 0.6979,
      "step": 15514
    },
    {
      "epoch": 6.0065814943863725,
      "grad_norm": 64.1458969116211,
      "learning_rate": 4.437131672904031e-06,
      "loss": 1.9986,
      "step": 15515
    },
    {
      "epoch": 6.006968641114983,
      "grad_norm": 11.027713775634766,
      "learning_rate": 4.436701509872242e-06,
      "loss": 0.2543,
      "step": 15516
    },
    {
      "epoch": 6.007355787843593,
      "grad_norm": 64.47834777832031,
      "learning_rate": 4.436271346840453e-06,
      "loss": 1.5717,
      "step": 15517
    },
    {
      "epoch": 6.007742934572203,
      "grad_norm": 8.71810245513916,
      "learning_rate": 4.435841183808664e-06,
      "loss": 0.1191,
      "step": 15518
    },
    {
      "epoch": 6.008130081300813,
      "grad_norm": 24.86622428894043,
      "learning_rate": 4.4354110207768746e-06,
      "loss": 1.4964,
      "step": 15519
    },
    {
      "epoch": 6.008517228029423,
      "grad_norm": 75.27685546875,
      "learning_rate": 4.434980857745086e-06,
      "loss": 2.2172,
      "step": 15520
    },
    {
      "epoch": 6.008904374758034,
      "grad_norm": 42.23078155517578,
      "learning_rate": 4.4345506947132965e-06,
      "loss": 2.1875,
      "step": 15521
    },
    {
      "epoch": 6.009291521486643,
      "grad_norm": 18.31418800354004,
      "learning_rate": 4.434120531681508e-06,
      "loss": 0.3335,
      "step": 15522
    },
    {
      "epoch": 6.009678668215254,
      "grad_norm": 18.80942726135254,
      "learning_rate": 4.4336903686497185e-06,
      "loss": 0.5425,
      "step": 15523
    },
    {
      "epoch": 6.0100658149438635,
      "grad_norm": 5.493092060089111,
      "learning_rate": 4.43326020561793e-06,
      "loss": 0.077,
      "step": 15524
    },
    {
      "epoch": 6.010452961672474,
      "grad_norm": 22.007736206054688,
      "learning_rate": 4.4328300425861405e-06,
      "loss": 2.632,
      "step": 15525
    },
    {
      "epoch": 6.010840108401084,
      "grad_norm": 2.858367681503296,
      "learning_rate": 4.432399879554352e-06,
      "loss": 0.0718,
      "step": 15526
    },
    {
      "epoch": 6.011227255129694,
      "grad_norm": 46.56931686401367,
      "learning_rate": 4.4319697165225625e-06,
      "loss": 1.1901,
      "step": 15527
    },
    {
      "epoch": 6.011614401858305,
      "grad_norm": 46.34940719604492,
      "learning_rate": 4.431539553490773e-06,
      "loss": 0.8574,
      "step": 15528
    },
    {
      "epoch": 6.012001548586914,
      "grad_norm": 13.477930068969727,
      "learning_rate": 4.4311093904589845e-06,
      "loss": 1.2809,
      "step": 15529
    },
    {
      "epoch": 6.012388695315525,
      "grad_norm": 10.683328628540039,
      "learning_rate": 4.430679227427195e-06,
      "loss": 0.1745,
      "step": 15530
    },
    {
      "epoch": 6.012775842044134,
      "grad_norm": 11.051679611206055,
      "learning_rate": 4.430249064395406e-06,
      "loss": 0.3961,
      "step": 15531
    },
    {
      "epoch": 6.013162988772745,
      "grad_norm": 25.51541519165039,
      "learning_rate": 4.429818901363617e-06,
      "loss": 1.0447,
      "step": 15532
    },
    {
      "epoch": 6.013550135501355,
      "grad_norm": 21.209653854370117,
      "learning_rate": 4.429388738331828e-06,
      "loss": 1.574,
      "step": 15533
    },
    {
      "epoch": 6.013937282229965,
      "grad_norm": 74.0716781616211,
      "learning_rate": 4.428958575300039e-06,
      "loss": 0.7179,
      "step": 15534
    },
    {
      "epoch": 6.0143244289585756,
      "grad_norm": 47.81515884399414,
      "learning_rate": 4.42852841226825e-06,
      "loss": 1.6744,
      "step": 15535
    },
    {
      "epoch": 6.014711575687185,
      "grad_norm": 11.377459526062012,
      "learning_rate": 4.428098249236461e-06,
      "loss": 0.334,
      "step": 15536
    },
    {
      "epoch": 6.015098722415796,
      "grad_norm": 26.043123245239258,
      "learning_rate": 4.4276680862046716e-06,
      "loss": 0.1707,
      "step": 15537
    },
    {
      "epoch": 6.015485869144405,
      "grad_norm": 58.35651397705078,
      "learning_rate": 4.427237923172883e-06,
      "loss": 1.4768,
      "step": 15538
    },
    {
      "epoch": 6.015873015873016,
      "grad_norm": 20.122282028198242,
      "learning_rate": 4.4268077601410936e-06,
      "loss": 0.2308,
      "step": 15539
    },
    {
      "epoch": 6.016260162601626,
      "grad_norm": 15.556160926818848,
      "learning_rate": 4.426377597109305e-06,
      "loss": 0.4466,
      "step": 15540
    },
    {
      "epoch": 6.016647309330236,
      "grad_norm": 18.462757110595703,
      "learning_rate": 4.4259474340775155e-06,
      "loss": 0.3706,
      "step": 15541
    },
    {
      "epoch": 6.0170344560588465,
      "grad_norm": 58.54119110107422,
      "learning_rate": 4.425517271045727e-06,
      "loss": 1.5616,
      "step": 15542
    },
    {
      "epoch": 6.017421602787456,
      "grad_norm": 73.66764831542969,
      "learning_rate": 4.4250871080139375e-06,
      "loss": 0.5429,
      "step": 15543
    },
    {
      "epoch": 6.017808749516067,
      "grad_norm": 3.588325262069702,
      "learning_rate": 4.424656944982149e-06,
      "loss": 0.1743,
      "step": 15544
    },
    {
      "epoch": 6.018195896244677,
      "grad_norm": 63.689449310302734,
      "learning_rate": 4.4242267819503595e-06,
      "loss": 0.8201,
      "step": 15545
    },
    {
      "epoch": 6.018583042973287,
      "grad_norm": 5.664450645446777,
      "learning_rate": 4.42379661891857e-06,
      "loss": 0.221,
      "step": 15546
    },
    {
      "epoch": 6.018970189701897,
      "grad_norm": 0.8248540759086609,
      "learning_rate": 4.4233664558867815e-06,
      "loss": 0.0211,
      "step": 15547
    },
    {
      "epoch": 6.019357336430507,
      "grad_norm": 56.76923370361328,
      "learning_rate": 4.422936292854992e-06,
      "loss": 0.3781,
      "step": 15548
    },
    {
      "epoch": 6.019744483159117,
      "grad_norm": 16.11194610595703,
      "learning_rate": 4.4225061298232035e-06,
      "loss": 0.9616,
      "step": 15549
    },
    {
      "epoch": 6.020131629887728,
      "grad_norm": 10.565021514892578,
      "learning_rate": 4.422075966791414e-06,
      "loss": 0.1398,
      "step": 15550
    },
    {
      "epoch": 6.0205187766163375,
      "grad_norm": 28.03904914855957,
      "learning_rate": 4.4216458037596255e-06,
      "loss": 0.697,
      "step": 15551
    },
    {
      "epoch": 6.020905923344948,
      "grad_norm": 14.150099754333496,
      "learning_rate": 4.421215640727836e-06,
      "loss": 0.631,
      "step": 15552
    },
    {
      "epoch": 6.021293070073558,
      "grad_norm": 44.85147476196289,
      "learning_rate": 4.4207854776960475e-06,
      "loss": 1.151,
      "step": 15553
    },
    {
      "epoch": 6.021680216802168,
      "grad_norm": 38.4635124206543,
      "learning_rate": 4.420355314664258e-06,
      "loss": 0.7605,
      "step": 15554
    },
    {
      "epoch": 6.022067363530778,
      "grad_norm": 36.03340148925781,
      "learning_rate": 4.4199251516324694e-06,
      "loss": 0.4088,
      "step": 15555
    },
    {
      "epoch": 6.022454510259388,
      "grad_norm": 54.847145080566406,
      "learning_rate": 4.41949498860068e-06,
      "loss": 0.8991,
      "step": 15556
    },
    {
      "epoch": 6.022841656987999,
      "grad_norm": 113.82549285888672,
      "learning_rate": 4.419064825568891e-06,
      "loss": 3.1464,
      "step": 15557
    },
    {
      "epoch": 6.023228803716608,
      "grad_norm": 59.632545471191406,
      "learning_rate": 4.418634662537102e-06,
      "loss": 1.896,
      "step": 15558
    },
    {
      "epoch": 6.023615950445219,
      "grad_norm": 85.44586944580078,
      "learning_rate": 4.418204499505313e-06,
      "loss": 1.0768,
      "step": 15559
    },
    {
      "epoch": 6.0240030971738285,
      "grad_norm": 4.135526657104492,
      "learning_rate": 4.417774336473524e-06,
      "loss": 0.1166,
      "step": 15560
    },
    {
      "epoch": 6.024390243902439,
      "grad_norm": 121.35421752929688,
      "learning_rate": 4.4173441734417345e-06,
      "loss": 0.499,
      "step": 15561
    },
    {
      "epoch": 6.02477739063105,
      "grad_norm": 53.79600524902344,
      "learning_rate": 4.416914010409946e-06,
      "loss": 1.2111,
      "step": 15562
    },
    {
      "epoch": 6.025164537359659,
      "grad_norm": 150.4375762939453,
      "learning_rate": 4.4164838473781565e-06,
      "loss": 1.8149,
      "step": 15563
    },
    {
      "epoch": 6.02555168408827,
      "grad_norm": 91.18562316894531,
      "learning_rate": 4.416053684346367e-06,
      "loss": 0.9522,
      "step": 15564
    },
    {
      "epoch": 6.025938830816879,
      "grad_norm": 22.4163818359375,
      "learning_rate": 4.4156235213145785e-06,
      "loss": 1.2011,
      "step": 15565
    },
    {
      "epoch": 6.02632597754549,
      "grad_norm": 7.1166486740112305,
      "learning_rate": 4.415193358282789e-06,
      "loss": 0.2124,
      "step": 15566
    },
    {
      "epoch": 6.026713124274099,
      "grad_norm": 36.72600555419922,
      "learning_rate": 4.4147631952510005e-06,
      "loss": 0.5887,
      "step": 15567
    },
    {
      "epoch": 6.02710027100271,
      "grad_norm": 26.03067970275879,
      "learning_rate": 4.414333032219211e-06,
      "loss": 0.6697,
      "step": 15568
    },
    {
      "epoch": 6.0274874177313205,
      "grad_norm": 10.148598670959473,
      "learning_rate": 4.4139028691874225e-06,
      "loss": 0.1826,
      "step": 15569
    },
    {
      "epoch": 6.02787456445993,
      "grad_norm": 47.596797943115234,
      "learning_rate": 4.413472706155633e-06,
      "loss": 1.0149,
      "step": 15570
    },
    {
      "epoch": 6.028261711188541,
      "grad_norm": 3.985630750656128,
      "learning_rate": 4.4130425431238445e-06,
      "loss": 0.075,
      "step": 15571
    },
    {
      "epoch": 6.02864885791715,
      "grad_norm": 35.337398529052734,
      "learning_rate": 4.412612380092055e-06,
      "loss": 1.2116,
      "step": 15572
    },
    {
      "epoch": 6.029036004645761,
      "grad_norm": 58.360511779785156,
      "learning_rate": 4.4121822170602664e-06,
      "loss": 0.9807,
      "step": 15573
    },
    {
      "epoch": 6.029423151374371,
      "grad_norm": 3.5424017906188965,
      "learning_rate": 4.411752054028477e-06,
      "loss": 0.1152,
      "step": 15574
    },
    {
      "epoch": 6.029810298102981,
      "grad_norm": 43.340576171875,
      "learning_rate": 4.4113218909966884e-06,
      "loss": 2.4673,
      "step": 15575
    },
    {
      "epoch": 6.030197444831591,
      "grad_norm": 30.43913459777832,
      "learning_rate": 4.410891727964899e-06,
      "loss": 0.6707,
      "step": 15576
    },
    {
      "epoch": 6.030584591560201,
      "grad_norm": 35.67218017578125,
      "learning_rate": 4.41046156493311e-06,
      "loss": 1.3948,
      "step": 15577
    },
    {
      "epoch": 6.0309717382888115,
      "grad_norm": 4.120447158813477,
      "learning_rate": 4.410031401901321e-06,
      "loss": 0.1125,
      "step": 15578
    },
    {
      "epoch": 6.031358885017422,
      "grad_norm": 30.63478660583496,
      "learning_rate": 4.4096012388695316e-06,
      "loss": 1.2775,
      "step": 15579
    },
    {
      "epoch": 6.031746031746032,
      "grad_norm": 13.181918144226074,
      "learning_rate": 4.409171075837743e-06,
      "loss": 0.2853,
      "step": 15580
    },
    {
      "epoch": 6.032133178474642,
      "grad_norm": 31.708837509155273,
      "learning_rate": 4.4087409128059535e-06,
      "loss": 2.6382,
      "step": 15581
    },
    {
      "epoch": 6.032520325203252,
      "grad_norm": 0.525385320186615,
      "learning_rate": 4.408310749774164e-06,
      "loss": 0.0139,
      "step": 15582
    },
    {
      "epoch": 6.032907471931862,
      "grad_norm": 184.64239501953125,
      "learning_rate": 4.4078805867423755e-06,
      "loss": 0.5751,
      "step": 15583
    },
    {
      "epoch": 6.033294618660472,
      "grad_norm": 119.90308380126953,
      "learning_rate": 4.407450423710586e-06,
      "loss": 0.4978,
      "step": 15584
    },
    {
      "epoch": 6.033681765389082,
      "grad_norm": 54.87839126586914,
      "learning_rate": 4.4070202606787975e-06,
      "loss": 0.688,
      "step": 15585
    },
    {
      "epoch": 6.034068912117693,
      "grad_norm": 80.38615417480469,
      "learning_rate": 4.406590097647009e-06,
      "loss": 0.3677,
      "step": 15586
    },
    {
      "epoch": 6.0344560588463025,
      "grad_norm": 66.96466827392578,
      "learning_rate": 4.4061599346152195e-06,
      "loss": 1.3878,
      "step": 15587
    },
    {
      "epoch": 6.034843205574913,
      "grad_norm": 37.33510208129883,
      "learning_rate": 4.405729771583431e-06,
      "loss": 2.381,
      "step": 15588
    },
    {
      "epoch": 6.035230352303523,
      "grad_norm": 25.185123443603516,
      "learning_rate": 4.4052996085516415e-06,
      "loss": 0.1951,
      "step": 15589
    },
    {
      "epoch": 6.035617499032133,
      "grad_norm": 37.59660720825195,
      "learning_rate": 4.404869445519853e-06,
      "loss": 0.9021,
      "step": 15590
    },
    {
      "epoch": 6.036004645760744,
      "grad_norm": 18.443552017211914,
      "learning_rate": 4.4044392824880635e-06,
      "loss": 1.397,
      "step": 15591
    },
    {
      "epoch": 6.036391792489353,
      "grad_norm": 33.975276947021484,
      "learning_rate": 4.404009119456275e-06,
      "loss": 1.7086,
      "step": 15592
    },
    {
      "epoch": 6.036778939217964,
      "grad_norm": 65.75367736816406,
      "learning_rate": 4.4035789564244854e-06,
      "loss": 1.6827,
      "step": 15593
    },
    {
      "epoch": 6.0371660859465734,
      "grad_norm": 204.24623107910156,
      "learning_rate": 4.403148793392696e-06,
      "loss": 1.9829,
      "step": 15594
    },
    {
      "epoch": 6.037553232675184,
      "grad_norm": 18.859882354736328,
      "learning_rate": 4.4027186303609074e-06,
      "loss": 0.3717,
      "step": 15595
    },
    {
      "epoch": 6.0379403794037945,
      "grad_norm": 30.966506958007812,
      "learning_rate": 4.402288467329118e-06,
      "loss": 0.7543,
      "step": 15596
    },
    {
      "epoch": 6.038327526132404,
      "grad_norm": 23.411598205566406,
      "learning_rate": 4.4018583042973286e-06,
      "loss": 1.4729,
      "step": 15597
    },
    {
      "epoch": 6.038714672861015,
      "grad_norm": 18.972070693969727,
      "learning_rate": 4.40142814126554e-06,
      "loss": 1.6426,
      "step": 15598
    },
    {
      "epoch": 6.039101819589624,
      "grad_norm": 54.557037353515625,
      "learning_rate": 4.4009979782337506e-06,
      "loss": 0.9209,
      "step": 15599
    },
    {
      "epoch": 6.039488966318235,
      "grad_norm": 47.406551361083984,
      "learning_rate": 4.400567815201962e-06,
      "loss": 1.5344,
      "step": 15600
    },
    {
      "epoch": 6.039876113046844,
      "grad_norm": 24.734159469604492,
      "learning_rate": 4.4001376521701725e-06,
      "loss": 0.6856,
      "step": 15601
    },
    {
      "epoch": 6.040263259775455,
      "grad_norm": 3.2261314392089844,
      "learning_rate": 4.399707489138384e-06,
      "loss": 0.0921,
      "step": 15602
    },
    {
      "epoch": 6.040650406504065,
      "grad_norm": 0.7492618560791016,
      "learning_rate": 4.3992773261065945e-06,
      "loss": 0.0178,
      "step": 15603
    },
    {
      "epoch": 6.041037553232675,
      "grad_norm": 181.56912231445312,
      "learning_rate": 4.398847163074806e-06,
      "loss": 0.9065,
      "step": 15604
    },
    {
      "epoch": 6.0414246999612855,
      "grad_norm": 47.02281188964844,
      "learning_rate": 4.3984170000430165e-06,
      "loss": 2.5174,
      "step": 15605
    },
    {
      "epoch": 6.041811846689895,
      "grad_norm": 11.341938018798828,
      "learning_rate": 4.397986837011228e-06,
      "loss": 0.2361,
      "step": 15606
    },
    {
      "epoch": 6.042198993418506,
      "grad_norm": 36.267784118652344,
      "learning_rate": 4.3975566739794385e-06,
      "loss": 0.7019,
      "step": 15607
    },
    {
      "epoch": 6.042586140147116,
      "grad_norm": 42.57843780517578,
      "learning_rate": 4.39712651094765e-06,
      "loss": 0.3646,
      "step": 15608
    },
    {
      "epoch": 6.042973286875726,
      "grad_norm": 15.403823852539062,
      "learning_rate": 4.3966963479158605e-06,
      "loss": 0.7087,
      "step": 15609
    },
    {
      "epoch": 6.043360433604336,
      "grad_norm": 23.98572540283203,
      "learning_rate": 4.396266184884072e-06,
      "loss": 1.6248,
      "step": 15610
    },
    {
      "epoch": 6.043747580332946,
      "grad_norm": 28.385107040405273,
      "learning_rate": 4.3958360218522825e-06,
      "loss": 0.7112,
      "step": 15611
    },
    {
      "epoch": 6.044134727061556,
      "grad_norm": 61.8040885925293,
      "learning_rate": 4.395405858820493e-06,
      "loss": 2.1018,
      "step": 15612
    },
    {
      "epoch": 6.044521873790166,
      "grad_norm": 134.12142944335938,
      "learning_rate": 4.3949756957887044e-06,
      "loss": 2.4975,
      "step": 15613
    },
    {
      "epoch": 6.0449090205187765,
      "grad_norm": 18.349458694458008,
      "learning_rate": 4.394545532756915e-06,
      "loss": 1.797,
      "step": 15614
    },
    {
      "epoch": 6.045296167247387,
      "grad_norm": 22.783161163330078,
      "learning_rate": 4.394115369725126e-06,
      "loss": 0.344,
      "step": 15615
    },
    {
      "epoch": 6.045683313975997,
      "grad_norm": 52.980987548828125,
      "learning_rate": 4.393685206693337e-06,
      "loss": 1.5794,
      "step": 15616
    },
    {
      "epoch": 6.046070460704607,
      "grad_norm": 49.15897750854492,
      "learning_rate": 4.3932550436615476e-06,
      "loss": 1.8883,
      "step": 15617
    },
    {
      "epoch": 6.046457607433217,
      "grad_norm": 105.82260131835938,
      "learning_rate": 4.392824880629759e-06,
      "loss": 2.835,
      "step": 15618
    },
    {
      "epoch": 6.046844754161827,
      "grad_norm": 36.140403747558594,
      "learning_rate": 4.3923947175979696e-06,
      "loss": 0.5496,
      "step": 15619
    },
    {
      "epoch": 6.047231900890438,
      "grad_norm": 39.275943756103516,
      "learning_rate": 4.391964554566181e-06,
      "loss": 3.9999,
      "step": 15620
    },
    {
      "epoch": 6.0476190476190474,
      "grad_norm": 3.0465848445892334,
      "learning_rate": 4.3915343915343915e-06,
      "loss": 0.0862,
      "step": 15621
    },
    {
      "epoch": 6.048006194347658,
      "grad_norm": 13.623282432556152,
      "learning_rate": 4.391104228502603e-06,
      "loss": 0.1407,
      "step": 15622
    },
    {
      "epoch": 6.048393341076268,
      "grad_norm": 108.70056915283203,
      "learning_rate": 4.3906740654708135e-06,
      "loss": 4.6259,
      "step": 15623
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 71.57061767578125,
      "learning_rate": 4.390243902439025e-06,
      "loss": 1.8841,
      "step": 15624
    },
    {
      "epoch": 6.049167634533489,
      "grad_norm": 44.388275146484375,
      "learning_rate": 4.3898137394072355e-06,
      "loss": 1.8405,
      "step": 15625
    },
    {
      "epoch": 6.049554781262098,
      "grad_norm": 100.88239288330078,
      "learning_rate": 4.389383576375447e-06,
      "loss": 1.3912,
      "step": 15626
    },
    {
      "epoch": 6.049941927990709,
      "grad_norm": 7.1864118576049805,
      "learning_rate": 4.3889534133436575e-06,
      "loss": 0.2212,
      "step": 15627
    },
    {
      "epoch": 6.050329074719318,
      "grad_norm": 6.550029277801514,
      "learning_rate": 4.388523250311869e-06,
      "loss": 0.1933,
      "step": 15628
    },
    {
      "epoch": 6.050716221447929,
      "grad_norm": 81.82988739013672,
      "learning_rate": 4.3880930872800795e-06,
      "loss": 1.3672,
      "step": 15629
    },
    {
      "epoch": 6.0511033681765385,
      "grad_norm": 22.591358184814453,
      "learning_rate": 4.38766292424829e-06,
      "loss": 0.5168,
      "step": 15630
    },
    {
      "epoch": 6.051490514905149,
      "grad_norm": 2.8096630573272705,
      "learning_rate": 4.3872327612165015e-06,
      "loss": 0.1228,
      "step": 15631
    },
    {
      "epoch": 6.0518776616337595,
      "grad_norm": 36.52828598022461,
      "learning_rate": 4.386802598184712e-06,
      "loss": 1.4182,
      "step": 15632
    },
    {
      "epoch": 6.052264808362369,
      "grad_norm": 30.018022537231445,
      "learning_rate": 4.3863724351529234e-06,
      "loss": 2.2716,
      "step": 15633
    },
    {
      "epoch": 6.05265195509098,
      "grad_norm": 143.82557678222656,
      "learning_rate": 4.385942272121134e-06,
      "loss": 1.6381,
      "step": 15634
    },
    {
      "epoch": 6.053039101819589,
      "grad_norm": 40.67649841308594,
      "learning_rate": 4.3855121090893454e-06,
      "loss": 0.5957,
      "step": 15635
    },
    {
      "epoch": 6.0534262485482,
      "grad_norm": 4.440903186798096,
      "learning_rate": 4.385081946057556e-06,
      "loss": 0.2243,
      "step": 15636
    },
    {
      "epoch": 6.05381339527681,
      "grad_norm": 6.980080604553223,
      "learning_rate": 4.384651783025767e-06,
      "loss": 0.1859,
      "step": 15637
    },
    {
      "epoch": 6.05420054200542,
      "grad_norm": 65.15398406982422,
      "learning_rate": 4.384221619993978e-06,
      "loss": 1.0506,
      "step": 15638
    },
    {
      "epoch": 6.05458768873403,
      "grad_norm": 1.103723406791687,
      "learning_rate": 4.383791456962189e-06,
      "loss": 0.0252,
      "step": 15639
    },
    {
      "epoch": 6.05497483546264,
      "grad_norm": 10.401304244995117,
      "learning_rate": 4.3833612939304e-06,
      "loss": 0.198,
      "step": 15640
    },
    {
      "epoch": 6.0553619821912505,
      "grad_norm": 4.31199836730957,
      "learning_rate": 4.382931130898611e-06,
      "loss": 0.1633,
      "step": 15641
    },
    {
      "epoch": 6.055749128919861,
      "grad_norm": 5.016330242156982,
      "learning_rate": 4.382500967866822e-06,
      "loss": 0.1743,
      "step": 15642
    },
    {
      "epoch": 6.056136275648471,
      "grad_norm": 42.574256896972656,
      "learning_rate": 4.382070804835033e-06,
      "loss": 1.1509,
      "step": 15643
    },
    {
      "epoch": 6.056523422377081,
      "grad_norm": 2.1157279014587402,
      "learning_rate": 4.381640641803244e-06,
      "loss": 0.0598,
      "step": 15644
    },
    {
      "epoch": 6.056910569105691,
      "grad_norm": 4.101676940917969,
      "learning_rate": 4.3812104787714545e-06,
      "loss": 0.1917,
      "step": 15645
    },
    {
      "epoch": 6.057297715834301,
      "grad_norm": 65.78036499023438,
      "learning_rate": 4.380780315739666e-06,
      "loss": 1.5671,
      "step": 15646
    },
    {
      "epoch": 6.057684862562911,
      "grad_norm": 4.615649700164795,
      "learning_rate": 4.3803501527078765e-06,
      "loss": 0.1767,
      "step": 15647
    },
    {
      "epoch": 6.0580720092915215,
      "grad_norm": 59.26122283935547,
      "learning_rate": 4.379919989676087e-06,
      "loss": 3.0909,
      "step": 15648
    },
    {
      "epoch": 6.058459156020132,
      "grad_norm": 36.40238571166992,
      "learning_rate": 4.3794898266442985e-06,
      "loss": 1.3448,
      "step": 15649
    },
    {
      "epoch": 6.058846302748742,
      "grad_norm": 190.46629333496094,
      "learning_rate": 4.379059663612509e-06,
      "loss": 0.6123,
      "step": 15650
    },
    {
      "epoch": 6.059233449477352,
      "grad_norm": 5.12970495223999,
      "learning_rate": 4.3786295005807205e-06,
      "loss": 0.1704,
      "step": 15651
    },
    {
      "epoch": 6.059620596205962,
      "grad_norm": 27.531831741333008,
      "learning_rate": 4.378199337548931e-06,
      "loss": 1.2575,
      "step": 15652
    },
    {
      "epoch": 6.060007742934572,
      "grad_norm": 68.8448486328125,
      "learning_rate": 4.3777691745171424e-06,
      "loss": 0.3481,
      "step": 15653
    },
    {
      "epoch": 6.060394889663183,
      "grad_norm": 3.0619516372680664,
      "learning_rate": 4.377339011485353e-06,
      "loss": 0.0854,
      "step": 15654
    },
    {
      "epoch": 6.060782036391792,
      "grad_norm": 109.16828155517578,
      "learning_rate": 4.3769088484535644e-06,
      "loss": 1.3561,
      "step": 15655
    },
    {
      "epoch": 6.061169183120403,
      "grad_norm": 41.53456497192383,
      "learning_rate": 4.376478685421775e-06,
      "loss": 0.2653,
      "step": 15656
    },
    {
      "epoch": 6.0615563298490125,
      "grad_norm": 22.813899993896484,
      "learning_rate": 4.376048522389986e-06,
      "loss": 1.6118,
      "step": 15657
    },
    {
      "epoch": 6.061943476577623,
      "grad_norm": 123.78997039794922,
      "learning_rate": 4.375618359358197e-06,
      "loss": 0.6321,
      "step": 15658
    },
    {
      "epoch": 6.062330623306233,
      "grad_norm": 101.6791000366211,
      "learning_rate": 4.375188196326408e-06,
      "loss": 2.3291,
      "step": 15659
    },
    {
      "epoch": 6.062717770034843,
      "grad_norm": 7.802579879760742,
      "learning_rate": 4.374758033294619e-06,
      "loss": 0.1792,
      "step": 15660
    },
    {
      "epoch": 6.063104916763454,
      "grad_norm": 18.04389762878418,
      "learning_rate": 4.37432787026283e-06,
      "loss": 1.9108,
      "step": 15661
    },
    {
      "epoch": 6.063492063492063,
      "grad_norm": 15.580796241760254,
      "learning_rate": 4.373897707231041e-06,
      "loss": 1.209,
      "step": 15662
    },
    {
      "epoch": 6.063879210220674,
      "grad_norm": 93.59740447998047,
      "learning_rate": 4.3734675441992515e-06,
      "loss": 1.1732,
      "step": 15663
    },
    {
      "epoch": 6.064266356949283,
      "grad_norm": 104.64275360107422,
      "learning_rate": 4.373037381167463e-06,
      "loss": 1.2282,
      "step": 15664
    },
    {
      "epoch": 6.064653503677894,
      "grad_norm": 33.386104583740234,
      "learning_rate": 4.3726072181356735e-06,
      "loss": 1.5075,
      "step": 15665
    },
    {
      "epoch": 6.065040650406504,
      "grad_norm": 2.201200246810913,
      "learning_rate": 4.372177055103884e-06,
      "loss": 0.0842,
      "step": 15666
    },
    {
      "epoch": 6.065427797135114,
      "grad_norm": 137.7388916015625,
      "learning_rate": 4.3717468920720955e-06,
      "loss": 1.3142,
      "step": 15667
    },
    {
      "epoch": 6.0658149438637246,
      "grad_norm": 2.2529149055480957,
      "learning_rate": 4.371316729040307e-06,
      "loss": 0.1156,
      "step": 15668
    },
    {
      "epoch": 6.066202090592334,
      "grad_norm": 28.923908233642578,
      "learning_rate": 4.3708865660085175e-06,
      "loss": 0.7062,
      "step": 15669
    },
    {
      "epoch": 6.066589237320945,
      "grad_norm": 30.27183723449707,
      "learning_rate": 4.370456402976729e-06,
      "loss": 0.6123,
      "step": 15670
    },
    {
      "epoch": 6.066976384049555,
      "grad_norm": 27.97642707824707,
      "learning_rate": 4.3700262399449395e-06,
      "loss": 0.3407,
      "step": 15671
    },
    {
      "epoch": 6.067363530778165,
      "grad_norm": 29.700271606445312,
      "learning_rate": 4.369596076913151e-06,
      "loss": 0.1836,
      "step": 15672
    },
    {
      "epoch": 6.067750677506775,
      "grad_norm": 5.970388889312744,
      "learning_rate": 4.3691659138813614e-06,
      "loss": 0.1584,
      "step": 15673
    },
    {
      "epoch": 6.068137824235385,
      "grad_norm": 92.2706069946289,
      "learning_rate": 4.368735750849573e-06,
      "loss": 2.6663,
      "step": 15674
    },
    {
      "epoch": 6.0685249709639955,
      "grad_norm": 16.907617568969727,
      "learning_rate": 4.368305587817783e-06,
      "loss": 0.443,
      "step": 15675
    },
    {
      "epoch": 6.068912117692605,
      "grad_norm": 4.159678936004639,
      "learning_rate": 4.367875424785995e-06,
      "loss": 0.1112,
      "step": 15676
    },
    {
      "epoch": 6.069299264421216,
      "grad_norm": 107.78074645996094,
      "learning_rate": 4.367445261754205e-06,
      "loss": 0.8169,
      "step": 15677
    },
    {
      "epoch": 6.069686411149826,
      "grad_norm": 3.4473876953125,
      "learning_rate": 4.367015098722416e-06,
      "loss": 0.1724,
      "step": 15678
    },
    {
      "epoch": 6.070073557878436,
      "grad_norm": 82.69690704345703,
      "learning_rate": 4.366584935690627e-06,
      "loss": 2.8581,
      "step": 15679
    },
    {
      "epoch": 6.070460704607046,
      "grad_norm": 19.0034236907959,
      "learning_rate": 4.366154772658838e-06,
      "loss": 0.2096,
      "step": 15680
    },
    {
      "epoch": 6.070847851335656,
      "grad_norm": 31.563644409179688,
      "learning_rate": 4.3657246096270485e-06,
      "loss": 1.9207,
      "step": 15681
    },
    {
      "epoch": 6.071234998064266,
      "grad_norm": 203.95004272460938,
      "learning_rate": 4.36529444659526e-06,
      "loss": 1.438,
      "step": 15682
    },
    {
      "epoch": 6.071622144792877,
      "grad_norm": 95.47083282470703,
      "learning_rate": 4.3648642835634705e-06,
      "loss": 0.5378,
      "step": 15683
    },
    {
      "epoch": 6.0720092915214865,
      "grad_norm": 19.964998245239258,
      "learning_rate": 4.364434120531682e-06,
      "loss": 1.4748,
      "step": 15684
    },
    {
      "epoch": 6.072396438250097,
      "grad_norm": 0.41077372431755066,
      "learning_rate": 4.3640039574998925e-06,
      "loss": 0.0116,
      "step": 15685
    },
    {
      "epoch": 6.072783584978707,
      "grad_norm": 27.308670043945312,
      "learning_rate": 4.363573794468104e-06,
      "loss": 1.5024,
      "step": 15686
    },
    {
      "epoch": 6.073170731707317,
      "grad_norm": 26.42904281616211,
      "learning_rate": 4.3631436314363145e-06,
      "loss": 1.3901,
      "step": 15687
    },
    {
      "epoch": 6.073557878435928,
      "grad_norm": 79.1080093383789,
      "learning_rate": 4.362713468404526e-06,
      "loss": 2.6844,
      "step": 15688
    },
    {
      "epoch": 6.073945025164537,
      "grad_norm": 26.45281219482422,
      "learning_rate": 4.3622833053727365e-06,
      "loss": 0.5871,
      "step": 15689
    },
    {
      "epoch": 6.074332171893148,
      "grad_norm": 35.764747619628906,
      "learning_rate": 4.361853142340948e-06,
      "loss": 1.3425,
      "step": 15690
    },
    {
      "epoch": 6.074719318621757,
      "grad_norm": 44.27668762207031,
      "learning_rate": 4.3614229793091585e-06,
      "loss": 0.577,
      "step": 15691
    },
    {
      "epoch": 6.075106465350368,
      "grad_norm": 4.3833160400390625,
      "learning_rate": 4.36099281627737e-06,
      "loss": 0.1672,
      "step": 15692
    },
    {
      "epoch": 6.0754936120789775,
      "grad_norm": 18.39435577392578,
      "learning_rate": 4.3605626532455804e-06,
      "loss": 1.0027,
      "step": 15693
    },
    {
      "epoch": 6.075880758807588,
      "grad_norm": 85.76400756835938,
      "learning_rate": 4.360132490213792e-06,
      "loss": 2.3499,
      "step": 15694
    },
    {
      "epoch": 6.0762679055361986,
      "grad_norm": 36.88994216918945,
      "learning_rate": 4.359702327182002e-06,
      "loss": 2.0507,
      "step": 15695
    },
    {
      "epoch": 6.076655052264808,
      "grad_norm": 5.912968635559082,
      "learning_rate": 4.359272164150213e-06,
      "loss": 0.1598,
      "step": 15696
    },
    {
      "epoch": 6.077042198993419,
      "grad_norm": 36.639591217041016,
      "learning_rate": 4.358842001118424e-06,
      "loss": 1.4583,
      "step": 15697
    },
    {
      "epoch": 6.077429345722028,
      "grad_norm": 40.55198287963867,
      "learning_rate": 4.358411838086635e-06,
      "loss": 1.6466,
      "step": 15698
    },
    {
      "epoch": 6.077816492450639,
      "grad_norm": 28.106731414794922,
      "learning_rate": 4.3579816750548455e-06,
      "loss": 1.5692,
      "step": 15699
    },
    {
      "epoch": 6.078203639179249,
      "grad_norm": 19.28209686279297,
      "learning_rate": 4.357551512023057e-06,
      "loss": 2.0089,
      "step": 15700
    },
    {
      "epoch": 6.078590785907859,
      "grad_norm": 12.86439323425293,
      "learning_rate": 4.3571213489912675e-06,
      "loss": 0.3594,
      "step": 15701
    },
    {
      "epoch": 6.0789779326364695,
      "grad_norm": 184.57107543945312,
      "learning_rate": 4.356691185959479e-06,
      "loss": 1.4267,
      "step": 15702
    },
    {
      "epoch": 6.079365079365079,
      "grad_norm": 76.05949401855469,
      "learning_rate": 4.3562610229276895e-06,
      "loss": 0.3451,
      "step": 15703
    },
    {
      "epoch": 6.07975222609369,
      "grad_norm": 3.5189008712768555,
      "learning_rate": 4.355830859895901e-06,
      "loss": 0.0899,
      "step": 15704
    },
    {
      "epoch": 6.080139372822299,
      "grad_norm": 26.18446922302246,
      "learning_rate": 4.3554006968641115e-06,
      "loss": 1.071,
      "step": 15705
    },
    {
      "epoch": 6.08052651955091,
      "grad_norm": 93.46044158935547,
      "learning_rate": 4.354970533832323e-06,
      "loss": 2.0666,
      "step": 15706
    },
    {
      "epoch": 6.08091366627952,
      "grad_norm": 3.4034922122955322,
      "learning_rate": 4.3545403708005335e-06,
      "loss": 0.1607,
      "step": 15707
    },
    {
      "epoch": 6.08130081300813,
      "grad_norm": 4.7984395027160645,
      "learning_rate": 4.354110207768745e-06,
      "loss": 0.1461,
      "step": 15708
    },
    {
      "epoch": 6.08168795973674,
      "grad_norm": 113.57984924316406,
      "learning_rate": 4.353680044736956e-06,
      "loss": 2.4135,
      "step": 15709
    },
    {
      "epoch": 6.08207510646535,
      "grad_norm": 4.907589912414551,
      "learning_rate": 4.353249881705167e-06,
      "loss": 0.2463,
      "step": 15710
    },
    {
      "epoch": 6.0824622531939605,
      "grad_norm": 40.362937927246094,
      "learning_rate": 4.3528197186733774e-06,
      "loss": 1.5407,
      "step": 15711
    },
    {
      "epoch": 6.082849399922571,
      "grad_norm": 114.8516616821289,
      "learning_rate": 4.352389555641589e-06,
      "loss": 1.3685,
      "step": 15712
    },
    {
      "epoch": 6.083236546651181,
      "grad_norm": 8.74158763885498,
      "learning_rate": 4.3519593926097994e-06,
      "loss": 0.3194,
      "step": 15713
    },
    {
      "epoch": 6.083623693379791,
      "grad_norm": 64.64298248291016,
      "learning_rate": 4.35152922957801e-06,
      "loss": 0.876,
      "step": 15714
    },
    {
      "epoch": 6.084010840108401,
      "grad_norm": 33.46247482299805,
      "learning_rate": 4.351099066546221e-06,
      "loss": 1.3772,
      "step": 15715
    },
    {
      "epoch": 6.084397986837011,
      "grad_norm": 38.31082534790039,
      "learning_rate": 4.350668903514432e-06,
      "loss": 1.3985,
      "step": 15716
    },
    {
      "epoch": 6.084785133565622,
      "grad_norm": 30.116464614868164,
      "learning_rate": 4.350238740482643e-06,
      "loss": 1.3174,
      "step": 15717
    },
    {
      "epoch": 6.085172280294231,
      "grad_norm": 69.78070831298828,
      "learning_rate": 4.349808577450854e-06,
      "loss": 0.2963,
      "step": 15718
    },
    {
      "epoch": 6.085559427022842,
      "grad_norm": 28.044843673706055,
      "learning_rate": 4.349378414419065e-06,
      "loss": 1.5369,
      "step": 15719
    },
    {
      "epoch": 6.0859465737514515,
      "grad_norm": 49.62215042114258,
      "learning_rate": 4.348948251387276e-06,
      "loss": 2.0566,
      "step": 15720
    },
    {
      "epoch": 6.086333720480062,
      "grad_norm": 109.0972900390625,
      "learning_rate": 4.348518088355487e-06,
      "loss": 0.6896,
      "step": 15721
    },
    {
      "epoch": 6.086720867208672,
      "grad_norm": 57.78602600097656,
      "learning_rate": 4.348087925323698e-06,
      "loss": 1.5288,
      "step": 15722
    },
    {
      "epoch": 6.087108013937282,
      "grad_norm": 3.297999382019043,
      "learning_rate": 4.347657762291909e-06,
      "loss": 0.0815,
      "step": 15723
    },
    {
      "epoch": 6.087495160665893,
      "grad_norm": 99.03543853759766,
      "learning_rate": 4.34722759926012e-06,
      "loss": 2.2868,
      "step": 15724
    },
    {
      "epoch": 6.087882307394502,
      "grad_norm": 20.678503036499023,
      "learning_rate": 4.346797436228331e-06,
      "loss": 0.5088,
      "step": 15725
    },
    {
      "epoch": 6.088269454123113,
      "grad_norm": 13.840597152709961,
      "learning_rate": 4.346367273196542e-06,
      "loss": 0.4535,
      "step": 15726
    },
    {
      "epoch": 6.0886566008517224,
      "grad_norm": 98.05167388916016,
      "learning_rate": 4.345937110164753e-06,
      "loss": 0.9861,
      "step": 15727
    },
    {
      "epoch": 6.089043747580333,
      "grad_norm": 61.82770538330078,
      "learning_rate": 4.345506947132964e-06,
      "loss": 0.242,
      "step": 15728
    },
    {
      "epoch": 6.0894308943089435,
      "grad_norm": 11.800725936889648,
      "learning_rate": 4.3450767841011745e-06,
      "loss": 0.2072,
      "step": 15729
    },
    {
      "epoch": 6.089818041037553,
      "grad_norm": 23.98857879638672,
      "learning_rate": 4.344646621069386e-06,
      "loss": 2.5086,
      "step": 15730
    },
    {
      "epoch": 6.090205187766164,
      "grad_norm": 60.23301315307617,
      "learning_rate": 4.3442164580375964e-06,
      "loss": 2.1956,
      "step": 15731
    },
    {
      "epoch": 6.090592334494773,
      "grad_norm": 4.171816349029541,
      "learning_rate": 4.343786295005807e-06,
      "loss": 0.2113,
      "step": 15732
    },
    {
      "epoch": 6.090979481223384,
      "grad_norm": 19.35213279724121,
      "learning_rate": 4.3433561319740184e-06,
      "loss": 1.4182,
      "step": 15733
    },
    {
      "epoch": 6.091366627951994,
      "grad_norm": 84.18885040283203,
      "learning_rate": 4.342925968942229e-06,
      "loss": 1.269,
      "step": 15734
    },
    {
      "epoch": 6.091753774680604,
      "grad_norm": 65.72460174560547,
      "learning_rate": 4.34249580591044e-06,
      "loss": 1.1306,
      "step": 15735
    },
    {
      "epoch": 6.092140921409214,
      "grad_norm": 37.83633041381836,
      "learning_rate": 4.342065642878651e-06,
      "loss": 0.6447,
      "step": 15736
    },
    {
      "epoch": 6.092528068137824,
      "grad_norm": 4.900839328765869,
      "learning_rate": 4.341635479846862e-06,
      "loss": 0.117,
      "step": 15737
    },
    {
      "epoch": 6.0929152148664345,
      "grad_norm": 76.90824127197266,
      "learning_rate": 4.341205316815073e-06,
      "loss": 1.4785,
      "step": 15738
    },
    {
      "epoch": 6.093302361595044,
      "grad_norm": 59.81470489501953,
      "learning_rate": 4.340775153783284e-06,
      "loss": 1.4227,
      "step": 15739
    },
    {
      "epoch": 6.093689508323655,
      "grad_norm": 65.08908081054688,
      "learning_rate": 4.340344990751495e-06,
      "loss": 2.0066,
      "step": 15740
    },
    {
      "epoch": 6.094076655052265,
      "grad_norm": 34.6552734375,
      "learning_rate": 4.339914827719706e-06,
      "loss": 1.6124,
      "step": 15741
    },
    {
      "epoch": 6.094463801780875,
      "grad_norm": 27.379146575927734,
      "learning_rate": 4.339484664687917e-06,
      "loss": 0.4471,
      "step": 15742
    },
    {
      "epoch": 6.094850948509485,
      "grad_norm": 36.49042892456055,
      "learning_rate": 4.339054501656128e-06,
      "loss": 0.2687,
      "step": 15743
    },
    {
      "epoch": 6.095238095238095,
      "grad_norm": 38.58343505859375,
      "learning_rate": 4.338624338624339e-06,
      "loss": 2.3197,
      "step": 15744
    },
    {
      "epoch": 6.095625241966705,
      "grad_norm": 13.181130409240723,
      "learning_rate": 4.33819417559255e-06,
      "loss": 1.1415,
      "step": 15745
    },
    {
      "epoch": 6.096012388695316,
      "grad_norm": 3.1291322708129883,
      "learning_rate": 4.337764012560761e-06,
      "loss": 0.1327,
      "step": 15746
    },
    {
      "epoch": 6.0963995354239255,
      "grad_norm": 8.307250022888184,
      "learning_rate": 4.3373338495289715e-06,
      "loss": 0.2365,
      "step": 15747
    },
    {
      "epoch": 6.096786682152536,
      "grad_norm": 82.5162124633789,
      "learning_rate": 4.336903686497183e-06,
      "loss": 1.0207,
      "step": 15748
    },
    {
      "epoch": 6.097173828881146,
      "grad_norm": 28.297060012817383,
      "learning_rate": 4.3364735234653935e-06,
      "loss": 0.7266,
      "step": 15749
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 2.8164188861846924,
      "learning_rate": 4.336043360433605e-06,
      "loss": 0.1135,
      "step": 15750
    },
    {
      "epoch": 6.097948122338366,
      "grad_norm": 3.8407552242279053,
      "learning_rate": 4.3356131974018154e-06,
      "loss": 0.096,
      "step": 15751
    },
    {
      "epoch": 6.098335269066976,
      "grad_norm": 3.920114040374756,
      "learning_rate": 4.335183034370027e-06,
      "loss": 0.1434,
      "step": 15752
    },
    {
      "epoch": 6.098722415795587,
      "grad_norm": 35.89198303222656,
      "learning_rate": 4.3347528713382374e-06,
      "loss": 2.5341,
      "step": 15753
    },
    {
      "epoch": 6.0991095625241964,
      "grad_norm": 38.00995635986328,
      "learning_rate": 4.334322708306449e-06,
      "loss": 0.6932,
      "step": 15754
    },
    {
      "epoch": 6.099496709252807,
      "grad_norm": 16.031648635864258,
      "learning_rate": 4.333892545274659e-06,
      "loss": 1.4611,
      "step": 15755
    },
    {
      "epoch": 6.099883855981417,
      "grad_norm": 31.905563354492188,
      "learning_rate": 4.333462382242871e-06,
      "loss": 0.8281,
      "step": 15756
    },
    {
      "epoch": 6.100271002710027,
      "grad_norm": 28.836156845092773,
      "learning_rate": 4.333032219211081e-06,
      "loss": 1.6918,
      "step": 15757
    },
    {
      "epoch": 6.100658149438638,
      "grad_norm": 19.928983688354492,
      "learning_rate": 4.332602056179293e-06,
      "loss": 1.7335,
      "step": 15758
    },
    {
      "epoch": 6.101045296167247,
      "grad_norm": 45.158958435058594,
      "learning_rate": 4.332171893147503e-06,
      "loss": 1.7693,
      "step": 15759
    },
    {
      "epoch": 6.101432442895858,
      "grad_norm": 27.61777687072754,
      "learning_rate": 4.331741730115715e-06,
      "loss": 2.1989,
      "step": 15760
    },
    {
      "epoch": 6.101819589624467,
      "grad_norm": 39.66214370727539,
      "learning_rate": 4.331311567083925e-06,
      "loss": 0.9248,
      "step": 15761
    },
    {
      "epoch": 6.102206736353078,
      "grad_norm": 7.864927768707275,
      "learning_rate": 4.330881404052136e-06,
      "loss": 0.3005,
      "step": 15762
    },
    {
      "epoch": 6.102593883081688,
      "grad_norm": 1.728996753692627,
      "learning_rate": 4.330451241020347e-06,
      "loss": 0.0291,
      "step": 15763
    },
    {
      "epoch": 6.102981029810298,
      "grad_norm": 62.545597076416016,
      "learning_rate": 4.330021077988558e-06,
      "loss": 1.0797,
      "step": 15764
    },
    {
      "epoch": 6.1033681765389085,
      "grad_norm": 36.63307189941406,
      "learning_rate": 4.3295909149567685e-06,
      "loss": 1.9092,
      "step": 15765
    },
    {
      "epoch": 6.103755323267518,
      "grad_norm": 61.05140686035156,
      "learning_rate": 4.32916075192498e-06,
      "loss": 2.3705,
      "step": 15766
    },
    {
      "epoch": 6.104142469996129,
      "grad_norm": 59.55052185058594,
      "learning_rate": 4.3287305888931905e-06,
      "loss": 1.7428,
      "step": 15767
    },
    {
      "epoch": 6.104529616724738,
      "grad_norm": 14.194921493530273,
      "learning_rate": 4.328300425861402e-06,
      "loss": 0.2124,
      "step": 15768
    },
    {
      "epoch": 6.104916763453349,
      "grad_norm": 7.156171798706055,
      "learning_rate": 4.3278702628296125e-06,
      "loss": 0.1247,
      "step": 15769
    },
    {
      "epoch": 6.105303910181959,
      "grad_norm": 17.061750411987305,
      "learning_rate": 4.327440099797824e-06,
      "loss": 0.6027,
      "step": 15770
    },
    {
      "epoch": 6.105691056910569,
      "grad_norm": 11.186955451965332,
      "learning_rate": 4.3270099367660344e-06,
      "loss": 0.1834,
      "step": 15771
    },
    {
      "epoch": 6.106078203639179,
      "grad_norm": 62.922325134277344,
      "learning_rate": 4.326579773734246e-06,
      "loss": 0.6864,
      "step": 15772
    },
    {
      "epoch": 6.106465350367789,
      "grad_norm": 3.440518617630005,
      "learning_rate": 4.3261496107024564e-06,
      "loss": 0.1056,
      "step": 15773
    },
    {
      "epoch": 6.1068524970963995,
      "grad_norm": 3.499631404876709,
      "learning_rate": 4.325719447670668e-06,
      "loss": 0.0977,
      "step": 15774
    },
    {
      "epoch": 6.10723964382501,
      "grad_norm": 19.388477325439453,
      "learning_rate": 4.325289284638878e-06,
      "loss": 1.0093,
      "step": 15775
    },
    {
      "epoch": 6.10762679055362,
      "grad_norm": 67.10333251953125,
      "learning_rate": 4.32485912160709e-06,
      "loss": 2.1472,
      "step": 15776
    },
    {
      "epoch": 6.10801393728223,
      "grad_norm": 11.210041999816895,
      "learning_rate": 4.3244289585753e-06,
      "loss": 0.3629,
      "step": 15777
    },
    {
      "epoch": 6.10840108401084,
      "grad_norm": 2.850796937942505,
      "learning_rate": 4.323998795543512e-06,
      "loss": 0.1343,
      "step": 15778
    },
    {
      "epoch": 6.10878823073945,
      "grad_norm": 81.0978775024414,
      "learning_rate": 4.323568632511722e-06,
      "loss": 1.226,
      "step": 15779
    },
    {
      "epoch": 6.109175377468061,
      "grad_norm": 49.811588287353516,
      "learning_rate": 4.323138469479933e-06,
      "loss": 1.1298,
      "step": 15780
    },
    {
      "epoch": 6.1095625241966705,
      "grad_norm": 71.8161849975586,
      "learning_rate": 4.322708306448144e-06,
      "loss": 0.7327,
      "step": 15781
    },
    {
      "epoch": 6.109949670925281,
      "grad_norm": 29.431215286254883,
      "learning_rate": 4.322278143416355e-06,
      "loss": 0.9547,
      "step": 15782
    },
    {
      "epoch": 6.110336817653891,
      "grad_norm": 14.792461395263672,
      "learning_rate": 4.3218479803845655e-06,
      "loss": 0.1859,
      "step": 15783
    },
    {
      "epoch": 6.110723964382501,
      "grad_norm": 28.587602615356445,
      "learning_rate": 4.321417817352777e-06,
      "loss": 0.4975,
      "step": 15784
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 34.19587707519531,
      "learning_rate": 4.3209876543209875e-06,
      "loss": 0.3817,
      "step": 15785
    },
    {
      "epoch": 6.111498257839721,
      "grad_norm": 81.71638488769531,
      "learning_rate": 4.320557491289199e-06,
      "loss": 1.9746,
      "step": 15786
    },
    {
      "epoch": 6.111885404568332,
      "grad_norm": 36.134910583496094,
      "learning_rate": 4.3201273282574095e-06,
      "loss": 0.7295,
      "step": 15787
    },
    {
      "epoch": 6.112272551296941,
      "grad_norm": 1.781965970993042,
      "learning_rate": 4.319697165225621e-06,
      "loss": 0.0654,
      "step": 15788
    },
    {
      "epoch": 6.112659698025552,
      "grad_norm": 66.68168640136719,
      "learning_rate": 4.3192670021938315e-06,
      "loss": 0.7306,
      "step": 15789
    },
    {
      "epoch": 6.1130468447541615,
      "grad_norm": 9.204696655273438,
      "learning_rate": 4.318836839162043e-06,
      "loss": 0.2402,
      "step": 15790
    },
    {
      "epoch": 6.113433991482772,
      "grad_norm": 28.449508666992188,
      "learning_rate": 4.318406676130254e-06,
      "loss": 1.5797,
      "step": 15791
    },
    {
      "epoch": 6.1138211382113825,
      "grad_norm": 19.62908935546875,
      "learning_rate": 4.317976513098465e-06,
      "loss": 0.4137,
      "step": 15792
    },
    {
      "epoch": 6.114208284939992,
      "grad_norm": 28.412464141845703,
      "learning_rate": 4.317546350066676e-06,
      "loss": 1.8167,
      "step": 15793
    },
    {
      "epoch": 6.114595431668603,
      "grad_norm": 28.907588958740234,
      "learning_rate": 4.317116187034887e-06,
      "loss": 0.3349,
      "step": 15794
    },
    {
      "epoch": 6.114982578397212,
      "grad_norm": 3.5092403888702393,
      "learning_rate": 4.316686024003097e-06,
      "loss": 0.2045,
      "step": 15795
    },
    {
      "epoch": 6.115369725125823,
      "grad_norm": 44.48519515991211,
      "learning_rate": 4.316255860971309e-06,
      "loss": 0.735,
      "step": 15796
    },
    {
      "epoch": 6.115756871854432,
      "grad_norm": 36.87538528442383,
      "learning_rate": 4.315825697939519e-06,
      "loss": 0.9835,
      "step": 15797
    },
    {
      "epoch": 6.116144018583043,
      "grad_norm": 76.91046905517578,
      "learning_rate": 4.31539553490773e-06,
      "loss": 1.154,
      "step": 15798
    },
    {
      "epoch": 6.116531165311653,
      "grad_norm": 49.405494689941406,
      "learning_rate": 4.314965371875941e-06,
      "loss": 1.7442,
      "step": 15799
    },
    {
      "epoch": 6.116918312040263,
      "grad_norm": 12.767789840698242,
      "learning_rate": 4.314535208844152e-06,
      "loss": 0.2692,
      "step": 15800
    },
    {
      "epoch": 6.1173054587688735,
      "grad_norm": 4.310499668121338,
      "learning_rate": 4.314105045812363e-06,
      "loss": 0.0939,
      "step": 15801
    },
    {
      "epoch": 6.117692605497483,
      "grad_norm": 64.13439178466797,
      "learning_rate": 4.313674882780574e-06,
      "loss": 0.7533,
      "step": 15802
    },
    {
      "epoch": 6.118079752226094,
      "grad_norm": 36.705631256103516,
      "learning_rate": 4.313244719748785e-06,
      "loss": 0.2087,
      "step": 15803
    },
    {
      "epoch": 6.118466898954704,
      "grad_norm": 38.073944091796875,
      "learning_rate": 4.312814556716996e-06,
      "loss": 1.9575,
      "step": 15804
    },
    {
      "epoch": 6.118854045683314,
      "grad_norm": 34.488285064697266,
      "learning_rate": 4.312384393685207e-06,
      "loss": 0.4528,
      "step": 15805
    },
    {
      "epoch": 6.119241192411924,
      "grad_norm": 31.62961196899414,
      "learning_rate": 4.311954230653418e-06,
      "loss": 0.3629,
      "step": 15806
    },
    {
      "epoch": 6.119628339140534,
      "grad_norm": 11.331825256347656,
      "learning_rate": 4.311524067621629e-06,
      "loss": 0.2927,
      "step": 15807
    },
    {
      "epoch": 6.1200154858691445,
      "grad_norm": 8.092854499816895,
      "learning_rate": 4.31109390458984e-06,
      "loss": 0.2973,
      "step": 15808
    },
    {
      "epoch": 6.120402632597755,
      "grad_norm": 8.315093994140625,
      "learning_rate": 4.310663741558051e-06,
      "loss": 0.2624,
      "step": 15809
    },
    {
      "epoch": 6.120789779326365,
      "grad_norm": 95.2670669555664,
      "learning_rate": 4.310233578526262e-06,
      "loss": 1.3922,
      "step": 15810
    },
    {
      "epoch": 6.121176926054975,
      "grad_norm": 163.62039184570312,
      "learning_rate": 4.309803415494473e-06,
      "loss": 0.9798,
      "step": 15811
    },
    {
      "epoch": 6.121564072783585,
      "grad_norm": 3.33052134513855,
      "learning_rate": 4.309373252462684e-06,
      "loss": 0.1862,
      "step": 15812
    },
    {
      "epoch": 6.121951219512195,
      "grad_norm": 34.53432083129883,
      "learning_rate": 4.308943089430894e-06,
      "loss": 0.6025,
      "step": 15813
    },
    {
      "epoch": 6.122338366240805,
      "grad_norm": 4.750860214233398,
      "learning_rate": 4.308512926399106e-06,
      "loss": 0.272,
      "step": 15814
    },
    {
      "epoch": 6.122725512969415,
      "grad_norm": 145.09483337402344,
      "learning_rate": 4.308082763367316e-06,
      "loss": 0.3352,
      "step": 15815
    },
    {
      "epoch": 6.123112659698026,
      "grad_norm": 56.488929748535156,
      "learning_rate": 4.307652600335527e-06,
      "loss": 1.2495,
      "step": 15816
    },
    {
      "epoch": 6.1234998064266355,
      "grad_norm": 21.312997817993164,
      "learning_rate": 4.307222437303738e-06,
      "loss": 0.9131,
      "step": 15817
    },
    {
      "epoch": 6.123886953155246,
      "grad_norm": 7.755805969238281,
      "learning_rate": 4.306792274271949e-06,
      "loss": 0.1178,
      "step": 15818
    },
    {
      "epoch": 6.124274099883856,
      "grad_norm": 96.55906677246094,
      "learning_rate": 4.30636211124016e-06,
      "loss": 2.1428,
      "step": 15819
    },
    {
      "epoch": 6.124661246612466,
      "grad_norm": 39.27421569824219,
      "learning_rate": 4.305931948208371e-06,
      "loss": 1.5526,
      "step": 15820
    },
    {
      "epoch": 6.125048393341077,
      "grad_norm": 54.5595703125,
      "learning_rate": 4.305501785176582e-06,
      "loss": 1.1091,
      "step": 15821
    },
    {
      "epoch": 6.125435540069686,
      "grad_norm": 73.37845611572266,
      "learning_rate": 4.305071622144793e-06,
      "loss": 0.5493,
      "step": 15822
    },
    {
      "epoch": 6.125822686798297,
      "grad_norm": 41.55517578125,
      "learning_rate": 4.304641459113004e-06,
      "loss": 0.1633,
      "step": 15823
    },
    {
      "epoch": 6.126209833526906,
      "grad_norm": 105.73979187011719,
      "learning_rate": 4.304211296081215e-06,
      "loss": 0.7618,
      "step": 15824
    },
    {
      "epoch": 6.126596980255517,
      "grad_norm": 90.6340103149414,
      "learning_rate": 4.303781133049426e-06,
      "loss": 0.3584,
      "step": 15825
    },
    {
      "epoch": 6.1269841269841265,
      "grad_norm": 106.02169036865234,
      "learning_rate": 4.303350970017637e-06,
      "loss": 2.1854,
      "step": 15826
    },
    {
      "epoch": 6.127371273712737,
      "grad_norm": 118.97230529785156,
      "learning_rate": 4.302920806985848e-06,
      "loss": 2.3319,
      "step": 15827
    },
    {
      "epoch": 6.1277584204413476,
      "grad_norm": 49.665618896484375,
      "learning_rate": 4.302490643954059e-06,
      "loss": 1.4992,
      "step": 15828
    },
    {
      "epoch": 6.128145567169957,
      "grad_norm": 82.16507720947266,
      "learning_rate": 4.30206048092227e-06,
      "loss": 1.1202,
      "step": 15829
    },
    {
      "epoch": 6.128532713898568,
      "grad_norm": 143.64117431640625,
      "learning_rate": 4.301630317890481e-06,
      "loss": 1.4757,
      "step": 15830
    },
    {
      "epoch": 6.128919860627177,
      "grad_norm": 23.233661651611328,
      "learning_rate": 4.3012001548586914e-06,
      "loss": 1.2053,
      "step": 15831
    },
    {
      "epoch": 6.129307007355788,
      "grad_norm": 92.11186218261719,
      "learning_rate": 4.300769991826903e-06,
      "loss": 2.7707,
      "step": 15832
    },
    {
      "epoch": 6.129694154084398,
      "grad_norm": 9.960221290588379,
      "learning_rate": 4.300339828795113e-06,
      "loss": 0.0937,
      "step": 15833
    },
    {
      "epoch": 6.130081300813008,
      "grad_norm": 35.0628547668457,
      "learning_rate": 4.299909665763325e-06,
      "loss": 0.2191,
      "step": 15834
    },
    {
      "epoch": 6.1304684475416185,
      "grad_norm": 69.14375305175781,
      "learning_rate": 4.299479502731535e-06,
      "loss": 1.1691,
      "step": 15835
    },
    {
      "epoch": 6.130855594270228,
      "grad_norm": 5.675145626068115,
      "learning_rate": 4.299049339699747e-06,
      "loss": 0.3227,
      "step": 15836
    },
    {
      "epoch": 6.131242740998839,
      "grad_norm": 48.146697998046875,
      "learning_rate": 4.298619176667957e-06,
      "loss": 1.3345,
      "step": 15837
    },
    {
      "epoch": 6.131629887727449,
      "grad_norm": 20.14503288269043,
      "learning_rate": 4.298189013636169e-06,
      "loss": 0.2943,
      "step": 15838
    },
    {
      "epoch": 6.132017034456059,
      "grad_norm": 9.584155082702637,
      "learning_rate": 4.297758850604379e-06,
      "loss": 0.1869,
      "step": 15839
    },
    {
      "epoch": 6.132404181184669,
      "grad_norm": 28.367746353149414,
      "learning_rate": 4.297328687572591e-06,
      "loss": 1.6549,
      "step": 15840
    },
    {
      "epoch": 6.132791327913279,
      "grad_norm": 92.79940795898438,
      "learning_rate": 4.296898524540801e-06,
      "loss": 2.3542,
      "step": 15841
    },
    {
      "epoch": 6.133178474641889,
      "grad_norm": 95.30482482910156,
      "learning_rate": 4.296468361509013e-06,
      "loss": 2.0159,
      "step": 15842
    },
    {
      "epoch": 6.133565621370499,
      "grad_norm": 34.435577392578125,
      "learning_rate": 4.296038198477223e-06,
      "loss": 1.8722,
      "step": 15843
    },
    {
      "epoch": 6.1339527680991095,
      "grad_norm": 128.0676727294922,
      "learning_rate": 4.295608035445435e-06,
      "loss": 1.8731,
      "step": 15844
    },
    {
      "epoch": 6.13433991482772,
      "grad_norm": 48.284828186035156,
      "learning_rate": 4.295177872413645e-06,
      "loss": 1.4147,
      "step": 15845
    },
    {
      "epoch": 6.13472706155633,
      "grad_norm": 20.579538345336914,
      "learning_rate": 4.294747709381856e-06,
      "loss": 0.9182,
      "step": 15846
    },
    {
      "epoch": 6.13511420828494,
      "grad_norm": 18.180553436279297,
      "learning_rate": 4.294317546350067e-06,
      "loss": 1.3169,
      "step": 15847
    },
    {
      "epoch": 6.13550135501355,
      "grad_norm": 75.1075210571289,
      "learning_rate": 4.293887383318278e-06,
      "loss": 0.4735,
      "step": 15848
    },
    {
      "epoch": 6.13588850174216,
      "grad_norm": 116.84059143066406,
      "learning_rate": 4.2934572202864884e-06,
      "loss": 1.6337,
      "step": 15849
    },
    {
      "epoch": 6.136275648470771,
      "grad_norm": 46.608211517333984,
      "learning_rate": 4.2930270572547e-06,
      "loss": 1.6193,
      "step": 15850
    },
    {
      "epoch": 6.13666279519938,
      "grad_norm": 35.33357238769531,
      "learning_rate": 4.2925968942229104e-06,
      "loss": 0.4588,
      "step": 15851
    },
    {
      "epoch": 6.137049941927991,
      "grad_norm": 34.256797790527344,
      "learning_rate": 4.292166731191122e-06,
      "loss": 1.2123,
      "step": 15852
    },
    {
      "epoch": 6.1374370886566005,
      "grad_norm": 109.406982421875,
      "learning_rate": 4.291736568159332e-06,
      "loss": 1.8143,
      "step": 15853
    },
    {
      "epoch": 6.137824235385211,
      "grad_norm": 34.3519287109375,
      "learning_rate": 4.291306405127544e-06,
      "loss": 1.9999,
      "step": 15854
    },
    {
      "epoch": 6.138211382113822,
      "grad_norm": 63.40446853637695,
      "learning_rate": 4.290876242095754e-06,
      "loss": 1.2877,
      "step": 15855
    },
    {
      "epoch": 6.138598528842431,
      "grad_norm": 20.49630355834961,
      "learning_rate": 4.290446079063966e-06,
      "loss": 0.3368,
      "step": 15856
    },
    {
      "epoch": 6.138985675571042,
      "grad_norm": 16.94408416748047,
      "learning_rate": 4.290015916032176e-06,
      "loss": 1.0683,
      "step": 15857
    },
    {
      "epoch": 6.139372822299651,
      "grad_norm": 32.4167366027832,
      "learning_rate": 4.289585753000388e-06,
      "loss": 3.2427,
      "step": 15858
    },
    {
      "epoch": 6.139759969028262,
      "grad_norm": 60.78056335449219,
      "learning_rate": 4.289155589968598e-06,
      "loss": 1.9498,
      "step": 15859
    },
    {
      "epoch": 6.140147115756871,
      "grad_norm": 3.03726863861084,
      "learning_rate": 4.28872542693681e-06,
      "loss": 0.1257,
      "step": 15860
    },
    {
      "epoch": 6.140534262485482,
      "grad_norm": 46.14522933959961,
      "learning_rate": 4.28829526390502e-06,
      "loss": 1.0333,
      "step": 15861
    },
    {
      "epoch": 6.1409214092140925,
      "grad_norm": 87.85910034179688,
      "learning_rate": 4.287865100873232e-06,
      "loss": 1.2075,
      "step": 15862
    },
    {
      "epoch": 6.141308555942702,
      "grad_norm": 109.84192657470703,
      "learning_rate": 4.287434937841442e-06,
      "loss": 0.8557,
      "step": 15863
    },
    {
      "epoch": 6.141695702671313,
      "grad_norm": 48.92852783203125,
      "learning_rate": 4.287004774809653e-06,
      "loss": 3.0914,
      "step": 15864
    },
    {
      "epoch": 6.142082849399922,
      "grad_norm": 41.57136154174805,
      "learning_rate": 4.286574611777864e-06,
      "loss": 0.4897,
      "step": 15865
    },
    {
      "epoch": 6.142469996128533,
      "grad_norm": 108.36912536621094,
      "learning_rate": 4.286144448746075e-06,
      "loss": 0.9562,
      "step": 15866
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 82.11722564697266,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 1.535,
      "step": 15867
    },
    {
      "epoch": 6.143244289585753,
      "grad_norm": 49.270999908447266,
      "learning_rate": 4.285284122682497e-06,
      "loss": 0.4282,
      "step": 15868
    },
    {
      "epoch": 6.143631436314363,
      "grad_norm": 40.99784469604492,
      "learning_rate": 4.2848539596507074e-06,
      "loss": 0.2218,
      "step": 15869
    },
    {
      "epoch": 6.144018583042973,
      "grad_norm": 54.594383239746094,
      "learning_rate": 4.284423796618919e-06,
      "loss": 3.2073,
      "step": 15870
    },
    {
      "epoch": 6.1444057297715835,
      "grad_norm": 25.853872299194336,
      "learning_rate": 4.2839936335871294e-06,
      "loss": 0.4627,
      "step": 15871
    },
    {
      "epoch": 6.144792876500194,
      "grad_norm": 65.5498275756836,
      "learning_rate": 4.283563470555341e-06,
      "loss": 1.1158,
      "step": 15872
    },
    {
      "epoch": 6.145180023228804,
      "grad_norm": 5.1215033531188965,
      "learning_rate": 4.283133307523552e-06,
      "loss": 0.3247,
      "step": 15873
    },
    {
      "epoch": 6.145567169957414,
      "grad_norm": 29.63260269165039,
      "learning_rate": 4.282703144491763e-06,
      "loss": 3.9868,
      "step": 15874
    },
    {
      "epoch": 6.145954316686024,
      "grad_norm": 17.370464324951172,
      "learning_rate": 4.282272981459974e-06,
      "loss": 0.3556,
      "step": 15875
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 4.561586856842041,
      "learning_rate": 4.281842818428185e-06,
      "loss": 0.1639,
      "step": 15876
    },
    {
      "epoch": 6.146728610143244,
      "grad_norm": 139.06536865234375,
      "learning_rate": 4.281412655396396e-06,
      "loss": 1.1377,
      "step": 15877
    },
    {
      "epoch": 6.147115756871854,
      "grad_norm": 18.989355087280273,
      "learning_rate": 4.280982492364607e-06,
      "loss": 0.6241,
      "step": 15878
    },
    {
      "epoch": 6.147502903600465,
      "grad_norm": 43.0738639831543,
      "learning_rate": 4.280552329332817e-06,
      "loss": 1.1776,
      "step": 15879
    },
    {
      "epoch": 6.1478900503290745,
      "grad_norm": 84.97856903076172,
      "learning_rate": 4.280122166301029e-06,
      "loss": 0.4626,
      "step": 15880
    },
    {
      "epoch": 6.148277197057685,
      "grad_norm": 30.99712562561035,
      "learning_rate": 4.279692003269239e-06,
      "loss": 0.4656,
      "step": 15881
    },
    {
      "epoch": 6.148664343786295,
      "grad_norm": 33.1590461730957,
      "learning_rate": 4.27926184023745e-06,
      "loss": 1.4842,
      "step": 15882
    },
    {
      "epoch": 6.149051490514905,
      "grad_norm": 66.55416870117188,
      "learning_rate": 4.278831677205661e-06,
      "loss": 1.3496,
      "step": 15883
    },
    {
      "epoch": 6.149438637243516,
      "grad_norm": 17.24201011657715,
      "learning_rate": 4.278401514173872e-06,
      "loss": 0.8886,
      "step": 15884
    },
    {
      "epoch": 6.149825783972125,
      "grad_norm": 103.06387329101562,
      "learning_rate": 4.277971351142083e-06,
      "loss": 1.4774,
      "step": 15885
    },
    {
      "epoch": 6.150212930700736,
      "grad_norm": 19.51656723022461,
      "learning_rate": 4.277541188110294e-06,
      "loss": 1.464,
      "step": 15886
    },
    {
      "epoch": 6.1506000774293454,
      "grad_norm": 6.87163782119751,
      "learning_rate": 4.277111025078505e-06,
      "loss": 0.3197,
      "step": 15887
    },
    {
      "epoch": 6.150987224157956,
      "grad_norm": 47.993045806884766,
      "learning_rate": 4.276680862046716e-06,
      "loss": 0.5155,
      "step": 15888
    },
    {
      "epoch": 6.151374370886566,
      "grad_norm": 54.14578628540039,
      "learning_rate": 4.276250699014927e-06,
      "loss": 2.1152,
      "step": 15889
    },
    {
      "epoch": 6.151761517615176,
      "grad_norm": 7.320725917816162,
      "learning_rate": 4.275820535983138e-06,
      "loss": 0.1828,
      "step": 15890
    },
    {
      "epoch": 6.152148664343787,
      "grad_norm": 37.61144256591797,
      "learning_rate": 4.275390372951349e-06,
      "loss": 1.0653,
      "step": 15891
    },
    {
      "epoch": 6.152535811072396,
      "grad_norm": 47.42014694213867,
      "learning_rate": 4.27496020991956e-06,
      "loss": 1.3409,
      "step": 15892
    },
    {
      "epoch": 6.152922957801007,
      "grad_norm": 11.757681846618652,
      "learning_rate": 4.274530046887771e-06,
      "loss": 0.1402,
      "step": 15893
    },
    {
      "epoch": 6.153310104529616,
      "grad_norm": 42.315799713134766,
      "learning_rate": 4.274099883855982e-06,
      "loss": 2.1041,
      "step": 15894
    },
    {
      "epoch": 6.153697251258227,
      "grad_norm": 53.92271423339844,
      "learning_rate": 4.273669720824193e-06,
      "loss": 1.0756,
      "step": 15895
    },
    {
      "epoch": 6.154084397986837,
      "grad_norm": 5.002671718597412,
      "learning_rate": 4.273239557792404e-06,
      "loss": 0.3676,
      "step": 15896
    },
    {
      "epoch": 6.154471544715447,
      "grad_norm": 37.23592758178711,
      "learning_rate": 4.272809394760614e-06,
      "loss": 1.0404,
      "step": 15897
    },
    {
      "epoch": 6.1548586914440575,
      "grad_norm": 17.529403686523438,
      "learning_rate": 4.272379231728826e-06,
      "loss": 0.3719,
      "step": 15898
    },
    {
      "epoch": 6.155245838172667,
      "grad_norm": 17.899192810058594,
      "learning_rate": 4.271949068697036e-06,
      "loss": 0.2273,
      "step": 15899
    },
    {
      "epoch": 6.155632984901278,
      "grad_norm": 44.83864974975586,
      "learning_rate": 4.271518905665247e-06,
      "loss": 2.2036,
      "step": 15900
    },
    {
      "epoch": 6.156020131629888,
      "grad_norm": 35.09907150268555,
      "learning_rate": 4.271088742633458e-06,
      "loss": 2.2289,
      "step": 15901
    },
    {
      "epoch": 6.156407278358498,
      "grad_norm": 20.209646224975586,
      "learning_rate": 4.270658579601669e-06,
      "loss": 2.3493,
      "step": 15902
    },
    {
      "epoch": 6.156794425087108,
      "grad_norm": 41.470184326171875,
      "learning_rate": 4.27022841656988e-06,
      "loss": 0.4312,
      "step": 15903
    },
    {
      "epoch": 6.157181571815718,
      "grad_norm": 1.7261347770690918,
      "learning_rate": 4.269798253538091e-06,
      "loss": 0.0572,
      "step": 15904
    },
    {
      "epoch": 6.157568718544328,
      "grad_norm": 42.57576370239258,
      "learning_rate": 4.269368090506302e-06,
      "loss": 0.4854,
      "step": 15905
    },
    {
      "epoch": 6.157955865272938,
      "grad_norm": 21.296802520751953,
      "learning_rate": 4.268937927474513e-06,
      "loss": 1.753,
      "step": 15906
    },
    {
      "epoch": 6.1583430120015485,
      "grad_norm": 169.3287811279297,
      "learning_rate": 4.268507764442724e-06,
      "loss": 2.0004,
      "step": 15907
    },
    {
      "epoch": 6.158730158730159,
      "grad_norm": 37.01972579956055,
      "learning_rate": 4.268077601410935e-06,
      "loss": 0.6334,
      "step": 15908
    },
    {
      "epoch": 6.159117305458769,
      "grad_norm": 35.9378547668457,
      "learning_rate": 4.267647438379146e-06,
      "loss": 1.0433,
      "step": 15909
    },
    {
      "epoch": 6.159504452187379,
      "grad_norm": 27.806650161743164,
      "learning_rate": 4.267217275347357e-06,
      "loss": 1.4091,
      "step": 15910
    },
    {
      "epoch": 6.159891598915989,
      "grad_norm": 64.91154479980469,
      "learning_rate": 4.266787112315568e-06,
      "loss": 2.1339,
      "step": 15911
    },
    {
      "epoch": 6.160278745644599,
      "grad_norm": 142.731201171875,
      "learning_rate": 4.266356949283779e-06,
      "loss": 2.4011,
      "step": 15912
    },
    {
      "epoch": 6.16066589237321,
      "grad_norm": 91.41876220703125,
      "learning_rate": 4.26592678625199e-06,
      "loss": 2.1658,
      "step": 15913
    },
    {
      "epoch": 6.1610530391018195,
      "grad_norm": 50.55134582519531,
      "learning_rate": 4.265496623220201e-06,
      "loss": 3.0578,
      "step": 15914
    },
    {
      "epoch": 6.16144018583043,
      "grad_norm": 105.88679504394531,
      "learning_rate": 4.265066460188411e-06,
      "loss": 1.0817,
      "step": 15915
    },
    {
      "epoch": 6.16182733255904,
      "grad_norm": 3.3717801570892334,
      "learning_rate": 4.264636297156623e-06,
      "loss": 0.1399,
      "step": 15916
    },
    {
      "epoch": 6.16221447928765,
      "grad_norm": 72.80903625488281,
      "learning_rate": 4.264206134124833e-06,
      "loss": 0.6001,
      "step": 15917
    },
    {
      "epoch": 6.16260162601626,
      "grad_norm": 29.229032516479492,
      "learning_rate": 4.263775971093045e-06,
      "loss": 0.7546,
      "step": 15918
    },
    {
      "epoch": 6.16298877274487,
      "grad_norm": 95.48462677001953,
      "learning_rate": 4.263345808061255e-06,
      "loss": 1.334,
      "step": 15919
    },
    {
      "epoch": 6.163375919473481,
      "grad_norm": 62.27566909790039,
      "learning_rate": 4.262915645029467e-06,
      "loss": 1.2302,
      "step": 15920
    },
    {
      "epoch": 6.16376306620209,
      "grad_norm": 28.325763702392578,
      "learning_rate": 4.262485481997677e-06,
      "loss": 0.4055,
      "step": 15921
    },
    {
      "epoch": 6.164150212930701,
      "grad_norm": 60.695682525634766,
      "learning_rate": 4.262055318965889e-06,
      "loss": 0.4548,
      "step": 15922
    },
    {
      "epoch": 6.1645373596593105,
      "grad_norm": 36.141536712646484,
      "learning_rate": 4.261625155934099e-06,
      "loss": 1.4056,
      "step": 15923
    },
    {
      "epoch": 6.164924506387921,
      "grad_norm": 33.23031997680664,
      "learning_rate": 4.261194992902311e-06,
      "loss": 1.7772,
      "step": 15924
    },
    {
      "epoch": 6.1653116531165315,
      "grad_norm": 5.323408126831055,
      "learning_rate": 4.260764829870521e-06,
      "loss": 0.2665,
      "step": 15925
    },
    {
      "epoch": 6.165698799845141,
      "grad_norm": 42.76238250732422,
      "learning_rate": 4.260334666838733e-06,
      "loss": 1.3477,
      "step": 15926
    },
    {
      "epoch": 6.166085946573752,
      "grad_norm": 24.328649520874023,
      "learning_rate": 4.259904503806943e-06,
      "loss": 1.2277,
      "step": 15927
    },
    {
      "epoch": 6.166473093302361,
      "grad_norm": 35.20673370361328,
      "learning_rate": 4.259474340775155e-06,
      "loss": 1.6821,
      "step": 15928
    },
    {
      "epoch": 6.166860240030972,
      "grad_norm": 134.125,
      "learning_rate": 4.259044177743365e-06,
      "loss": 1.4577,
      "step": 15929
    },
    {
      "epoch": 6.167247386759582,
      "grad_norm": 26.338117599487305,
      "learning_rate": 4.258614014711576e-06,
      "loss": 0.3624,
      "step": 15930
    },
    {
      "epoch": 6.167634533488192,
      "grad_norm": 51.89132308959961,
      "learning_rate": 4.258183851679787e-06,
      "loss": 2.5278,
      "step": 15931
    },
    {
      "epoch": 6.168021680216802,
      "grad_norm": 99.25555419921875,
      "learning_rate": 4.257753688647998e-06,
      "loss": 0.7115,
      "step": 15932
    },
    {
      "epoch": 6.168408826945412,
      "grad_norm": 69.24518585205078,
      "learning_rate": 4.257323525616208e-06,
      "loss": 0.8909,
      "step": 15933
    },
    {
      "epoch": 6.1687959736740225,
      "grad_norm": 43.97584533691406,
      "learning_rate": 4.25689336258442e-06,
      "loss": 0.6063,
      "step": 15934
    },
    {
      "epoch": 6.169183120402632,
      "grad_norm": 61.64397048950195,
      "learning_rate": 4.25646319955263e-06,
      "loss": 1.1441,
      "step": 15935
    },
    {
      "epoch": 6.169570267131243,
      "grad_norm": 3.9914886951446533,
      "learning_rate": 4.256033036520842e-06,
      "loss": 0.1382,
      "step": 15936
    },
    {
      "epoch": 6.169957413859853,
      "grad_norm": 2.19958758354187,
      "learning_rate": 4.255602873489052e-06,
      "loss": 0.0794,
      "step": 15937
    },
    {
      "epoch": 6.170344560588463,
      "grad_norm": 120.67630004882812,
      "learning_rate": 4.255172710457264e-06,
      "loss": 1.4074,
      "step": 15938
    },
    {
      "epoch": 6.170731707317073,
      "grad_norm": 56.03476333618164,
      "learning_rate": 4.254742547425474e-06,
      "loss": 2.5356,
      "step": 15939
    },
    {
      "epoch": 6.171118854045683,
      "grad_norm": 22.810049057006836,
      "learning_rate": 4.254312384393686e-06,
      "loss": 0.3561,
      "step": 15940
    },
    {
      "epoch": 6.1715060007742935,
      "grad_norm": 37.14510726928711,
      "learning_rate": 4.253882221361896e-06,
      "loss": 0.6832,
      "step": 15941
    },
    {
      "epoch": 6.171893147502904,
      "grad_norm": 96.1570816040039,
      "learning_rate": 4.253452058330108e-06,
      "loss": 0.5099,
      "step": 15942
    },
    {
      "epoch": 6.172280294231514,
      "grad_norm": 128.4961395263672,
      "learning_rate": 4.253021895298318e-06,
      "loss": 3.8526,
      "step": 15943
    },
    {
      "epoch": 6.172667440960124,
      "grad_norm": 53.42757034301758,
      "learning_rate": 4.25259173226653e-06,
      "loss": 0.2705,
      "step": 15944
    },
    {
      "epoch": 6.173054587688734,
      "grad_norm": 3.959078788757324,
      "learning_rate": 4.25216156923474e-06,
      "loss": 0.0868,
      "step": 15945
    },
    {
      "epoch": 6.173441734417344,
      "grad_norm": 13.054715156555176,
      "learning_rate": 4.251731406202952e-06,
      "loss": 0.4689,
      "step": 15946
    },
    {
      "epoch": 6.173828881145955,
      "grad_norm": 30.39086151123047,
      "learning_rate": 4.251301243171162e-06,
      "loss": 1.6926,
      "step": 15947
    },
    {
      "epoch": 6.174216027874564,
      "grad_norm": 33.91019058227539,
      "learning_rate": 4.250871080139373e-06,
      "loss": 1.3109,
      "step": 15948
    },
    {
      "epoch": 6.174603174603175,
      "grad_norm": 79.18241119384766,
      "learning_rate": 4.250440917107584e-06,
      "loss": 0.8879,
      "step": 15949
    },
    {
      "epoch": 6.1749903213317845,
      "grad_norm": 36.93363571166992,
      "learning_rate": 4.250010754075795e-06,
      "loss": 1.4338,
      "step": 15950
    },
    {
      "epoch": 6.175377468060395,
      "grad_norm": 69.5132064819336,
      "learning_rate": 4.249580591044005e-06,
      "loss": 1.0876,
      "step": 15951
    },
    {
      "epoch": 6.175764614789005,
      "grad_norm": 12.620458602905273,
      "learning_rate": 4.249150428012217e-06,
      "loss": 0.246,
      "step": 15952
    },
    {
      "epoch": 6.176151761517615,
      "grad_norm": 30.724130630493164,
      "learning_rate": 4.248720264980427e-06,
      "loss": 0.238,
      "step": 15953
    },
    {
      "epoch": 6.176538908246226,
      "grad_norm": 75.29344177246094,
      "learning_rate": 4.248290101948639e-06,
      "loss": 1.2138,
      "step": 15954
    },
    {
      "epoch": 6.176926054974835,
      "grad_norm": 40.59165954589844,
      "learning_rate": 4.247859938916849e-06,
      "loss": 1.6433,
      "step": 15955
    },
    {
      "epoch": 6.177313201703446,
      "grad_norm": 40.348358154296875,
      "learning_rate": 4.247429775885061e-06,
      "loss": 0.713,
      "step": 15956
    },
    {
      "epoch": 6.177700348432055,
      "grad_norm": 21.581451416015625,
      "learning_rate": 4.246999612853272e-06,
      "loss": 1.9049,
      "step": 15957
    },
    {
      "epoch": 6.178087495160666,
      "grad_norm": 90.61471557617188,
      "learning_rate": 4.246569449821483e-06,
      "loss": 0.5021,
      "step": 15958
    },
    {
      "epoch": 6.178474641889276,
      "grad_norm": 50.877197265625,
      "learning_rate": 4.246139286789694e-06,
      "loss": 2.423,
      "step": 15959
    },
    {
      "epoch": 6.178861788617886,
      "grad_norm": 10.029766082763672,
      "learning_rate": 4.245709123757905e-06,
      "loss": 0.3226,
      "step": 15960
    },
    {
      "epoch": 6.1792489353464966,
      "grad_norm": 75.47479248046875,
      "learning_rate": 4.245278960726116e-06,
      "loss": 3.8052,
      "step": 15961
    },
    {
      "epoch": 6.179636082075106,
      "grad_norm": 30.512197494506836,
      "learning_rate": 4.244848797694327e-06,
      "loss": 0.2469,
      "step": 15962
    },
    {
      "epoch": 6.180023228803717,
      "grad_norm": 12.551444053649902,
      "learning_rate": 4.244418634662537e-06,
      "loss": 0.2545,
      "step": 15963
    },
    {
      "epoch": 6.180410375532327,
      "grad_norm": 3.5170609951019287,
      "learning_rate": 4.243988471630749e-06,
      "loss": 0.15,
      "step": 15964
    },
    {
      "epoch": 6.180797522260937,
      "grad_norm": 21.5907039642334,
      "learning_rate": 4.243558308598959e-06,
      "loss": 0.4417,
      "step": 15965
    },
    {
      "epoch": 6.181184668989547,
      "grad_norm": 98.60734558105469,
      "learning_rate": 4.24312814556717e-06,
      "loss": 1.5434,
      "step": 15966
    },
    {
      "epoch": 6.181571815718157,
      "grad_norm": 5.2445197105407715,
      "learning_rate": 4.242697982535381e-06,
      "loss": 0.2509,
      "step": 15967
    },
    {
      "epoch": 6.1819589624467675,
      "grad_norm": 59.32805633544922,
      "learning_rate": 4.242267819503592e-06,
      "loss": 1.1102,
      "step": 15968
    },
    {
      "epoch": 6.182346109175377,
      "grad_norm": 8.3984375,
      "learning_rate": 4.241837656471803e-06,
      "loss": 0.203,
      "step": 15969
    },
    {
      "epoch": 6.182733255903988,
      "grad_norm": 36.62947082519531,
      "learning_rate": 4.241407493440014e-06,
      "loss": 0.5566,
      "step": 15970
    },
    {
      "epoch": 6.183120402632598,
      "grad_norm": 134.21128845214844,
      "learning_rate": 4.240977330408225e-06,
      "loss": 0.628,
      "step": 15971
    },
    {
      "epoch": 6.183507549361208,
      "grad_norm": 12.410181045532227,
      "learning_rate": 4.240547167376436e-06,
      "loss": 0.4444,
      "step": 15972
    },
    {
      "epoch": 6.183894696089818,
      "grad_norm": 303.3728332519531,
      "learning_rate": 4.240117004344647e-06,
      "loss": 0.9545,
      "step": 15973
    },
    {
      "epoch": 6.184281842818428,
      "grad_norm": 97.13829803466797,
      "learning_rate": 4.239686841312858e-06,
      "loss": 2.9553,
      "step": 15974
    },
    {
      "epoch": 6.184668989547038,
      "grad_norm": 28.50130271911621,
      "learning_rate": 4.239256678281069e-06,
      "loss": 0.9527,
      "step": 15975
    },
    {
      "epoch": 6.185056136275649,
      "grad_norm": 21.92306137084961,
      "learning_rate": 4.23882651524928e-06,
      "loss": 0.4857,
      "step": 15976
    },
    {
      "epoch": 6.1854432830042585,
      "grad_norm": 46.910823822021484,
      "learning_rate": 4.238396352217491e-06,
      "loss": 1.3339,
      "step": 15977
    },
    {
      "epoch": 6.185830429732869,
      "grad_norm": 26.033723831176758,
      "learning_rate": 4.237966189185702e-06,
      "loss": 1.5659,
      "step": 15978
    },
    {
      "epoch": 6.186217576461479,
      "grad_norm": 30.01841163635254,
      "learning_rate": 4.237536026153913e-06,
      "loss": 0.9733,
      "step": 15979
    },
    {
      "epoch": 6.186604723190089,
      "grad_norm": 48.818912506103516,
      "learning_rate": 4.237105863122124e-06,
      "loss": 1.4782,
      "step": 15980
    },
    {
      "epoch": 6.186991869918699,
      "grad_norm": 2.628459930419922,
      "learning_rate": 4.236675700090334e-06,
      "loss": 0.0785,
      "step": 15981
    },
    {
      "epoch": 6.187379016647309,
      "grad_norm": 95.96076965332031,
      "learning_rate": 4.236245537058546e-06,
      "loss": 0.6632,
      "step": 15982
    },
    {
      "epoch": 6.18776616337592,
      "grad_norm": 47.16115188598633,
      "learning_rate": 4.235815374026756e-06,
      "loss": 0.4762,
      "step": 15983
    },
    {
      "epoch": 6.188153310104529,
      "grad_norm": 50.52618408203125,
      "learning_rate": 4.235385210994967e-06,
      "loss": 2.1121,
      "step": 15984
    },
    {
      "epoch": 6.18854045683314,
      "grad_norm": 5.377509117126465,
      "learning_rate": 4.234955047963178e-06,
      "loss": 0.1594,
      "step": 15985
    },
    {
      "epoch": 6.1889276035617495,
      "grad_norm": 11.99035358428955,
      "learning_rate": 4.234524884931389e-06,
      "loss": 0.2226,
      "step": 15986
    },
    {
      "epoch": 6.18931475029036,
      "grad_norm": 44.783905029296875,
      "learning_rate": 4.2340947218996e-06,
      "loss": 0.327,
      "step": 15987
    },
    {
      "epoch": 6.1897018970189706,
      "grad_norm": 100.13462829589844,
      "learning_rate": 4.233664558867811e-06,
      "loss": 1.4268,
      "step": 15988
    },
    {
      "epoch": 6.19008904374758,
      "grad_norm": 11.418112754821777,
      "learning_rate": 4.233234395836022e-06,
      "loss": 0.1743,
      "step": 15989
    },
    {
      "epoch": 6.190476190476191,
      "grad_norm": 48.571495056152344,
      "learning_rate": 4.232804232804233e-06,
      "loss": 1.402,
      "step": 15990
    },
    {
      "epoch": 6.1908633372048,
      "grad_norm": 82.29216766357422,
      "learning_rate": 4.232374069772444e-06,
      "loss": 0.3685,
      "step": 15991
    },
    {
      "epoch": 6.191250483933411,
      "grad_norm": 19.703540802001953,
      "learning_rate": 4.231943906740655e-06,
      "loss": 1.2475,
      "step": 15992
    },
    {
      "epoch": 6.191637630662021,
      "grad_norm": 39.68526840209961,
      "learning_rate": 4.231513743708866e-06,
      "loss": 1.0093,
      "step": 15993
    },
    {
      "epoch": 6.192024777390631,
      "grad_norm": 3.1581132411956787,
      "learning_rate": 4.231083580677077e-06,
      "loss": 0.0901,
      "step": 15994
    },
    {
      "epoch": 6.1924119241192415,
      "grad_norm": 4.862890720367432,
      "learning_rate": 4.230653417645288e-06,
      "loss": 0.2378,
      "step": 15995
    },
    {
      "epoch": 6.192799070847851,
      "grad_norm": 48.2930908203125,
      "learning_rate": 4.230223254613499e-06,
      "loss": 1.8134,
      "step": 15996
    },
    {
      "epoch": 6.193186217576462,
      "grad_norm": 7.632140636444092,
      "learning_rate": 4.22979309158171e-06,
      "loss": 0.3548,
      "step": 15997
    },
    {
      "epoch": 6.193573364305071,
      "grad_norm": 5.553513050079346,
      "learning_rate": 4.229362928549921e-06,
      "loss": 0.264,
      "step": 15998
    },
    {
      "epoch": 6.193960511033682,
      "grad_norm": 1.135972499847412,
      "learning_rate": 4.228932765518131e-06,
      "loss": 0.0422,
      "step": 15999
    },
    {
      "epoch": 6.194347657762292,
      "grad_norm": 39.28239440917969,
      "learning_rate": 4.228502602486343e-06,
      "loss": 0.3317,
      "step": 16000
    },
    {
      "epoch": 6.194734804490902,
      "grad_norm": 1.8015844821929932,
      "learning_rate": 4.228072439454553e-06,
      "loss": 0.0372,
      "step": 16001
    },
    {
      "epoch": 6.195121951219512,
      "grad_norm": 76.20970916748047,
      "learning_rate": 4.227642276422765e-06,
      "loss": 2.7387,
      "step": 16002
    },
    {
      "epoch": 6.195509097948122,
      "grad_norm": 21.54056167602539,
      "learning_rate": 4.227212113390975e-06,
      "loss": 0.2616,
      "step": 16003
    },
    {
      "epoch": 6.1958962446767325,
      "grad_norm": 34.27851104736328,
      "learning_rate": 4.226781950359187e-06,
      "loss": 4.2111,
      "step": 16004
    },
    {
      "epoch": 6.196283391405343,
      "grad_norm": 24.898239135742188,
      "learning_rate": 4.226351787327397e-06,
      "loss": 0.5891,
      "step": 16005
    },
    {
      "epoch": 6.196670538133953,
      "grad_norm": 2.5856993198394775,
      "learning_rate": 4.225921624295609e-06,
      "loss": 0.1179,
      "step": 16006
    },
    {
      "epoch": 6.197057684862563,
      "grad_norm": 35.27410888671875,
      "learning_rate": 4.225491461263819e-06,
      "loss": 1.3833,
      "step": 16007
    },
    {
      "epoch": 6.197444831591173,
      "grad_norm": 57.29146194458008,
      "learning_rate": 4.225061298232031e-06,
      "loss": 0.5803,
      "step": 16008
    },
    {
      "epoch": 6.197831978319783,
      "grad_norm": 1.2330313920974731,
      "learning_rate": 4.224631135200241e-06,
      "loss": 0.0217,
      "step": 16009
    },
    {
      "epoch": 6.198219125048393,
      "grad_norm": 10.424817085266113,
      "learning_rate": 4.224200972168453e-06,
      "loss": 0.142,
      "step": 16010
    },
    {
      "epoch": 6.198606271777003,
      "grad_norm": 50.41172790527344,
      "learning_rate": 4.223770809136663e-06,
      "loss": 2.7053,
      "step": 16011
    },
    {
      "epoch": 6.198993418505614,
      "grad_norm": 21.652814865112305,
      "learning_rate": 4.223340646104875e-06,
      "loss": 1.8135,
      "step": 16012
    },
    {
      "epoch": 6.1993805652342235,
      "grad_norm": 1.686539649963379,
      "learning_rate": 4.222910483073085e-06,
      "loss": 0.0688,
      "step": 16013
    },
    {
      "epoch": 6.199767711962834,
      "grad_norm": 236.80429077148438,
      "learning_rate": 4.222480320041296e-06,
      "loss": 2.4088,
      "step": 16014
    },
    {
      "epoch": 6.200154858691444,
      "grad_norm": 61.01065444946289,
      "learning_rate": 4.222050157009507e-06,
      "loss": 1.2349,
      "step": 16015
    },
    {
      "epoch": 6.200542005420054,
      "grad_norm": 56.35344314575195,
      "learning_rate": 4.221619993977718e-06,
      "loss": 0.8628,
      "step": 16016
    },
    {
      "epoch": 6.200929152148665,
      "grad_norm": 23.44645881652832,
      "learning_rate": 4.221189830945928e-06,
      "loss": 1.4142,
      "step": 16017
    },
    {
      "epoch": 6.201316298877274,
      "grad_norm": 43.77815628051758,
      "learning_rate": 4.22075966791414e-06,
      "loss": 0.6806,
      "step": 16018
    },
    {
      "epoch": 6.201703445605885,
      "grad_norm": 27.897741317749023,
      "learning_rate": 4.22032950488235e-06,
      "loss": 0.6932,
      "step": 16019
    },
    {
      "epoch": 6.2020905923344944,
      "grad_norm": 78.96044921875,
      "learning_rate": 4.219899341850562e-06,
      "loss": 0.9577,
      "step": 16020
    },
    {
      "epoch": 6.202477739063105,
      "grad_norm": 45.75065612792969,
      "learning_rate": 4.219469178818772e-06,
      "loss": 0.5125,
      "step": 16021
    },
    {
      "epoch": 6.2028648857917155,
      "grad_norm": 7.526527404785156,
      "learning_rate": 4.219039015786984e-06,
      "loss": 0.1007,
      "step": 16022
    },
    {
      "epoch": 6.203252032520325,
      "grad_norm": 9.594670295715332,
      "learning_rate": 4.218608852755194e-06,
      "loss": 0.3439,
      "step": 16023
    },
    {
      "epoch": 6.203639179248936,
      "grad_norm": 79.10855865478516,
      "learning_rate": 4.218178689723406e-06,
      "loss": 0.8188,
      "step": 16024
    },
    {
      "epoch": 6.204026325977545,
      "grad_norm": 1.967631459236145,
      "learning_rate": 4.217748526691616e-06,
      "loss": 0.0737,
      "step": 16025
    },
    {
      "epoch": 6.204413472706156,
      "grad_norm": 23.691083908081055,
      "learning_rate": 4.217318363659828e-06,
      "loss": 1.0296,
      "step": 16026
    },
    {
      "epoch": 6.204800619434765,
      "grad_norm": 67.0125961303711,
      "learning_rate": 4.216888200628038e-06,
      "loss": 1.1453,
      "step": 16027
    },
    {
      "epoch": 6.205187766163376,
      "grad_norm": 32.65027618408203,
      "learning_rate": 4.21645803759625e-06,
      "loss": 1.7308,
      "step": 16028
    },
    {
      "epoch": 6.205574912891986,
      "grad_norm": 55.2778434753418,
      "learning_rate": 4.21602787456446e-06,
      "loss": 1.1314,
      "step": 16029
    },
    {
      "epoch": 6.205962059620596,
      "grad_norm": 81.12899780273438,
      "learning_rate": 4.215597711532672e-06,
      "loss": 3.2766,
      "step": 16030
    },
    {
      "epoch": 6.2063492063492065,
      "grad_norm": 24.855907440185547,
      "learning_rate": 4.215167548500882e-06,
      "loss": 3.5795,
      "step": 16031
    },
    {
      "epoch": 6.206736353077816,
      "grad_norm": 12.891149520874023,
      "learning_rate": 4.214737385469093e-06,
      "loss": 0.237,
      "step": 16032
    },
    {
      "epoch": 6.207123499806427,
      "grad_norm": 0.36632150411605835,
      "learning_rate": 4.214307222437304e-06,
      "loss": 0.0104,
      "step": 16033
    },
    {
      "epoch": 6.207510646535037,
      "grad_norm": 11.609025955200195,
      "learning_rate": 4.213877059405515e-06,
      "loss": 0.2131,
      "step": 16034
    },
    {
      "epoch": 6.207897793263647,
      "grad_norm": 22.13773536682129,
      "learning_rate": 4.213446896373725e-06,
      "loss": 0.6575,
      "step": 16035
    },
    {
      "epoch": 6.208284939992257,
      "grad_norm": 6.473174095153809,
      "learning_rate": 4.213016733341937e-06,
      "loss": 0.1295,
      "step": 16036
    },
    {
      "epoch": 6.208672086720867,
      "grad_norm": 38.12356948852539,
      "learning_rate": 4.212586570310147e-06,
      "loss": 0.224,
      "step": 16037
    },
    {
      "epoch": 6.209059233449477,
      "grad_norm": 38.872283935546875,
      "learning_rate": 4.212156407278359e-06,
      "loss": 1.2878,
      "step": 16038
    },
    {
      "epoch": 6.209446380178088,
      "grad_norm": 5.922656536102295,
      "learning_rate": 4.21172624424657e-06,
      "loss": 0.1878,
      "step": 16039
    },
    {
      "epoch": 6.2098335269066975,
      "grad_norm": 61.6564826965332,
      "learning_rate": 4.211296081214781e-06,
      "loss": 0.6888,
      "step": 16040
    },
    {
      "epoch": 6.210220673635308,
      "grad_norm": 8.882497787475586,
      "learning_rate": 4.210865918182992e-06,
      "loss": 0.3895,
      "step": 16041
    },
    {
      "epoch": 6.210607820363918,
      "grad_norm": 14.742898941040039,
      "learning_rate": 4.210435755151203e-06,
      "loss": 0.1853,
      "step": 16042
    },
    {
      "epoch": 6.210994967092528,
      "grad_norm": 5.220696449279785,
      "learning_rate": 4.210005592119414e-06,
      "loss": 0.2035,
      "step": 16043
    },
    {
      "epoch": 6.211382113821138,
      "grad_norm": 47.6777229309082,
      "learning_rate": 4.209575429087625e-06,
      "loss": 2.0116,
      "step": 16044
    },
    {
      "epoch": 6.211769260549748,
      "grad_norm": 129.7741241455078,
      "learning_rate": 4.209145266055836e-06,
      "loss": 1.3748,
      "step": 16045
    },
    {
      "epoch": 6.212156407278359,
      "grad_norm": 57.899131774902344,
      "learning_rate": 4.208715103024047e-06,
      "loss": 3.0322,
      "step": 16046
    },
    {
      "epoch": 6.2125435540069684,
      "grad_norm": 70.53904724121094,
      "learning_rate": 4.208284939992257e-06,
      "loss": 1.4344,
      "step": 16047
    },
    {
      "epoch": 6.212930700735579,
      "grad_norm": 54.416202545166016,
      "learning_rate": 4.207854776960469e-06,
      "loss": 0.7072,
      "step": 16048
    },
    {
      "epoch": 6.213317847464189,
      "grad_norm": 0.38174644112586975,
      "learning_rate": 4.207424613928679e-06,
      "loss": 0.0105,
      "step": 16049
    },
    {
      "epoch": 6.213704994192799,
      "grad_norm": 113.27318572998047,
      "learning_rate": 4.20699445089689e-06,
      "loss": 1.0192,
      "step": 16050
    },
    {
      "epoch": 6.21409214092141,
      "grad_norm": 262.58709716796875,
      "learning_rate": 4.206564287865101e-06,
      "loss": 2.6463,
      "step": 16051
    },
    {
      "epoch": 6.214479287650019,
      "grad_norm": 4.095371246337891,
      "learning_rate": 4.206134124833312e-06,
      "loss": 0.0999,
      "step": 16052
    },
    {
      "epoch": 6.21486643437863,
      "grad_norm": 34.5947380065918,
      "learning_rate": 4.205703961801523e-06,
      "loss": 1.6244,
      "step": 16053
    },
    {
      "epoch": 6.215253581107239,
      "grad_norm": 47.40716552734375,
      "learning_rate": 4.205273798769734e-06,
      "loss": 0.6335,
      "step": 16054
    },
    {
      "epoch": 6.21564072783585,
      "grad_norm": 36.264312744140625,
      "learning_rate": 4.204843635737945e-06,
      "loss": 1.4256,
      "step": 16055
    },
    {
      "epoch": 6.21602787456446,
      "grad_norm": 6.676345348358154,
      "learning_rate": 4.204413472706156e-06,
      "loss": 0.3111,
      "step": 16056
    },
    {
      "epoch": 6.21641502129307,
      "grad_norm": 100.28607177734375,
      "learning_rate": 4.203983309674367e-06,
      "loss": 1.3981,
      "step": 16057
    },
    {
      "epoch": 6.2168021680216805,
      "grad_norm": 20.727081298828125,
      "learning_rate": 4.203553146642578e-06,
      "loss": 1.4819,
      "step": 16058
    },
    {
      "epoch": 6.21718931475029,
      "grad_norm": 63.095550537109375,
      "learning_rate": 4.203122983610789e-06,
      "loss": 1.0169,
      "step": 16059
    },
    {
      "epoch": 6.217576461478901,
      "grad_norm": 13.059576988220215,
      "learning_rate": 4.202692820579e-06,
      "loss": 0.4897,
      "step": 16060
    },
    {
      "epoch": 6.21796360820751,
      "grad_norm": 44.1562385559082,
      "learning_rate": 4.202262657547211e-06,
      "loss": 0.2759,
      "step": 16061
    },
    {
      "epoch": 6.218350754936121,
      "grad_norm": 123.79447174072266,
      "learning_rate": 4.201832494515422e-06,
      "loss": 1.1384,
      "step": 16062
    },
    {
      "epoch": 6.218737901664731,
      "grad_norm": 110.58940124511719,
      "learning_rate": 4.201402331483633e-06,
      "loss": 1.0185,
      "step": 16063
    },
    {
      "epoch": 6.219125048393341,
      "grad_norm": 15.55201530456543,
      "learning_rate": 4.200972168451844e-06,
      "loss": 0.1124,
      "step": 16064
    },
    {
      "epoch": 6.219512195121951,
      "grad_norm": 6.719018936157227,
      "learning_rate": 4.200542005420054e-06,
      "loss": 0.2959,
      "step": 16065
    },
    {
      "epoch": 6.219899341850561,
      "grad_norm": 84.47547149658203,
      "learning_rate": 4.200111842388266e-06,
      "loss": 0.8709,
      "step": 16066
    },
    {
      "epoch": 6.2202864885791715,
      "grad_norm": 5.52630090713501,
      "learning_rate": 4.199681679356476e-06,
      "loss": 0.2583,
      "step": 16067
    },
    {
      "epoch": 6.220673635307782,
      "grad_norm": 7.216533184051514,
      "learning_rate": 4.199251516324687e-06,
      "loss": 0.2057,
      "step": 16068
    },
    {
      "epoch": 6.221060782036392,
      "grad_norm": 28.283681869506836,
      "learning_rate": 4.198821353292898e-06,
      "loss": 1.7349,
      "step": 16069
    },
    {
      "epoch": 6.221447928765002,
      "grad_norm": 5.1888813972473145,
      "learning_rate": 4.198391190261109e-06,
      "loss": 0.1299,
      "step": 16070
    },
    {
      "epoch": 6.221835075493612,
      "grad_norm": 27.36644172668457,
      "learning_rate": 4.19796102722932e-06,
      "loss": 1.9928,
      "step": 16071
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 56.481624603271484,
      "learning_rate": 4.197530864197531e-06,
      "loss": 1.3164,
      "step": 16072
    },
    {
      "epoch": 6.222609368950832,
      "grad_norm": 26.101648330688477,
      "learning_rate": 4.197100701165742e-06,
      "loss": 0.2173,
      "step": 16073
    },
    {
      "epoch": 6.2229965156794425,
      "grad_norm": 292.1960754394531,
      "learning_rate": 4.196670538133953e-06,
      "loss": 0.9034,
      "step": 16074
    },
    {
      "epoch": 6.223383662408053,
      "grad_norm": 93.4015884399414,
      "learning_rate": 4.196240375102164e-06,
      "loss": 0.6515,
      "step": 16075
    },
    {
      "epoch": 6.223770809136663,
      "grad_norm": 147.3240203857422,
      "learning_rate": 4.195810212070375e-06,
      "loss": 2.7666,
      "step": 16076
    },
    {
      "epoch": 6.224157955865273,
      "grad_norm": 43.717124938964844,
      "learning_rate": 4.195380049038586e-06,
      "loss": 1.6953,
      "step": 16077
    },
    {
      "epoch": 6.224545102593883,
      "grad_norm": 28.554725646972656,
      "learning_rate": 4.194949886006797e-06,
      "loss": 1.4085,
      "step": 16078
    },
    {
      "epoch": 6.224932249322493,
      "grad_norm": 39.988929748535156,
      "learning_rate": 4.194519722975008e-06,
      "loss": 3.9976,
      "step": 16079
    },
    {
      "epoch": 6.225319396051104,
      "grad_norm": 206.1786651611328,
      "learning_rate": 4.194089559943219e-06,
      "loss": 1.2943,
      "step": 16080
    },
    {
      "epoch": 6.225706542779713,
      "grad_norm": 73.2939453125,
      "learning_rate": 4.19365939691143e-06,
      "loss": 2.098,
      "step": 16081
    },
    {
      "epoch": 6.226093689508324,
      "grad_norm": 20.104528427124023,
      "learning_rate": 4.193229233879641e-06,
      "loss": 1.549,
      "step": 16082
    },
    {
      "epoch": 6.2264808362369335,
      "grad_norm": 112.91156005859375,
      "learning_rate": 4.192799070847851e-06,
      "loss": 3.0926,
      "step": 16083
    },
    {
      "epoch": 6.226867982965544,
      "grad_norm": 22.058765411376953,
      "learning_rate": 4.192368907816063e-06,
      "loss": 2.0678,
      "step": 16084
    },
    {
      "epoch": 6.2272551296941545,
      "grad_norm": 31.829587936401367,
      "learning_rate": 4.191938744784273e-06,
      "loss": 1.4498,
      "step": 16085
    },
    {
      "epoch": 6.227642276422764,
      "grad_norm": 44.575191497802734,
      "learning_rate": 4.191508581752485e-06,
      "loss": 0.439,
      "step": 16086
    },
    {
      "epoch": 6.228029423151375,
      "grad_norm": 58.0238037109375,
      "learning_rate": 4.191078418720695e-06,
      "loss": 2.1865,
      "step": 16087
    },
    {
      "epoch": 6.228416569879984,
      "grad_norm": 91.2144546508789,
      "learning_rate": 4.190648255688907e-06,
      "loss": 2.4094,
      "step": 16088
    },
    {
      "epoch": 6.228803716608595,
      "grad_norm": 52.81351089477539,
      "learning_rate": 4.190218092657117e-06,
      "loss": 1.059,
      "step": 16089
    },
    {
      "epoch": 6.229190863337204,
      "grad_norm": 24.987749099731445,
      "learning_rate": 4.189787929625329e-06,
      "loss": 1.1865,
      "step": 16090
    },
    {
      "epoch": 6.229578010065815,
      "grad_norm": 2.9220170974731445,
      "learning_rate": 4.189357766593539e-06,
      "loss": 0.1381,
      "step": 16091
    },
    {
      "epoch": 6.229965156794425,
      "grad_norm": 58.109317779541016,
      "learning_rate": 4.188927603561751e-06,
      "loss": 1.7194,
      "step": 16092
    },
    {
      "epoch": 6.230352303523035,
      "grad_norm": 25.777318954467773,
      "learning_rate": 4.188497440529961e-06,
      "loss": 2.0737,
      "step": 16093
    },
    {
      "epoch": 6.2307394502516456,
      "grad_norm": 60.22049331665039,
      "learning_rate": 4.188067277498173e-06,
      "loss": 1.0601,
      "step": 16094
    },
    {
      "epoch": 6.231126596980255,
      "grad_norm": 79.38854217529297,
      "learning_rate": 4.187637114466383e-06,
      "loss": 1.6353,
      "step": 16095
    },
    {
      "epoch": 6.231513743708866,
      "grad_norm": 61.73865509033203,
      "learning_rate": 4.187206951434595e-06,
      "loss": 0.2959,
      "step": 16096
    },
    {
      "epoch": 6.231900890437476,
      "grad_norm": 46.531471252441406,
      "learning_rate": 4.186776788402805e-06,
      "loss": 1.3112,
      "step": 16097
    },
    {
      "epoch": 6.232288037166086,
      "grad_norm": 8.189105987548828,
      "learning_rate": 4.186346625371016e-06,
      "loss": 0.253,
      "step": 16098
    },
    {
      "epoch": 6.232675183894696,
      "grad_norm": 3.195484161376953,
      "learning_rate": 4.185916462339227e-06,
      "loss": 0.0779,
      "step": 16099
    },
    {
      "epoch": 6.233062330623306,
      "grad_norm": 26.32720375061035,
      "learning_rate": 4.185486299307438e-06,
      "loss": 0.4528,
      "step": 16100
    },
    {
      "epoch": 6.2334494773519165,
      "grad_norm": 60.56409454345703,
      "learning_rate": 4.185056136275648e-06,
      "loss": 2.9562,
      "step": 16101
    },
    {
      "epoch": 6.233836624080526,
      "grad_norm": 33.61823654174805,
      "learning_rate": 4.18462597324386e-06,
      "loss": 1.9813,
      "step": 16102
    },
    {
      "epoch": 6.234223770809137,
      "grad_norm": 164.3322296142578,
      "learning_rate": 4.18419581021207e-06,
      "loss": 0.9549,
      "step": 16103
    },
    {
      "epoch": 6.234610917537747,
      "grad_norm": 24.044689178466797,
      "learning_rate": 4.183765647180282e-06,
      "loss": 0.297,
      "step": 16104
    },
    {
      "epoch": 6.234998064266357,
      "grad_norm": 85.88890075683594,
      "learning_rate": 4.183335484148492e-06,
      "loss": 0.9357,
      "step": 16105
    },
    {
      "epoch": 6.235385210994967,
      "grad_norm": 27.83869743347168,
      "learning_rate": 4.182905321116704e-06,
      "loss": 1.9962,
      "step": 16106
    },
    {
      "epoch": 6.235772357723577,
      "grad_norm": 2.3338329792022705,
      "learning_rate": 4.182475158084914e-06,
      "loss": 0.0773,
      "step": 16107
    },
    {
      "epoch": 6.236159504452187,
      "grad_norm": 31.053361892700195,
      "learning_rate": 4.182044995053126e-06,
      "loss": 1.9711,
      "step": 16108
    },
    {
      "epoch": 6.236546651180798,
      "grad_norm": 66.87215423583984,
      "learning_rate": 4.181614832021336e-06,
      "loss": 0.4262,
      "step": 16109
    },
    {
      "epoch": 6.2369337979094075,
      "grad_norm": 36.690582275390625,
      "learning_rate": 4.181184668989548e-06,
      "loss": 1.6184,
      "step": 16110
    },
    {
      "epoch": 6.237320944638018,
      "grad_norm": 40.73662567138672,
      "learning_rate": 4.180754505957758e-06,
      "loss": 2.2081,
      "step": 16111
    },
    {
      "epoch": 6.237708091366628,
      "grad_norm": 5.254405975341797,
      "learning_rate": 4.18032434292597e-06,
      "loss": 0.2367,
      "step": 16112
    },
    {
      "epoch": 6.238095238095238,
      "grad_norm": 121.19449615478516,
      "learning_rate": 4.17989417989418e-06,
      "loss": 2.1182,
      "step": 16113
    },
    {
      "epoch": 6.238482384823849,
      "grad_norm": 78.84912872314453,
      "learning_rate": 4.179464016862392e-06,
      "loss": 2.3948,
      "step": 16114
    },
    {
      "epoch": 6.238869531552458,
      "grad_norm": 22.075048446655273,
      "learning_rate": 4.179033853830602e-06,
      "loss": 2.1897,
      "step": 16115
    },
    {
      "epoch": 6.239256678281069,
      "grad_norm": 22.658205032348633,
      "learning_rate": 4.178603690798813e-06,
      "loss": 0.902,
      "step": 16116
    },
    {
      "epoch": 6.239643825009678,
      "grad_norm": 66.15412139892578,
      "learning_rate": 4.178173527767024e-06,
      "loss": 0.8056,
      "step": 16117
    },
    {
      "epoch": 6.240030971738289,
      "grad_norm": 67.20203399658203,
      "learning_rate": 4.177743364735235e-06,
      "loss": 2.2732,
      "step": 16118
    },
    {
      "epoch": 6.2404181184668985,
      "grad_norm": 86.45066833496094,
      "learning_rate": 4.177313201703445e-06,
      "loss": 3.2509,
      "step": 16119
    },
    {
      "epoch": 6.240805265195509,
      "grad_norm": 6.149118423461914,
      "learning_rate": 4.176883038671657e-06,
      "loss": 0.17,
      "step": 16120
    },
    {
      "epoch": 6.2411924119241196,
      "grad_norm": 2.7110073566436768,
      "learning_rate": 4.176452875639868e-06,
      "loss": 0.0955,
      "step": 16121
    },
    {
      "epoch": 6.241579558652729,
      "grad_norm": 203.29388427734375,
      "learning_rate": 4.176022712608079e-06,
      "loss": 1.8558,
      "step": 16122
    },
    {
      "epoch": 6.24196670538134,
      "grad_norm": 31.740629196166992,
      "learning_rate": 4.17559254957629e-06,
      "loss": 1.1779,
      "step": 16123
    },
    {
      "epoch": 6.242353852109949,
      "grad_norm": 55.09574890136719,
      "learning_rate": 4.175162386544501e-06,
      "loss": 1.4242,
      "step": 16124
    },
    {
      "epoch": 6.24274099883856,
      "grad_norm": 41.35789489746094,
      "learning_rate": 4.174732223512712e-06,
      "loss": 2.8902,
      "step": 16125
    },
    {
      "epoch": 6.24312814556717,
      "grad_norm": 62.068851470947266,
      "learning_rate": 4.174302060480923e-06,
      "loss": 1.4304,
      "step": 16126
    },
    {
      "epoch": 6.24351529229578,
      "grad_norm": 22.490644454956055,
      "learning_rate": 4.173871897449134e-06,
      "loss": 0.2,
      "step": 16127
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 16.711139678955078,
      "learning_rate": 4.173441734417345e-06,
      "loss": 0.2455,
      "step": 16128
    },
    {
      "epoch": 6.244289585753,
      "grad_norm": 63.55257034301758,
      "learning_rate": 4.173011571385555e-06,
      "loss": 0.4679,
      "step": 16129
    },
    {
      "epoch": 6.244676732481611,
      "grad_norm": 3.7191708087921143,
      "learning_rate": 4.172581408353767e-06,
      "loss": 0.1358,
      "step": 16130
    },
    {
      "epoch": 6.245063879210221,
      "grad_norm": 56.06536865234375,
      "learning_rate": 4.172151245321977e-06,
      "loss": 3.5713,
      "step": 16131
    },
    {
      "epoch": 6.245451025938831,
      "grad_norm": 10.88016128540039,
      "learning_rate": 4.171721082290189e-06,
      "loss": 0.1882,
      "step": 16132
    },
    {
      "epoch": 6.245838172667441,
      "grad_norm": 108.44871520996094,
      "learning_rate": 4.171290919258399e-06,
      "loss": 0.7072,
      "step": 16133
    },
    {
      "epoch": 6.246225319396051,
      "grad_norm": 9.324304580688477,
      "learning_rate": 4.17086075622661e-06,
      "loss": 0.1577,
      "step": 16134
    },
    {
      "epoch": 6.246612466124661,
      "grad_norm": 42.799034118652344,
      "learning_rate": 4.170430593194821e-06,
      "loss": 3.1587,
      "step": 16135
    },
    {
      "epoch": 6.246999612853271,
      "grad_norm": 139.14434814453125,
      "learning_rate": 4.170000430163032e-06,
      "loss": 1.6299,
      "step": 16136
    },
    {
      "epoch": 6.2473867595818815,
      "grad_norm": 6.163055896759033,
      "learning_rate": 4.169570267131243e-06,
      "loss": 0.2487,
      "step": 16137
    },
    {
      "epoch": 6.247773906310492,
      "grad_norm": 22.634233474731445,
      "learning_rate": 4.169140104099454e-06,
      "loss": 1.4673,
      "step": 16138
    },
    {
      "epoch": 6.248161053039102,
      "grad_norm": 82.15751647949219,
      "learning_rate": 4.168709941067665e-06,
      "loss": 1.0025,
      "step": 16139
    },
    {
      "epoch": 6.248548199767712,
      "grad_norm": 4.575098037719727,
      "learning_rate": 4.168279778035876e-06,
      "loss": 0.2228,
      "step": 16140
    },
    {
      "epoch": 6.248935346496322,
      "grad_norm": 45.63630676269531,
      "learning_rate": 4.167849615004087e-06,
      "loss": 1.7526,
      "step": 16141
    },
    {
      "epoch": 6.249322493224932,
      "grad_norm": 68.6935806274414,
      "learning_rate": 4.167419451972298e-06,
      "loss": 1.5846,
      "step": 16142
    },
    {
      "epoch": 6.249709639953543,
      "grad_norm": 60.52950668334961,
      "learning_rate": 4.166989288940509e-06,
      "loss": 1.5507,
      "step": 16143
    },
    {
      "epoch": 6.250096786682152,
      "grad_norm": 19.42516326904297,
      "learning_rate": 4.16655912590872e-06,
      "loss": 1.3,
      "step": 16144
    },
    {
      "epoch": 6.250483933410763,
      "grad_norm": 65.44618225097656,
      "learning_rate": 4.166128962876931e-06,
      "loss": 0.7758,
      "step": 16145
    },
    {
      "epoch": 6.2508710801393725,
      "grad_norm": 153.94058227539062,
      "learning_rate": 4.165698799845142e-06,
      "loss": 1.4621,
      "step": 16146
    },
    {
      "epoch": 6.251258226867983,
      "grad_norm": 10.103118896484375,
      "learning_rate": 4.165268636813352e-06,
      "loss": 0.2037,
      "step": 16147
    },
    {
      "epoch": 6.251645373596594,
      "grad_norm": 225.0158233642578,
      "learning_rate": 4.164838473781564e-06,
      "loss": 1.3105,
      "step": 16148
    },
    {
      "epoch": 6.252032520325203,
      "grad_norm": 4.758232116699219,
      "learning_rate": 4.164408310749774e-06,
      "loss": 0.2946,
      "step": 16149
    },
    {
      "epoch": 6.252419667053814,
      "grad_norm": 28.609912872314453,
      "learning_rate": 4.163978147717986e-06,
      "loss": 0.1506,
      "step": 16150
    },
    {
      "epoch": 6.252806813782423,
      "grad_norm": 135.0670623779297,
      "learning_rate": 4.163547984686196e-06,
      "loss": 3.7014,
      "step": 16151
    },
    {
      "epoch": 6.253193960511034,
      "grad_norm": 2.5200819969177246,
      "learning_rate": 4.163117821654407e-06,
      "loss": 0.1175,
      "step": 16152
    },
    {
      "epoch": 6.253581107239643,
      "grad_norm": 77.34593200683594,
      "learning_rate": 4.162687658622618e-06,
      "loss": 2.1726,
      "step": 16153
    },
    {
      "epoch": 6.253968253968254,
      "grad_norm": 4.658425807952881,
      "learning_rate": 4.162257495590829e-06,
      "loss": 0.2498,
      "step": 16154
    },
    {
      "epoch": 6.2543554006968645,
      "grad_norm": 72.76007080078125,
      "learning_rate": 4.16182733255904e-06,
      "loss": 1.635,
      "step": 16155
    },
    {
      "epoch": 6.254742547425474,
      "grad_norm": 29.33452796936035,
      "learning_rate": 4.161397169527251e-06,
      "loss": 4.8789,
      "step": 16156
    },
    {
      "epoch": 6.255129694154085,
      "grad_norm": 40.40013885498047,
      "learning_rate": 4.160967006495462e-06,
      "loss": 0.8757,
      "step": 16157
    },
    {
      "epoch": 6.255516840882694,
      "grad_norm": 3.7944881916046143,
      "learning_rate": 4.160536843463673e-06,
      "loss": 0.1888,
      "step": 16158
    },
    {
      "epoch": 6.255903987611305,
      "grad_norm": 16.069204330444336,
      "learning_rate": 4.160106680431884e-06,
      "loss": 0.2466,
      "step": 16159
    },
    {
      "epoch": 6.256291134339915,
      "grad_norm": 53.11798858642578,
      "learning_rate": 4.159676517400095e-06,
      "loss": 0.406,
      "step": 16160
    },
    {
      "epoch": 6.256678281068525,
      "grad_norm": 53.325523376464844,
      "learning_rate": 4.159246354368306e-06,
      "loss": 2.5349,
      "step": 16161
    },
    {
      "epoch": 6.257065427797135,
      "grad_norm": 35.79798126220703,
      "learning_rate": 4.158816191336517e-06,
      "loss": 2.0935,
      "step": 16162
    },
    {
      "epoch": 6.257452574525745,
      "grad_norm": 107.50293731689453,
      "learning_rate": 4.158386028304728e-06,
      "loss": 0.8125,
      "step": 16163
    },
    {
      "epoch": 6.2578397212543555,
      "grad_norm": 94.58570098876953,
      "learning_rate": 4.157955865272939e-06,
      "loss": 2.6073,
      "step": 16164
    },
    {
      "epoch": 6.258226867982966,
      "grad_norm": 5.165667533874512,
      "learning_rate": 4.157525702241149e-06,
      "loss": 0.1846,
      "step": 16165
    },
    {
      "epoch": 6.258614014711576,
      "grad_norm": 175.9445037841797,
      "learning_rate": 4.157095539209361e-06,
      "loss": 0.5759,
      "step": 16166
    },
    {
      "epoch": 6.259001161440186,
      "grad_norm": 84.49462890625,
      "learning_rate": 4.156665376177571e-06,
      "loss": 1.5577,
      "step": 16167
    },
    {
      "epoch": 6.259388308168796,
      "grad_norm": 65.48421478271484,
      "learning_rate": 4.156235213145783e-06,
      "loss": 1.1601,
      "step": 16168
    },
    {
      "epoch": 6.259775454897406,
      "grad_norm": 57.0166015625,
      "learning_rate": 4.155805050113993e-06,
      "loss": 0.6986,
      "step": 16169
    },
    {
      "epoch": 6.260162601626016,
      "grad_norm": 3.4649691581726074,
      "learning_rate": 4.155374887082205e-06,
      "loss": 0.1938,
      "step": 16170
    },
    {
      "epoch": 6.260549748354626,
      "grad_norm": 1.5592936277389526,
      "learning_rate": 4.154944724050415e-06,
      "loss": 0.0629,
      "step": 16171
    },
    {
      "epoch": 6.260936895083237,
      "grad_norm": 6.394379615783691,
      "learning_rate": 4.154514561018627e-06,
      "loss": 0.1336,
      "step": 16172
    },
    {
      "epoch": 6.2613240418118465,
      "grad_norm": 46.207122802734375,
      "learning_rate": 4.154084397986837e-06,
      "loss": 0.6054,
      "step": 16173
    },
    {
      "epoch": 6.261711188540457,
      "grad_norm": 44.202571868896484,
      "learning_rate": 4.153654234955049e-06,
      "loss": 0.888,
      "step": 16174
    },
    {
      "epoch": 6.262098335269067,
      "grad_norm": 83.11705017089844,
      "learning_rate": 4.153224071923259e-06,
      "loss": 1.6094,
      "step": 16175
    },
    {
      "epoch": 6.262485481997677,
      "grad_norm": 48.86593246459961,
      "learning_rate": 4.152793908891471e-06,
      "loss": 0.5518,
      "step": 16176
    },
    {
      "epoch": 6.262872628726287,
      "grad_norm": 5.314663887023926,
      "learning_rate": 4.152363745859681e-06,
      "loss": 0.2871,
      "step": 16177
    },
    {
      "epoch": 6.263259775454897,
      "grad_norm": 172.0220184326172,
      "learning_rate": 4.151933582827893e-06,
      "loss": 0.5172,
      "step": 16178
    },
    {
      "epoch": 6.263646922183508,
      "grad_norm": 55.785675048828125,
      "learning_rate": 4.151503419796103e-06,
      "loss": 1.363,
      "step": 16179
    },
    {
      "epoch": 6.2640340689121174,
      "grad_norm": 3.3398759365081787,
      "learning_rate": 4.151073256764314e-06,
      "loss": 0.1618,
      "step": 16180
    },
    {
      "epoch": 6.264421215640728,
      "grad_norm": 20.61097526550293,
      "learning_rate": 4.150643093732525e-06,
      "loss": 0.2955,
      "step": 16181
    },
    {
      "epoch": 6.264808362369338,
      "grad_norm": 111.23403930664062,
      "learning_rate": 4.150212930700736e-06,
      "loss": 1.7057,
      "step": 16182
    },
    {
      "epoch": 6.265195509097948,
      "grad_norm": 52.324668884277344,
      "learning_rate": 4.149782767668946e-06,
      "loss": 0.5787,
      "step": 16183
    },
    {
      "epoch": 6.265582655826559,
      "grad_norm": 26.20088005065918,
      "learning_rate": 4.149352604637158e-06,
      "loss": 1.0845,
      "step": 16184
    },
    {
      "epoch": 6.265969802555168,
      "grad_norm": 32.21481704711914,
      "learning_rate": 4.148922441605368e-06,
      "loss": 0.1318,
      "step": 16185
    },
    {
      "epoch": 6.266356949283779,
      "grad_norm": 12.008190155029297,
      "learning_rate": 4.14849227857358e-06,
      "loss": 0.2246,
      "step": 16186
    },
    {
      "epoch": 6.266744096012388,
      "grad_norm": 6.604092597961426,
      "learning_rate": 4.14806211554179e-06,
      "loss": 0.2903,
      "step": 16187
    },
    {
      "epoch": 6.267131242740999,
      "grad_norm": 51.5927734375,
      "learning_rate": 4.147631952510002e-06,
      "loss": 1.3366,
      "step": 16188
    },
    {
      "epoch": 6.267518389469609,
      "grad_norm": 97.37091064453125,
      "learning_rate": 4.147201789478212e-06,
      "loss": 1.469,
      "step": 16189
    },
    {
      "epoch": 6.267905536198219,
      "grad_norm": 5.119112014770508,
      "learning_rate": 4.146771626446424e-06,
      "loss": 0.1393,
      "step": 16190
    },
    {
      "epoch": 6.2682926829268295,
      "grad_norm": 238.10084533691406,
      "learning_rate": 4.146341463414634e-06,
      "loss": 1.7099,
      "step": 16191
    },
    {
      "epoch": 6.268679829655439,
      "grad_norm": 10.192630767822266,
      "learning_rate": 4.145911300382846e-06,
      "loss": 0.2843,
      "step": 16192
    },
    {
      "epoch": 6.26906697638405,
      "grad_norm": 104.69111633300781,
      "learning_rate": 4.145481137351056e-06,
      "loss": 1.4462,
      "step": 16193
    },
    {
      "epoch": 6.269454123112659,
      "grad_norm": 6.225422382354736,
      "learning_rate": 4.145050974319268e-06,
      "loss": 0.3458,
      "step": 16194
    },
    {
      "epoch": 6.26984126984127,
      "grad_norm": 114.74771118164062,
      "learning_rate": 4.144620811287478e-06,
      "loss": 1.691,
      "step": 16195
    },
    {
      "epoch": 6.27022841656988,
      "grad_norm": 60.43049240112305,
      "learning_rate": 4.14419064825569e-06,
      "loss": 0.5448,
      "step": 16196
    },
    {
      "epoch": 6.27061556329849,
      "grad_norm": 73.84182739257812,
      "learning_rate": 4.1437604852239e-06,
      "loss": 2.0671,
      "step": 16197
    },
    {
      "epoch": 6.2710027100271,
      "grad_norm": 3.8840668201446533,
      "learning_rate": 4.143330322192111e-06,
      "loss": 0.1192,
      "step": 16198
    },
    {
      "epoch": 6.27138985675571,
      "grad_norm": 96.47187042236328,
      "learning_rate": 4.142900159160322e-06,
      "loss": 4.2414,
      "step": 16199
    },
    {
      "epoch": 6.2717770034843205,
      "grad_norm": 36.615447998046875,
      "learning_rate": 4.142469996128533e-06,
      "loss": 1.5574,
      "step": 16200
    },
    {
      "epoch": 6.272164150212931,
      "grad_norm": 0.7474973797798157,
      "learning_rate": 4.142039833096743e-06,
      "loss": 0.0194,
      "step": 16201
    },
    {
      "epoch": 6.272551296941541,
      "grad_norm": 113.91033935546875,
      "learning_rate": 4.141609670064955e-06,
      "loss": 0.9057,
      "step": 16202
    },
    {
      "epoch": 6.272938443670151,
      "grad_norm": 94.76258087158203,
      "learning_rate": 4.141179507033166e-06,
      "loss": 0.735,
      "step": 16203
    },
    {
      "epoch": 6.273325590398761,
      "grad_norm": 71.79886627197266,
      "learning_rate": 4.140749344001377e-06,
      "loss": 1.916,
      "step": 16204
    },
    {
      "epoch": 6.273712737127371,
      "grad_norm": 11.814501762390137,
      "learning_rate": 4.140319180969588e-06,
      "loss": 0.1861,
      "step": 16205
    },
    {
      "epoch": 6.274099883855982,
      "grad_norm": 34.97327423095703,
      "learning_rate": 4.139889017937799e-06,
      "loss": 2.1906,
      "step": 16206
    },
    {
      "epoch": 6.2744870305845915,
      "grad_norm": 32.27909469604492,
      "learning_rate": 4.13945885490601e-06,
      "loss": 1.5866,
      "step": 16207
    },
    {
      "epoch": 6.274874177313202,
      "grad_norm": 29.86334991455078,
      "learning_rate": 4.139028691874221e-06,
      "loss": 1.9109,
      "step": 16208
    },
    {
      "epoch": 6.275261324041812,
      "grad_norm": 5.246284484863281,
      "learning_rate": 4.138598528842432e-06,
      "loss": 0.204,
      "step": 16209
    },
    {
      "epoch": 6.275648470770422,
      "grad_norm": 3.324592113494873,
      "learning_rate": 4.138168365810643e-06,
      "loss": 0.1637,
      "step": 16210
    },
    {
      "epoch": 6.276035617499032,
      "grad_norm": 0.9756768345832825,
      "learning_rate": 4.137738202778854e-06,
      "loss": 0.0183,
      "step": 16211
    },
    {
      "epoch": 6.276422764227642,
      "grad_norm": 14.6127347946167,
      "learning_rate": 4.137308039747065e-06,
      "loss": 0.1814,
      "step": 16212
    },
    {
      "epoch": 6.276809910956253,
      "grad_norm": 95.9164047241211,
      "learning_rate": 4.136877876715275e-06,
      "loss": 0.7071,
      "step": 16213
    },
    {
      "epoch": 6.277197057684862,
      "grad_norm": 69.7873764038086,
      "learning_rate": 4.136447713683487e-06,
      "loss": 1.0931,
      "step": 16214
    },
    {
      "epoch": 6.277584204413473,
      "grad_norm": 47.972774505615234,
      "learning_rate": 4.136017550651697e-06,
      "loss": 0.2685,
      "step": 16215
    },
    {
      "epoch": 6.2779713511420825,
      "grad_norm": 6.047062397003174,
      "learning_rate": 4.135587387619908e-06,
      "loss": 0.1765,
      "step": 16216
    },
    {
      "epoch": 6.278358497870693,
      "grad_norm": 17.93195152282715,
      "learning_rate": 4.135157224588119e-06,
      "loss": 0.191,
      "step": 16217
    },
    {
      "epoch": 6.2787456445993035,
      "grad_norm": 131.68316650390625,
      "learning_rate": 4.13472706155633e-06,
      "loss": 2.6802,
      "step": 16218
    },
    {
      "epoch": 6.279132791327913,
      "grad_norm": 1.7782007455825806,
      "learning_rate": 4.134296898524541e-06,
      "loss": 0.0729,
      "step": 16219
    },
    {
      "epoch": 6.279519938056524,
      "grad_norm": 15.482817649841309,
      "learning_rate": 4.133866735492752e-06,
      "loss": 0.2721,
      "step": 16220
    },
    {
      "epoch": 6.279907084785133,
      "grad_norm": 41.91044998168945,
      "learning_rate": 4.133436572460963e-06,
      "loss": 1.1545,
      "step": 16221
    },
    {
      "epoch": 6.280294231513744,
      "grad_norm": 1.9145431518554688,
      "learning_rate": 4.133006409429174e-06,
      "loss": 0.0755,
      "step": 16222
    },
    {
      "epoch": 6.280681378242354,
      "grad_norm": 47.73097610473633,
      "learning_rate": 4.132576246397385e-06,
      "loss": 1.5203,
      "step": 16223
    },
    {
      "epoch": 6.281068524970964,
      "grad_norm": 211.94500732421875,
      "learning_rate": 4.132146083365596e-06,
      "loss": 1.5917,
      "step": 16224
    },
    {
      "epoch": 6.281455671699574,
      "grad_norm": 55.42660140991211,
      "learning_rate": 4.131715920333807e-06,
      "loss": 1.6634,
      "step": 16225
    },
    {
      "epoch": 6.281842818428184,
      "grad_norm": 15.742082595825195,
      "learning_rate": 4.131285757302018e-06,
      "loss": 1.7124,
      "step": 16226
    },
    {
      "epoch": 6.2822299651567945,
      "grad_norm": 18.266124725341797,
      "learning_rate": 4.130855594270229e-06,
      "loss": 0.2864,
      "step": 16227
    },
    {
      "epoch": 6.282617111885404,
      "grad_norm": 21.000864028930664,
      "learning_rate": 4.13042543123844e-06,
      "loss": 0.1444,
      "step": 16228
    },
    {
      "epoch": 6.283004258614015,
      "grad_norm": 18.692296981811523,
      "learning_rate": 4.129995268206651e-06,
      "loss": 0.2703,
      "step": 16229
    },
    {
      "epoch": 6.283391405342625,
      "grad_norm": 23.08289909362793,
      "learning_rate": 4.129565105174862e-06,
      "loss": 0.2365,
      "step": 16230
    },
    {
      "epoch": 6.283778552071235,
      "grad_norm": 32.414825439453125,
      "learning_rate": 4.129134942143072e-06,
      "loss": 0.261,
      "step": 16231
    },
    {
      "epoch": 6.284165698799845,
      "grad_norm": 2.0945475101470947,
      "learning_rate": 4.128704779111284e-06,
      "loss": 0.0831,
      "step": 16232
    },
    {
      "epoch": 6.284552845528455,
      "grad_norm": 23.661041259765625,
      "learning_rate": 4.128274616079494e-06,
      "loss": 0.3565,
      "step": 16233
    },
    {
      "epoch": 6.2849399922570655,
      "grad_norm": 128.87127685546875,
      "learning_rate": 4.127844453047705e-06,
      "loss": 3.3743,
      "step": 16234
    },
    {
      "epoch": 6.285327138985676,
      "grad_norm": 55.375770568847656,
      "learning_rate": 4.127414290015916e-06,
      "loss": 1.2367,
      "step": 16235
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 23.387550354003906,
      "learning_rate": 4.126984126984127e-06,
      "loss": 1.6855,
      "step": 16236
    },
    {
      "epoch": 6.286101432442896,
      "grad_norm": 20.21829605102539,
      "learning_rate": 4.126553963952338e-06,
      "loss": 0.0942,
      "step": 16237
    },
    {
      "epoch": 6.286488579171506,
      "grad_norm": 101.81945037841797,
      "learning_rate": 4.126123800920549e-06,
      "loss": 0.9322,
      "step": 16238
    },
    {
      "epoch": 6.286875725900116,
      "grad_norm": 106.40978240966797,
      "learning_rate": 4.12569363788876e-06,
      "loss": 1.6673,
      "step": 16239
    },
    {
      "epoch": 6.287262872628727,
      "grad_norm": 3.5160269737243652,
      "learning_rate": 4.125263474856971e-06,
      "loss": 0.0872,
      "step": 16240
    },
    {
      "epoch": 6.287650019357336,
      "grad_norm": 118.14704132080078,
      "learning_rate": 4.124833311825182e-06,
      "loss": 0.7668,
      "step": 16241
    },
    {
      "epoch": 6.288037166085947,
      "grad_norm": 21.723506927490234,
      "learning_rate": 4.124403148793393e-06,
      "loss": 1.9835,
      "step": 16242
    },
    {
      "epoch": 6.2884243128145565,
      "grad_norm": 72.81682586669922,
      "learning_rate": 4.123972985761604e-06,
      "loss": 0.8486,
      "step": 16243
    },
    {
      "epoch": 6.288811459543167,
      "grad_norm": 5.4769792556762695,
      "learning_rate": 4.1235428227298156e-06,
      "loss": 0.1717,
      "step": 16244
    },
    {
      "epoch": 6.289198606271777,
      "grad_norm": 7.899479389190674,
      "learning_rate": 4.123112659698026e-06,
      "loss": 0.2228,
      "step": 16245
    },
    {
      "epoch": 6.289585753000387,
      "grad_norm": 10.123050689697266,
      "learning_rate": 4.122682496666237e-06,
      "loss": 0.1067,
      "step": 16246
    },
    {
      "epoch": 6.289972899728998,
      "grad_norm": 45.6754035949707,
      "learning_rate": 4.122252333634448e-06,
      "loss": 0.3852,
      "step": 16247
    },
    {
      "epoch": 6.290360046457607,
      "grad_norm": 65.49882507324219,
      "learning_rate": 4.121822170602659e-06,
      "loss": 2.6261,
      "step": 16248
    },
    {
      "epoch": 6.290747193186218,
      "grad_norm": 97.74388885498047,
      "learning_rate": 4.121392007570869e-06,
      "loss": 2.4591,
      "step": 16249
    },
    {
      "epoch": 6.291134339914827,
      "grad_norm": 75.16244506835938,
      "learning_rate": 4.120961844539081e-06,
      "loss": 0.2759,
      "step": 16250
    },
    {
      "epoch": 6.291521486643438,
      "grad_norm": 82.87957763671875,
      "learning_rate": 4.120531681507291e-06,
      "loss": 0.7365,
      "step": 16251
    },
    {
      "epoch": 6.291908633372048,
      "grad_norm": 3.612217903137207,
      "learning_rate": 4.120101518475503e-06,
      "loss": 0.1525,
      "step": 16252
    },
    {
      "epoch": 6.292295780100658,
      "grad_norm": 2.9590165615081787,
      "learning_rate": 4.119671355443713e-06,
      "loss": 0.0984,
      "step": 16253
    },
    {
      "epoch": 6.2926829268292686,
      "grad_norm": 2.24684739112854,
      "learning_rate": 4.119241192411925e-06,
      "loss": 0.0615,
      "step": 16254
    },
    {
      "epoch": 6.293070073557878,
      "grad_norm": 36.00981521606445,
      "learning_rate": 4.118811029380135e-06,
      "loss": 0.199,
      "step": 16255
    },
    {
      "epoch": 6.293457220286489,
      "grad_norm": 17.110328674316406,
      "learning_rate": 4.118380866348347e-06,
      "loss": 0.3365,
      "step": 16256
    },
    {
      "epoch": 6.293844367015098,
      "grad_norm": 21.278669357299805,
      "learning_rate": 4.117950703316557e-06,
      "loss": 1.5575,
      "step": 16257
    },
    {
      "epoch": 6.294231513743709,
      "grad_norm": 128.16053771972656,
      "learning_rate": 4.117520540284769e-06,
      "loss": 0.4573,
      "step": 16258
    },
    {
      "epoch": 6.294618660472319,
      "grad_norm": 35.43513107299805,
      "learning_rate": 4.117090377252979e-06,
      "loss": 3.5394,
      "step": 16259
    },
    {
      "epoch": 6.295005807200929,
      "grad_norm": 81.0884017944336,
      "learning_rate": 4.116660214221191e-06,
      "loss": 0.6353,
      "step": 16260
    },
    {
      "epoch": 6.2953929539295395,
      "grad_norm": 16.308652877807617,
      "learning_rate": 4.116230051189401e-06,
      "loss": 0.4095,
      "step": 16261
    },
    {
      "epoch": 6.295780100658149,
      "grad_norm": 4.384659290313721,
      "learning_rate": 4.1157998881576126e-06,
      "loss": 0.2842,
      "step": 16262
    },
    {
      "epoch": 6.29616724738676,
      "grad_norm": 71.7393798828125,
      "learning_rate": 4.115369725125823e-06,
      "loss": 0.8916,
      "step": 16263
    },
    {
      "epoch": 6.29655439411537,
      "grad_norm": 9.88036823272705,
      "learning_rate": 4.114939562094034e-06,
      "loss": 0.1516,
      "step": 16264
    },
    {
      "epoch": 6.29694154084398,
      "grad_norm": 127.17623901367188,
      "learning_rate": 4.114509399062245e-06,
      "loss": 1.3616,
      "step": 16265
    },
    {
      "epoch": 6.29732868757259,
      "grad_norm": 40.381351470947266,
      "learning_rate": 4.114079236030456e-06,
      "loss": 0.3846,
      "step": 16266
    },
    {
      "epoch": 6.2977158343012,
      "grad_norm": 80.5867919921875,
      "learning_rate": 4.113649072998666e-06,
      "loss": 2.4057,
      "step": 16267
    },
    {
      "epoch": 6.29810298102981,
      "grad_norm": 43.29951858520508,
      "learning_rate": 4.113218909966878e-06,
      "loss": 1.72,
      "step": 16268
    },
    {
      "epoch": 6.29849012775842,
      "grad_norm": 35.232337951660156,
      "learning_rate": 4.112788746935088e-06,
      "loss": 0.4995,
      "step": 16269
    },
    {
      "epoch": 6.2988772744870305,
      "grad_norm": 67.380859375,
      "learning_rate": 4.1123585839033e-06,
      "loss": 1.8973,
      "step": 16270
    },
    {
      "epoch": 6.299264421215641,
      "grad_norm": 1.7933247089385986,
      "learning_rate": 4.11192842087151e-06,
      "loss": 0.0647,
      "step": 16271
    },
    {
      "epoch": 6.299651567944251,
      "grad_norm": 101.40858459472656,
      "learning_rate": 4.111498257839722e-06,
      "loss": 0.9152,
      "step": 16272
    },
    {
      "epoch": 6.300038714672861,
      "grad_norm": 21.789630889892578,
      "learning_rate": 4.111068094807932e-06,
      "loss": 1.5318,
      "step": 16273
    },
    {
      "epoch": 6.300425861401471,
      "grad_norm": 125.26535034179688,
      "learning_rate": 4.110637931776144e-06,
      "loss": 1.5515,
      "step": 16274
    },
    {
      "epoch": 6.300813008130081,
      "grad_norm": 65.63874816894531,
      "learning_rate": 4.110207768744354e-06,
      "loss": 2.6915,
      "step": 16275
    },
    {
      "epoch": 6.301200154858692,
      "grad_norm": 147.85498046875,
      "learning_rate": 4.109777605712566e-06,
      "loss": 0.4572,
      "step": 16276
    },
    {
      "epoch": 6.301587301587301,
      "grad_norm": 145.32073974609375,
      "learning_rate": 4.109347442680776e-06,
      "loss": 2.5311,
      "step": 16277
    },
    {
      "epoch": 6.301974448315912,
      "grad_norm": 38.87288284301758,
      "learning_rate": 4.108917279648988e-06,
      "loss": 1.0766,
      "step": 16278
    },
    {
      "epoch": 6.3023615950445215,
      "grad_norm": 43.23752212524414,
      "learning_rate": 4.108487116617198e-06,
      "loss": 0.5837,
      "step": 16279
    },
    {
      "epoch": 6.302748741773132,
      "grad_norm": 22.00958251953125,
      "learning_rate": 4.10805695358541e-06,
      "loss": 1.646,
      "step": 16280
    },
    {
      "epoch": 6.303135888501743,
      "grad_norm": 145.33358764648438,
      "learning_rate": 4.10762679055362e-06,
      "loss": 2.1081,
      "step": 16281
    },
    {
      "epoch": 6.303523035230352,
      "grad_norm": 81.23983001708984,
      "learning_rate": 4.107196627521831e-06,
      "loss": 1.0283,
      "step": 16282
    },
    {
      "epoch": 6.303910181958963,
      "grad_norm": 281.29931640625,
      "learning_rate": 4.106766464490042e-06,
      "loss": 1.2141,
      "step": 16283
    },
    {
      "epoch": 6.304297328687572,
      "grad_norm": 41.68852615356445,
      "learning_rate": 4.106336301458253e-06,
      "loss": 2.0351,
      "step": 16284
    },
    {
      "epoch": 6.304684475416183,
      "grad_norm": 7.725903034210205,
      "learning_rate": 4.105906138426464e-06,
      "loss": 0.2458,
      "step": 16285
    },
    {
      "epoch": 6.305071622144792,
      "grad_norm": 134.2368621826172,
      "learning_rate": 4.105475975394675e-06,
      "loss": 2.3326,
      "step": 16286
    },
    {
      "epoch": 6.305458768873403,
      "grad_norm": 32.47955322265625,
      "learning_rate": 4.105045812362886e-06,
      "loss": 0.3615,
      "step": 16287
    },
    {
      "epoch": 6.3058459156020135,
      "grad_norm": 22.96001434326172,
      "learning_rate": 4.104615649331097e-06,
      "loss": 1.1444,
      "step": 16288
    },
    {
      "epoch": 6.306233062330623,
      "grad_norm": 42.25033950805664,
      "learning_rate": 4.104185486299308e-06,
      "loss": 1.4967,
      "step": 16289
    },
    {
      "epoch": 6.306620209059234,
      "grad_norm": 20.716127395629883,
      "learning_rate": 4.103755323267519e-06,
      "loss": 2.0911,
      "step": 16290
    },
    {
      "epoch": 6.307007355787843,
      "grad_norm": 20.381868362426758,
      "learning_rate": 4.10332516023573e-06,
      "loss": 0.3492,
      "step": 16291
    },
    {
      "epoch": 6.307394502516454,
      "grad_norm": 56.1123161315918,
      "learning_rate": 4.102894997203941e-06,
      "loss": 1.835,
      "step": 16292
    },
    {
      "epoch": 6.307781649245064,
      "grad_norm": 18.699295043945312,
      "learning_rate": 4.102464834172152e-06,
      "loss": 0.5301,
      "step": 16293
    },
    {
      "epoch": 6.308168795973674,
      "grad_norm": 5.335602760314941,
      "learning_rate": 4.102034671140363e-06,
      "loss": 0.238,
      "step": 16294
    },
    {
      "epoch": 6.308555942702284,
      "grad_norm": 78.59512329101562,
      "learning_rate": 4.101604508108574e-06,
      "loss": 1.3384,
      "step": 16295
    },
    {
      "epoch": 6.308943089430894,
      "grad_norm": 21.136476516723633,
      "learning_rate": 4.101174345076785e-06,
      "loss": 1.1327,
      "step": 16296
    },
    {
      "epoch": 6.3093302361595045,
      "grad_norm": 81.38446807861328,
      "learning_rate": 4.100744182044995e-06,
      "loss": 1.8383,
      "step": 16297
    },
    {
      "epoch": 6.309717382888115,
      "grad_norm": 39.34322738647461,
      "learning_rate": 4.100314019013207e-06,
      "loss": 0.8694,
      "step": 16298
    },
    {
      "epoch": 6.310104529616725,
      "grad_norm": 3.1103670597076416,
      "learning_rate": 4.099883855981417e-06,
      "loss": 0.1457,
      "step": 16299
    },
    {
      "epoch": 6.310491676345335,
      "grad_norm": 67.74913024902344,
      "learning_rate": 4.099453692949628e-06,
      "loss": 2.7003,
      "step": 16300
    },
    {
      "epoch": 6.310878823073945,
      "grad_norm": 49.77237319946289,
      "learning_rate": 4.099023529917839e-06,
      "loss": 1.4635,
      "step": 16301
    },
    {
      "epoch": 6.311265969802555,
      "grad_norm": 19.21277618408203,
      "learning_rate": 4.09859336688605e-06,
      "loss": 1.3666,
      "step": 16302
    },
    {
      "epoch": 6.311653116531165,
      "grad_norm": 142.1333465576172,
      "learning_rate": 4.098163203854261e-06,
      "loss": 1.5186,
      "step": 16303
    },
    {
      "epoch": 6.312040263259775,
      "grad_norm": 56.74618148803711,
      "learning_rate": 4.097733040822472e-06,
      "loss": 0.5651,
      "step": 16304
    },
    {
      "epoch": 6.312427409988386,
      "grad_norm": 40.57890319824219,
      "learning_rate": 4.097302877790683e-06,
      "loss": 0.6942,
      "step": 16305
    },
    {
      "epoch": 6.3128145567169955,
      "grad_norm": 83.79254150390625,
      "learning_rate": 4.096872714758894e-06,
      "loss": 2.0733,
      "step": 16306
    },
    {
      "epoch": 6.313201703445606,
      "grad_norm": 60.16709518432617,
      "learning_rate": 4.096442551727105e-06,
      "loss": 3.6569,
      "step": 16307
    },
    {
      "epoch": 6.313588850174216,
      "grad_norm": 59.85600662231445,
      "learning_rate": 4.096012388695316e-06,
      "loss": 1.2843,
      "step": 16308
    },
    {
      "epoch": 6.313975996902826,
      "grad_norm": 55.3932991027832,
      "learning_rate": 4.095582225663527e-06,
      "loss": 1.6491,
      "step": 16309
    },
    {
      "epoch": 6.314363143631437,
      "grad_norm": 34.49918746948242,
      "learning_rate": 4.095152062631738e-06,
      "loss": 1.0327,
      "step": 16310
    },
    {
      "epoch": 6.314750290360046,
      "grad_norm": 4.659833908081055,
      "learning_rate": 4.094721899599949e-06,
      "loss": 0.1357,
      "step": 16311
    },
    {
      "epoch": 6.315137437088657,
      "grad_norm": 130.4228973388672,
      "learning_rate": 4.09429173656816e-06,
      "loss": 0.5389,
      "step": 16312
    },
    {
      "epoch": 6.3155245838172664,
      "grad_norm": 36.05064010620117,
      "learning_rate": 4.093861573536371e-06,
      "loss": 2.6685,
      "step": 16313
    },
    {
      "epoch": 6.315911730545877,
      "grad_norm": 17.871509552001953,
      "learning_rate": 4.093431410504582e-06,
      "loss": 2.3534,
      "step": 16314
    },
    {
      "epoch": 6.3162988772744875,
      "grad_norm": 2.698838949203491,
      "learning_rate": 4.093001247472792e-06,
      "loss": 0.1077,
      "step": 16315
    },
    {
      "epoch": 6.316686024003097,
      "grad_norm": 121.0246353149414,
      "learning_rate": 4.092571084441004e-06,
      "loss": 1.5162,
      "step": 16316
    },
    {
      "epoch": 6.317073170731708,
      "grad_norm": 49.205867767333984,
      "learning_rate": 4.092140921409214e-06,
      "loss": 2.4185,
      "step": 16317
    },
    {
      "epoch": 6.317460317460317,
      "grad_norm": 79.71334838867188,
      "learning_rate": 4.091710758377425e-06,
      "loss": 2.3757,
      "step": 16318
    },
    {
      "epoch": 6.317847464188928,
      "grad_norm": 70.5520248413086,
      "learning_rate": 4.091280595345636e-06,
      "loss": 1.4233,
      "step": 16319
    },
    {
      "epoch": 6.318234610917537,
      "grad_norm": 14.60397720336914,
      "learning_rate": 4.090850432313847e-06,
      "loss": 0.114,
      "step": 16320
    },
    {
      "epoch": 6.318621757646148,
      "grad_norm": 68.38790130615234,
      "learning_rate": 4.090420269282058e-06,
      "loss": 1.6968,
      "step": 16321
    },
    {
      "epoch": 6.319008904374758,
      "grad_norm": 14.98282527923584,
      "learning_rate": 4.089990106250269e-06,
      "loss": 0.4546,
      "step": 16322
    },
    {
      "epoch": 6.319396051103368,
      "grad_norm": 3.266470193862915,
      "learning_rate": 4.08955994321848e-06,
      "loss": 0.0453,
      "step": 16323
    },
    {
      "epoch": 6.3197831978319785,
      "grad_norm": 25.435104370117188,
      "learning_rate": 4.089129780186691e-06,
      "loss": 0.1431,
      "step": 16324
    },
    {
      "epoch": 6.320170344560588,
      "grad_norm": 133.50457763671875,
      "learning_rate": 4.088699617154902e-06,
      "loss": 2.3095,
      "step": 16325
    },
    {
      "epoch": 6.320557491289199,
      "grad_norm": 44.12232208251953,
      "learning_rate": 4.0882694541231135e-06,
      "loss": 1.2055,
      "step": 16326
    },
    {
      "epoch": 6.320944638017809,
      "grad_norm": 27.492733001708984,
      "learning_rate": 4.087839291091324e-06,
      "loss": 0.5214,
      "step": 16327
    },
    {
      "epoch": 6.321331784746419,
      "grad_norm": 117.00328063964844,
      "learning_rate": 4.0874091280595355e-06,
      "loss": 1.2421,
      "step": 16328
    },
    {
      "epoch": 6.321718931475029,
      "grad_norm": 4.702465534210205,
      "learning_rate": 4.086978965027746e-06,
      "loss": 0.1309,
      "step": 16329
    },
    {
      "epoch": 6.322106078203639,
      "grad_norm": 97.83903503417969,
      "learning_rate": 4.086548801995957e-06,
      "loss": 2.1594,
      "step": 16330
    },
    {
      "epoch": 6.322493224932249,
      "grad_norm": 59.66963577270508,
      "learning_rate": 4.086118638964168e-06,
      "loss": 1.5081,
      "step": 16331
    },
    {
      "epoch": 6.32288037166086,
      "grad_norm": 101.33729553222656,
      "learning_rate": 4.085688475932379e-06,
      "loss": 0.5085,
      "step": 16332
    },
    {
      "epoch": 6.3232675183894695,
      "grad_norm": 14.125852584838867,
      "learning_rate": 4.085258312900589e-06,
      "loss": 0.2911,
      "step": 16333
    },
    {
      "epoch": 6.32365466511808,
      "grad_norm": 120.83419799804688,
      "learning_rate": 4.084828149868801e-06,
      "loss": 1.1888,
      "step": 16334
    },
    {
      "epoch": 6.32404181184669,
      "grad_norm": 50.92362976074219,
      "learning_rate": 4.084397986837011e-06,
      "loss": 1.2173,
      "step": 16335
    },
    {
      "epoch": 6.3244289585753,
      "grad_norm": 125.81837463378906,
      "learning_rate": 4.083967823805223e-06,
      "loss": 0.6417,
      "step": 16336
    },
    {
      "epoch": 6.32481610530391,
      "grad_norm": 35.973392486572266,
      "learning_rate": 4.083537660773433e-06,
      "loss": 1.7987,
      "step": 16337
    },
    {
      "epoch": 6.32520325203252,
      "grad_norm": 1.9555926322937012,
      "learning_rate": 4.083107497741645e-06,
      "loss": 0.0307,
      "step": 16338
    },
    {
      "epoch": 6.325590398761131,
      "grad_norm": 16.28929901123047,
      "learning_rate": 4.082677334709855e-06,
      "loss": 0.157,
      "step": 16339
    },
    {
      "epoch": 6.3259775454897405,
      "grad_norm": 28.917747497558594,
      "learning_rate": 4.0822471716780666e-06,
      "loss": 0.3717,
      "step": 16340
    },
    {
      "epoch": 6.326364692218351,
      "grad_norm": 34.4206428527832,
      "learning_rate": 4.081817008646277e-06,
      "loss": 1.6035,
      "step": 16341
    },
    {
      "epoch": 6.326751838946961,
      "grad_norm": 46.573211669921875,
      "learning_rate": 4.0813868456144886e-06,
      "loss": 1.1925,
      "step": 16342
    },
    {
      "epoch": 6.327138985675571,
      "grad_norm": 7.732907295227051,
      "learning_rate": 4.080956682582699e-06,
      "loss": 0.1343,
      "step": 16343
    },
    {
      "epoch": 6.327526132404181,
      "grad_norm": 25.333194732666016,
      "learning_rate": 4.0805265195509105e-06,
      "loss": 1.6123,
      "step": 16344
    },
    {
      "epoch": 6.327913279132791,
      "grad_norm": 63.589622497558594,
      "learning_rate": 4.080096356519121e-06,
      "loss": 0.4826,
      "step": 16345
    },
    {
      "epoch": 6.328300425861402,
      "grad_norm": 25.56551742553711,
      "learning_rate": 4.0796661934873325e-06,
      "loss": 0.4832,
      "step": 16346
    },
    {
      "epoch": 6.328687572590011,
      "grad_norm": 47.06803512573242,
      "learning_rate": 4.079236030455543e-06,
      "loss": 0.7258,
      "step": 16347
    },
    {
      "epoch": 6.329074719318622,
      "grad_norm": 7.882024765014648,
      "learning_rate": 4.078805867423754e-06,
      "loss": 0.0889,
      "step": 16348
    },
    {
      "epoch": 6.3294618660472315,
      "grad_norm": 56.15665054321289,
      "learning_rate": 4.078375704391965e-06,
      "loss": 0.3692,
      "step": 16349
    },
    {
      "epoch": 6.329849012775842,
      "grad_norm": 5.242977619171143,
      "learning_rate": 4.077945541360176e-06,
      "loss": 0.2068,
      "step": 16350
    },
    {
      "epoch": 6.3302361595044525,
      "grad_norm": 35.53830337524414,
      "learning_rate": 4.077515378328386e-06,
      "loss": 2.6574,
      "step": 16351
    },
    {
      "epoch": 6.330623306233062,
      "grad_norm": 12.08942985534668,
      "learning_rate": 4.077085215296598e-06,
      "loss": 0.4111,
      "step": 16352
    },
    {
      "epoch": 6.331010452961673,
      "grad_norm": 2.753628730773926,
      "learning_rate": 4.076655052264808e-06,
      "loss": 0.0213,
      "step": 16353
    },
    {
      "epoch": 6.331397599690282,
      "grad_norm": 83.2732925415039,
      "learning_rate": 4.07622488923302e-06,
      "loss": 2.6479,
      "step": 16354
    },
    {
      "epoch": 6.331784746418893,
      "grad_norm": 19.813892364501953,
      "learning_rate": 4.07579472620123e-06,
      "loss": 2.089,
      "step": 16355
    },
    {
      "epoch": 6.332171893147503,
      "grad_norm": 41.476680755615234,
      "learning_rate": 4.075364563169442e-06,
      "loss": 1.4347,
      "step": 16356
    },
    {
      "epoch": 6.332559039876113,
      "grad_norm": 2.899596691131592,
      "learning_rate": 4.074934400137652e-06,
      "loss": 0.0741,
      "step": 16357
    },
    {
      "epoch": 6.332946186604723,
      "grad_norm": 8.871683120727539,
      "learning_rate": 4.074504237105864e-06,
      "loss": 0.2783,
      "step": 16358
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 16.760854721069336,
      "learning_rate": 4.074074074074074e-06,
      "loss": 0.2108,
      "step": 16359
    },
    {
      "epoch": 6.3337204800619435,
      "grad_norm": 33.8359375,
      "learning_rate": 4.0736439110422856e-06,
      "loss": 0.5586,
      "step": 16360
    },
    {
      "epoch": 6.334107626790553,
      "grad_norm": 17.589691162109375,
      "learning_rate": 4.073213748010496e-06,
      "loss": 0.4337,
      "step": 16361
    },
    {
      "epoch": 6.334494773519164,
      "grad_norm": 4.5261359214782715,
      "learning_rate": 4.0727835849787076e-06,
      "loss": 0.2026,
      "step": 16362
    },
    {
      "epoch": 6.334881920247774,
      "grad_norm": 19.459331512451172,
      "learning_rate": 4.072353421946918e-06,
      "loss": 1.2823,
      "step": 16363
    },
    {
      "epoch": 6.335269066976384,
      "grad_norm": 80.81575012207031,
      "learning_rate": 4.0719232589151295e-06,
      "loss": 3.1097,
      "step": 16364
    },
    {
      "epoch": 6.335656213704994,
      "grad_norm": 33.71406936645508,
      "learning_rate": 4.07149309588334e-06,
      "loss": 1.1344,
      "step": 16365
    },
    {
      "epoch": 6.336043360433604,
      "grad_norm": 45.351131439208984,
      "learning_rate": 4.071062932851551e-06,
      "loss": 1.2224,
      "step": 16366
    },
    {
      "epoch": 6.3364305071622145,
      "grad_norm": 64.70663452148438,
      "learning_rate": 4.070632769819762e-06,
      "loss": 0.6842,
      "step": 16367
    },
    {
      "epoch": 6.336817653890825,
      "grad_norm": 287.4940490722656,
      "learning_rate": 4.070202606787973e-06,
      "loss": 0.953,
      "step": 16368
    },
    {
      "epoch": 6.337204800619435,
      "grad_norm": 31.082473754882812,
      "learning_rate": 4.069772443756184e-06,
      "loss": 1.5798,
      "step": 16369
    },
    {
      "epoch": 6.337591947348045,
      "grad_norm": 68.43350219726562,
      "learning_rate": 4.069342280724395e-06,
      "loss": 0.5077,
      "step": 16370
    },
    {
      "epoch": 6.337979094076655,
      "grad_norm": 22.783893585205078,
      "learning_rate": 4.068912117692606e-06,
      "loss": 1.0765,
      "step": 16371
    },
    {
      "epoch": 6.338366240805265,
      "grad_norm": 4.954275608062744,
      "learning_rate": 4.068481954660817e-06,
      "loss": 0.1971,
      "step": 16372
    },
    {
      "epoch": 6.338753387533876,
      "grad_norm": 40.16315460205078,
      "learning_rate": 4.068051791629028e-06,
      "loss": 1.5942,
      "step": 16373
    },
    {
      "epoch": 6.339140534262485,
      "grad_norm": 9.381139755249023,
      "learning_rate": 4.067621628597239e-06,
      "loss": 0.3534,
      "step": 16374
    },
    {
      "epoch": 6.339527680991096,
      "grad_norm": 42.851280212402344,
      "learning_rate": 4.06719146556545e-06,
      "loss": 2.8948,
      "step": 16375
    },
    {
      "epoch": 6.3399148277197055,
      "grad_norm": 56.09461212158203,
      "learning_rate": 4.066761302533661e-06,
      "loss": 1.4729,
      "step": 16376
    },
    {
      "epoch": 6.340301974448316,
      "grad_norm": 35.85913848876953,
      "learning_rate": 4.066331139501872e-06,
      "loss": 2.0546,
      "step": 16377
    },
    {
      "epoch": 6.340689121176926,
      "grad_norm": 35.57706832885742,
      "learning_rate": 4.065900976470083e-06,
      "loss": 0.2295,
      "step": 16378
    },
    {
      "epoch": 6.341076267905536,
      "grad_norm": 72.52306365966797,
      "learning_rate": 4.065470813438294e-06,
      "loss": 0.9285,
      "step": 16379
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 105.47518920898438,
      "learning_rate": 4.0650406504065046e-06,
      "loss": 1.0227,
      "step": 16380
    },
    {
      "epoch": 6.341850561362756,
      "grad_norm": 84.56989288330078,
      "learning_rate": 4.064610487374715e-06,
      "loss": 0.8688,
      "step": 16381
    },
    {
      "epoch": 6.342237708091367,
      "grad_norm": 9.637350082397461,
      "learning_rate": 4.0641803243429266e-06,
      "loss": 0.1956,
      "step": 16382
    },
    {
      "epoch": 6.342624854819976,
      "grad_norm": 37.24407958984375,
      "learning_rate": 4.063750161311137e-06,
      "loss": 0.3796,
      "step": 16383
    },
    {
      "epoch": 6.343012001548587,
      "grad_norm": 46.49480056762695,
      "learning_rate": 4.063319998279348e-06,
      "loss": 0.3276,
      "step": 16384
    },
    {
      "epoch": 6.343399148277197,
      "grad_norm": 33.62998962402344,
      "learning_rate": 4.062889835247559e-06,
      "loss": 0.4816,
      "step": 16385
    },
    {
      "epoch": 6.343786295005807,
      "grad_norm": 63.54804229736328,
      "learning_rate": 4.06245967221577e-06,
      "loss": 1.0111,
      "step": 16386
    },
    {
      "epoch": 6.3441734417344176,
      "grad_norm": 14.016469955444336,
      "learning_rate": 4.062029509183981e-06,
      "loss": 0.9475,
      "step": 16387
    },
    {
      "epoch": 6.344560588463027,
      "grad_norm": 22.474903106689453,
      "learning_rate": 4.061599346152192e-06,
      "loss": 1.2563,
      "step": 16388
    },
    {
      "epoch": 6.344947735191638,
      "grad_norm": 1.96927809715271,
      "learning_rate": 4.061169183120403e-06,
      "loss": 0.0752,
      "step": 16389
    },
    {
      "epoch": 6.345334881920248,
      "grad_norm": 105.76329803466797,
      "learning_rate": 4.060739020088614e-06,
      "loss": 2.0634,
      "step": 16390
    },
    {
      "epoch": 6.345722028648858,
      "grad_norm": 21.440135955810547,
      "learning_rate": 4.060308857056825e-06,
      "loss": 0.2784,
      "step": 16391
    },
    {
      "epoch": 6.346109175377468,
      "grad_norm": 34.77433776855469,
      "learning_rate": 4.059878694025036e-06,
      "loss": 2.3448,
      "step": 16392
    },
    {
      "epoch": 6.346496322106078,
      "grad_norm": 244.5651397705078,
      "learning_rate": 4.059448530993247e-06,
      "loss": 1.1775,
      "step": 16393
    },
    {
      "epoch": 6.3468834688346885,
      "grad_norm": 0.4676702916622162,
      "learning_rate": 4.059018367961458e-06,
      "loss": 0.0132,
      "step": 16394
    },
    {
      "epoch": 6.347270615563298,
      "grad_norm": 3.543433427810669,
      "learning_rate": 4.058588204929669e-06,
      "loss": 0.1625,
      "step": 16395
    },
    {
      "epoch": 6.347657762291909,
      "grad_norm": 25.490732192993164,
      "learning_rate": 4.05815804189788e-06,
      "loss": 1.2491,
      "step": 16396
    },
    {
      "epoch": 6.348044909020519,
      "grad_norm": 108.54557800292969,
      "learning_rate": 4.057727878866091e-06,
      "loss": 1.4697,
      "step": 16397
    },
    {
      "epoch": 6.348432055749129,
      "grad_norm": 18.1637020111084,
      "learning_rate": 4.057297715834302e-06,
      "loss": 0.4568,
      "step": 16398
    },
    {
      "epoch": 6.348819202477739,
      "grad_norm": 4.3741888999938965,
      "learning_rate": 4.056867552802512e-06,
      "loss": 0.1976,
      "step": 16399
    },
    {
      "epoch": 6.349206349206349,
      "grad_norm": 62.12837219238281,
      "learning_rate": 4.0564373897707236e-06,
      "loss": 1.3244,
      "step": 16400
    },
    {
      "epoch": 6.349593495934959,
      "grad_norm": 10.949471473693848,
      "learning_rate": 4.056007226738934e-06,
      "loss": 0.2905,
      "step": 16401
    },
    {
      "epoch": 6.34998064266357,
      "grad_norm": 30.873497009277344,
      "learning_rate": 4.055577063707145e-06,
      "loss": 0.3242,
      "step": 16402
    },
    {
      "epoch": 6.3503677893921795,
      "grad_norm": 4.700995445251465,
      "learning_rate": 4.055146900675356e-06,
      "loss": 0.1198,
      "step": 16403
    },
    {
      "epoch": 6.35075493612079,
      "grad_norm": 3.605485677719116,
      "learning_rate": 4.054716737643567e-06,
      "loss": 0.112,
      "step": 16404
    },
    {
      "epoch": 6.3511420828494,
      "grad_norm": 6.829814910888672,
      "learning_rate": 4.054286574611778e-06,
      "loss": 0.1858,
      "step": 16405
    },
    {
      "epoch": 6.35152922957801,
      "grad_norm": 21.57465171813965,
      "learning_rate": 4.053856411579989e-06,
      "loss": 1.7104,
      "step": 16406
    },
    {
      "epoch": 6.351916376306621,
      "grad_norm": 27.546157836914062,
      "learning_rate": 4.0534262485482e-06,
      "loss": 1.0409,
      "step": 16407
    },
    {
      "epoch": 6.35230352303523,
      "grad_norm": 61.022972106933594,
      "learning_rate": 4.0529960855164115e-06,
      "loss": 3.4291,
      "step": 16408
    },
    {
      "epoch": 6.352690669763841,
      "grad_norm": 15.273175239562988,
      "learning_rate": 4.052565922484622e-06,
      "loss": 0.1918,
      "step": 16409
    },
    {
      "epoch": 6.35307781649245,
      "grad_norm": 66.03092193603516,
      "learning_rate": 4.0521357594528335e-06,
      "loss": 1.084,
      "step": 16410
    },
    {
      "epoch": 6.353464963221061,
      "grad_norm": 196.69081115722656,
      "learning_rate": 4.051705596421044e-06,
      "loss": 0.5869,
      "step": 16411
    },
    {
      "epoch": 6.3538521099496705,
      "grad_norm": 29.626380920410156,
      "learning_rate": 4.0512754333892555e-06,
      "loss": 0.6019,
      "step": 16412
    },
    {
      "epoch": 6.354239256678281,
      "grad_norm": 45.431758880615234,
      "learning_rate": 4.050845270357466e-06,
      "loss": 3.1153,
      "step": 16413
    },
    {
      "epoch": 6.3546264034068916,
      "grad_norm": 49.93478775024414,
      "learning_rate": 4.050415107325677e-06,
      "loss": 0.4785,
      "step": 16414
    },
    {
      "epoch": 6.355013550135501,
      "grad_norm": 5.846182346343994,
      "learning_rate": 4.049984944293888e-06,
      "loss": 0.2259,
      "step": 16415
    },
    {
      "epoch": 6.355400696864112,
      "grad_norm": 104.33271789550781,
      "learning_rate": 4.049554781262099e-06,
      "loss": 0.6983,
      "step": 16416
    },
    {
      "epoch": 6.355787843592721,
      "grad_norm": 27.358856201171875,
      "learning_rate": 4.049124618230309e-06,
      "loss": 1.3824,
      "step": 16417
    },
    {
      "epoch": 6.356174990321332,
      "grad_norm": 42.78129959106445,
      "learning_rate": 4.048694455198521e-06,
      "loss": 1.8411,
      "step": 16418
    },
    {
      "epoch": 6.356562137049942,
      "grad_norm": 25.40873146057129,
      "learning_rate": 4.048264292166731e-06,
      "loss": 1.6459,
      "step": 16419
    },
    {
      "epoch": 6.356949283778552,
      "grad_norm": 75.37519073486328,
      "learning_rate": 4.0478341291349426e-06,
      "loss": 0.7255,
      "step": 16420
    },
    {
      "epoch": 6.3573364305071625,
      "grad_norm": 76.53629302978516,
      "learning_rate": 4.047403966103153e-06,
      "loss": 1.2097,
      "step": 16421
    },
    {
      "epoch": 6.357723577235772,
      "grad_norm": 58.87221908569336,
      "learning_rate": 4.0469738030713645e-06,
      "loss": 0.5018,
      "step": 16422
    },
    {
      "epoch": 6.358110723964383,
      "grad_norm": 28.037418365478516,
      "learning_rate": 4.046543640039575e-06,
      "loss": 0.3127,
      "step": 16423
    },
    {
      "epoch": 6.358497870692993,
      "grad_norm": 43.60621643066406,
      "learning_rate": 4.0461134770077865e-06,
      "loss": 1.7639,
      "step": 16424
    },
    {
      "epoch": 6.358885017421603,
      "grad_norm": 30.859621047973633,
      "learning_rate": 4.045683313975997e-06,
      "loss": 0.9452,
      "step": 16425
    },
    {
      "epoch": 6.359272164150213,
      "grad_norm": 5.035832405090332,
      "learning_rate": 4.0452531509442085e-06,
      "loss": 0.1607,
      "step": 16426
    },
    {
      "epoch": 6.359659310878823,
      "grad_norm": 77.58658599853516,
      "learning_rate": 4.044822987912419e-06,
      "loss": 1.1784,
      "step": 16427
    },
    {
      "epoch": 6.360046457607433,
      "grad_norm": 16.15233612060547,
      "learning_rate": 4.0443928248806305e-06,
      "loss": 0.1615,
      "step": 16428
    },
    {
      "epoch": 6.360433604336043,
      "grad_norm": 17.77418327331543,
      "learning_rate": 4.043962661848841e-06,
      "loss": 0.2333,
      "step": 16429
    },
    {
      "epoch": 6.3608207510646535,
      "grad_norm": 39.2507209777832,
      "learning_rate": 4.0435324988170525e-06,
      "loss": 1.1022,
      "step": 16430
    },
    {
      "epoch": 6.361207897793264,
      "grad_norm": 49.33466339111328,
      "learning_rate": 4.043102335785263e-06,
      "loss": 0.685,
      "step": 16431
    },
    {
      "epoch": 6.361595044521874,
      "grad_norm": 34.68562316894531,
      "learning_rate": 4.042672172753474e-06,
      "loss": 0.7807,
      "step": 16432
    },
    {
      "epoch": 6.361982191250484,
      "grad_norm": 25.341585159301758,
      "learning_rate": 4.042242009721685e-06,
      "loss": 0.6216,
      "step": 16433
    },
    {
      "epoch": 6.362369337979094,
      "grad_norm": 281.6878662109375,
      "learning_rate": 4.041811846689896e-06,
      "loss": 3.0017,
      "step": 16434
    },
    {
      "epoch": 6.362756484707704,
      "grad_norm": 37.90138244628906,
      "learning_rate": 4.041381683658106e-06,
      "loss": 1.5829,
      "step": 16435
    },
    {
      "epoch": 6.363143631436314,
      "grad_norm": 30.968889236450195,
      "learning_rate": 4.040951520626318e-06,
      "loss": 0.2546,
      "step": 16436
    },
    {
      "epoch": 6.363530778164924,
      "grad_norm": 76.68331146240234,
      "learning_rate": 4.040521357594528e-06,
      "loss": 0.4031,
      "step": 16437
    },
    {
      "epoch": 6.363917924893535,
      "grad_norm": 52.982322692871094,
      "learning_rate": 4.04009119456274e-06,
      "loss": 1.1661,
      "step": 16438
    },
    {
      "epoch": 6.3643050716221445,
      "grad_norm": 64.25743865966797,
      "learning_rate": 4.03966103153095e-06,
      "loss": 3.1783,
      "step": 16439
    },
    {
      "epoch": 6.364692218350755,
      "grad_norm": 55.31742858886719,
      "learning_rate": 4.0392308684991616e-06,
      "loss": 0.3294,
      "step": 16440
    },
    {
      "epoch": 6.365079365079365,
      "grad_norm": 81.93191528320312,
      "learning_rate": 4.038800705467372e-06,
      "loss": 2.3389,
      "step": 16441
    },
    {
      "epoch": 6.365466511807975,
      "grad_norm": 13.007502555847168,
      "learning_rate": 4.0383705424355835e-06,
      "loss": 0.1339,
      "step": 16442
    },
    {
      "epoch": 6.365853658536586,
      "grad_norm": 125.10177612304688,
      "learning_rate": 4.037940379403794e-06,
      "loss": 1.4231,
      "step": 16443
    },
    {
      "epoch": 6.366240805265195,
      "grad_norm": 1.3356294631958008,
      "learning_rate": 4.0375102163720055e-06,
      "loss": 0.0489,
      "step": 16444
    },
    {
      "epoch": 6.366627951993806,
      "grad_norm": 21.115890502929688,
      "learning_rate": 4.037080053340216e-06,
      "loss": 1.588,
      "step": 16445
    },
    {
      "epoch": 6.3670150987224154,
      "grad_norm": 31.421463012695312,
      "learning_rate": 4.0366498903084275e-06,
      "loss": 0.1873,
      "step": 16446
    },
    {
      "epoch": 6.367402245451026,
      "grad_norm": 54.685672760009766,
      "learning_rate": 4.036219727276638e-06,
      "loss": 2.4437,
      "step": 16447
    },
    {
      "epoch": 6.3677893921796365,
      "grad_norm": 104.0043716430664,
      "learning_rate": 4.0357895642448495e-06,
      "loss": 0.2898,
      "step": 16448
    },
    {
      "epoch": 6.368176538908246,
      "grad_norm": 72.9365234375,
      "learning_rate": 4.03535940121306e-06,
      "loss": 2.3648,
      "step": 16449
    },
    {
      "epoch": 6.368563685636857,
      "grad_norm": 2.1457149982452393,
      "learning_rate": 4.034929238181271e-06,
      "loss": 0.0953,
      "step": 16450
    },
    {
      "epoch": 6.368950832365466,
      "grad_norm": 111.86092376708984,
      "learning_rate": 4.034499075149482e-06,
      "loss": 2.1055,
      "step": 16451
    },
    {
      "epoch": 6.369337979094077,
      "grad_norm": 212.50025939941406,
      "learning_rate": 4.034068912117693e-06,
      "loss": 1.3436,
      "step": 16452
    },
    {
      "epoch": 6.369725125822686,
      "grad_norm": 96.5271987915039,
      "learning_rate": 4.033638749085904e-06,
      "loss": 0.5564,
      "step": 16453
    },
    {
      "epoch": 6.370112272551297,
      "grad_norm": 31.341203689575195,
      "learning_rate": 4.033208586054115e-06,
      "loss": 1.5429,
      "step": 16454
    },
    {
      "epoch": 6.370499419279907,
      "grad_norm": 46.79853439331055,
      "learning_rate": 4.032778423022326e-06,
      "loss": 2.6252,
      "step": 16455
    },
    {
      "epoch": 6.370886566008517,
      "grad_norm": 11.73595142364502,
      "learning_rate": 4.032348259990537e-06,
      "loss": 0.1682,
      "step": 16456
    },
    {
      "epoch": 6.3712737127371275,
      "grad_norm": 21.28544807434082,
      "learning_rate": 4.031918096958748e-06,
      "loss": 1.8151,
      "step": 16457
    },
    {
      "epoch": 6.371660859465737,
      "grad_norm": 13.026762008666992,
      "learning_rate": 4.0314879339269586e-06,
      "loss": 0.3432,
      "step": 16458
    },
    {
      "epoch": 6.372048006194348,
      "grad_norm": 85.78651428222656,
      "learning_rate": 4.03105777089517e-06,
      "loss": 1.1413,
      "step": 16459
    },
    {
      "epoch": 6.372435152922958,
      "grad_norm": 92.57714080810547,
      "learning_rate": 4.0306276078633806e-06,
      "loss": 1.8911,
      "step": 16460
    },
    {
      "epoch": 6.372822299651568,
      "grad_norm": 2.5036520957946777,
      "learning_rate": 4.030197444831592e-06,
      "loss": 0.1125,
      "step": 16461
    },
    {
      "epoch": 6.373209446380178,
      "grad_norm": 30.278696060180664,
      "learning_rate": 4.0297672817998025e-06,
      "loss": 1.1579,
      "step": 16462
    },
    {
      "epoch": 6.373596593108788,
      "grad_norm": 2.6579110622406006,
      "learning_rate": 4.029337118768014e-06,
      "loss": 0.0719,
      "step": 16463
    },
    {
      "epoch": 6.373983739837398,
      "grad_norm": 11.740707397460938,
      "learning_rate": 4.0289069557362245e-06,
      "loss": 0.3758,
      "step": 16464
    },
    {
      "epoch": 6.374370886566009,
      "grad_norm": 23.389488220214844,
      "learning_rate": 4.028476792704435e-06,
      "loss": 0.2487,
      "step": 16465
    },
    {
      "epoch": 6.3747580332946185,
      "grad_norm": 25.016260147094727,
      "learning_rate": 4.0280466296726465e-06,
      "loss": 0.1724,
      "step": 16466
    },
    {
      "epoch": 6.375145180023229,
      "grad_norm": 21.67081642150879,
      "learning_rate": 4.027616466640857e-06,
      "loss": 0.2665,
      "step": 16467
    },
    {
      "epoch": 6.375532326751839,
      "grad_norm": 1.8967353105545044,
      "learning_rate": 4.027186303609068e-06,
      "loss": 0.0598,
      "step": 16468
    },
    {
      "epoch": 6.375919473480449,
      "grad_norm": 167.73635864257812,
      "learning_rate": 4.026756140577279e-06,
      "loss": 2.3631,
      "step": 16469
    },
    {
      "epoch": 6.376306620209059,
      "grad_norm": 50.038429260253906,
      "learning_rate": 4.02632597754549e-06,
      "loss": 0.2404,
      "step": 16470
    },
    {
      "epoch": 6.376693766937669,
      "grad_norm": 170.27137756347656,
      "learning_rate": 4.025895814513701e-06,
      "loss": 0.8976,
      "step": 16471
    },
    {
      "epoch": 6.37708091366628,
      "grad_norm": 24.901371002197266,
      "learning_rate": 4.025465651481912e-06,
      "loss": 2.7486,
      "step": 16472
    },
    {
      "epoch": 6.3774680603948894,
      "grad_norm": 52.377296447753906,
      "learning_rate": 4.025035488450123e-06,
      "loss": 0.7292,
      "step": 16473
    },
    {
      "epoch": 6.3778552071235,
      "grad_norm": 50.006343841552734,
      "learning_rate": 4.024605325418334e-06,
      "loss": 0.995,
      "step": 16474
    },
    {
      "epoch": 6.37824235385211,
      "grad_norm": 481.2847595214844,
      "learning_rate": 4.024175162386545e-06,
      "loss": 1.579,
      "step": 16475
    },
    {
      "epoch": 6.37862950058072,
      "grad_norm": 52.01008987426758,
      "learning_rate": 4.023744999354756e-06,
      "loss": 0.9962,
      "step": 16476
    },
    {
      "epoch": 6.379016647309331,
      "grad_norm": 34.75242614746094,
      "learning_rate": 4.023314836322967e-06,
      "loss": 1.4307,
      "step": 16477
    },
    {
      "epoch": 6.37940379403794,
      "grad_norm": 51.329402923583984,
      "learning_rate": 4.0228846732911776e-06,
      "loss": 2.3974,
      "step": 16478
    },
    {
      "epoch": 6.379790940766551,
      "grad_norm": 2.0951268672943115,
      "learning_rate": 4.022454510259389e-06,
      "loss": 0.0723,
      "step": 16479
    },
    {
      "epoch": 6.38017808749516,
      "grad_norm": 84.45683288574219,
      "learning_rate": 4.0220243472275996e-06,
      "loss": 2.7978,
      "step": 16480
    },
    {
      "epoch": 6.380565234223771,
      "grad_norm": 2.8044114112854004,
      "learning_rate": 4.021594184195811e-06,
      "loss": 0.1443,
      "step": 16481
    },
    {
      "epoch": 6.380952380952381,
      "grad_norm": 30.125320434570312,
      "learning_rate": 4.0211640211640215e-06,
      "loss": 1.5967,
      "step": 16482
    },
    {
      "epoch": 6.381339527680991,
      "grad_norm": 91.1645278930664,
      "learning_rate": 4.020733858132232e-06,
      "loss": 4.6845,
      "step": 16483
    },
    {
      "epoch": 6.3817266744096015,
      "grad_norm": 208.28485107421875,
      "learning_rate": 4.0203036951004435e-06,
      "loss": 0.2571,
      "step": 16484
    },
    {
      "epoch": 6.382113821138211,
      "grad_norm": 24.833717346191406,
      "learning_rate": 4.019873532068654e-06,
      "loss": 1.3153,
      "step": 16485
    },
    {
      "epoch": 6.382500967866822,
      "grad_norm": 25.06410026550293,
      "learning_rate": 4.019443369036865e-06,
      "loss": 1.1218,
      "step": 16486
    },
    {
      "epoch": 6.382888114595431,
      "grad_norm": 13.726001739501953,
      "learning_rate": 4.019013206005076e-06,
      "loss": 0.3066,
      "step": 16487
    },
    {
      "epoch": 6.383275261324042,
      "grad_norm": 51.81325149536133,
      "learning_rate": 4.018583042973287e-06,
      "loss": 0.3755,
      "step": 16488
    },
    {
      "epoch": 6.383662408052652,
      "grad_norm": 164.00021362304688,
      "learning_rate": 4.018152879941498e-06,
      "loss": 0.4221,
      "step": 16489
    },
    {
      "epoch": 6.384049554781262,
      "grad_norm": 3.852813720703125,
      "learning_rate": 4.017722716909709e-06,
      "loss": 0.1244,
      "step": 16490
    },
    {
      "epoch": 6.384436701509872,
      "grad_norm": 23.145938873291016,
      "learning_rate": 4.01729255387792e-06,
      "loss": 0.2002,
      "step": 16491
    },
    {
      "epoch": 6.384823848238482,
      "grad_norm": 25.024625778198242,
      "learning_rate": 4.0168623908461315e-06,
      "loss": 1.3406,
      "step": 16492
    },
    {
      "epoch": 6.3852109949670925,
      "grad_norm": 2.1734707355499268,
      "learning_rate": 4.016432227814342e-06,
      "loss": 0.0948,
      "step": 16493
    },
    {
      "epoch": 6.385598141695703,
      "grad_norm": 3.1223275661468506,
      "learning_rate": 4.0160020647825534e-06,
      "loss": 0.1079,
      "step": 16494
    },
    {
      "epoch": 6.385985288424313,
      "grad_norm": 311.569580078125,
      "learning_rate": 4.015571901750764e-06,
      "loss": 1.2839,
      "step": 16495
    },
    {
      "epoch": 6.386372435152923,
      "grad_norm": 94.34767150878906,
      "learning_rate": 4.0151417387189754e-06,
      "loss": 1.2237,
      "step": 16496
    },
    {
      "epoch": 6.386759581881533,
      "grad_norm": 126.93090057373047,
      "learning_rate": 4.014711575687186e-06,
      "loss": 1.2111,
      "step": 16497
    },
    {
      "epoch": 6.387146728610143,
      "grad_norm": 70.00653839111328,
      "learning_rate": 4.0142814126553966e-06,
      "loss": 0.7532,
      "step": 16498
    },
    {
      "epoch": 6.387533875338754,
      "grad_norm": 27.78108024597168,
      "learning_rate": 4.013851249623608e-06,
      "loss": 0.565,
      "step": 16499
    },
    {
      "epoch": 6.3879210220673635,
      "grad_norm": 72.23658752441406,
      "learning_rate": 4.0134210865918186e-06,
      "loss": 1.1676,
      "step": 16500
    },
    {
      "epoch": 6.388308168795974,
      "grad_norm": 2.9875824451446533,
      "learning_rate": 4.012990923560029e-06,
      "loss": 0.0299,
      "step": 16501
    },
    {
      "epoch": 6.388695315524584,
      "grad_norm": 24.296951293945312,
      "learning_rate": 4.0125607605282405e-06,
      "loss": 1.723,
      "step": 16502
    },
    {
      "epoch": 6.389082462253194,
      "grad_norm": 32.03996658325195,
      "learning_rate": 4.012130597496451e-06,
      "loss": 0.8433,
      "step": 16503
    },
    {
      "epoch": 6.389469608981804,
      "grad_norm": 91.97700500488281,
      "learning_rate": 4.0117004344646625e-06,
      "loss": 0.6599,
      "step": 16504
    },
    {
      "epoch": 6.389856755710414,
      "grad_norm": 0.49825844168663025,
      "learning_rate": 4.011270271432873e-06,
      "loss": 0.0129,
      "step": 16505
    },
    {
      "epoch": 6.390243902439025,
      "grad_norm": 24.70786476135254,
      "learning_rate": 4.0108401084010845e-06,
      "loss": 0.4318,
      "step": 16506
    },
    {
      "epoch": 6.390631049167634,
      "grad_norm": 8.789849281311035,
      "learning_rate": 4.010409945369295e-06,
      "loss": 0.2039,
      "step": 16507
    },
    {
      "epoch": 6.391018195896245,
      "grad_norm": 141.34701538085938,
      "learning_rate": 4.0099797823375065e-06,
      "loss": 3.7754,
      "step": 16508
    },
    {
      "epoch": 6.3914053426248545,
      "grad_norm": 38.634281158447266,
      "learning_rate": 4.009549619305717e-06,
      "loss": 0.3842,
      "step": 16509
    },
    {
      "epoch": 6.391792489353465,
      "grad_norm": 62.74729919433594,
      "learning_rate": 4.0091194562739285e-06,
      "loss": 0.4307,
      "step": 16510
    },
    {
      "epoch": 6.3921796360820755,
      "grad_norm": 2.5709753036499023,
      "learning_rate": 4.008689293242139e-06,
      "loss": 0.0745,
      "step": 16511
    },
    {
      "epoch": 6.392566782810685,
      "grad_norm": 29.240158081054688,
      "learning_rate": 4.0082591302103505e-06,
      "loss": 0.3179,
      "step": 16512
    },
    {
      "epoch": 6.392953929539296,
      "grad_norm": 65.7951889038086,
      "learning_rate": 4.007828967178561e-06,
      "loss": 2.6408,
      "step": 16513
    },
    {
      "epoch": 6.393341076267905,
      "grad_norm": 21.64876937866211,
      "learning_rate": 4.0073988041467724e-06,
      "loss": 1.1751,
      "step": 16514
    },
    {
      "epoch": 6.393728222996516,
      "grad_norm": 6.772802352905273,
      "learning_rate": 4.006968641114983e-06,
      "loss": 0.1356,
      "step": 16515
    },
    {
      "epoch": 6.394115369725126,
      "grad_norm": 14.669692039489746,
      "learning_rate": 4.006538478083194e-06,
      "loss": 0.2356,
      "step": 16516
    },
    {
      "epoch": 6.394502516453736,
      "grad_norm": 59.11539077758789,
      "learning_rate": 4.006108315051405e-06,
      "loss": 1.5268,
      "step": 16517
    },
    {
      "epoch": 6.394889663182346,
      "grad_norm": 6.541231155395508,
      "learning_rate": 4.0056781520196156e-06,
      "loss": 0.13,
      "step": 16518
    },
    {
      "epoch": 6.395276809910956,
      "grad_norm": 3.1321187019348145,
      "learning_rate": 4.005247988987826e-06,
      "loss": 0.137,
      "step": 16519
    },
    {
      "epoch": 6.3956639566395665,
      "grad_norm": 78.53579711914062,
      "learning_rate": 4.0048178259560376e-06,
      "loss": 0.3738,
      "step": 16520
    },
    {
      "epoch": 6.396051103368176,
      "grad_norm": 65.63630676269531,
      "learning_rate": 4.004387662924248e-06,
      "loss": 2.4507,
      "step": 16521
    },
    {
      "epoch": 6.396438250096787,
      "grad_norm": 32.859981536865234,
      "learning_rate": 4.0039574998924595e-06,
      "loss": 1.7238,
      "step": 16522
    },
    {
      "epoch": 6.396825396825397,
      "grad_norm": 7.364040374755859,
      "learning_rate": 4.00352733686067e-06,
      "loss": 0.1601,
      "step": 16523
    },
    {
      "epoch": 6.397212543554007,
      "grad_norm": 42.66196823120117,
      "learning_rate": 4.0030971738288815e-06,
      "loss": 1.376,
      "step": 16524
    },
    {
      "epoch": 6.397599690282617,
      "grad_norm": 590.43701171875,
      "learning_rate": 4.002667010797092e-06,
      "loss": 1.9083,
      "step": 16525
    },
    {
      "epoch": 6.397986837011227,
      "grad_norm": 35.52775955200195,
      "learning_rate": 4.0022368477653035e-06,
      "loss": 0.4649,
      "step": 16526
    },
    {
      "epoch": 6.3983739837398375,
      "grad_norm": 48.56617736816406,
      "learning_rate": 4.001806684733514e-06,
      "loss": 1.5069,
      "step": 16527
    },
    {
      "epoch": 6.398761130468447,
      "grad_norm": 23.63317108154297,
      "learning_rate": 4.0013765217017255e-06,
      "loss": 1.5171,
      "step": 16528
    },
    {
      "epoch": 6.399148277197058,
      "grad_norm": 1.4932725429534912,
      "learning_rate": 4.000946358669936e-06,
      "loss": 0.0553,
      "step": 16529
    },
    {
      "epoch": 6.399535423925668,
      "grad_norm": 44.87461853027344,
      "learning_rate": 4.0005161956381475e-06,
      "loss": 1.317,
      "step": 16530
    },
    {
      "epoch": 6.399922570654278,
      "grad_norm": 30.82122802734375,
      "learning_rate": 4.000086032606358e-06,
      "loss": 1.2192,
      "step": 16531
    },
    {
      "epoch": 6.400309717382888,
      "grad_norm": 68.4761962890625,
      "learning_rate": 3.9996558695745695e-06,
      "loss": 1.02,
      "step": 16532
    },
    {
      "epoch": 6.400696864111498,
      "grad_norm": 15.312804222106934,
      "learning_rate": 3.99922570654278e-06,
      "loss": 0.473,
      "step": 16533
    },
    {
      "epoch": 6.401084010840108,
      "grad_norm": 14.021261215209961,
      "learning_rate": 3.998795543510991e-06,
      "loss": 0.4506,
      "step": 16534
    },
    {
      "epoch": 6.401471157568719,
      "grad_norm": 81.84634399414062,
      "learning_rate": 3.998365380479202e-06,
      "loss": 1.8652,
      "step": 16535
    },
    {
      "epoch": 6.4018583042973285,
      "grad_norm": 98.83096313476562,
      "learning_rate": 3.997935217447413e-06,
      "loss": 0.5009,
      "step": 16536
    },
    {
      "epoch": 6.402245451025939,
      "grad_norm": 36.06990432739258,
      "learning_rate": 3.997505054415624e-06,
      "loss": 0.367,
      "step": 16537
    },
    {
      "epoch": 6.402632597754549,
      "grad_norm": 105.02601623535156,
      "learning_rate": 3.9970748913838346e-06,
      "loss": 3.7098,
      "step": 16538
    },
    {
      "epoch": 6.403019744483159,
      "grad_norm": 7.075827598571777,
      "learning_rate": 3.996644728352046e-06,
      "loss": 0.2343,
      "step": 16539
    },
    {
      "epoch": 6.40340689121177,
      "grad_norm": 79.43644714355469,
      "learning_rate": 3.9962145653202566e-06,
      "loss": 1.2531,
      "step": 16540
    },
    {
      "epoch": 6.403794037940379,
      "grad_norm": 150.66232299804688,
      "learning_rate": 3.995784402288468e-06,
      "loss": 1.3902,
      "step": 16541
    },
    {
      "epoch": 6.40418118466899,
      "grad_norm": 2.7055325508117676,
      "learning_rate": 3.9953542392566785e-06,
      "loss": 0.1084,
      "step": 16542
    },
    {
      "epoch": 6.404568331397599,
      "grad_norm": 167.78857421875,
      "learning_rate": 3.99492407622489e-06,
      "loss": 0.6348,
      "step": 16543
    },
    {
      "epoch": 6.40495547812621,
      "grad_norm": 35.41798400878906,
      "learning_rate": 3.9944939131931005e-06,
      "loss": 3.1108,
      "step": 16544
    },
    {
      "epoch": 6.4053426248548195,
      "grad_norm": 81.11697387695312,
      "learning_rate": 3.994063750161312e-06,
      "loss": 1.3211,
      "step": 16545
    },
    {
      "epoch": 6.40572977158343,
      "grad_norm": 32.28697204589844,
      "learning_rate": 3.9936335871295225e-06,
      "loss": 1.3709,
      "step": 16546
    },
    {
      "epoch": 6.4061169183120406,
      "grad_norm": 170.28436279296875,
      "learning_rate": 3.993203424097734e-06,
      "loss": 1.9005,
      "step": 16547
    },
    {
      "epoch": 6.40650406504065,
      "grad_norm": 43.07307052612305,
      "learning_rate": 3.9927732610659445e-06,
      "loss": 1.3046,
      "step": 16548
    },
    {
      "epoch": 6.406891211769261,
      "grad_norm": 67.22115325927734,
      "learning_rate": 3.992343098034155e-06,
      "loss": 2.2824,
      "step": 16549
    },
    {
      "epoch": 6.40727835849787,
      "grad_norm": 22.82233428955078,
      "learning_rate": 3.9919129350023665e-06,
      "loss": 0.4277,
      "step": 16550
    },
    {
      "epoch": 6.407665505226481,
      "grad_norm": 3.229708433151245,
      "learning_rate": 3.991482771970577e-06,
      "loss": 0.113,
      "step": 16551
    },
    {
      "epoch": 6.408052651955091,
      "grad_norm": 62.628746032714844,
      "learning_rate": 3.991052608938788e-06,
      "loss": 0.6342,
      "step": 16552
    },
    {
      "epoch": 6.408439798683701,
      "grad_norm": 77.17291259765625,
      "learning_rate": 3.990622445906999e-06,
      "loss": 2.5439,
      "step": 16553
    },
    {
      "epoch": 6.4088269454123115,
      "grad_norm": 4.002838611602783,
      "learning_rate": 3.99019228287521e-06,
      "loss": 0.1943,
      "step": 16554
    },
    {
      "epoch": 6.409214092140921,
      "grad_norm": 49.10755920410156,
      "learning_rate": 3.989762119843421e-06,
      "loss": 2.8912,
      "step": 16555
    },
    {
      "epoch": 6.409601238869532,
      "grad_norm": 52.880523681640625,
      "learning_rate": 3.989331956811632e-06,
      "loss": 0.572,
      "step": 16556
    },
    {
      "epoch": 6.409988385598142,
      "grad_norm": 4.210647106170654,
      "learning_rate": 3.988901793779843e-06,
      "loss": 0.0793,
      "step": 16557
    },
    {
      "epoch": 6.410375532326752,
      "grad_norm": 1.3720571994781494,
      "learning_rate": 3.9884716307480536e-06,
      "loss": 0.0516,
      "step": 16558
    },
    {
      "epoch": 6.410762679055362,
      "grad_norm": 50.508750915527344,
      "learning_rate": 3.988041467716265e-06,
      "loss": 0.4685,
      "step": 16559
    },
    {
      "epoch": 6.411149825783972,
      "grad_norm": 109.14430236816406,
      "learning_rate": 3.9876113046844755e-06,
      "loss": 0.7068,
      "step": 16560
    },
    {
      "epoch": 6.411536972512582,
      "grad_norm": 68.0615463256836,
      "learning_rate": 3.987181141652687e-06,
      "loss": 4.9868,
      "step": 16561
    },
    {
      "epoch": 6.411924119241192,
      "grad_norm": 2.6052603721618652,
      "learning_rate": 3.9867509786208975e-06,
      "loss": 0.0692,
      "step": 16562
    },
    {
      "epoch": 6.4123112659698025,
      "grad_norm": 80.02035522460938,
      "learning_rate": 3.986320815589109e-06,
      "loss": 1.1388,
      "step": 16563
    },
    {
      "epoch": 6.412698412698413,
      "grad_norm": 126.93112182617188,
      "learning_rate": 3.9858906525573195e-06,
      "loss": 2.066,
      "step": 16564
    },
    {
      "epoch": 6.413085559427023,
      "grad_norm": 26.829391479492188,
      "learning_rate": 3.985460489525531e-06,
      "loss": 0.2618,
      "step": 16565
    },
    {
      "epoch": 6.413472706155633,
      "grad_norm": 27.758459091186523,
      "learning_rate": 3.9850303264937415e-06,
      "loss": 0.5509,
      "step": 16566
    },
    {
      "epoch": 6.413859852884243,
      "grad_norm": 44.53437805175781,
      "learning_rate": 3.984600163461952e-06,
      "loss": 1.2662,
      "step": 16567
    },
    {
      "epoch": 6.414246999612853,
      "grad_norm": 28.56446075439453,
      "learning_rate": 3.9841700004301635e-06,
      "loss": 2.565,
      "step": 16568
    },
    {
      "epoch": 6.414634146341464,
      "grad_norm": 61.70379638671875,
      "learning_rate": 3.983739837398374e-06,
      "loss": 1.7202,
      "step": 16569
    },
    {
      "epoch": 6.415021293070073,
      "grad_norm": 78.76542663574219,
      "learning_rate": 3.983309674366585e-06,
      "loss": 1.9999,
      "step": 16570
    },
    {
      "epoch": 6.415408439798684,
      "grad_norm": 26.481311798095703,
      "learning_rate": 3.982879511334796e-06,
      "loss": 0.878,
      "step": 16571
    },
    {
      "epoch": 6.4157955865272935,
      "grad_norm": 6.633949279785156,
      "learning_rate": 3.982449348303007e-06,
      "loss": 0.1672,
      "step": 16572
    },
    {
      "epoch": 6.416182733255904,
      "grad_norm": 136.14682006835938,
      "learning_rate": 3.982019185271218e-06,
      "loss": 0.9574,
      "step": 16573
    },
    {
      "epoch": 6.416569879984515,
      "grad_norm": 1.9452197551727295,
      "learning_rate": 3.9815890222394294e-06,
      "loss": 0.0911,
      "step": 16574
    },
    {
      "epoch": 6.416957026713124,
      "grad_norm": 68.3101806640625,
      "learning_rate": 3.98115885920764e-06,
      "loss": 1.7488,
      "step": 16575
    },
    {
      "epoch": 6.417344173441735,
      "grad_norm": 33.25360870361328,
      "learning_rate": 3.9807286961758514e-06,
      "loss": 1.9477,
      "step": 16576
    },
    {
      "epoch": 6.417731320170344,
      "grad_norm": 26.468915939331055,
      "learning_rate": 3.980298533144062e-06,
      "loss": 0.3984,
      "step": 16577
    },
    {
      "epoch": 6.418118466898955,
      "grad_norm": 78.40107727050781,
      "learning_rate": 3.979868370112273e-06,
      "loss": 0.9154,
      "step": 16578
    },
    {
      "epoch": 6.418505613627564,
      "grad_norm": 17.302751541137695,
      "learning_rate": 3.979438207080484e-06,
      "loss": 0.8813,
      "step": 16579
    },
    {
      "epoch": 6.418892760356175,
      "grad_norm": 45.80519485473633,
      "learning_rate": 3.979008044048695e-06,
      "loss": 1.8689,
      "step": 16580
    },
    {
      "epoch": 6.4192799070847855,
      "grad_norm": 42.907474517822266,
      "learning_rate": 3.978577881016906e-06,
      "loss": 0.9145,
      "step": 16581
    },
    {
      "epoch": 6.419667053813395,
      "grad_norm": 9.374871253967285,
      "learning_rate": 3.9781477179851165e-06,
      "loss": 0.1026,
      "step": 16582
    },
    {
      "epoch": 6.420054200542006,
      "grad_norm": 4.69630765914917,
      "learning_rate": 3.977717554953328e-06,
      "loss": 0.1607,
      "step": 16583
    },
    {
      "epoch": 6.420441347270615,
      "grad_norm": 7.236364364624023,
      "learning_rate": 3.9772873919215385e-06,
      "loss": 0.1523,
      "step": 16584
    },
    {
      "epoch": 6.420828493999226,
      "grad_norm": 23.507566452026367,
      "learning_rate": 3.976857228889749e-06,
      "loss": 0.2202,
      "step": 16585
    },
    {
      "epoch": 6.421215640727836,
      "grad_norm": 149.8948516845703,
      "learning_rate": 3.9764270658579605e-06,
      "loss": 1.9866,
      "step": 16586
    },
    {
      "epoch": 6.421602787456446,
      "grad_norm": 177.13150024414062,
      "learning_rate": 3.975996902826171e-06,
      "loss": 2.1381,
      "step": 16587
    },
    {
      "epoch": 6.421989934185056,
      "grad_norm": 10.16452407836914,
      "learning_rate": 3.9755667397943825e-06,
      "loss": 0.3238,
      "step": 16588
    },
    {
      "epoch": 6.422377080913666,
      "grad_norm": 17.791330337524414,
      "learning_rate": 3.975136576762593e-06,
      "loss": 1.5441,
      "step": 16589
    },
    {
      "epoch": 6.4227642276422765,
      "grad_norm": 51.240901947021484,
      "learning_rate": 3.9747064137308045e-06,
      "loss": 2.2354,
      "step": 16590
    },
    {
      "epoch": 6.423151374370887,
      "grad_norm": 10.06203556060791,
      "learning_rate": 3.974276250699015e-06,
      "loss": 0.1334,
      "step": 16591
    },
    {
      "epoch": 6.423538521099497,
      "grad_norm": 45.790828704833984,
      "learning_rate": 3.9738460876672265e-06,
      "loss": 4.4362,
      "step": 16592
    },
    {
      "epoch": 6.423925667828107,
      "grad_norm": 3.6369423866271973,
      "learning_rate": 3.973415924635437e-06,
      "loss": 0.1049,
      "step": 16593
    },
    {
      "epoch": 6.424312814556717,
      "grad_norm": 100.58675384521484,
      "learning_rate": 3.9729857616036484e-06,
      "loss": 2.7513,
      "step": 16594
    },
    {
      "epoch": 6.424699961285327,
      "grad_norm": 20.359373092651367,
      "learning_rate": 3.972555598571859e-06,
      "loss": 0.4519,
      "step": 16595
    },
    {
      "epoch": 6.425087108013937,
      "grad_norm": 20.1676025390625,
      "learning_rate": 3.97212543554007e-06,
      "loss": 0.3789,
      "step": 16596
    },
    {
      "epoch": 6.425474254742547,
      "grad_norm": 46.38084411621094,
      "learning_rate": 3.971695272508281e-06,
      "loss": 1.3312,
      "step": 16597
    },
    {
      "epoch": 6.425861401471158,
      "grad_norm": 57.88697814941406,
      "learning_rate": 3.971265109476492e-06,
      "loss": 1.4236,
      "step": 16598
    },
    {
      "epoch": 6.4262485481997675,
      "grad_norm": 89.46482849121094,
      "learning_rate": 3.970834946444703e-06,
      "loss": 1.4771,
      "step": 16599
    },
    {
      "epoch": 6.426635694928378,
      "grad_norm": 75.12574005126953,
      "learning_rate": 3.9704047834129135e-06,
      "loss": 0.7144,
      "step": 16600
    },
    {
      "epoch": 6.427022841656988,
      "grad_norm": 40.094520568847656,
      "learning_rate": 3.969974620381125e-06,
      "loss": 0.3472,
      "step": 16601
    },
    {
      "epoch": 6.427409988385598,
      "grad_norm": 57.72454833984375,
      "learning_rate": 3.9695444573493355e-06,
      "loss": 1.2323,
      "step": 16602
    },
    {
      "epoch": 6.427797135114209,
      "grad_norm": 19.34400177001953,
      "learning_rate": 3.969114294317546e-06,
      "loss": 0.2849,
      "step": 16603
    },
    {
      "epoch": 6.428184281842818,
      "grad_norm": 131.39657592773438,
      "learning_rate": 3.9686841312857575e-06,
      "loss": 0.8107,
      "step": 16604
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 110.92405700683594,
      "learning_rate": 3.968253968253968e-06,
      "loss": 0.5566,
      "step": 16605
    },
    {
      "epoch": 6.4289585753000384,
      "grad_norm": 4.885941505432129,
      "learning_rate": 3.9678238052221795e-06,
      "loss": 0.1909,
      "step": 16606
    },
    {
      "epoch": 6.429345722028649,
      "grad_norm": 6.897287368774414,
      "learning_rate": 3.96739364219039e-06,
      "loss": 0.2434,
      "step": 16607
    },
    {
      "epoch": 6.4297328687572595,
      "grad_norm": 8.230443954467773,
      "learning_rate": 3.9669634791586015e-06,
      "loss": 0.258,
      "step": 16608
    },
    {
      "epoch": 6.430120015485869,
      "grad_norm": 165.08009338378906,
      "learning_rate": 3.966533316126812e-06,
      "loss": 2.7819,
      "step": 16609
    },
    {
      "epoch": 6.43050716221448,
      "grad_norm": 3.5796453952789307,
      "learning_rate": 3.9661031530950235e-06,
      "loss": 0.1595,
      "step": 16610
    },
    {
      "epoch": 6.430894308943089,
      "grad_norm": 38.06316375732422,
      "learning_rate": 3.965672990063234e-06,
      "loss": 0.3866,
      "step": 16611
    },
    {
      "epoch": 6.4312814556717,
      "grad_norm": 84.2591323852539,
      "learning_rate": 3.9652428270314455e-06,
      "loss": 2.8186,
      "step": 16612
    },
    {
      "epoch": 6.431668602400309,
      "grad_norm": 2.64400315284729,
      "learning_rate": 3.964812663999656e-06,
      "loss": 0.0804,
      "step": 16613
    },
    {
      "epoch": 6.43205574912892,
      "grad_norm": 78.22985076904297,
      "learning_rate": 3.9643825009678674e-06,
      "loss": 2.1167,
      "step": 16614
    },
    {
      "epoch": 6.43244289585753,
      "grad_norm": 34.9721794128418,
      "learning_rate": 3.963952337936078e-06,
      "loss": 1.6297,
      "step": 16615
    },
    {
      "epoch": 6.43283004258614,
      "grad_norm": 51.36152267456055,
      "learning_rate": 3.963522174904289e-06,
      "loss": 3.474,
      "step": 16616
    },
    {
      "epoch": 6.4332171893147505,
      "grad_norm": 60.516319274902344,
      "learning_rate": 3.9630920118725e-06,
      "loss": 3.0101,
      "step": 16617
    },
    {
      "epoch": 6.43360433604336,
      "grad_norm": 33.356014251708984,
      "learning_rate": 3.9626618488407106e-06,
      "loss": 0.3754,
      "step": 16618
    },
    {
      "epoch": 6.433991482771971,
      "grad_norm": 4.30477237701416,
      "learning_rate": 3.962231685808922e-06,
      "loss": 0.1995,
      "step": 16619
    },
    {
      "epoch": 6.43437862950058,
      "grad_norm": 154.94183349609375,
      "learning_rate": 3.9618015227771325e-06,
      "loss": 1.1305,
      "step": 16620
    },
    {
      "epoch": 6.434765776229191,
      "grad_norm": 17.51529884338379,
      "learning_rate": 3.961371359745344e-06,
      "loss": 0.4445,
      "step": 16621
    },
    {
      "epoch": 6.435152922957801,
      "grad_norm": 4.617576599121094,
      "learning_rate": 3.9609411967135545e-06,
      "loss": 0.2153,
      "step": 16622
    },
    {
      "epoch": 6.435540069686411,
      "grad_norm": 46.98491668701172,
      "learning_rate": 3.960511033681766e-06,
      "loss": 1.8076,
      "step": 16623
    },
    {
      "epoch": 6.435927216415021,
      "grad_norm": 59.99580764770508,
      "learning_rate": 3.9600808706499765e-06,
      "loss": 1.3596,
      "step": 16624
    },
    {
      "epoch": 6.436314363143631,
      "grad_norm": 53.9073371887207,
      "learning_rate": 3.959650707618188e-06,
      "loss": 1.7165,
      "step": 16625
    },
    {
      "epoch": 6.4367015098722415,
      "grad_norm": 20.275894165039062,
      "learning_rate": 3.9592205445863985e-06,
      "loss": 1.7317,
      "step": 16626
    },
    {
      "epoch": 6.437088656600852,
      "grad_norm": 95.8057632446289,
      "learning_rate": 3.95879038155461e-06,
      "loss": 0.8609,
      "step": 16627
    },
    {
      "epoch": 6.437475803329462,
      "grad_norm": 56.91713333129883,
      "learning_rate": 3.9583602185228205e-06,
      "loss": 1.2443,
      "step": 16628
    },
    {
      "epoch": 6.437862950058072,
      "grad_norm": 80.44127655029297,
      "learning_rate": 3.957930055491032e-06,
      "loss": 0.3927,
      "step": 16629
    },
    {
      "epoch": 6.438250096786682,
      "grad_norm": 45.10319519042969,
      "learning_rate": 3.9574998924592425e-06,
      "loss": 2.8451,
      "step": 16630
    },
    {
      "epoch": 6.438637243515292,
      "grad_norm": 45.43196105957031,
      "learning_rate": 3.957069729427454e-06,
      "loss": 1.9299,
      "step": 16631
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 56.35736083984375,
      "learning_rate": 3.9566395663956644e-06,
      "loss": 1.1506,
      "step": 16632
    },
    {
      "epoch": 6.4394115369725125,
      "grad_norm": 122.70932006835938,
      "learning_rate": 3.956209403363875e-06,
      "loss": 0.8252,
      "step": 16633
    },
    {
      "epoch": 6.439798683701123,
      "grad_norm": 69.56637573242188,
      "learning_rate": 3.9557792403320864e-06,
      "loss": 0.5461,
      "step": 16634
    },
    {
      "epoch": 6.440185830429733,
      "grad_norm": 1.348119854927063,
      "learning_rate": 3.955349077300297e-06,
      "loss": 0.0461,
      "step": 16635
    },
    {
      "epoch": 6.440572977158343,
      "grad_norm": 16.014005661010742,
      "learning_rate": 3.9549189142685076e-06,
      "loss": 1.2471,
      "step": 16636
    },
    {
      "epoch": 6.440960123886953,
      "grad_norm": 7.6188435554504395,
      "learning_rate": 3.954488751236719e-06,
      "loss": 0.295,
      "step": 16637
    },
    {
      "epoch": 6.441347270615563,
      "grad_norm": 33.33568572998047,
      "learning_rate": 3.9540585882049296e-06,
      "loss": 0.5912,
      "step": 16638
    },
    {
      "epoch": 6.441734417344174,
      "grad_norm": 28.218603134155273,
      "learning_rate": 3.953628425173141e-06,
      "loss": 1.6708,
      "step": 16639
    },
    {
      "epoch": 6.442121564072783,
      "grad_norm": 25.043773651123047,
      "learning_rate": 3.9531982621413515e-06,
      "loss": 1.0874,
      "step": 16640
    },
    {
      "epoch": 6.442508710801394,
      "grad_norm": 29.082002639770508,
      "learning_rate": 3.952768099109563e-06,
      "loss": 0.6199,
      "step": 16641
    },
    {
      "epoch": 6.4428958575300035,
      "grad_norm": 65.27921295166016,
      "learning_rate": 3.9523379360777735e-06,
      "loss": 2.8172,
      "step": 16642
    },
    {
      "epoch": 6.443283004258614,
      "grad_norm": 40.69182205200195,
      "learning_rate": 3.951907773045985e-06,
      "loss": 1.9709,
      "step": 16643
    },
    {
      "epoch": 6.4436701509872245,
      "grad_norm": 44.87008285522461,
      "learning_rate": 3.9514776100141955e-06,
      "loss": 1.6553,
      "step": 16644
    },
    {
      "epoch": 6.444057297715834,
      "grad_norm": 19.083114624023438,
      "learning_rate": 3.951047446982407e-06,
      "loss": 1.3186,
      "step": 16645
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 25.92132568359375,
      "learning_rate": 3.9506172839506175e-06,
      "loss": 1.8298,
      "step": 16646
    },
    {
      "epoch": 6.444831591173054,
      "grad_norm": 29.591520309448242,
      "learning_rate": 3.950187120918829e-06,
      "loss": 1.107,
      "step": 16647
    },
    {
      "epoch": 6.445218737901665,
      "grad_norm": 29.423912048339844,
      "learning_rate": 3.9497569578870395e-06,
      "loss": 1.3795,
      "step": 16648
    },
    {
      "epoch": 6.445605884630275,
      "grad_norm": 4.729371070861816,
      "learning_rate": 3.949326794855251e-06,
      "loss": 0.2015,
      "step": 16649
    },
    {
      "epoch": 6.445993031358885,
      "grad_norm": 19.341331481933594,
      "learning_rate": 3.9488966318234615e-06,
      "loss": 0.1104,
      "step": 16650
    },
    {
      "epoch": 6.446380178087495,
      "grad_norm": 22.63774299621582,
      "learning_rate": 3.948466468791672e-06,
      "loss": 1.426,
      "step": 16651
    },
    {
      "epoch": 6.446767324816105,
      "grad_norm": 9.699005126953125,
      "learning_rate": 3.9480363057598834e-06,
      "loss": 0.3373,
      "step": 16652
    },
    {
      "epoch": 6.4471544715447155,
      "grad_norm": 83.44664764404297,
      "learning_rate": 3.947606142728094e-06,
      "loss": 0.4353,
      "step": 16653
    },
    {
      "epoch": 6.447541618273325,
      "grad_norm": 2.4530487060546875,
      "learning_rate": 3.947175979696305e-06,
      "loss": 0.1021,
      "step": 16654
    },
    {
      "epoch": 6.447928765001936,
      "grad_norm": 62.16608428955078,
      "learning_rate": 3.946745816664516e-06,
      "loss": 1.3492,
      "step": 16655
    },
    {
      "epoch": 6.448315911730546,
      "grad_norm": 61.143497467041016,
      "learning_rate": 3.946315653632727e-06,
      "loss": 1.4398,
      "step": 16656
    },
    {
      "epoch": 6.448703058459156,
      "grad_norm": 11.531622886657715,
      "learning_rate": 3.945885490600938e-06,
      "loss": 0.1553,
      "step": 16657
    },
    {
      "epoch": 6.449090205187766,
      "grad_norm": 46.333499908447266,
      "learning_rate": 3.945455327569149e-06,
      "loss": 1.419,
      "step": 16658
    },
    {
      "epoch": 6.449477351916376,
      "grad_norm": 52.11561584472656,
      "learning_rate": 3.94502516453736e-06,
      "loss": 0.607,
      "step": 16659
    },
    {
      "epoch": 6.4498644986449865,
      "grad_norm": 49.589176177978516,
      "learning_rate": 3.944595001505571e-06,
      "loss": 1.9536,
      "step": 16660
    },
    {
      "epoch": 6.450251645373597,
      "grad_norm": 102.43608856201172,
      "learning_rate": 3.944164838473782e-06,
      "loss": 1.5753,
      "step": 16661
    },
    {
      "epoch": 6.450638792102207,
      "grad_norm": 30.459941864013672,
      "learning_rate": 3.943734675441993e-06,
      "loss": 1.2544,
      "step": 16662
    },
    {
      "epoch": 6.451025938830817,
      "grad_norm": 132.62843322753906,
      "learning_rate": 3.943304512410204e-06,
      "loss": 0.7772,
      "step": 16663
    },
    {
      "epoch": 6.451413085559427,
      "grad_norm": 92.2394790649414,
      "learning_rate": 3.942874349378415e-06,
      "loss": 3.6367,
      "step": 16664
    },
    {
      "epoch": 6.451800232288037,
      "grad_norm": 141.98851013183594,
      "learning_rate": 3.942444186346626e-06,
      "loss": 0.8272,
      "step": 16665
    },
    {
      "epoch": 6.452187379016648,
      "grad_norm": 119.20719909667969,
      "learning_rate": 3.9420140233148365e-06,
      "loss": 1.4216,
      "step": 16666
    },
    {
      "epoch": 6.452574525745257,
      "grad_norm": 50.45307540893555,
      "learning_rate": 3.941583860283048e-06,
      "loss": 1.8795,
      "step": 16667
    },
    {
      "epoch": 6.452961672473868,
      "grad_norm": 17.812095642089844,
      "learning_rate": 3.9411536972512585e-06,
      "loss": 1.909,
      "step": 16668
    },
    {
      "epoch": 6.4533488192024775,
      "grad_norm": 10.890395164489746,
      "learning_rate": 3.940723534219469e-06,
      "loss": 0.1832,
      "step": 16669
    },
    {
      "epoch": 6.453735965931088,
      "grad_norm": 50.516754150390625,
      "learning_rate": 3.9402933711876805e-06,
      "loss": 3.1209,
      "step": 16670
    },
    {
      "epoch": 6.454123112659698,
      "grad_norm": 31.155757904052734,
      "learning_rate": 3.939863208155891e-06,
      "loss": 1.7506,
      "step": 16671
    },
    {
      "epoch": 6.454510259388308,
      "grad_norm": 62.049171447753906,
      "learning_rate": 3.9394330451241024e-06,
      "loss": 3.8336,
      "step": 16672
    },
    {
      "epoch": 6.454897406116919,
      "grad_norm": 32.674747467041016,
      "learning_rate": 3.939002882092313e-06,
      "loss": 0.4221,
      "step": 16673
    },
    {
      "epoch": 6.455284552845528,
      "grad_norm": 178.31797790527344,
      "learning_rate": 3.9385727190605244e-06,
      "loss": 0.96,
      "step": 16674
    },
    {
      "epoch": 6.455671699574139,
      "grad_norm": 25.239330291748047,
      "learning_rate": 3.938142556028735e-06,
      "loss": 0.37,
      "step": 16675
    },
    {
      "epoch": 6.456058846302748,
      "grad_norm": 9.40837287902832,
      "learning_rate": 3.937712392996946e-06,
      "loss": 0.3775,
      "step": 16676
    },
    {
      "epoch": 6.456445993031359,
      "grad_norm": 49.37171173095703,
      "learning_rate": 3.937282229965157e-06,
      "loss": 0.5522,
      "step": 16677
    },
    {
      "epoch": 6.456833139759969,
      "grad_norm": 89.199462890625,
      "learning_rate": 3.936852066933368e-06,
      "loss": 2.4461,
      "step": 16678
    },
    {
      "epoch": 6.457220286488579,
      "grad_norm": 53.307823181152344,
      "learning_rate": 3.936421903901579e-06,
      "loss": 3.0687,
      "step": 16679
    },
    {
      "epoch": 6.4576074332171896,
      "grad_norm": 85.30248260498047,
      "learning_rate": 3.93599174086979e-06,
      "loss": 3.4867,
      "step": 16680
    },
    {
      "epoch": 6.457994579945799,
      "grad_norm": 43.374786376953125,
      "learning_rate": 3.935561577838001e-06,
      "loss": 0.4232,
      "step": 16681
    },
    {
      "epoch": 6.45838172667441,
      "grad_norm": 107.70519256591797,
      "learning_rate": 3.935131414806212e-06,
      "loss": 0.7394,
      "step": 16682
    },
    {
      "epoch": 6.45876887340302,
      "grad_norm": 44.005592346191406,
      "learning_rate": 3.934701251774423e-06,
      "loss": 1.2184,
      "step": 16683
    },
    {
      "epoch": 6.45915602013163,
      "grad_norm": 108.54393005371094,
      "learning_rate": 3.9342710887426335e-06,
      "loss": 1.5657,
      "step": 16684
    },
    {
      "epoch": 6.45954316686024,
      "grad_norm": 81.57831573486328,
      "learning_rate": 3.933840925710845e-06,
      "loss": 3.4549,
      "step": 16685
    },
    {
      "epoch": 6.45993031358885,
      "grad_norm": 24.56281852722168,
      "learning_rate": 3.9334107626790555e-06,
      "loss": 1.5342,
      "step": 16686
    },
    {
      "epoch": 6.4603174603174605,
      "grad_norm": 114.67996215820312,
      "learning_rate": 3.932980599647266e-06,
      "loss": 0.583,
      "step": 16687
    },
    {
      "epoch": 6.46070460704607,
      "grad_norm": 47.97976303100586,
      "learning_rate": 3.9325504366154775e-06,
      "loss": 0.8725,
      "step": 16688
    },
    {
      "epoch": 6.461091753774681,
      "grad_norm": 9.992048263549805,
      "learning_rate": 3.932120273583688e-06,
      "loss": 0.1659,
      "step": 16689
    },
    {
      "epoch": 6.461478900503291,
      "grad_norm": 22.029129028320312,
      "learning_rate": 3.9316901105518995e-06,
      "loss": 1.234,
      "step": 16690
    },
    {
      "epoch": 6.461866047231901,
      "grad_norm": 83.30427551269531,
      "learning_rate": 3.93125994752011e-06,
      "loss": 0.8493,
      "step": 16691
    },
    {
      "epoch": 6.462253193960511,
      "grad_norm": 62.5548210144043,
      "learning_rate": 3.9308297844883214e-06,
      "loss": 1.0192,
      "step": 16692
    },
    {
      "epoch": 6.462640340689121,
      "grad_norm": 6.13209342956543,
      "learning_rate": 3.930399621456532e-06,
      "loss": 0.2236,
      "step": 16693
    },
    {
      "epoch": 6.463027487417731,
      "grad_norm": 66.0074691772461,
      "learning_rate": 3.9299694584247434e-06,
      "loss": 0.2771,
      "step": 16694
    },
    {
      "epoch": 6.463414634146342,
      "grad_norm": 64.86366271972656,
      "learning_rate": 3.929539295392954e-06,
      "loss": 1.9353,
      "step": 16695
    },
    {
      "epoch": 6.4638017808749515,
      "grad_norm": 56.56036376953125,
      "learning_rate": 3.929109132361165e-06,
      "loss": 0.286,
      "step": 16696
    },
    {
      "epoch": 6.464188927603562,
      "grad_norm": 10.157875061035156,
      "learning_rate": 3.928678969329377e-06,
      "loss": 0.1871,
      "step": 16697
    },
    {
      "epoch": 6.464576074332172,
      "grad_norm": 80.5477294921875,
      "learning_rate": 3.928248806297587e-06,
      "loss": 2.7679,
      "step": 16698
    },
    {
      "epoch": 6.464963221060782,
      "grad_norm": 87.16950988769531,
      "learning_rate": 3.927818643265798e-06,
      "loss": 0.3347,
      "step": 16699
    },
    {
      "epoch": 6.465350367789393,
      "grad_norm": 161.78268432617188,
      "learning_rate": 3.927388480234009e-06,
      "loss": 1.2204,
      "step": 16700
    },
    {
      "epoch": 6.465737514518002,
      "grad_norm": 111.11609649658203,
      "learning_rate": 3.92695831720222e-06,
      "loss": 0.5504,
      "step": 16701
    },
    {
      "epoch": 6.466124661246613,
      "grad_norm": 10.262077331542969,
      "learning_rate": 3.9265281541704305e-06,
      "loss": 0.2195,
      "step": 16702
    },
    {
      "epoch": 6.466511807975222,
      "grad_norm": 48.91183853149414,
      "learning_rate": 3.926097991138642e-06,
      "loss": 1.1762,
      "step": 16703
    },
    {
      "epoch": 6.466898954703833,
      "grad_norm": 161.59518432617188,
      "learning_rate": 3.9256678281068525e-06,
      "loss": 1.1123,
      "step": 16704
    },
    {
      "epoch": 6.4672861014324425,
      "grad_norm": 221.79786682128906,
      "learning_rate": 3.925237665075064e-06,
      "loss": 3.763,
      "step": 16705
    },
    {
      "epoch": 6.467673248161053,
      "grad_norm": 84.83345031738281,
      "learning_rate": 3.9248075020432745e-06,
      "loss": 2.8996,
      "step": 16706
    },
    {
      "epoch": 6.4680603948896636,
      "grad_norm": 3.2884128093719482,
      "learning_rate": 3.924377339011486e-06,
      "loss": 0.0976,
      "step": 16707
    },
    {
      "epoch": 6.468447541618273,
      "grad_norm": 3.3714122772216797,
      "learning_rate": 3.9239471759796965e-06,
      "loss": 0.1668,
      "step": 16708
    },
    {
      "epoch": 6.468834688346884,
      "grad_norm": 21.55080223083496,
      "learning_rate": 3.923517012947908e-06,
      "loss": 1.5924,
      "step": 16709
    },
    {
      "epoch": 6.469221835075493,
      "grad_norm": 178.14625549316406,
      "learning_rate": 3.9230868499161185e-06,
      "loss": 1.0276,
      "step": 16710
    },
    {
      "epoch": 6.469608981804104,
      "grad_norm": 14.964271545410156,
      "learning_rate": 3.92265668688433e-06,
      "loss": 0.212,
      "step": 16711
    },
    {
      "epoch": 6.469996128532713,
      "grad_norm": 35.6827392578125,
      "learning_rate": 3.9222265238525404e-06,
      "loss": 1.5399,
      "step": 16712
    },
    {
      "epoch": 6.470383275261324,
      "grad_norm": 3.6144590377807617,
      "learning_rate": 3.921796360820752e-06,
      "loss": 0.251,
      "step": 16713
    },
    {
      "epoch": 6.4707704219899345,
      "grad_norm": 150.42767333984375,
      "learning_rate": 3.921366197788962e-06,
      "loss": 0.6111,
      "step": 16714
    },
    {
      "epoch": 6.471157568718544,
      "grad_norm": 58.510398864746094,
      "learning_rate": 3.920936034757174e-06,
      "loss": 1.611,
      "step": 16715
    },
    {
      "epoch": 6.471544715447155,
      "grad_norm": 13.058052062988281,
      "learning_rate": 3.920505871725384e-06,
      "loss": 0.1483,
      "step": 16716
    },
    {
      "epoch": 6.471931862175764,
      "grad_norm": 33.037803649902344,
      "learning_rate": 3.920075708693595e-06,
      "loss": 2.6004,
      "step": 16717
    },
    {
      "epoch": 6.472319008904375,
      "grad_norm": 7.29180383682251,
      "learning_rate": 3.919645545661806e-06,
      "loss": 0.2435,
      "step": 16718
    },
    {
      "epoch": 6.472706155632985,
      "grad_norm": 6.766960620880127,
      "learning_rate": 3.919215382630017e-06,
      "loss": 0.1993,
      "step": 16719
    },
    {
      "epoch": 6.473093302361595,
      "grad_norm": 49.609989166259766,
      "learning_rate": 3.9187852195982275e-06,
      "loss": 2.9883,
      "step": 16720
    },
    {
      "epoch": 6.473480449090205,
      "grad_norm": 27.36806869506836,
      "learning_rate": 3.918355056566439e-06,
      "loss": 1.8155,
      "step": 16721
    },
    {
      "epoch": 6.473867595818815,
      "grad_norm": 4.9114885330200195,
      "learning_rate": 3.9179248935346495e-06,
      "loss": 0.1059,
      "step": 16722
    },
    {
      "epoch": 6.4742547425474255,
      "grad_norm": 12.04177474975586,
      "learning_rate": 3.917494730502861e-06,
      "loss": 0.2714,
      "step": 16723
    },
    {
      "epoch": 6.474641889276036,
      "grad_norm": 79.687255859375,
      "learning_rate": 3.9170645674710715e-06,
      "loss": 1.8052,
      "step": 16724
    },
    {
      "epoch": 6.475029036004646,
      "grad_norm": 3.365607976913452,
      "learning_rate": 3.916634404439283e-06,
      "loss": 0.0979,
      "step": 16725
    },
    {
      "epoch": 6.475416182733256,
      "grad_norm": 62.943016052246094,
      "learning_rate": 3.9162042414074935e-06,
      "loss": 3.0314,
      "step": 16726
    },
    {
      "epoch": 6.475803329461866,
      "grad_norm": 53.8971061706543,
      "learning_rate": 3.915774078375705e-06,
      "loss": 0.925,
      "step": 16727
    },
    {
      "epoch": 6.476190476190476,
      "grad_norm": 4.740561485290527,
      "learning_rate": 3.9153439153439155e-06,
      "loss": 0.2241,
      "step": 16728
    },
    {
      "epoch": 6.476577622919086,
      "grad_norm": 76.84601593017578,
      "learning_rate": 3.914913752312127e-06,
      "loss": 1.2804,
      "step": 16729
    },
    {
      "epoch": 6.476964769647696,
      "grad_norm": 19.287885665893555,
      "learning_rate": 3.9144835892803375e-06,
      "loss": 1.6257,
      "step": 16730
    },
    {
      "epoch": 6.477351916376307,
      "grad_norm": 20.275882720947266,
      "learning_rate": 3.914053426248549e-06,
      "loss": 0.2523,
      "step": 16731
    },
    {
      "epoch": 6.4777390631049165,
      "grad_norm": 8.48652458190918,
      "learning_rate": 3.9136232632167594e-06,
      "loss": 0.3528,
      "step": 16732
    },
    {
      "epoch": 6.478126209833527,
      "grad_norm": 56.485984802246094,
      "learning_rate": 3.913193100184971e-06,
      "loss": 0.1037,
      "step": 16733
    },
    {
      "epoch": 6.478513356562137,
      "grad_norm": 6.608112335205078,
      "learning_rate": 3.912762937153181e-06,
      "loss": 0.1542,
      "step": 16734
    },
    {
      "epoch": 6.478900503290747,
      "grad_norm": 2.4349470138549805,
      "learning_rate": 3.912332774121392e-06,
      "loss": 0.1008,
      "step": 16735
    },
    {
      "epoch": 6.479287650019358,
      "grad_norm": 170.78826904296875,
      "learning_rate": 3.911902611089603e-06,
      "loss": 1.3894,
      "step": 16736
    },
    {
      "epoch": 6.479674796747967,
      "grad_norm": 87.23159790039062,
      "learning_rate": 3.911472448057814e-06,
      "loss": 0.475,
      "step": 16737
    },
    {
      "epoch": 6.480061943476578,
      "grad_norm": 8.451165199279785,
      "learning_rate": 3.911042285026025e-06,
      "loss": 0.1363,
      "step": 16738
    },
    {
      "epoch": 6.4804490902051874,
      "grad_norm": 142.40760803222656,
      "learning_rate": 3.910612121994236e-06,
      "loss": 1.6915,
      "step": 16739
    },
    {
      "epoch": 6.480836236933798,
      "grad_norm": 76.22339630126953,
      "learning_rate": 3.910181958962447e-06,
      "loss": 3.0526,
      "step": 16740
    },
    {
      "epoch": 6.4812233836624085,
      "grad_norm": 28.101280212402344,
      "learning_rate": 3.909751795930658e-06,
      "loss": 1.8985,
      "step": 16741
    },
    {
      "epoch": 6.481610530391018,
      "grad_norm": 97.70507049560547,
      "learning_rate": 3.909321632898869e-06,
      "loss": 0.9543,
      "step": 16742
    },
    {
      "epoch": 6.481997677119629,
      "grad_norm": 25.98193359375,
      "learning_rate": 3.90889146986708e-06,
      "loss": 1.3154,
      "step": 16743
    },
    {
      "epoch": 6.482384823848238,
      "grad_norm": 40.1254768371582,
      "learning_rate": 3.908461306835291e-06,
      "loss": 3.4793,
      "step": 16744
    },
    {
      "epoch": 6.482771970576849,
      "grad_norm": 15.711996078491211,
      "learning_rate": 3.908031143803502e-06,
      "loss": 0.2757,
      "step": 16745
    },
    {
      "epoch": 6.483159117305458,
      "grad_norm": 24.50459861755371,
      "learning_rate": 3.907600980771713e-06,
      "loss": 1.5092,
      "step": 16746
    },
    {
      "epoch": 6.483546264034069,
      "grad_norm": 3.775082588195801,
      "learning_rate": 3.907170817739924e-06,
      "loss": 0.256,
      "step": 16747
    },
    {
      "epoch": 6.483933410762679,
      "grad_norm": 3.733126640319824,
      "learning_rate": 3.906740654708135e-06,
      "loss": 0.1758,
      "step": 16748
    },
    {
      "epoch": 6.484320557491289,
      "grad_norm": 20.539897918701172,
      "learning_rate": 3.906310491676346e-06,
      "loss": 1.7932,
      "step": 16749
    },
    {
      "epoch": 6.4847077042198995,
      "grad_norm": 13.725704193115234,
      "learning_rate": 3.9058803286445564e-06,
      "loss": 0.4412,
      "step": 16750
    },
    {
      "epoch": 6.485094850948509,
      "grad_norm": 147.5629425048828,
      "learning_rate": 3.905450165612768e-06,
      "loss": 0.9175,
      "step": 16751
    },
    {
      "epoch": 6.48548199767712,
      "grad_norm": 77.940673828125,
      "learning_rate": 3.9050200025809784e-06,
      "loss": 1.9487,
      "step": 16752
    },
    {
      "epoch": 6.48586914440573,
      "grad_norm": 81.26876068115234,
      "learning_rate": 3.904589839549189e-06,
      "loss": 0.6261,
      "step": 16753
    },
    {
      "epoch": 6.48625629113434,
      "grad_norm": 35.37425994873047,
      "learning_rate": 3.9041596765174e-06,
      "loss": 1.6873,
      "step": 16754
    },
    {
      "epoch": 6.48664343786295,
      "grad_norm": 100.02684020996094,
      "learning_rate": 3.903729513485611e-06,
      "loss": 1.1963,
      "step": 16755
    },
    {
      "epoch": 6.48703058459156,
      "grad_norm": 46.44770431518555,
      "learning_rate": 3.903299350453822e-06,
      "loss": 0.1743,
      "step": 16756
    },
    {
      "epoch": 6.48741773132017,
      "grad_norm": 2.642862558364868,
      "learning_rate": 3.902869187422033e-06,
      "loss": 0.0713,
      "step": 16757
    },
    {
      "epoch": 6.487804878048781,
      "grad_norm": 2.5378475189208984,
      "learning_rate": 3.902439024390244e-06,
      "loss": 0.1372,
      "step": 16758
    },
    {
      "epoch": 6.4881920247773905,
      "grad_norm": 28.129072189331055,
      "learning_rate": 3.902008861358455e-06,
      "loss": 0.4032,
      "step": 16759
    },
    {
      "epoch": 6.488579171506001,
      "grad_norm": 105.5244140625,
      "learning_rate": 3.901578698326666e-06,
      "loss": 1.2872,
      "step": 16760
    },
    {
      "epoch": 6.488966318234611,
      "grad_norm": 12.203402519226074,
      "learning_rate": 3.901148535294877e-06,
      "loss": 0.207,
      "step": 16761
    },
    {
      "epoch": 6.489353464963221,
      "grad_norm": 30.973567962646484,
      "learning_rate": 3.900718372263088e-06,
      "loss": 0.1494,
      "step": 16762
    },
    {
      "epoch": 6.489740611691831,
      "grad_norm": 48.9723014831543,
      "learning_rate": 3.900288209231299e-06,
      "loss": 0.604,
      "step": 16763
    },
    {
      "epoch": 6.490127758420441,
      "grad_norm": 5.517734050750732,
      "learning_rate": 3.89985804619951e-06,
      "loss": 0.1846,
      "step": 16764
    },
    {
      "epoch": 6.490514905149052,
      "grad_norm": 50.51055908203125,
      "learning_rate": 3.899427883167721e-06,
      "loss": 0.2733,
      "step": 16765
    },
    {
      "epoch": 6.4909020518776614,
      "grad_norm": 23.18008041381836,
      "learning_rate": 3.898997720135932e-06,
      "loss": 1.5293,
      "step": 16766
    },
    {
      "epoch": 6.491289198606272,
      "grad_norm": 5.980047225952148,
      "learning_rate": 3.898567557104143e-06,
      "loss": 0.1845,
      "step": 16767
    },
    {
      "epoch": 6.491676345334882,
      "grad_norm": 2.459334135055542,
      "learning_rate": 3.8981373940723535e-06,
      "loss": 0.1172,
      "step": 16768
    },
    {
      "epoch": 6.492063492063492,
      "grad_norm": 11.714486122131348,
      "learning_rate": 3.897707231040565e-06,
      "loss": 0.2738,
      "step": 16769
    },
    {
      "epoch": 6.492450638792103,
      "grad_norm": 16.30752944946289,
      "learning_rate": 3.8972770680087754e-06,
      "loss": 0.3425,
      "step": 16770
    },
    {
      "epoch": 6.492837785520712,
      "grad_norm": 40.73613739013672,
      "learning_rate": 3.896846904976986e-06,
      "loss": 1.8293,
      "step": 16771
    },
    {
      "epoch": 6.493224932249323,
      "grad_norm": 4.276121616363525,
      "learning_rate": 3.8964167419451974e-06,
      "loss": 0.1083,
      "step": 16772
    },
    {
      "epoch": 6.493612078977932,
      "grad_norm": 49.43301010131836,
      "learning_rate": 3.895986578913408e-06,
      "loss": 0.6721,
      "step": 16773
    },
    {
      "epoch": 6.493999225706543,
      "grad_norm": 1.8506031036376953,
      "learning_rate": 3.895556415881619e-06,
      "loss": 0.0918,
      "step": 16774
    },
    {
      "epoch": 6.494386372435153,
      "grad_norm": 146.8451385498047,
      "learning_rate": 3.89512625284983e-06,
      "loss": 1.4821,
      "step": 16775
    },
    {
      "epoch": 6.494773519163763,
      "grad_norm": 48.12788772583008,
      "learning_rate": 3.894696089818041e-06,
      "loss": 2.8303,
      "step": 16776
    },
    {
      "epoch": 6.4951606658923735,
      "grad_norm": 3.891018867492676,
      "learning_rate": 3.894265926786252e-06,
      "loss": 0.1025,
      "step": 16777
    },
    {
      "epoch": 6.495547812620983,
      "grad_norm": 12.062602996826172,
      "learning_rate": 3.893835763754463e-06,
      "loss": 0.0812,
      "step": 16778
    },
    {
      "epoch": 6.495934959349594,
      "grad_norm": 12.321541786193848,
      "learning_rate": 3.893405600722675e-06,
      "loss": 0.3668,
      "step": 16779
    },
    {
      "epoch": 6.496322106078203,
      "grad_norm": 42.42012023925781,
      "learning_rate": 3.892975437690885e-06,
      "loss": 0.1587,
      "step": 16780
    },
    {
      "epoch": 6.496709252806814,
      "grad_norm": 121.00811004638672,
      "learning_rate": 3.892545274659097e-06,
      "loss": 1.9804,
      "step": 16781
    },
    {
      "epoch": 6.497096399535424,
      "grad_norm": 120.69017028808594,
      "learning_rate": 3.892115111627307e-06,
      "loss": 0.938,
      "step": 16782
    },
    {
      "epoch": 6.497483546264034,
      "grad_norm": 31.95501708984375,
      "learning_rate": 3.891684948595518e-06,
      "loss": 0.3722,
      "step": 16783
    },
    {
      "epoch": 6.497870692992644,
      "grad_norm": 49.63804626464844,
      "learning_rate": 3.891254785563729e-06,
      "loss": 0.328,
      "step": 16784
    },
    {
      "epoch": 6.498257839721254,
      "grad_norm": 44.03561782836914,
      "learning_rate": 3.89082462253194e-06,
      "loss": 1.4439,
      "step": 16785
    },
    {
      "epoch": 6.4986449864498645,
      "grad_norm": 67.78761291503906,
      "learning_rate": 3.8903944595001505e-06,
      "loss": 0.5853,
      "step": 16786
    },
    {
      "epoch": 6.499032133178475,
      "grad_norm": 86.50558471679688,
      "learning_rate": 3.889964296468362e-06,
      "loss": 0.7861,
      "step": 16787
    },
    {
      "epoch": 6.499419279907085,
      "grad_norm": 71.22673797607422,
      "learning_rate": 3.8895341334365725e-06,
      "loss": 1.1355,
      "step": 16788
    },
    {
      "epoch": 6.499806426635695,
      "grad_norm": 248.4732208251953,
      "learning_rate": 3.889103970404784e-06,
      "loss": 0.6108,
      "step": 16789
    },
    {
      "epoch": 6.500193573364305,
      "grad_norm": 38.08505630493164,
      "learning_rate": 3.8886738073729944e-06,
      "loss": 0.2197,
      "step": 16790
    },
    {
      "epoch": 6.500580720092915,
      "grad_norm": 7.29006290435791,
      "learning_rate": 3.888243644341206e-06,
      "loss": 0.1552,
      "step": 16791
    },
    {
      "epoch": 6.500967866821526,
      "grad_norm": 22.944881439208984,
      "learning_rate": 3.8878134813094164e-06,
      "loss": 0.1824,
      "step": 16792
    },
    {
      "epoch": 6.5013550135501355,
      "grad_norm": 75.25818634033203,
      "learning_rate": 3.887383318277628e-06,
      "loss": 4.1855,
      "step": 16793
    },
    {
      "epoch": 6.501742160278746,
      "grad_norm": 191.13632202148438,
      "learning_rate": 3.886953155245838e-06,
      "loss": 1.4543,
      "step": 16794
    },
    {
      "epoch": 6.502129307007356,
      "grad_norm": 55.4647331237793,
      "learning_rate": 3.88652299221405e-06,
      "loss": 0.5356,
      "step": 16795
    },
    {
      "epoch": 6.502516453735966,
      "grad_norm": 21.36655044555664,
      "learning_rate": 3.88609282918226e-06,
      "loss": 1.8802,
      "step": 16796
    },
    {
      "epoch": 6.502903600464576,
      "grad_norm": 23.8237361907959,
      "learning_rate": 3.885662666150472e-06,
      "loss": 1.988,
      "step": 16797
    },
    {
      "epoch": 6.503290747193186,
      "grad_norm": 67.11495971679688,
      "learning_rate": 3.885232503118682e-06,
      "loss": 2.2778,
      "step": 16798
    },
    {
      "epoch": 6.503677893921797,
      "grad_norm": 123.1499252319336,
      "learning_rate": 3.884802340086894e-06,
      "loss": 1.3207,
      "step": 16799
    },
    {
      "epoch": 6.504065040650406,
      "grad_norm": 57.74546813964844,
      "learning_rate": 3.884372177055104e-06,
      "loss": 0.6637,
      "step": 16800
    },
    {
      "epoch": 6.504452187379017,
      "grad_norm": 169.55885314941406,
      "learning_rate": 3.883942014023315e-06,
      "loss": 0.8176,
      "step": 16801
    },
    {
      "epoch": 6.5048393341076265,
      "grad_norm": 15.839720726013184,
      "learning_rate": 3.883511850991526e-06,
      "loss": 0.3774,
      "step": 16802
    },
    {
      "epoch": 6.505226480836237,
      "grad_norm": 32.87038040161133,
      "learning_rate": 3.883081687959737e-06,
      "loss": 1.7184,
      "step": 16803
    },
    {
      "epoch": 6.505613627564847,
      "grad_norm": 99.6502685546875,
      "learning_rate": 3.8826515249279475e-06,
      "loss": 0.5276,
      "step": 16804
    },
    {
      "epoch": 6.506000774293457,
      "grad_norm": 44.68525695800781,
      "learning_rate": 3.882221361896159e-06,
      "loss": 1.5048,
      "step": 16805
    },
    {
      "epoch": 6.506387921022068,
      "grad_norm": 9.821867942810059,
      "learning_rate": 3.8817911988643695e-06,
      "loss": 0.1399,
      "step": 16806
    },
    {
      "epoch": 6.506775067750677,
      "grad_norm": 6.5722479820251465,
      "learning_rate": 3.881361035832581e-06,
      "loss": 0.1908,
      "step": 16807
    },
    {
      "epoch": 6.507162214479288,
      "grad_norm": 118.12150573730469,
      "learning_rate": 3.8809308728007915e-06,
      "loss": 0.6763,
      "step": 16808
    },
    {
      "epoch": 6.507549361207898,
      "grad_norm": 261.0971984863281,
      "learning_rate": 3.880500709769003e-06,
      "loss": 1.2852,
      "step": 16809
    },
    {
      "epoch": 6.507936507936508,
      "grad_norm": 0.9177573919296265,
      "learning_rate": 3.8800705467372134e-06,
      "loss": 0.0168,
      "step": 16810
    },
    {
      "epoch": 6.508323654665118,
      "grad_norm": 71.71571350097656,
      "learning_rate": 3.879640383705425e-06,
      "loss": 1.8696,
      "step": 16811
    },
    {
      "epoch": 6.508710801393728,
      "grad_norm": 4.213743686676025,
      "learning_rate": 3.8792102206736354e-06,
      "loss": 0.0776,
      "step": 16812
    },
    {
      "epoch": 6.5090979481223386,
      "grad_norm": 38.944549560546875,
      "learning_rate": 3.878780057641847e-06,
      "loss": 1.6936,
      "step": 16813
    },
    {
      "epoch": 6.509485094850948,
      "grad_norm": 159.07110595703125,
      "learning_rate": 3.878349894610057e-06,
      "loss": 3.0194,
      "step": 16814
    },
    {
      "epoch": 6.509872241579559,
      "grad_norm": 2.968949556350708,
      "learning_rate": 3.877919731578269e-06,
      "loss": 0.1233,
      "step": 16815
    },
    {
      "epoch": 6.510259388308169,
      "grad_norm": 23.880420684814453,
      "learning_rate": 3.877489568546479e-06,
      "loss": 3.2222,
      "step": 16816
    },
    {
      "epoch": 6.510646535036779,
      "grad_norm": 9.177096366882324,
      "learning_rate": 3.877059405514691e-06,
      "loss": 0.2273,
      "step": 16817
    },
    {
      "epoch": 6.511033681765389,
      "grad_norm": 2.625491142272949,
      "learning_rate": 3.876629242482901e-06,
      "loss": 0.1352,
      "step": 16818
    },
    {
      "epoch": 6.511420828493999,
      "grad_norm": 20.74495506286621,
      "learning_rate": 3.876199079451112e-06,
      "loss": 1.4261,
      "step": 16819
    },
    {
      "epoch": 6.5118079752226095,
      "grad_norm": 1.6489580869674683,
      "learning_rate": 3.875768916419323e-06,
      "loss": 0.0588,
      "step": 16820
    },
    {
      "epoch": 6.512195121951219,
      "grad_norm": 35.94369888305664,
      "learning_rate": 3.875338753387534e-06,
      "loss": 0.8461,
      "step": 16821
    },
    {
      "epoch": 6.51258226867983,
      "grad_norm": 70.0607681274414,
      "learning_rate": 3.874908590355745e-06,
      "loss": 0.8371,
      "step": 16822
    },
    {
      "epoch": 6.51296941540844,
      "grad_norm": 196.017822265625,
      "learning_rate": 3.874478427323956e-06,
      "loss": 0.7637,
      "step": 16823
    },
    {
      "epoch": 6.51335656213705,
      "grad_norm": 81.54621887207031,
      "learning_rate": 3.874048264292167e-06,
      "loss": 1.2768,
      "step": 16824
    },
    {
      "epoch": 6.51374370886566,
      "grad_norm": 31.00656509399414,
      "learning_rate": 3.873618101260378e-06,
      "loss": 2.1753,
      "step": 16825
    },
    {
      "epoch": 6.51413085559427,
      "grad_norm": 93.42423248291016,
      "learning_rate": 3.873187938228589e-06,
      "loss": 3.2636,
      "step": 16826
    },
    {
      "epoch": 6.51451800232288,
      "grad_norm": 14.845305442810059,
      "learning_rate": 3.8727577751968e-06,
      "loss": 0.1063,
      "step": 16827
    },
    {
      "epoch": 6.514905149051491,
      "grad_norm": 60.87006378173828,
      "learning_rate": 3.872327612165011e-06,
      "loss": 2.175,
      "step": 16828
    },
    {
      "epoch": 6.5152922957801005,
      "grad_norm": 79.06655883789062,
      "learning_rate": 3.871897449133222e-06,
      "loss": 1.5714,
      "step": 16829
    },
    {
      "epoch": 6.515679442508711,
      "grad_norm": 38.180809020996094,
      "learning_rate": 3.871467286101433e-06,
      "loss": 0.2504,
      "step": 16830
    },
    {
      "epoch": 6.516066589237321,
      "grad_norm": 33.461421966552734,
      "learning_rate": 3.871037123069644e-06,
      "loss": 1.2395,
      "step": 16831
    },
    {
      "epoch": 6.516453735965931,
      "grad_norm": 31.77981185913086,
      "learning_rate": 3.870606960037855e-06,
      "loss": 1.0255,
      "step": 16832
    },
    {
      "epoch": 6.516840882694542,
      "grad_norm": 3.895660638809204,
      "learning_rate": 3.870176797006066e-06,
      "loss": 0.0917,
      "step": 16833
    },
    {
      "epoch": 6.517228029423151,
      "grad_norm": 95.28694152832031,
      "learning_rate": 3.869746633974276e-06,
      "loss": 0.9695,
      "step": 16834
    },
    {
      "epoch": 6.517615176151762,
      "grad_norm": 95.73438262939453,
      "learning_rate": 3.869316470942488e-06,
      "loss": 1.8955,
      "step": 16835
    },
    {
      "epoch": 6.518002322880371,
      "grad_norm": 12.09997272491455,
      "learning_rate": 3.868886307910698e-06,
      "loss": 0.1403,
      "step": 16836
    },
    {
      "epoch": 6.518389469608982,
      "grad_norm": 92.2616195678711,
      "learning_rate": 3.868456144878909e-06,
      "loss": 0.6252,
      "step": 16837
    },
    {
      "epoch": 6.5187766163375915,
      "grad_norm": 35.293846130371094,
      "learning_rate": 3.86802598184712e-06,
      "loss": 0.5007,
      "step": 16838
    },
    {
      "epoch": 6.519163763066202,
      "grad_norm": 103.98503875732422,
      "learning_rate": 3.867595818815331e-06,
      "loss": 0.3169,
      "step": 16839
    },
    {
      "epoch": 6.5195509097948126,
      "grad_norm": 83.31414794921875,
      "learning_rate": 3.867165655783542e-06,
      "loss": 1.7722,
      "step": 16840
    },
    {
      "epoch": 6.519938056523422,
      "grad_norm": 4.0670084953308105,
      "learning_rate": 3.866735492751753e-06,
      "loss": 0.1092,
      "step": 16841
    },
    {
      "epoch": 6.520325203252033,
      "grad_norm": 9.815908432006836,
      "learning_rate": 3.866305329719964e-06,
      "loss": 0.2691,
      "step": 16842
    },
    {
      "epoch": 6.520712349980642,
      "grad_norm": 1.580823302268982,
      "learning_rate": 3.865875166688175e-06,
      "loss": 0.0535,
      "step": 16843
    },
    {
      "epoch": 6.521099496709253,
      "grad_norm": 180.29672241210938,
      "learning_rate": 3.865445003656386e-06,
      "loss": 1.578,
      "step": 16844
    },
    {
      "epoch": 6.521486643437863,
      "grad_norm": 38.05303192138672,
      "learning_rate": 3.865014840624597e-06,
      "loss": 1.7541,
      "step": 16845
    },
    {
      "epoch": 6.521873790166473,
      "grad_norm": 22.943187713623047,
      "learning_rate": 3.864584677592808e-06,
      "loss": 0.2705,
      "step": 16846
    },
    {
      "epoch": 6.5222609368950835,
      "grad_norm": 7.356720447540283,
      "learning_rate": 3.864154514561019e-06,
      "loss": 0.0764,
      "step": 16847
    },
    {
      "epoch": 6.522648083623693,
      "grad_norm": 49.439239501953125,
      "learning_rate": 3.86372435152923e-06,
      "loss": 0.5902,
      "step": 16848
    },
    {
      "epoch": 6.523035230352304,
      "grad_norm": 4.971363067626953,
      "learning_rate": 3.863294188497441e-06,
      "loss": 0.1106,
      "step": 16849
    },
    {
      "epoch": 6.523422377080914,
      "grad_norm": 4.478330135345459,
      "learning_rate": 3.862864025465652e-06,
      "loss": 0.1546,
      "step": 16850
    },
    {
      "epoch": 6.523809523809524,
      "grad_norm": 87.90196228027344,
      "learning_rate": 3.862433862433863e-06,
      "loss": 1.3221,
      "step": 16851
    },
    {
      "epoch": 6.524196670538134,
      "grad_norm": 46.3437385559082,
      "learning_rate": 3.862003699402073e-06,
      "loss": 1.1705,
      "step": 16852
    },
    {
      "epoch": 6.524583817266744,
      "grad_norm": 69.10163116455078,
      "learning_rate": 3.861573536370285e-06,
      "loss": 1.4434,
      "step": 16853
    },
    {
      "epoch": 6.524970963995354,
      "grad_norm": 53.81128692626953,
      "learning_rate": 3.861143373338495e-06,
      "loss": 1.639,
      "step": 16854
    },
    {
      "epoch": 6.525358110723964,
      "grad_norm": 1.302708387374878,
      "learning_rate": 3.860713210306706e-06,
      "loss": 0.0459,
      "step": 16855
    },
    {
      "epoch": 6.5257452574525745,
      "grad_norm": 51.59466552734375,
      "learning_rate": 3.860283047274917e-06,
      "loss": 0.8668,
      "step": 16856
    },
    {
      "epoch": 6.526132404181185,
      "grad_norm": 29.800071716308594,
      "learning_rate": 3.859852884243128e-06,
      "loss": 1.4268,
      "step": 16857
    },
    {
      "epoch": 6.526519550909795,
      "grad_norm": 5.086474418640137,
      "learning_rate": 3.859422721211339e-06,
      "loss": 0.2447,
      "step": 16858
    },
    {
      "epoch": 6.526906697638405,
      "grad_norm": 2.857051134109497,
      "learning_rate": 3.85899255817955e-06,
      "loss": 0.139,
      "step": 16859
    },
    {
      "epoch": 6.527293844367015,
      "grad_norm": 1.8433202505111694,
      "learning_rate": 3.858562395147761e-06,
      "loss": 0.0751,
      "step": 16860
    },
    {
      "epoch": 6.527680991095625,
      "grad_norm": 148.0991668701172,
      "learning_rate": 3.858132232115973e-06,
      "loss": 2.3597,
      "step": 16861
    },
    {
      "epoch": 6.528068137824235,
      "grad_norm": 64.34876251220703,
      "learning_rate": 3.857702069084183e-06,
      "loss": 1.3028,
      "step": 16862
    },
    {
      "epoch": 6.528455284552845,
      "grad_norm": 27.687393188476562,
      "learning_rate": 3.857271906052395e-06,
      "loss": 1.048,
      "step": 16863
    },
    {
      "epoch": 6.528842431281456,
      "grad_norm": 20.855449676513672,
      "learning_rate": 3.856841743020605e-06,
      "loss": 0.4495,
      "step": 16864
    },
    {
      "epoch": 6.5292295780100655,
      "grad_norm": 17.669084548950195,
      "learning_rate": 3.856411579988817e-06,
      "loss": 0.1535,
      "step": 16865
    },
    {
      "epoch": 6.529616724738676,
      "grad_norm": 29.07000732421875,
      "learning_rate": 3.855981416957027e-06,
      "loss": 0.8137,
      "step": 16866
    },
    {
      "epoch": 6.530003871467287,
      "grad_norm": 2.5717012882232666,
      "learning_rate": 3.855551253925238e-06,
      "loss": 0.13,
      "step": 16867
    },
    {
      "epoch": 6.530391018195896,
      "grad_norm": 86.19427490234375,
      "learning_rate": 3.855121090893449e-06,
      "loss": 2.4639,
      "step": 16868
    },
    {
      "epoch": 6.530778164924507,
      "grad_norm": 44.78451156616211,
      "learning_rate": 3.85469092786166e-06,
      "loss": 2.168,
      "step": 16869
    },
    {
      "epoch": 6.531165311653116,
      "grad_norm": 2.122720241546631,
      "learning_rate": 3.8542607648298704e-06,
      "loss": 0.0959,
      "step": 16870
    },
    {
      "epoch": 6.531552458381727,
      "grad_norm": 0.55085688829422,
      "learning_rate": 3.853830601798082e-06,
      "loss": 0.0135,
      "step": 16871
    },
    {
      "epoch": 6.5319396051103364,
      "grad_norm": 93.86965942382812,
      "learning_rate": 3.853400438766292e-06,
      "loss": 0.2014,
      "step": 16872
    },
    {
      "epoch": 6.532326751838947,
      "grad_norm": 20.407167434692383,
      "learning_rate": 3.852970275734504e-06,
      "loss": 0.321,
      "step": 16873
    },
    {
      "epoch": 6.5327138985675575,
      "grad_norm": 58.440086364746094,
      "learning_rate": 3.852540112702714e-06,
      "loss": 0.479,
      "step": 16874
    },
    {
      "epoch": 6.533101045296167,
      "grad_norm": 50.434959411621094,
      "learning_rate": 3.852109949670926e-06,
      "loss": 1.0005,
      "step": 16875
    },
    {
      "epoch": 6.533488192024778,
      "grad_norm": 36.36882019042969,
      "learning_rate": 3.851679786639136e-06,
      "loss": 2.1379,
      "step": 16876
    },
    {
      "epoch": 6.533875338753387,
      "grad_norm": 24.78774070739746,
      "learning_rate": 3.851249623607348e-06,
      "loss": 1.5807,
      "step": 16877
    },
    {
      "epoch": 6.534262485481998,
      "grad_norm": 7.733453750610352,
      "learning_rate": 3.850819460575558e-06,
      "loss": 0.1907,
      "step": 16878
    },
    {
      "epoch": 6.534649632210607,
      "grad_norm": 223.5177764892578,
      "learning_rate": 3.85038929754377e-06,
      "loss": 0.4874,
      "step": 16879
    },
    {
      "epoch": 6.535036778939218,
      "grad_norm": 75.4996566772461,
      "learning_rate": 3.84995913451198e-06,
      "loss": 1.1969,
      "step": 16880
    },
    {
      "epoch": 6.535423925667828,
      "grad_norm": 140.25949096679688,
      "learning_rate": 3.849528971480192e-06,
      "loss": 2.6724,
      "step": 16881
    },
    {
      "epoch": 6.535811072396438,
      "grad_norm": 20.072715759277344,
      "learning_rate": 3.849098808448402e-06,
      "loss": 2.7599,
      "step": 16882
    },
    {
      "epoch": 6.5361982191250485,
      "grad_norm": 51.83286666870117,
      "learning_rate": 3.848668645416614e-06,
      "loss": 3.0664,
      "step": 16883
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 57.1431999206543,
      "learning_rate": 3.848238482384824e-06,
      "loss": 0.8988,
      "step": 16884
    },
    {
      "epoch": 6.536972512582269,
      "grad_norm": 71.11859130859375,
      "learning_rate": 3.847808319353035e-06,
      "loss": 2.1789,
      "step": 16885
    },
    {
      "epoch": 6.537359659310879,
      "grad_norm": 11.292304039001465,
      "learning_rate": 3.847378156321246e-06,
      "loss": 0.216,
      "step": 16886
    },
    {
      "epoch": 6.537746806039489,
      "grad_norm": 2.1349403858184814,
      "learning_rate": 3.846947993289457e-06,
      "loss": 0.0574,
      "step": 16887
    },
    {
      "epoch": 6.538133952768099,
      "grad_norm": 23.96710205078125,
      "learning_rate": 3.8465178302576674e-06,
      "loss": 0.1558,
      "step": 16888
    },
    {
      "epoch": 6.538521099496709,
      "grad_norm": 2.548156976699829,
      "learning_rate": 3.846087667225879e-06,
      "loss": 0.0805,
      "step": 16889
    },
    {
      "epoch": 6.538908246225319,
      "grad_norm": 130.02987670898438,
      "learning_rate": 3.8456575041940894e-06,
      "loss": 1.171,
      "step": 16890
    },
    {
      "epoch": 6.53929539295393,
      "grad_norm": 3.86195707321167,
      "learning_rate": 3.845227341162301e-06,
      "loss": 0.1264,
      "step": 16891
    },
    {
      "epoch": 6.5396825396825395,
      "grad_norm": 2.947146415710449,
      "learning_rate": 3.844797178130511e-06,
      "loss": 0.1143,
      "step": 16892
    },
    {
      "epoch": 6.54006968641115,
      "grad_norm": 33.18610763549805,
      "learning_rate": 3.844367015098723e-06,
      "loss": 0.218,
      "step": 16893
    },
    {
      "epoch": 6.54045683313976,
      "grad_norm": 10.169625282287598,
      "learning_rate": 3.843936852066933e-06,
      "loss": 0.3469,
      "step": 16894
    },
    {
      "epoch": 6.54084397986837,
      "grad_norm": 93.10063934326172,
      "learning_rate": 3.843506689035145e-06,
      "loss": 1.6981,
      "step": 16895
    },
    {
      "epoch": 6.54123112659698,
      "grad_norm": 38.08564758300781,
      "learning_rate": 3.843076526003355e-06,
      "loss": 0.2837,
      "step": 16896
    },
    {
      "epoch": 6.54161827332559,
      "grad_norm": 1.405949592590332,
      "learning_rate": 3.842646362971567e-06,
      "loss": 0.0525,
      "step": 16897
    },
    {
      "epoch": 6.542005420054201,
      "grad_norm": 45.127593994140625,
      "learning_rate": 3.842216199939777e-06,
      "loss": 1.8345,
      "step": 16898
    },
    {
      "epoch": 6.5423925667828104,
      "grad_norm": 5.756645679473877,
      "learning_rate": 3.841786036907989e-06,
      "loss": 0.1997,
      "step": 16899
    },
    {
      "epoch": 6.542779713511421,
      "grad_norm": 7.073272228240967,
      "learning_rate": 3.841355873876199e-06,
      "loss": 0.2395,
      "step": 16900
    },
    {
      "epoch": 6.5431668602400315,
      "grad_norm": 217.5654754638672,
      "learning_rate": 3.840925710844411e-06,
      "loss": 1.1225,
      "step": 16901
    },
    {
      "epoch": 6.543554006968641,
      "grad_norm": 46.732242584228516,
      "learning_rate": 3.840495547812621e-06,
      "loss": 1.2691,
      "step": 16902
    },
    {
      "epoch": 6.543941153697252,
      "grad_norm": 1.3500550985336304,
      "learning_rate": 3.840065384780832e-06,
      "loss": 0.0426,
      "step": 16903
    },
    {
      "epoch": 6.544328300425861,
      "grad_norm": 18.86652946472168,
      "learning_rate": 3.839635221749043e-06,
      "loss": 0.5955,
      "step": 16904
    },
    {
      "epoch": 6.544715447154472,
      "grad_norm": 4.858520984649658,
      "learning_rate": 3.839205058717254e-06,
      "loss": 0.0544,
      "step": 16905
    },
    {
      "epoch": 6.545102593883081,
      "grad_norm": 32.41841125488281,
      "learning_rate": 3.838774895685465e-06,
      "loss": 0.2244,
      "step": 16906
    },
    {
      "epoch": 6.545489740611692,
      "grad_norm": 66.2499008178711,
      "learning_rate": 3.838344732653676e-06,
      "loss": 0.4047,
      "step": 16907
    },
    {
      "epoch": 6.545876887340302,
      "grad_norm": 39.4930305480957,
      "learning_rate": 3.837914569621887e-06,
      "loss": 1.0255,
      "step": 16908
    },
    {
      "epoch": 6.546264034068912,
      "grad_norm": 56.24333572387695,
      "learning_rate": 3.837484406590098e-06,
      "loss": 2.0455,
      "step": 16909
    },
    {
      "epoch": 6.5466511807975225,
      "grad_norm": 39.53816223144531,
      "learning_rate": 3.837054243558309e-06,
      "loss": 3.5865,
      "step": 16910
    },
    {
      "epoch": 6.547038327526132,
      "grad_norm": 34.913421630859375,
      "learning_rate": 3.83662408052652e-06,
      "loss": 2.125,
      "step": 16911
    },
    {
      "epoch": 6.547425474254743,
      "grad_norm": 214.6145782470703,
      "learning_rate": 3.836193917494731e-06,
      "loss": 0.9304,
      "step": 16912
    },
    {
      "epoch": 6.547812620983352,
      "grad_norm": 13.23312759399414,
      "learning_rate": 3.835763754462942e-06,
      "loss": 0.1363,
      "step": 16913
    },
    {
      "epoch": 6.548199767711963,
      "grad_norm": 94.7500228881836,
      "learning_rate": 3.835333591431153e-06,
      "loss": 0.7965,
      "step": 16914
    },
    {
      "epoch": 6.548586914440573,
      "grad_norm": 18.820308685302734,
      "learning_rate": 3.834903428399364e-06,
      "loss": 0.3502,
      "step": 16915
    },
    {
      "epoch": 6.548974061169183,
      "grad_norm": 5.637850284576416,
      "learning_rate": 3.834473265367574e-06,
      "loss": 0.1198,
      "step": 16916
    },
    {
      "epoch": 6.549361207897793,
      "grad_norm": 38.97169494628906,
      "learning_rate": 3.834043102335786e-06,
      "loss": 0.3311,
      "step": 16917
    },
    {
      "epoch": 6.549748354626403,
      "grad_norm": 23.316314697265625,
      "learning_rate": 3.833612939303996e-06,
      "loss": 0.2317,
      "step": 16918
    },
    {
      "epoch": 6.5501355013550135,
      "grad_norm": 2.2031939029693604,
      "learning_rate": 3.833182776272208e-06,
      "loss": 0.1201,
      "step": 16919
    },
    {
      "epoch": 6.550522648083624,
      "grad_norm": 264.7539367675781,
      "learning_rate": 3.832752613240418e-06,
      "loss": 1.9339,
      "step": 16920
    },
    {
      "epoch": 6.550909794812234,
      "grad_norm": 26.204801559448242,
      "learning_rate": 3.832322450208629e-06,
      "loss": 0.5122,
      "step": 16921
    },
    {
      "epoch": 6.551296941540844,
      "grad_norm": 85.59925079345703,
      "learning_rate": 3.83189228717684e-06,
      "loss": 0.3918,
      "step": 16922
    },
    {
      "epoch": 6.551684088269454,
      "grad_norm": 27.12759017944336,
      "learning_rate": 3.831462124145051e-06,
      "loss": 1.2101,
      "step": 16923
    },
    {
      "epoch": 6.552071234998064,
      "grad_norm": 24.696687698364258,
      "learning_rate": 3.831031961113262e-06,
      "loss": 1.6925,
      "step": 16924
    },
    {
      "epoch": 6.552458381726675,
      "grad_norm": 49.96152114868164,
      "learning_rate": 3.830601798081473e-06,
      "loss": 3.0229,
      "step": 16925
    },
    {
      "epoch": 6.5528455284552845,
      "grad_norm": 26.131044387817383,
      "learning_rate": 3.830171635049684e-06,
      "loss": 0.2184,
      "step": 16926
    },
    {
      "epoch": 6.553232675183895,
      "grad_norm": 33.19841003417969,
      "learning_rate": 3.829741472017895e-06,
      "loss": 0.2107,
      "step": 16927
    },
    {
      "epoch": 6.553619821912505,
      "grad_norm": 23.530902862548828,
      "learning_rate": 3.829311308986106e-06,
      "loss": 1.3306,
      "step": 16928
    },
    {
      "epoch": 6.554006968641115,
      "grad_norm": 18.524208068847656,
      "learning_rate": 3.828881145954317e-06,
      "loss": 1.6713,
      "step": 16929
    },
    {
      "epoch": 6.554394115369725,
      "grad_norm": 206.9491424560547,
      "learning_rate": 3.828450982922528e-06,
      "loss": 0.899,
      "step": 16930
    },
    {
      "epoch": 6.554781262098335,
      "grad_norm": 67.719482421875,
      "learning_rate": 3.828020819890739e-06,
      "loss": 4.5016,
      "step": 16931
    },
    {
      "epoch": 6.555168408826946,
      "grad_norm": 20.570615768432617,
      "learning_rate": 3.82759065685895e-06,
      "loss": 0.2555,
      "step": 16932
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 49.98353958129883,
      "learning_rate": 3.827160493827161e-06,
      "loss": 0.3354,
      "step": 16933
    },
    {
      "epoch": 6.555942702284166,
      "grad_norm": 36.73215103149414,
      "learning_rate": 3.826730330795371e-06,
      "loss": 1.4423,
      "step": 16934
    },
    {
      "epoch": 6.5563298490127755,
      "grad_norm": 140.6311492919922,
      "learning_rate": 3.826300167763583e-06,
      "loss": 2.4404,
      "step": 16935
    },
    {
      "epoch": 6.556716995741386,
      "grad_norm": 32.76033401489258,
      "learning_rate": 3.825870004731793e-06,
      "loss": 0.3554,
      "step": 16936
    },
    {
      "epoch": 6.5571041424699965,
      "grad_norm": 56.07676315307617,
      "learning_rate": 3.825439841700005e-06,
      "loss": 1.3418,
      "step": 16937
    },
    {
      "epoch": 6.557491289198606,
      "grad_norm": 5.4807586669921875,
      "learning_rate": 3.825009678668215e-06,
      "loss": 0.1838,
      "step": 16938
    },
    {
      "epoch": 6.557878435927217,
      "grad_norm": 73.86370849609375,
      "learning_rate": 3.824579515636426e-06,
      "loss": 2.1379,
      "step": 16939
    },
    {
      "epoch": 6.558265582655826,
      "grad_norm": 226.1577911376953,
      "learning_rate": 3.824149352604637e-06,
      "loss": 1.0752,
      "step": 16940
    },
    {
      "epoch": 6.558652729384437,
      "grad_norm": 55.236629486083984,
      "learning_rate": 3.823719189572848e-06,
      "loss": 0.7652,
      "step": 16941
    },
    {
      "epoch": 6.559039876113047,
      "grad_norm": 16.32173728942871,
      "learning_rate": 3.823289026541059e-06,
      "loss": 0.2665,
      "step": 16942
    },
    {
      "epoch": 6.559427022841657,
      "grad_norm": 19.44956398010254,
      "learning_rate": 3.822858863509271e-06,
      "loss": 1.7811,
      "step": 16943
    },
    {
      "epoch": 6.559814169570267,
      "grad_norm": 52.13212203979492,
      "learning_rate": 3.822428700477481e-06,
      "loss": 1.1464,
      "step": 16944
    },
    {
      "epoch": 6.560201316298877,
      "grad_norm": 35.97283172607422,
      "learning_rate": 3.821998537445693e-06,
      "loss": 0.5946,
      "step": 16945
    },
    {
      "epoch": 6.5605884630274875,
      "grad_norm": 15.204251289367676,
      "learning_rate": 3.821568374413903e-06,
      "loss": 0.1755,
      "step": 16946
    },
    {
      "epoch": 6.560975609756097,
      "grad_norm": 2.735211133956909,
      "learning_rate": 3.821138211382115e-06,
      "loss": 0.1075,
      "step": 16947
    },
    {
      "epoch": 6.561362756484708,
      "grad_norm": 24.952510833740234,
      "learning_rate": 3.820708048350325e-06,
      "loss": 1.2947,
      "step": 16948
    },
    {
      "epoch": 6.561749903213318,
      "grad_norm": 10.684760093688965,
      "learning_rate": 3.820277885318536e-06,
      "loss": 0.1015,
      "step": 16949
    },
    {
      "epoch": 6.562137049941928,
      "grad_norm": 54.6307373046875,
      "learning_rate": 3.819847722286747e-06,
      "loss": 1.4241,
      "step": 16950
    },
    {
      "epoch": 6.562524196670538,
      "grad_norm": 4.927549839019775,
      "learning_rate": 3.819417559254958e-06,
      "loss": 0.2342,
      "step": 16951
    },
    {
      "epoch": 6.562911343399148,
      "grad_norm": 31.239206314086914,
      "learning_rate": 3.818987396223168e-06,
      "loss": 1.2349,
      "step": 16952
    },
    {
      "epoch": 6.5632984901277585,
      "grad_norm": 107.65674591064453,
      "learning_rate": 3.81855723319138e-06,
      "loss": 1.0827,
      "step": 16953
    },
    {
      "epoch": 6.563685636856368,
      "grad_norm": 3.7256176471710205,
      "learning_rate": 3.81812707015959e-06,
      "loss": 0.1402,
      "step": 16954
    },
    {
      "epoch": 6.564072783584979,
      "grad_norm": 5.280852317810059,
      "learning_rate": 3.817696907127802e-06,
      "loss": 0.3231,
      "step": 16955
    },
    {
      "epoch": 6.564459930313589,
      "grad_norm": 37.040775299072266,
      "learning_rate": 3.817266744096012e-06,
      "loss": 0.2111,
      "step": 16956
    },
    {
      "epoch": 6.564847077042199,
      "grad_norm": 86.21096801757812,
      "learning_rate": 3.816836581064224e-06,
      "loss": 1.0332,
      "step": 16957
    },
    {
      "epoch": 6.565234223770809,
      "grad_norm": 64.830810546875,
      "learning_rate": 3.816406418032434e-06,
      "loss": 0.4055,
      "step": 16958
    },
    {
      "epoch": 6.56562137049942,
      "grad_norm": 352.0668029785156,
      "learning_rate": 3.815976255000646e-06,
      "loss": 0.6678,
      "step": 16959
    },
    {
      "epoch": 6.566008517228029,
      "grad_norm": 84.44428253173828,
      "learning_rate": 3.815546091968856e-06,
      "loss": 3.2661,
      "step": 16960
    },
    {
      "epoch": 6.56639566395664,
      "grad_norm": 25.002172470092773,
      "learning_rate": 3.815115928937068e-06,
      "loss": 0.4277,
      "step": 16961
    },
    {
      "epoch": 6.5667828106852495,
      "grad_norm": 6.497437000274658,
      "learning_rate": 3.8146857659052783e-06,
      "loss": 0.1684,
      "step": 16962
    },
    {
      "epoch": 6.56716995741386,
      "grad_norm": 59.13423538208008,
      "learning_rate": 3.8142556028734893e-06,
      "loss": 0.3027,
      "step": 16963
    },
    {
      "epoch": 6.56755710414247,
      "grad_norm": 51.13518524169922,
      "learning_rate": 3.8138254398417003e-06,
      "loss": 1.5686,
      "step": 16964
    },
    {
      "epoch": 6.56794425087108,
      "grad_norm": 81.82100677490234,
      "learning_rate": 3.8133952768099113e-06,
      "loss": 3.6352,
      "step": 16965
    },
    {
      "epoch": 6.568331397599691,
      "grad_norm": 129.2749481201172,
      "learning_rate": 3.812965113778122e-06,
      "loss": 1.9823,
      "step": 16966
    },
    {
      "epoch": 6.5687185443283,
      "grad_norm": 9.49686336517334,
      "learning_rate": 3.8125349507463333e-06,
      "loss": 0.1083,
      "step": 16967
    },
    {
      "epoch": 6.569105691056911,
      "grad_norm": 233.9740447998047,
      "learning_rate": 3.812104787714544e-06,
      "loss": 1.1567,
      "step": 16968
    },
    {
      "epoch": 6.56949283778552,
      "grad_norm": 42.09914016723633,
      "learning_rate": 3.8116746246827553e-06,
      "loss": 0.1711,
      "step": 16969
    },
    {
      "epoch": 6.569879984514131,
      "grad_norm": 131.100341796875,
      "learning_rate": 3.811244461650966e-06,
      "loss": 3.1779,
      "step": 16970
    },
    {
      "epoch": 6.5702671312427405,
      "grad_norm": 18.303138732910156,
      "learning_rate": 3.8108142986191773e-06,
      "loss": 1.7626,
      "step": 16971
    },
    {
      "epoch": 6.570654277971351,
      "grad_norm": 30.06760025024414,
      "learning_rate": 3.810384135587388e-06,
      "loss": 3.0363,
      "step": 16972
    },
    {
      "epoch": 6.5710414246999616,
      "grad_norm": 2.1495184898376465,
      "learning_rate": 3.809953972555599e-06,
      "loss": 0.1153,
      "step": 16973
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 42.70459747314453,
      "learning_rate": 3.80952380952381e-06,
      "loss": 0.3875,
      "step": 16974
    },
    {
      "epoch": 6.571815718157182,
      "grad_norm": 154.9440155029297,
      "learning_rate": 3.809093646492021e-06,
      "loss": 3.4906,
      "step": 16975
    },
    {
      "epoch": 6.572202864885792,
      "grad_norm": 193.9521484375,
      "learning_rate": 3.8086634834602314e-06,
      "loss": 0.9544,
      "step": 16976
    },
    {
      "epoch": 6.572590011614402,
      "grad_norm": 67.27995300292969,
      "learning_rate": 3.808233320428443e-06,
      "loss": 0.9195,
      "step": 16977
    },
    {
      "epoch": 6.572977158343012,
      "grad_norm": 71.13630676269531,
      "learning_rate": 3.8078031573966534e-06,
      "loss": 0.5215,
      "step": 16978
    },
    {
      "epoch": 6.573364305071622,
      "grad_norm": 3.3187906742095947,
      "learning_rate": 3.8073729943648648e-06,
      "loss": 0.0677,
      "step": 16979
    },
    {
      "epoch": 6.5737514518002325,
      "grad_norm": 37.19074630737305,
      "learning_rate": 3.8069428313330753e-06,
      "loss": 2.4886,
      "step": 16980
    },
    {
      "epoch": 6.574138598528842,
      "grad_norm": 5.338411808013916,
      "learning_rate": 3.8065126683012863e-06,
      "loss": 0.2207,
      "step": 16981
    },
    {
      "epoch": 6.574525745257453,
      "grad_norm": 80.13703155517578,
      "learning_rate": 3.8060825052694973e-06,
      "loss": 3.6591,
      "step": 16982
    },
    {
      "epoch": 6.574912891986063,
      "grad_norm": 201.7528076171875,
      "learning_rate": 3.8056523422377083e-06,
      "loss": 1.7398,
      "step": 16983
    },
    {
      "epoch": 6.575300038714673,
      "grad_norm": 145.47654724121094,
      "learning_rate": 3.8052221792059197e-06,
      "loss": 2.6451,
      "step": 16984
    },
    {
      "epoch": 6.575687185443283,
      "grad_norm": 95.72186279296875,
      "learning_rate": 3.8047920161741303e-06,
      "loss": 0.8736,
      "step": 16985
    },
    {
      "epoch": 6.576074332171893,
      "grad_norm": 29.363325119018555,
      "learning_rate": 3.8043618531423417e-06,
      "loss": 1.8149,
      "step": 16986
    },
    {
      "epoch": 6.576461478900503,
      "grad_norm": 69.83644104003906,
      "learning_rate": 3.8039316901105523e-06,
      "loss": 2.4086,
      "step": 16987
    },
    {
      "epoch": 6.576848625629113,
      "grad_norm": 48.404930114746094,
      "learning_rate": 3.8035015270787633e-06,
      "loss": 1.1645,
      "step": 16988
    },
    {
      "epoch": 6.5772357723577235,
      "grad_norm": 92.49333190917969,
      "learning_rate": 3.8030713640469743e-06,
      "loss": 1.6054,
      "step": 16989
    },
    {
      "epoch": 6.577622919086334,
      "grad_norm": 16.96000862121582,
      "learning_rate": 3.8026412010151853e-06,
      "loss": 1.1685,
      "step": 16990
    },
    {
      "epoch": 6.578010065814944,
      "grad_norm": 4.516651630401611,
      "learning_rate": 3.802211037983396e-06,
      "loss": 0.1536,
      "step": 16991
    },
    {
      "epoch": 6.578397212543554,
      "grad_norm": 29.28874397277832,
      "learning_rate": 3.8017808749516073e-06,
      "loss": 0.1611,
      "step": 16992
    },
    {
      "epoch": 6.578784359272164,
      "grad_norm": 53.38499069213867,
      "learning_rate": 3.801350711919818e-06,
      "loss": 0.4345,
      "step": 16993
    },
    {
      "epoch": 6.579171506000774,
      "grad_norm": 6.061359405517578,
      "learning_rate": 3.8009205488880292e-06,
      "loss": 0.1031,
      "step": 16994
    },
    {
      "epoch": 6.579558652729385,
      "grad_norm": 89.64258575439453,
      "learning_rate": 3.80049038585624e-06,
      "loss": 0.9853,
      "step": 16995
    },
    {
      "epoch": 6.579945799457994,
      "grad_norm": 78.45426177978516,
      "learning_rate": 3.800060222824451e-06,
      "loss": 0.3532,
      "step": 16996
    },
    {
      "epoch": 6.580332946186605,
      "grad_norm": 155.9795379638672,
      "learning_rate": 3.799630059792662e-06,
      "loss": 0.336,
      "step": 16997
    },
    {
      "epoch": 6.5807200929152145,
      "grad_norm": 45.92500305175781,
      "learning_rate": 3.7991998967608728e-06,
      "loss": 1.5018,
      "step": 16998
    },
    {
      "epoch": 6.581107239643825,
      "grad_norm": 2.9738314151763916,
      "learning_rate": 3.7987697337290834e-06,
      "loss": 0.1197,
      "step": 16999
    },
    {
      "epoch": 6.581494386372436,
      "grad_norm": 14.376143455505371,
      "learning_rate": 3.7983395706972948e-06,
      "loss": 0.1446,
      "step": 17000
    },
    {
      "epoch": 6.581881533101045,
      "grad_norm": 74.4965591430664,
      "learning_rate": 3.7979094076655053e-06,
      "loss": 0.6509,
      "step": 17001
    },
    {
      "epoch": 6.582268679829656,
      "grad_norm": 17.05461883544922,
      "learning_rate": 3.7974792446337168e-06,
      "loss": 0.2298,
      "step": 17002
    },
    {
      "epoch": 6.582655826558265,
      "grad_norm": 47.02519607543945,
      "learning_rate": 3.7970490816019273e-06,
      "loss": 1.6023,
      "step": 17003
    },
    {
      "epoch": 6.583042973286876,
      "grad_norm": 10.337636947631836,
      "learning_rate": 3.7966189185701387e-06,
      "loss": 0.1096,
      "step": 17004
    },
    {
      "epoch": 6.583430120015485,
      "grad_norm": 59.68425369262695,
      "learning_rate": 3.7961887555383493e-06,
      "loss": 1.3724,
      "step": 17005
    },
    {
      "epoch": 6.583817266744096,
      "grad_norm": 3.140497922897339,
      "learning_rate": 3.7957585925065603e-06,
      "loss": 0.131,
      "step": 17006
    },
    {
      "epoch": 6.5842044134727065,
      "grad_norm": 54.98593521118164,
      "learning_rate": 3.7953284294747713e-06,
      "loss": 0.9332,
      "step": 17007
    },
    {
      "epoch": 6.584591560201316,
      "grad_norm": 7.604703903198242,
      "learning_rate": 3.7948982664429823e-06,
      "loss": 0.1171,
      "step": 17008
    },
    {
      "epoch": 6.584978706929927,
      "grad_norm": 4.580615043640137,
      "learning_rate": 3.794468103411193e-06,
      "loss": 0.1946,
      "step": 17009
    },
    {
      "epoch": 6.585365853658536,
      "grad_norm": 85.97113800048828,
      "learning_rate": 3.7940379403794043e-06,
      "loss": 1.3345,
      "step": 17010
    },
    {
      "epoch": 6.585753000387147,
      "grad_norm": 40.88031768798828,
      "learning_rate": 3.793607777347615e-06,
      "loss": 1.5926,
      "step": 17011
    },
    {
      "epoch": 6.586140147115757,
      "grad_norm": 51.586456298828125,
      "learning_rate": 3.7931776143158262e-06,
      "loss": 0.954,
      "step": 17012
    },
    {
      "epoch": 6.586527293844367,
      "grad_norm": 112.70919799804688,
      "learning_rate": 3.792747451284037e-06,
      "loss": 0.4644,
      "step": 17013
    },
    {
      "epoch": 6.586914440572977,
      "grad_norm": 117.06298065185547,
      "learning_rate": 3.792317288252248e-06,
      "loss": 1.5959,
      "step": 17014
    },
    {
      "epoch": 6.587301587301587,
      "grad_norm": 43.57124710083008,
      "learning_rate": 3.791887125220459e-06,
      "loss": 2.085,
      "step": 17015
    },
    {
      "epoch": 6.5876887340301975,
      "grad_norm": 51.412635803222656,
      "learning_rate": 3.79145696218867e-06,
      "loss": 0.2928,
      "step": 17016
    },
    {
      "epoch": 6.588075880758808,
      "grad_norm": 8.639495849609375,
      "learning_rate": 3.7910267991568804e-06,
      "loss": 0.2398,
      "step": 17017
    },
    {
      "epoch": 6.588463027487418,
      "grad_norm": 40.54818344116211,
      "learning_rate": 3.7905966361250918e-06,
      "loss": 2.0633,
      "step": 17018
    },
    {
      "epoch": 6.588850174216028,
      "grad_norm": 112.23983001708984,
      "learning_rate": 3.7901664730933023e-06,
      "loss": 3.2391,
      "step": 17019
    },
    {
      "epoch": 6.589237320944638,
      "grad_norm": 46.984161376953125,
      "learning_rate": 3.7897363100615138e-06,
      "loss": 1.6227,
      "step": 17020
    },
    {
      "epoch": 6.589624467673248,
      "grad_norm": 104.8699951171875,
      "learning_rate": 3.7893061470297243e-06,
      "loss": 0.8305,
      "step": 17021
    },
    {
      "epoch": 6.590011614401858,
      "grad_norm": 58.25994110107422,
      "learning_rate": 3.7888759839979357e-06,
      "loss": 2.9323,
      "step": 17022
    },
    {
      "epoch": 6.590398761130468,
      "grad_norm": 8.768111228942871,
      "learning_rate": 3.7884458209661463e-06,
      "loss": 0.1799,
      "step": 17023
    },
    {
      "epoch": 6.590785907859079,
      "grad_norm": 62.886234283447266,
      "learning_rate": 3.7880156579343573e-06,
      "loss": 0.4153,
      "step": 17024
    },
    {
      "epoch": 6.5911730545876885,
      "grad_norm": 21.713424682617188,
      "learning_rate": 3.7875854949025687e-06,
      "loss": 1.9407,
      "step": 17025
    },
    {
      "epoch": 6.591560201316299,
      "grad_norm": 26.53363609313965,
      "learning_rate": 3.7871553318707793e-06,
      "loss": 3.1763,
      "step": 17026
    },
    {
      "epoch": 6.591947348044909,
      "grad_norm": 3.4761898517608643,
      "learning_rate": 3.7867251688389907e-06,
      "loss": 0.1934,
      "step": 17027
    },
    {
      "epoch": 6.592334494773519,
      "grad_norm": 73.90975189208984,
      "learning_rate": 3.7862950058072013e-06,
      "loss": 0.6225,
      "step": 17028
    },
    {
      "epoch": 6.59272164150213,
      "grad_norm": 20.966066360473633,
      "learning_rate": 3.7858648427754123e-06,
      "loss": 0.1697,
      "step": 17029
    },
    {
      "epoch": 6.593108788230739,
      "grad_norm": 106.60820007324219,
      "learning_rate": 3.7854346797436233e-06,
      "loss": 0.9078,
      "step": 17030
    },
    {
      "epoch": 6.59349593495935,
      "grad_norm": 72.4736328125,
      "learning_rate": 3.7850045167118343e-06,
      "loss": 2.7215,
      "step": 17031
    },
    {
      "epoch": 6.5938830816879594,
      "grad_norm": 21.681583404541016,
      "learning_rate": 3.784574353680045e-06,
      "loss": 1.6821,
      "step": 17032
    },
    {
      "epoch": 6.59427022841657,
      "grad_norm": 50.6774787902832,
      "learning_rate": 3.7841441906482562e-06,
      "loss": 1.5285,
      "step": 17033
    },
    {
      "epoch": 6.5946573751451805,
      "grad_norm": 1.0439664125442505,
      "learning_rate": 3.783714027616467e-06,
      "loss": 0.0359,
      "step": 17034
    },
    {
      "epoch": 6.59504452187379,
      "grad_norm": 64.17688751220703,
      "learning_rate": 3.7832838645846782e-06,
      "loss": 2.3936,
      "step": 17035
    },
    {
      "epoch": 6.595431668602401,
      "grad_norm": 1.2352575063705444,
      "learning_rate": 3.782853701552889e-06,
      "loss": 0.0384,
      "step": 17036
    },
    {
      "epoch": 6.59581881533101,
      "grad_norm": 105.14682006835938,
      "learning_rate": 3.7824235385211e-06,
      "loss": 1.3558,
      "step": 17037
    },
    {
      "epoch": 6.596205962059621,
      "grad_norm": 3.7765839099884033,
      "learning_rate": 3.7819933754893108e-06,
      "loss": 0.0818,
      "step": 17038
    },
    {
      "epoch": 6.59659310878823,
      "grad_norm": 69.38471221923828,
      "learning_rate": 3.7815632124575218e-06,
      "loss": 1.4781,
      "step": 17039
    },
    {
      "epoch": 6.596980255516841,
      "grad_norm": 127.70243835449219,
      "learning_rate": 3.7811330494257328e-06,
      "loss": 0.6028,
      "step": 17040
    },
    {
      "epoch": 6.597367402245451,
      "grad_norm": 1.1951191425323486,
      "learning_rate": 3.7807028863939438e-06,
      "loss": 0.0422,
      "step": 17041
    },
    {
      "epoch": 6.597754548974061,
      "grad_norm": 2.4427192211151123,
      "learning_rate": 3.7802727233621543e-06,
      "loss": 0.1355,
      "step": 17042
    },
    {
      "epoch": 6.5981416957026715,
      "grad_norm": 0.35922273993492126,
      "learning_rate": 3.7798425603303657e-06,
      "loss": 0.0103,
      "step": 17043
    },
    {
      "epoch": 6.598528842431281,
      "grad_norm": 15.906487464904785,
      "learning_rate": 3.7794123972985763e-06,
      "loss": 0.6919,
      "step": 17044
    },
    {
      "epoch": 6.598915989159892,
      "grad_norm": 3.3142449855804443,
      "learning_rate": 3.7789822342667877e-06,
      "loss": 0.0957,
      "step": 17045
    },
    {
      "epoch": 6.599303135888501,
      "grad_norm": 124.11925506591797,
      "learning_rate": 3.7785520712349983e-06,
      "loss": 1.0071,
      "step": 17046
    },
    {
      "epoch": 6.599690282617112,
      "grad_norm": 13.70416259765625,
      "learning_rate": 3.7781219082032093e-06,
      "loss": 0.1508,
      "step": 17047
    },
    {
      "epoch": 6.600077429345722,
      "grad_norm": 42.0428466796875,
      "learning_rate": 3.7776917451714203e-06,
      "loss": 1.5307,
      "step": 17048
    },
    {
      "epoch": 6.600464576074332,
      "grad_norm": 172.6678466796875,
      "learning_rate": 3.7772615821396313e-06,
      "loss": 1.6615,
      "step": 17049
    },
    {
      "epoch": 6.600851722802942,
      "grad_norm": 7.569042205810547,
      "learning_rate": 3.776831419107842e-06,
      "loss": 0.2416,
      "step": 17050
    },
    {
      "epoch": 6.601238869531553,
      "grad_norm": 38.6898078918457,
      "learning_rate": 3.7764012560760533e-06,
      "loss": 1.6011,
      "step": 17051
    },
    {
      "epoch": 6.6016260162601625,
      "grad_norm": 30.30088233947754,
      "learning_rate": 3.775971093044264e-06,
      "loss": 2.0992,
      "step": 17052
    },
    {
      "epoch": 6.602013162988773,
      "grad_norm": 129.19888305664062,
      "learning_rate": 3.7755409300124752e-06,
      "loss": 3.0778,
      "step": 17053
    },
    {
      "epoch": 6.602400309717383,
      "grad_norm": 16.605995178222656,
      "learning_rate": 3.775110766980686e-06,
      "loss": 0.1287,
      "step": 17054
    },
    {
      "epoch": 6.602787456445993,
      "grad_norm": 20.47093963623047,
      "learning_rate": 3.7746806039488972e-06,
      "loss": 0.3332,
      "step": 17055
    },
    {
      "epoch": 6.603174603174603,
      "grad_norm": 105.37516784667969,
      "learning_rate": 3.774250440917108e-06,
      "loss": 2.9624,
      "step": 17056
    },
    {
      "epoch": 6.603561749903213,
      "grad_norm": 20.065580368041992,
      "learning_rate": 3.7738202778853188e-06,
      "loss": 0.3062,
      "step": 17057
    },
    {
      "epoch": 6.603948896631824,
      "grad_norm": 148.6962432861328,
      "learning_rate": 3.7733901148535298e-06,
      "loss": 0.4535,
      "step": 17058
    },
    {
      "epoch": 6.6043360433604335,
      "grad_norm": 103.49532318115234,
      "learning_rate": 3.7729599518217408e-06,
      "loss": 0.5171,
      "step": 17059
    },
    {
      "epoch": 6.604723190089044,
      "grad_norm": 27.695858001708984,
      "learning_rate": 3.7725297887899513e-06,
      "loss": 0.4018,
      "step": 17060
    },
    {
      "epoch": 6.605110336817654,
      "grad_norm": 35.39335632324219,
      "learning_rate": 3.7720996257581628e-06,
      "loss": 0.5112,
      "step": 17061
    },
    {
      "epoch": 6.605497483546264,
      "grad_norm": 95.1202621459961,
      "learning_rate": 3.7716694627263733e-06,
      "loss": 3.0794,
      "step": 17062
    },
    {
      "epoch": 6.605884630274874,
      "grad_norm": 178.51266479492188,
      "learning_rate": 3.7712392996945847e-06,
      "loss": 2.2342,
      "step": 17063
    },
    {
      "epoch": 6.606271777003484,
      "grad_norm": 83.35832214355469,
      "learning_rate": 3.7708091366627953e-06,
      "loss": 1.0777,
      "step": 17064
    },
    {
      "epoch": 6.606658923732095,
      "grad_norm": 62.0540885925293,
      "learning_rate": 3.7703789736310063e-06,
      "loss": 1.0291,
      "step": 17065
    },
    {
      "epoch": 6.607046070460704,
      "grad_norm": 178.6923065185547,
      "learning_rate": 3.7699488105992173e-06,
      "loss": 1.5985,
      "step": 17066
    },
    {
      "epoch": 6.607433217189315,
      "grad_norm": 211.0055694580078,
      "learning_rate": 3.7695186475674283e-06,
      "loss": 2.6504,
      "step": 17067
    },
    {
      "epoch": 6.607820363917925,
      "grad_norm": 71.48331451416016,
      "learning_rate": 3.7690884845356397e-06,
      "loss": 1.9983,
      "step": 17068
    },
    {
      "epoch": 6.608207510646535,
      "grad_norm": 257.824462890625,
      "learning_rate": 3.7686583215038503e-06,
      "loss": 3.067,
      "step": 17069
    },
    {
      "epoch": 6.6085946573751455,
      "grad_norm": 3.754549264907837,
      "learning_rate": 3.7682281584720617e-06,
      "loss": 0.19,
      "step": 17070
    },
    {
      "epoch": 6.608981804103755,
      "grad_norm": 86.72371673583984,
      "learning_rate": 3.7677979954402723e-06,
      "loss": 0.1838,
      "step": 17071
    },
    {
      "epoch": 6.609368950832366,
      "grad_norm": 75.06985473632812,
      "learning_rate": 3.7673678324084832e-06,
      "loss": 0.7841,
      "step": 17072
    },
    {
      "epoch": 6.609756097560975,
      "grad_norm": 101.11791229248047,
      "learning_rate": 3.7669376693766942e-06,
      "loss": 1.2013,
      "step": 17073
    },
    {
      "epoch": 6.610143244289586,
      "grad_norm": 68.88075256347656,
      "learning_rate": 3.7665075063449052e-06,
      "loss": 1.9795,
      "step": 17074
    },
    {
      "epoch": 6.610530391018196,
      "grad_norm": 30.475011825561523,
      "learning_rate": 3.766077343313116e-06,
      "loss": 0.2358,
      "step": 17075
    },
    {
      "epoch": 6.610917537746806,
      "grad_norm": 27.35886573791504,
      "learning_rate": 3.765647180281327e-06,
      "loss": 1.3324,
      "step": 17076
    },
    {
      "epoch": 6.611304684475416,
      "grad_norm": 82.99754333496094,
      "learning_rate": 3.7652170172495378e-06,
      "loss": 1.4004,
      "step": 17077
    },
    {
      "epoch": 6.611691831204026,
      "grad_norm": 103.73914337158203,
      "learning_rate": 3.764786854217749e-06,
      "loss": 1.0764,
      "step": 17078
    },
    {
      "epoch": 6.6120789779326365,
      "grad_norm": 143.88870239257812,
      "learning_rate": 3.7643566911859598e-06,
      "loss": 2.2017,
      "step": 17079
    },
    {
      "epoch": 6.612466124661246,
      "grad_norm": 32.6055793762207,
      "learning_rate": 3.7639265281541708e-06,
      "loss": 1.4389,
      "step": 17080
    },
    {
      "epoch": 6.612853271389857,
      "grad_norm": 15.190638542175293,
      "learning_rate": 3.7634963651223817e-06,
      "loss": 1.2313,
      "step": 17081
    },
    {
      "epoch": 6.613240418118467,
      "grad_norm": 248.40090942382812,
      "learning_rate": 3.7630662020905927e-06,
      "loss": 0.5363,
      "step": 17082
    },
    {
      "epoch": 6.613627564847077,
      "grad_norm": 18.878719329833984,
      "learning_rate": 3.7626360390588033e-06,
      "loss": 1.3335,
      "step": 17083
    },
    {
      "epoch": 6.614014711575687,
      "grad_norm": 164.819091796875,
      "learning_rate": 3.7622058760270147e-06,
      "loss": 1.5773,
      "step": 17084
    },
    {
      "epoch": 6.614401858304297,
      "grad_norm": 56.35404586791992,
      "learning_rate": 3.7617757129952253e-06,
      "loss": 0.3676,
      "step": 17085
    },
    {
      "epoch": 6.6147890050329075,
      "grad_norm": 49.440155029296875,
      "learning_rate": 3.7613455499634367e-06,
      "loss": 1.4312,
      "step": 17086
    },
    {
      "epoch": 6.615176151761518,
      "grad_norm": 41.82056427001953,
      "learning_rate": 3.7609153869316473e-06,
      "loss": 2.231,
      "step": 17087
    },
    {
      "epoch": 6.615563298490128,
      "grad_norm": 8.279802322387695,
      "learning_rate": 3.7604852238998587e-06,
      "loss": 0.2225,
      "step": 17088
    },
    {
      "epoch": 6.615950445218738,
      "grad_norm": 196.98544311523438,
      "learning_rate": 3.7600550608680693e-06,
      "loss": 5.2414,
      "step": 17089
    },
    {
      "epoch": 6.616337591947348,
      "grad_norm": 50.329246520996094,
      "learning_rate": 3.7596248978362803e-06,
      "loss": 1.4679,
      "step": 17090
    },
    {
      "epoch": 6.616724738675958,
      "grad_norm": 31.933298110961914,
      "learning_rate": 3.7591947348044912e-06,
      "loss": 0.3878,
      "step": 17091
    },
    {
      "epoch": 6.617111885404569,
      "grad_norm": 28.150070190429688,
      "learning_rate": 3.7587645717727022e-06,
      "loss": 0.2236,
      "step": 17092
    },
    {
      "epoch": 6.617499032133178,
      "grad_norm": 30.9401912689209,
      "learning_rate": 3.758334408740913e-06,
      "loss": 2.123,
      "step": 17093
    },
    {
      "epoch": 6.617886178861789,
      "grad_norm": 32.78035354614258,
      "learning_rate": 3.7579042457091242e-06,
      "loss": 0.1328,
      "step": 17094
    },
    {
      "epoch": 6.6182733255903985,
      "grad_norm": 111.236328125,
      "learning_rate": 3.757474082677335e-06,
      "loss": 1.1606,
      "step": 17095
    },
    {
      "epoch": 6.618660472319009,
      "grad_norm": 1.264638900756836,
      "learning_rate": 3.757043919645546e-06,
      "loss": 0.04,
      "step": 17096
    },
    {
      "epoch": 6.619047619047619,
      "grad_norm": 145.0433807373047,
      "learning_rate": 3.7566137566137568e-06,
      "loss": 0.9567,
      "step": 17097
    },
    {
      "epoch": 6.619434765776229,
      "grad_norm": 32.13282012939453,
      "learning_rate": 3.7561835935819678e-06,
      "loss": 1.1428,
      "step": 17098
    },
    {
      "epoch": 6.61982191250484,
      "grad_norm": 52.6122932434082,
      "learning_rate": 3.7557534305501788e-06,
      "loss": 1.3545,
      "step": 17099
    },
    {
      "epoch": 6.620209059233449,
      "grad_norm": 20.538009643554688,
      "learning_rate": 3.7553232675183898e-06,
      "loss": 2.2418,
      "step": 17100
    },
    {
      "epoch": 6.62059620596206,
      "grad_norm": 0.756554126739502,
      "learning_rate": 3.7548931044866003e-06,
      "loss": 0.0275,
      "step": 17101
    },
    {
      "epoch": 6.620983352690669,
      "grad_norm": 117.1634750366211,
      "learning_rate": 3.7544629414548117e-06,
      "loss": 1.606,
      "step": 17102
    },
    {
      "epoch": 6.62137049941928,
      "grad_norm": 30.470399856567383,
      "learning_rate": 3.7540327784230223e-06,
      "loss": 1.1975,
      "step": 17103
    },
    {
      "epoch": 6.62175764614789,
      "grad_norm": 22.620067596435547,
      "learning_rate": 3.7536026153912337e-06,
      "loss": 1.0026,
      "step": 17104
    },
    {
      "epoch": 6.6221447928765,
      "grad_norm": 3.339890241622925,
      "learning_rate": 3.7531724523594443e-06,
      "loss": 0.1384,
      "step": 17105
    },
    {
      "epoch": 6.6225319396051106,
      "grad_norm": 69.43438720703125,
      "learning_rate": 3.7527422893276557e-06,
      "loss": 1.4268,
      "step": 17106
    },
    {
      "epoch": 6.62291908633372,
      "grad_norm": 51.65077590942383,
      "learning_rate": 3.7523121262958663e-06,
      "loss": 2.9659,
      "step": 17107
    },
    {
      "epoch": 6.623306233062331,
      "grad_norm": 5.636601448059082,
      "learning_rate": 3.7518819632640773e-06,
      "loss": 0.2191,
      "step": 17108
    },
    {
      "epoch": 6.623693379790941,
      "grad_norm": 93.25410461425781,
      "learning_rate": 3.7514518002322887e-06,
      "loss": 1.6937,
      "step": 17109
    },
    {
      "epoch": 6.624080526519551,
      "grad_norm": 66.03909301757812,
      "learning_rate": 3.7510216372004993e-06,
      "loss": 1.7522,
      "step": 17110
    },
    {
      "epoch": 6.624467673248161,
      "grad_norm": 20.03553581237793,
      "learning_rate": 3.7505914741687107e-06,
      "loss": 2.0157,
      "step": 17111
    },
    {
      "epoch": 6.624854819976771,
      "grad_norm": 40.57963943481445,
      "learning_rate": 3.7501613111369212e-06,
      "loss": 1.1835,
      "step": 17112
    },
    {
      "epoch": 6.6252419667053815,
      "grad_norm": 70.87571716308594,
      "learning_rate": 3.7497311481051322e-06,
      "loss": 0.6713,
      "step": 17113
    },
    {
      "epoch": 6.625629113433991,
      "grad_norm": 49.00045394897461,
      "learning_rate": 3.7493009850733432e-06,
      "loss": 0.937,
      "step": 17114
    },
    {
      "epoch": 6.626016260162602,
      "grad_norm": 30.935461044311523,
      "learning_rate": 3.7488708220415542e-06,
      "loss": 1.5902,
      "step": 17115
    },
    {
      "epoch": 6.626403406891212,
      "grad_norm": 10.53866958618164,
      "learning_rate": 3.7484406590097648e-06,
      "loss": 0.1097,
      "step": 17116
    },
    {
      "epoch": 6.626790553619822,
      "grad_norm": 48.164756774902344,
      "learning_rate": 3.748010495977976e-06,
      "loss": 2.6939,
      "step": 17117
    },
    {
      "epoch": 6.627177700348432,
      "grad_norm": 74.6524429321289,
      "learning_rate": 3.7475803329461868e-06,
      "loss": 0.3817,
      "step": 17118
    },
    {
      "epoch": 6.627564847077042,
      "grad_norm": 59.58700942993164,
      "learning_rate": 3.747150169914398e-06,
      "loss": 0.5241,
      "step": 17119
    },
    {
      "epoch": 6.627951993805652,
      "grad_norm": 2.5077085494995117,
      "learning_rate": 3.7467200068826088e-06,
      "loss": 0.0818,
      "step": 17120
    },
    {
      "epoch": 6.628339140534262,
      "grad_norm": 81.67218780517578,
      "learning_rate": 3.7462898438508197e-06,
      "loss": 0.8469,
      "step": 17121
    },
    {
      "epoch": 6.6287262872628725,
      "grad_norm": 95.67801666259766,
      "learning_rate": 3.7458596808190307e-06,
      "loss": 3.0694,
      "step": 17122
    },
    {
      "epoch": 6.629113433991483,
      "grad_norm": 2.7635414600372314,
      "learning_rate": 3.7454295177872417e-06,
      "loss": 0.0628,
      "step": 17123
    },
    {
      "epoch": 6.629500580720093,
      "grad_norm": 25.980854034423828,
      "learning_rate": 3.7449993547554527e-06,
      "loss": 1.5362,
      "step": 17124
    },
    {
      "epoch": 6.629887727448703,
      "grad_norm": 14.30511474609375,
      "learning_rate": 3.7445691917236637e-06,
      "loss": 0.1738,
      "step": 17125
    },
    {
      "epoch": 6.630274874177314,
      "grad_norm": 72.6119155883789,
      "learning_rate": 3.7441390286918743e-06,
      "loss": 3.7859,
      "step": 17126
    },
    {
      "epoch": 6.630662020905923,
      "grad_norm": 95.76126098632812,
      "learning_rate": 3.7437088656600857e-06,
      "loss": 0.5445,
      "step": 17127
    },
    {
      "epoch": 6.631049167634534,
      "grad_norm": 7.091097831726074,
      "learning_rate": 3.7432787026282963e-06,
      "loss": 0.3183,
      "step": 17128
    },
    {
      "epoch": 6.631436314363143,
      "grad_norm": 74.72859954833984,
      "learning_rate": 3.7428485395965077e-06,
      "loss": 4.2208,
      "step": 17129
    },
    {
      "epoch": 6.631823461091754,
      "grad_norm": 4.207855701446533,
      "learning_rate": 3.7424183765647183e-06,
      "loss": 0.1,
      "step": 17130
    },
    {
      "epoch": 6.6322106078203635,
      "grad_norm": 93.71411895751953,
      "learning_rate": 3.7419882135329292e-06,
      "loss": 1.2461,
      "step": 17131
    },
    {
      "epoch": 6.632597754548974,
      "grad_norm": 72.07498931884766,
      "learning_rate": 3.7415580505011402e-06,
      "loss": 2.5033,
      "step": 17132
    },
    {
      "epoch": 6.6329849012775846,
      "grad_norm": 27.49213409423828,
      "learning_rate": 3.7411278874693512e-06,
      "loss": 4.0647,
      "step": 17133
    },
    {
      "epoch": 6.633372048006194,
      "grad_norm": 21.13739776611328,
      "learning_rate": 3.740697724437562e-06,
      "loss": 2.0014,
      "step": 17134
    },
    {
      "epoch": 6.633759194734805,
      "grad_norm": 23.759536743164062,
      "learning_rate": 3.7402675614057732e-06,
      "loss": 2.5147,
      "step": 17135
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 50.69586181640625,
      "learning_rate": 3.7398373983739838e-06,
      "loss": 0.4407,
      "step": 17136
    },
    {
      "epoch": 6.634533488192025,
      "grad_norm": 232.22506713867188,
      "learning_rate": 3.739407235342195e-06,
      "loss": 0.7435,
      "step": 17137
    },
    {
      "epoch": 6.634920634920634,
      "grad_norm": 41.64580535888672,
      "learning_rate": 3.7389770723104058e-06,
      "loss": 0.5211,
      "step": 17138
    },
    {
      "epoch": 6.635307781649245,
      "grad_norm": 40.02440643310547,
      "learning_rate": 3.7385469092786168e-06,
      "loss": 1.1673,
      "step": 17139
    },
    {
      "epoch": 6.6356949283778555,
      "grad_norm": 28.753767013549805,
      "learning_rate": 3.7381167462468277e-06,
      "loss": 0.2555,
      "step": 17140
    },
    {
      "epoch": 6.636082075106465,
      "grad_norm": 62.239784240722656,
      "learning_rate": 3.7376865832150387e-06,
      "loss": 0.9009,
      "step": 17141
    },
    {
      "epoch": 6.636469221835076,
      "grad_norm": 6.453210830688477,
      "learning_rate": 3.7372564201832497e-06,
      "loss": 0.1571,
      "step": 17142
    },
    {
      "epoch": 6.636856368563686,
      "grad_norm": 3.375420093536377,
      "learning_rate": 3.7368262571514607e-06,
      "loss": 0.1473,
      "step": 17143
    },
    {
      "epoch": 6.637243515292296,
      "grad_norm": 30.318891525268555,
      "learning_rate": 3.7363960941196713e-06,
      "loss": 0.8159,
      "step": 17144
    },
    {
      "epoch": 6.637630662020906,
      "grad_norm": 80.24012756347656,
      "learning_rate": 3.7359659310878827e-06,
      "loss": 1.65,
      "step": 17145
    },
    {
      "epoch": 6.638017808749516,
      "grad_norm": 105.71321868896484,
      "learning_rate": 3.7355357680560933e-06,
      "loss": 1.5258,
      "step": 17146
    },
    {
      "epoch": 6.638404955478126,
      "grad_norm": 248.7469482421875,
      "learning_rate": 3.7351056050243047e-06,
      "loss": 1.7008,
      "step": 17147
    },
    {
      "epoch": 6.638792102206736,
      "grad_norm": 82.53556823730469,
      "learning_rate": 3.7346754419925153e-06,
      "loss": 1.326,
      "step": 17148
    },
    {
      "epoch": 6.6391792489353465,
      "grad_norm": 20.800275802612305,
      "learning_rate": 3.7342452789607263e-06,
      "loss": 0.1485,
      "step": 17149
    },
    {
      "epoch": 6.639566395663957,
      "grad_norm": 21.452106475830078,
      "learning_rate": 3.7338151159289377e-06,
      "loss": 0.1581,
      "step": 17150
    },
    {
      "epoch": 6.639953542392567,
      "grad_norm": 75.76585388183594,
      "learning_rate": 3.7333849528971482e-06,
      "loss": 1.271,
      "step": 17151
    },
    {
      "epoch": 6.640340689121177,
      "grad_norm": 61.33045959472656,
      "learning_rate": 3.7329547898653597e-06,
      "loss": 0.3683,
      "step": 17152
    },
    {
      "epoch": 6.640727835849787,
      "grad_norm": 34.72744369506836,
      "learning_rate": 3.7325246268335702e-06,
      "loss": 1.5257,
      "step": 17153
    },
    {
      "epoch": 6.641114982578397,
      "grad_norm": 86.65779876708984,
      "learning_rate": 3.7320944638017812e-06,
      "loss": 0.2755,
      "step": 17154
    },
    {
      "epoch": 6.641502129307007,
      "grad_norm": 7.98444128036499,
      "learning_rate": 3.731664300769992e-06,
      "loss": 0.3428,
      "step": 17155
    },
    {
      "epoch": 6.641889276035617,
      "grad_norm": 65.40840148925781,
      "learning_rate": 3.731234137738203e-06,
      "loss": 2.8135,
      "step": 17156
    },
    {
      "epoch": 6.642276422764228,
      "grad_norm": 77.6694107055664,
      "learning_rate": 3.7308039747064138e-06,
      "loss": 1.8862,
      "step": 17157
    },
    {
      "epoch": 6.6426635694928375,
      "grad_norm": 32.768245697021484,
      "learning_rate": 3.730373811674625e-06,
      "loss": 0.3288,
      "step": 17158
    },
    {
      "epoch": 6.643050716221448,
      "grad_norm": 39.928565979003906,
      "learning_rate": 3.7299436486428358e-06,
      "loss": 0.4208,
      "step": 17159
    },
    {
      "epoch": 6.643437862950059,
      "grad_norm": 1.3680373430252075,
      "learning_rate": 3.729513485611047e-06,
      "loss": 0.0525,
      "step": 17160
    },
    {
      "epoch": 6.643825009678668,
      "grad_norm": 83.87093353271484,
      "learning_rate": 3.7290833225792577e-06,
      "loss": 0.2609,
      "step": 17161
    },
    {
      "epoch": 6.644212156407279,
      "grad_norm": 26.97333335876465,
      "learning_rate": 3.728653159547469e-06,
      "loss": 0.5967,
      "step": 17162
    },
    {
      "epoch": 6.644599303135888,
      "grad_norm": 77.09832000732422,
      "learning_rate": 3.7282229965156797e-06,
      "loss": 0.3353,
      "step": 17163
    },
    {
      "epoch": 6.644986449864499,
      "grad_norm": 47.979366302490234,
      "learning_rate": 3.7277928334838907e-06,
      "loss": 0.2332,
      "step": 17164
    },
    {
      "epoch": 6.6453735965931084,
      "grad_norm": 2.801913022994995,
      "learning_rate": 3.7273626704521017e-06,
      "loss": 0.1323,
      "step": 17165
    },
    {
      "epoch": 6.645760743321719,
      "grad_norm": 24.686729431152344,
      "learning_rate": 3.7269325074203127e-06,
      "loss": 1.799,
      "step": 17166
    },
    {
      "epoch": 6.6461478900503295,
      "grad_norm": 1.5528696775436401,
      "learning_rate": 3.7265023443885233e-06,
      "loss": 0.0546,
      "step": 17167
    },
    {
      "epoch": 6.646535036778939,
      "grad_norm": 15.604697227478027,
      "learning_rate": 3.7260721813567347e-06,
      "loss": 0.2475,
      "step": 17168
    },
    {
      "epoch": 6.64692218350755,
      "grad_norm": 7.362029075622559,
      "learning_rate": 3.7256420183249453e-06,
      "loss": 0.1118,
      "step": 17169
    },
    {
      "epoch": 6.647309330236159,
      "grad_norm": 42.895320892333984,
      "learning_rate": 3.7252118552931567e-06,
      "loss": 1.6213,
      "step": 17170
    },
    {
      "epoch": 6.64769647696477,
      "grad_norm": 52.48141098022461,
      "learning_rate": 3.7247816922613672e-06,
      "loss": 3.0471,
      "step": 17171
    },
    {
      "epoch": 6.648083623693379,
      "grad_norm": 144.69285583496094,
      "learning_rate": 3.7243515292295782e-06,
      "loss": 2.014,
      "step": 17172
    },
    {
      "epoch": 6.64847077042199,
      "grad_norm": 2.609941005706787,
      "learning_rate": 3.7239213661977892e-06,
      "loss": 0.1092,
      "step": 17173
    },
    {
      "epoch": 6.6488579171506,
      "grad_norm": 14.414036750793457,
      "learning_rate": 3.7234912031660002e-06,
      "loss": 1.3903,
      "step": 17174
    },
    {
      "epoch": 6.64924506387921,
      "grad_norm": 166.9432373046875,
      "learning_rate": 3.7230610401342108e-06,
      "loss": 0.3663,
      "step": 17175
    },
    {
      "epoch": 6.6496322106078205,
      "grad_norm": 61.11735153198242,
      "learning_rate": 3.722630877102422e-06,
      "loss": 2.3566,
      "step": 17176
    },
    {
      "epoch": 6.65001935733643,
      "grad_norm": 59.45015335083008,
      "learning_rate": 3.7222007140706328e-06,
      "loss": 0.4174,
      "step": 17177
    },
    {
      "epoch": 6.650406504065041,
      "grad_norm": 50.49940872192383,
      "learning_rate": 3.721770551038844e-06,
      "loss": 0.4156,
      "step": 17178
    },
    {
      "epoch": 6.650793650793651,
      "grad_norm": 4.597740173339844,
      "learning_rate": 3.7213403880070548e-06,
      "loss": 0.0825,
      "step": 17179
    },
    {
      "epoch": 6.651180797522261,
      "grad_norm": 104.96685791015625,
      "learning_rate": 3.720910224975266e-06,
      "loss": 1.5523,
      "step": 17180
    },
    {
      "epoch": 6.651567944250871,
      "grad_norm": 159.6128692626953,
      "learning_rate": 3.7204800619434767e-06,
      "loss": 1.6202,
      "step": 17181
    },
    {
      "epoch": 6.651955090979481,
      "grad_norm": 6.517353534698486,
      "learning_rate": 3.7200498989116877e-06,
      "loss": 0.2376,
      "step": 17182
    },
    {
      "epoch": 6.652342237708091,
      "grad_norm": 26.153656005859375,
      "learning_rate": 3.7196197358798987e-06,
      "loss": 3.0688,
      "step": 17183
    },
    {
      "epoch": 6.652729384436702,
      "grad_norm": 0.3793904781341553,
      "learning_rate": 3.7191895728481097e-06,
      "loss": 0.0109,
      "step": 17184
    },
    {
      "epoch": 6.6531165311653115,
      "grad_norm": 4.946423530578613,
      "learning_rate": 3.7187594098163203e-06,
      "loss": 0.0871,
      "step": 17185
    },
    {
      "epoch": 6.653503677893922,
      "grad_norm": 78.90470123291016,
      "learning_rate": 3.7183292467845317e-06,
      "loss": 0.3361,
      "step": 17186
    },
    {
      "epoch": 6.653890824622532,
      "grad_norm": 133.85630798339844,
      "learning_rate": 3.7178990837527423e-06,
      "loss": 2.8605,
      "step": 17187
    },
    {
      "epoch": 6.654277971351142,
      "grad_norm": 12.5665283203125,
      "learning_rate": 3.7174689207209537e-06,
      "loss": 0.3399,
      "step": 17188
    },
    {
      "epoch": 6.654665118079752,
      "grad_norm": 30.90230941772461,
      "learning_rate": 3.7170387576891643e-06,
      "loss": 2.0972,
      "step": 17189
    },
    {
      "epoch": 6.655052264808362,
      "grad_norm": 44.61126708984375,
      "learning_rate": 3.7166085946573752e-06,
      "loss": 0.38,
      "step": 17190
    },
    {
      "epoch": 6.655439411536973,
      "grad_norm": 18.763566970825195,
      "learning_rate": 3.7161784316255867e-06,
      "loss": 0.2099,
      "step": 17191
    },
    {
      "epoch": 6.6558265582655824,
      "grad_norm": 65.98206329345703,
      "learning_rate": 3.7157482685937972e-06,
      "loss": 0.7559,
      "step": 17192
    },
    {
      "epoch": 6.656213704994193,
      "grad_norm": 85.90503692626953,
      "learning_rate": 3.7153181055620086e-06,
      "loss": 1.6636,
      "step": 17193
    },
    {
      "epoch": 6.656600851722803,
      "grad_norm": 64.88312530517578,
      "learning_rate": 3.7148879425302192e-06,
      "loss": 0.6131,
      "step": 17194
    },
    {
      "epoch": 6.656987998451413,
      "grad_norm": 33.83625793457031,
      "learning_rate": 3.7144577794984306e-06,
      "loss": 1.983,
      "step": 17195
    },
    {
      "epoch": 6.657375145180024,
      "grad_norm": 177.9911651611328,
      "learning_rate": 3.714027616466641e-06,
      "loss": 1.1134,
      "step": 17196
    },
    {
      "epoch": 6.657762291908633,
      "grad_norm": 31.908052444458008,
      "learning_rate": 3.713597453434852e-06,
      "loss": 4.1193,
      "step": 17197
    },
    {
      "epoch": 6.658149438637244,
      "grad_norm": 234.517822265625,
      "learning_rate": 3.713167290403063e-06,
      "loss": 2.3013,
      "step": 17198
    },
    {
      "epoch": 6.658536585365853,
      "grad_norm": 57.64543533325195,
      "learning_rate": 3.712737127371274e-06,
      "loss": 0.8989,
      "step": 17199
    },
    {
      "epoch": 6.658923732094464,
      "grad_norm": 98.77626037597656,
      "learning_rate": 3.7123069643394847e-06,
      "loss": 0.6461,
      "step": 17200
    },
    {
      "epoch": 6.659310878823074,
      "grad_norm": 44.63703155517578,
      "learning_rate": 3.711876801307696e-06,
      "loss": 0.7832,
      "step": 17201
    },
    {
      "epoch": 6.659698025551684,
      "grad_norm": 4.56407356262207,
      "learning_rate": 3.7114466382759067e-06,
      "loss": 0.0691,
      "step": 17202
    },
    {
      "epoch": 6.6600851722802945,
      "grad_norm": 22.606203079223633,
      "learning_rate": 3.711016475244118e-06,
      "loss": 1.818,
      "step": 17203
    },
    {
      "epoch": 6.660472319008904,
      "grad_norm": 15.915704727172852,
      "learning_rate": 3.7105863122123287e-06,
      "loss": 1.4072,
      "step": 17204
    },
    {
      "epoch": 6.660859465737515,
      "grad_norm": 54.19490051269531,
      "learning_rate": 3.7101561491805397e-06,
      "loss": 1.9788,
      "step": 17205
    },
    {
      "epoch": 6.661246612466124,
      "grad_norm": 17.009662628173828,
      "learning_rate": 3.7097259861487507e-06,
      "loss": 0.2387,
      "step": 17206
    },
    {
      "epoch": 6.661633759194735,
      "grad_norm": 27.602434158325195,
      "learning_rate": 3.7092958231169617e-06,
      "loss": 2.0755,
      "step": 17207
    },
    {
      "epoch": 6.662020905923345,
      "grad_norm": 3.157738447189331,
      "learning_rate": 3.7088656600851723e-06,
      "loss": 0.1441,
      "step": 17208
    },
    {
      "epoch": 6.662408052651955,
      "grad_norm": 47.23392868041992,
      "learning_rate": 3.7084354970533837e-06,
      "loss": 1.8977,
      "step": 17209
    },
    {
      "epoch": 6.662795199380565,
      "grad_norm": 26.43064308166504,
      "learning_rate": 3.7080053340215942e-06,
      "loss": 0.343,
      "step": 17210
    },
    {
      "epoch": 6.663182346109175,
      "grad_norm": 313.8988952636719,
      "learning_rate": 3.7075751709898057e-06,
      "loss": 1.3509,
      "step": 17211
    },
    {
      "epoch": 6.6635694928377855,
      "grad_norm": 185.60134887695312,
      "learning_rate": 3.7071450079580162e-06,
      "loss": 0.4054,
      "step": 17212
    },
    {
      "epoch": 6.663956639566395,
      "grad_norm": 77.3360366821289,
      "learning_rate": 3.7067148449262276e-06,
      "loss": 0.3583,
      "step": 17213
    },
    {
      "epoch": 6.664343786295006,
      "grad_norm": 106.83261108398438,
      "learning_rate": 3.706284681894438e-06,
      "loss": 1.6598,
      "step": 17214
    },
    {
      "epoch": 6.664730933023616,
      "grad_norm": 32.576725006103516,
      "learning_rate": 3.705854518862649e-06,
      "loss": 1.4681,
      "step": 17215
    },
    {
      "epoch": 6.665118079752226,
      "grad_norm": 24.572717666625977,
      "learning_rate": 3.70542435583086e-06,
      "loss": 0.2674,
      "step": 17216
    },
    {
      "epoch": 6.665505226480836,
      "grad_norm": 36.36735153198242,
      "learning_rate": 3.704994192799071e-06,
      "loss": 0.5853,
      "step": 17217
    },
    {
      "epoch": 6.665892373209447,
      "grad_norm": 1.7632352113723755,
      "learning_rate": 3.7045640297672818e-06,
      "loss": 0.0968,
      "step": 17218
    },
    {
      "epoch": 6.6662795199380565,
      "grad_norm": 164.9586181640625,
      "learning_rate": 3.704133866735493e-06,
      "loss": 3.4953,
      "step": 17219
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 60.05735397338867,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.9029,
      "step": 17220
    },
    {
      "epoch": 6.667053813395277,
      "grad_norm": 33.65596008300781,
      "learning_rate": 3.703273540671915e-06,
      "loss": 0.4996,
      "step": 17221
    },
    {
      "epoch": 6.667440960123887,
      "grad_norm": 121.84571838378906,
      "learning_rate": 3.7028433776401257e-06,
      "loss": 2.3362,
      "step": 17222
    },
    {
      "epoch": 6.667828106852497,
      "grad_norm": 1.9053434133529663,
      "learning_rate": 3.7024132146083367e-06,
      "loss": 0.0706,
      "step": 17223
    },
    {
      "epoch": 6.668215253581107,
      "grad_norm": 80.79874420166016,
      "learning_rate": 3.7019830515765477e-06,
      "loss": 0.5731,
      "step": 17224
    },
    {
      "epoch": 6.668602400309718,
      "grad_norm": 46.13291931152344,
      "learning_rate": 3.7015528885447587e-06,
      "loss": 0.3556,
      "step": 17225
    },
    {
      "epoch": 6.668989547038327,
      "grad_norm": 143.15499877929688,
      "learning_rate": 3.7011227255129693e-06,
      "loss": 0.646,
      "step": 17226
    },
    {
      "epoch": 6.669376693766938,
      "grad_norm": 62.573081970214844,
      "learning_rate": 3.7006925624811807e-06,
      "loss": 0.587,
      "step": 17227
    },
    {
      "epoch": 6.6697638404955475,
      "grad_norm": 39.87960433959961,
      "learning_rate": 3.7002623994493913e-06,
      "loss": 0.2297,
      "step": 17228
    },
    {
      "epoch": 6.670150987224158,
      "grad_norm": 34.27610397338867,
      "learning_rate": 3.6998322364176027e-06,
      "loss": 1.5118,
      "step": 17229
    },
    {
      "epoch": 6.670538133952768,
      "grad_norm": 140.42991638183594,
      "learning_rate": 3.6994020733858132e-06,
      "loss": 2.6215,
      "step": 17230
    },
    {
      "epoch": 6.670925280681378,
      "grad_norm": 71.17727661132812,
      "learning_rate": 3.6989719103540247e-06,
      "loss": 0.8538,
      "step": 17231
    },
    {
      "epoch": 6.671312427409989,
      "grad_norm": 64.62034606933594,
      "learning_rate": 3.6985417473222356e-06,
      "loss": 2.0729,
      "step": 17232
    },
    {
      "epoch": 6.671699574138598,
      "grad_norm": 60.20990753173828,
      "learning_rate": 3.6981115842904462e-06,
      "loss": 1.1308,
      "step": 17233
    },
    {
      "epoch": 6.672086720867209,
      "grad_norm": 16.153366088867188,
      "learning_rate": 3.6976814212586576e-06,
      "loss": 0.2525,
      "step": 17234
    },
    {
      "epoch": 6.672473867595819,
      "grad_norm": 27.044082641601562,
      "learning_rate": 3.697251258226868e-06,
      "loss": 0.1757,
      "step": 17235
    },
    {
      "epoch": 6.672861014324429,
      "grad_norm": 21.911821365356445,
      "learning_rate": 3.6968210951950796e-06,
      "loss": 1.5569,
      "step": 17236
    },
    {
      "epoch": 6.673248161053039,
      "grad_norm": 50.08214569091797,
      "learning_rate": 3.69639093216329e-06,
      "loss": 2.1277,
      "step": 17237
    },
    {
      "epoch": 6.673635307781649,
      "grad_norm": 82.6405258178711,
      "learning_rate": 3.695960769131501e-06,
      "loss": 2.5546,
      "step": 17238
    },
    {
      "epoch": 6.6740224545102595,
      "grad_norm": 26.01152229309082,
      "learning_rate": 3.695530606099712e-06,
      "loss": 4.4711,
      "step": 17239
    },
    {
      "epoch": 6.674409601238869,
      "grad_norm": 76.0677719116211,
      "learning_rate": 3.695100443067923e-06,
      "loss": 1.1137,
      "step": 17240
    },
    {
      "epoch": 6.67479674796748,
      "grad_norm": 3.0237083435058594,
      "learning_rate": 3.6946702800361337e-06,
      "loss": 0.0208,
      "step": 17241
    },
    {
      "epoch": 6.67518389469609,
      "grad_norm": 4.915804386138916,
      "learning_rate": 3.694240117004345e-06,
      "loss": 0.1565,
      "step": 17242
    },
    {
      "epoch": 6.6755710414247,
      "grad_norm": 34.975547790527344,
      "learning_rate": 3.6938099539725557e-06,
      "loss": 1.5487,
      "step": 17243
    },
    {
      "epoch": 6.67595818815331,
      "grad_norm": 33.120704650878906,
      "learning_rate": 3.693379790940767e-06,
      "loss": 1.6203,
      "step": 17244
    },
    {
      "epoch": 6.67634533488192,
      "grad_norm": 220.2090606689453,
      "learning_rate": 3.6929496279089777e-06,
      "loss": 1.185,
      "step": 17245
    },
    {
      "epoch": 6.6767324816105305,
      "grad_norm": 14.585039138793945,
      "learning_rate": 3.692519464877189e-06,
      "loss": 0.1451,
      "step": 17246
    },
    {
      "epoch": 6.67711962833914,
      "grad_norm": 1.8659873008728027,
      "learning_rate": 3.6920893018453997e-06,
      "loss": 0.0688,
      "step": 17247
    },
    {
      "epoch": 6.677506775067751,
      "grad_norm": 2.770360231399536,
      "learning_rate": 3.6916591388136107e-06,
      "loss": 0.1242,
      "step": 17248
    },
    {
      "epoch": 6.677893921796361,
      "grad_norm": 231.61729431152344,
      "learning_rate": 3.6912289757818217e-06,
      "loss": 2.6023,
      "step": 17249
    },
    {
      "epoch": 6.678281068524971,
      "grad_norm": 43.48537826538086,
      "learning_rate": 3.6907988127500327e-06,
      "loss": 1.1044,
      "step": 17250
    },
    {
      "epoch": 6.678668215253581,
      "grad_norm": 3.121213912963867,
      "learning_rate": 3.6903686497182432e-06,
      "loss": 0.1136,
      "step": 17251
    },
    {
      "epoch": 6.679055361982192,
      "grad_norm": 82.73912048339844,
      "learning_rate": 3.6899384866864546e-06,
      "loss": 1.6591,
      "step": 17252
    },
    {
      "epoch": 6.679442508710801,
      "grad_norm": 202.0779266357422,
      "learning_rate": 3.6895083236546652e-06,
      "loss": 0.7561,
      "step": 17253
    },
    {
      "epoch": 6.679829655439412,
      "grad_norm": 11.499916076660156,
      "learning_rate": 3.6890781606228766e-06,
      "loss": 0.134,
      "step": 17254
    },
    {
      "epoch": 6.6802168021680215,
      "grad_norm": 85.92981719970703,
      "learning_rate": 3.688647997591087e-06,
      "loss": 4.1114,
      "step": 17255
    },
    {
      "epoch": 6.680603948896632,
      "grad_norm": 163.6044921875,
      "learning_rate": 3.688217834559298e-06,
      "loss": 0.9866,
      "step": 17256
    },
    {
      "epoch": 6.680991095625242,
      "grad_norm": 86.142333984375,
      "learning_rate": 3.687787671527509e-06,
      "loss": 0.3921,
      "step": 17257
    },
    {
      "epoch": 6.681378242353852,
      "grad_norm": 17.17438507080078,
      "learning_rate": 3.68735750849572e-06,
      "loss": 0.1592,
      "step": 17258
    },
    {
      "epoch": 6.681765389082463,
      "grad_norm": 104.35337829589844,
      "learning_rate": 3.6869273454639307e-06,
      "loss": 1.9022,
      "step": 17259
    },
    {
      "epoch": 6.682152535811072,
      "grad_norm": 62.90684509277344,
      "learning_rate": 3.686497182432142e-06,
      "loss": 0.452,
      "step": 17260
    },
    {
      "epoch": 6.682539682539683,
      "grad_norm": 73.92678833007812,
      "learning_rate": 3.6860670194003527e-06,
      "loss": 3.7397,
      "step": 17261
    },
    {
      "epoch": 6.682926829268292,
      "grad_norm": 123.46723937988281,
      "learning_rate": 3.685636856368564e-06,
      "loss": 1.4781,
      "step": 17262
    },
    {
      "epoch": 6.683313975996903,
      "grad_norm": 82.64956665039062,
      "learning_rate": 3.6852066933367747e-06,
      "loss": 0.9824,
      "step": 17263
    },
    {
      "epoch": 6.6837011227255125,
      "grad_norm": 4.353309631347656,
      "learning_rate": 3.684776530304986e-06,
      "loss": 0.0545,
      "step": 17264
    },
    {
      "epoch": 6.684088269454123,
      "grad_norm": 75.41001892089844,
      "learning_rate": 3.6843463672731967e-06,
      "loss": 1.4448,
      "step": 17265
    },
    {
      "epoch": 6.6844754161827336,
      "grad_norm": 155.62155151367188,
      "learning_rate": 3.6839162042414077e-06,
      "loss": 1.4162,
      "step": 17266
    },
    {
      "epoch": 6.684862562911343,
      "grad_norm": 17.094526290893555,
      "learning_rate": 3.6834860412096187e-06,
      "loss": 0.5085,
      "step": 17267
    },
    {
      "epoch": 6.685249709639954,
      "grad_norm": 73.5382308959961,
      "learning_rate": 3.6830558781778297e-06,
      "loss": 0.6672,
      "step": 17268
    },
    {
      "epoch": 6.685636856368563,
      "grad_norm": 58.05642318725586,
      "learning_rate": 3.6826257151460402e-06,
      "loss": 0.98,
      "step": 17269
    },
    {
      "epoch": 6.686024003097174,
      "grad_norm": 58.492454528808594,
      "learning_rate": 3.6821955521142517e-06,
      "loss": 0.3001,
      "step": 17270
    },
    {
      "epoch": 6.686411149825784,
      "grad_norm": 61.51340103149414,
      "learning_rate": 3.6817653890824622e-06,
      "loss": 3.739,
      "step": 17271
    },
    {
      "epoch": 6.686798296554394,
      "grad_norm": 222.80783081054688,
      "learning_rate": 3.6813352260506736e-06,
      "loss": 1.3114,
      "step": 17272
    },
    {
      "epoch": 6.6871854432830045,
      "grad_norm": 4.097984790802002,
      "learning_rate": 3.6809050630188846e-06,
      "loss": 0.1316,
      "step": 17273
    },
    {
      "epoch": 6.687572590011614,
      "grad_norm": 5.549611568450928,
      "learning_rate": 3.680474899987095e-06,
      "loss": 0.1722,
      "step": 17274
    },
    {
      "epoch": 6.687959736740225,
      "grad_norm": 77.44828033447266,
      "learning_rate": 3.6800447369553066e-06,
      "loss": 0.2643,
      "step": 17275
    },
    {
      "epoch": 6.688346883468835,
      "grad_norm": 15.762252807617188,
      "learning_rate": 3.679614573923517e-06,
      "loss": 0.2137,
      "step": 17276
    },
    {
      "epoch": 6.688734030197445,
      "grad_norm": 153.6383819580078,
      "learning_rate": 3.6791844108917286e-06,
      "loss": 1.006,
      "step": 17277
    },
    {
      "epoch": 6.689121176926055,
      "grad_norm": 66.78257751464844,
      "learning_rate": 3.678754247859939e-06,
      "loss": 2.6917,
      "step": 17278
    },
    {
      "epoch": 6.689508323654665,
      "grad_norm": 111.59141540527344,
      "learning_rate": 3.6783240848281506e-06,
      "loss": 2.267,
      "step": 17279
    },
    {
      "epoch": 6.689895470383275,
      "grad_norm": 125.63799285888672,
      "learning_rate": 3.677893921796361e-06,
      "loss": 3.9099,
      "step": 17280
    },
    {
      "epoch": 6.690282617111885,
      "grad_norm": 34.41042709350586,
      "learning_rate": 3.677463758764572e-06,
      "loss": 0.1869,
      "step": 17281
    },
    {
      "epoch": 6.6906697638404955,
      "grad_norm": 1.7290624380111694,
      "learning_rate": 3.677033595732783e-06,
      "loss": 0.0525,
      "step": 17282
    },
    {
      "epoch": 6.691056910569106,
      "grad_norm": 200.5142059326172,
      "learning_rate": 3.676603432700994e-06,
      "loss": 1.1963,
      "step": 17283
    },
    {
      "epoch": 6.691444057297716,
      "grad_norm": 16.76268196105957,
      "learning_rate": 3.6761732696692047e-06,
      "loss": 0.3255,
      "step": 17284
    },
    {
      "epoch": 6.691831204026326,
      "grad_norm": 28.557249069213867,
      "learning_rate": 3.675743106637416e-06,
      "loss": 0.1705,
      "step": 17285
    },
    {
      "epoch": 6.692218350754936,
      "grad_norm": 5.314969062805176,
      "learning_rate": 3.6753129436056267e-06,
      "loss": 0.1522,
      "step": 17286
    },
    {
      "epoch": 6.692605497483546,
      "grad_norm": 56.192073822021484,
      "learning_rate": 3.674882780573838e-06,
      "loss": 1.1767,
      "step": 17287
    },
    {
      "epoch": 6.692992644212157,
      "grad_norm": 85.00666046142578,
      "learning_rate": 3.6744526175420487e-06,
      "loss": 3.7417,
      "step": 17288
    },
    {
      "epoch": 6.693379790940766,
      "grad_norm": 121.42826843261719,
      "learning_rate": 3.6740224545102597e-06,
      "loss": 0.493,
      "step": 17289
    },
    {
      "epoch": 6.693766937669377,
      "grad_norm": 235.5126953125,
      "learning_rate": 3.6735922914784707e-06,
      "loss": 0.8038,
      "step": 17290
    },
    {
      "epoch": 6.6941540843979865,
      "grad_norm": 20.716768264770508,
      "learning_rate": 3.6731621284466816e-06,
      "loss": 0.4544,
      "step": 17291
    },
    {
      "epoch": 6.694541231126597,
      "grad_norm": 124.98584747314453,
      "learning_rate": 3.6727319654148922e-06,
      "loss": 1.1118,
      "step": 17292
    },
    {
      "epoch": 6.694928377855208,
      "grad_norm": 75.0335922241211,
      "learning_rate": 3.6723018023831036e-06,
      "loss": 0.5532,
      "step": 17293
    },
    {
      "epoch": 6.695315524583817,
      "grad_norm": 7.889726161956787,
      "learning_rate": 3.671871639351314e-06,
      "loss": 0.2793,
      "step": 17294
    },
    {
      "epoch": 6.695702671312428,
      "grad_norm": 14.983854293823242,
      "learning_rate": 3.6714414763195256e-06,
      "loss": 0.3937,
      "step": 17295
    },
    {
      "epoch": 6.696089818041037,
      "grad_norm": 24.903532028198242,
      "learning_rate": 3.671011313287736e-06,
      "loss": 3.0224,
      "step": 17296
    },
    {
      "epoch": 6.696476964769648,
      "grad_norm": 61.96324920654297,
      "learning_rate": 3.6705811502559476e-06,
      "loss": 0.2129,
      "step": 17297
    },
    {
      "epoch": 6.696864111498257,
      "grad_norm": 36.159507751464844,
      "learning_rate": 3.670150987224158e-06,
      "loss": 1.2077,
      "step": 17298
    },
    {
      "epoch": 6.697251258226868,
      "grad_norm": 10.485511779785156,
      "learning_rate": 3.669720824192369e-06,
      "loss": 0.1978,
      "step": 17299
    },
    {
      "epoch": 6.6976384049554785,
      "grad_norm": 2.3327109813690186,
      "learning_rate": 3.66929066116058e-06,
      "loss": 0.0853,
      "step": 17300
    },
    {
      "epoch": 6.698025551684088,
      "grad_norm": 1.9881163835525513,
      "learning_rate": 3.668860498128791e-06,
      "loss": 0.0985,
      "step": 17301
    },
    {
      "epoch": 6.698412698412699,
      "grad_norm": 87.35798645019531,
      "learning_rate": 3.6684303350970017e-06,
      "loss": 0.7166,
      "step": 17302
    },
    {
      "epoch": 6.698799845141308,
      "grad_norm": 95.78907775878906,
      "learning_rate": 3.668000172065213e-06,
      "loss": 0.9273,
      "step": 17303
    },
    {
      "epoch": 6.699186991869919,
      "grad_norm": 12.021515846252441,
      "learning_rate": 3.6675700090334237e-06,
      "loss": 0.1054,
      "step": 17304
    },
    {
      "epoch": 6.699574138598528,
      "grad_norm": 4.405719757080078,
      "learning_rate": 3.667139846001635e-06,
      "loss": 0.1815,
      "step": 17305
    },
    {
      "epoch": 6.699961285327139,
      "grad_norm": 34.6425895690918,
      "learning_rate": 3.6667096829698457e-06,
      "loss": 0.5127,
      "step": 17306
    },
    {
      "epoch": 6.700348432055749,
      "grad_norm": 110.1173324584961,
      "learning_rate": 3.6662795199380567e-06,
      "loss": 1.306,
      "step": 17307
    },
    {
      "epoch": 6.700735578784359,
      "grad_norm": 99.34048461914062,
      "learning_rate": 3.6658493569062677e-06,
      "loss": 1.909,
      "step": 17308
    },
    {
      "epoch": 6.7011227255129695,
      "grad_norm": 133.8982696533203,
      "learning_rate": 3.6654191938744787e-06,
      "loss": 0.7791,
      "step": 17309
    },
    {
      "epoch": 6.70150987224158,
      "grad_norm": 2.6897754669189453,
      "learning_rate": 3.6649890308426892e-06,
      "loss": 0.1066,
      "step": 17310
    },
    {
      "epoch": 6.70189701897019,
      "grad_norm": 37.64836502075195,
      "learning_rate": 3.6645588678109006e-06,
      "loss": 1.5712,
      "step": 17311
    },
    {
      "epoch": 6.7022841656988,
      "grad_norm": 59.14707565307617,
      "learning_rate": 3.6641287047791112e-06,
      "loss": 3.788,
      "step": 17312
    },
    {
      "epoch": 6.70267131242741,
      "grad_norm": 62.229434967041016,
      "learning_rate": 3.6636985417473226e-06,
      "loss": 2.3747,
      "step": 17313
    },
    {
      "epoch": 6.70305845915602,
      "grad_norm": 8.729098320007324,
      "learning_rate": 3.6632683787155336e-06,
      "loss": 0.2517,
      "step": 17314
    },
    {
      "epoch": 6.70344560588463,
      "grad_norm": 122.33980560302734,
      "learning_rate": 3.6628382156837446e-06,
      "loss": 2.269,
      "step": 17315
    },
    {
      "epoch": 6.70383275261324,
      "grad_norm": 61.4908332824707,
      "learning_rate": 3.6624080526519556e-06,
      "loss": 0.6879,
      "step": 17316
    },
    {
      "epoch": 6.704219899341851,
      "grad_norm": 9.799057960510254,
      "learning_rate": 3.661977889620166e-06,
      "loss": 0.2156,
      "step": 17317
    },
    {
      "epoch": 6.7046070460704605,
      "grad_norm": 27.519683837890625,
      "learning_rate": 3.6615477265883776e-06,
      "loss": 2.1866,
      "step": 17318
    },
    {
      "epoch": 6.704994192799071,
      "grad_norm": 26.302284240722656,
      "learning_rate": 3.661117563556588e-06,
      "loss": 2.7613,
      "step": 17319
    },
    {
      "epoch": 6.705381339527681,
      "grad_norm": 81.1813735961914,
      "learning_rate": 3.6606874005247996e-06,
      "loss": 0.827,
      "step": 17320
    },
    {
      "epoch": 6.705768486256291,
      "grad_norm": 171.4794464111328,
      "learning_rate": 3.66025723749301e-06,
      "loss": 0.4721,
      "step": 17321
    },
    {
      "epoch": 6.706155632984901,
      "grad_norm": 33.825653076171875,
      "learning_rate": 3.659827074461221e-06,
      "loss": 0.1357,
      "step": 17322
    },
    {
      "epoch": 6.706542779713511,
      "grad_norm": 3.536288261413574,
      "learning_rate": 3.659396911429432e-06,
      "loss": 0.1565,
      "step": 17323
    },
    {
      "epoch": 6.706929926442122,
      "grad_norm": 3.4703879356384277,
      "learning_rate": 3.658966748397643e-06,
      "loss": 0.1037,
      "step": 17324
    },
    {
      "epoch": 6.7073170731707314,
      "grad_norm": 3.002734661102295,
      "learning_rate": 3.6585365853658537e-06,
      "loss": 0.078,
      "step": 17325
    },
    {
      "epoch": 6.707704219899342,
      "grad_norm": 23.389541625976562,
      "learning_rate": 3.658106422334065e-06,
      "loss": 2.936,
      "step": 17326
    },
    {
      "epoch": 6.7080913666279525,
      "grad_norm": 124.01612091064453,
      "learning_rate": 3.6576762593022757e-06,
      "loss": 1.4138,
      "step": 17327
    },
    {
      "epoch": 6.708478513356562,
      "grad_norm": 99.05439758300781,
      "learning_rate": 3.657246096270487e-06,
      "loss": 2.7616,
      "step": 17328
    },
    {
      "epoch": 6.708865660085173,
      "grad_norm": 54.52574157714844,
      "learning_rate": 3.6568159332386977e-06,
      "loss": 1.4785,
      "step": 17329
    },
    {
      "epoch": 6.709252806813782,
      "grad_norm": 210.1918487548828,
      "learning_rate": 3.656385770206909e-06,
      "loss": 0.7428,
      "step": 17330
    },
    {
      "epoch": 6.709639953542393,
      "grad_norm": 72.51647186279297,
      "learning_rate": 3.6559556071751196e-06,
      "loss": 0.6806,
      "step": 17331
    },
    {
      "epoch": 6.710027100271002,
      "grad_norm": 28.191219329833984,
      "learning_rate": 3.6555254441433306e-06,
      "loss": 2.0396,
      "step": 17332
    },
    {
      "epoch": 6.710414246999613,
      "grad_norm": 37.602088928222656,
      "learning_rate": 3.6550952811115416e-06,
      "loss": 1.3454,
      "step": 17333
    },
    {
      "epoch": 6.710801393728223,
      "grad_norm": 230.9791717529297,
      "learning_rate": 3.6546651180797526e-06,
      "loss": 1.0516,
      "step": 17334
    },
    {
      "epoch": 6.711188540456833,
      "grad_norm": 90.84053802490234,
      "learning_rate": 3.654234955047963e-06,
      "loss": 0.4603,
      "step": 17335
    },
    {
      "epoch": 6.7115756871854435,
      "grad_norm": 9.746087074279785,
      "learning_rate": 3.6538047920161746e-06,
      "loss": 0.0988,
      "step": 17336
    },
    {
      "epoch": 6.711962833914053,
      "grad_norm": 27.74677276611328,
      "learning_rate": 3.653374628984385e-06,
      "loss": 0.3462,
      "step": 17337
    },
    {
      "epoch": 6.712349980642664,
      "grad_norm": 46.59728240966797,
      "learning_rate": 3.6529444659525966e-06,
      "loss": 2.9273,
      "step": 17338
    },
    {
      "epoch": 6.712737127371273,
      "grad_norm": 92.72962188720703,
      "learning_rate": 3.652514302920807e-06,
      "loss": 2.7001,
      "step": 17339
    },
    {
      "epoch": 6.713124274099884,
      "grad_norm": 303.51544189453125,
      "learning_rate": 3.652084139889018e-06,
      "loss": 1.6671,
      "step": 17340
    },
    {
      "epoch": 6.713511420828494,
      "grad_norm": 13.251214027404785,
      "learning_rate": 3.651653976857229e-06,
      "loss": 0.2832,
      "step": 17341
    },
    {
      "epoch": 6.713898567557104,
      "grad_norm": 0.5244566202163696,
      "learning_rate": 3.65122381382544e-06,
      "loss": 0.0136,
      "step": 17342
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 4.281545162200928,
      "learning_rate": 3.6507936507936507e-06,
      "loss": 0.1918,
      "step": 17343
    },
    {
      "epoch": 6.714672861014325,
      "grad_norm": 1.2889829874038696,
      "learning_rate": 3.650363487761862e-06,
      "loss": 0.0497,
      "step": 17344
    },
    {
      "epoch": 6.7150600077429345,
      "grad_norm": 79.03474426269531,
      "learning_rate": 3.6499333247300727e-06,
      "loss": 1.2575,
      "step": 17345
    },
    {
      "epoch": 6.715447154471545,
      "grad_norm": 41.42071533203125,
      "learning_rate": 3.649503161698284e-06,
      "loss": 1.5917,
      "step": 17346
    },
    {
      "epoch": 6.715834301200155,
      "grad_norm": 3.259929656982422,
      "learning_rate": 3.6490729986664947e-06,
      "loss": 0.0744,
      "step": 17347
    },
    {
      "epoch": 6.716221447928765,
      "grad_norm": 136.7573699951172,
      "learning_rate": 3.648642835634706e-06,
      "loss": 0.4708,
      "step": 17348
    },
    {
      "epoch": 6.716608594657375,
      "grad_norm": 3.5071959495544434,
      "learning_rate": 3.6482126726029167e-06,
      "loss": 0.1117,
      "step": 17349
    },
    {
      "epoch": 6.716995741385985,
      "grad_norm": 48.10734558105469,
      "learning_rate": 3.6477825095711276e-06,
      "loss": 1.8952,
      "step": 17350
    },
    {
      "epoch": 6.717382888114596,
      "grad_norm": 43.35593795776367,
      "learning_rate": 3.6473523465393386e-06,
      "loss": 1.1364,
      "step": 17351
    },
    {
      "epoch": 6.7177700348432055,
      "grad_norm": 39.983158111572266,
      "learning_rate": 3.6469221835075496e-06,
      "loss": 1.4617,
      "step": 17352
    },
    {
      "epoch": 6.718157181571816,
      "grad_norm": 43.08499526977539,
      "learning_rate": 3.64649202047576e-06,
      "loss": 2.9031,
      "step": 17353
    },
    {
      "epoch": 6.718544328300426,
      "grad_norm": 38.0274543762207,
      "learning_rate": 3.6460618574439716e-06,
      "loss": 1.3455,
      "step": 17354
    },
    {
      "epoch": 6.718931475029036,
      "grad_norm": 4.794626712799072,
      "learning_rate": 3.6456316944121826e-06,
      "loss": 0.1503,
      "step": 17355
    },
    {
      "epoch": 6.719318621757646,
      "grad_norm": 84.89065551757812,
      "learning_rate": 3.6452015313803936e-06,
      "loss": 1.975,
      "step": 17356
    },
    {
      "epoch": 6.719705768486256,
      "grad_norm": 17.22208595275879,
      "learning_rate": 3.6447713683486046e-06,
      "loss": 0.1157,
      "step": 17357
    },
    {
      "epoch": 6.720092915214867,
      "grad_norm": 12.139689445495605,
      "learning_rate": 3.644341205316815e-06,
      "loss": 0.1571,
      "step": 17358
    },
    {
      "epoch": 6.720480061943476,
      "grad_norm": 2.522238254547119,
      "learning_rate": 3.6439110422850266e-06,
      "loss": 0.0839,
      "step": 17359
    },
    {
      "epoch": 6.720867208672087,
      "grad_norm": 21.346269607543945,
      "learning_rate": 3.643480879253237e-06,
      "loss": 2.2318,
      "step": 17360
    },
    {
      "epoch": 6.7212543554006965,
      "grad_norm": 14.086750030517578,
      "learning_rate": 3.6430507162214486e-06,
      "loss": 0.1827,
      "step": 17361
    },
    {
      "epoch": 6.721641502129307,
      "grad_norm": 63.40196228027344,
      "learning_rate": 3.642620553189659e-06,
      "loss": 1.2564,
      "step": 17362
    },
    {
      "epoch": 6.7220286488579175,
      "grad_norm": 67.1745834350586,
      "learning_rate": 3.6421903901578705e-06,
      "loss": 1.124,
      "step": 17363
    },
    {
      "epoch": 6.722415795586527,
      "grad_norm": 4.255198001861572,
      "learning_rate": 3.641760227126081e-06,
      "loss": 0.1898,
      "step": 17364
    },
    {
      "epoch": 6.722802942315138,
      "grad_norm": 71.13815307617188,
      "learning_rate": 3.641330064094292e-06,
      "loss": 0.4582,
      "step": 17365
    },
    {
      "epoch": 6.723190089043747,
      "grad_norm": 3.038346529006958,
      "learning_rate": 3.640899901062503e-06,
      "loss": 0.0705,
      "step": 17366
    },
    {
      "epoch": 6.723577235772358,
      "grad_norm": 274.4087829589844,
      "learning_rate": 3.640469738030714e-06,
      "loss": 1.5305,
      "step": 17367
    },
    {
      "epoch": 6.723964382500968,
      "grad_norm": 39.729183197021484,
      "learning_rate": 3.6400395749989247e-06,
      "loss": 0.4298,
      "step": 17368
    },
    {
      "epoch": 6.724351529229578,
      "grad_norm": 73.37715911865234,
      "learning_rate": 3.639609411967136e-06,
      "loss": 0.3459,
      "step": 17369
    },
    {
      "epoch": 6.724738675958188,
      "grad_norm": 99.29782104492188,
      "learning_rate": 3.6391792489353466e-06,
      "loss": 1.4761,
      "step": 17370
    },
    {
      "epoch": 6.725125822686798,
      "grad_norm": 46.61746597290039,
      "learning_rate": 3.638749085903558e-06,
      "loss": 0.4865,
      "step": 17371
    },
    {
      "epoch": 6.7255129694154085,
      "grad_norm": 64.4510269165039,
      "learning_rate": 3.6383189228717686e-06,
      "loss": 1.2949,
      "step": 17372
    },
    {
      "epoch": 6.725900116144018,
      "grad_norm": 30.415891647338867,
      "learning_rate": 3.6378887598399796e-06,
      "loss": 1.0325,
      "step": 17373
    },
    {
      "epoch": 6.726287262872629,
      "grad_norm": 98.2964859008789,
      "learning_rate": 3.6374585968081906e-06,
      "loss": 0.8817,
      "step": 17374
    },
    {
      "epoch": 6.726674409601239,
      "grad_norm": 76.7997817993164,
      "learning_rate": 3.6370284337764016e-06,
      "loss": 0.2076,
      "step": 17375
    },
    {
      "epoch": 6.727061556329849,
      "grad_norm": 9.461277961730957,
      "learning_rate": 3.636598270744612e-06,
      "loss": 0.1737,
      "step": 17376
    },
    {
      "epoch": 6.727448703058459,
      "grad_norm": 8.595588684082031,
      "learning_rate": 3.6361681077128236e-06,
      "loss": 0.0956,
      "step": 17377
    },
    {
      "epoch": 6.727835849787069,
      "grad_norm": 40.9642448425293,
      "learning_rate": 3.635737944681034e-06,
      "loss": 0.7074,
      "step": 17378
    },
    {
      "epoch": 6.7282229965156795,
      "grad_norm": 18.99565887451172,
      "learning_rate": 3.6353077816492456e-06,
      "loss": 1.9146,
      "step": 17379
    },
    {
      "epoch": 6.72861014324429,
      "grad_norm": 83.34979248046875,
      "learning_rate": 3.634877618617456e-06,
      "loss": 2.5257,
      "step": 17380
    },
    {
      "epoch": 6.7289972899729,
      "grad_norm": 97.01792907714844,
      "learning_rate": 3.6344474555856676e-06,
      "loss": 1.4013,
      "step": 17381
    },
    {
      "epoch": 6.72938443670151,
      "grad_norm": 178.69012451171875,
      "learning_rate": 3.634017292553878e-06,
      "loss": 1.3531,
      "step": 17382
    },
    {
      "epoch": 6.72977158343012,
      "grad_norm": 12.655264854431152,
      "learning_rate": 3.633587129522089e-06,
      "loss": 0.1683,
      "step": 17383
    },
    {
      "epoch": 6.73015873015873,
      "grad_norm": 90.68042755126953,
      "learning_rate": 3.6331569664903e-06,
      "loss": 0.6161,
      "step": 17384
    },
    {
      "epoch": 6.730545876887341,
      "grad_norm": 46.23551559448242,
      "learning_rate": 3.632726803458511e-06,
      "loss": 0.5137,
      "step": 17385
    },
    {
      "epoch": 6.73093302361595,
      "grad_norm": 139.78634643554688,
      "learning_rate": 3.6322966404267217e-06,
      "loss": 0.9175,
      "step": 17386
    },
    {
      "epoch": 6.731320170344561,
      "grad_norm": 87.88990783691406,
      "learning_rate": 3.631866477394933e-06,
      "loss": 1.3042,
      "step": 17387
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 114.28050994873047,
      "learning_rate": 3.6314363143631437e-06,
      "loss": 1.0943,
      "step": 17388
    },
    {
      "epoch": 6.732094463801781,
      "grad_norm": 85.42407989501953,
      "learning_rate": 3.631006151331355e-06,
      "loss": 1.4238,
      "step": 17389
    },
    {
      "epoch": 6.732481610530391,
      "grad_norm": 19.56680679321289,
      "learning_rate": 3.6305759882995656e-06,
      "loss": 0.3186,
      "step": 17390
    },
    {
      "epoch": 6.732868757259001,
      "grad_norm": 5.304091453552246,
      "learning_rate": 3.6301458252677766e-06,
      "loss": 0.0682,
      "step": 17391
    },
    {
      "epoch": 6.733255903987612,
      "grad_norm": 81.05848693847656,
      "learning_rate": 3.6297156622359876e-06,
      "loss": 4.3158,
      "step": 17392
    },
    {
      "epoch": 6.733643050716221,
      "grad_norm": 30.22057342529297,
      "learning_rate": 3.6292854992041986e-06,
      "loss": 0.3112,
      "step": 17393
    },
    {
      "epoch": 6.734030197444832,
      "grad_norm": 60.39250946044922,
      "learning_rate": 3.628855336172409e-06,
      "loss": 2.1848,
      "step": 17394
    },
    {
      "epoch": 6.734417344173441,
      "grad_norm": 3.2693421840667725,
      "learning_rate": 3.6284251731406206e-06,
      "loss": 0.1375,
      "step": 17395
    },
    {
      "epoch": 6.734804490902052,
      "grad_norm": 21.92061424255371,
      "learning_rate": 3.627995010108832e-06,
      "loss": 0.1628,
      "step": 17396
    },
    {
      "epoch": 6.7351916376306615,
      "grad_norm": 81.49005126953125,
      "learning_rate": 3.6275648470770426e-06,
      "loss": 2.5258,
      "step": 17397
    },
    {
      "epoch": 6.735578784359272,
      "grad_norm": 3.1224465370178223,
      "learning_rate": 3.6271346840452536e-06,
      "loss": 0.0201,
      "step": 17398
    },
    {
      "epoch": 6.7359659310878826,
      "grad_norm": 1.5672101974487305,
      "learning_rate": 3.6267045210134646e-06,
      "loss": 0.0568,
      "step": 17399
    },
    {
      "epoch": 6.736353077816492,
      "grad_norm": 45.1927490234375,
      "learning_rate": 3.6262743579816756e-06,
      "loss": 1.7781,
      "step": 17400
    },
    {
      "epoch": 6.736740224545103,
      "grad_norm": 93.26976776123047,
      "learning_rate": 3.625844194949886e-06,
      "loss": 1.361,
      "step": 17401
    },
    {
      "epoch": 6.737127371273713,
      "grad_norm": 43.76176834106445,
      "learning_rate": 3.6254140319180976e-06,
      "loss": 2.5459,
      "step": 17402
    },
    {
      "epoch": 6.737514518002323,
      "grad_norm": 85.95924377441406,
      "learning_rate": 3.624983868886308e-06,
      "loss": 1.055,
      "step": 17403
    },
    {
      "epoch": 6.737901664730933,
      "grad_norm": 12.344536781311035,
      "learning_rate": 3.6245537058545195e-06,
      "loss": 0.1073,
      "step": 17404
    },
    {
      "epoch": 6.738288811459543,
      "grad_norm": 54.037200927734375,
      "learning_rate": 3.62412354282273e-06,
      "loss": 0.248,
      "step": 17405
    },
    {
      "epoch": 6.7386759581881535,
      "grad_norm": 64.9955062866211,
      "learning_rate": 3.623693379790941e-06,
      "loss": 0.6231,
      "step": 17406
    },
    {
      "epoch": 6.739063104916763,
      "grad_norm": 57.967811584472656,
      "learning_rate": 3.623263216759152e-06,
      "loss": 3.4354,
      "step": 17407
    },
    {
      "epoch": 6.739450251645374,
      "grad_norm": 2.9978408813476562,
      "learning_rate": 3.622833053727363e-06,
      "loss": 0.1278,
      "step": 17408
    },
    {
      "epoch": 6.739837398373984,
      "grad_norm": 8.635990142822266,
      "learning_rate": 3.6224028906955736e-06,
      "loss": 0.1925,
      "step": 17409
    },
    {
      "epoch": 6.740224545102594,
      "grad_norm": 62.188636779785156,
      "learning_rate": 3.621972727663785e-06,
      "loss": 1.388,
      "step": 17410
    },
    {
      "epoch": 6.740611691831204,
      "grad_norm": 26.90323829650879,
      "learning_rate": 3.6215425646319956e-06,
      "loss": 1.2269,
      "step": 17411
    },
    {
      "epoch": 6.740998838559814,
      "grad_norm": 72.8374252319336,
      "learning_rate": 3.621112401600207e-06,
      "loss": 3.4838,
      "step": 17412
    },
    {
      "epoch": 6.741385985288424,
      "grad_norm": 112.66852569580078,
      "learning_rate": 3.6206822385684176e-06,
      "loss": 1.9304,
      "step": 17413
    },
    {
      "epoch": 6.741773132017034,
      "grad_norm": 1.1864627599716187,
      "learning_rate": 3.620252075536629e-06,
      "loss": 0.0387,
      "step": 17414
    },
    {
      "epoch": 6.7421602787456445,
      "grad_norm": 4.332644939422607,
      "learning_rate": 3.6198219125048396e-06,
      "loss": 0.0953,
      "step": 17415
    },
    {
      "epoch": 6.742547425474255,
      "grad_norm": 4.252901554107666,
      "learning_rate": 3.6193917494730506e-06,
      "loss": 0.1192,
      "step": 17416
    },
    {
      "epoch": 6.742934572202865,
      "grad_norm": 35.382415771484375,
      "learning_rate": 3.6189615864412616e-06,
      "loss": 1.5463,
      "step": 17417
    },
    {
      "epoch": 6.743321718931475,
      "grad_norm": 151.38450622558594,
      "learning_rate": 3.6185314234094726e-06,
      "loss": 0.8119,
      "step": 17418
    },
    {
      "epoch": 6.743708865660086,
      "grad_norm": 1.672682523727417,
      "learning_rate": 3.618101260377683e-06,
      "loss": 0.0567,
      "step": 17419
    },
    {
      "epoch": 6.744096012388695,
      "grad_norm": 164.91383361816406,
      "learning_rate": 3.6176710973458946e-06,
      "loss": 0.8009,
      "step": 17420
    },
    {
      "epoch": 6.744483159117306,
      "grad_norm": 89.28659057617188,
      "learning_rate": 3.617240934314105e-06,
      "loss": 0.3133,
      "step": 17421
    },
    {
      "epoch": 6.744870305845915,
      "grad_norm": 188.0026397705078,
      "learning_rate": 3.6168107712823165e-06,
      "loss": 1.9717,
      "step": 17422
    },
    {
      "epoch": 6.745257452574526,
      "grad_norm": 1.4258064031600952,
      "learning_rate": 3.616380608250527e-06,
      "loss": 0.0203,
      "step": 17423
    },
    {
      "epoch": 6.7456445993031355,
      "grad_norm": 26.941679000854492,
      "learning_rate": 3.615950445218738e-06,
      "loss": 0.4735,
      "step": 17424
    },
    {
      "epoch": 6.746031746031746,
      "grad_norm": 16.96067237854004,
      "learning_rate": 3.615520282186949e-06,
      "loss": 0.1541,
      "step": 17425
    },
    {
      "epoch": 6.746418892760357,
      "grad_norm": 2.5335733890533447,
      "learning_rate": 3.61509011915516e-06,
      "loss": 0.0841,
      "step": 17426
    },
    {
      "epoch": 6.746806039488966,
      "grad_norm": 103.19871520996094,
      "learning_rate": 3.6146599561233707e-06,
      "loss": 0.4673,
      "step": 17427
    },
    {
      "epoch": 6.747193186217577,
      "grad_norm": 55.16454315185547,
      "learning_rate": 3.614229793091582e-06,
      "loss": 1.7419,
      "step": 17428
    },
    {
      "epoch": 6.747580332946186,
      "grad_norm": 1.7229918241500854,
      "learning_rate": 3.6137996300597926e-06,
      "loss": 0.0509,
      "step": 17429
    },
    {
      "epoch": 6.747967479674797,
      "grad_norm": 50.09940719604492,
      "learning_rate": 3.613369467028004e-06,
      "loss": 0.3195,
      "step": 17430
    },
    {
      "epoch": 6.748354626403406,
      "grad_norm": 60.6209602355957,
      "learning_rate": 3.6129393039962146e-06,
      "loss": 0.4903,
      "step": 17431
    },
    {
      "epoch": 6.748741773132017,
      "grad_norm": 34.203224182128906,
      "learning_rate": 3.612509140964426e-06,
      "loss": 1.6262,
      "step": 17432
    },
    {
      "epoch": 6.7491289198606275,
      "grad_norm": 2.7636067867279053,
      "learning_rate": 3.6120789779326366e-06,
      "loss": 0.0662,
      "step": 17433
    },
    {
      "epoch": 6.749516066589237,
      "grad_norm": 9.524238586425781,
      "learning_rate": 3.6116488149008476e-06,
      "loss": 0.1419,
      "step": 17434
    },
    {
      "epoch": 6.749903213317848,
      "grad_norm": 66.728759765625,
      "learning_rate": 3.6112186518690586e-06,
      "loss": 3.8033,
      "step": 17435
    },
    {
      "epoch": 6.750290360046458,
      "grad_norm": 2.5360076427459717,
      "learning_rate": 3.6107884888372696e-06,
      "loss": 0.0902,
      "step": 17436
    },
    {
      "epoch": 6.750677506775068,
      "grad_norm": 47.96660614013672,
      "learning_rate": 3.610358325805481e-06,
      "loss": 1.3838,
      "step": 17437
    },
    {
      "epoch": 6.751064653503678,
      "grad_norm": 22.329273223876953,
      "learning_rate": 3.6099281627736916e-06,
      "loss": 1.8097,
      "step": 17438
    },
    {
      "epoch": 6.751451800232288,
      "grad_norm": 190.3284912109375,
      "learning_rate": 3.6094979997419026e-06,
      "loss": 2.1498,
      "step": 17439
    },
    {
      "epoch": 6.751838946960898,
      "grad_norm": 9.387123107910156,
      "learning_rate": 3.6090678367101136e-06,
      "loss": 0.0796,
      "step": 17440
    },
    {
      "epoch": 6.752226093689508,
      "grad_norm": 17.50397300720215,
      "learning_rate": 3.6086376736783246e-06,
      "loss": 0.2908,
      "step": 17441
    },
    {
      "epoch": 6.7526132404181185,
      "grad_norm": 4.060871601104736,
      "learning_rate": 3.608207510646535e-06,
      "loss": 0.1735,
      "step": 17442
    },
    {
      "epoch": 6.753000387146729,
      "grad_norm": 88.21704864501953,
      "learning_rate": 3.6077773476147465e-06,
      "loss": 1.5391,
      "step": 17443
    },
    {
      "epoch": 6.753387533875339,
      "grad_norm": 76.93965911865234,
      "learning_rate": 3.607347184582957e-06,
      "loss": 0.4043,
      "step": 17444
    },
    {
      "epoch": 6.753774680603949,
      "grad_norm": 56.141624450683594,
      "learning_rate": 3.6069170215511685e-06,
      "loss": 1.7656,
      "step": 17445
    },
    {
      "epoch": 6.754161827332559,
      "grad_norm": 28.35108184814453,
      "learning_rate": 3.606486858519379e-06,
      "loss": 0.3482,
      "step": 17446
    },
    {
      "epoch": 6.754548974061169,
      "grad_norm": 143.69984436035156,
      "learning_rate": 3.6060566954875905e-06,
      "loss": 1.0289,
      "step": 17447
    },
    {
      "epoch": 6.754936120789779,
      "grad_norm": 179.65283203125,
      "learning_rate": 3.605626532455801e-06,
      "loss": 1.1193,
      "step": 17448
    },
    {
      "epoch": 6.755323267518389,
      "grad_norm": 28.502365112304688,
      "learning_rate": 3.605196369424012e-06,
      "loss": 1.9097,
      "step": 17449
    },
    {
      "epoch": 6.755710414247,
      "grad_norm": 40.98151397705078,
      "learning_rate": 3.604766206392223e-06,
      "loss": 2.4066,
      "step": 17450
    },
    {
      "epoch": 6.7560975609756095,
      "grad_norm": 31.314125061035156,
      "learning_rate": 3.604336043360434e-06,
      "loss": 1.6991,
      "step": 17451
    },
    {
      "epoch": 6.75648470770422,
      "grad_norm": 43.3831672668457,
      "learning_rate": 3.6039058803286446e-06,
      "loss": 0.9789,
      "step": 17452
    },
    {
      "epoch": 6.75687185443283,
      "grad_norm": 6.286244869232178,
      "learning_rate": 3.603475717296856e-06,
      "loss": 0.19,
      "step": 17453
    },
    {
      "epoch": 6.75725900116144,
      "grad_norm": 29.134552001953125,
      "learning_rate": 3.6030455542650666e-06,
      "loss": 0.9834,
      "step": 17454
    },
    {
      "epoch": 6.757646147890051,
      "grad_norm": 79.69115447998047,
      "learning_rate": 3.602615391233278e-06,
      "loss": 1.1148,
      "step": 17455
    },
    {
      "epoch": 6.75803329461866,
      "grad_norm": 33.8470573425293,
      "learning_rate": 3.6021852282014886e-06,
      "loss": 0.2117,
      "step": 17456
    },
    {
      "epoch": 6.758420441347271,
      "grad_norm": 44.32381057739258,
      "learning_rate": 3.6017550651696996e-06,
      "loss": 1.3706,
      "step": 17457
    },
    {
      "epoch": 6.7588075880758804,
      "grad_norm": 362.8825988769531,
      "learning_rate": 3.6013249021379106e-06,
      "loss": 0.4276,
      "step": 17458
    },
    {
      "epoch": 6.759194734804491,
      "grad_norm": 1.4600001573562622,
      "learning_rate": 3.6008947391061216e-06,
      "loss": 0.0479,
      "step": 17459
    },
    {
      "epoch": 6.7595818815331015,
      "grad_norm": 0.36705586314201355,
      "learning_rate": 3.600464576074332e-06,
      "loss": 0.0103,
      "step": 17460
    },
    {
      "epoch": 6.759969028261711,
      "grad_norm": 52.702232360839844,
      "learning_rate": 3.6000344130425436e-06,
      "loss": 1.1804,
      "step": 17461
    },
    {
      "epoch": 6.760356174990322,
      "grad_norm": 17.285070419311523,
      "learning_rate": 3.599604250010754e-06,
      "loss": 0.1379,
      "step": 17462
    },
    {
      "epoch": 6.760743321718931,
      "grad_norm": 27.747631072998047,
      "learning_rate": 3.5991740869789655e-06,
      "loss": 0.3189,
      "step": 17463
    },
    {
      "epoch": 6.761130468447542,
      "grad_norm": 4.61076021194458,
      "learning_rate": 3.598743923947176e-06,
      "loss": 0.1808,
      "step": 17464
    },
    {
      "epoch": 6.761517615176151,
      "grad_norm": 93.54168701171875,
      "learning_rate": 3.5983137609153875e-06,
      "loss": 0.3479,
      "step": 17465
    },
    {
      "epoch": 6.761904761904762,
      "grad_norm": 6.885838031768799,
      "learning_rate": 3.597883597883598e-06,
      "loss": 0.0588,
      "step": 17466
    },
    {
      "epoch": 6.762291908633372,
      "grad_norm": 39.84126281738281,
      "learning_rate": 3.597453434851809e-06,
      "loss": 1.6303,
      "step": 17467
    },
    {
      "epoch": 6.762679055361982,
      "grad_norm": 98.2572021484375,
      "learning_rate": 3.59702327182002e-06,
      "loss": 0.8648,
      "step": 17468
    },
    {
      "epoch": 6.7630662020905925,
      "grad_norm": 0.5211208462715149,
      "learning_rate": 3.596593108788231e-06,
      "loss": 0.0131,
      "step": 17469
    },
    {
      "epoch": 6.763453348819202,
      "grad_norm": 41.69879150390625,
      "learning_rate": 3.5961629457564416e-06,
      "loss": 0.5338,
      "step": 17470
    },
    {
      "epoch": 6.763840495547813,
      "grad_norm": 357.72515869140625,
      "learning_rate": 3.595732782724653e-06,
      "loss": 1.251,
      "step": 17471
    },
    {
      "epoch": 6.764227642276423,
      "grad_norm": 119.65816497802734,
      "learning_rate": 3.5953026196928636e-06,
      "loss": 0.6398,
      "step": 17472
    },
    {
      "epoch": 6.764614789005033,
      "grad_norm": 521.8999633789062,
      "learning_rate": 3.594872456661075e-06,
      "loss": 2.8049,
      "step": 17473
    },
    {
      "epoch": 6.765001935733643,
      "grad_norm": 214.8853759765625,
      "learning_rate": 3.5944422936292856e-06,
      "loss": 1.939,
      "step": 17474
    },
    {
      "epoch": 6.765389082462253,
      "grad_norm": 292.3722839355469,
      "learning_rate": 3.5940121305974966e-06,
      "loss": 1.6317,
      "step": 17475
    },
    {
      "epoch": 6.765776229190863,
      "grad_norm": 9.362248420715332,
      "learning_rate": 3.5935819675657076e-06,
      "loss": 0.3411,
      "step": 17476
    },
    {
      "epoch": 6.766163375919474,
      "grad_norm": 114.43817901611328,
      "learning_rate": 3.5931518045339186e-06,
      "loss": 1.0667,
      "step": 17477
    },
    {
      "epoch": 6.7665505226480835,
      "grad_norm": 112.28678894042969,
      "learning_rate": 3.59272164150213e-06,
      "loss": 1.4639,
      "step": 17478
    },
    {
      "epoch": 6.766937669376694,
      "grad_norm": 135.49755859375,
      "learning_rate": 3.5922914784703406e-06,
      "loss": 2.6879,
      "step": 17479
    },
    {
      "epoch": 6.767324816105304,
      "grad_norm": 12.36087417602539,
      "learning_rate": 3.5918613154385516e-06,
      "loss": 0.2675,
      "step": 17480
    },
    {
      "epoch": 6.767711962833914,
      "grad_norm": 63.36980056762695,
      "learning_rate": 3.5914311524067625e-06,
      "loss": 0.9368,
      "step": 17481
    },
    {
      "epoch": 6.768099109562524,
      "grad_norm": 14.695687294006348,
      "learning_rate": 3.5910009893749735e-06,
      "loss": 0.3052,
      "step": 17482
    },
    {
      "epoch": 6.768486256291134,
      "grad_norm": 46.0286750793457,
      "learning_rate": 3.5905708263431845e-06,
      "loss": 1.8149,
      "step": 17483
    },
    {
      "epoch": 6.768873403019745,
      "grad_norm": 27.26968002319336,
      "learning_rate": 3.5901406633113955e-06,
      "loss": 1.6375,
      "step": 17484
    },
    {
      "epoch": 6.7692605497483544,
      "grad_norm": 66.7206802368164,
      "learning_rate": 3.589710500279606e-06,
      "loss": 3.5161,
      "step": 17485
    },
    {
      "epoch": 6.769647696476965,
      "grad_norm": 131.46200561523438,
      "learning_rate": 3.5892803372478175e-06,
      "loss": 2.1017,
      "step": 17486
    },
    {
      "epoch": 6.770034843205575,
      "grad_norm": 11.413740158081055,
      "learning_rate": 3.588850174216028e-06,
      "loss": 0.1242,
      "step": 17487
    },
    {
      "epoch": 6.770421989934185,
      "grad_norm": 38.13248825073242,
      "learning_rate": 3.5884200111842395e-06,
      "loss": 1.752,
      "step": 17488
    },
    {
      "epoch": 6.770809136662795,
      "grad_norm": 21.24696159362793,
      "learning_rate": 3.58798984815245e-06,
      "loss": 0.1503,
      "step": 17489
    },
    {
      "epoch": 6.771196283391405,
      "grad_norm": 0.5687112212181091,
      "learning_rate": 3.587559685120661e-06,
      "loss": 0.011,
      "step": 17490
    },
    {
      "epoch": 6.771583430120016,
      "grad_norm": 85.0757064819336,
      "learning_rate": 3.587129522088872e-06,
      "loss": 0.5707,
      "step": 17491
    },
    {
      "epoch": 6.771970576848625,
      "grad_norm": 60.66273498535156,
      "learning_rate": 3.586699359057083e-06,
      "loss": 2.8169,
      "step": 17492
    },
    {
      "epoch": 6.772357723577236,
      "grad_norm": 55.741493225097656,
      "learning_rate": 3.5862691960252936e-06,
      "loss": 1.5429,
      "step": 17493
    },
    {
      "epoch": 6.772744870305846,
      "grad_norm": 83.26631927490234,
      "learning_rate": 3.585839032993505e-06,
      "loss": 0.8149,
      "step": 17494
    },
    {
      "epoch": 6.773132017034456,
      "grad_norm": 32.17775344848633,
      "learning_rate": 3.5854088699617156e-06,
      "loss": 1.7881,
      "step": 17495
    },
    {
      "epoch": 6.7735191637630665,
      "grad_norm": 44.71854782104492,
      "learning_rate": 3.584978706929927e-06,
      "loss": 1.1716,
      "step": 17496
    },
    {
      "epoch": 6.773906310491676,
      "grad_norm": 52.43443298339844,
      "learning_rate": 3.5845485438981376e-06,
      "loss": 1.5975,
      "step": 17497
    },
    {
      "epoch": 6.774293457220287,
      "grad_norm": 75.1585464477539,
      "learning_rate": 3.5841183808663486e-06,
      "loss": 1.7077,
      "step": 17498
    },
    {
      "epoch": 6.774680603948896,
      "grad_norm": 45.17436218261719,
      "learning_rate": 3.5836882178345596e-06,
      "loss": 3.1221,
      "step": 17499
    },
    {
      "epoch": 6.775067750677507,
      "grad_norm": 129.30514526367188,
      "learning_rate": 3.5832580548027706e-06,
      "loss": 1.9849,
      "step": 17500
    },
    {
      "epoch": 6.775454897406117,
      "grad_norm": 54.90906524658203,
      "learning_rate": 3.5828278917709815e-06,
      "loss": 2.2982,
      "step": 17501
    },
    {
      "epoch": 6.775842044134727,
      "grad_norm": 68.64875793457031,
      "learning_rate": 3.5823977287391925e-06,
      "loss": 1.7238,
      "step": 17502
    },
    {
      "epoch": 6.776229190863337,
      "grad_norm": 55.89289474487305,
      "learning_rate": 3.581967565707403e-06,
      "loss": 1.8674,
      "step": 17503
    },
    {
      "epoch": 6.776616337591947,
      "grad_norm": 155.0858154296875,
      "learning_rate": 3.5815374026756145e-06,
      "loss": 2.8406,
      "step": 17504
    },
    {
      "epoch": 6.7770034843205575,
      "grad_norm": 9.966005325317383,
      "learning_rate": 3.581107239643825e-06,
      "loss": 0.093,
      "step": 17505
    },
    {
      "epoch": 6.777390631049167,
      "grad_norm": 3.4682445526123047,
      "learning_rate": 3.5806770766120365e-06,
      "loss": 0.1379,
      "step": 17506
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 34.017513275146484,
      "learning_rate": 3.580246913580247e-06,
      "loss": 1.2275,
      "step": 17507
    },
    {
      "epoch": 6.778164924506388,
      "grad_norm": 67.83641815185547,
      "learning_rate": 3.579816750548458e-06,
      "loss": 0.389,
      "step": 17508
    },
    {
      "epoch": 6.778552071234998,
      "grad_norm": 123.91637420654297,
      "learning_rate": 3.579386587516669e-06,
      "loss": 0.9787,
      "step": 17509
    },
    {
      "epoch": 6.778939217963608,
      "grad_norm": 62.830543518066406,
      "learning_rate": 3.57895642448488e-06,
      "loss": 1.1217,
      "step": 17510
    },
    {
      "epoch": 6.779326364692219,
      "grad_norm": 81.33712768554688,
      "learning_rate": 3.5785262614530906e-06,
      "loss": 1.0323,
      "step": 17511
    },
    {
      "epoch": 6.7797135114208285,
      "grad_norm": 62.22770690917969,
      "learning_rate": 3.578096098421302e-06,
      "loss": 1.617,
      "step": 17512
    },
    {
      "epoch": 6.780100658149439,
      "grad_norm": 72.30701446533203,
      "learning_rate": 3.5776659353895126e-06,
      "loss": 0.8142,
      "step": 17513
    },
    {
      "epoch": 6.780487804878049,
      "grad_norm": 0.6264483332633972,
      "learning_rate": 3.577235772357724e-06,
      "loss": 0.0133,
      "step": 17514
    },
    {
      "epoch": 6.780874951606659,
      "grad_norm": 26.6646671295166,
      "learning_rate": 3.5768056093259346e-06,
      "loss": 1.9368,
      "step": 17515
    },
    {
      "epoch": 6.781262098335269,
      "grad_norm": 34.96747970581055,
      "learning_rate": 3.5763754462941456e-06,
      "loss": 0.3458,
      "step": 17516
    },
    {
      "epoch": 6.781649245063879,
      "grad_norm": 73.7088394165039,
      "learning_rate": 3.5759452832623566e-06,
      "loss": 1.242,
      "step": 17517
    },
    {
      "epoch": 6.78203639179249,
      "grad_norm": 38.5759162902832,
      "learning_rate": 3.5755151202305676e-06,
      "loss": 0.2315,
      "step": 17518
    },
    {
      "epoch": 6.782423538521099,
      "grad_norm": 45.02414321899414,
      "learning_rate": 3.575084957198779e-06,
      "loss": 0.2597,
      "step": 17519
    },
    {
      "epoch": 6.78281068524971,
      "grad_norm": 140.4026641845703,
      "learning_rate": 3.5746547941669896e-06,
      "loss": 0.6358,
      "step": 17520
    },
    {
      "epoch": 6.7831978319783195,
      "grad_norm": 101.00083923339844,
      "learning_rate": 3.574224631135201e-06,
      "loss": 1.0784,
      "step": 17521
    },
    {
      "epoch": 6.78358497870693,
      "grad_norm": 81.45516204833984,
      "learning_rate": 3.5737944681034115e-06,
      "loss": 3.5187,
      "step": 17522
    },
    {
      "epoch": 6.78397212543554,
      "grad_norm": 104.4319076538086,
      "learning_rate": 3.5733643050716225e-06,
      "loss": 2.2224,
      "step": 17523
    },
    {
      "epoch": 6.78435927216415,
      "grad_norm": 75.06554412841797,
      "learning_rate": 3.5729341420398335e-06,
      "loss": 0.6588,
      "step": 17524
    },
    {
      "epoch": 6.784746418892761,
      "grad_norm": 110.49868774414062,
      "learning_rate": 3.5725039790080445e-06,
      "loss": 3.7805,
      "step": 17525
    },
    {
      "epoch": 6.78513356562137,
      "grad_norm": 1.3079086542129517,
      "learning_rate": 3.572073815976255e-06,
      "loss": 0.0501,
      "step": 17526
    },
    {
      "epoch": 6.785520712349981,
      "grad_norm": 99.78087615966797,
      "learning_rate": 3.5716436529444665e-06,
      "loss": 2.0978,
      "step": 17527
    },
    {
      "epoch": 6.78590785907859,
      "grad_norm": 74.76128387451172,
      "learning_rate": 3.571213489912677e-06,
      "loss": 1.4593,
      "step": 17528
    },
    {
      "epoch": 6.786295005807201,
      "grad_norm": 117.0319595336914,
      "learning_rate": 3.5707833268808885e-06,
      "loss": 1.3135,
      "step": 17529
    },
    {
      "epoch": 6.786682152535811,
      "grad_norm": 41.356624603271484,
      "learning_rate": 3.570353163849099e-06,
      "loss": 2.1273,
      "step": 17530
    },
    {
      "epoch": 6.787069299264421,
      "grad_norm": 3.0321414470672607,
      "learning_rate": 3.56992300081731e-06,
      "loss": 0.0902,
      "step": 17531
    },
    {
      "epoch": 6.7874564459930316,
      "grad_norm": 114.37348937988281,
      "learning_rate": 3.569492837785521e-06,
      "loss": 1.5144,
      "step": 17532
    },
    {
      "epoch": 6.787843592721641,
      "grad_norm": 18.185810089111328,
      "learning_rate": 3.569062674753732e-06,
      "loss": 1.76,
      "step": 17533
    },
    {
      "epoch": 6.788230739450252,
      "grad_norm": 75.04122924804688,
      "learning_rate": 3.5686325117219426e-06,
      "loss": 1.188,
      "step": 17534
    },
    {
      "epoch": 6.788617886178862,
      "grad_norm": 3.1511316299438477,
      "learning_rate": 3.568202348690154e-06,
      "loss": 0.1345,
      "step": 17535
    },
    {
      "epoch": 6.789005032907472,
      "grad_norm": 60.883201599121094,
      "learning_rate": 3.5677721856583646e-06,
      "loss": 1.906,
      "step": 17536
    },
    {
      "epoch": 6.789392179636082,
      "grad_norm": 12.444422721862793,
      "learning_rate": 3.567342022626576e-06,
      "loss": 0.2095,
      "step": 17537
    },
    {
      "epoch": 6.789779326364692,
      "grad_norm": 56.68727111816406,
      "learning_rate": 3.5669118595947866e-06,
      "loss": 1.3955,
      "step": 17538
    },
    {
      "epoch": 6.7901664730933025,
      "grad_norm": 59.538028717041016,
      "learning_rate": 3.566481696562998e-06,
      "loss": 1.4997,
      "step": 17539
    },
    {
      "epoch": 6.790553619821912,
      "grad_norm": 113.11560821533203,
      "learning_rate": 3.5660515335312085e-06,
      "loss": 1.2775,
      "step": 17540
    },
    {
      "epoch": 6.790940766550523,
      "grad_norm": 33.03897476196289,
      "learning_rate": 3.5656213704994195e-06,
      "loss": 1.3366,
      "step": 17541
    },
    {
      "epoch": 6.791327913279133,
      "grad_norm": 181.7487030029297,
      "learning_rate": 3.5651912074676305e-06,
      "loss": 1.2391,
      "step": 17542
    },
    {
      "epoch": 6.791715060007743,
      "grad_norm": 100.19873046875,
      "learning_rate": 3.5647610444358415e-06,
      "loss": 0.9757,
      "step": 17543
    },
    {
      "epoch": 6.792102206736353,
      "grad_norm": 73.91351318359375,
      "learning_rate": 3.564330881404052e-06,
      "loss": 0.6218,
      "step": 17544
    },
    {
      "epoch": 6.792489353464963,
      "grad_norm": 54.66936492919922,
      "learning_rate": 3.5639007183722635e-06,
      "loss": 0.8648,
      "step": 17545
    },
    {
      "epoch": 6.792876500193573,
      "grad_norm": 0.5520467162132263,
      "learning_rate": 3.563470555340474e-06,
      "loss": 0.015,
      "step": 17546
    },
    {
      "epoch": 6.793263646922184,
      "grad_norm": 108.38268280029297,
      "learning_rate": 3.5630403923086855e-06,
      "loss": 2.9447,
      "step": 17547
    },
    {
      "epoch": 6.7936507936507935,
      "grad_norm": 116.1023178100586,
      "learning_rate": 3.562610229276896e-06,
      "loss": 2.6762,
      "step": 17548
    },
    {
      "epoch": 6.794037940379404,
      "grad_norm": 29.897541046142578,
      "learning_rate": 3.562180066245107e-06,
      "loss": 0.5943,
      "step": 17549
    },
    {
      "epoch": 6.794425087108014,
      "grad_norm": 121.03263854980469,
      "learning_rate": 3.561749903213318e-06,
      "loss": 1.344,
      "step": 17550
    },
    {
      "epoch": 6.794812233836624,
      "grad_norm": 3.5604398250579834,
      "learning_rate": 3.561319740181529e-06,
      "loss": 0.139,
      "step": 17551
    },
    {
      "epoch": 6.795199380565235,
      "grad_norm": 91.8807373046875,
      "learning_rate": 3.5608895771497396e-06,
      "loss": 0.9879,
      "step": 17552
    },
    {
      "epoch": 6.795586527293844,
      "grad_norm": 0.43884211778640747,
      "learning_rate": 3.560459414117951e-06,
      "loss": 0.0108,
      "step": 17553
    },
    {
      "epoch": 6.795973674022455,
      "grad_norm": 52.75946044921875,
      "learning_rate": 3.5600292510861616e-06,
      "loss": 1.0013,
      "step": 17554
    },
    {
      "epoch": 6.796360820751064,
      "grad_norm": 5.845742702484131,
      "learning_rate": 3.559599088054373e-06,
      "loss": 0.1835,
      "step": 17555
    },
    {
      "epoch": 6.796747967479675,
      "grad_norm": 10.287483215332031,
      "learning_rate": 3.5591689250225836e-06,
      "loss": 0.4818,
      "step": 17556
    },
    {
      "epoch": 6.7971351142082845,
      "grad_norm": 60.574485778808594,
      "learning_rate": 3.558738761990795e-06,
      "loss": 2.351,
      "step": 17557
    },
    {
      "epoch": 6.797522260936895,
      "grad_norm": 16.703628540039062,
      "learning_rate": 3.5583085989590056e-06,
      "loss": 1.221,
      "step": 17558
    },
    {
      "epoch": 6.7979094076655056,
      "grad_norm": 73.09281158447266,
      "learning_rate": 3.5578784359272166e-06,
      "loss": 2.5671,
      "step": 17559
    },
    {
      "epoch": 6.798296554394115,
      "grad_norm": 49.750423431396484,
      "learning_rate": 3.557448272895428e-06,
      "loss": 2.6753,
      "step": 17560
    },
    {
      "epoch": 6.798683701122726,
      "grad_norm": 66.17726135253906,
      "learning_rate": 3.5570181098636385e-06,
      "loss": 0.8122,
      "step": 17561
    },
    {
      "epoch": 6.799070847851335,
      "grad_norm": 65.45787048339844,
      "learning_rate": 3.55658794683185e-06,
      "loss": 1.3727,
      "step": 17562
    },
    {
      "epoch": 6.799457994579946,
      "grad_norm": 24.021547317504883,
      "learning_rate": 3.5561577838000605e-06,
      "loss": 0.1708,
      "step": 17563
    },
    {
      "epoch": 6.799845141308556,
      "grad_norm": 136.1326141357422,
      "learning_rate": 3.5557276207682715e-06,
      "loss": 2.7218,
      "step": 17564
    },
    {
      "epoch": 6.800232288037166,
      "grad_norm": 64.44048309326172,
      "learning_rate": 3.5552974577364825e-06,
      "loss": 0.6282,
      "step": 17565
    },
    {
      "epoch": 6.8006194347657765,
      "grad_norm": 80.2757797241211,
      "learning_rate": 3.5548672947046935e-06,
      "loss": 2.642,
      "step": 17566
    },
    {
      "epoch": 6.801006581494386,
      "grad_norm": 4.440354824066162,
      "learning_rate": 3.554437131672904e-06,
      "loss": 0.091,
      "step": 17567
    },
    {
      "epoch": 6.801393728222997,
      "grad_norm": 29.862064361572266,
      "learning_rate": 3.5540069686411155e-06,
      "loss": 1.611,
      "step": 17568
    },
    {
      "epoch": 6.801780874951607,
      "grad_norm": 143.1516571044922,
      "learning_rate": 3.553576805609326e-06,
      "loss": 1.8859,
      "step": 17569
    },
    {
      "epoch": 6.802168021680217,
      "grad_norm": 160.76611328125,
      "learning_rate": 3.5531466425775375e-06,
      "loss": 3.014,
      "step": 17570
    },
    {
      "epoch": 6.802555168408827,
      "grad_norm": 63.05644989013672,
      "learning_rate": 3.552716479545748e-06,
      "loss": 0.3386,
      "step": 17571
    },
    {
      "epoch": 6.802942315137437,
      "grad_norm": 46.60236358642578,
      "learning_rate": 3.5522863165139595e-06,
      "loss": 0.1862,
      "step": 17572
    },
    {
      "epoch": 6.803329461866047,
      "grad_norm": 37.36014938354492,
      "learning_rate": 3.55185615348217e-06,
      "loss": 0.9024,
      "step": 17573
    },
    {
      "epoch": 6.803716608594657,
      "grad_norm": 13.725850105285645,
      "learning_rate": 3.551425990450381e-06,
      "loss": 0.2313,
      "step": 17574
    },
    {
      "epoch": 6.8041037553232675,
      "grad_norm": 31.31415557861328,
      "learning_rate": 3.550995827418592e-06,
      "loss": 1.5213,
      "step": 17575
    },
    {
      "epoch": 6.804490902051878,
      "grad_norm": 19.950056076049805,
      "learning_rate": 3.550565664386803e-06,
      "loss": 0.2392,
      "step": 17576
    },
    {
      "epoch": 6.804878048780488,
      "grad_norm": 1.781446099281311,
      "learning_rate": 3.5501355013550136e-06,
      "loss": 0.0484,
      "step": 17577
    },
    {
      "epoch": 6.805265195509098,
      "grad_norm": 58.15421676635742,
      "learning_rate": 3.549705338323225e-06,
      "loss": 2.6866,
      "step": 17578
    },
    {
      "epoch": 6.805652342237708,
      "grad_norm": 28.997655868530273,
      "learning_rate": 3.5492751752914356e-06,
      "loss": 2.4823,
      "step": 17579
    },
    {
      "epoch": 6.806039488966318,
      "grad_norm": 23.27159881591797,
      "learning_rate": 3.548845012259647e-06,
      "loss": 0.2581,
      "step": 17580
    },
    {
      "epoch": 6.806426635694928,
      "grad_norm": 114.43865203857422,
      "learning_rate": 3.5484148492278575e-06,
      "loss": 2.1198,
      "step": 17581
    },
    {
      "epoch": 6.806813782423538,
      "grad_norm": 4.313918113708496,
      "learning_rate": 3.5479846861960685e-06,
      "loss": 0.1751,
      "step": 17582
    },
    {
      "epoch": 6.807200929152149,
      "grad_norm": 1.5582275390625,
      "learning_rate": 3.5475545231642795e-06,
      "loss": 0.0681,
      "step": 17583
    },
    {
      "epoch": 6.8075880758807585,
      "grad_norm": 43.518009185791016,
      "learning_rate": 3.5471243601324905e-06,
      "loss": 0.8819,
      "step": 17584
    },
    {
      "epoch": 6.807975222609369,
      "grad_norm": 65.11880493164062,
      "learning_rate": 3.546694197100701e-06,
      "loss": 0.6999,
      "step": 17585
    },
    {
      "epoch": 6.80836236933798,
      "grad_norm": 0.32174479961395264,
      "learning_rate": 3.5462640340689125e-06,
      "loss": 0.0094,
      "step": 17586
    },
    {
      "epoch": 6.808749516066589,
      "grad_norm": 41.596920013427734,
      "learning_rate": 3.545833871037123e-06,
      "loss": 1.2788,
      "step": 17587
    },
    {
      "epoch": 6.8091366627952,
      "grad_norm": 95.34912109375,
      "learning_rate": 3.5454037080053345e-06,
      "loss": 1.398,
      "step": 17588
    },
    {
      "epoch": 6.809523809523809,
      "grad_norm": 81.33643341064453,
      "learning_rate": 3.544973544973545e-06,
      "loss": 1.2955,
      "step": 17589
    },
    {
      "epoch": 6.80991095625242,
      "grad_norm": 252.92404174804688,
      "learning_rate": 3.5445433819417565e-06,
      "loss": 0.986,
      "step": 17590
    },
    {
      "epoch": 6.8102981029810294,
      "grad_norm": 170.13980102539062,
      "learning_rate": 3.544113218909967e-06,
      "loss": 0.4187,
      "step": 17591
    },
    {
      "epoch": 6.81068524970964,
      "grad_norm": 31.461162567138672,
      "learning_rate": 3.543683055878178e-06,
      "loss": 0.1632,
      "step": 17592
    },
    {
      "epoch": 6.8110723964382505,
      "grad_norm": 25.807893753051758,
      "learning_rate": 3.543252892846389e-06,
      "loss": 2.009,
      "step": 17593
    },
    {
      "epoch": 6.81145954316686,
      "grad_norm": 62.53872299194336,
      "learning_rate": 3.5428227298146e-06,
      "loss": 1.9067,
      "step": 17594
    },
    {
      "epoch": 6.811846689895471,
      "grad_norm": 53.659217834472656,
      "learning_rate": 3.5423925667828106e-06,
      "loss": 1.3319,
      "step": 17595
    },
    {
      "epoch": 6.81223383662408,
      "grad_norm": 1.485720157623291,
      "learning_rate": 3.541962403751022e-06,
      "loss": 0.0647,
      "step": 17596
    },
    {
      "epoch": 6.812620983352691,
      "grad_norm": 84.76750946044922,
      "learning_rate": 3.5415322407192326e-06,
      "loss": 1.1052,
      "step": 17597
    },
    {
      "epoch": 6.8130081300813,
      "grad_norm": 8.209709167480469,
      "learning_rate": 3.541102077687444e-06,
      "loss": 0.1347,
      "step": 17598
    },
    {
      "epoch": 6.813395276809911,
      "grad_norm": 178.5352783203125,
      "learning_rate": 3.5406719146556545e-06,
      "loss": 4.3187,
      "step": 17599
    },
    {
      "epoch": 6.813782423538521,
      "grad_norm": 61.65618896484375,
      "learning_rate": 3.5402417516238655e-06,
      "loss": 0.446,
      "step": 17600
    },
    {
      "epoch": 6.814169570267131,
      "grad_norm": 0.8545629382133484,
      "learning_rate": 3.539811588592077e-06,
      "loss": 0.0303,
      "step": 17601
    },
    {
      "epoch": 6.8145567169957415,
      "grad_norm": 164.50613403320312,
      "learning_rate": 3.5393814255602875e-06,
      "loss": 3.4346,
      "step": 17602
    },
    {
      "epoch": 6.814943863724352,
      "grad_norm": 43.22678756713867,
      "learning_rate": 3.538951262528499e-06,
      "loss": 0.2393,
      "step": 17603
    },
    {
      "epoch": 6.815331010452962,
      "grad_norm": 82.18109130859375,
      "learning_rate": 3.5385210994967095e-06,
      "loss": 1.6998,
      "step": 17604
    },
    {
      "epoch": 6.815718157181572,
      "grad_norm": 37.99620056152344,
      "learning_rate": 3.538090936464921e-06,
      "loss": 1.7817,
      "step": 17605
    },
    {
      "epoch": 6.816105303910182,
      "grad_norm": 83.73548889160156,
      "learning_rate": 3.5376607734331315e-06,
      "loss": 1.0828,
      "step": 17606
    },
    {
      "epoch": 6.816492450638792,
      "grad_norm": 75.8106460571289,
      "learning_rate": 3.5372306104013425e-06,
      "loss": 0.8924,
      "step": 17607
    },
    {
      "epoch": 6.816879597367402,
      "grad_norm": 69.01390075683594,
      "learning_rate": 3.5368004473695535e-06,
      "loss": 1.1299,
      "step": 17608
    },
    {
      "epoch": 6.817266744096012,
      "grad_norm": 1.6005555391311646,
      "learning_rate": 3.5363702843377645e-06,
      "loss": 0.0482,
      "step": 17609
    },
    {
      "epoch": 6.817653890824623,
      "grad_norm": 7.068851470947266,
      "learning_rate": 3.535940121305975e-06,
      "loss": 0.1475,
      "step": 17610
    },
    {
      "epoch": 6.8180410375532325,
      "grad_norm": 2.4974794387817383,
      "learning_rate": 3.5355099582741865e-06,
      "loss": 0.1116,
      "step": 17611
    },
    {
      "epoch": 6.818428184281843,
      "grad_norm": 44.67329406738281,
      "learning_rate": 3.535079795242397e-06,
      "loss": 1.9812,
      "step": 17612
    },
    {
      "epoch": 6.818815331010453,
      "grad_norm": 95.99407196044922,
      "learning_rate": 3.5346496322106084e-06,
      "loss": 2.0817,
      "step": 17613
    },
    {
      "epoch": 6.819202477739063,
      "grad_norm": 135.51126098632812,
      "learning_rate": 3.534219469178819e-06,
      "loss": 2.3035,
      "step": 17614
    },
    {
      "epoch": 6.819589624467673,
      "grad_norm": 95.91214752197266,
      "learning_rate": 3.53378930614703e-06,
      "loss": 0.5805,
      "step": 17615
    },
    {
      "epoch": 6.819976771196283,
      "grad_norm": 14.052467346191406,
      "learning_rate": 3.533359143115241e-06,
      "loss": 0.3485,
      "step": 17616
    },
    {
      "epoch": 6.820363917924894,
      "grad_norm": 68.44277954101562,
      "learning_rate": 3.532928980083452e-06,
      "loss": 1.6886,
      "step": 17617
    },
    {
      "epoch": 6.8207510646535034,
      "grad_norm": 3.7768023014068604,
      "learning_rate": 3.5324988170516626e-06,
      "loss": 0.0911,
      "step": 17618
    },
    {
      "epoch": 6.821138211382114,
      "grad_norm": 71.0859375,
      "learning_rate": 3.532068654019874e-06,
      "loss": 2.4395,
      "step": 17619
    },
    {
      "epoch": 6.821525358110724,
      "grad_norm": 142.5010223388672,
      "learning_rate": 3.5316384909880845e-06,
      "loss": 3.4537,
      "step": 17620
    },
    {
      "epoch": 6.821912504839334,
      "grad_norm": 23.01165199279785,
      "learning_rate": 3.531208327956296e-06,
      "loss": 0.1485,
      "step": 17621
    },
    {
      "epoch": 6.822299651567945,
      "grad_norm": 121.53657531738281,
      "learning_rate": 3.5307781649245065e-06,
      "loss": 0.8179,
      "step": 17622
    },
    {
      "epoch": 6.822686798296554,
      "grad_norm": 29.931495666503906,
      "learning_rate": 3.530348001892718e-06,
      "loss": 0.941,
      "step": 17623
    },
    {
      "epoch": 6.823073945025165,
      "grad_norm": 80.36004638671875,
      "learning_rate": 3.5299178388609285e-06,
      "loss": 2.3577,
      "step": 17624
    },
    {
      "epoch": 6.823461091753774,
      "grad_norm": 46.108306884765625,
      "learning_rate": 3.5294876758291395e-06,
      "loss": 0.4127,
      "step": 17625
    },
    {
      "epoch": 6.823848238482385,
      "grad_norm": 86.42865753173828,
      "learning_rate": 3.5290575127973505e-06,
      "loss": 0.986,
      "step": 17626
    },
    {
      "epoch": 6.824235385210995,
      "grad_norm": 113.76649475097656,
      "learning_rate": 3.5286273497655615e-06,
      "loss": 1.5874,
      "step": 17627
    },
    {
      "epoch": 6.824622531939605,
      "grad_norm": 5.423168659210205,
      "learning_rate": 3.528197186733772e-06,
      "loss": 0.2261,
      "step": 17628
    },
    {
      "epoch": 6.8250096786682155,
      "grad_norm": 129.74359130859375,
      "learning_rate": 3.5277670237019835e-06,
      "loss": 3.8439,
      "step": 17629
    },
    {
      "epoch": 6.825396825396825,
      "grad_norm": 26.053001403808594,
      "learning_rate": 3.527336860670194e-06,
      "loss": 2.1114,
      "step": 17630
    },
    {
      "epoch": 6.825783972125436,
      "grad_norm": 120.7295150756836,
      "learning_rate": 3.5269066976384055e-06,
      "loss": 1.7992,
      "step": 17631
    },
    {
      "epoch": 6.826171118854045,
      "grad_norm": 2.13836669921875,
      "learning_rate": 3.526476534606616e-06,
      "loss": 0.063,
      "step": 17632
    },
    {
      "epoch": 6.826558265582656,
      "grad_norm": 67.27528381347656,
      "learning_rate": 3.526046371574827e-06,
      "loss": 1.629,
      "step": 17633
    },
    {
      "epoch": 6.826945412311266,
      "grad_norm": 32.749359130859375,
      "learning_rate": 3.525616208543038e-06,
      "loss": 0.5945,
      "step": 17634
    },
    {
      "epoch": 6.827332559039876,
      "grad_norm": 139.0722198486328,
      "learning_rate": 3.525186045511249e-06,
      "loss": 2.1978,
      "step": 17635
    },
    {
      "epoch": 6.827719705768486,
      "grad_norm": 65.7649154663086,
      "learning_rate": 3.5247558824794596e-06,
      "loss": 0.9972,
      "step": 17636
    },
    {
      "epoch": 6.828106852497096,
      "grad_norm": 100.85055541992188,
      "learning_rate": 3.524325719447671e-06,
      "loss": 1.3735,
      "step": 17637
    },
    {
      "epoch": 6.8284939992257065,
      "grad_norm": 0.3237970173358917,
      "learning_rate": 3.5238955564158816e-06,
      "loss": 0.0092,
      "step": 17638
    },
    {
      "epoch": 6.828881145954317,
      "grad_norm": 18.04633331298828,
      "learning_rate": 3.523465393384093e-06,
      "loss": 1.1543,
      "step": 17639
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 1.5304818153381348,
      "learning_rate": 3.5230352303523035e-06,
      "loss": 0.043,
      "step": 17640
    },
    {
      "epoch": 6.829655439411537,
      "grad_norm": 9.483353614807129,
      "learning_rate": 3.522605067320515e-06,
      "loss": 0.0958,
      "step": 17641
    },
    {
      "epoch": 6.830042586140147,
      "grad_norm": 45.03661346435547,
      "learning_rate": 3.5221749042887255e-06,
      "loss": 1.5837,
      "step": 17642
    },
    {
      "epoch": 6.830429732868757,
      "grad_norm": 109.33319091796875,
      "learning_rate": 3.5217447412569365e-06,
      "loss": 3.1408,
      "step": 17643
    },
    {
      "epoch": 6.830816879597368,
      "grad_norm": 137.62701416015625,
      "learning_rate": 3.521314578225148e-06,
      "loss": 1.0928,
      "step": 17644
    },
    {
      "epoch": 6.8312040263259775,
      "grad_norm": 93.13899993896484,
      "learning_rate": 3.5208844151933585e-06,
      "loss": 2.0214,
      "step": 17645
    },
    {
      "epoch": 6.831591173054588,
      "grad_norm": 111.0959701538086,
      "learning_rate": 3.52045425216157e-06,
      "loss": 2.0716,
      "step": 17646
    },
    {
      "epoch": 6.831978319783198,
      "grad_norm": 1.6910278797149658,
      "learning_rate": 3.5200240891297805e-06,
      "loss": 0.0394,
      "step": 17647
    },
    {
      "epoch": 6.832365466511808,
      "grad_norm": 116.64506530761719,
      "learning_rate": 3.5195939260979915e-06,
      "loss": 1.4454,
      "step": 17648
    },
    {
      "epoch": 6.832752613240418,
      "grad_norm": 301.9859924316406,
      "learning_rate": 3.5191637630662025e-06,
      "loss": 1.1125,
      "step": 17649
    },
    {
      "epoch": 6.833139759969028,
      "grad_norm": 1.1814759969711304,
      "learning_rate": 3.5187336000344135e-06,
      "loss": 0.0392,
      "step": 17650
    },
    {
      "epoch": 6.833526906697639,
      "grad_norm": 20.502273559570312,
      "learning_rate": 3.518303437002624e-06,
      "loss": 0.1639,
      "step": 17651
    },
    {
      "epoch": 6.833914053426248,
      "grad_norm": 40.44293975830078,
      "learning_rate": 3.5178732739708354e-06,
      "loss": 0.861,
      "step": 17652
    },
    {
      "epoch": 6.834301200154859,
      "grad_norm": 62.95201110839844,
      "learning_rate": 3.517443110939046e-06,
      "loss": 0.3399,
      "step": 17653
    },
    {
      "epoch": 6.8346883468834685,
      "grad_norm": 39.9498405456543,
      "learning_rate": 3.5170129479072574e-06,
      "loss": 0.2602,
      "step": 17654
    },
    {
      "epoch": 6.835075493612079,
      "grad_norm": 37.82442855834961,
      "learning_rate": 3.516582784875468e-06,
      "loss": 2.1965,
      "step": 17655
    },
    {
      "epoch": 6.835462640340689,
      "grad_norm": 1.564161777496338,
      "learning_rate": 3.5161526218436794e-06,
      "loss": 0.0462,
      "step": 17656
    },
    {
      "epoch": 6.835849787069299,
      "grad_norm": 96.05986785888672,
      "learning_rate": 3.51572245881189e-06,
      "loss": 1.2521,
      "step": 17657
    },
    {
      "epoch": 6.83623693379791,
      "grad_norm": 36.779823303222656,
      "learning_rate": 3.515292295780101e-06,
      "loss": 1.6495,
      "step": 17658
    },
    {
      "epoch": 6.836624080526519,
      "grad_norm": 41.104373931884766,
      "learning_rate": 3.514862132748312e-06,
      "loss": 1.7906,
      "step": 17659
    },
    {
      "epoch": 6.83701122725513,
      "grad_norm": 25.989614486694336,
      "learning_rate": 3.514431969716523e-06,
      "loss": 1.9593,
      "step": 17660
    },
    {
      "epoch": 6.83739837398374,
      "grad_norm": 2.395686626434326,
      "learning_rate": 3.5140018066847335e-06,
      "loss": 0.1065,
      "step": 17661
    },
    {
      "epoch": 6.83778552071235,
      "grad_norm": 130.36752319335938,
      "learning_rate": 3.513571643652945e-06,
      "loss": 1.244,
      "step": 17662
    },
    {
      "epoch": 6.83817266744096,
      "grad_norm": 68.49971771240234,
      "learning_rate": 3.5131414806211555e-06,
      "loss": 1.2342,
      "step": 17663
    },
    {
      "epoch": 6.83855981416957,
      "grad_norm": 58.484596252441406,
      "learning_rate": 3.512711317589367e-06,
      "loss": 2.2322,
      "step": 17664
    },
    {
      "epoch": 6.8389469608981805,
      "grad_norm": 6.113955497741699,
      "learning_rate": 3.5122811545575775e-06,
      "loss": 0.203,
      "step": 17665
    },
    {
      "epoch": 6.83933410762679,
      "grad_norm": 34.82466125488281,
      "learning_rate": 3.5118509915257885e-06,
      "loss": 1.8736,
      "step": 17666
    },
    {
      "epoch": 6.839721254355401,
      "grad_norm": 73.05782318115234,
      "learning_rate": 3.5114208284939995e-06,
      "loss": 1.6753,
      "step": 17667
    },
    {
      "epoch": 6.840108401084011,
      "grad_norm": 70.93805694580078,
      "learning_rate": 3.5109906654622105e-06,
      "loss": 0.5092,
      "step": 17668
    },
    {
      "epoch": 6.840495547812621,
      "grad_norm": 80.60826110839844,
      "learning_rate": 3.510560502430421e-06,
      "loss": 1.1025,
      "step": 17669
    },
    {
      "epoch": 6.840882694541231,
      "grad_norm": 27.359397888183594,
      "learning_rate": 3.5101303393986325e-06,
      "loss": 0.284,
      "step": 17670
    },
    {
      "epoch": 6.841269841269841,
      "grad_norm": 171.12130737304688,
      "learning_rate": 3.509700176366843e-06,
      "loss": 1.2531,
      "step": 17671
    },
    {
      "epoch": 6.8416569879984515,
      "grad_norm": 36.96753692626953,
      "learning_rate": 3.5092700133350544e-06,
      "loss": 1.4885,
      "step": 17672
    },
    {
      "epoch": 6.842044134727061,
      "grad_norm": 163.85104370117188,
      "learning_rate": 3.508839850303265e-06,
      "loss": 3.9015,
      "step": 17673
    },
    {
      "epoch": 6.842431281455672,
      "grad_norm": 88.71878814697266,
      "learning_rate": 3.5084096872714764e-06,
      "loss": 0.6631,
      "step": 17674
    },
    {
      "epoch": 6.842818428184282,
      "grad_norm": 28.653453826904297,
      "learning_rate": 3.507979524239687e-06,
      "loss": 1.7825,
      "step": 17675
    },
    {
      "epoch": 6.843205574912892,
      "grad_norm": 49.00855255126953,
      "learning_rate": 3.507549361207898e-06,
      "loss": 0.1904,
      "step": 17676
    },
    {
      "epoch": 6.843592721641502,
      "grad_norm": 3.1927995681762695,
      "learning_rate": 3.507119198176109e-06,
      "loss": 0.142,
      "step": 17677
    },
    {
      "epoch": 6.843979868370113,
      "grad_norm": 41.39910888671875,
      "learning_rate": 3.50668903514432e-06,
      "loss": 0.3303,
      "step": 17678
    },
    {
      "epoch": 6.844367015098722,
      "grad_norm": 50.789363861083984,
      "learning_rate": 3.5062588721125305e-06,
      "loss": 0.9964,
      "step": 17679
    },
    {
      "epoch": 6.844754161827333,
      "grad_norm": 85.40393829345703,
      "learning_rate": 3.505828709080742e-06,
      "loss": 1.2905,
      "step": 17680
    },
    {
      "epoch": 6.8451413085559425,
      "grad_norm": 84.13240051269531,
      "learning_rate": 3.5053985460489525e-06,
      "loss": 0.7063,
      "step": 17681
    },
    {
      "epoch": 6.845528455284553,
      "grad_norm": 87.15357971191406,
      "learning_rate": 3.504968383017164e-06,
      "loss": 1.5354,
      "step": 17682
    },
    {
      "epoch": 6.845915602013163,
      "grad_norm": 87.26119232177734,
      "learning_rate": 3.5045382199853745e-06,
      "loss": 3.9305,
      "step": 17683
    },
    {
      "epoch": 6.846302748741773,
      "grad_norm": 2.8892202377319336,
      "learning_rate": 3.5041080569535855e-06,
      "loss": 0.0892,
      "step": 17684
    },
    {
      "epoch": 6.846689895470384,
      "grad_norm": 149.6455078125,
      "learning_rate": 3.503677893921797e-06,
      "loss": 2.0159,
      "step": 17685
    },
    {
      "epoch": 6.847077042198993,
      "grad_norm": 64.8740005493164,
      "learning_rate": 3.5032477308900075e-06,
      "loss": 2.0872,
      "step": 17686
    },
    {
      "epoch": 6.847464188927604,
      "grad_norm": 110.917724609375,
      "learning_rate": 3.502817567858219e-06,
      "loss": 0.4582,
      "step": 17687
    },
    {
      "epoch": 6.847851335656213,
      "grad_norm": 62.524803161621094,
      "learning_rate": 3.5023874048264295e-06,
      "loss": 1.1351,
      "step": 17688
    },
    {
      "epoch": 6.848238482384824,
      "grad_norm": 0.2614354193210602,
      "learning_rate": 3.501957241794641e-06,
      "loss": 0.0071,
      "step": 17689
    },
    {
      "epoch": 6.8486256291134335,
      "grad_norm": 18.269418716430664,
      "learning_rate": 3.5015270787628515e-06,
      "loss": 0.1483,
      "step": 17690
    },
    {
      "epoch": 6.849012775842044,
      "grad_norm": 95.03685760498047,
      "learning_rate": 3.5010969157310624e-06,
      "loss": 0.5729,
      "step": 17691
    },
    {
      "epoch": 6.8493999225706546,
      "grad_norm": 106.68997955322266,
      "learning_rate": 3.5006667526992734e-06,
      "loss": 0.7406,
      "step": 17692
    },
    {
      "epoch": 6.849787069299264,
      "grad_norm": 62.88195037841797,
      "learning_rate": 3.5002365896674844e-06,
      "loss": 1.3224,
      "step": 17693
    },
    {
      "epoch": 6.850174216027875,
      "grad_norm": 6.179044723510742,
      "learning_rate": 3.499806426635695e-06,
      "loss": 0.1153,
      "step": 17694
    },
    {
      "epoch": 6.850561362756485,
      "grad_norm": 94.52236938476562,
      "learning_rate": 3.4993762636039064e-06,
      "loss": 1.0094,
      "step": 17695
    },
    {
      "epoch": 6.850948509485095,
      "grad_norm": 97.9405746459961,
      "learning_rate": 3.498946100572117e-06,
      "loss": 3.2641,
      "step": 17696
    },
    {
      "epoch": 6.851335656213705,
      "grad_norm": 60.47191619873047,
      "learning_rate": 3.4985159375403284e-06,
      "loss": 0.6729,
      "step": 17697
    },
    {
      "epoch": 6.851722802942315,
      "grad_norm": 47.633785247802734,
      "learning_rate": 3.498085774508539e-06,
      "loss": 1.2835,
      "step": 17698
    },
    {
      "epoch": 6.8521099496709255,
      "grad_norm": 6.982568740844727,
      "learning_rate": 3.49765561147675e-06,
      "loss": 0.2499,
      "step": 17699
    },
    {
      "epoch": 6.852497096399535,
      "grad_norm": 18.1377010345459,
      "learning_rate": 3.497225448444961e-06,
      "loss": 0.2765,
      "step": 17700
    },
    {
      "epoch": 6.852884243128146,
      "grad_norm": 34.14485168457031,
      "learning_rate": 3.496795285413172e-06,
      "loss": 0.1956,
      "step": 17701
    },
    {
      "epoch": 6.853271389856756,
      "grad_norm": 30.13745880126953,
      "learning_rate": 3.4963651223813825e-06,
      "loss": 1.8621,
      "step": 17702
    },
    {
      "epoch": 6.853658536585366,
      "grad_norm": 1.1528302431106567,
      "learning_rate": 3.495934959349594e-06,
      "loss": 0.0371,
      "step": 17703
    },
    {
      "epoch": 6.854045683313976,
      "grad_norm": 2.4819986820220947,
      "learning_rate": 3.4955047963178045e-06,
      "loss": 0.0786,
      "step": 17704
    },
    {
      "epoch": 6.854432830042586,
      "grad_norm": 3.2496862411499023,
      "learning_rate": 3.495074633286016e-06,
      "loss": 0.1084,
      "step": 17705
    },
    {
      "epoch": 6.854819976771196,
      "grad_norm": 144.69168090820312,
      "learning_rate": 3.4946444702542265e-06,
      "loss": 1.387,
      "step": 17706
    },
    {
      "epoch": 6.855207123499806,
      "grad_norm": 77.91929626464844,
      "learning_rate": 3.494214307222438e-06,
      "loss": 0.8163,
      "step": 17707
    },
    {
      "epoch": 6.8555942702284165,
      "grad_norm": 51.8870735168457,
      "learning_rate": 3.4937841441906485e-06,
      "loss": 4.049,
      "step": 17708
    },
    {
      "epoch": 6.855981416957027,
      "grad_norm": 91.97522735595703,
      "learning_rate": 3.4933539811588595e-06,
      "loss": 0.3055,
      "step": 17709
    },
    {
      "epoch": 6.856368563685637,
      "grad_norm": 3.5624732971191406,
      "learning_rate": 3.4929238181270705e-06,
      "loss": 0.1381,
      "step": 17710
    },
    {
      "epoch": 6.856755710414247,
      "grad_norm": 30.72173500061035,
      "learning_rate": 3.4924936550952814e-06,
      "loss": 2.2337,
      "step": 17711
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 132.7651824951172,
      "learning_rate": 3.492063492063492e-06,
      "loss": 2.3104,
      "step": 17712
    },
    {
      "epoch": 6.857530003871467,
      "grad_norm": 27.89759635925293,
      "learning_rate": 3.4916333290317034e-06,
      "loss": 0.1324,
      "step": 17713
    },
    {
      "epoch": 6.857917150600078,
      "grad_norm": 12.243722915649414,
      "learning_rate": 3.491203165999914e-06,
      "loss": 0.1604,
      "step": 17714
    },
    {
      "epoch": 6.858304297328687,
      "grad_norm": 119.76885986328125,
      "learning_rate": 3.4907730029681254e-06,
      "loss": 2.8105,
      "step": 17715
    },
    {
      "epoch": 6.858691444057298,
      "grad_norm": 25.50185775756836,
      "learning_rate": 3.490342839936336e-06,
      "loss": 0.6551,
      "step": 17716
    },
    {
      "epoch": 6.8590785907859075,
      "grad_norm": 36.871429443359375,
      "learning_rate": 3.489912676904547e-06,
      "loss": 1.4507,
      "step": 17717
    },
    {
      "epoch": 6.859465737514518,
      "grad_norm": 440.423095703125,
      "learning_rate": 3.489482513872758e-06,
      "loss": 2.6215,
      "step": 17718
    },
    {
      "epoch": 6.859852884243129,
      "grad_norm": 82.2196273803711,
      "learning_rate": 3.489052350840969e-06,
      "loss": 0.1798,
      "step": 17719
    },
    {
      "epoch": 6.860240030971738,
      "grad_norm": 3.758168935775757,
      "learning_rate": 3.4886221878091795e-06,
      "loss": 0.1493,
      "step": 17720
    },
    {
      "epoch": 6.860627177700349,
      "grad_norm": 9.440156936645508,
      "learning_rate": 3.488192024777391e-06,
      "loss": 0.1289,
      "step": 17721
    },
    {
      "epoch": 6.861014324428958,
      "grad_norm": 28.816537857055664,
      "learning_rate": 3.4877618617456015e-06,
      "loss": 2.0182,
      "step": 17722
    },
    {
      "epoch": 6.861401471157569,
      "grad_norm": 74.23091125488281,
      "learning_rate": 3.487331698713813e-06,
      "loss": 0.5427,
      "step": 17723
    },
    {
      "epoch": 6.861788617886178,
      "grad_norm": 2.735963821411133,
      "learning_rate": 3.4869015356820235e-06,
      "loss": 0.1274,
      "step": 17724
    },
    {
      "epoch": 6.862175764614789,
      "grad_norm": 9.65881633758545,
      "learning_rate": 3.486471372650235e-06,
      "loss": 0.1465,
      "step": 17725
    },
    {
      "epoch": 6.8625629113433995,
      "grad_norm": 53.36284637451172,
      "learning_rate": 3.486041209618446e-06,
      "loss": 0.5626,
      "step": 17726
    },
    {
      "epoch": 6.862950058072009,
      "grad_norm": 21.81894302368164,
      "learning_rate": 3.4856110465866565e-06,
      "loss": 2.0782,
      "step": 17727
    },
    {
      "epoch": 6.86333720480062,
      "grad_norm": 39.449363708496094,
      "learning_rate": 3.485180883554868e-06,
      "loss": 2.5067,
      "step": 17728
    },
    {
      "epoch": 6.863724351529229,
      "grad_norm": 36.89070510864258,
      "learning_rate": 3.4847507205230785e-06,
      "loss": 0.3397,
      "step": 17729
    },
    {
      "epoch": 6.86411149825784,
      "grad_norm": 19.744110107421875,
      "learning_rate": 3.48432055749129e-06,
      "loss": 1.8196,
      "step": 17730
    },
    {
      "epoch": 6.86449864498645,
      "grad_norm": 4.515297889709473,
      "learning_rate": 3.4838903944595004e-06,
      "loss": 0.2044,
      "step": 17731
    },
    {
      "epoch": 6.86488579171506,
      "grad_norm": 20.159992218017578,
      "learning_rate": 3.4834602314277114e-06,
      "loss": 4.0888,
      "step": 17732
    },
    {
      "epoch": 6.86527293844367,
      "grad_norm": 31.64769172668457,
      "learning_rate": 3.4830300683959224e-06,
      "loss": 1.357,
      "step": 17733
    },
    {
      "epoch": 6.86566008517228,
      "grad_norm": 109.01421356201172,
      "learning_rate": 3.4825999053641334e-06,
      "loss": 1.4845,
      "step": 17734
    },
    {
      "epoch": 6.8660472319008905,
      "grad_norm": 48.81608200073242,
      "learning_rate": 3.482169742332344e-06,
      "loss": 3.1737,
      "step": 17735
    },
    {
      "epoch": 6.866434378629501,
      "grad_norm": 86.64449310302734,
      "learning_rate": 3.4817395793005554e-06,
      "loss": 3.3982,
      "step": 17736
    },
    {
      "epoch": 6.866821525358111,
      "grad_norm": 80.8429183959961,
      "learning_rate": 3.481309416268766e-06,
      "loss": 2.8804,
      "step": 17737
    },
    {
      "epoch": 6.867208672086721,
      "grad_norm": 1.7373442649841309,
      "learning_rate": 3.4808792532369774e-06,
      "loss": 0.0753,
      "step": 17738
    },
    {
      "epoch": 6.867595818815331,
      "grad_norm": 31.042194366455078,
      "learning_rate": 3.480449090205188e-06,
      "loss": 1.538,
      "step": 17739
    },
    {
      "epoch": 6.867982965543941,
      "grad_norm": 48.9168701171875,
      "learning_rate": 3.4800189271733994e-06,
      "loss": 3.2616,
      "step": 17740
    },
    {
      "epoch": 6.868370112272551,
      "grad_norm": 19.842281341552734,
      "learning_rate": 3.47958876414161e-06,
      "loss": 3.6144,
      "step": 17741
    },
    {
      "epoch": 6.868757259001161,
      "grad_norm": 4.43798828125,
      "learning_rate": 3.479158601109821e-06,
      "loss": 0.1235,
      "step": 17742
    },
    {
      "epoch": 6.869144405729772,
      "grad_norm": 96.89936828613281,
      "learning_rate": 3.478728438078032e-06,
      "loss": 1.3912,
      "step": 17743
    },
    {
      "epoch": 6.8695315524583815,
      "grad_norm": 2.226696014404297,
      "learning_rate": 3.478298275046243e-06,
      "loss": 0.0748,
      "step": 17744
    },
    {
      "epoch": 6.869918699186992,
      "grad_norm": 16.432050704956055,
      "learning_rate": 3.4778681120144535e-06,
      "loss": 0.3494,
      "step": 17745
    },
    {
      "epoch": 6.870305845915602,
      "grad_norm": 37.30805969238281,
      "learning_rate": 3.477437948982665e-06,
      "loss": 1.1503,
      "step": 17746
    },
    {
      "epoch": 6.870692992644212,
      "grad_norm": 18.88799285888672,
      "learning_rate": 3.4770077859508755e-06,
      "loss": 1.7802,
      "step": 17747
    },
    {
      "epoch": 6.871080139372822,
      "grad_norm": 22.51873016357422,
      "learning_rate": 3.476577622919087e-06,
      "loss": 1.4204,
      "step": 17748
    },
    {
      "epoch": 6.871467286101432,
      "grad_norm": 3.3315911293029785,
      "learning_rate": 3.4761474598872975e-06,
      "loss": 0.0841,
      "step": 17749
    },
    {
      "epoch": 6.871854432830043,
      "grad_norm": 22.456871032714844,
      "learning_rate": 3.4757172968555084e-06,
      "loss": 4.2083,
      "step": 17750
    },
    {
      "epoch": 6.8722415795586524,
      "grad_norm": 22.5799503326416,
      "learning_rate": 3.4752871338237194e-06,
      "loss": 1.8453,
      "step": 17751
    },
    {
      "epoch": 6.872628726287263,
      "grad_norm": 10.876604080200195,
      "learning_rate": 3.4748569707919304e-06,
      "loss": 0.2119,
      "step": 17752
    },
    {
      "epoch": 6.8730158730158735,
      "grad_norm": 15.054901123046875,
      "learning_rate": 3.474426807760141e-06,
      "loss": 1.546,
      "step": 17753
    },
    {
      "epoch": 6.873403019744483,
      "grad_norm": 84.06167602539062,
      "learning_rate": 3.4739966447283524e-06,
      "loss": 3.2943,
      "step": 17754
    },
    {
      "epoch": 6.873790166473094,
      "grad_norm": 13.911004066467285,
      "learning_rate": 3.473566481696563e-06,
      "loss": 0.1421,
      "step": 17755
    },
    {
      "epoch": 6.874177313201703,
      "grad_norm": 28.165725708007812,
      "learning_rate": 3.4731363186647744e-06,
      "loss": 0.267,
      "step": 17756
    },
    {
      "epoch": 6.874564459930314,
      "grad_norm": 128.27207946777344,
      "learning_rate": 3.472706155632985e-06,
      "loss": 1.6916,
      "step": 17757
    },
    {
      "epoch": 6.874951606658923,
      "grad_norm": 60.96880340576172,
      "learning_rate": 3.4722759926011964e-06,
      "loss": 1.4609,
      "step": 17758
    },
    {
      "epoch": 6.875338753387534,
      "grad_norm": 42.46281433105469,
      "learning_rate": 3.471845829569407e-06,
      "loss": 0.2377,
      "step": 17759
    },
    {
      "epoch": 6.875725900116144,
      "grad_norm": 99.06326293945312,
      "learning_rate": 3.471415666537618e-06,
      "loss": 1.2929,
      "step": 17760
    },
    {
      "epoch": 6.876113046844754,
      "grad_norm": 48.832183837890625,
      "learning_rate": 3.470985503505829e-06,
      "loss": 1.8356,
      "step": 17761
    },
    {
      "epoch": 6.8765001935733645,
      "grad_norm": 49.209049224853516,
      "learning_rate": 3.47055534047404e-06,
      "loss": 1.7546,
      "step": 17762
    },
    {
      "epoch": 6.876887340301974,
      "grad_norm": 39.83536911010742,
      "learning_rate": 3.4701251774422505e-06,
      "loss": 0.1556,
      "step": 17763
    },
    {
      "epoch": 6.877274487030585,
      "grad_norm": 3.417677402496338,
      "learning_rate": 3.469695014410462e-06,
      "loss": 0.0855,
      "step": 17764
    },
    {
      "epoch": 6.877661633759194,
      "grad_norm": 43.716896057128906,
      "learning_rate": 3.4692648513786725e-06,
      "loss": 0.4928,
      "step": 17765
    },
    {
      "epoch": 6.878048780487805,
      "grad_norm": 85.34549713134766,
      "learning_rate": 3.468834688346884e-06,
      "loss": 1.2614,
      "step": 17766
    },
    {
      "epoch": 6.878435927216415,
      "grad_norm": 150.07101440429688,
      "learning_rate": 3.468404525315095e-06,
      "loss": 0.8034,
      "step": 17767
    },
    {
      "epoch": 6.878823073945025,
      "grad_norm": 1.8953098058700562,
      "learning_rate": 3.4679743622833055e-06,
      "loss": 0.0902,
      "step": 17768
    },
    {
      "epoch": 6.879210220673635,
      "grad_norm": 41.25638961791992,
      "learning_rate": 3.467544199251517e-06,
      "loss": 1.3181,
      "step": 17769
    },
    {
      "epoch": 6.879597367402246,
      "grad_norm": 2.504483699798584,
      "learning_rate": 3.4671140362197274e-06,
      "loss": 0.0848,
      "step": 17770
    },
    {
      "epoch": 6.8799845141308555,
      "grad_norm": 1.9245400428771973,
      "learning_rate": 3.466683873187939e-06,
      "loss": 0.0946,
      "step": 17771
    },
    {
      "epoch": 6.880371660859466,
      "grad_norm": 55.423683166503906,
      "learning_rate": 3.4662537101561494e-06,
      "loss": 1.8059,
      "step": 17772
    },
    {
      "epoch": 6.880758807588076,
      "grad_norm": 6.0821661949157715,
      "learning_rate": 3.465823547124361e-06,
      "loss": 0.2022,
      "step": 17773
    },
    {
      "epoch": 6.881145954316686,
      "grad_norm": 92.212158203125,
      "learning_rate": 3.4653933840925714e-06,
      "loss": 0.1583,
      "step": 17774
    },
    {
      "epoch": 6.881533101045296,
      "grad_norm": 111.19467163085938,
      "learning_rate": 3.4649632210607824e-06,
      "loss": 1.5093,
      "step": 17775
    },
    {
      "epoch": 6.881920247773906,
      "grad_norm": 4.667759895324707,
      "learning_rate": 3.4645330580289934e-06,
      "loss": 0.2199,
      "step": 17776
    },
    {
      "epoch": 6.882307394502517,
      "grad_norm": 52.37053298950195,
      "learning_rate": 3.4641028949972044e-06,
      "loss": 1.0634,
      "step": 17777
    },
    {
      "epoch": 6.8826945412311265,
      "grad_norm": 36.47439193725586,
      "learning_rate": 3.463672731965415e-06,
      "loss": 0.2955,
      "step": 17778
    },
    {
      "epoch": 6.883081687959737,
      "grad_norm": 104.14799499511719,
      "learning_rate": 3.4632425689336264e-06,
      "loss": 1.0647,
      "step": 17779
    },
    {
      "epoch": 6.883468834688347,
      "grad_norm": 42.56207275390625,
      "learning_rate": 3.462812405901837e-06,
      "loss": 1.0315,
      "step": 17780
    },
    {
      "epoch": 6.883855981416957,
      "grad_norm": 17.412599563598633,
      "learning_rate": 3.4623822428700484e-06,
      "loss": 0.1123,
      "step": 17781
    },
    {
      "epoch": 6.884243128145567,
      "grad_norm": 4.523642539978027,
      "learning_rate": 3.461952079838259e-06,
      "loss": 0.1148,
      "step": 17782
    },
    {
      "epoch": 6.884630274874177,
      "grad_norm": 144.5068817138672,
      "learning_rate": 3.46152191680647e-06,
      "loss": 2.6016,
      "step": 17783
    },
    {
      "epoch": 6.885017421602788,
      "grad_norm": 84.54195404052734,
      "learning_rate": 3.461091753774681e-06,
      "loss": 1.6749,
      "step": 17784
    },
    {
      "epoch": 6.885404568331397,
      "grad_norm": 76.80307006835938,
      "learning_rate": 3.460661590742892e-06,
      "loss": 1.9321,
      "step": 17785
    },
    {
      "epoch": 6.885791715060008,
      "grad_norm": 0.3552461564540863,
      "learning_rate": 3.4602314277111025e-06,
      "loss": 0.0085,
      "step": 17786
    },
    {
      "epoch": 6.886178861788618,
      "grad_norm": 140.0669403076172,
      "learning_rate": 3.459801264679314e-06,
      "loss": 4.6169,
      "step": 17787
    },
    {
      "epoch": 6.886566008517228,
      "grad_norm": 3.8495264053344727,
      "learning_rate": 3.4593711016475245e-06,
      "loss": 0.1089,
      "step": 17788
    },
    {
      "epoch": 6.8869531552458385,
      "grad_norm": 31.874011993408203,
      "learning_rate": 3.458940938615736e-06,
      "loss": 0.2843,
      "step": 17789
    },
    {
      "epoch": 6.887340301974448,
      "grad_norm": 49.60497283935547,
      "learning_rate": 3.4585107755839464e-06,
      "loss": 0.3932,
      "step": 17790
    },
    {
      "epoch": 6.887727448703059,
      "grad_norm": 22.743680953979492,
      "learning_rate": 3.458080612552158e-06,
      "loss": 0.1907,
      "step": 17791
    },
    {
      "epoch": 6.888114595431668,
      "grad_norm": 20.225738525390625,
      "learning_rate": 3.4576504495203684e-06,
      "loss": 1.9841,
      "step": 17792
    },
    {
      "epoch": 6.888501742160279,
      "grad_norm": 116.85236358642578,
      "learning_rate": 3.4572202864885794e-06,
      "loss": 2.6119,
      "step": 17793
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 116.03377532958984,
      "learning_rate": 3.4567901234567904e-06,
      "loss": 1.1426,
      "step": 17794
    },
    {
      "epoch": 6.889276035617499,
      "grad_norm": 112.2020263671875,
      "learning_rate": 3.4563599604250014e-06,
      "loss": 1.4929,
      "step": 17795
    },
    {
      "epoch": 6.889663182346109,
      "grad_norm": 43.197879791259766,
      "learning_rate": 3.455929797393212e-06,
      "loss": 0.2654,
      "step": 17796
    },
    {
      "epoch": 6.890050329074719,
      "grad_norm": 8.542186737060547,
      "learning_rate": 3.4554996343614234e-06,
      "loss": 0.109,
      "step": 17797
    },
    {
      "epoch": 6.8904374758033295,
      "grad_norm": 88.51770782470703,
      "learning_rate": 3.455069471329634e-06,
      "loss": 0.8806,
      "step": 17798
    },
    {
      "epoch": 6.890824622531939,
      "grad_norm": 68.23273468017578,
      "learning_rate": 3.4546393082978454e-06,
      "loss": 1.2009,
      "step": 17799
    },
    {
      "epoch": 6.89121176926055,
      "grad_norm": 8.669587135314941,
      "learning_rate": 3.454209145266056e-06,
      "loss": 0.1466,
      "step": 17800
    },
    {
      "epoch": 6.89159891598916,
      "grad_norm": 16.276479721069336,
      "learning_rate": 3.453778982234267e-06,
      "loss": 0.1236,
      "step": 17801
    },
    {
      "epoch": 6.89198606271777,
      "grad_norm": 65.62370300292969,
      "learning_rate": 3.453348819202478e-06,
      "loss": 0.7927,
      "step": 17802
    },
    {
      "epoch": 6.89237320944638,
      "grad_norm": 52.358436584472656,
      "learning_rate": 3.452918656170689e-06,
      "loss": 0.5785,
      "step": 17803
    },
    {
      "epoch": 6.89276035617499,
      "grad_norm": 6.293413162231445,
      "learning_rate": 3.4524884931388995e-06,
      "loss": 0.2413,
      "step": 17804
    },
    {
      "epoch": 6.8931475029036005,
      "grad_norm": 276.1654052734375,
      "learning_rate": 3.452058330107111e-06,
      "loss": 3.2472,
      "step": 17805
    },
    {
      "epoch": 6.893534649632211,
      "grad_norm": 59.93800735473633,
      "learning_rate": 3.4516281670753215e-06,
      "loss": 1.3719,
      "step": 17806
    },
    {
      "epoch": 6.893921796360821,
      "grad_norm": 1.8268036842346191,
      "learning_rate": 3.451198004043533e-06,
      "loss": 0.0791,
      "step": 17807
    },
    {
      "epoch": 6.894308943089431,
      "grad_norm": 1.551034688949585,
      "learning_rate": 3.450767841011744e-06,
      "loss": 0.0685,
      "step": 17808
    },
    {
      "epoch": 6.894696089818041,
      "grad_norm": 49.881919860839844,
      "learning_rate": 3.450337677979955e-06,
      "loss": 1.3547,
      "step": 17809
    },
    {
      "epoch": 6.895083236546651,
      "grad_norm": 1.2527223825454712,
      "learning_rate": 3.449907514948166e-06,
      "loss": 0.0558,
      "step": 17810
    },
    {
      "epoch": 6.895470383275262,
      "grad_norm": 56.582115173339844,
      "learning_rate": 3.4494773519163764e-06,
      "loss": 2.2065,
      "step": 17811
    },
    {
      "epoch": 6.895857530003871,
      "grad_norm": 67.74600982666016,
      "learning_rate": 3.449047188884588e-06,
      "loss": 1.9412,
      "step": 17812
    },
    {
      "epoch": 6.896244676732482,
      "grad_norm": 9.275917053222656,
      "learning_rate": 3.4486170258527984e-06,
      "loss": 0.296,
      "step": 17813
    },
    {
      "epoch": 6.8966318234610915,
      "grad_norm": 24.731985092163086,
      "learning_rate": 3.44818686282101e-06,
      "loss": 0.3117,
      "step": 17814
    },
    {
      "epoch": 6.897018970189702,
      "grad_norm": 3.5145933628082275,
      "learning_rate": 3.4477566997892204e-06,
      "loss": 0.1232,
      "step": 17815
    },
    {
      "epoch": 6.897406116918312,
      "grad_norm": 148.56500244140625,
      "learning_rate": 3.4473265367574314e-06,
      "loss": 3.1456,
      "step": 17816
    },
    {
      "epoch": 6.897793263646922,
      "grad_norm": 156.51260375976562,
      "learning_rate": 3.4468963737256424e-06,
      "loss": 0.5146,
      "step": 17817
    },
    {
      "epoch": 6.898180410375533,
      "grad_norm": 50.677703857421875,
      "learning_rate": 3.4464662106938534e-06,
      "loss": 1.4593,
      "step": 17818
    },
    {
      "epoch": 6.898567557104142,
      "grad_norm": 20.091415405273438,
      "learning_rate": 3.446036047662064e-06,
      "loss": 0.2494,
      "step": 17819
    },
    {
      "epoch": 6.898954703832753,
      "grad_norm": 36.48271942138672,
      "learning_rate": 3.4456058846302754e-06,
      "loss": 1.6443,
      "step": 17820
    },
    {
      "epoch": 6.899341850561362,
      "grad_norm": 5.745509147644043,
      "learning_rate": 3.445175721598486e-06,
      "loss": 0.0902,
      "step": 17821
    },
    {
      "epoch": 6.899728997289973,
      "grad_norm": 25.61952781677246,
      "learning_rate": 3.4447455585666973e-06,
      "loss": 1.4113,
      "step": 17822
    },
    {
      "epoch": 6.900116144018583,
      "grad_norm": 34.33509063720703,
      "learning_rate": 3.444315395534908e-06,
      "loss": 1.6179,
      "step": 17823
    },
    {
      "epoch": 6.900503290747193,
      "grad_norm": 1.0506049394607544,
      "learning_rate": 3.4438852325031193e-06,
      "loss": 0.0353,
      "step": 17824
    },
    {
      "epoch": 6.9008904374758036,
      "grad_norm": 36.152835845947266,
      "learning_rate": 3.44345506947133e-06,
      "loss": 3.2397,
      "step": 17825
    },
    {
      "epoch": 6.901277584204413,
      "grad_norm": 2.7470390796661377,
      "learning_rate": 3.443024906439541e-06,
      "loss": 0.0945,
      "step": 17826
    },
    {
      "epoch": 6.901664730933024,
      "grad_norm": 59.122703552246094,
      "learning_rate": 3.442594743407752e-06,
      "loss": 1.6325,
      "step": 17827
    },
    {
      "epoch": 6.902051877661634,
      "grad_norm": 16.924114227294922,
      "learning_rate": 3.442164580375963e-06,
      "loss": 0.3544,
      "step": 17828
    },
    {
      "epoch": 6.902439024390244,
      "grad_norm": 21.994441986083984,
      "learning_rate": 3.4417344173441734e-06,
      "loss": 0.1325,
      "step": 17829
    },
    {
      "epoch": 6.902826171118854,
      "grad_norm": 10.655875205993652,
      "learning_rate": 3.441304254312385e-06,
      "loss": 0.11,
      "step": 17830
    },
    {
      "epoch": 6.903213317847464,
      "grad_norm": 62.79151153564453,
      "learning_rate": 3.4408740912805954e-06,
      "loss": 2.0849,
      "step": 17831
    },
    {
      "epoch": 6.9036004645760745,
      "grad_norm": 197.736083984375,
      "learning_rate": 3.440443928248807e-06,
      "loss": 1.2061,
      "step": 17832
    },
    {
      "epoch": 6.903987611304684,
      "grad_norm": 28.961463928222656,
      "learning_rate": 3.4400137652170174e-06,
      "loss": 2.0329,
      "step": 17833
    },
    {
      "epoch": 6.904374758033295,
      "grad_norm": 46.17431640625,
      "learning_rate": 3.4395836021852284e-06,
      "loss": 0.6892,
      "step": 17834
    },
    {
      "epoch": 6.904761904761905,
      "grad_norm": 43.05814743041992,
      "learning_rate": 3.4391534391534394e-06,
      "loss": 1.338,
      "step": 17835
    },
    {
      "epoch": 6.905149051490515,
      "grad_norm": 99.73922729492188,
      "learning_rate": 3.4387232761216504e-06,
      "loss": 0.5064,
      "step": 17836
    },
    {
      "epoch": 6.905536198219125,
      "grad_norm": 31.739416122436523,
      "learning_rate": 3.438293113089861e-06,
      "loss": 2.1915,
      "step": 17837
    },
    {
      "epoch": 6.905923344947735,
      "grad_norm": 72.21666717529297,
      "learning_rate": 3.4378629500580724e-06,
      "loss": 2.7661,
      "step": 17838
    },
    {
      "epoch": 6.906310491676345,
      "grad_norm": 10.560348510742188,
      "learning_rate": 3.437432787026283e-06,
      "loss": 0.1214,
      "step": 17839
    },
    {
      "epoch": 6.906697638404955,
      "grad_norm": 85.85308074951172,
      "learning_rate": 3.4370026239944944e-06,
      "loss": 3.8149,
      "step": 17840
    },
    {
      "epoch": 6.9070847851335655,
      "grad_norm": 7.113988399505615,
      "learning_rate": 3.436572460962705e-06,
      "loss": 0.1036,
      "step": 17841
    },
    {
      "epoch": 6.907471931862176,
      "grad_norm": 113.35852813720703,
      "learning_rate": 3.4361422979309163e-06,
      "loss": 3.3732,
      "step": 17842
    },
    {
      "epoch": 6.907859078590786,
      "grad_norm": 70.4931411743164,
      "learning_rate": 3.435712134899127e-06,
      "loss": 1.5578,
      "step": 17843
    },
    {
      "epoch": 6.908246225319396,
      "grad_norm": 1.5405725240707397,
      "learning_rate": 3.435281971867338e-06,
      "loss": 0.0407,
      "step": 17844
    },
    {
      "epoch": 6.908633372048007,
      "grad_norm": 78.76057434082031,
      "learning_rate": 3.434851808835549e-06,
      "loss": 3.07,
      "step": 17845
    },
    {
      "epoch": 6.909020518776616,
      "grad_norm": 109.985595703125,
      "learning_rate": 3.43442164580376e-06,
      "loss": 1.0202,
      "step": 17846
    },
    {
      "epoch": 6.909407665505227,
      "grad_norm": 2.5907275676727295,
      "learning_rate": 3.4339914827719705e-06,
      "loss": 0.1301,
      "step": 17847
    },
    {
      "epoch": 6.909794812233836,
      "grad_norm": 2.462158203125,
      "learning_rate": 3.433561319740182e-06,
      "loss": 0.0474,
      "step": 17848
    },
    {
      "epoch": 6.910181958962447,
      "grad_norm": 35.38298034667969,
      "learning_rate": 3.433131156708393e-06,
      "loss": 2.5117,
      "step": 17849
    },
    {
      "epoch": 6.9105691056910565,
      "grad_norm": 49.57246017456055,
      "learning_rate": 3.432700993676604e-06,
      "loss": 1.5179,
      "step": 17850
    },
    {
      "epoch": 6.910956252419667,
      "grad_norm": 69.75029754638672,
      "learning_rate": 3.432270830644815e-06,
      "loss": 0.3059,
      "step": 17851
    },
    {
      "epoch": 6.9113433991482776,
      "grad_norm": 53.25206756591797,
      "learning_rate": 3.4318406676130254e-06,
      "loss": 1.7385,
      "step": 17852
    },
    {
      "epoch": 6.911730545876887,
      "grad_norm": 75.34595489501953,
      "learning_rate": 3.431410504581237e-06,
      "loss": 0.697,
      "step": 17853
    },
    {
      "epoch": 6.912117692605498,
      "grad_norm": 28.1270751953125,
      "learning_rate": 3.4309803415494474e-06,
      "loss": 2.2072,
      "step": 17854
    },
    {
      "epoch": 6.912504839334107,
      "grad_norm": 8.388982772827148,
      "learning_rate": 3.430550178517659e-06,
      "loss": 0.1854,
      "step": 17855
    },
    {
      "epoch": 6.912891986062718,
      "grad_norm": 52.864967346191406,
      "learning_rate": 3.4301200154858694e-06,
      "loss": 2.1235,
      "step": 17856
    },
    {
      "epoch": 6.913279132791327,
      "grad_norm": 66.24195861816406,
      "learning_rate": 3.4296898524540804e-06,
      "loss": 0.806,
      "step": 17857
    },
    {
      "epoch": 6.913666279519938,
      "grad_norm": 66.73849487304688,
      "learning_rate": 3.4292596894222914e-06,
      "loss": 1.1086,
      "step": 17858
    },
    {
      "epoch": 6.9140534262485485,
      "grad_norm": 23.320201873779297,
      "learning_rate": 3.4288295263905024e-06,
      "loss": 0.635,
      "step": 17859
    },
    {
      "epoch": 6.914440572977158,
      "grad_norm": 122.70948791503906,
      "learning_rate": 3.4283993633587134e-06,
      "loss": 0.7836,
      "step": 17860
    },
    {
      "epoch": 6.914827719705769,
      "grad_norm": 61.507118225097656,
      "learning_rate": 3.4279692003269243e-06,
      "loss": 0.9761,
      "step": 17861
    },
    {
      "epoch": 6.915214866434379,
      "grad_norm": 267.71746826171875,
      "learning_rate": 3.427539037295135e-06,
      "loss": 2.3979,
      "step": 17862
    },
    {
      "epoch": 6.915602013162989,
      "grad_norm": 52.99520492553711,
      "learning_rate": 3.4271088742633463e-06,
      "loss": 2.3915,
      "step": 17863
    },
    {
      "epoch": 6.915989159891599,
      "grad_norm": 33.43590545654297,
      "learning_rate": 3.426678711231557e-06,
      "loss": 1.8662,
      "step": 17864
    },
    {
      "epoch": 6.916376306620209,
      "grad_norm": 35.73053741455078,
      "learning_rate": 3.4262485481997683e-06,
      "loss": 0.2737,
      "step": 17865
    },
    {
      "epoch": 6.916763453348819,
      "grad_norm": 11.974617958068848,
      "learning_rate": 3.425818385167979e-06,
      "loss": 0.2938,
      "step": 17866
    },
    {
      "epoch": 6.917150600077429,
      "grad_norm": 56.8953742980957,
      "learning_rate": 3.42538822213619e-06,
      "loss": 0.9346,
      "step": 17867
    },
    {
      "epoch": 6.9175377468060395,
      "grad_norm": 27.342342376708984,
      "learning_rate": 3.424958059104401e-06,
      "loss": 2.5038,
      "step": 17868
    },
    {
      "epoch": 6.91792489353465,
      "grad_norm": 38.44198226928711,
      "learning_rate": 3.424527896072612e-06,
      "loss": 0.5196,
      "step": 17869
    },
    {
      "epoch": 6.91831204026326,
      "grad_norm": 2.4192371368408203,
      "learning_rate": 3.4240977330408224e-06,
      "loss": 0.077,
      "step": 17870
    },
    {
      "epoch": 6.91869918699187,
      "grad_norm": 4.823819160461426,
      "learning_rate": 3.423667570009034e-06,
      "loss": 0.1976,
      "step": 17871
    },
    {
      "epoch": 6.91908633372048,
      "grad_norm": 19.44052505493164,
      "learning_rate": 3.4232374069772444e-06,
      "loss": 0.3239,
      "step": 17872
    },
    {
      "epoch": 6.91947348044909,
      "grad_norm": 93.62024688720703,
      "learning_rate": 3.422807243945456e-06,
      "loss": 0.3709,
      "step": 17873
    },
    {
      "epoch": 6.9198606271777,
      "grad_norm": 47.708091735839844,
      "learning_rate": 3.4223770809136664e-06,
      "loss": 0.4264,
      "step": 17874
    },
    {
      "epoch": 6.92024777390631,
      "grad_norm": 23.262008666992188,
      "learning_rate": 3.4219469178818774e-06,
      "loss": 1.8799,
      "step": 17875
    },
    {
      "epoch": 6.920634920634921,
      "grad_norm": 29.37993621826172,
      "learning_rate": 3.4215167548500884e-06,
      "loss": 1.0913,
      "step": 17876
    },
    {
      "epoch": 6.9210220673635305,
      "grad_norm": 67.2072525024414,
      "learning_rate": 3.4210865918182994e-06,
      "loss": 0.5017,
      "step": 17877
    },
    {
      "epoch": 6.921409214092141,
      "grad_norm": 47.5425910949707,
      "learning_rate": 3.4206564287865104e-06,
      "loss": 3.5799,
      "step": 17878
    },
    {
      "epoch": 6.921796360820752,
      "grad_norm": 2.5498952865600586,
      "learning_rate": 3.4202262657547214e-06,
      "loss": 0.1033,
      "step": 17879
    },
    {
      "epoch": 6.922183507549361,
      "grad_norm": 3.316415786743164,
      "learning_rate": 3.419796102722932e-06,
      "loss": 0.1174,
      "step": 17880
    },
    {
      "epoch": 6.922570654277972,
      "grad_norm": 81.7474136352539,
      "learning_rate": 3.4193659396911433e-06,
      "loss": 3.2207,
      "step": 17881
    },
    {
      "epoch": 6.922957801006581,
      "grad_norm": 127.17951202392578,
      "learning_rate": 3.418935776659354e-06,
      "loss": 3.2842,
      "step": 17882
    },
    {
      "epoch": 6.923344947735192,
      "grad_norm": 3.364529609680176,
      "learning_rate": 3.4185056136275653e-06,
      "loss": 0.0521,
      "step": 17883
    },
    {
      "epoch": 6.9237320944638014,
      "grad_norm": 26.304195404052734,
      "learning_rate": 3.418075450595776e-06,
      "loss": 1.0888,
      "step": 17884
    },
    {
      "epoch": 6.924119241192412,
      "grad_norm": 41.303226470947266,
      "learning_rate": 3.417645287563987e-06,
      "loss": 1.8542,
      "step": 17885
    },
    {
      "epoch": 6.9245063879210225,
      "grad_norm": 49.028385162353516,
      "learning_rate": 3.417215124532198e-06,
      "loss": 1.0294,
      "step": 17886
    },
    {
      "epoch": 6.924893534649632,
      "grad_norm": 26.432098388671875,
      "learning_rate": 3.416784961500409e-06,
      "loss": 1.8182,
      "step": 17887
    },
    {
      "epoch": 6.925280681378243,
      "grad_norm": 65.717041015625,
      "learning_rate": 3.4163547984686194e-06,
      "loss": 2.8955,
      "step": 17888
    },
    {
      "epoch": 6.925667828106852,
      "grad_norm": 6.478727340698242,
      "learning_rate": 3.415924635436831e-06,
      "loss": 0.2126,
      "step": 17889
    },
    {
      "epoch": 6.926054974835463,
      "grad_norm": 129.7886199951172,
      "learning_rate": 3.415494472405042e-06,
      "loss": 0.6465,
      "step": 17890
    },
    {
      "epoch": 6.926442121564072,
      "grad_norm": 182.6102752685547,
      "learning_rate": 3.415064309373253e-06,
      "loss": 3.3719,
      "step": 17891
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 72.7024917602539,
      "learning_rate": 3.414634146341464e-06,
      "loss": 0.4881,
      "step": 17892
    },
    {
      "epoch": 6.927216415021293,
      "grad_norm": 67.87251281738281,
      "learning_rate": 3.4142039833096744e-06,
      "loss": 1.6983,
      "step": 17893
    },
    {
      "epoch": 6.927603561749903,
      "grad_norm": 87.5527572631836,
      "learning_rate": 3.413773820277886e-06,
      "loss": 2.5693,
      "step": 17894
    },
    {
      "epoch": 6.9279907084785135,
      "grad_norm": 82.78553771972656,
      "learning_rate": 3.4133436572460964e-06,
      "loss": 2.3478,
      "step": 17895
    },
    {
      "epoch": 6.928377855207123,
      "grad_norm": 21.058855056762695,
      "learning_rate": 3.412913494214308e-06,
      "loss": 1.6598,
      "step": 17896
    },
    {
      "epoch": 6.928765001935734,
      "grad_norm": 66.68766021728516,
      "learning_rate": 3.4124833311825184e-06,
      "loss": 1.5245,
      "step": 17897
    },
    {
      "epoch": 6.929152148664344,
      "grad_norm": 9.268633842468262,
      "learning_rate": 3.41205316815073e-06,
      "loss": 0.2337,
      "step": 17898
    },
    {
      "epoch": 6.929539295392954,
      "grad_norm": 36.84824752807617,
      "learning_rate": 3.4116230051189404e-06,
      "loss": 0.6444,
      "step": 17899
    },
    {
      "epoch": 6.929926442121564,
      "grad_norm": 93.91290283203125,
      "learning_rate": 3.4111928420871514e-06,
      "loss": 2.8449,
      "step": 17900
    },
    {
      "epoch": 6.930313588850174,
      "grad_norm": 0.78840571641922,
      "learning_rate": 3.4107626790553623e-06,
      "loss": 0.0295,
      "step": 17901
    },
    {
      "epoch": 6.930700735578784,
      "grad_norm": 118.63340759277344,
      "learning_rate": 3.4103325160235733e-06,
      "loss": 0.7274,
      "step": 17902
    },
    {
      "epoch": 6.931087882307395,
      "grad_norm": 239.71353149414062,
      "learning_rate": 3.409902352991784e-06,
      "loss": 0.4762,
      "step": 17903
    },
    {
      "epoch": 6.9314750290360045,
      "grad_norm": 71.53964233398438,
      "learning_rate": 3.4094721899599953e-06,
      "loss": 1.564,
      "step": 17904
    },
    {
      "epoch": 6.931862175764615,
      "grad_norm": 34.691184997558594,
      "learning_rate": 3.409042026928206e-06,
      "loss": 0.9506,
      "step": 17905
    },
    {
      "epoch": 6.932249322493225,
      "grad_norm": 22.778669357299805,
      "learning_rate": 3.4086118638964173e-06,
      "loss": 1.9364,
      "step": 17906
    },
    {
      "epoch": 6.932636469221835,
      "grad_norm": 21.624561309814453,
      "learning_rate": 3.408181700864628e-06,
      "loss": 1.4061,
      "step": 17907
    },
    {
      "epoch": 6.933023615950445,
      "grad_norm": 24.80489730834961,
      "learning_rate": 3.407751537832839e-06,
      "loss": 0.3528,
      "step": 17908
    },
    {
      "epoch": 6.933410762679055,
      "grad_norm": 27.067800521850586,
      "learning_rate": 3.40732137480105e-06,
      "loss": 1.5116,
      "step": 17909
    },
    {
      "epoch": 6.933797909407666,
      "grad_norm": 33.62017059326172,
      "learning_rate": 3.406891211769261e-06,
      "loss": 0.3743,
      "step": 17910
    },
    {
      "epoch": 6.9341850561362754,
      "grad_norm": 0.2543147802352905,
      "learning_rate": 3.4064610487374714e-06,
      "loss": 0.0071,
      "step": 17911
    },
    {
      "epoch": 6.934572202864886,
      "grad_norm": 334.68878173828125,
      "learning_rate": 3.406030885705683e-06,
      "loss": 1.8685,
      "step": 17912
    },
    {
      "epoch": 6.934959349593496,
      "grad_norm": 38.418060302734375,
      "learning_rate": 3.4056007226738934e-06,
      "loss": 0.4379,
      "step": 17913
    },
    {
      "epoch": 6.935346496322106,
      "grad_norm": 25.884004592895508,
      "learning_rate": 3.405170559642105e-06,
      "loss": 1.3511,
      "step": 17914
    },
    {
      "epoch": 6.935733643050717,
      "grad_norm": 96.25636291503906,
      "learning_rate": 3.4047403966103154e-06,
      "loss": 2.2581,
      "step": 17915
    },
    {
      "epoch": 6.936120789779326,
      "grad_norm": 3.798424482345581,
      "learning_rate": 3.404310233578527e-06,
      "loss": 0.1202,
      "step": 17916
    },
    {
      "epoch": 6.936507936507937,
      "grad_norm": 32.743648529052734,
      "learning_rate": 3.4038800705467374e-06,
      "loss": 1.4531,
      "step": 17917
    },
    {
      "epoch": 6.936895083236546,
      "grad_norm": 76.47920227050781,
      "learning_rate": 3.4034499075149484e-06,
      "loss": 0.8685,
      "step": 17918
    },
    {
      "epoch": 6.937282229965157,
      "grad_norm": 22.17854118347168,
      "learning_rate": 3.4030197444831594e-06,
      "loss": 0.4356,
      "step": 17919
    },
    {
      "epoch": 6.937669376693767,
      "grad_norm": 34.407047271728516,
      "learning_rate": 3.4025895814513704e-06,
      "loss": 1.2864,
      "step": 17920
    },
    {
      "epoch": 6.938056523422377,
      "grad_norm": 36.337337493896484,
      "learning_rate": 3.402159418419581e-06,
      "loss": 2.1192,
      "step": 17921
    },
    {
      "epoch": 6.9384436701509875,
      "grad_norm": 1.344280481338501,
      "learning_rate": 3.4017292553877923e-06,
      "loss": 0.0536,
      "step": 17922
    },
    {
      "epoch": 6.938830816879597,
      "grad_norm": 271.4542541503906,
      "learning_rate": 3.401299092356003e-06,
      "loss": 1.7192,
      "step": 17923
    },
    {
      "epoch": 6.939217963608208,
      "grad_norm": 90.96064758300781,
      "learning_rate": 3.4008689293242143e-06,
      "loss": 3.2082,
      "step": 17924
    },
    {
      "epoch": 6.939605110336817,
      "grad_norm": 216.7658233642578,
      "learning_rate": 3.400438766292425e-06,
      "loss": 0.585,
      "step": 17925
    },
    {
      "epoch": 6.939992257065428,
      "grad_norm": 35.44340896606445,
      "learning_rate": 3.400008603260636e-06,
      "loss": 0.3752,
      "step": 17926
    },
    {
      "epoch": 6.940379403794038,
      "grad_norm": 20.87147331237793,
      "learning_rate": 3.399578440228847e-06,
      "loss": 0.3488,
      "step": 17927
    },
    {
      "epoch": 6.940766550522648,
      "grad_norm": 18.922775268554688,
      "learning_rate": 3.399148277197058e-06,
      "loss": 0.4558,
      "step": 17928
    },
    {
      "epoch": 6.941153697251258,
      "grad_norm": 24.563678741455078,
      "learning_rate": 3.3987181141652684e-06,
      "loss": 0.9528,
      "step": 17929
    },
    {
      "epoch": 6.941540843979868,
      "grad_norm": 17.026458740234375,
      "learning_rate": 3.39828795113348e-06,
      "loss": 0.0947,
      "step": 17930
    },
    {
      "epoch": 6.9419279907084785,
      "grad_norm": 124.81405639648438,
      "learning_rate": 3.3978577881016913e-06,
      "loss": 2.7415,
      "step": 17931
    },
    {
      "epoch": 6.942315137437088,
      "grad_norm": 253.6207275390625,
      "learning_rate": 3.397427625069902e-06,
      "loss": 1.187,
      "step": 17932
    },
    {
      "epoch": 6.942702284165699,
      "grad_norm": 9.573399543762207,
      "learning_rate": 3.396997462038113e-06,
      "loss": 0.7726,
      "step": 17933
    },
    {
      "epoch": 6.943089430894309,
      "grad_norm": 93.8978500366211,
      "learning_rate": 3.396567299006324e-06,
      "loss": 2.2395,
      "step": 17934
    },
    {
      "epoch": 6.943476577622919,
      "grad_norm": 70.45435333251953,
      "learning_rate": 3.396137135974535e-06,
      "loss": 0.7284,
      "step": 17935
    },
    {
      "epoch": 6.943863724351529,
      "grad_norm": 30.602115631103516,
      "learning_rate": 3.3957069729427454e-06,
      "loss": 0.3045,
      "step": 17936
    },
    {
      "epoch": 6.94425087108014,
      "grad_norm": 117.17681884765625,
      "learning_rate": 3.395276809910957e-06,
      "loss": 3.6781,
      "step": 17937
    },
    {
      "epoch": 6.9446380178087495,
      "grad_norm": 46.74944305419922,
      "learning_rate": 3.3948466468791674e-06,
      "loss": 1.3588,
      "step": 17938
    },
    {
      "epoch": 6.94502516453736,
      "grad_norm": 41.81248092651367,
      "learning_rate": 3.3944164838473788e-06,
      "loss": 2.4024,
      "step": 17939
    },
    {
      "epoch": 6.94541231126597,
      "grad_norm": 11.005111694335938,
      "learning_rate": 3.3939863208155893e-06,
      "loss": 0.5697,
      "step": 17940
    },
    {
      "epoch": 6.94579945799458,
      "grad_norm": 42.0886116027832,
      "learning_rate": 3.3935561577838003e-06,
      "loss": 0.7309,
      "step": 17941
    },
    {
      "epoch": 6.94618660472319,
      "grad_norm": 36.71902084350586,
      "learning_rate": 3.3931259947520113e-06,
      "loss": 1.643,
      "step": 17942
    },
    {
      "epoch": 6.9465737514518,
      "grad_norm": 125.58158111572266,
      "learning_rate": 3.3926958317202223e-06,
      "loss": 3.6016,
      "step": 17943
    },
    {
      "epoch": 6.946960898180411,
      "grad_norm": 145.89013671875,
      "learning_rate": 3.392265668688433e-06,
      "loss": 0.9646,
      "step": 17944
    },
    {
      "epoch": 6.94734804490902,
      "grad_norm": 76.11920928955078,
      "learning_rate": 3.3918355056566443e-06,
      "loss": 0.7232,
      "step": 17945
    },
    {
      "epoch": 6.947735191637631,
      "grad_norm": 126.29844665527344,
      "learning_rate": 3.391405342624855e-06,
      "loss": 1.903,
      "step": 17946
    },
    {
      "epoch": 6.9481223383662405,
      "grad_norm": 239.3292694091797,
      "learning_rate": 3.3909751795930663e-06,
      "loss": 2.7877,
      "step": 17947
    },
    {
      "epoch": 6.948509485094851,
      "grad_norm": 27.40382957458496,
      "learning_rate": 3.390545016561277e-06,
      "loss": 1.6473,
      "step": 17948
    },
    {
      "epoch": 6.948896631823461,
      "grad_norm": 60.190208435058594,
      "learning_rate": 3.3901148535294883e-06,
      "loss": 0.3555,
      "step": 17949
    },
    {
      "epoch": 6.949283778552071,
      "grad_norm": 72.21501922607422,
      "learning_rate": 3.389684690497699e-06,
      "loss": 0.6808,
      "step": 17950
    },
    {
      "epoch": 6.949670925280682,
      "grad_norm": 4.538604736328125,
      "learning_rate": 3.38925452746591e-06,
      "loss": 0.0663,
      "step": 17951
    },
    {
      "epoch": 6.950058072009291,
      "grad_norm": 25.877914428710938,
      "learning_rate": 3.388824364434121e-06,
      "loss": 1.6541,
      "step": 17952
    },
    {
      "epoch": 6.950445218737902,
      "grad_norm": 2.7487916946411133,
      "learning_rate": 3.388394201402332e-06,
      "loss": 0.0997,
      "step": 17953
    },
    {
      "epoch": 6.950832365466512,
      "grad_norm": 97.9586181640625,
      "learning_rate": 3.3879640383705424e-06,
      "loss": 1.6863,
      "step": 17954
    },
    {
      "epoch": 6.951219512195122,
      "grad_norm": 47.47054672241211,
      "learning_rate": 3.387533875338754e-06,
      "loss": 1.8914,
      "step": 17955
    },
    {
      "epoch": 6.951606658923732,
      "grad_norm": 93.4120864868164,
      "learning_rate": 3.3871037123069644e-06,
      "loss": 2.8927,
      "step": 17956
    },
    {
      "epoch": 6.951993805652342,
      "grad_norm": 23.089824676513672,
      "learning_rate": 3.386673549275176e-06,
      "loss": 0.3441,
      "step": 17957
    },
    {
      "epoch": 6.9523809523809526,
      "grad_norm": 49.93156051635742,
      "learning_rate": 3.3862433862433864e-06,
      "loss": 0.9644,
      "step": 17958
    },
    {
      "epoch": 6.952768099109562,
      "grad_norm": 41.14244842529297,
      "learning_rate": 3.3858132232115974e-06,
      "loss": 0.6625,
      "step": 17959
    },
    {
      "epoch": 6.953155245838173,
      "grad_norm": 145.78738403320312,
      "learning_rate": 3.3853830601798083e-06,
      "loss": 1.9044,
      "step": 17960
    },
    {
      "epoch": 6.953542392566783,
      "grad_norm": 9.81805419921875,
      "learning_rate": 3.3849528971480193e-06,
      "loss": 0.3244,
      "step": 17961
    },
    {
      "epoch": 6.953929539295393,
      "grad_norm": 81.24503326416016,
      "learning_rate": 3.38452273411623e-06,
      "loss": 0.2774,
      "step": 17962
    },
    {
      "epoch": 6.954316686024003,
      "grad_norm": 184.3164520263672,
      "learning_rate": 3.3840925710844413e-06,
      "loss": 1.667,
      "step": 17963
    },
    {
      "epoch": 6.954703832752613,
      "grad_norm": 20.893234252929688,
      "learning_rate": 3.383662408052652e-06,
      "loss": 0.5217,
      "step": 17964
    },
    {
      "epoch": 6.9550909794812235,
      "grad_norm": 46.48451232910156,
      "learning_rate": 3.3832322450208633e-06,
      "loss": 0.3104,
      "step": 17965
    },
    {
      "epoch": 6.955478126209833,
      "grad_norm": 84.120361328125,
      "learning_rate": 3.382802081989074e-06,
      "loss": 0.8892,
      "step": 17966
    },
    {
      "epoch": 6.955865272938444,
      "grad_norm": 0.399539977312088,
      "learning_rate": 3.3823719189572853e-06,
      "loss": 0.0115,
      "step": 17967
    },
    {
      "epoch": 6.956252419667054,
      "grad_norm": 52.510807037353516,
      "learning_rate": 3.381941755925496e-06,
      "loss": 0.2421,
      "step": 17968
    },
    {
      "epoch": 6.956639566395664,
      "grad_norm": 32.28465270996094,
      "learning_rate": 3.381511592893707e-06,
      "loss": 0.2279,
      "step": 17969
    },
    {
      "epoch": 6.957026713124274,
      "grad_norm": 100.81425476074219,
      "learning_rate": 3.381081429861918e-06,
      "loss": 1.1892,
      "step": 17970
    },
    {
      "epoch": 6.957413859852885,
      "grad_norm": 3.4339327812194824,
      "learning_rate": 3.380651266830129e-06,
      "loss": 0.1044,
      "step": 17971
    },
    {
      "epoch": 6.957801006581494,
      "grad_norm": 46.625328063964844,
      "learning_rate": 3.3802211037983403e-06,
      "loss": 0.6264,
      "step": 17972
    },
    {
      "epoch": 6.958188153310105,
      "grad_norm": 48.60700607299805,
      "learning_rate": 3.379790940766551e-06,
      "loss": 2.6288,
      "step": 17973
    },
    {
      "epoch": 6.9585753000387145,
      "grad_norm": 145.29660034179688,
      "learning_rate": 3.379360777734762e-06,
      "loss": 1.0181,
      "step": 17974
    },
    {
      "epoch": 6.958962446767325,
      "grad_norm": 77.99224090576172,
      "learning_rate": 3.378930614702973e-06,
      "loss": 1.2017,
      "step": 17975
    },
    {
      "epoch": 6.959349593495935,
      "grad_norm": 68.42285919189453,
      "learning_rate": 3.378500451671184e-06,
      "loss": 1.2339,
      "step": 17976
    },
    {
      "epoch": 6.959736740224545,
      "grad_norm": 56.96781921386719,
      "learning_rate": 3.3780702886393944e-06,
      "loss": 1.0911,
      "step": 17977
    },
    {
      "epoch": 6.960123886953156,
      "grad_norm": 142.8821563720703,
      "learning_rate": 3.3776401256076058e-06,
      "loss": 1.2478,
      "step": 17978
    },
    {
      "epoch": 6.960511033681765,
      "grad_norm": 28.303199768066406,
      "learning_rate": 3.3772099625758164e-06,
      "loss": 0.379,
      "step": 17979
    },
    {
      "epoch": 6.960898180410376,
      "grad_norm": 53.11016845703125,
      "learning_rate": 3.3767797995440278e-06,
      "loss": 1.1722,
      "step": 17980
    },
    {
      "epoch": 6.961285327138985,
      "grad_norm": 97.64057922363281,
      "learning_rate": 3.3763496365122383e-06,
      "loss": 0.384,
      "step": 17981
    },
    {
      "epoch": 6.961672473867596,
      "grad_norm": 126.08113861083984,
      "learning_rate": 3.3759194734804498e-06,
      "loss": 1.9775,
      "step": 17982
    },
    {
      "epoch": 6.9620596205962055,
      "grad_norm": 31.069576263427734,
      "learning_rate": 3.3754893104486603e-06,
      "loss": 1.1256,
      "step": 17983
    },
    {
      "epoch": 6.962446767324816,
      "grad_norm": 36.35097885131836,
      "learning_rate": 3.3750591474168713e-06,
      "loss": 3.1174,
      "step": 17984
    },
    {
      "epoch": 6.9628339140534266,
      "grad_norm": 32.17632293701172,
      "learning_rate": 3.3746289843850823e-06,
      "loss": 1.8282,
      "step": 17985
    },
    {
      "epoch": 6.963221060782036,
      "grad_norm": 0.3472159206867218,
      "learning_rate": 3.3741988213532933e-06,
      "loss": 0.0102,
      "step": 17986
    },
    {
      "epoch": 6.963608207510647,
      "grad_norm": 4.136961460113525,
      "learning_rate": 3.373768658321504e-06,
      "loss": 0.0797,
      "step": 17987
    },
    {
      "epoch": 6.963995354239256,
      "grad_norm": 11.026156425476074,
      "learning_rate": 3.3733384952897153e-06,
      "loss": 0.2057,
      "step": 17988
    },
    {
      "epoch": 6.964382500967867,
      "grad_norm": 17.900341033935547,
      "learning_rate": 3.372908332257926e-06,
      "loss": 0.1466,
      "step": 17989
    },
    {
      "epoch": 6.964769647696477,
      "grad_norm": 45.22222137451172,
      "learning_rate": 3.3724781692261373e-06,
      "loss": 1.2863,
      "step": 17990
    },
    {
      "epoch": 6.965156794425087,
      "grad_norm": 60.723880767822266,
      "learning_rate": 3.372048006194348e-06,
      "loss": 1.0271,
      "step": 17991
    },
    {
      "epoch": 6.9655439411536975,
      "grad_norm": 1.9039454460144043,
      "learning_rate": 3.371617843162559e-06,
      "loss": 0.0771,
      "step": 17992
    },
    {
      "epoch": 6.965931087882307,
      "grad_norm": 146.17591857910156,
      "learning_rate": 3.37118768013077e-06,
      "loss": 1.2977,
      "step": 17993
    },
    {
      "epoch": 6.966318234610918,
      "grad_norm": 77.18399810791016,
      "learning_rate": 3.370757517098981e-06,
      "loss": 0.5568,
      "step": 17994
    },
    {
      "epoch": 6.966705381339528,
      "grad_norm": 0.31435590982437134,
      "learning_rate": 3.3703273540671914e-06,
      "loss": 0.008,
      "step": 17995
    },
    {
      "epoch": 6.967092528068138,
      "grad_norm": 4.934040069580078,
      "learning_rate": 3.369897191035403e-06,
      "loss": 0.1151,
      "step": 17996
    },
    {
      "epoch": 6.967479674796748,
      "grad_norm": 2.082812547683716,
      "learning_rate": 3.3694670280036134e-06,
      "loss": 0.0594,
      "step": 17997
    },
    {
      "epoch": 6.967866821525358,
      "grad_norm": 102.40740966796875,
      "learning_rate": 3.3690368649718248e-06,
      "loss": 2.2374,
      "step": 17998
    },
    {
      "epoch": 6.968253968253968,
      "grad_norm": 53.984649658203125,
      "learning_rate": 3.3686067019400353e-06,
      "loss": 1.0773,
      "step": 17999
    },
    {
      "epoch": 6.968641114982578,
      "grad_norm": 4.235306262969971,
      "learning_rate": 3.3681765389082468e-06,
      "loss": 0.2298,
      "step": 18000
    },
    {
      "epoch": 6.9690282617111885,
      "grad_norm": 9.63077449798584,
      "learning_rate": 3.3677463758764573e-06,
      "loss": 0.2407,
      "step": 18001
    },
    {
      "epoch": 6.969415408439799,
      "grad_norm": 1.3602429628372192,
      "learning_rate": 3.3673162128446683e-06,
      "loss": 0.06,
      "step": 18002
    },
    {
      "epoch": 6.969802555168409,
      "grad_norm": 31.335308074951172,
      "learning_rate": 3.3668860498128793e-06,
      "loss": 1.9923,
      "step": 18003
    },
    {
      "epoch": 6.970189701897019,
      "grad_norm": 44.79970932006836,
      "learning_rate": 3.3664558867810903e-06,
      "loss": 1.569,
      "step": 18004
    },
    {
      "epoch": 6.970576848625629,
      "grad_norm": 3.8492588996887207,
      "learning_rate": 3.366025723749301e-06,
      "loss": 0.1187,
      "step": 18005
    },
    {
      "epoch": 6.970963995354239,
      "grad_norm": 54.51559829711914,
      "learning_rate": 3.3655955607175123e-06,
      "loss": 1.7848,
      "step": 18006
    },
    {
      "epoch": 6.97135114208285,
      "grad_norm": 42.33557891845703,
      "learning_rate": 3.365165397685723e-06,
      "loss": 1.4987,
      "step": 18007
    },
    {
      "epoch": 6.971738288811459,
      "grad_norm": 120.30680847167969,
      "learning_rate": 3.3647352346539343e-06,
      "loss": 2.739,
      "step": 18008
    },
    {
      "epoch": 6.97212543554007,
      "grad_norm": 3.8260462284088135,
      "learning_rate": 3.364305071622145e-06,
      "loss": 0.0906,
      "step": 18009
    },
    {
      "epoch": 6.9725125822686795,
      "grad_norm": 33.81477737426758,
      "learning_rate": 3.363874908590356e-06,
      "loss": 1.2129,
      "step": 18010
    },
    {
      "epoch": 6.97289972899729,
      "grad_norm": 42.837738037109375,
      "learning_rate": 3.363444745558567e-06,
      "loss": 0.195,
      "step": 18011
    },
    {
      "epoch": 6.973286875725901,
      "grad_norm": 44.59908676147461,
      "learning_rate": 3.363014582526778e-06,
      "loss": 0.3369,
      "step": 18012
    },
    {
      "epoch": 6.97367402245451,
      "grad_norm": 165.5467987060547,
      "learning_rate": 3.3625844194949892e-06,
      "loss": 0.2854,
      "step": 18013
    },
    {
      "epoch": 6.974061169183121,
      "grad_norm": 29.902450561523438,
      "learning_rate": 3.3621542564632e-06,
      "loss": 0.1734,
      "step": 18014
    },
    {
      "epoch": 6.97444831591173,
      "grad_norm": 15.580896377563477,
      "learning_rate": 3.3617240934314112e-06,
      "loss": 0.3275,
      "step": 18015
    },
    {
      "epoch": 6.974835462640341,
      "grad_norm": 26.318418502807617,
      "learning_rate": 3.361293930399622e-06,
      "loss": 1.7845,
      "step": 18016
    },
    {
      "epoch": 6.97522260936895,
      "grad_norm": 65.22309112548828,
      "learning_rate": 3.3608637673678328e-06,
      "loss": 1.014,
      "step": 18017
    },
    {
      "epoch": 6.975609756097561,
      "grad_norm": 64.76811981201172,
      "learning_rate": 3.3604336043360438e-06,
      "loss": 1.2676,
      "step": 18018
    },
    {
      "epoch": 6.9759969028261715,
      "grad_norm": 2.47536039352417,
      "learning_rate": 3.3600034413042548e-06,
      "loss": 0.0686,
      "step": 18019
    },
    {
      "epoch": 6.976384049554781,
      "grad_norm": 183.1161346435547,
      "learning_rate": 3.3595732782724653e-06,
      "loss": 2.9049,
      "step": 18020
    },
    {
      "epoch": 6.976771196283392,
      "grad_norm": 5.406613826751709,
      "learning_rate": 3.3591431152406768e-06,
      "loss": 0.1047,
      "step": 18021
    },
    {
      "epoch": 6.977158343012001,
      "grad_norm": 6.599511623382568,
      "learning_rate": 3.3587129522088873e-06,
      "loss": 0.1825,
      "step": 18022
    },
    {
      "epoch": 6.977545489740612,
      "grad_norm": 92.91204833984375,
      "learning_rate": 3.3582827891770987e-06,
      "loss": 0.6803,
      "step": 18023
    },
    {
      "epoch": 6.977932636469221,
      "grad_norm": 84.6150131225586,
      "learning_rate": 3.3578526261453093e-06,
      "loss": 2.8795,
      "step": 18024
    },
    {
      "epoch": 6.978319783197832,
      "grad_norm": 2.421079158782959,
      "learning_rate": 3.3574224631135203e-06,
      "loss": 0.117,
      "step": 18025
    },
    {
      "epoch": 6.978706929926442,
      "grad_norm": 9.188634872436523,
      "learning_rate": 3.3569923000817313e-06,
      "loss": 0.2461,
      "step": 18026
    },
    {
      "epoch": 6.979094076655052,
      "grad_norm": 2.8503663539886475,
      "learning_rate": 3.3565621370499423e-06,
      "loss": 0.07,
      "step": 18027
    },
    {
      "epoch": 6.9794812233836625,
      "grad_norm": 11.297945976257324,
      "learning_rate": 3.356131974018153e-06,
      "loss": 1.1068,
      "step": 18028
    },
    {
      "epoch": 6.979868370112273,
      "grad_norm": 2.401106595993042,
      "learning_rate": 3.3557018109863643e-06,
      "loss": 0.0985,
      "step": 18029
    },
    {
      "epoch": 6.980255516840883,
      "grad_norm": 60.801578521728516,
      "learning_rate": 3.355271647954575e-06,
      "loss": 1.4016,
      "step": 18030
    },
    {
      "epoch": 6.980642663569493,
      "grad_norm": 41.45427322387695,
      "learning_rate": 3.3548414849227863e-06,
      "loss": 1.7455,
      "step": 18031
    },
    {
      "epoch": 6.981029810298103,
      "grad_norm": 39.13978576660156,
      "learning_rate": 3.354411321890997e-06,
      "loss": 1.525,
      "step": 18032
    },
    {
      "epoch": 6.981416957026713,
      "grad_norm": 57.7957763671875,
      "learning_rate": 3.3539811588592082e-06,
      "loss": 2.4404,
      "step": 18033
    },
    {
      "epoch": 6.981804103755323,
      "grad_norm": 38.814998626708984,
      "learning_rate": 3.353550995827419e-06,
      "loss": 0.1151,
      "step": 18034
    },
    {
      "epoch": 6.982191250483933,
      "grad_norm": 107.82579803466797,
      "learning_rate": 3.35312083279563e-06,
      "loss": 0.9865,
      "step": 18035
    },
    {
      "epoch": 6.982578397212544,
      "grad_norm": 194.35382080078125,
      "learning_rate": 3.352690669763841e-06,
      "loss": 0.439,
      "step": 18036
    },
    {
      "epoch": 6.9829655439411535,
      "grad_norm": 49.644046783447266,
      "learning_rate": 3.3522605067320518e-06,
      "loss": 1.1039,
      "step": 18037
    },
    {
      "epoch": 6.983352690669764,
      "grad_norm": 59.339298248291016,
      "learning_rate": 3.3518303437002624e-06,
      "loss": 3.4101,
      "step": 18038
    },
    {
      "epoch": 6.983739837398374,
      "grad_norm": 99.5078353881836,
      "learning_rate": 3.3514001806684738e-06,
      "loss": 1.8497,
      "step": 18039
    },
    {
      "epoch": 6.984126984126984,
      "grad_norm": 45.261905670166016,
      "learning_rate": 3.3509700176366843e-06,
      "loss": 1.2558,
      "step": 18040
    },
    {
      "epoch": 6.984514130855594,
      "grad_norm": 170.23663330078125,
      "learning_rate": 3.3505398546048958e-06,
      "loss": 0.3078,
      "step": 18041
    },
    {
      "epoch": 6.984901277584204,
      "grad_norm": 9.457770347595215,
      "learning_rate": 3.3501096915731063e-06,
      "loss": 0.1532,
      "step": 18042
    },
    {
      "epoch": 6.985288424312815,
      "grad_norm": 4.50407075881958,
      "learning_rate": 3.3496795285413173e-06,
      "loss": 0.1016,
      "step": 18043
    },
    {
      "epoch": 6.9856755710414244,
      "grad_norm": 46.33267593383789,
      "learning_rate": 3.3492493655095283e-06,
      "loss": 3.2289,
      "step": 18044
    },
    {
      "epoch": 6.986062717770035,
      "grad_norm": 51.905677795410156,
      "learning_rate": 3.3488192024777393e-06,
      "loss": 2.8876,
      "step": 18045
    },
    {
      "epoch": 6.9864498644986455,
      "grad_norm": 3.3065531253814697,
      "learning_rate": 3.34838903944595e-06,
      "loss": 0.0983,
      "step": 18046
    },
    {
      "epoch": 6.986837011227255,
      "grad_norm": 42.1514892578125,
      "learning_rate": 3.3479588764141613e-06,
      "loss": 0.4956,
      "step": 18047
    },
    {
      "epoch": 6.987224157955866,
      "grad_norm": 4.836465835571289,
      "learning_rate": 3.347528713382372e-06,
      "loss": 0.1327,
      "step": 18048
    },
    {
      "epoch": 6.987611304684475,
      "grad_norm": 0.42026451230049133,
      "learning_rate": 3.3470985503505833e-06,
      "loss": 0.0104,
      "step": 18049
    },
    {
      "epoch": 6.987998451413086,
      "grad_norm": 92.5092544555664,
      "learning_rate": 3.346668387318794e-06,
      "loss": 0.2666,
      "step": 18050
    },
    {
      "epoch": 6.988385598141695,
      "grad_norm": 101.02532196044922,
      "learning_rate": 3.3462382242870053e-06,
      "loss": 1.1243,
      "step": 18051
    },
    {
      "epoch": 6.988772744870306,
      "grad_norm": 3.9104342460632324,
      "learning_rate": 3.345808061255216e-06,
      "loss": 0.1228,
      "step": 18052
    },
    {
      "epoch": 6.989159891598916,
      "grad_norm": 35.19221496582031,
      "learning_rate": 3.345377898223427e-06,
      "loss": 1.8144,
      "step": 18053
    },
    {
      "epoch": 6.989547038327526,
      "grad_norm": 50.45186996459961,
      "learning_rate": 3.3449477351916382e-06,
      "loss": 1.0314,
      "step": 18054
    },
    {
      "epoch": 6.9899341850561365,
      "grad_norm": 58.22872543334961,
      "learning_rate": 3.344517572159849e-06,
      "loss": 0.5022,
      "step": 18055
    },
    {
      "epoch": 6.990321331784746,
      "grad_norm": 4.091630935668945,
      "learning_rate": 3.3440874091280602e-06,
      "loss": 0.0507,
      "step": 18056
    },
    {
      "epoch": 6.990708478513357,
      "grad_norm": 73.2872085571289,
      "learning_rate": 3.3436572460962708e-06,
      "loss": 1.7162,
      "step": 18057
    },
    {
      "epoch": 6.991095625241966,
      "grad_norm": 40.615787506103516,
      "learning_rate": 3.3432270830644818e-06,
      "loss": 0.2236,
      "step": 18058
    },
    {
      "epoch": 6.991482771970577,
      "grad_norm": 6.042634010314941,
      "learning_rate": 3.3427969200326928e-06,
      "loss": 0.191,
      "step": 18059
    },
    {
      "epoch": 6.991869918699187,
      "grad_norm": 81.208984375,
      "learning_rate": 3.3423667570009038e-06,
      "loss": 0.9939,
      "step": 18060
    },
    {
      "epoch": 6.992257065427797,
      "grad_norm": 1.3775124549865723,
      "learning_rate": 3.3419365939691143e-06,
      "loss": 0.0388,
      "step": 18061
    },
    {
      "epoch": 6.992644212156407,
      "grad_norm": 28.42531967163086,
      "learning_rate": 3.3415064309373257e-06,
      "loss": 1.8727,
      "step": 18062
    },
    {
      "epoch": 6.993031358885017,
      "grad_norm": 125.51176452636719,
      "learning_rate": 3.3410762679055363e-06,
      "loss": 0.6845,
      "step": 18063
    },
    {
      "epoch": 6.9934185056136275,
      "grad_norm": 55.680877685546875,
      "learning_rate": 3.3406461048737477e-06,
      "loss": 0.7757,
      "step": 18064
    },
    {
      "epoch": 6.993805652342238,
      "grad_norm": 30.89479637145996,
      "learning_rate": 3.3402159418419583e-06,
      "loss": 1.7728,
      "step": 18065
    },
    {
      "epoch": 6.994192799070848,
      "grad_norm": 36.41496658325195,
      "learning_rate": 3.3397857788101697e-06,
      "loss": 2.6572,
      "step": 18066
    },
    {
      "epoch": 6.994579945799458,
      "grad_norm": 1.537683367729187,
      "learning_rate": 3.3393556157783803e-06,
      "loss": 0.0432,
      "step": 18067
    },
    {
      "epoch": 6.994967092528068,
      "grad_norm": 102.54216766357422,
      "learning_rate": 3.3389254527465913e-06,
      "loss": 0.3587,
      "step": 18068
    },
    {
      "epoch": 6.995354239256678,
      "grad_norm": 88.05677032470703,
      "learning_rate": 3.3384952897148023e-06,
      "loss": 1.9987,
      "step": 18069
    },
    {
      "epoch": 6.995741385985289,
      "grad_norm": 169.49795532226562,
      "learning_rate": 3.3380651266830133e-06,
      "loss": 0.3413,
      "step": 18070
    },
    {
      "epoch": 6.9961285327138985,
      "grad_norm": 92.23121643066406,
      "learning_rate": 3.337634963651224e-06,
      "loss": 3.717,
      "step": 18071
    },
    {
      "epoch": 6.996515679442509,
      "grad_norm": 126.40686798095703,
      "learning_rate": 3.3372048006194352e-06,
      "loss": 0.5037,
      "step": 18072
    },
    {
      "epoch": 6.996902826171119,
      "grad_norm": 48.89601516723633,
      "learning_rate": 3.336774637587646e-06,
      "loss": 1.7543,
      "step": 18073
    },
    {
      "epoch": 6.997289972899729,
      "grad_norm": 35.93513488769531,
      "learning_rate": 3.3363444745558572e-06,
      "loss": 2.1334,
      "step": 18074
    },
    {
      "epoch": 6.997677119628339,
      "grad_norm": 65.93344116210938,
      "learning_rate": 3.335914311524068e-06,
      "loss": 4.068,
      "step": 18075
    },
    {
      "epoch": 6.998064266356949,
      "grad_norm": 88.83814239501953,
      "learning_rate": 3.3354841484922788e-06,
      "loss": 1.1349,
      "step": 18076
    },
    {
      "epoch": 6.99845141308556,
      "grad_norm": 56.32497787475586,
      "learning_rate": 3.3350539854604898e-06,
      "loss": 1.7951,
      "step": 18077
    },
    {
      "epoch": 6.998838559814169,
      "grad_norm": 40.190391540527344,
      "learning_rate": 3.3346238224287008e-06,
      "loss": 1.1843,
      "step": 18078
    },
    {
      "epoch": 6.99922570654278,
      "grad_norm": 2.8150527477264404,
      "learning_rate": 3.3341936593969113e-06,
      "loss": 0.1097,
      "step": 18079
    },
    {
      "epoch": 6.9996128532713895,
      "grad_norm": 49.608001708984375,
      "learning_rate": 3.3337634963651228e-06,
      "loss": 0.6782,
      "step": 18080
    },
    {
      "epoch": 7.0,
      "grad_norm": 9.259533882141113,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.4023,
      "step": 18081
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.5886792452830188,
      "eval_f1": 0.5767473347850071,
      "eval_loss": 1.782976746559143,
      "eval_runtime": 383.5131,
      "eval_samples_per_second": 2.764,
      "eval_steps_per_second": 1.382,
      "step": 18081
    },
    {
      "epoch": 7.0003871467286105,
      "grad_norm": 44.202274322509766,
      "learning_rate": 3.3329031703015447e-06,
      "loss": 0.7706,
      "step": 18082
    },
    {
      "epoch": 7.00077429345722,
      "grad_norm": 83.87547302246094,
      "learning_rate": 3.3324730072697553e-06,
      "loss": 1.9051,
      "step": 18083
    },
    {
      "epoch": 7.001161440185831,
      "grad_norm": 3.164097547531128,
      "learning_rate": 3.3320428442379667e-06,
      "loss": 0.1672,
      "step": 18084
    },
    {
      "epoch": 7.00154858691444,
      "grad_norm": 34.69268798828125,
      "learning_rate": 3.3316126812061773e-06,
      "loss": 0.2499,
      "step": 18085
    },
    {
      "epoch": 7.001935733643051,
      "grad_norm": 56.77085494995117,
      "learning_rate": 3.3311825181743883e-06,
      "loss": 0.997,
      "step": 18086
    },
    {
      "epoch": 7.002322880371661,
      "grad_norm": 4.174006938934326,
      "learning_rate": 3.3307523551425993e-06,
      "loss": 0.1712,
      "step": 18087
    },
    {
      "epoch": 7.002710027100271,
      "grad_norm": 23.796497344970703,
      "learning_rate": 3.3303221921108103e-06,
      "loss": 1.9303,
      "step": 18088
    },
    {
      "epoch": 7.003097173828881,
      "grad_norm": 39.323307037353516,
      "learning_rate": 3.329892029079021e-06,
      "loss": 1.7503,
      "step": 18089
    },
    {
      "epoch": 7.003484320557491,
      "grad_norm": 140.91896057128906,
      "learning_rate": 3.3294618660472323e-06,
      "loss": 4.755,
      "step": 18090
    },
    {
      "epoch": 7.0038714672861015,
      "grad_norm": 4.245402812957764,
      "learning_rate": 3.329031703015443e-06,
      "loss": 0.1035,
      "step": 18091
    },
    {
      "epoch": 7.004258614014711,
      "grad_norm": 2.352785348892212,
      "learning_rate": 3.3286015399836542e-06,
      "loss": 0.0623,
      "step": 18092
    },
    {
      "epoch": 7.004645760743322,
      "grad_norm": 52.062530517578125,
      "learning_rate": 3.328171376951865e-06,
      "loss": 1.5856,
      "step": 18093
    },
    {
      "epoch": 7.005032907471932,
      "grad_norm": 131.25076293945312,
      "learning_rate": 3.327741213920076e-06,
      "loss": 0.5168,
      "step": 18094
    },
    {
      "epoch": 7.005420054200542,
      "grad_norm": 3.1254820823669434,
      "learning_rate": 3.3273110508882872e-06,
      "loss": 0.0709,
      "step": 18095
    },
    {
      "epoch": 7.005807200929152,
      "grad_norm": 99.38420104980469,
      "learning_rate": 3.3268808878564978e-06,
      "loss": 1.1045,
      "step": 18096
    },
    {
      "epoch": 7.006194347657762,
      "grad_norm": 33.89779281616211,
      "learning_rate": 3.326450724824709e-06,
      "loss": 1.6409,
      "step": 18097
    },
    {
      "epoch": 7.0065814943863725,
      "grad_norm": 3.7132937908172607,
      "learning_rate": 3.3260205617929198e-06,
      "loss": 0.1143,
      "step": 18098
    },
    {
      "epoch": 7.006968641114983,
      "grad_norm": 111.46456146240234,
      "learning_rate": 3.325590398761131e-06,
      "loss": 2.9331,
      "step": 18099
    },
    {
      "epoch": 7.007355787843593,
      "grad_norm": 96.6546401977539,
      "learning_rate": 3.3251602357293418e-06,
      "loss": 2.0966,
      "step": 18100
    },
    {
      "epoch": 7.007742934572203,
      "grad_norm": 342.2303771972656,
      "learning_rate": 3.3247300726975527e-06,
      "loss": 1.5442,
      "step": 18101
    },
    {
      "epoch": 7.008130081300813,
      "grad_norm": 2.736374855041504,
      "learning_rate": 3.3242999096657637e-06,
      "loss": 0.1078,
      "step": 18102
    },
    {
      "epoch": 7.008517228029423,
      "grad_norm": 51.232521057128906,
      "learning_rate": 3.3238697466339747e-06,
      "loss": 3.1134,
      "step": 18103
    },
    {
      "epoch": 7.008904374758034,
      "grad_norm": 0.8450824022293091,
      "learning_rate": 3.3234395836021853e-06,
      "loss": 0.0118,
      "step": 18104
    },
    {
      "epoch": 7.009291521486643,
      "grad_norm": 25.331802368164062,
      "learning_rate": 3.3230094205703967e-06,
      "loss": 1.3843,
      "step": 18105
    },
    {
      "epoch": 7.009678668215254,
      "grad_norm": 3.19140887260437,
      "learning_rate": 3.3225792575386073e-06,
      "loss": 0.0743,
      "step": 18106
    },
    {
      "epoch": 7.0100658149438635,
      "grad_norm": 177.59828186035156,
      "learning_rate": 3.3221490945068187e-06,
      "loss": 0.5005,
      "step": 18107
    },
    {
      "epoch": 7.010452961672474,
      "grad_norm": 1.4786485433578491,
      "learning_rate": 3.3217189314750293e-06,
      "loss": 0.0749,
      "step": 18108
    },
    {
      "epoch": 7.010840108401084,
      "grad_norm": 128.78546142578125,
      "learning_rate": 3.3212887684432403e-06,
      "loss": 1.1838,
      "step": 18109
    },
    {
      "epoch": 7.011227255129694,
      "grad_norm": 22.300207138061523,
      "learning_rate": 3.3208586054114513e-06,
      "loss": 1.7984,
      "step": 18110
    },
    {
      "epoch": 7.011614401858305,
      "grad_norm": 36.40981674194336,
      "learning_rate": 3.3204284423796622e-06,
      "loss": 0.2145,
      "step": 18111
    },
    {
      "epoch": 7.012001548586914,
      "grad_norm": 22.008686065673828,
      "learning_rate": 3.319998279347873e-06,
      "loss": 0.2556,
      "step": 18112
    },
    {
      "epoch": 7.012388695315525,
      "grad_norm": 15.476442337036133,
      "learning_rate": 3.3195681163160842e-06,
      "loss": 0.4014,
      "step": 18113
    },
    {
      "epoch": 7.012775842044134,
      "grad_norm": 96.28895568847656,
      "learning_rate": 3.319137953284295e-06,
      "loss": 0.9758,
      "step": 18114
    },
    {
      "epoch": 7.013162988772745,
      "grad_norm": 50.32124710083008,
      "learning_rate": 3.3187077902525062e-06,
      "loss": 0.6702,
      "step": 18115
    },
    {
      "epoch": 7.013550135501355,
      "grad_norm": 1.6516307592391968,
      "learning_rate": 3.3182776272207168e-06,
      "loss": 0.0644,
      "step": 18116
    },
    {
      "epoch": 7.013937282229965,
      "grad_norm": 8.845272064208984,
      "learning_rate": 3.317847464188928e-06,
      "loss": 0.3508,
      "step": 18117
    },
    {
      "epoch": 7.0143244289585756,
      "grad_norm": 24.714567184448242,
      "learning_rate": 3.3174173011571388e-06,
      "loss": 0.1469,
      "step": 18118
    },
    {
      "epoch": 7.014711575687185,
      "grad_norm": 2.5321204662323,
      "learning_rate": 3.3169871381253498e-06,
      "loss": 0.0698,
      "step": 18119
    },
    {
      "epoch": 7.015098722415796,
      "grad_norm": 61.32563400268555,
      "learning_rate": 3.3165569750935608e-06,
      "loss": 0.8579,
      "step": 18120
    },
    {
      "epoch": 7.015485869144405,
      "grad_norm": 34.562400817871094,
      "learning_rate": 3.3161268120617717e-06,
      "loss": 0.1626,
      "step": 18121
    },
    {
      "epoch": 7.015873015873016,
      "grad_norm": 64.2581787109375,
      "learning_rate": 3.3156966490299823e-06,
      "loss": 1.4412,
      "step": 18122
    },
    {
      "epoch": 7.016260162601626,
      "grad_norm": 19.22545623779297,
      "learning_rate": 3.3152664859981937e-06,
      "loss": 2.4421,
      "step": 18123
    },
    {
      "epoch": 7.016647309330236,
      "grad_norm": 21.476177215576172,
      "learning_rate": 3.3148363229664043e-06,
      "loss": 0.1583,
      "step": 18124
    },
    {
      "epoch": 7.0170344560588465,
      "grad_norm": 21.610904693603516,
      "learning_rate": 3.3144061599346157e-06,
      "loss": 0.2238,
      "step": 18125
    },
    {
      "epoch": 7.017421602787456,
      "grad_norm": 3.1836764812469482,
      "learning_rate": 3.3139759969028263e-06,
      "loss": 0.1804,
      "step": 18126
    },
    {
      "epoch": 7.017808749516067,
      "grad_norm": 13.485332489013672,
      "learning_rate": 3.3135458338710373e-06,
      "loss": 0.1103,
      "step": 18127
    },
    {
      "epoch": 7.018195896244677,
      "grad_norm": 42.38595199584961,
      "learning_rate": 3.3131156708392483e-06,
      "loss": 2.4154,
      "step": 18128
    },
    {
      "epoch": 7.018583042973287,
      "grad_norm": 227.8695526123047,
      "learning_rate": 3.3126855078074593e-06,
      "loss": 1.0518,
      "step": 18129
    },
    {
      "epoch": 7.018970189701897,
      "grad_norm": 84.44152069091797,
      "learning_rate": 3.31225534477567e-06,
      "loss": 4.4037,
      "step": 18130
    },
    {
      "epoch": 7.019357336430507,
      "grad_norm": 2.5562546253204346,
      "learning_rate": 3.3118251817438812e-06,
      "loss": 0.0928,
      "step": 18131
    },
    {
      "epoch": 7.019744483159117,
      "grad_norm": 2.747788429260254,
      "learning_rate": 3.311395018712092e-06,
      "loss": 0.0573,
      "step": 18132
    },
    {
      "epoch": 7.020131629887728,
      "grad_norm": 95.8337631225586,
      "learning_rate": 3.3109648556803032e-06,
      "loss": 1.246,
      "step": 18133
    },
    {
      "epoch": 7.0205187766163375,
      "grad_norm": 5.6489176750183105,
      "learning_rate": 3.310534692648514e-06,
      "loss": 0.1998,
      "step": 18134
    },
    {
      "epoch": 7.020905923344948,
      "grad_norm": 20.522287368774414,
      "learning_rate": 3.310104529616725e-06,
      "loss": 3.7054,
      "step": 18135
    },
    {
      "epoch": 7.021293070073558,
      "grad_norm": 4.31626558303833,
      "learning_rate": 3.309674366584936e-06,
      "loss": 0.0935,
      "step": 18136
    },
    {
      "epoch": 7.021680216802168,
      "grad_norm": 43.46659851074219,
      "learning_rate": 3.3092442035531468e-06,
      "loss": 1.1477,
      "step": 18137
    },
    {
      "epoch": 7.022067363530778,
      "grad_norm": 57.67817306518555,
      "learning_rate": 3.308814040521358e-06,
      "loss": 1.2474,
      "step": 18138
    },
    {
      "epoch": 7.022454510259388,
      "grad_norm": 47.32870101928711,
      "learning_rate": 3.3083838774895688e-06,
      "loss": 2.4217,
      "step": 18139
    },
    {
      "epoch": 7.022841656987999,
      "grad_norm": 1.2791506052017212,
      "learning_rate": 3.30795371445778e-06,
      "loss": 0.0582,
      "step": 18140
    },
    {
      "epoch": 7.023228803716608,
      "grad_norm": 45.78659439086914,
      "learning_rate": 3.3075235514259907e-06,
      "loss": 2.661,
      "step": 18141
    },
    {
      "epoch": 7.023615950445219,
      "grad_norm": 2.612865447998047,
      "learning_rate": 3.3070933883942017e-06,
      "loss": 0.0916,
      "step": 18142
    },
    {
      "epoch": 7.0240030971738285,
      "grad_norm": 47.50080108642578,
      "learning_rate": 3.3066632253624127e-06,
      "loss": 1.7539,
      "step": 18143
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 19.481855392456055,
      "learning_rate": 3.3062330623306237e-06,
      "loss": 0.1499,
      "step": 18144
    },
    {
      "epoch": 7.02477739063105,
      "grad_norm": 88.53274536132812,
      "learning_rate": 3.3058028992988343e-06,
      "loss": 1.2052,
      "step": 18145
    },
    {
      "epoch": 7.025164537359659,
      "grad_norm": 100.6812744140625,
      "learning_rate": 3.3053727362670457e-06,
      "loss": 2.495,
      "step": 18146
    },
    {
      "epoch": 7.02555168408827,
      "grad_norm": 166.0907745361328,
      "learning_rate": 3.3049425732352563e-06,
      "loss": 0.8694,
      "step": 18147
    },
    {
      "epoch": 7.025938830816879,
      "grad_norm": 1.1226935386657715,
      "learning_rate": 3.3045124102034677e-06,
      "loss": 0.0444,
      "step": 18148
    },
    {
      "epoch": 7.02632597754549,
      "grad_norm": 2.018423318862915,
      "learning_rate": 3.3040822471716783e-06,
      "loss": 0.0832,
      "step": 18149
    },
    {
      "epoch": 7.026713124274099,
      "grad_norm": 18.975698471069336,
      "learning_rate": 3.3036520841398897e-06,
      "loss": 0.3757,
      "step": 18150
    },
    {
      "epoch": 7.02710027100271,
      "grad_norm": 35.023983001708984,
      "learning_rate": 3.3032219211081002e-06,
      "loss": 0.398,
      "step": 18151
    },
    {
      "epoch": 7.0274874177313205,
      "grad_norm": 1.6786673069000244,
      "learning_rate": 3.3027917580763112e-06,
      "loss": 0.0459,
      "step": 18152
    },
    {
      "epoch": 7.02787456445993,
      "grad_norm": 221.74533081054688,
      "learning_rate": 3.3023615950445222e-06,
      "loss": 0.8402,
      "step": 18153
    },
    {
      "epoch": 7.028261711188541,
      "grad_norm": 67.92984771728516,
      "learning_rate": 3.3019314320127332e-06,
      "loss": 1.9073,
      "step": 18154
    },
    {
      "epoch": 7.02864885791715,
      "grad_norm": 59.18413162231445,
      "learning_rate": 3.3015012689809438e-06,
      "loss": 0.4914,
      "step": 18155
    },
    {
      "epoch": 7.029036004645761,
      "grad_norm": 18.757699966430664,
      "learning_rate": 3.301071105949155e-06,
      "loss": 0.1778,
      "step": 18156
    },
    {
      "epoch": 7.029423151374371,
      "grad_norm": 97.58902740478516,
      "learning_rate": 3.3006409429173658e-06,
      "loss": 1.4859,
      "step": 18157
    },
    {
      "epoch": 7.029810298102981,
      "grad_norm": 16.81132698059082,
      "learning_rate": 3.300210779885577e-06,
      "loss": 0.1938,
      "step": 18158
    },
    {
      "epoch": 7.030197444831591,
      "grad_norm": 2.3160903453826904,
      "learning_rate": 3.2997806168537878e-06,
      "loss": 0.0961,
      "step": 18159
    },
    {
      "epoch": 7.030584591560201,
      "grad_norm": 52.595088958740234,
      "learning_rate": 3.2993504538219987e-06,
      "loss": 0.3339,
      "step": 18160
    },
    {
      "epoch": 7.0309717382888115,
      "grad_norm": 40.07204055786133,
      "learning_rate": 3.2989202907902097e-06,
      "loss": 1.4598,
      "step": 18161
    },
    {
      "epoch": 7.031358885017422,
      "grad_norm": 18.911725997924805,
      "learning_rate": 3.2984901277584207e-06,
      "loss": 0.1444,
      "step": 18162
    },
    {
      "epoch": 7.031746031746032,
      "grad_norm": 5.94926118850708,
      "learning_rate": 3.2980599647266313e-06,
      "loss": 0.083,
      "step": 18163
    },
    {
      "epoch": 7.032133178474642,
      "grad_norm": 79.59992980957031,
      "learning_rate": 3.2976298016948427e-06,
      "loss": 0.4409,
      "step": 18164
    },
    {
      "epoch": 7.032520325203252,
      "grad_norm": 14.301562309265137,
      "learning_rate": 3.2971996386630533e-06,
      "loss": 0.1658,
      "step": 18165
    },
    {
      "epoch": 7.032907471931862,
      "grad_norm": 4.687490940093994,
      "learning_rate": 3.2967694756312647e-06,
      "loss": 0.1646,
      "step": 18166
    },
    {
      "epoch": 7.033294618660472,
      "grad_norm": 100.49285888671875,
      "learning_rate": 3.2963393125994753e-06,
      "loss": 1.3127,
      "step": 18167
    },
    {
      "epoch": 7.033681765389082,
      "grad_norm": 1.2815427780151367,
      "learning_rate": 3.2959091495676867e-06,
      "loss": 0.0376,
      "step": 18168
    },
    {
      "epoch": 7.034068912117693,
      "grad_norm": 18.361209869384766,
      "learning_rate": 3.2954789865358973e-06,
      "loss": 1.6151,
      "step": 18169
    },
    {
      "epoch": 7.0344560588463025,
      "grad_norm": 4.253819942474365,
      "learning_rate": 3.2950488235041082e-06,
      "loss": 0.1629,
      "step": 18170
    },
    {
      "epoch": 7.034843205574913,
      "grad_norm": 172.27023315429688,
      "learning_rate": 3.2946186604723192e-06,
      "loss": 2.5202,
      "step": 18171
    },
    {
      "epoch": 7.035230352303523,
      "grad_norm": 30.152660369873047,
      "learning_rate": 3.2941884974405302e-06,
      "loss": 1.6416,
      "step": 18172
    },
    {
      "epoch": 7.035617499032133,
      "grad_norm": 42.21393966674805,
      "learning_rate": 3.293758334408741e-06,
      "loss": 0.3885,
      "step": 18173
    },
    {
      "epoch": 7.036004645760744,
      "grad_norm": 162.87278747558594,
      "learning_rate": 3.2933281713769522e-06,
      "loss": 3.0181,
      "step": 18174
    },
    {
      "epoch": 7.036391792489353,
      "grad_norm": 13.268854141235352,
      "learning_rate": 3.2928980083451628e-06,
      "loss": 0.1369,
      "step": 18175
    },
    {
      "epoch": 7.036778939217964,
      "grad_norm": 116.1937484741211,
      "learning_rate": 3.292467845313374e-06,
      "loss": 3.6836,
      "step": 18176
    },
    {
      "epoch": 7.0371660859465734,
      "grad_norm": 15.880470275878906,
      "learning_rate": 3.2920376822815848e-06,
      "loss": 0.2645,
      "step": 18177
    },
    {
      "epoch": 7.037553232675184,
      "grad_norm": 6.6041717529296875,
      "learning_rate": 3.2916075192497958e-06,
      "loss": 0.1483,
      "step": 18178
    },
    {
      "epoch": 7.0379403794037945,
      "grad_norm": 4.038273811340332,
      "learning_rate": 3.291177356218007e-06,
      "loss": 0.1042,
      "step": 18179
    },
    {
      "epoch": 7.038327526132404,
      "grad_norm": 1.7109955549240112,
      "learning_rate": 3.2907471931862177e-06,
      "loss": 0.0629,
      "step": 18180
    },
    {
      "epoch": 7.038714672861015,
      "grad_norm": 28.733983993530273,
      "learning_rate": 3.290317030154429e-06,
      "loss": 1.6289,
      "step": 18181
    },
    {
      "epoch": 7.039101819589624,
      "grad_norm": 6.738824367523193,
      "learning_rate": 3.2898868671226397e-06,
      "loss": 0.1447,
      "step": 18182
    },
    {
      "epoch": 7.039488966318235,
      "grad_norm": 27.530733108520508,
      "learning_rate": 3.289456704090851e-06,
      "loss": 1.4785,
      "step": 18183
    },
    {
      "epoch": 7.039876113046844,
      "grad_norm": 106.3207015991211,
      "learning_rate": 3.2890265410590617e-06,
      "loss": 0.2891,
      "step": 18184
    },
    {
      "epoch": 7.040263259775455,
      "grad_norm": 72.60904693603516,
      "learning_rate": 3.2885963780272727e-06,
      "loss": 0.3113,
      "step": 18185
    },
    {
      "epoch": 7.040650406504065,
      "grad_norm": 3.505816698074341,
      "learning_rate": 3.2881662149954837e-06,
      "loss": 0.0616,
      "step": 18186
    },
    {
      "epoch": 7.041037553232675,
      "grad_norm": 53.087013244628906,
      "learning_rate": 3.2877360519636947e-06,
      "loss": 0.948,
      "step": 18187
    },
    {
      "epoch": 7.0414246999612855,
      "grad_norm": 11.732230186462402,
      "learning_rate": 3.2873058889319053e-06,
      "loss": 0.3987,
      "step": 18188
    },
    {
      "epoch": 7.041811846689895,
      "grad_norm": 117.41444396972656,
      "learning_rate": 3.2868757259001167e-06,
      "loss": 0.9297,
      "step": 18189
    },
    {
      "epoch": 7.042198993418506,
      "grad_norm": 10.613473892211914,
      "learning_rate": 3.2864455628683272e-06,
      "loss": 0.1418,
      "step": 18190
    },
    {
      "epoch": 7.042586140147116,
      "grad_norm": 174.07676696777344,
      "learning_rate": 3.2860153998365387e-06,
      "loss": 2.6608,
      "step": 18191
    },
    {
      "epoch": 7.042973286875726,
      "grad_norm": 52.174110412597656,
      "learning_rate": 3.2855852368047492e-06,
      "loss": 0.2527,
      "step": 18192
    },
    {
      "epoch": 7.043360433604336,
      "grad_norm": 136.2465057373047,
      "learning_rate": 3.2851550737729602e-06,
      "loss": 0.5109,
      "step": 18193
    },
    {
      "epoch": 7.043747580332946,
      "grad_norm": 5.101407527923584,
      "learning_rate": 3.284724910741171e-06,
      "loss": 0.1457,
      "step": 18194
    },
    {
      "epoch": 7.044134727061556,
      "grad_norm": 0.9869425892829895,
      "learning_rate": 3.284294747709382e-06,
      "loss": 0.0333,
      "step": 18195
    },
    {
      "epoch": 7.044521873790166,
      "grad_norm": 51.580162048339844,
      "learning_rate": 3.2838645846775928e-06,
      "loss": 1.1056,
      "step": 18196
    },
    {
      "epoch": 7.0449090205187765,
      "grad_norm": 31.641691207885742,
      "learning_rate": 3.283434421645804e-06,
      "loss": 1.7108,
      "step": 18197
    },
    {
      "epoch": 7.045296167247387,
      "grad_norm": 73.67231750488281,
      "learning_rate": 3.2830042586140148e-06,
      "loss": 0.2381,
      "step": 18198
    },
    {
      "epoch": 7.045683313975997,
      "grad_norm": 68.86959075927734,
      "learning_rate": 3.282574095582226e-06,
      "loss": 0.9175,
      "step": 18199
    },
    {
      "epoch": 7.046070460704607,
      "grad_norm": 91.12254333496094,
      "learning_rate": 3.2821439325504367e-06,
      "loss": 0.5645,
      "step": 18200
    },
    {
      "epoch": 7.046457607433217,
      "grad_norm": 27.77997398376465,
      "learning_rate": 3.281713769518648e-06,
      "loss": 1.8592,
      "step": 18201
    },
    {
      "epoch": 7.046844754161827,
      "grad_norm": 59.65989685058594,
      "learning_rate": 3.2812836064868587e-06,
      "loss": 1.625,
      "step": 18202
    },
    {
      "epoch": 7.047231900890438,
      "grad_norm": 168.69961547851562,
      "learning_rate": 3.2808534434550697e-06,
      "loss": 1.0125,
      "step": 18203
    },
    {
      "epoch": 7.0476190476190474,
      "grad_norm": 0.5157764554023743,
      "learning_rate": 3.2804232804232807e-06,
      "loss": 0.0087,
      "step": 18204
    },
    {
      "epoch": 7.048006194347658,
      "grad_norm": 3.400768995285034,
      "learning_rate": 3.2799931173914917e-06,
      "loss": 0.0833,
      "step": 18205
    },
    {
      "epoch": 7.048393341076268,
      "grad_norm": 1.270570158958435,
      "learning_rate": 3.2795629543597023e-06,
      "loss": 0.0338,
      "step": 18206
    },
    {
      "epoch": 7.048780487804878,
      "grad_norm": 27.09808349609375,
      "learning_rate": 3.2791327913279137e-06,
      "loss": 0.2414,
      "step": 18207
    },
    {
      "epoch": 7.049167634533489,
      "grad_norm": 48.30734634399414,
      "learning_rate": 3.2787026282961243e-06,
      "loss": 2.0368,
      "step": 18208
    },
    {
      "epoch": 7.049554781262098,
      "grad_norm": 80.71778106689453,
      "learning_rate": 3.2782724652643357e-06,
      "loss": 1.9118,
      "step": 18209
    },
    {
      "epoch": 7.049941927990709,
      "grad_norm": 32.14857864379883,
      "learning_rate": 3.2778423022325462e-06,
      "loss": 4.3022,
      "step": 18210
    },
    {
      "epoch": 7.050329074719318,
      "grad_norm": 272.61962890625,
      "learning_rate": 3.2774121392007572e-06,
      "loss": 1.423,
      "step": 18211
    },
    {
      "epoch": 7.050716221447929,
      "grad_norm": 238.43080139160156,
      "learning_rate": 3.2769819761689682e-06,
      "loss": 2.5194,
      "step": 18212
    },
    {
      "epoch": 7.0511033681765385,
      "grad_norm": 30.513025283813477,
      "learning_rate": 3.2765518131371792e-06,
      "loss": 0.308,
      "step": 18213
    },
    {
      "epoch": 7.051490514905149,
      "grad_norm": 306.02569580078125,
      "learning_rate": 3.2761216501053898e-06,
      "loss": 2.3963,
      "step": 18214
    },
    {
      "epoch": 7.0518776616337595,
      "grad_norm": 5.532515525817871,
      "learning_rate": 3.275691487073601e-06,
      "loss": 0.1111,
      "step": 18215
    },
    {
      "epoch": 7.052264808362369,
      "grad_norm": 93.60901641845703,
      "learning_rate": 3.2752613240418118e-06,
      "loss": 0.4843,
      "step": 18216
    },
    {
      "epoch": 7.05265195509098,
      "grad_norm": 30.03371238708496,
      "learning_rate": 3.274831161010023e-06,
      "loss": 1.5935,
      "step": 18217
    },
    {
      "epoch": 7.053039101819589,
      "grad_norm": 26.41221046447754,
      "learning_rate": 3.2744009979782338e-06,
      "loss": 0.6991,
      "step": 18218
    },
    {
      "epoch": 7.0534262485482,
      "grad_norm": 4.0388922691345215,
      "learning_rate": 3.273970834946445e-06,
      "loss": 0.02,
      "step": 18219
    },
    {
      "epoch": 7.05381339527681,
      "grad_norm": 21.429821014404297,
      "learning_rate": 3.273540671914656e-06,
      "loss": 0.1774,
      "step": 18220
    },
    {
      "epoch": 7.05420054200542,
      "grad_norm": 29.646957397460938,
      "learning_rate": 3.2731105088828667e-06,
      "loss": 2.0812,
      "step": 18221
    },
    {
      "epoch": 7.05458768873403,
      "grad_norm": 169.9146270751953,
      "learning_rate": 3.272680345851078e-06,
      "loss": 2.1721,
      "step": 18222
    },
    {
      "epoch": 7.05497483546264,
      "grad_norm": 187.79649353027344,
      "learning_rate": 3.2722501828192887e-06,
      "loss": 2.0271,
      "step": 18223
    },
    {
      "epoch": 7.0553619821912505,
      "grad_norm": 55.64835739135742,
      "learning_rate": 3.2718200197875e-06,
      "loss": 2.6784,
      "step": 18224
    },
    {
      "epoch": 7.055749128919861,
      "grad_norm": 14.105313301086426,
      "learning_rate": 3.2713898567557107e-06,
      "loss": 0.1922,
      "step": 18225
    },
    {
      "epoch": 7.056136275648471,
      "grad_norm": 33.586116790771484,
      "learning_rate": 3.2709596937239217e-06,
      "loss": 1.9832,
      "step": 18226
    },
    {
      "epoch": 7.056523422377081,
      "grad_norm": 181.69021606445312,
      "learning_rate": 3.2705295306921327e-06,
      "loss": 1.2512,
      "step": 18227
    },
    {
      "epoch": 7.056910569105691,
      "grad_norm": 26.07010269165039,
      "learning_rate": 3.2700993676603437e-06,
      "loss": 0.3474,
      "step": 18228
    },
    {
      "epoch": 7.057297715834301,
      "grad_norm": 23.711904525756836,
      "learning_rate": 3.2696692046285542e-06,
      "loss": 2.8759,
      "step": 18229
    },
    {
      "epoch": 7.057684862562911,
      "grad_norm": 54.95945358276367,
      "learning_rate": 3.2692390415967657e-06,
      "loss": 1.1833,
      "step": 18230
    },
    {
      "epoch": 7.0580720092915215,
      "grad_norm": 35.52648162841797,
      "learning_rate": 3.2688088785649762e-06,
      "loss": 1.1302,
      "step": 18231
    },
    {
      "epoch": 7.058459156020132,
      "grad_norm": 131.62411499023438,
      "learning_rate": 3.2683787155331876e-06,
      "loss": 1.4932,
      "step": 18232
    },
    {
      "epoch": 7.058846302748742,
      "grad_norm": 70.1419448852539,
      "learning_rate": 3.2679485525013982e-06,
      "loss": 1.1066,
      "step": 18233
    },
    {
      "epoch": 7.059233449477352,
      "grad_norm": 61.22010040283203,
      "learning_rate": 3.2675183894696096e-06,
      "loss": 1.4405,
      "step": 18234
    },
    {
      "epoch": 7.059620596205962,
      "grad_norm": 1.2479276657104492,
      "learning_rate": 3.26708822643782e-06,
      "loss": 0.0357,
      "step": 18235
    },
    {
      "epoch": 7.060007742934572,
      "grad_norm": 56.08927917480469,
      "learning_rate": 3.266658063406031e-06,
      "loss": 1.1176,
      "step": 18236
    },
    {
      "epoch": 7.060394889663183,
      "grad_norm": 61.801883697509766,
      "learning_rate": 3.266227900374242e-06,
      "loss": 0.4415,
      "step": 18237
    },
    {
      "epoch": 7.060782036391792,
      "grad_norm": 70.68976593017578,
      "learning_rate": 3.265797737342453e-06,
      "loss": 2.0524,
      "step": 18238
    },
    {
      "epoch": 7.061169183120403,
      "grad_norm": 36.678260803222656,
      "learning_rate": 3.2653675743106637e-06,
      "loss": 2.8402,
      "step": 18239
    },
    {
      "epoch": 7.0615563298490125,
      "grad_norm": 232.23846435546875,
      "learning_rate": 3.264937411278875e-06,
      "loss": 0.8311,
      "step": 18240
    },
    {
      "epoch": 7.061943476577623,
      "grad_norm": 100.85953521728516,
      "learning_rate": 3.2645072482470857e-06,
      "loss": 0.4122,
      "step": 18241
    },
    {
      "epoch": 7.062330623306233,
      "grad_norm": 30.593198776245117,
      "learning_rate": 3.264077085215297e-06,
      "loss": 0.4302,
      "step": 18242
    },
    {
      "epoch": 7.062717770034843,
      "grad_norm": 76.19611358642578,
      "learning_rate": 3.2636469221835077e-06,
      "loss": 1.4388,
      "step": 18243
    },
    {
      "epoch": 7.063104916763454,
      "grad_norm": 30.198482513427734,
      "learning_rate": 3.2632167591517187e-06,
      "loss": 2.045,
      "step": 18244
    },
    {
      "epoch": 7.063492063492063,
      "grad_norm": 77.96089935302734,
      "learning_rate": 3.2627865961199297e-06,
      "loss": 1.2456,
      "step": 18245
    },
    {
      "epoch": 7.063879210220674,
      "grad_norm": 8.194231033325195,
      "learning_rate": 3.2623564330881407e-06,
      "loss": 0.0533,
      "step": 18246
    },
    {
      "epoch": 7.064266356949283,
      "grad_norm": 66.24879455566406,
      "learning_rate": 3.2619262700563513e-06,
      "loss": 1.3782,
      "step": 18247
    },
    {
      "epoch": 7.064653503677894,
      "grad_norm": 3.8334710597991943,
      "learning_rate": 3.2614961070245627e-06,
      "loss": 0.1748,
      "step": 18248
    },
    {
      "epoch": 7.065040650406504,
      "grad_norm": 0.9197494387626648,
      "learning_rate": 3.2610659439927732e-06,
      "loss": 0.0342,
      "step": 18249
    },
    {
      "epoch": 7.065427797135114,
      "grad_norm": 54.81914138793945,
      "learning_rate": 3.2606357809609847e-06,
      "loss": 1.3247,
      "step": 18250
    },
    {
      "epoch": 7.0658149438637246,
      "grad_norm": 42.25971603393555,
      "learning_rate": 3.2602056179291952e-06,
      "loss": 1.4024,
      "step": 18251
    },
    {
      "epoch": 7.066202090592334,
      "grad_norm": 67.53836059570312,
      "learning_rate": 3.2597754548974066e-06,
      "loss": 1.957,
      "step": 18252
    },
    {
      "epoch": 7.066589237320945,
      "grad_norm": 1.4002894163131714,
      "learning_rate": 3.259345291865617e-06,
      "loss": 0.0346,
      "step": 18253
    },
    {
      "epoch": 7.066976384049555,
      "grad_norm": 7.9037041664123535,
      "learning_rate": 3.258915128833828e-06,
      "loss": 0.0764,
      "step": 18254
    },
    {
      "epoch": 7.067363530778165,
      "grad_norm": 29.33357048034668,
      "learning_rate": 3.258484965802039e-06,
      "loss": 1.3591,
      "step": 18255
    },
    {
      "epoch": 7.067750677506775,
      "grad_norm": 1.2233225107192993,
      "learning_rate": 3.25805480277025e-06,
      "loss": 0.0443,
      "step": 18256
    },
    {
      "epoch": 7.068137824235385,
      "grad_norm": 5.82212495803833,
      "learning_rate": 3.2576246397384608e-06,
      "loss": 0.1839,
      "step": 18257
    },
    {
      "epoch": 7.0685249709639955,
      "grad_norm": 1.5099914073944092,
      "learning_rate": 3.257194476706672e-06,
      "loss": 0.0439,
      "step": 18258
    },
    {
      "epoch": 7.068912117692605,
      "grad_norm": 136.57464599609375,
      "learning_rate": 3.2567643136748827e-06,
      "loss": 0.591,
      "step": 18259
    },
    {
      "epoch": 7.069299264421216,
      "grad_norm": 53.285457611083984,
      "learning_rate": 3.256334150643094e-06,
      "loss": 1.9053,
      "step": 18260
    },
    {
      "epoch": 7.069686411149826,
      "grad_norm": 4.8467512130737305,
      "learning_rate": 3.255903987611305e-06,
      "loss": 0.2522,
      "step": 18261
    },
    {
      "epoch": 7.070073557878436,
      "grad_norm": 121.86126708984375,
      "learning_rate": 3.2554738245795157e-06,
      "loss": 1.6997,
      "step": 18262
    },
    {
      "epoch": 7.070460704607046,
      "grad_norm": 82.25084686279297,
      "learning_rate": 3.255043661547727e-06,
      "loss": 1.0629,
      "step": 18263
    },
    {
      "epoch": 7.070847851335656,
      "grad_norm": 147.18002319335938,
      "learning_rate": 3.2546134985159377e-06,
      "loss": 0.1878,
      "step": 18264
    },
    {
      "epoch": 7.071234998064266,
      "grad_norm": 95.18325805664062,
      "learning_rate": 3.254183335484149e-06,
      "loss": 0.4531,
      "step": 18265
    },
    {
      "epoch": 7.071622144792877,
      "grad_norm": 115.64141082763672,
      "learning_rate": 3.2537531724523597e-06,
      "loss": 1.0119,
      "step": 18266
    },
    {
      "epoch": 7.0720092915214865,
      "grad_norm": 55.19975662231445,
      "learning_rate": 3.2533230094205707e-06,
      "loss": 1.6396,
      "step": 18267
    },
    {
      "epoch": 7.072396438250097,
      "grad_norm": 94.37013244628906,
      "learning_rate": 3.2528928463887817e-06,
      "loss": 0.8637,
      "step": 18268
    },
    {
      "epoch": 7.072783584978707,
      "grad_norm": 265.05242919921875,
      "learning_rate": 3.2524626833569927e-06,
      "loss": 1.4892,
      "step": 18269
    },
    {
      "epoch": 7.073170731707317,
      "grad_norm": 46.11347579956055,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 0.7334,
      "step": 18270
    },
    {
      "epoch": 7.073557878435928,
      "grad_norm": 3.3586974143981934,
      "learning_rate": 3.2516023572934146e-06,
      "loss": 0.174,
      "step": 18271
    },
    {
      "epoch": 7.073945025164537,
      "grad_norm": 145.8928680419922,
      "learning_rate": 3.2511721942616252e-06,
      "loss": 1.3397,
      "step": 18272
    },
    {
      "epoch": 7.074332171893148,
      "grad_norm": 18.809612274169922,
      "learning_rate": 3.2507420312298366e-06,
      "loss": 0.342,
      "step": 18273
    },
    {
      "epoch": 7.074719318621757,
      "grad_norm": 57.85496139526367,
      "learning_rate": 3.250311868198047e-06,
      "loss": 0.8086,
      "step": 18274
    },
    {
      "epoch": 7.075106465350368,
      "grad_norm": 26.42622947692871,
      "learning_rate": 3.2498817051662586e-06,
      "loss": 2.1635,
      "step": 18275
    },
    {
      "epoch": 7.0754936120789775,
      "grad_norm": 39.244171142578125,
      "learning_rate": 3.249451542134469e-06,
      "loss": 2.5458,
      "step": 18276
    },
    {
      "epoch": 7.075880758807588,
      "grad_norm": 3.8726587295532227,
      "learning_rate": 3.24902137910268e-06,
      "loss": 0.1293,
      "step": 18277
    },
    {
      "epoch": 7.0762679055361986,
      "grad_norm": 1.555936336517334,
      "learning_rate": 3.248591216070891e-06,
      "loss": 0.0805,
      "step": 18278
    },
    {
      "epoch": 7.076655052264808,
      "grad_norm": 61.92533493041992,
      "learning_rate": 3.248161053039102e-06,
      "loss": 2.9482,
      "step": 18279
    },
    {
      "epoch": 7.077042198993419,
      "grad_norm": 3.9721407890319824,
      "learning_rate": 3.2477308900073127e-06,
      "loss": 0.0552,
      "step": 18280
    },
    {
      "epoch": 7.077429345722028,
      "grad_norm": 92.63749694824219,
      "learning_rate": 3.247300726975524e-06,
      "loss": 1.5153,
      "step": 18281
    },
    {
      "epoch": 7.077816492450639,
      "grad_norm": 120.58267211914062,
      "learning_rate": 3.2468705639437347e-06,
      "loss": 1.3983,
      "step": 18282
    },
    {
      "epoch": 7.078203639179249,
      "grad_norm": 25.0247859954834,
      "learning_rate": 3.246440400911946e-06,
      "loss": 1.4885,
      "step": 18283
    },
    {
      "epoch": 7.078590785907859,
      "grad_norm": 8.160200119018555,
      "learning_rate": 3.2460102378801567e-06,
      "loss": 0.1063,
      "step": 18284
    },
    {
      "epoch": 7.0789779326364695,
      "grad_norm": 186.48458862304688,
      "learning_rate": 3.2455800748483677e-06,
      "loss": 0.6574,
      "step": 18285
    },
    {
      "epoch": 7.079365079365079,
      "grad_norm": 34.08158874511719,
      "learning_rate": 3.2451499118165787e-06,
      "loss": 1.5484,
      "step": 18286
    },
    {
      "epoch": 7.07975222609369,
      "grad_norm": 103.1619873046875,
      "learning_rate": 3.2447197487847897e-06,
      "loss": 1.788,
      "step": 18287
    },
    {
      "epoch": 7.080139372822299,
      "grad_norm": 57.82707977294922,
      "learning_rate": 3.2442895857530007e-06,
      "loss": 4.2777,
      "step": 18288
    },
    {
      "epoch": 7.08052651955091,
      "grad_norm": 51.11015701293945,
      "learning_rate": 3.2438594227212117e-06,
      "loss": 0.4813,
      "step": 18289
    },
    {
      "epoch": 7.08091366627952,
      "grad_norm": 68.61820983886719,
      "learning_rate": 3.2434292596894222e-06,
      "loss": 1.6893,
      "step": 18290
    },
    {
      "epoch": 7.08130081300813,
      "grad_norm": 78.02254486083984,
      "learning_rate": 3.2429990966576336e-06,
      "loss": 0.9663,
      "step": 18291
    },
    {
      "epoch": 7.08168795973674,
      "grad_norm": 84.52960968017578,
      "learning_rate": 3.2425689336258442e-06,
      "loss": 1.7899,
      "step": 18292
    },
    {
      "epoch": 7.08207510646535,
      "grad_norm": 176.68453979492188,
      "learning_rate": 3.2421387705940556e-06,
      "loss": 0.8667,
      "step": 18293
    },
    {
      "epoch": 7.0824622531939605,
      "grad_norm": 8.952953338623047,
      "learning_rate": 3.241708607562266e-06,
      "loss": 0.1202,
      "step": 18294
    },
    {
      "epoch": 7.082849399922571,
      "grad_norm": 92.9749755859375,
      "learning_rate": 3.241278444530477e-06,
      "loss": 0.394,
      "step": 18295
    },
    {
      "epoch": 7.083236546651181,
      "grad_norm": 57.20380783081055,
      "learning_rate": 3.240848281498688e-06,
      "loss": 2.0321,
      "step": 18296
    },
    {
      "epoch": 7.083623693379791,
      "grad_norm": 71.1332778930664,
      "learning_rate": 3.240418118466899e-06,
      "loss": 2.0414,
      "step": 18297
    },
    {
      "epoch": 7.084010840108401,
      "grad_norm": 107.3546142578125,
      "learning_rate": 3.2399879554351097e-06,
      "loss": 0.9651,
      "step": 18298
    },
    {
      "epoch": 7.084397986837011,
      "grad_norm": 32.10589599609375,
      "learning_rate": 3.239557792403321e-06,
      "loss": 1.5536,
      "step": 18299
    },
    {
      "epoch": 7.084785133565622,
      "grad_norm": 58.24812316894531,
      "learning_rate": 3.2391276293715317e-06,
      "loss": 0.3679,
      "step": 18300
    },
    {
      "epoch": 7.085172280294231,
      "grad_norm": 2.0661580562591553,
      "learning_rate": 3.238697466339743e-06,
      "loss": 0.0861,
      "step": 18301
    },
    {
      "epoch": 7.085559427022842,
      "grad_norm": 77.77993774414062,
      "learning_rate": 3.238267303307954e-06,
      "loss": 0.8143,
      "step": 18302
    },
    {
      "epoch": 7.0859465737514515,
      "grad_norm": 121.89229583740234,
      "learning_rate": 3.2378371402761647e-06,
      "loss": 0.8052,
      "step": 18303
    },
    {
      "epoch": 7.086333720480062,
      "grad_norm": 1.6250370740890503,
      "learning_rate": 3.237406977244376e-06,
      "loss": 0.0668,
      "step": 18304
    },
    {
      "epoch": 7.086720867208672,
      "grad_norm": 68.293212890625,
      "learning_rate": 3.2369768142125867e-06,
      "loss": 1.5161,
      "step": 18305
    },
    {
      "epoch": 7.087108013937282,
      "grad_norm": 30.126115798950195,
      "learning_rate": 3.236546651180798e-06,
      "loss": 1.0606,
      "step": 18306
    },
    {
      "epoch": 7.087495160665893,
      "grad_norm": 27.61036491394043,
      "learning_rate": 3.2361164881490087e-06,
      "loss": 1.1779,
      "step": 18307
    },
    {
      "epoch": 7.087882307394502,
      "grad_norm": 24.08818244934082,
      "learning_rate": 3.23568632511722e-06,
      "loss": 2.1898,
      "step": 18308
    },
    {
      "epoch": 7.088269454123113,
      "grad_norm": 42.79659652709961,
      "learning_rate": 3.2352561620854307e-06,
      "loss": 0.9373,
      "step": 18309
    },
    {
      "epoch": 7.0886566008517224,
      "grad_norm": 8.286910057067871,
      "learning_rate": 3.2348259990536417e-06,
      "loss": 0.1508,
      "step": 18310
    },
    {
      "epoch": 7.089043747580333,
      "grad_norm": 3.597978353500366,
      "learning_rate": 3.2343958360218526e-06,
      "loss": 0.1483,
      "step": 18311
    },
    {
      "epoch": 7.0894308943089435,
      "grad_norm": 117.98095703125,
      "learning_rate": 3.2339656729900636e-06,
      "loss": 0.1803,
      "step": 18312
    },
    {
      "epoch": 7.089818041037553,
      "grad_norm": 5.30233907699585,
      "learning_rate": 3.233535509958274e-06,
      "loss": 0.1632,
      "step": 18313
    },
    {
      "epoch": 7.090205187766164,
      "grad_norm": 25.21823501586914,
      "learning_rate": 3.2331053469264856e-06,
      "loss": 0.6256,
      "step": 18314
    },
    {
      "epoch": 7.090592334494773,
      "grad_norm": 51.995338439941406,
      "learning_rate": 3.232675183894696e-06,
      "loss": 1.53,
      "step": 18315
    },
    {
      "epoch": 7.090979481223384,
      "grad_norm": 48.64448165893555,
      "learning_rate": 3.2322450208629076e-06,
      "loss": 1.1442,
      "step": 18316
    },
    {
      "epoch": 7.091366627951994,
      "grad_norm": 22.441015243530273,
      "learning_rate": 3.231814857831118e-06,
      "loss": 2.1668,
      "step": 18317
    },
    {
      "epoch": 7.091753774680604,
      "grad_norm": 117.79603576660156,
      "learning_rate": 3.231384694799329e-06,
      "loss": 0.5713,
      "step": 18318
    },
    {
      "epoch": 7.092140921409214,
      "grad_norm": 4.079635143280029,
      "learning_rate": 3.23095453176754e-06,
      "loss": 0.0948,
      "step": 18319
    },
    {
      "epoch": 7.092528068137824,
      "grad_norm": 2.1315715312957764,
      "learning_rate": 3.230524368735751e-06,
      "loss": 0.0877,
      "step": 18320
    },
    {
      "epoch": 7.0929152148664345,
      "grad_norm": 5.507943153381348,
      "learning_rate": 3.2300942057039617e-06,
      "loss": 0.1115,
      "step": 18321
    },
    {
      "epoch": 7.093302361595044,
      "grad_norm": 75.31257629394531,
      "learning_rate": 3.229664042672173e-06,
      "loss": 0.8608,
      "step": 18322
    },
    {
      "epoch": 7.093689508323655,
      "grad_norm": 0.6634010076522827,
      "learning_rate": 3.2292338796403837e-06,
      "loss": 0.0238,
      "step": 18323
    },
    {
      "epoch": 7.094076655052265,
      "grad_norm": 8.583902359008789,
      "learning_rate": 3.228803716608595e-06,
      "loss": 0.1051,
      "step": 18324
    },
    {
      "epoch": 7.094463801780875,
      "grad_norm": 56.702247619628906,
      "learning_rate": 3.2283735535768057e-06,
      "loss": 0.6676,
      "step": 18325
    },
    {
      "epoch": 7.094850948509485,
      "grad_norm": 33.31722640991211,
      "learning_rate": 3.227943390545017e-06,
      "loss": 0.3729,
      "step": 18326
    },
    {
      "epoch": 7.095238095238095,
      "grad_norm": 2.9365196228027344,
      "learning_rate": 3.2275132275132277e-06,
      "loss": 0.0646,
      "step": 18327
    },
    {
      "epoch": 7.095625241966705,
      "grad_norm": 47.26615524291992,
      "learning_rate": 3.2270830644814387e-06,
      "loss": 3.5003,
      "step": 18328
    },
    {
      "epoch": 7.096012388695316,
      "grad_norm": 40.78413009643555,
      "learning_rate": 3.2266529014496497e-06,
      "loss": 2.1135,
      "step": 18329
    },
    {
      "epoch": 7.0963995354239255,
      "grad_norm": 38.738712310791016,
      "learning_rate": 3.2262227384178606e-06,
      "loss": 1.7034,
      "step": 18330
    },
    {
      "epoch": 7.096786682152536,
      "grad_norm": 30.578664779663086,
      "learning_rate": 3.2257925753860712e-06,
      "loss": 1.9707,
      "step": 18331
    },
    {
      "epoch": 7.097173828881146,
      "grad_norm": 4.4227776527404785,
      "learning_rate": 3.2253624123542826e-06,
      "loss": 0.1499,
      "step": 18332
    },
    {
      "epoch": 7.097560975609756,
      "grad_norm": 36.24916458129883,
      "learning_rate": 3.224932249322493e-06,
      "loss": 0.4442,
      "step": 18333
    },
    {
      "epoch": 7.097948122338366,
      "grad_norm": 49.59074020385742,
      "learning_rate": 3.2245020862907046e-06,
      "loss": 1.2392,
      "step": 18334
    },
    {
      "epoch": 7.098335269066976,
      "grad_norm": 15.381044387817383,
      "learning_rate": 3.224071923258915e-06,
      "loss": 0.2822,
      "step": 18335
    },
    {
      "epoch": 7.098722415795587,
      "grad_norm": 143.85720825195312,
      "learning_rate": 3.223641760227126e-06,
      "loss": 1.7156,
      "step": 18336
    },
    {
      "epoch": 7.0991095625241964,
      "grad_norm": 59.654388427734375,
      "learning_rate": 3.223211597195337e-06,
      "loss": 1.71,
      "step": 18337
    },
    {
      "epoch": 7.099496709252807,
      "grad_norm": 97.9039077758789,
      "learning_rate": 3.222781434163548e-06,
      "loss": 2.7725,
      "step": 18338
    },
    {
      "epoch": 7.099883855981417,
      "grad_norm": 53.35197830200195,
      "learning_rate": 3.2223512711317587e-06,
      "loss": 0.6445,
      "step": 18339
    },
    {
      "epoch": 7.100271002710027,
      "grad_norm": 2.5051686763763428,
      "learning_rate": 3.22192110809997e-06,
      "loss": 0.0276,
      "step": 18340
    },
    {
      "epoch": 7.100658149438638,
      "grad_norm": 47.11643981933594,
      "learning_rate": 3.2214909450681807e-06,
      "loss": 1.7744,
      "step": 18341
    },
    {
      "epoch": 7.101045296167247,
      "grad_norm": 5.240942001342773,
      "learning_rate": 3.221060782036392e-06,
      "loss": 0.1392,
      "step": 18342
    },
    {
      "epoch": 7.101432442895858,
      "grad_norm": 10.406783103942871,
      "learning_rate": 3.220630619004603e-06,
      "loss": 0.2417,
      "step": 18343
    },
    {
      "epoch": 7.101819589624467,
      "grad_norm": 115.12544250488281,
      "learning_rate": 3.220200455972814e-06,
      "loss": 0.9698,
      "step": 18344
    },
    {
      "epoch": 7.102206736353078,
      "grad_norm": 103.92440795898438,
      "learning_rate": 3.219770292941025e-06,
      "loss": 2.0704,
      "step": 18345
    },
    {
      "epoch": 7.102593883081688,
      "grad_norm": 1.5081005096435547,
      "learning_rate": 3.2193401299092357e-06,
      "loss": 0.0685,
      "step": 18346
    },
    {
      "epoch": 7.102981029810298,
      "grad_norm": 33.61229705810547,
      "learning_rate": 3.218909966877447e-06,
      "loss": 1.7327,
      "step": 18347
    },
    {
      "epoch": 7.1033681765389085,
      "grad_norm": 30.52760124206543,
      "learning_rate": 3.2184798038456577e-06,
      "loss": 0.4691,
      "step": 18348
    },
    {
      "epoch": 7.103755323267518,
      "grad_norm": 1.391257405281067,
      "learning_rate": 3.218049640813869e-06,
      "loss": 0.0751,
      "step": 18349
    },
    {
      "epoch": 7.104142469996129,
      "grad_norm": 1.0248422622680664,
      "learning_rate": 3.2176194777820796e-06,
      "loss": 0.0461,
      "step": 18350
    },
    {
      "epoch": 7.104529616724738,
      "grad_norm": 68.70366668701172,
      "learning_rate": 3.2171893147502906e-06,
      "loss": 1.7177,
      "step": 18351
    },
    {
      "epoch": 7.104916763453349,
      "grad_norm": 137.91600036621094,
      "learning_rate": 3.2167591517185016e-06,
      "loss": 2.2585,
      "step": 18352
    },
    {
      "epoch": 7.105303910181959,
      "grad_norm": 32.808082580566406,
      "learning_rate": 3.2163289886867126e-06,
      "loss": 1.1717,
      "step": 18353
    },
    {
      "epoch": 7.105691056910569,
      "grad_norm": 12.681781768798828,
      "learning_rate": 3.215898825654923e-06,
      "loss": 0.0831,
      "step": 18354
    },
    {
      "epoch": 7.106078203639179,
      "grad_norm": 24.874181747436523,
      "learning_rate": 3.2154686626231346e-06,
      "loss": 2.7513,
      "step": 18355
    },
    {
      "epoch": 7.106465350367789,
      "grad_norm": 6.071530342102051,
      "learning_rate": 3.215038499591345e-06,
      "loss": 0.2154,
      "step": 18356
    },
    {
      "epoch": 7.1068524970963995,
      "grad_norm": 88.62079620361328,
      "learning_rate": 3.2146083365595566e-06,
      "loss": 0.5244,
      "step": 18357
    },
    {
      "epoch": 7.10723964382501,
      "grad_norm": 76.62324523925781,
      "learning_rate": 3.214178173527767e-06,
      "loss": 0.2386,
      "step": 18358
    },
    {
      "epoch": 7.10762679055362,
      "grad_norm": 61.779151916503906,
      "learning_rate": 3.2137480104959786e-06,
      "loss": 2.2434,
      "step": 18359
    },
    {
      "epoch": 7.10801393728223,
      "grad_norm": 3.2752983570098877,
      "learning_rate": 3.213317847464189e-06,
      "loss": 0.1931,
      "step": 18360
    },
    {
      "epoch": 7.10840108401084,
      "grad_norm": 33.3595085144043,
      "learning_rate": 3.2128876844324e-06,
      "loss": 1.8269,
      "step": 18361
    },
    {
      "epoch": 7.10878823073945,
      "grad_norm": 89.16995239257812,
      "learning_rate": 3.212457521400611e-06,
      "loss": 2.237,
      "step": 18362
    },
    {
      "epoch": 7.109175377468061,
      "grad_norm": 122.10762786865234,
      "learning_rate": 3.212027358368822e-06,
      "loss": 1.5741,
      "step": 18363
    },
    {
      "epoch": 7.1095625241966705,
      "grad_norm": 3.1264936923980713,
      "learning_rate": 3.2115971953370327e-06,
      "loss": 0.0798,
      "step": 18364
    },
    {
      "epoch": 7.109949670925281,
      "grad_norm": 178.0132598876953,
      "learning_rate": 3.211167032305244e-06,
      "loss": 0.9704,
      "step": 18365
    },
    {
      "epoch": 7.110336817653891,
      "grad_norm": 77.75613403320312,
      "learning_rate": 3.2107368692734547e-06,
      "loss": 0.8028,
      "step": 18366
    },
    {
      "epoch": 7.110723964382501,
      "grad_norm": 26.161054611206055,
      "learning_rate": 3.210306706241666e-06,
      "loss": 2.2795,
      "step": 18367
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 220.67495727539062,
      "learning_rate": 3.2098765432098767e-06,
      "loss": 0.6494,
      "step": 18368
    },
    {
      "epoch": 7.111498257839721,
      "grad_norm": 114.55718231201172,
      "learning_rate": 3.2094463801780877e-06,
      "loss": 2.6693,
      "step": 18369
    },
    {
      "epoch": 7.111885404568332,
      "grad_norm": 131.9727325439453,
      "learning_rate": 3.2090162171462986e-06,
      "loss": 0.606,
      "step": 18370
    },
    {
      "epoch": 7.112272551296941,
      "grad_norm": 36.02125549316406,
      "learning_rate": 3.2085860541145096e-06,
      "loss": 2.2398,
      "step": 18371
    },
    {
      "epoch": 7.112659698025552,
      "grad_norm": 22.674880981445312,
      "learning_rate": 3.20815589108272e-06,
      "loss": 0.4267,
      "step": 18372
    },
    {
      "epoch": 7.1130468447541615,
      "grad_norm": 323.7124328613281,
      "learning_rate": 3.2077257280509316e-06,
      "loss": 1.1608,
      "step": 18373
    },
    {
      "epoch": 7.113433991482772,
      "grad_norm": 181.84971618652344,
      "learning_rate": 3.207295565019142e-06,
      "loss": 0.75,
      "step": 18374
    },
    {
      "epoch": 7.1138211382113825,
      "grad_norm": 107.61529541015625,
      "learning_rate": 3.2068654019873536e-06,
      "loss": 0.6333,
      "step": 18375
    },
    {
      "epoch": 7.114208284939992,
      "grad_norm": 6.15855598449707,
      "learning_rate": 3.206435238955564e-06,
      "loss": 0.1158,
      "step": 18376
    },
    {
      "epoch": 7.114595431668603,
      "grad_norm": 102.29268646240234,
      "learning_rate": 3.2060050759237756e-06,
      "loss": 1.9432,
      "step": 18377
    },
    {
      "epoch": 7.114982578397212,
      "grad_norm": 65.80992889404297,
      "learning_rate": 3.205574912891986e-06,
      "loss": 0.4042,
      "step": 18378
    },
    {
      "epoch": 7.115369725125823,
      "grad_norm": 209.88388061523438,
      "learning_rate": 3.205144749860197e-06,
      "loss": 0.6781,
      "step": 18379
    },
    {
      "epoch": 7.115756871854432,
      "grad_norm": 57.118045806884766,
      "learning_rate": 3.204714586828408e-06,
      "loss": 0.4114,
      "step": 18380
    },
    {
      "epoch": 7.116144018583043,
      "grad_norm": 76.0039291381836,
      "learning_rate": 3.204284423796619e-06,
      "loss": 1.4301,
      "step": 18381
    },
    {
      "epoch": 7.116531165311653,
      "grad_norm": 170.51002502441406,
      "learning_rate": 3.2038542607648297e-06,
      "loss": 1.4446,
      "step": 18382
    },
    {
      "epoch": 7.116918312040263,
      "grad_norm": 17.25620460510254,
      "learning_rate": 3.203424097733041e-06,
      "loss": 0.091,
      "step": 18383
    },
    {
      "epoch": 7.1173054587688735,
      "grad_norm": 57.74658203125,
      "learning_rate": 3.202993934701252e-06,
      "loss": 0.2649,
      "step": 18384
    },
    {
      "epoch": 7.117692605497483,
      "grad_norm": 48.855369567871094,
      "learning_rate": 3.202563771669463e-06,
      "loss": 0.5951,
      "step": 18385
    },
    {
      "epoch": 7.118079752226094,
      "grad_norm": 44.08096694946289,
      "learning_rate": 3.202133608637674e-06,
      "loss": 0.1546,
      "step": 18386
    },
    {
      "epoch": 7.118466898954704,
      "grad_norm": 43.05923080444336,
      "learning_rate": 3.2017034456058847e-06,
      "loss": 0.2603,
      "step": 18387
    },
    {
      "epoch": 7.118854045683314,
      "grad_norm": 3.0676090717315674,
      "learning_rate": 3.201273282574096e-06,
      "loss": 0.159,
      "step": 18388
    },
    {
      "epoch": 7.119241192411924,
      "grad_norm": 4.751317977905273,
      "learning_rate": 3.2008431195423066e-06,
      "loss": 0.1,
      "step": 18389
    },
    {
      "epoch": 7.119628339140534,
      "grad_norm": 47.63132095336914,
      "learning_rate": 3.200412956510518e-06,
      "loss": 1.0092,
      "step": 18390
    },
    {
      "epoch": 7.1200154858691445,
      "grad_norm": 18.23301124572754,
      "learning_rate": 3.1999827934787286e-06,
      "loss": 0.2508,
      "step": 18391
    },
    {
      "epoch": 7.120402632597755,
      "grad_norm": 115.18770599365234,
      "learning_rate": 3.19955263044694e-06,
      "loss": 1.0103,
      "step": 18392
    },
    {
      "epoch": 7.120789779326365,
      "grad_norm": 28.67046356201172,
      "learning_rate": 3.1991224674151506e-06,
      "loss": 0.1904,
      "step": 18393
    },
    {
      "epoch": 7.121176926054975,
      "grad_norm": 29.392053604125977,
      "learning_rate": 3.1986923043833616e-06,
      "loss": 0.4741,
      "step": 18394
    },
    {
      "epoch": 7.121564072783585,
      "grad_norm": 54.83473587036133,
      "learning_rate": 3.1982621413515726e-06,
      "loss": 1.8109,
      "step": 18395
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 55.945892333984375,
      "learning_rate": 3.1978319783197836e-06,
      "loss": 0.4446,
      "step": 18396
    },
    {
      "epoch": 7.122338366240805,
      "grad_norm": 46.187782287597656,
      "learning_rate": 3.197401815287994e-06,
      "loss": 1.5714,
      "step": 18397
    },
    {
      "epoch": 7.122725512969415,
      "grad_norm": 54.39268112182617,
      "learning_rate": 3.1969716522562056e-06,
      "loss": 1.7268,
      "step": 18398
    },
    {
      "epoch": 7.123112659698026,
      "grad_norm": 60.94226837158203,
      "learning_rate": 3.196541489224416e-06,
      "loss": 2.6398,
      "step": 18399
    },
    {
      "epoch": 7.1234998064266355,
      "grad_norm": 2.597186326980591,
      "learning_rate": 3.1961113261926276e-06,
      "loss": 0.0588,
      "step": 18400
    },
    {
      "epoch": 7.123886953155246,
      "grad_norm": 2.429211378097534,
      "learning_rate": 3.195681163160838e-06,
      "loss": 0.0644,
      "step": 18401
    },
    {
      "epoch": 7.124274099883856,
      "grad_norm": 2.0246293544769287,
      "learning_rate": 3.195251000129049e-06,
      "loss": 0.0874,
      "step": 18402
    },
    {
      "epoch": 7.124661246612466,
      "grad_norm": 26.41025733947754,
      "learning_rate": 3.19482083709726e-06,
      "loss": 1.9305,
      "step": 18403
    },
    {
      "epoch": 7.125048393341077,
      "grad_norm": 66.86095428466797,
      "learning_rate": 3.194390674065471e-06,
      "loss": 0.8425,
      "step": 18404
    },
    {
      "epoch": 7.125435540069686,
      "grad_norm": 90.7550048828125,
      "learning_rate": 3.1939605110336817e-06,
      "loss": 1.6213,
      "step": 18405
    },
    {
      "epoch": 7.125822686798297,
      "grad_norm": 37.92253875732422,
      "learning_rate": 3.193530348001893e-06,
      "loss": 1.3092,
      "step": 18406
    },
    {
      "epoch": 7.126209833526906,
      "grad_norm": 61.791160583496094,
      "learning_rate": 3.1931001849701037e-06,
      "loss": 2.644,
      "step": 18407
    },
    {
      "epoch": 7.126596980255517,
      "grad_norm": 127.92574310302734,
      "learning_rate": 3.192670021938315e-06,
      "loss": 0.8615,
      "step": 18408
    },
    {
      "epoch": 7.1269841269841265,
      "grad_norm": 59.06807327270508,
      "learning_rate": 3.1922398589065256e-06,
      "loss": 0.376,
      "step": 18409
    },
    {
      "epoch": 7.127371273712737,
      "grad_norm": 81.73735046386719,
      "learning_rate": 3.191809695874737e-06,
      "loss": 0.9306,
      "step": 18410
    },
    {
      "epoch": 7.1277584204413476,
      "grad_norm": 2.267061471939087,
      "learning_rate": 3.1913795328429476e-06,
      "loss": 0.0965,
      "step": 18411
    },
    {
      "epoch": 7.128145567169957,
      "grad_norm": 11.48010540008545,
      "learning_rate": 3.1909493698111586e-06,
      "loss": 0.2801,
      "step": 18412
    },
    {
      "epoch": 7.128532713898568,
      "grad_norm": 72.34696960449219,
      "learning_rate": 3.1905192067793696e-06,
      "loss": 0.8133,
      "step": 18413
    },
    {
      "epoch": 7.128919860627177,
      "grad_norm": 96.85682678222656,
      "learning_rate": 3.1900890437475806e-06,
      "loss": 3.2086,
      "step": 18414
    },
    {
      "epoch": 7.129307007355788,
      "grad_norm": 0.9899790287017822,
      "learning_rate": 3.189658880715791e-06,
      "loss": 0.0355,
      "step": 18415
    },
    {
      "epoch": 7.129694154084398,
      "grad_norm": 40.28730392456055,
      "learning_rate": 3.1892287176840026e-06,
      "loss": 1.6894,
      "step": 18416
    },
    {
      "epoch": 7.130081300813008,
      "grad_norm": 37.16744613647461,
      "learning_rate": 3.188798554652213e-06,
      "loss": 1.8343,
      "step": 18417
    },
    {
      "epoch": 7.1304684475416185,
      "grad_norm": 92.67011260986328,
      "learning_rate": 3.1883683916204246e-06,
      "loss": 5.1964,
      "step": 18418
    },
    {
      "epoch": 7.130855594270228,
      "grad_norm": 111.26947784423828,
      "learning_rate": 3.187938228588635e-06,
      "loss": 1.9799,
      "step": 18419
    },
    {
      "epoch": 7.131242740998839,
      "grad_norm": 2.430987596511841,
      "learning_rate": 3.187508065556846e-06,
      "loss": 0.0843,
      "step": 18420
    },
    {
      "epoch": 7.131629887727449,
      "grad_norm": 1.086352825164795,
      "learning_rate": 3.187077902525057e-06,
      "loss": 0.0299,
      "step": 18421
    },
    {
      "epoch": 7.132017034456059,
      "grad_norm": 115.90997314453125,
      "learning_rate": 3.186647739493268e-06,
      "loss": 0.5487,
      "step": 18422
    },
    {
      "epoch": 7.132404181184669,
      "grad_norm": 68.40071868896484,
      "learning_rate": 3.1862175764614787e-06,
      "loss": 0.7515,
      "step": 18423
    },
    {
      "epoch": 7.132791327913279,
      "grad_norm": 103.87051391601562,
      "learning_rate": 3.18578741342969e-06,
      "loss": 3.2485,
      "step": 18424
    },
    {
      "epoch": 7.133178474641889,
      "grad_norm": 67.57504272460938,
      "learning_rate": 3.1853572503979015e-06,
      "loss": 3.1785,
      "step": 18425
    },
    {
      "epoch": 7.133565621370499,
      "grad_norm": 143.18521118164062,
      "learning_rate": 3.184927087366112e-06,
      "loss": 2.4545,
      "step": 18426
    },
    {
      "epoch": 7.1339527680991095,
      "grad_norm": 195.02481079101562,
      "learning_rate": 3.184496924334323e-06,
      "loss": 2.5353,
      "step": 18427
    },
    {
      "epoch": 7.13433991482772,
      "grad_norm": 114.72998809814453,
      "learning_rate": 3.184066761302534e-06,
      "loss": 3.1138,
      "step": 18428
    },
    {
      "epoch": 7.13472706155633,
      "grad_norm": 116.30349731445312,
      "learning_rate": 3.183636598270745e-06,
      "loss": 0.9446,
      "step": 18429
    },
    {
      "epoch": 7.13511420828494,
      "grad_norm": 407.57843017578125,
      "learning_rate": 3.1832064352389556e-06,
      "loss": 0.5461,
      "step": 18430
    },
    {
      "epoch": 7.13550135501355,
      "grad_norm": 74.27853393554688,
      "learning_rate": 3.182776272207167e-06,
      "loss": 3.483,
      "step": 18431
    },
    {
      "epoch": 7.13588850174216,
      "grad_norm": 47.21125030517578,
      "learning_rate": 3.1823461091753776e-06,
      "loss": 0.9824,
      "step": 18432
    },
    {
      "epoch": 7.136275648470771,
      "grad_norm": 2.4493348598480225,
      "learning_rate": 3.181915946143589e-06,
      "loss": 0.0479,
      "step": 18433
    },
    {
      "epoch": 7.13666279519938,
      "grad_norm": 31.688758850097656,
      "learning_rate": 3.1814857831117996e-06,
      "loss": 0.2781,
      "step": 18434
    },
    {
      "epoch": 7.137049941927991,
      "grad_norm": 11.044419288635254,
      "learning_rate": 3.1810556200800106e-06,
      "loss": 0.2723,
      "step": 18435
    },
    {
      "epoch": 7.1374370886566005,
      "grad_norm": 6.595887184143066,
      "learning_rate": 3.1806254570482216e-06,
      "loss": 0.0825,
      "step": 18436
    },
    {
      "epoch": 7.137824235385211,
      "grad_norm": 32.10580825805664,
      "learning_rate": 3.1801952940164326e-06,
      "loss": 0.327,
      "step": 18437
    },
    {
      "epoch": 7.138211382113822,
      "grad_norm": 4.300795555114746,
      "learning_rate": 3.179765130984643e-06,
      "loss": 0.2099,
      "step": 18438
    },
    {
      "epoch": 7.138598528842431,
      "grad_norm": 70.69538116455078,
      "learning_rate": 3.1793349679528546e-06,
      "loss": 2.6395,
      "step": 18439
    },
    {
      "epoch": 7.138985675571042,
      "grad_norm": 37.31608200073242,
      "learning_rate": 3.178904804921065e-06,
      "loss": 0.8818,
      "step": 18440
    },
    {
      "epoch": 7.139372822299651,
      "grad_norm": 5.008061408996582,
      "learning_rate": 3.1784746418892766e-06,
      "loss": 0.0724,
      "step": 18441
    },
    {
      "epoch": 7.139759969028262,
      "grad_norm": 157.63339233398438,
      "learning_rate": 3.178044478857487e-06,
      "loss": 0.576,
      "step": 18442
    },
    {
      "epoch": 7.140147115756871,
      "grad_norm": 16.283315658569336,
      "learning_rate": 3.1776143158256985e-06,
      "loss": 0.1587,
      "step": 18443
    },
    {
      "epoch": 7.140534262485482,
      "grad_norm": 13.65116024017334,
      "learning_rate": 3.177184152793909e-06,
      "loss": 0.0777,
      "step": 18444
    },
    {
      "epoch": 7.1409214092140925,
      "grad_norm": 42.99494171142578,
      "learning_rate": 3.17675398976212e-06,
      "loss": 2.0253,
      "step": 18445
    },
    {
      "epoch": 7.141308555942702,
      "grad_norm": 8.456018447875977,
      "learning_rate": 3.176323826730331e-06,
      "loss": 0.2436,
      "step": 18446
    },
    {
      "epoch": 7.141695702671313,
      "grad_norm": 34.35799789428711,
      "learning_rate": 3.175893663698542e-06,
      "loss": 1.3566,
      "step": 18447
    },
    {
      "epoch": 7.142082849399922,
      "grad_norm": 3.127326488494873,
      "learning_rate": 3.1754635006667526e-06,
      "loss": 0.1495,
      "step": 18448
    },
    {
      "epoch": 7.142469996128533,
      "grad_norm": 59.765045166015625,
      "learning_rate": 3.175033337634964e-06,
      "loss": 1.6351,
      "step": 18449
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 170.8473663330078,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 0.9734,
      "step": 18450
    },
    {
      "epoch": 7.143244289585753,
      "grad_norm": 48.386749267578125,
      "learning_rate": 3.174173011571386e-06,
      "loss": 0.1258,
      "step": 18451
    },
    {
      "epoch": 7.143631436314363,
      "grad_norm": 86.68120574951172,
      "learning_rate": 3.1737428485395966e-06,
      "loss": 0.8241,
      "step": 18452
    },
    {
      "epoch": 7.144018583042973,
      "grad_norm": 64.17847442626953,
      "learning_rate": 3.1733126855078076e-06,
      "loss": 3.9854,
      "step": 18453
    },
    {
      "epoch": 7.1444057297715835,
      "grad_norm": 144.91200256347656,
      "learning_rate": 3.1728825224760186e-06,
      "loss": 2.8216,
      "step": 18454
    },
    {
      "epoch": 7.144792876500194,
      "grad_norm": 635.7317504882812,
      "learning_rate": 3.1724523594442296e-06,
      "loss": 3.1309,
      "step": 18455
    },
    {
      "epoch": 7.145180023228804,
      "grad_norm": 110.01751708984375,
      "learning_rate": 3.17202219641244e-06,
      "loss": 0.9543,
      "step": 18456
    },
    {
      "epoch": 7.145567169957414,
      "grad_norm": 6.548089981079102,
      "learning_rate": 3.1715920333806516e-06,
      "loss": 0.1858,
      "step": 18457
    },
    {
      "epoch": 7.145954316686024,
      "grad_norm": 34.27014923095703,
      "learning_rate": 3.171161870348862e-06,
      "loss": 1.2112,
      "step": 18458
    },
    {
      "epoch": 7.146341463414634,
      "grad_norm": 32.37309265136719,
      "learning_rate": 3.1707317073170736e-06,
      "loss": 0.3766,
      "step": 18459
    },
    {
      "epoch": 7.146728610143244,
      "grad_norm": 54.691650390625,
      "learning_rate": 3.170301544285284e-06,
      "loss": 3.9804,
      "step": 18460
    },
    {
      "epoch": 7.147115756871854,
      "grad_norm": 57.8478889465332,
      "learning_rate": 3.1698713812534955e-06,
      "loss": 0.8334,
      "step": 18461
    },
    {
      "epoch": 7.147502903600465,
      "grad_norm": 11.107481002807617,
      "learning_rate": 3.169441218221706e-06,
      "loss": 0.1144,
      "step": 18462
    },
    {
      "epoch": 7.1478900503290745,
      "grad_norm": 55.89045715332031,
      "learning_rate": 3.169011055189917e-06,
      "loss": 0.6733,
      "step": 18463
    },
    {
      "epoch": 7.148277197057685,
      "grad_norm": 103.64398193359375,
      "learning_rate": 3.168580892158128e-06,
      "loss": 0.8226,
      "step": 18464
    },
    {
      "epoch": 7.148664343786295,
      "grad_norm": 76.96346282958984,
      "learning_rate": 3.168150729126339e-06,
      "loss": 0.5434,
      "step": 18465
    },
    {
      "epoch": 7.149051490514905,
      "grad_norm": 69.45552825927734,
      "learning_rate": 3.1677205660945505e-06,
      "loss": 0.9578,
      "step": 18466
    },
    {
      "epoch": 7.149438637243516,
      "grad_norm": 49.75795364379883,
      "learning_rate": 3.167290403062761e-06,
      "loss": 0.5689,
      "step": 18467
    },
    {
      "epoch": 7.149825783972125,
      "grad_norm": 59.8064079284668,
      "learning_rate": 3.166860240030972e-06,
      "loss": 0.69,
      "step": 18468
    },
    {
      "epoch": 7.150212930700736,
      "grad_norm": 51.6201057434082,
      "learning_rate": 3.166430076999183e-06,
      "loss": 1.3,
      "step": 18469
    },
    {
      "epoch": 7.1506000774293454,
      "grad_norm": 64.23214721679688,
      "learning_rate": 3.165999913967394e-06,
      "loss": 3.0513,
      "step": 18470
    },
    {
      "epoch": 7.150987224157956,
      "grad_norm": 196.84413146972656,
      "learning_rate": 3.1655697509356046e-06,
      "loss": 2.0047,
      "step": 18471
    },
    {
      "epoch": 7.151374370886566,
      "grad_norm": 64.8094711303711,
      "learning_rate": 3.165139587903816e-06,
      "loss": 2.9268,
      "step": 18472
    },
    {
      "epoch": 7.151761517615176,
      "grad_norm": 17.356155395507812,
      "learning_rate": 3.1647094248720266e-06,
      "loss": 0.1355,
      "step": 18473
    },
    {
      "epoch": 7.152148664343787,
      "grad_norm": 31.074045181274414,
      "learning_rate": 3.164279261840238e-06,
      "loss": 2.1274,
      "step": 18474
    },
    {
      "epoch": 7.152535811072396,
      "grad_norm": 3.142246961593628,
      "learning_rate": 3.1638490988084486e-06,
      "loss": 0.1336,
      "step": 18475
    },
    {
      "epoch": 7.152922957801007,
      "grad_norm": 6.7608113288879395,
      "learning_rate": 3.16341893577666e-06,
      "loss": 0.1363,
      "step": 18476
    },
    {
      "epoch": 7.153310104529616,
      "grad_norm": 38.098915100097656,
      "learning_rate": 3.1629887727448706e-06,
      "loss": 1.2885,
      "step": 18477
    },
    {
      "epoch": 7.153697251258227,
      "grad_norm": 2.850955009460449,
      "learning_rate": 3.1625586097130816e-06,
      "loss": 0.0704,
      "step": 18478
    },
    {
      "epoch": 7.154084397986837,
      "grad_norm": 17.373523712158203,
      "learning_rate": 3.1621284466812926e-06,
      "loss": 0.327,
      "step": 18479
    },
    {
      "epoch": 7.154471544715447,
      "grad_norm": 43.57352066040039,
      "learning_rate": 3.1616982836495036e-06,
      "loss": 1.2708,
      "step": 18480
    },
    {
      "epoch": 7.1548586914440575,
      "grad_norm": 3.0066537857055664,
      "learning_rate": 3.161268120617714e-06,
      "loss": 0.1497,
      "step": 18481
    },
    {
      "epoch": 7.155245838172667,
      "grad_norm": 9.093144416809082,
      "learning_rate": 3.1608379575859255e-06,
      "loss": 0.2312,
      "step": 18482
    },
    {
      "epoch": 7.155632984901278,
      "grad_norm": 35.929176330566406,
      "learning_rate": 3.160407794554136e-06,
      "loss": 0.099,
      "step": 18483
    },
    {
      "epoch": 7.156020131629888,
      "grad_norm": 68.04480743408203,
      "learning_rate": 3.1599776315223475e-06,
      "loss": 1.0542,
      "step": 18484
    },
    {
      "epoch": 7.156407278358498,
      "grad_norm": 31.983394622802734,
      "learning_rate": 3.159547468490558e-06,
      "loss": 1.8457,
      "step": 18485
    },
    {
      "epoch": 7.156794425087108,
      "grad_norm": 66.36520385742188,
      "learning_rate": 3.159117305458769e-06,
      "loss": 1.5412,
      "step": 18486
    },
    {
      "epoch": 7.157181571815718,
      "grad_norm": 75.66246032714844,
      "learning_rate": 3.15868714242698e-06,
      "loss": 0.7323,
      "step": 18487
    },
    {
      "epoch": 7.157568718544328,
      "grad_norm": 3.7320241928100586,
      "learning_rate": 3.158256979395191e-06,
      "loss": 0.1735,
      "step": 18488
    },
    {
      "epoch": 7.157955865272938,
      "grad_norm": 39.107608795166016,
      "learning_rate": 3.1578268163634016e-06,
      "loss": 2.2775,
      "step": 18489
    },
    {
      "epoch": 7.1583430120015485,
      "grad_norm": 62.93730545043945,
      "learning_rate": 3.157396653331613e-06,
      "loss": 1.5374,
      "step": 18490
    },
    {
      "epoch": 7.158730158730159,
      "grad_norm": 85.60858154296875,
      "learning_rate": 3.1569664902998236e-06,
      "loss": 0.9744,
      "step": 18491
    },
    {
      "epoch": 7.159117305458769,
      "grad_norm": 37.20821762084961,
      "learning_rate": 3.156536327268035e-06,
      "loss": 0.9234,
      "step": 18492
    },
    {
      "epoch": 7.159504452187379,
      "grad_norm": 24.780670166015625,
      "learning_rate": 3.1561061642362456e-06,
      "loss": 0.2762,
      "step": 18493
    },
    {
      "epoch": 7.159891598915989,
      "grad_norm": 8.642133712768555,
      "learning_rate": 3.155676001204457e-06,
      "loss": 0.2236,
      "step": 18494
    },
    {
      "epoch": 7.160278745644599,
      "grad_norm": 8.504179000854492,
      "learning_rate": 3.1552458381726676e-06,
      "loss": 0.0797,
      "step": 18495
    },
    {
      "epoch": 7.16066589237321,
      "grad_norm": 15.172879219055176,
      "learning_rate": 3.1548156751408786e-06,
      "loss": 0.6097,
      "step": 18496
    },
    {
      "epoch": 7.1610530391018195,
      "grad_norm": 40.24188995361328,
      "learning_rate": 3.1543855121090896e-06,
      "loss": 0.2148,
      "step": 18497
    },
    {
      "epoch": 7.16144018583043,
      "grad_norm": 92.18267822265625,
      "learning_rate": 3.1539553490773006e-06,
      "loss": 1.2278,
      "step": 18498
    },
    {
      "epoch": 7.16182733255904,
      "grad_norm": 127.1252670288086,
      "learning_rate": 3.153525186045511e-06,
      "loss": 2.0423,
      "step": 18499
    },
    {
      "epoch": 7.16221447928765,
      "grad_norm": 95.5158462524414,
      "learning_rate": 3.1530950230137226e-06,
      "loss": 1.1878,
      "step": 18500
    },
    {
      "epoch": 7.16260162601626,
      "grad_norm": 50.871009826660156,
      "learning_rate": 3.152664859981933e-06,
      "loss": 0.1794,
      "step": 18501
    },
    {
      "epoch": 7.16298877274487,
      "grad_norm": 31.8577938079834,
      "learning_rate": 3.1522346969501445e-06,
      "loss": 0.313,
      "step": 18502
    },
    {
      "epoch": 7.163375919473481,
      "grad_norm": 207.95175170898438,
      "learning_rate": 3.151804533918355e-06,
      "loss": 2.7794,
      "step": 18503
    },
    {
      "epoch": 7.16376306620209,
      "grad_norm": 6.821345806121826,
      "learning_rate": 3.151374370886566e-06,
      "loss": 0.1939,
      "step": 18504
    },
    {
      "epoch": 7.164150212930701,
      "grad_norm": 2.2808260917663574,
      "learning_rate": 3.150944207854777e-06,
      "loss": 0.0434,
      "step": 18505
    },
    {
      "epoch": 7.1645373596593105,
      "grad_norm": 9.085165023803711,
      "learning_rate": 3.150514044822988e-06,
      "loss": 0.2309,
      "step": 18506
    },
    {
      "epoch": 7.164924506387921,
      "grad_norm": 183.53817749023438,
      "learning_rate": 3.1500838817911995e-06,
      "loss": 1.6552,
      "step": 18507
    },
    {
      "epoch": 7.1653116531165315,
      "grad_norm": 15.02695083618164,
      "learning_rate": 3.14965371875941e-06,
      "loss": 0.1413,
      "step": 18508
    },
    {
      "epoch": 7.165698799845141,
      "grad_norm": 39.566749572753906,
      "learning_rate": 3.1492235557276215e-06,
      "loss": 2.4075,
      "step": 18509
    },
    {
      "epoch": 7.166085946573752,
      "grad_norm": 4.6142072677612305,
      "learning_rate": 3.148793392695832e-06,
      "loss": 0.0255,
      "step": 18510
    },
    {
      "epoch": 7.166473093302361,
      "grad_norm": 74.52599334716797,
      "learning_rate": 3.148363229664043e-06,
      "loss": 2.7051,
      "step": 18511
    },
    {
      "epoch": 7.166860240030972,
      "grad_norm": 90.6738052368164,
      "learning_rate": 3.147933066632254e-06,
      "loss": 0.2056,
      "step": 18512
    },
    {
      "epoch": 7.167247386759582,
      "grad_norm": 70.55902099609375,
      "learning_rate": 3.147502903600465e-06,
      "loss": 1.6541,
      "step": 18513
    },
    {
      "epoch": 7.167634533488192,
      "grad_norm": 43.262691497802734,
      "learning_rate": 3.1470727405686756e-06,
      "loss": 1.4417,
      "step": 18514
    },
    {
      "epoch": 7.168021680216802,
      "grad_norm": 54.92911911010742,
      "learning_rate": 3.146642577536887e-06,
      "loss": 3.3468,
      "step": 18515
    },
    {
      "epoch": 7.168408826945412,
      "grad_norm": 73.88362884521484,
      "learning_rate": 3.1462124145050976e-06,
      "loss": 0.3775,
      "step": 18516
    },
    {
      "epoch": 7.1687959736740225,
      "grad_norm": 3.322652578353882,
      "learning_rate": 3.145782251473309e-06,
      "loss": 0.1208,
      "step": 18517
    },
    {
      "epoch": 7.169183120402632,
      "grad_norm": 20.27936363220215,
      "learning_rate": 3.1453520884415196e-06,
      "loss": 1.9741,
      "step": 18518
    },
    {
      "epoch": 7.169570267131243,
      "grad_norm": 1.5209262371063232,
      "learning_rate": 3.1449219254097306e-06,
      "loss": 0.0411,
      "step": 18519
    },
    {
      "epoch": 7.169957413859853,
      "grad_norm": 187.07015991210938,
      "learning_rate": 3.1444917623779415e-06,
      "loss": 0.6226,
      "step": 18520
    },
    {
      "epoch": 7.170344560588463,
      "grad_norm": 65.8480453491211,
      "learning_rate": 3.1440615993461525e-06,
      "loss": 0.4076,
      "step": 18521
    },
    {
      "epoch": 7.170731707317073,
      "grad_norm": 88.1145248413086,
      "learning_rate": 3.143631436314363e-06,
      "loss": 0.2947,
      "step": 18522
    },
    {
      "epoch": 7.171118854045683,
      "grad_norm": 2.6999504566192627,
      "learning_rate": 3.1432012732825745e-06,
      "loss": 0.1175,
      "step": 18523
    },
    {
      "epoch": 7.1715060007742935,
      "grad_norm": 51.36284255981445,
      "learning_rate": 3.142771110250785e-06,
      "loss": 0.9602,
      "step": 18524
    },
    {
      "epoch": 7.171893147502904,
      "grad_norm": 19.318918228149414,
      "learning_rate": 3.1423409472189965e-06,
      "loss": 0.2357,
      "step": 18525
    },
    {
      "epoch": 7.172280294231514,
      "grad_norm": 0.6663840413093567,
      "learning_rate": 3.141910784187207e-06,
      "loss": 0.0117,
      "step": 18526
    },
    {
      "epoch": 7.172667440960124,
      "grad_norm": 83.20907592773438,
      "learning_rate": 3.1414806211554185e-06,
      "loss": 0.2821,
      "step": 18527
    },
    {
      "epoch": 7.173054587688734,
      "grad_norm": 54.70197296142578,
      "learning_rate": 3.141050458123629e-06,
      "loss": 1.8532,
      "step": 18528
    },
    {
      "epoch": 7.173441734417344,
      "grad_norm": 80.68433380126953,
      "learning_rate": 3.14062029509184e-06,
      "loss": 0.8738,
      "step": 18529
    },
    {
      "epoch": 7.173828881145955,
      "grad_norm": 16.48959732055664,
      "learning_rate": 3.140190132060051e-06,
      "loss": 0.3341,
      "step": 18530
    },
    {
      "epoch": 7.174216027874564,
      "grad_norm": 1.4768285751342773,
      "learning_rate": 3.139759969028262e-06,
      "loss": 0.0447,
      "step": 18531
    },
    {
      "epoch": 7.174603174603175,
      "grad_norm": 1.629744291305542,
      "learning_rate": 3.1393298059964726e-06,
      "loss": 0.0868,
      "step": 18532
    },
    {
      "epoch": 7.1749903213317845,
      "grad_norm": 77.1971664428711,
      "learning_rate": 3.138899642964684e-06,
      "loss": 1.6146,
      "step": 18533
    },
    {
      "epoch": 7.175377468060395,
      "grad_norm": 16.583940505981445,
      "learning_rate": 3.1384694799328946e-06,
      "loss": 0.325,
      "step": 18534
    },
    {
      "epoch": 7.175764614789005,
      "grad_norm": 56.492584228515625,
      "learning_rate": 3.138039316901106e-06,
      "loss": 2.3712,
      "step": 18535
    },
    {
      "epoch": 7.176151761517615,
      "grad_norm": 65.04608154296875,
      "learning_rate": 3.1376091538693166e-06,
      "loss": 3.9435,
      "step": 18536
    },
    {
      "epoch": 7.176538908246226,
      "grad_norm": 26.795116424560547,
      "learning_rate": 3.1371789908375276e-06,
      "loss": 0.2819,
      "step": 18537
    },
    {
      "epoch": 7.176926054974835,
      "grad_norm": 91.0682601928711,
      "learning_rate": 3.1367488278057386e-06,
      "loss": 1.3046,
      "step": 18538
    },
    {
      "epoch": 7.177313201703446,
      "grad_norm": 152.52935791015625,
      "learning_rate": 3.1363186647739496e-06,
      "loss": 2.564,
      "step": 18539
    },
    {
      "epoch": 7.177700348432055,
      "grad_norm": 1.4650161266326904,
      "learning_rate": 3.13588850174216e-06,
      "loss": 0.0378,
      "step": 18540
    },
    {
      "epoch": 7.178087495160666,
      "grad_norm": 85.01407623291016,
      "learning_rate": 3.1354583387103715e-06,
      "loss": 0.82,
      "step": 18541
    },
    {
      "epoch": 7.178474641889276,
      "grad_norm": 2.8692760467529297,
      "learning_rate": 3.135028175678582e-06,
      "loss": 0.112,
      "step": 18542
    },
    {
      "epoch": 7.178861788617886,
      "grad_norm": 76.25696563720703,
      "learning_rate": 3.1345980126467935e-06,
      "loss": 2.8441,
      "step": 18543
    },
    {
      "epoch": 7.1792489353464966,
      "grad_norm": 4.038754463195801,
      "learning_rate": 3.134167849615004e-06,
      "loss": 0.1779,
      "step": 18544
    },
    {
      "epoch": 7.179636082075106,
      "grad_norm": 3.2300689220428467,
      "learning_rate": 3.1337376865832155e-06,
      "loss": 0.0349,
      "step": 18545
    },
    {
      "epoch": 7.180023228803717,
      "grad_norm": 210.11732482910156,
      "learning_rate": 3.133307523551426e-06,
      "loss": 0.4642,
      "step": 18546
    },
    {
      "epoch": 7.180410375532327,
      "grad_norm": 214.38638305664062,
      "learning_rate": 3.132877360519637e-06,
      "loss": 0.966,
      "step": 18547
    },
    {
      "epoch": 7.180797522260937,
      "grad_norm": 5.727411270141602,
      "learning_rate": 3.1324471974878485e-06,
      "loss": 0.1245,
      "step": 18548
    },
    {
      "epoch": 7.181184668989547,
      "grad_norm": 1.448074221611023,
      "learning_rate": 3.132017034456059e-06,
      "loss": 0.0516,
      "step": 18549
    },
    {
      "epoch": 7.181571815718157,
      "grad_norm": 24.87788200378418,
      "learning_rate": 3.1315868714242705e-06,
      "loss": 1.2547,
      "step": 18550
    },
    {
      "epoch": 7.1819589624467675,
      "grad_norm": 6.9907450675964355,
      "learning_rate": 3.131156708392481e-06,
      "loss": 0.175,
      "step": 18551
    },
    {
      "epoch": 7.182346109175377,
      "grad_norm": 3.1258368492126465,
      "learning_rate": 3.130726545360692e-06,
      "loss": 0.0567,
      "step": 18552
    },
    {
      "epoch": 7.182733255903988,
      "grad_norm": 26.922889709472656,
      "learning_rate": 3.130296382328903e-06,
      "loss": 2.6248,
      "step": 18553
    },
    {
      "epoch": 7.183120402632598,
      "grad_norm": 53.0190315246582,
      "learning_rate": 3.129866219297114e-06,
      "loss": 0.2324,
      "step": 18554
    },
    {
      "epoch": 7.183507549361208,
      "grad_norm": 135.2856903076172,
      "learning_rate": 3.1294360562653246e-06,
      "loss": 1.7232,
      "step": 18555
    },
    {
      "epoch": 7.183894696089818,
      "grad_norm": 13.187108039855957,
      "learning_rate": 3.129005893233536e-06,
      "loss": 0.1588,
      "step": 18556
    },
    {
      "epoch": 7.184281842818428,
      "grad_norm": 2.435533285140991,
      "learning_rate": 3.1285757302017466e-06,
      "loss": 0.0235,
      "step": 18557
    },
    {
      "epoch": 7.184668989547038,
      "grad_norm": 13.312321662902832,
      "learning_rate": 3.128145567169958e-06,
      "loss": 0.1112,
      "step": 18558
    },
    {
      "epoch": 7.185056136275649,
      "grad_norm": 27.51699447631836,
      "learning_rate": 3.1277154041381686e-06,
      "loss": 0.2058,
      "step": 18559
    },
    {
      "epoch": 7.1854432830042585,
      "grad_norm": 49.523529052734375,
      "learning_rate": 3.12728524110638e-06,
      "loss": 1.7196,
      "step": 18560
    },
    {
      "epoch": 7.185830429732869,
      "grad_norm": 6.336460113525391,
      "learning_rate": 3.1268550780745905e-06,
      "loss": 0.1647,
      "step": 18561
    },
    {
      "epoch": 7.186217576461479,
      "grad_norm": 100.281982421875,
      "learning_rate": 3.1264249150428015e-06,
      "loss": 2.2461,
      "step": 18562
    },
    {
      "epoch": 7.186604723190089,
      "grad_norm": 10.08446216583252,
      "learning_rate": 3.1259947520110125e-06,
      "loss": 0.2013,
      "step": 18563
    },
    {
      "epoch": 7.186991869918699,
      "grad_norm": 111.3543472290039,
      "learning_rate": 3.1255645889792235e-06,
      "loss": 1.8051,
      "step": 18564
    },
    {
      "epoch": 7.187379016647309,
      "grad_norm": 5.239669322967529,
      "learning_rate": 3.125134425947434e-06,
      "loss": 0.1504,
      "step": 18565
    },
    {
      "epoch": 7.18776616337592,
      "grad_norm": 10.255026817321777,
      "learning_rate": 3.1247042629156455e-06,
      "loss": 0.1395,
      "step": 18566
    },
    {
      "epoch": 7.188153310104529,
      "grad_norm": 1.9809834957122803,
      "learning_rate": 3.124274099883856e-06,
      "loss": 0.0801,
      "step": 18567
    },
    {
      "epoch": 7.18854045683314,
      "grad_norm": 94.888916015625,
      "learning_rate": 3.1238439368520675e-06,
      "loss": 3.4194,
      "step": 18568
    },
    {
      "epoch": 7.1889276035617495,
      "grad_norm": 97.25084686279297,
      "learning_rate": 3.123413773820278e-06,
      "loss": 2.6659,
      "step": 18569
    },
    {
      "epoch": 7.18931475029036,
      "grad_norm": 9.245370864868164,
      "learning_rate": 3.122983610788489e-06,
      "loss": 0.0858,
      "step": 18570
    },
    {
      "epoch": 7.1897018970189706,
      "grad_norm": 21.899368286132812,
      "learning_rate": 3.1225534477567e-06,
      "loss": 0.3359,
      "step": 18571
    },
    {
      "epoch": 7.19008904374758,
      "grad_norm": 28.552047729492188,
      "learning_rate": 3.122123284724911e-06,
      "loss": 0.2601,
      "step": 18572
    },
    {
      "epoch": 7.190476190476191,
      "grad_norm": 0.9354265332221985,
      "learning_rate": 3.1216931216931216e-06,
      "loss": 0.0286,
      "step": 18573
    },
    {
      "epoch": 7.1908633372048,
      "grad_norm": 31.043832778930664,
      "learning_rate": 3.121262958661333e-06,
      "loss": 0.3464,
      "step": 18574
    },
    {
      "epoch": 7.191250483933411,
      "grad_norm": 0.3513011336326599,
      "learning_rate": 3.1208327956295436e-06,
      "loss": 0.0093,
      "step": 18575
    },
    {
      "epoch": 7.191637630662021,
      "grad_norm": 4.583715438842773,
      "learning_rate": 3.120402632597755e-06,
      "loss": 0.121,
      "step": 18576
    },
    {
      "epoch": 7.192024777390631,
      "grad_norm": 43.97075271606445,
      "learning_rate": 3.1199724695659656e-06,
      "loss": 2.0777,
      "step": 18577
    },
    {
      "epoch": 7.1924119241192415,
      "grad_norm": 110.63802337646484,
      "learning_rate": 3.119542306534177e-06,
      "loss": 3.1933,
      "step": 18578
    },
    {
      "epoch": 7.192799070847851,
      "grad_norm": 135.42242431640625,
      "learning_rate": 3.1191121435023876e-06,
      "loss": 1.1161,
      "step": 18579
    },
    {
      "epoch": 7.193186217576462,
      "grad_norm": 80.07664489746094,
      "learning_rate": 3.1186819804705985e-06,
      "loss": 1.3993,
      "step": 18580
    },
    {
      "epoch": 7.193573364305071,
      "grad_norm": 6.723100662231445,
      "learning_rate": 3.1182518174388095e-06,
      "loss": 0.1016,
      "step": 18581
    },
    {
      "epoch": 7.193960511033682,
      "grad_norm": 35.35786437988281,
      "learning_rate": 3.1178216544070205e-06,
      "loss": 1.2464,
      "step": 18582
    },
    {
      "epoch": 7.194347657762292,
      "grad_norm": 50.139373779296875,
      "learning_rate": 3.117391491375231e-06,
      "loss": 1.502,
      "step": 18583
    },
    {
      "epoch": 7.194734804490902,
      "grad_norm": 56.30813217163086,
      "learning_rate": 3.1169613283434425e-06,
      "loss": 0.1656,
      "step": 18584
    },
    {
      "epoch": 7.195121951219512,
      "grad_norm": 0.933983325958252,
      "learning_rate": 3.116531165311653e-06,
      "loss": 0.0354,
      "step": 18585
    },
    {
      "epoch": 7.195509097948122,
      "grad_norm": 52.160491943359375,
      "learning_rate": 3.1161010022798645e-06,
      "loss": 1.0534,
      "step": 18586
    },
    {
      "epoch": 7.1958962446767325,
      "grad_norm": 3.5391745567321777,
      "learning_rate": 3.115670839248075e-06,
      "loss": 0.1783,
      "step": 18587
    },
    {
      "epoch": 7.196283391405343,
      "grad_norm": 153.2515411376953,
      "learning_rate": 3.115240676216286e-06,
      "loss": 0.9959,
      "step": 18588
    },
    {
      "epoch": 7.196670538133953,
      "grad_norm": 49.843231201171875,
      "learning_rate": 3.1148105131844975e-06,
      "loss": 0.6532,
      "step": 18589
    },
    {
      "epoch": 7.197057684862563,
      "grad_norm": 29.194673538208008,
      "learning_rate": 3.114380350152708e-06,
      "loss": 2.1668,
      "step": 18590
    },
    {
      "epoch": 7.197444831591173,
      "grad_norm": 116.97048950195312,
      "learning_rate": 3.1139501871209195e-06,
      "loss": 3.0799,
      "step": 18591
    },
    {
      "epoch": 7.197831978319783,
      "grad_norm": 9.299267768859863,
      "learning_rate": 3.11352002408913e-06,
      "loss": 0.0999,
      "step": 18592
    },
    {
      "epoch": 7.198219125048393,
      "grad_norm": 2.5892221927642822,
      "learning_rate": 3.1130898610573414e-06,
      "loss": 0.0434,
      "step": 18593
    },
    {
      "epoch": 7.198606271777003,
      "grad_norm": 65.17169952392578,
      "learning_rate": 3.112659698025552e-06,
      "loss": 1.6896,
      "step": 18594
    },
    {
      "epoch": 7.198993418505614,
      "grad_norm": 15.843472480773926,
      "learning_rate": 3.112229534993763e-06,
      "loss": 0.2737,
      "step": 18595
    },
    {
      "epoch": 7.1993805652342235,
      "grad_norm": 106.13317108154297,
      "learning_rate": 3.111799371961974e-06,
      "loss": 0.2372,
      "step": 18596
    },
    {
      "epoch": 7.199767711962834,
      "grad_norm": 47.167213439941406,
      "learning_rate": 3.111369208930185e-06,
      "loss": 1.3546,
      "step": 18597
    },
    {
      "epoch": 7.200154858691444,
      "grad_norm": 13.724105834960938,
      "learning_rate": 3.1109390458983956e-06,
      "loss": 0.1933,
      "step": 18598
    },
    {
      "epoch": 7.200542005420054,
      "grad_norm": 93.4868392944336,
      "learning_rate": 3.110508882866607e-06,
      "loss": 1.1432,
      "step": 18599
    },
    {
      "epoch": 7.200929152148665,
      "grad_norm": 171.10487365722656,
      "learning_rate": 3.1100787198348175e-06,
      "loss": 2.6066,
      "step": 18600
    },
    {
      "epoch": 7.201316298877274,
      "grad_norm": 46.34136962890625,
      "learning_rate": 3.109648556803029e-06,
      "loss": 0.2284,
      "step": 18601
    },
    {
      "epoch": 7.201703445605885,
      "grad_norm": 94.23118591308594,
      "learning_rate": 3.1092183937712395e-06,
      "loss": 0.9435,
      "step": 18602
    },
    {
      "epoch": 7.2020905923344944,
      "grad_norm": 96.09617614746094,
      "learning_rate": 3.1087882307394505e-06,
      "loss": 2.2971,
      "step": 18603
    },
    {
      "epoch": 7.202477739063105,
      "grad_norm": 111.30435180664062,
      "learning_rate": 3.1083580677076615e-06,
      "loss": 1.3052,
      "step": 18604
    },
    {
      "epoch": 7.2028648857917155,
      "grad_norm": 73.5105972290039,
      "learning_rate": 3.1079279046758725e-06,
      "loss": 1.2106,
      "step": 18605
    },
    {
      "epoch": 7.203252032520325,
      "grad_norm": 104.13905334472656,
      "learning_rate": 3.107497741644083e-06,
      "loss": 1.6214,
      "step": 18606
    },
    {
      "epoch": 7.203639179248936,
      "grad_norm": 0.44793209433555603,
      "learning_rate": 3.1070675786122945e-06,
      "loss": 0.0115,
      "step": 18607
    },
    {
      "epoch": 7.204026325977545,
      "grad_norm": 50.503379821777344,
      "learning_rate": 3.106637415580505e-06,
      "loss": 1.6984,
      "step": 18608
    },
    {
      "epoch": 7.204413472706156,
      "grad_norm": 39.41508102416992,
      "learning_rate": 3.1062072525487165e-06,
      "loss": 1.7923,
      "step": 18609
    },
    {
      "epoch": 7.204800619434765,
      "grad_norm": 229.85292053222656,
      "learning_rate": 3.105777089516927e-06,
      "loss": 0.9718,
      "step": 18610
    },
    {
      "epoch": 7.205187766163376,
      "grad_norm": 82.69849395751953,
      "learning_rate": 3.1053469264851385e-06,
      "loss": 1.151,
      "step": 18611
    },
    {
      "epoch": 7.205574912891986,
      "grad_norm": 63.471649169921875,
      "learning_rate": 3.104916763453349e-06,
      "loss": 0.8534,
      "step": 18612
    },
    {
      "epoch": 7.205962059620596,
      "grad_norm": 11.35297679901123,
      "learning_rate": 3.10448660042156e-06,
      "loss": 0.2364,
      "step": 18613
    },
    {
      "epoch": 7.2063492063492065,
      "grad_norm": 152.81065368652344,
      "learning_rate": 3.104056437389771e-06,
      "loss": 0.8885,
      "step": 18614
    },
    {
      "epoch": 7.206736353077816,
      "grad_norm": 118.2302474975586,
      "learning_rate": 3.103626274357982e-06,
      "loss": 0.4627,
      "step": 18615
    },
    {
      "epoch": 7.207123499806427,
      "grad_norm": 1.5299935340881348,
      "learning_rate": 3.1031961113261926e-06,
      "loss": 0.0489,
      "step": 18616
    },
    {
      "epoch": 7.207510646535037,
      "grad_norm": 50.60501480102539,
      "learning_rate": 3.102765948294404e-06,
      "loss": 0.5973,
      "step": 18617
    },
    {
      "epoch": 7.207897793263647,
      "grad_norm": 152.8582000732422,
      "learning_rate": 3.1023357852626146e-06,
      "loss": 0.5871,
      "step": 18618
    },
    {
      "epoch": 7.208284939992257,
      "grad_norm": 125.04363250732422,
      "learning_rate": 3.101905622230826e-06,
      "loss": 0.5779,
      "step": 18619
    },
    {
      "epoch": 7.208672086720867,
      "grad_norm": 0.6840277314186096,
      "learning_rate": 3.1014754591990365e-06,
      "loss": 0.0176,
      "step": 18620
    },
    {
      "epoch": 7.209059233449477,
      "grad_norm": 3.9318103790283203,
      "learning_rate": 3.1010452961672475e-06,
      "loss": 0.1113,
      "step": 18621
    },
    {
      "epoch": 7.209446380178088,
      "grad_norm": 3.2733521461486816,
      "learning_rate": 3.1006151331354585e-06,
      "loss": 0.0938,
      "step": 18622
    },
    {
      "epoch": 7.2098335269066975,
      "grad_norm": 22.142181396484375,
      "learning_rate": 3.1001849701036695e-06,
      "loss": 1.6235,
      "step": 18623
    },
    {
      "epoch": 7.210220673635308,
      "grad_norm": 74.50604248046875,
      "learning_rate": 3.09975480707188e-06,
      "loss": 1.7958,
      "step": 18624
    },
    {
      "epoch": 7.210607820363918,
      "grad_norm": 61.63437271118164,
      "learning_rate": 3.0993246440400915e-06,
      "loss": 1.1582,
      "step": 18625
    },
    {
      "epoch": 7.210994967092528,
      "grad_norm": 59.34358215332031,
      "learning_rate": 3.098894481008302e-06,
      "loss": 1.7734,
      "step": 18626
    },
    {
      "epoch": 7.211382113821138,
      "grad_norm": 16.943870544433594,
      "learning_rate": 3.0984643179765135e-06,
      "loss": 0.6706,
      "step": 18627
    },
    {
      "epoch": 7.211769260549748,
      "grad_norm": 380.6250915527344,
      "learning_rate": 3.098034154944724e-06,
      "loss": 4.4877,
      "step": 18628
    },
    {
      "epoch": 7.212156407278359,
      "grad_norm": 3.8908698558807373,
      "learning_rate": 3.0976039919129355e-06,
      "loss": 0.1042,
      "step": 18629
    },
    {
      "epoch": 7.2125435540069684,
      "grad_norm": 35.54188537597656,
      "learning_rate": 3.0971738288811465e-06,
      "loss": 1.4111,
      "step": 18630
    },
    {
      "epoch": 7.212930700735579,
      "grad_norm": 1.9908969402313232,
      "learning_rate": 3.096743665849357e-06,
      "loss": 0.0443,
      "step": 18631
    },
    {
      "epoch": 7.213317847464189,
      "grad_norm": 40.399234771728516,
      "learning_rate": 3.0963135028175684e-06,
      "loss": 0.6273,
      "step": 18632
    },
    {
      "epoch": 7.213704994192799,
      "grad_norm": 22.228633880615234,
      "learning_rate": 3.095883339785779e-06,
      "loss": 1.9166,
      "step": 18633
    },
    {
      "epoch": 7.21409214092141,
      "grad_norm": 39.60536193847656,
      "learning_rate": 3.0954531767539904e-06,
      "loss": 1.4138,
      "step": 18634
    },
    {
      "epoch": 7.214479287650019,
      "grad_norm": 90.66437530517578,
      "learning_rate": 3.095023013722201e-06,
      "loss": 0.6005,
      "step": 18635
    },
    {
      "epoch": 7.21486643437863,
      "grad_norm": 71.76487731933594,
      "learning_rate": 3.094592850690412e-06,
      "loss": 3.1044,
      "step": 18636
    },
    {
      "epoch": 7.215253581107239,
      "grad_norm": 181.4688262939453,
      "learning_rate": 3.094162687658623e-06,
      "loss": 0.6862,
      "step": 18637
    },
    {
      "epoch": 7.21564072783585,
      "grad_norm": 124.85871887207031,
      "learning_rate": 3.093732524626834e-06,
      "loss": 2.036,
      "step": 18638
    },
    {
      "epoch": 7.21602787456446,
      "grad_norm": 11.708610534667969,
      "learning_rate": 3.0933023615950445e-06,
      "loss": 0.1959,
      "step": 18639
    },
    {
      "epoch": 7.21641502129307,
      "grad_norm": 47.716217041015625,
      "learning_rate": 3.092872198563256e-06,
      "loss": 2.069,
      "step": 18640
    },
    {
      "epoch": 7.2168021680216805,
      "grad_norm": 5.8648247718811035,
      "learning_rate": 3.0924420355314665e-06,
      "loss": 0.1397,
      "step": 18641
    },
    {
      "epoch": 7.21718931475029,
      "grad_norm": 11.728827476501465,
      "learning_rate": 3.092011872499678e-06,
      "loss": 0.0918,
      "step": 18642
    },
    {
      "epoch": 7.217576461478901,
      "grad_norm": 46.94626998901367,
      "learning_rate": 3.0915817094678885e-06,
      "loss": 0.8648,
      "step": 18643
    },
    {
      "epoch": 7.21796360820751,
      "grad_norm": 88.52554321289062,
      "learning_rate": 3.0911515464360995e-06,
      "loss": 0.6831,
      "step": 18644
    },
    {
      "epoch": 7.218350754936121,
      "grad_norm": 172.39012145996094,
      "learning_rate": 3.0907213834043105e-06,
      "loss": 2.69,
      "step": 18645
    },
    {
      "epoch": 7.218737901664731,
      "grad_norm": 1.3490699529647827,
      "learning_rate": 3.0902912203725215e-06,
      "loss": 0.0347,
      "step": 18646
    },
    {
      "epoch": 7.219125048393341,
      "grad_norm": 26.27596092224121,
      "learning_rate": 3.0898610573407325e-06,
      "loss": 0.1146,
      "step": 18647
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 36.32470703125,
      "learning_rate": 3.0894308943089435e-06,
      "loss": 0.7064,
      "step": 18648
    },
    {
      "epoch": 7.219899341850561,
      "grad_norm": 29.54905128479004,
      "learning_rate": 3.089000731277154e-06,
      "loss": 2.0187,
      "step": 18649
    },
    {
      "epoch": 7.2202864885791715,
      "grad_norm": 40.13302230834961,
      "learning_rate": 3.0885705682453655e-06,
      "loss": 1.8991,
      "step": 18650
    },
    {
      "epoch": 7.220673635307782,
      "grad_norm": 46.49088668823242,
      "learning_rate": 3.088140405213576e-06,
      "loss": 0.2734,
      "step": 18651
    },
    {
      "epoch": 7.221060782036392,
      "grad_norm": 3.453334331512451,
      "learning_rate": 3.0877102421817874e-06,
      "loss": 0.1153,
      "step": 18652
    },
    {
      "epoch": 7.221447928765002,
      "grad_norm": 65.72156524658203,
      "learning_rate": 3.087280079149998e-06,
      "loss": 0.9085,
      "step": 18653
    },
    {
      "epoch": 7.221835075493612,
      "grad_norm": 36.01569366455078,
      "learning_rate": 3.086849916118209e-06,
      "loss": 2.5618,
      "step": 18654
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 3.739238739013672,
      "learning_rate": 3.08641975308642e-06,
      "loss": 0.0695,
      "step": 18655
    },
    {
      "epoch": 7.222609368950832,
      "grad_norm": 1.878005027770996,
      "learning_rate": 3.085989590054631e-06,
      "loss": 0.0868,
      "step": 18656
    },
    {
      "epoch": 7.2229965156794425,
      "grad_norm": 71.96881866455078,
      "learning_rate": 3.0855594270228416e-06,
      "loss": 2.183,
      "step": 18657
    },
    {
      "epoch": 7.223383662408053,
      "grad_norm": 0.8994696140289307,
      "learning_rate": 3.085129263991053e-06,
      "loss": 0.0331,
      "step": 18658
    },
    {
      "epoch": 7.223770809136663,
      "grad_norm": 76.94525909423828,
      "learning_rate": 3.0846991009592635e-06,
      "loss": 1.3562,
      "step": 18659
    },
    {
      "epoch": 7.224157955865273,
      "grad_norm": 1.7568650245666504,
      "learning_rate": 3.084268937927475e-06,
      "loss": 0.0563,
      "step": 18660
    },
    {
      "epoch": 7.224545102593883,
      "grad_norm": 117.35314178466797,
      "learning_rate": 3.0838387748956855e-06,
      "loss": 2.2636,
      "step": 18661
    },
    {
      "epoch": 7.224932249322493,
      "grad_norm": 3.171959400177002,
      "learning_rate": 3.0834086118638965e-06,
      "loss": 0.1345,
      "step": 18662
    },
    {
      "epoch": 7.225319396051104,
      "grad_norm": 63.56470489501953,
      "learning_rate": 3.0829784488321075e-06,
      "loss": 2.0821,
      "step": 18663
    },
    {
      "epoch": 7.225706542779713,
      "grad_norm": 60.22530746459961,
      "learning_rate": 3.0825482858003185e-06,
      "loss": 1.3135,
      "step": 18664
    },
    {
      "epoch": 7.226093689508324,
      "grad_norm": 35.008522033691406,
      "learning_rate": 3.0821181227685295e-06,
      "loss": 0.9825,
      "step": 18665
    },
    {
      "epoch": 7.2264808362369335,
      "grad_norm": 34.81047058105469,
      "learning_rate": 3.0816879597367405e-06,
      "loss": 1.6182,
      "step": 18666
    },
    {
      "epoch": 7.226867982965544,
      "grad_norm": 6.131830215454102,
      "learning_rate": 3.081257796704951e-06,
      "loss": 0.1959,
      "step": 18667
    },
    {
      "epoch": 7.2272551296941545,
      "grad_norm": 115.35601043701172,
      "learning_rate": 3.0808276336731625e-06,
      "loss": 1.8075,
      "step": 18668
    },
    {
      "epoch": 7.227642276422764,
      "grad_norm": 64.48641967773438,
      "learning_rate": 3.080397470641373e-06,
      "loss": 0.3161,
      "step": 18669
    },
    {
      "epoch": 7.228029423151375,
      "grad_norm": 28.36837387084961,
      "learning_rate": 3.0799673076095845e-06,
      "loss": 0.1335,
      "step": 18670
    },
    {
      "epoch": 7.228416569879984,
      "grad_norm": 1.1581703424453735,
      "learning_rate": 3.0795371445777954e-06,
      "loss": 0.0314,
      "step": 18671
    },
    {
      "epoch": 7.228803716608595,
      "grad_norm": 119.8317642211914,
      "learning_rate": 3.079106981546006e-06,
      "loss": 0.5014,
      "step": 18672
    },
    {
      "epoch": 7.229190863337204,
      "grad_norm": 87.02146911621094,
      "learning_rate": 3.0786768185142174e-06,
      "loss": 3.118,
      "step": 18673
    },
    {
      "epoch": 7.229578010065815,
      "grad_norm": 2.250195026397705,
      "learning_rate": 3.078246655482428e-06,
      "loss": 0.0732,
      "step": 18674
    },
    {
      "epoch": 7.229965156794425,
      "grad_norm": 22.46486473083496,
      "learning_rate": 3.0778164924506394e-06,
      "loss": 2.0658,
      "step": 18675
    },
    {
      "epoch": 7.230352303523035,
      "grad_norm": 45.15589141845703,
      "learning_rate": 3.07738632941885e-06,
      "loss": 1.358,
      "step": 18676
    },
    {
      "epoch": 7.2307394502516456,
      "grad_norm": 98.13255310058594,
      "learning_rate": 3.076956166387061e-06,
      "loss": 2.9151,
      "step": 18677
    },
    {
      "epoch": 7.231126596980255,
      "grad_norm": 89.22063446044922,
      "learning_rate": 3.076526003355272e-06,
      "loss": 0.2403,
      "step": 18678
    },
    {
      "epoch": 7.231513743708866,
      "grad_norm": 8.90153980255127,
      "learning_rate": 3.076095840323483e-06,
      "loss": 0.1873,
      "step": 18679
    },
    {
      "epoch": 7.231900890437476,
      "grad_norm": 62.616233825683594,
      "learning_rate": 3.0756656772916935e-06,
      "loss": 1.0722,
      "step": 18680
    },
    {
      "epoch": 7.232288037166086,
      "grad_norm": 24.96406364440918,
      "learning_rate": 3.075235514259905e-06,
      "loss": 0.152,
      "step": 18681
    },
    {
      "epoch": 7.232675183894696,
      "grad_norm": 48.83932876586914,
      "learning_rate": 3.0748053512281155e-06,
      "loss": 0.3231,
      "step": 18682
    },
    {
      "epoch": 7.233062330623306,
      "grad_norm": 117.69148254394531,
      "learning_rate": 3.074375188196327e-06,
      "loss": 5.0813,
      "step": 18683
    },
    {
      "epoch": 7.2334494773519165,
      "grad_norm": 150.28109741210938,
      "learning_rate": 3.0739450251645375e-06,
      "loss": 0.7607,
      "step": 18684
    },
    {
      "epoch": 7.233836624080526,
      "grad_norm": 71.8635025024414,
      "learning_rate": 3.073514862132749e-06,
      "loss": 1.3562,
      "step": 18685
    },
    {
      "epoch": 7.234223770809137,
      "grad_norm": 19.116987228393555,
      "learning_rate": 3.0730846991009595e-06,
      "loss": 1.9782,
      "step": 18686
    },
    {
      "epoch": 7.234610917537747,
      "grad_norm": 27.21224594116211,
      "learning_rate": 3.0726545360691705e-06,
      "loss": 2.0095,
      "step": 18687
    },
    {
      "epoch": 7.234998064266357,
      "grad_norm": 3.803572654724121,
      "learning_rate": 3.0722243730373815e-06,
      "loss": 0.154,
      "step": 18688
    },
    {
      "epoch": 7.235385210994967,
      "grad_norm": 65.4342269897461,
      "learning_rate": 3.0717942100055925e-06,
      "loss": 1.4976,
      "step": 18689
    },
    {
      "epoch": 7.235772357723577,
      "grad_norm": 80.40232849121094,
      "learning_rate": 3.071364046973803e-06,
      "loss": 0.6713,
      "step": 18690
    },
    {
      "epoch": 7.236159504452187,
      "grad_norm": 50.3667106628418,
      "learning_rate": 3.0709338839420144e-06,
      "loss": 2.2339,
      "step": 18691
    },
    {
      "epoch": 7.236546651180798,
      "grad_norm": 53.579288482666016,
      "learning_rate": 3.070503720910225e-06,
      "loss": 1.3026,
      "step": 18692
    },
    {
      "epoch": 7.2369337979094075,
      "grad_norm": 22.466381072998047,
      "learning_rate": 3.0700735578784364e-06,
      "loss": 1.3342,
      "step": 18693
    },
    {
      "epoch": 7.237320944638018,
      "grad_norm": 92.85851287841797,
      "learning_rate": 3.069643394846647e-06,
      "loss": 0.388,
      "step": 18694
    },
    {
      "epoch": 7.237708091366628,
      "grad_norm": 139.34104919433594,
      "learning_rate": 3.069213231814858e-06,
      "loss": 4.498,
      "step": 18695
    },
    {
      "epoch": 7.238095238095238,
      "grad_norm": 1.9153366088867188,
      "learning_rate": 3.068783068783069e-06,
      "loss": 0.0524,
      "step": 18696
    },
    {
      "epoch": 7.238482384823849,
      "grad_norm": 41.70700454711914,
      "learning_rate": 3.06835290575128e-06,
      "loss": 0.4883,
      "step": 18697
    },
    {
      "epoch": 7.238869531552458,
      "grad_norm": 75.98974609375,
      "learning_rate": 3.0679227427194905e-06,
      "loss": 3.9526,
      "step": 18698
    },
    {
      "epoch": 7.239256678281069,
      "grad_norm": 22.031888961791992,
      "learning_rate": 3.067492579687702e-06,
      "loss": 0.901,
      "step": 18699
    },
    {
      "epoch": 7.239643825009678,
      "grad_norm": 1.8901407718658447,
      "learning_rate": 3.0670624166559125e-06,
      "loss": 0.0741,
      "step": 18700
    },
    {
      "epoch": 7.240030971738289,
      "grad_norm": 39.52077102661133,
      "learning_rate": 3.066632253624124e-06,
      "loss": 0.6561,
      "step": 18701
    },
    {
      "epoch": 7.2404181184668985,
      "grad_norm": 86.00454711914062,
      "learning_rate": 3.0662020905923345e-06,
      "loss": 0.4537,
      "step": 18702
    },
    {
      "epoch": 7.240805265195509,
      "grad_norm": 21.314041137695312,
      "learning_rate": 3.065771927560546e-06,
      "loss": 2.3185,
      "step": 18703
    },
    {
      "epoch": 7.2411924119241196,
      "grad_norm": 14.383016586303711,
      "learning_rate": 3.0653417645287565e-06,
      "loss": 0.1426,
      "step": 18704
    },
    {
      "epoch": 7.241579558652729,
      "grad_norm": 1.3968477249145508,
      "learning_rate": 3.0649116014969675e-06,
      "loss": 0.0364,
      "step": 18705
    },
    {
      "epoch": 7.24196670538134,
      "grad_norm": 54.89665222167969,
      "learning_rate": 3.0644814384651785e-06,
      "loss": 1.556,
      "step": 18706
    },
    {
      "epoch": 7.242353852109949,
      "grad_norm": 25.089441299438477,
      "learning_rate": 3.0640512754333895e-06,
      "loss": 0.1881,
      "step": 18707
    },
    {
      "epoch": 7.24274099883856,
      "grad_norm": 125.07096862792969,
      "learning_rate": 3.0636211124016e-06,
      "loss": 2.2057,
      "step": 18708
    },
    {
      "epoch": 7.24312814556717,
      "grad_norm": 23.200550079345703,
      "learning_rate": 3.0631909493698115e-06,
      "loss": 0.1562,
      "step": 18709
    },
    {
      "epoch": 7.24351529229578,
      "grad_norm": 69.06818389892578,
      "learning_rate": 3.062760786338022e-06,
      "loss": 2.7246,
      "step": 18710
    },
    {
      "epoch": 7.2439024390243905,
      "grad_norm": 1.1140210628509521,
      "learning_rate": 3.0623306233062334e-06,
      "loss": 0.0421,
      "step": 18711
    },
    {
      "epoch": 7.244289585753,
      "grad_norm": 2.1767871379852295,
      "learning_rate": 3.0619004602744444e-06,
      "loss": 0.0375,
      "step": 18712
    },
    {
      "epoch": 7.244676732481611,
      "grad_norm": 53.5567741394043,
      "learning_rate": 3.061470297242655e-06,
      "loss": 1.27,
      "step": 18713
    },
    {
      "epoch": 7.245063879210221,
      "grad_norm": 72.7430648803711,
      "learning_rate": 3.0610401342108664e-06,
      "loss": 1.4377,
      "step": 18714
    },
    {
      "epoch": 7.245451025938831,
      "grad_norm": 90.1851577758789,
      "learning_rate": 3.060609971179077e-06,
      "loss": 1.0925,
      "step": 18715
    },
    {
      "epoch": 7.245838172667441,
      "grad_norm": 76.7409896850586,
      "learning_rate": 3.0601798081472884e-06,
      "loss": 1.9064,
      "step": 18716
    },
    {
      "epoch": 7.246225319396051,
      "grad_norm": 38.2755126953125,
      "learning_rate": 3.059749645115499e-06,
      "loss": 1.8398,
      "step": 18717
    },
    {
      "epoch": 7.246612466124661,
      "grad_norm": 87.31009674072266,
      "learning_rate": 3.0593194820837104e-06,
      "loss": 1.9678,
      "step": 18718
    },
    {
      "epoch": 7.246999612853271,
      "grad_norm": 32.609004974365234,
      "learning_rate": 3.058889319051921e-06,
      "loss": 2.0256,
      "step": 18719
    },
    {
      "epoch": 7.2473867595818815,
      "grad_norm": 78.31588745117188,
      "learning_rate": 3.058459156020132e-06,
      "loss": 0.435,
      "step": 18720
    },
    {
      "epoch": 7.247773906310492,
      "grad_norm": 4.791705131530762,
      "learning_rate": 3.058028992988343e-06,
      "loss": 0.0714,
      "step": 18721
    },
    {
      "epoch": 7.248161053039102,
      "grad_norm": 4.111755847930908,
      "learning_rate": 3.057598829956554e-06,
      "loss": 0.1065,
      "step": 18722
    },
    {
      "epoch": 7.248548199767712,
      "grad_norm": 2.1490683555603027,
      "learning_rate": 3.0571686669247645e-06,
      "loss": 0.0559,
      "step": 18723
    },
    {
      "epoch": 7.248935346496322,
      "grad_norm": 1.4130228757858276,
      "learning_rate": 3.056738503892976e-06,
      "loss": 0.0137,
      "step": 18724
    },
    {
      "epoch": 7.249322493224932,
      "grad_norm": 1.5249757766723633,
      "learning_rate": 3.0563083408611865e-06,
      "loss": 0.0808,
      "step": 18725
    },
    {
      "epoch": 7.249709639953543,
      "grad_norm": 0.2982647716999054,
      "learning_rate": 3.055878177829398e-06,
      "loss": 0.0079,
      "step": 18726
    },
    {
      "epoch": 7.250096786682152,
      "grad_norm": 3.577672243118286,
      "learning_rate": 3.0554480147976085e-06,
      "loss": 0.1747,
      "step": 18727
    },
    {
      "epoch": 7.250483933410763,
      "grad_norm": 31.224252700805664,
      "learning_rate": 3.0550178517658195e-06,
      "loss": 2.7334,
      "step": 18728
    },
    {
      "epoch": 7.2508710801393725,
      "grad_norm": 1.4276665449142456,
      "learning_rate": 3.0545876887340305e-06,
      "loss": 0.0314,
      "step": 18729
    },
    {
      "epoch": 7.251258226867983,
      "grad_norm": 24.912504196166992,
      "learning_rate": 3.0541575257022414e-06,
      "loss": 1.5923,
      "step": 18730
    },
    {
      "epoch": 7.251645373596594,
      "grad_norm": 32.81173324584961,
      "learning_rate": 3.053727362670452e-06,
      "loss": 1.2531,
      "step": 18731
    },
    {
      "epoch": 7.252032520325203,
      "grad_norm": 108.559814453125,
      "learning_rate": 3.0532971996386634e-06,
      "loss": 2.9074,
      "step": 18732
    },
    {
      "epoch": 7.252419667053814,
      "grad_norm": 53.51841354370117,
      "learning_rate": 3.052867036606874e-06,
      "loss": 0.6641,
      "step": 18733
    },
    {
      "epoch": 7.252806813782423,
      "grad_norm": 21.837310791015625,
      "learning_rate": 3.0524368735750854e-06,
      "loss": 0.1625,
      "step": 18734
    },
    {
      "epoch": 7.253193960511034,
      "grad_norm": 33.56550598144531,
      "learning_rate": 3.052006710543296e-06,
      "loss": 0.1353,
      "step": 18735
    },
    {
      "epoch": 7.253581107239643,
      "grad_norm": 57.0472412109375,
      "learning_rate": 3.0515765475115074e-06,
      "loss": 3.055,
      "step": 18736
    },
    {
      "epoch": 7.253968253968254,
      "grad_norm": 174.94810485839844,
      "learning_rate": 3.051146384479718e-06,
      "loss": 1.2938,
      "step": 18737
    },
    {
      "epoch": 7.2543554006968645,
      "grad_norm": 1.6977227926254272,
      "learning_rate": 3.050716221447929e-06,
      "loss": 0.0667,
      "step": 18738
    },
    {
      "epoch": 7.254742547425474,
      "grad_norm": 31.699718475341797,
      "learning_rate": 3.05028605841614e-06,
      "loss": 0.4482,
      "step": 18739
    },
    {
      "epoch": 7.255129694154085,
      "grad_norm": 189.15574645996094,
      "learning_rate": 3.049855895384351e-06,
      "loss": 0.6764,
      "step": 18740
    },
    {
      "epoch": 7.255516840882694,
      "grad_norm": 16.254018783569336,
      "learning_rate": 3.0494257323525615e-06,
      "loss": 1.7819,
      "step": 18741
    },
    {
      "epoch": 7.255903987611305,
      "grad_norm": 158.5787811279297,
      "learning_rate": 3.048995569320773e-06,
      "loss": 0.8769,
      "step": 18742
    },
    {
      "epoch": 7.256291134339915,
      "grad_norm": 34.3546028137207,
      "learning_rate": 3.0485654062889835e-06,
      "loss": 0.435,
      "step": 18743
    },
    {
      "epoch": 7.256678281068525,
      "grad_norm": 26.138181686401367,
      "learning_rate": 3.048135243257195e-06,
      "loss": 1.8231,
      "step": 18744
    },
    {
      "epoch": 7.257065427797135,
      "grad_norm": 34.31858825683594,
      "learning_rate": 3.0477050802254055e-06,
      "loss": 0.7987,
      "step": 18745
    },
    {
      "epoch": 7.257452574525745,
      "grad_norm": 8.295196533203125,
      "learning_rate": 3.0472749171936165e-06,
      "loss": 0.1347,
      "step": 18746
    },
    {
      "epoch": 7.2578397212543555,
      "grad_norm": 127.46508026123047,
      "learning_rate": 3.0468447541618275e-06,
      "loss": 1.9958,
      "step": 18747
    },
    {
      "epoch": 7.258226867982966,
      "grad_norm": 7.546756267547607,
      "learning_rate": 3.0464145911300385e-06,
      "loss": 0.2496,
      "step": 18748
    },
    {
      "epoch": 7.258614014711576,
      "grad_norm": 3.725484609603882,
      "learning_rate": 3.045984428098249e-06,
      "loss": 0.1505,
      "step": 18749
    },
    {
      "epoch": 7.259001161440186,
      "grad_norm": 0.9462544322013855,
      "learning_rate": 3.0455542650664604e-06,
      "loss": 0.0361,
      "step": 18750
    },
    {
      "epoch": 7.259388308168796,
      "grad_norm": 4.206315040588379,
      "learning_rate": 3.045124102034671e-06,
      "loss": 0.0579,
      "step": 18751
    },
    {
      "epoch": 7.259775454897406,
      "grad_norm": 1.4848418235778809,
      "learning_rate": 3.0446939390028824e-06,
      "loss": 0.034,
      "step": 18752
    },
    {
      "epoch": 7.260162601626016,
      "grad_norm": 2.1007795333862305,
      "learning_rate": 3.044263775971093e-06,
      "loss": 0.0413,
      "step": 18753
    },
    {
      "epoch": 7.260549748354626,
      "grad_norm": 58.845767974853516,
      "learning_rate": 3.0438336129393044e-06,
      "loss": 1.6117,
      "step": 18754
    },
    {
      "epoch": 7.260936895083237,
      "grad_norm": 27.648983001708984,
      "learning_rate": 3.0434034499075154e-06,
      "loss": 0.1842,
      "step": 18755
    },
    {
      "epoch": 7.2613240418118465,
      "grad_norm": 77.23516082763672,
      "learning_rate": 3.042973286875726e-06,
      "loss": 2.6223,
      "step": 18756
    },
    {
      "epoch": 7.261711188540457,
      "grad_norm": 2.607438325881958,
      "learning_rate": 3.0425431238439374e-06,
      "loss": 0.1443,
      "step": 18757
    },
    {
      "epoch": 7.262098335269067,
      "grad_norm": 201.1043701171875,
      "learning_rate": 3.042112960812148e-06,
      "loss": 0.4576,
      "step": 18758
    },
    {
      "epoch": 7.262485481997677,
      "grad_norm": 0.9763090014457703,
      "learning_rate": 3.0416827977803594e-06,
      "loss": 0.035,
      "step": 18759
    },
    {
      "epoch": 7.262872628726287,
      "grad_norm": 33.740028381347656,
      "learning_rate": 3.04125263474857e-06,
      "loss": 0.7518,
      "step": 18760
    },
    {
      "epoch": 7.263259775454897,
      "grad_norm": 75.06938171386719,
      "learning_rate": 3.040822471716781e-06,
      "loss": 0.9518,
      "step": 18761
    },
    {
      "epoch": 7.263646922183508,
      "grad_norm": 55.138221740722656,
      "learning_rate": 3.040392308684992e-06,
      "loss": 1.6695,
      "step": 18762
    },
    {
      "epoch": 7.2640340689121174,
      "grad_norm": 1.9731096029281616,
      "learning_rate": 3.039962145653203e-06,
      "loss": 0.0745,
      "step": 18763
    },
    {
      "epoch": 7.264421215640728,
      "grad_norm": 202.26107788085938,
      "learning_rate": 3.0395319826214135e-06,
      "loss": 1.0362,
      "step": 18764
    },
    {
      "epoch": 7.264808362369338,
      "grad_norm": 8.53441333770752,
      "learning_rate": 3.039101819589625e-06,
      "loss": 0.0754,
      "step": 18765
    },
    {
      "epoch": 7.265195509097948,
      "grad_norm": 105.55656433105469,
      "learning_rate": 3.0386716565578355e-06,
      "loss": 1.497,
      "step": 18766
    },
    {
      "epoch": 7.265582655826559,
      "grad_norm": 89.80850219726562,
      "learning_rate": 3.038241493526047e-06,
      "loss": 1.3371,
      "step": 18767
    },
    {
      "epoch": 7.265969802555168,
      "grad_norm": 2.988029718399048,
      "learning_rate": 3.0378113304942575e-06,
      "loss": 0.0649,
      "step": 18768
    },
    {
      "epoch": 7.266356949283779,
      "grad_norm": 17.219457626342773,
      "learning_rate": 3.037381167462469e-06,
      "loss": 0.2185,
      "step": 18769
    },
    {
      "epoch": 7.266744096012388,
      "grad_norm": 171.03627014160156,
      "learning_rate": 3.0369510044306794e-06,
      "loss": 2.8168,
      "step": 18770
    },
    {
      "epoch": 7.267131242740999,
      "grad_norm": 38.90983963012695,
      "learning_rate": 3.0365208413988904e-06,
      "loss": 1.3105,
      "step": 18771
    },
    {
      "epoch": 7.267518389469609,
      "grad_norm": 34.44571304321289,
      "learning_rate": 3.0360906783671014e-06,
      "loss": 1.7705,
      "step": 18772
    },
    {
      "epoch": 7.267905536198219,
      "grad_norm": 79.49649047851562,
      "learning_rate": 3.0356605153353124e-06,
      "loss": 2.3985,
      "step": 18773
    },
    {
      "epoch": 7.2682926829268295,
      "grad_norm": 125.48241424560547,
      "learning_rate": 3.035230352303523e-06,
      "loss": 1.7341,
      "step": 18774
    },
    {
      "epoch": 7.268679829655439,
      "grad_norm": 77.48128509521484,
      "learning_rate": 3.0348001892717344e-06,
      "loss": 0.3296,
      "step": 18775
    },
    {
      "epoch": 7.26906697638405,
      "grad_norm": 51.636287689208984,
      "learning_rate": 3.034370026239945e-06,
      "loss": 0.6278,
      "step": 18776
    },
    {
      "epoch": 7.269454123112659,
      "grad_norm": 115.63882446289062,
      "learning_rate": 3.0339398632081564e-06,
      "loss": 1.7781,
      "step": 18777
    },
    {
      "epoch": 7.26984126984127,
      "grad_norm": 114.36109161376953,
      "learning_rate": 3.033509700176367e-06,
      "loss": 1.1142,
      "step": 18778
    },
    {
      "epoch": 7.27022841656988,
      "grad_norm": 52.597557067871094,
      "learning_rate": 3.033079537144578e-06,
      "loss": 1.6037,
      "step": 18779
    },
    {
      "epoch": 7.27061556329849,
      "grad_norm": 1.071786880493164,
      "learning_rate": 3.032649374112789e-06,
      "loss": 0.0272,
      "step": 18780
    },
    {
      "epoch": 7.2710027100271,
      "grad_norm": 75.04425048828125,
      "learning_rate": 3.032219211081e-06,
      "loss": 0.3504,
      "step": 18781
    },
    {
      "epoch": 7.27138985675571,
      "grad_norm": 69.33208465576172,
      "learning_rate": 3.0317890480492105e-06,
      "loss": 1.5555,
      "step": 18782
    },
    {
      "epoch": 7.2717770034843205,
      "grad_norm": 65.02545928955078,
      "learning_rate": 3.031358885017422e-06,
      "loss": 2.1466,
      "step": 18783
    },
    {
      "epoch": 7.272164150212931,
      "grad_norm": 51.65129089355469,
      "learning_rate": 3.0309287219856325e-06,
      "loss": 1.1352,
      "step": 18784
    },
    {
      "epoch": 7.272551296941541,
      "grad_norm": 97.48763275146484,
      "learning_rate": 3.030498558953844e-06,
      "loss": 3.4588,
      "step": 18785
    },
    {
      "epoch": 7.272938443670151,
      "grad_norm": 140.0753631591797,
      "learning_rate": 3.0300683959220545e-06,
      "loss": 1.7809,
      "step": 18786
    },
    {
      "epoch": 7.273325590398761,
      "grad_norm": 2.7022509574890137,
      "learning_rate": 3.029638232890266e-06,
      "loss": 0.1461,
      "step": 18787
    },
    {
      "epoch": 7.273712737127371,
      "grad_norm": 52.612342834472656,
      "learning_rate": 3.0292080698584765e-06,
      "loss": 2.1368,
      "step": 18788
    },
    {
      "epoch": 7.274099883855982,
      "grad_norm": 95.71334838867188,
      "learning_rate": 3.0287779068266874e-06,
      "loss": 0.8114,
      "step": 18789
    },
    {
      "epoch": 7.2744870305845915,
      "grad_norm": 96.07072448730469,
      "learning_rate": 3.0283477437948984e-06,
      "loss": 2.8016,
      "step": 18790
    },
    {
      "epoch": 7.274874177313202,
      "grad_norm": 13.652565956115723,
      "learning_rate": 3.0279175807631094e-06,
      "loss": 0.1291,
      "step": 18791
    },
    {
      "epoch": 7.275261324041812,
      "grad_norm": 53.33681869506836,
      "learning_rate": 3.02748741773132e-06,
      "loss": 2.3336,
      "step": 18792
    },
    {
      "epoch": 7.275648470770422,
      "grad_norm": 41.04460525512695,
      "learning_rate": 3.0270572546995314e-06,
      "loss": 0.4356,
      "step": 18793
    },
    {
      "epoch": 7.276035617499032,
      "grad_norm": 62.34609603881836,
      "learning_rate": 3.026627091667742e-06,
      "loss": 0.3738,
      "step": 18794
    },
    {
      "epoch": 7.276422764227642,
      "grad_norm": 198.2293243408203,
      "learning_rate": 3.0261969286359534e-06,
      "loss": 0.432,
      "step": 18795
    },
    {
      "epoch": 7.276809910956253,
      "grad_norm": 68.74835205078125,
      "learning_rate": 3.0257667656041644e-06,
      "loss": 2.6086,
      "step": 18796
    },
    {
      "epoch": 7.277197057684862,
      "grad_norm": 140.0128173828125,
      "learning_rate": 3.025336602572375e-06,
      "loss": 1.8932,
      "step": 18797
    },
    {
      "epoch": 7.277584204413473,
      "grad_norm": 2.6954634189605713,
      "learning_rate": 3.0249064395405864e-06,
      "loss": 0.0409,
      "step": 18798
    },
    {
      "epoch": 7.2779713511420825,
      "grad_norm": 33.897117614746094,
      "learning_rate": 3.024476276508797e-06,
      "loss": 1.8247,
      "step": 18799
    },
    {
      "epoch": 7.278358497870693,
      "grad_norm": 112.21498107910156,
      "learning_rate": 3.0240461134770084e-06,
      "loss": 2.1378,
      "step": 18800
    },
    {
      "epoch": 7.2787456445993035,
      "grad_norm": 28.493202209472656,
      "learning_rate": 3.023615950445219e-06,
      "loss": 1.7963,
      "step": 18801
    },
    {
      "epoch": 7.279132791327913,
      "grad_norm": 97.58615112304688,
      "learning_rate": 3.0231857874134303e-06,
      "loss": 3.4592,
      "step": 18802
    },
    {
      "epoch": 7.279519938056524,
      "grad_norm": 50.62348937988281,
      "learning_rate": 3.022755624381641e-06,
      "loss": 0.9788,
      "step": 18803
    },
    {
      "epoch": 7.279907084785133,
      "grad_norm": 18.958669662475586,
      "learning_rate": 3.022325461349852e-06,
      "loss": 2.0137,
      "step": 18804
    },
    {
      "epoch": 7.280294231513744,
      "grad_norm": 1.2995842695236206,
      "learning_rate": 3.021895298318063e-06,
      "loss": 0.0353,
      "step": 18805
    },
    {
      "epoch": 7.280681378242354,
      "grad_norm": 33.6901969909668,
      "learning_rate": 3.021465135286274e-06,
      "loss": 1.0794,
      "step": 18806
    },
    {
      "epoch": 7.281068524970964,
      "grad_norm": 207.18243408203125,
      "learning_rate": 3.0210349722544845e-06,
      "loss": 1.6537,
      "step": 18807
    },
    {
      "epoch": 7.281455671699574,
      "grad_norm": 147.7963409423828,
      "learning_rate": 3.020604809222696e-06,
      "loss": 3.9747,
      "step": 18808
    },
    {
      "epoch": 7.281842818428184,
      "grad_norm": 65.6066665649414,
      "learning_rate": 3.0201746461909064e-06,
      "loss": 0.7733,
      "step": 18809
    },
    {
      "epoch": 7.2822299651567945,
      "grad_norm": 23.28890609741211,
      "learning_rate": 3.019744483159118e-06,
      "loss": 0.2062,
      "step": 18810
    },
    {
      "epoch": 7.282617111885404,
      "grad_norm": 27.996795654296875,
      "learning_rate": 3.0193143201273284e-06,
      "loss": 2.2772,
      "step": 18811
    },
    {
      "epoch": 7.283004258614015,
      "grad_norm": 35.5942268371582,
      "learning_rate": 3.0188841570955394e-06,
      "loss": 0.8832,
      "step": 18812
    },
    {
      "epoch": 7.283391405342625,
      "grad_norm": 3.926715850830078,
      "learning_rate": 3.0184539940637504e-06,
      "loss": 0.1054,
      "step": 18813
    },
    {
      "epoch": 7.283778552071235,
      "grad_norm": 3.182588815689087,
      "learning_rate": 3.0180238310319614e-06,
      "loss": 0.105,
      "step": 18814
    },
    {
      "epoch": 7.284165698799845,
      "grad_norm": 101.21937561035156,
      "learning_rate": 3.017593668000172e-06,
      "loss": 3.2399,
      "step": 18815
    },
    {
      "epoch": 7.284552845528455,
      "grad_norm": 4.817190647125244,
      "learning_rate": 3.0171635049683834e-06,
      "loss": 0.0822,
      "step": 18816
    },
    {
      "epoch": 7.2849399922570655,
      "grad_norm": 56.4107551574707,
      "learning_rate": 3.016733341936594e-06,
      "loss": 0.2942,
      "step": 18817
    },
    {
      "epoch": 7.285327138985676,
      "grad_norm": 68.01222229003906,
      "learning_rate": 3.0163031789048054e-06,
      "loss": 1.0794,
      "step": 18818
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 1.5134117603302002,
      "learning_rate": 3.015873015873016e-06,
      "loss": 0.0431,
      "step": 18819
    },
    {
      "epoch": 7.286101432442896,
      "grad_norm": 50.132747650146484,
      "learning_rate": 3.0154428528412274e-06,
      "loss": 1.2886,
      "step": 18820
    },
    {
      "epoch": 7.286488579171506,
      "grad_norm": 77.7016372680664,
      "learning_rate": 3.015012689809438e-06,
      "loss": 1.7442,
      "step": 18821
    },
    {
      "epoch": 7.286875725900116,
      "grad_norm": 2.38370418548584,
      "learning_rate": 3.014582526777649e-06,
      "loss": 0.0711,
      "step": 18822
    },
    {
      "epoch": 7.287262872628727,
      "grad_norm": 1.9481042623519897,
      "learning_rate": 3.01415236374586e-06,
      "loss": 0.0672,
      "step": 18823
    },
    {
      "epoch": 7.287650019357336,
      "grad_norm": 97.4308853149414,
      "learning_rate": 3.013722200714071e-06,
      "loss": 2.6551,
      "step": 18824
    },
    {
      "epoch": 7.288037166085947,
      "grad_norm": 63.516326904296875,
      "learning_rate": 3.0132920376822815e-06,
      "loss": 1.5861,
      "step": 18825
    },
    {
      "epoch": 7.2884243128145565,
      "grad_norm": 66.3047103881836,
      "learning_rate": 3.012861874650493e-06,
      "loss": 1.0971,
      "step": 18826
    },
    {
      "epoch": 7.288811459543167,
      "grad_norm": 2.5504024028778076,
      "learning_rate": 3.0124317116187035e-06,
      "loss": 0.0943,
      "step": 18827
    },
    {
      "epoch": 7.289198606271777,
      "grad_norm": 11.003480911254883,
      "learning_rate": 3.012001548586915e-06,
      "loss": 0.1948,
      "step": 18828
    },
    {
      "epoch": 7.289585753000387,
      "grad_norm": 111.73204803466797,
      "learning_rate": 3.0115713855551254e-06,
      "loss": 1.3404,
      "step": 18829
    },
    {
      "epoch": 7.289972899728998,
      "grad_norm": 94.34674072265625,
      "learning_rate": 3.0111412225233364e-06,
      "loss": 0.3638,
      "step": 18830
    },
    {
      "epoch": 7.290360046457607,
      "grad_norm": 60.22366714477539,
      "learning_rate": 3.0107110594915474e-06,
      "loss": 1.7771,
      "step": 18831
    },
    {
      "epoch": 7.290747193186218,
      "grad_norm": 50.502593994140625,
      "learning_rate": 3.0102808964597584e-06,
      "loss": 1.6493,
      "step": 18832
    },
    {
      "epoch": 7.291134339914827,
      "grad_norm": 11.209396362304688,
      "learning_rate": 3.009850733427969e-06,
      "loss": 0.2188,
      "step": 18833
    },
    {
      "epoch": 7.291521486643438,
      "grad_norm": 2.2834632396698,
      "learning_rate": 3.0094205703961804e-06,
      "loss": 0.0506,
      "step": 18834
    },
    {
      "epoch": 7.291908633372048,
      "grad_norm": 213.4507598876953,
      "learning_rate": 3.008990407364391e-06,
      "loss": 3.9031,
      "step": 18835
    },
    {
      "epoch": 7.292295780100658,
      "grad_norm": 80.55379486083984,
      "learning_rate": 3.0085602443326024e-06,
      "loss": 0.9315,
      "step": 18836
    },
    {
      "epoch": 7.2926829268292686,
      "grad_norm": 68.36260986328125,
      "learning_rate": 3.0081300813008134e-06,
      "loss": 2.0294,
      "step": 18837
    },
    {
      "epoch": 7.293070073557878,
      "grad_norm": 31.388975143432617,
      "learning_rate": 3.0076999182690244e-06,
      "loss": 1.757,
      "step": 18838
    },
    {
      "epoch": 7.293457220286489,
      "grad_norm": 24.78888702392578,
      "learning_rate": 3.0072697552372354e-06,
      "loss": 0.4801,
      "step": 18839
    },
    {
      "epoch": 7.293844367015098,
      "grad_norm": 4.578821182250977,
      "learning_rate": 3.006839592205446e-06,
      "loss": 0.1151,
      "step": 18840
    },
    {
      "epoch": 7.294231513743709,
      "grad_norm": 3.7635557651519775,
      "learning_rate": 3.0064094291736574e-06,
      "loss": 0.1107,
      "step": 18841
    },
    {
      "epoch": 7.294618660472319,
      "grad_norm": 1.3591762781143188,
      "learning_rate": 3.005979266141868e-06,
      "loss": 0.049,
      "step": 18842
    },
    {
      "epoch": 7.295005807200929,
      "grad_norm": 5.775606155395508,
      "learning_rate": 3.0055491031100793e-06,
      "loss": 0.1446,
      "step": 18843
    },
    {
      "epoch": 7.2953929539295395,
      "grad_norm": 42.86717987060547,
      "learning_rate": 3.00511894007829e-06,
      "loss": 1.7656,
      "step": 18844
    },
    {
      "epoch": 7.295780100658149,
      "grad_norm": 64.52886199951172,
      "learning_rate": 3.004688777046501e-06,
      "loss": 0.3226,
      "step": 18845
    },
    {
      "epoch": 7.29616724738676,
      "grad_norm": 81.205322265625,
      "learning_rate": 3.004258614014712e-06,
      "loss": 1.0134,
      "step": 18846
    },
    {
      "epoch": 7.29655439411537,
      "grad_norm": 34.341304779052734,
      "learning_rate": 3.003828450982923e-06,
      "loss": 0.1668,
      "step": 18847
    },
    {
      "epoch": 7.29694154084398,
      "grad_norm": 92.63440704345703,
      "learning_rate": 3.0033982879511334e-06,
      "loss": 1.2439,
      "step": 18848
    },
    {
      "epoch": 7.29732868757259,
      "grad_norm": 37.706783294677734,
      "learning_rate": 3.002968124919345e-06,
      "loss": 1.4116,
      "step": 18849
    },
    {
      "epoch": 7.2977158343012,
      "grad_norm": 2.839961051940918,
      "learning_rate": 3.0025379618875554e-06,
      "loss": 0.0627,
      "step": 18850
    },
    {
      "epoch": 7.29810298102981,
      "grad_norm": 87.82919311523438,
      "learning_rate": 3.002107798855767e-06,
      "loss": 0.3783,
      "step": 18851
    },
    {
      "epoch": 7.29849012775842,
      "grad_norm": 123.17192077636719,
      "learning_rate": 3.0016776358239774e-06,
      "loss": 1.8513,
      "step": 18852
    },
    {
      "epoch": 7.2988772744870305,
      "grad_norm": 2.062822103500366,
      "learning_rate": 3.001247472792189e-06,
      "loss": 0.0598,
      "step": 18853
    },
    {
      "epoch": 7.299264421215641,
      "grad_norm": 6.628676414489746,
      "learning_rate": 3.0008173097603994e-06,
      "loss": 0.0945,
      "step": 18854
    },
    {
      "epoch": 7.299651567944251,
      "grad_norm": 4.188266754150391,
      "learning_rate": 3.0003871467286104e-06,
      "loss": 0.0412,
      "step": 18855
    },
    {
      "epoch": 7.300038714672861,
      "grad_norm": 91.67032623291016,
      "learning_rate": 2.9999569836968214e-06,
      "loss": 1.1753,
      "step": 18856
    },
    {
      "epoch": 7.300425861401471,
      "grad_norm": 81.17823791503906,
      "learning_rate": 2.9995268206650324e-06,
      "loss": 1.1353,
      "step": 18857
    },
    {
      "epoch": 7.300813008130081,
      "grad_norm": 2.1389830112457275,
      "learning_rate": 2.999096657633243e-06,
      "loss": 0.0918,
      "step": 18858
    },
    {
      "epoch": 7.301200154858692,
      "grad_norm": 33.20882797241211,
      "learning_rate": 2.9986664946014544e-06,
      "loss": 1.1982,
      "step": 18859
    },
    {
      "epoch": 7.301587301587301,
      "grad_norm": 2.7500271797180176,
      "learning_rate": 2.998236331569665e-06,
      "loss": 0.1562,
      "step": 18860
    },
    {
      "epoch": 7.301974448315912,
      "grad_norm": 60.8956184387207,
      "learning_rate": 2.9978061685378763e-06,
      "loss": 0.6593,
      "step": 18861
    },
    {
      "epoch": 7.3023615950445215,
      "grad_norm": 57.75001525878906,
      "learning_rate": 2.997376005506087e-06,
      "loss": 0.8266,
      "step": 18862
    },
    {
      "epoch": 7.302748741773132,
      "grad_norm": 151.7928009033203,
      "learning_rate": 2.996945842474298e-06,
      "loss": 0.6744,
      "step": 18863
    },
    {
      "epoch": 7.303135888501743,
      "grad_norm": 124.18112182617188,
      "learning_rate": 2.996515679442509e-06,
      "loss": 1.2624,
      "step": 18864
    },
    {
      "epoch": 7.303523035230352,
      "grad_norm": 4.113512992858887,
      "learning_rate": 2.99608551641072e-06,
      "loss": 0.1448,
      "step": 18865
    },
    {
      "epoch": 7.303910181958963,
      "grad_norm": 2.5943260192871094,
      "learning_rate": 2.9956553533789305e-06,
      "loss": 0.0659,
      "step": 18866
    },
    {
      "epoch": 7.304297328687572,
      "grad_norm": 1.1315315961837769,
      "learning_rate": 2.995225190347142e-06,
      "loss": 0.0302,
      "step": 18867
    },
    {
      "epoch": 7.304684475416183,
      "grad_norm": 41.55538558959961,
      "learning_rate": 2.9947950273153524e-06,
      "loss": 1.6916,
      "step": 18868
    },
    {
      "epoch": 7.305071622144792,
      "grad_norm": 185.0963897705078,
      "learning_rate": 2.994364864283564e-06,
      "loss": 1.1873,
      "step": 18869
    },
    {
      "epoch": 7.305458768873403,
      "grad_norm": 7.669679641723633,
      "learning_rate": 2.9939347012517744e-06,
      "loss": 0.1239,
      "step": 18870
    },
    {
      "epoch": 7.3058459156020135,
      "grad_norm": 7.158574104309082,
      "learning_rate": 2.993504538219986e-06,
      "loss": 0.0281,
      "step": 18871
    },
    {
      "epoch": 7.306233062330623,
      "grad_norm": 7.922400951385498,
      "learning_rate": 2.9930743751881964e-06,
      "loss": 0.0763,
      "step": 18872
    },
    {
      "epoch": 7.306620209059234,
      "grad_norm": 128.9886932373047,
      "learning_rate": 2.9926442121564074e-06,
      "loss": 3.5268,
      "step": 18873
    },
    {
      "epoch": 7.307007355787843,
      "grad_norm": 6.925832271575928,
      "learning_rate": 2.9922140491246184e-06,
      "loss": 0.1721,
      "step": 18874
    },
    {
      "epoch": 7.307394502516454,
      "grad_norm": 1.6826163530349731,
      "learning_rate": 2.9917838860928294e-06,
      "loss": 0.0596,
      "step": 18875
    },
    {
      "epoch": 7.307781649245064,
      "grad_norm": 35.8687744140625,
      "learning_rate": 2.99135372306104e-06,
      "loss": 1.3002,
      "step": 18876
    },
    {
      "epoch": 7.308168795973674,
      "grad_norm": 3.726473093032837,
      "learning_rate": 2.9909235600292514e-06,
      "loss": 0.0734,
      "step": 18877
    },
    {
      "epoch": 7.308555942702284,
      "grad_norm": 33.59736633300781,
      "learning_rate": 2.9904933969974624e-06,
      "loss": 0.1515,
      "step": 18878
    },
    {
      "epoch": 7.308943089430894,
      "grad_norm": 137.80950927734375,
      "learning_rate": 2.9900632339656734e-06,
      "loss": 0.8436,
      "step": 18879
    },
    {
      "epoch": 7.3093302361595045,
      "grad_norm": 4.729699611663818,
      "learning_rate": 2.9896330709338844e-06,
      "loss": 0.1854,
      "step": 18880
    },
    {
      "epoch": 7.309717382888115,
      "grad_norm": 1.7545223236083984,
      "learning_rate": 2.989202907902095e-06,
      "loss": 0.057,
      "step": 18881
    },
    {
      "epoch": 7.310104529616725,
      "grad_norm": 93.125732421875,
      "learning_rate": 2.9887727448703063e-06,
      "loss": 0.8253,
      "step": 18882
    },
    {
      "epoch": 7.310491676345335,
      "grad_norm": 1.1515357494354248,
      "learning_rate": 2.988342581838517e-06,
      "loss": 0.0568,
      "step": 18883
    },
    {
      "epoch": 7.310878823073945,
      "grad_norm": 108.93817901611328,
      "learning_rate": 2.9879124188067283e-06,
      "loss": 1.7398,
      "step": 18884
    },
    {
      "epoch": 7.311265969802555,
      "grad_norm": 0.8863805532455444,
      "learning_rate": 2.987482255774939e-06,
      "loss": 0.0244,
      "step": 18885
    },
    {
      "epoch": 7.311653116531165,
      "grad_norm": 23.634830474853516,
      "learning_rate": 2.9870520927431503e-06,
      "loss": 1.7951,
      "step": 18886
    },
    {
      "epoch": 7.312040263259775,
      "grad_norm": 94.56265258789062,
      "learning_rate": 2.986621929711361e-06,
      "loss": 2.4909,
      "step": 18887
    },
    {
      "epoch": 7.312427409988386,
      "grad_norm": 13.102787971496582,
      "learning_rate": 2.986191766679572e-06,
      "loss": 0.327,
      "step": 18888
    },
    {
      "epoch": 7.3128145567169955,
      "grad_norm": 100.29752349853516,
      "learning_rate": 2.985761603647783e-06,
      "loss": 2.252,
      "step": 18889
    },
    {
      "epoch": 7.313201703445606,
      "grad_norm": 100.42264556884766,
      "learning_rate": 2.985331440615994e-06,
      "loss": 1.6275,
      "step": 18890
    },
    {
      "epoch": 7.313588850174216,
      "grad_norm": 30.4950008392334,
      "learning_rate": 2.9849012775842044e-06,
      "loss": 2.1919,
      "step": 18891
    },
    {
      "epoch": 7.313975996902826,
      "grad_norm": 5.489679336547852,
      "learning_rate": 2.984471114552416e-06,
      "loss": 0.1837,
      "step": 18892
    },
    {
      "epoch": 7.314363143631437,
      "grad_norm": 56.28890609741211,
      "learning_rate": 2.9840409515206264e-06,
      "loss": 0.7015,
      "step": 18893
    },
    {
      "epoch": 7.314750290360046,
      "grad_norm": 43.428466796875,
      "learning_rate": 2.983610788488838e-06,
      "loss": 0.7376,
      "step": 18894
    },
    {
      "epoch": 7.315137437088657,
      "grad_norm": 49.60183334350586,
      "learning_rate": 2.9831806254570484e-06,
      "loss": 0.2534,
      "step": 18895
    },
    {
      "epoch": 7.3155245838172664,
      "grad_norm": 1.8822064399719238,
      "learning_rate": 2.9827504624252594e-06,
      "loss": 0.0561,
      "step": 18896
    },
    {
      "epoch": 7.315911730545877,
      "grad_norm": 66.61536407470703,
      "learning_rate": 2.9823202993934704e-06,
      "loss": 0.359,
      "step": 18897
    },
    {
      "epoch": 7.3162988772744875,
      "grad_norm": 89.14481353759766,
      "learning_rate": 2.9818901363616814e-06,
      "loss": 0.5824,
      "step": 18898
    },
    {
      "epoch": 7.316686024003097,
      "grad_norm": 14.515838623046875,
      "learning_rate": 2.981459973329892e-06,
      "loss": 0.1018,
      "step": 18899
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 91.82471466064453,
      "learning_rate": 2.9810298102981034e-06,
      "loss": 1.1379,
      "step": 18900
    },
    {
      "epoch": 7.317460317460317,
      "grad_norm": 101.87476348876953,
      "learning_rate": 2.980599647266314e-06,
      "loss": 1.1518,
      "step": 18901
    },
    {
      "epoch": 7.317847464188928,
      "grad_norm": 5.488855361938477,
      "learning_rate": 2.9801694842345253e-06,
      "loss": 0.0735,
      "step": 18902
    },
    {
      "epoch": 7.318234610917537,
      "grad_norm": 5.521287441253662,
      "learning_rate": 2.979739321202736e-06,
      "loss": 0.1263,
      "step": 18903
    },
    {
      "epoch": 7.318621757646148,
      "grad_norm": 1.2776644229888916,
      "learning_rate": 2.9793091581709473e-06,
      "loss": 0.0371,
      "step": 18904
    },
    {
      "epoch": 7.319008904374758,
      "grad_norm": 30.2502384185791,
      "learning_rate": 2.978878995139158e-06,
      "loss": 1.1851,
      "step": 18905
    },
    {
      "epoch": 7.319396051103368,
      "grad_norm": 58.22219467163086,
      "learning_rate": 2.978448832107369e-06,
      "loss": 1.3018,
      "step": 18906
    },
    {
      "epoch": 7.3197831978319785,
      "grad_norm": 11.216084480285645,
      "learning_rate": 2.97801866907558e-06,
      "loss": 0.2075,
      "step": 18907
    },
    {
      "epoch": 7.320170344560588,
      "grad_norm": 44.787025451660156,
      "learning_rate": 2.977588506043791e-06,
      "loss": 3.2712,
      "step": 18908
    },
    {
      "epoch": 7.320557491289199,
      "grad_norm": 158.03001403808594,
      "learning_rate": 2.9771583430120014e-06,
      "loss": 0.8062,
      "step": 18909
    },
    {
      "epoch": 7.320944638017809,
      "grad_norm": 6.157355785369873,
      "learning_rate": 2.976728179980213e-06,
      "loss": 0.2338,
      "step": 18910
    },
    {
      "epoch": 7.321331784746419,
      "grad_norm": 3.6944990158081055,
      "learning_rate": 2.9762980169484234e-06,
      "loss": 0.0933,
      "step": 18911
    },
    {
      "epoch": 7.321718931475029,
      "grad_norm": 17.86362648010254,
      "learning_rate": 2.975867853916635e-06,
      "loss": 1.3575,
      "step": 18912
    },
    {
      "epoch": 7.322106078203639,
      "grad_norm": 94.0101547241211,
      "learning_rate": 2.9754376908848454e-06,
      "loss": 2.4514,
      "step": 18913
    },
    {
      "epoch": 7.322493224932249,
      "grad_norm": 67.58919525146484,
      "learning_rate": 2.9750075278530564e-06,
      "loss": 1.7932,
      "step": 18914
    },
    {
      "epoch": 7.32288037166086,
      "grad_norm": 186.93531799316406,
      "learning_rate": 2.9745773648212674e-06,
      "loss": 2.6351,
      "step": 18915
    },
    {
      "epoch": 7.3232675183894695,
      "grad_norm": 6.568293571472168,
      "learning_rate": 2.9741472017894784e-06,
      "loss": 0.1853,
      "step": 18916
    },
    {
      "epoch": 7.32365466511808,
      "grad_norm": 1.3328461647033691,
      "learning_rate": 2.973717038757689e-06,
      "loss": 0.0394,
      "step": 18917
    },
    {
      "epoch": 7.32404181184669,
      "grad_norm": 37.54383850097656,
      "learning_rate": 2.9732868757259004e-06,
      "loss": 0.1057,
      "step": 18918
    },
    {
      "epoch": 7.3244289585753,
      "grad_norm": 47.887149810791016,
      "learning_rate": 2.9728567126941118e-06,
      "loss": 1.6729,
      "step": 18919
    },
    {
      "epoch": 7.32481610530391,
      "grad_norm": 54.08644104003906,
      "learning_rate": 2.9724265496623223e-06,
      "loss": 0.202,
      "step": 18920
    },
    {
      "epoch": 7.32520325203252,
      "grad_norm": 7.797720432281494,
      "learning_rate": 2.9719963866305333e-06,
      "loss": 0.1519,
      "step": 18921
    },
    {
      "epoch": 7.325590398761131,
      "grad_norm": 36.21728515625,
      "learning_rate": 2.9715662235987443e-06,
      "loss": 1.5661,
      "step": 18922
    },
    {
      "epoch": 7.3259775454897405,
      "grad_norm": 25.4754695892334,
      "learning_rate": 2.9711360605669553e-06,
      "loss": 0.2247,
      "step": 18923
    },
    {
      "epoch": 7.326364692218351,
      "grad_norm": 95.14498901367188,
      "learning_rate": 2.970705897535166e-06,
      "loss": 1.2496,
      "step": 18924
    },
    {
      "epoch": 7.326751838946961,
      "grad_norm": 3.1057567596435547,
      "learning_rate": 2.9702757345033773e-06,
      "loss": 0.0615,
      "step": 18925
    },
    {
      "epoch": 7.327138985675571,
      "grad_norm": 309.9792785644531,
      "learning_rate": 2.969845571471588e-06,
      "loss": 1.5276,
      "step": 18926
    },
    {
      "epoch": 7.327526132404181,
      "grad_norm": 26.89529800415039,
      "learning_rate": 2.9694154084397993e-06,
      "loss": 0.3198,
      "step": 18927
    },
    {
      "epoch": 7.327913279132791,
      "grad_norm": 126.30497741699219,
      "learning_rate": 2.96898524540801e-06,
      "loss": 1.0964,
      "step": 18928
    },
    {
      "epoch": 7.328300425861402,
      "grad_norm": 227.82608032226562,
      "learning_rate": 2.968555082376221e-06,
      "loss": 1.1842,
      "step": 18929
    },
    {
      "epoch": 7.328687572590011,
      "grad_norm": 11.330903053283691,
      "learning_rate": 2.968124919344432e-06,
      "loss": 0.0789,
      "step": 18930
    },
    {
      "epoch": 7.329074719318622,
      "grad_norm": 9.522804260253906,
      "learning_rate": 2.967694756312643e-06,
      "loss": 0.2222,
      "step": 18931
    },
    {
      "epoch": 7.3294618660472315,
      "grad_norm": 53.28836441040039,
      "learning_rate": 2.9672645932808534e-06,
      "loss": 2.0996,
      "step": 18932
    },
    {
      "epoch": 7.329849012775842,
      "grad_norm": 116.18695831298828,
      "learning_rate": 2.966834430249065e-06,
      "loss": 1.2277,
      "step": 18933
    },
    {
      "epoch": 7.3302361595044525,
      "grad_norm": 62.245670318603516,
      "learning_rate": 2.9664042672172754e-06,
      "loss": 1.8541,
      "step": 18934
    },
    {
      "epoch": 7.330623306233062,
      "grad_norm": 20.97759246826172,
      "learning_rate": 2.965974104185487e-06,
      "loss": 1.859,
      "step": 18935
    },
    {
      "epoch": 7.331010452961673,
      "grad_norm": 9.713737487792969,
      "learning_rate": 2.9655439411536974e-06,
      "loss": 0.1148,
      "step": 18936
    },
    {
      "epoch": 7.331397599690282,
      "grad_norm": 41.05358123779297,
      "learning_rate": 2.965113778121909e-06,
      "loss": 0.181,
      "step": 18937
    },
    {
      "epoch": 7.331784746418893,
      "grad_norm": 234.55052185058594,
      "learning_rate": 2.9646836150901194e-06,
      "loss": 2.8581,
      "step": 18938
    },
    {
      "epoch": 7.332171893147503,
      "grad_norm": 168.4113006591797,
      "learning_rate": 2.9642534520583304e-06,
      "loss": 0.5389,
      "step": 18939
    },
    {
      "epoch": 7.332559039876113,
      "grad_norm": 99.0336685180664,
      "learning_rate": 2.9638232890265413e-06,
      "loss": 2.0147,
      "step": 18940
    },
    {
      "epoch": 7.332946186604723,
      "grad_norm": 11.954330444335938,
      "learning_rate": 2.9633931259947523e-06,
      "loss": 0.1077,
      "step": 18941
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 57.35539245605469,
      "learning_rate": 2.962962962962963e-06,
      "loss": 0.9053,
      "step": 18942
    },
    {
      "epoch": 7.3337204800619435,
      "grad_norm": 21.42599868774414,
      "learning_rate": 2.9625327999311743e-06,
      "loss": 0.3384,
      "step": 18943
    },
    {
      "epoch": 7.334107626790553,
      "grad_norm": 42.47661590576172,
      "learning_rate": 2.962102636899385e-06,
      "loss": 2.0272,
      "step": 18944
    },
    {
      "epoch": 7.334494773519164,
      "grad_norm": 40.218448638916016,
      "learning_rate": 2.9616724738675963e-06,
      "loss": 0.3026,
      "step": 18945
    },
    {
      "epoch": 7.334881920247774,
      "grad_norm": 4.148898601531982,
      "learning_rate": 2.961242310835807e-06,
      "loss": 0.1533,
      "step": 18946
    },
    {
      "epoch": 7.335269066976384,
      "grad_norm": 2.5428626537323,
      "learning_rate": 2.960812147804018e-06,
      "loss": 0.1143,
      "step": 18947
    },
    {
      "epoch": 7.335656213704994,
      "grad_norm": 71.69768524169922,
      "learning_rate": 2.960381984772229e-06,
      "loss": 0.3458,
      "step": 18948
    },
    {
      "epoch": 7.336043360433604,
      "grad_norm": 279.9220886230469,
      "learning_rate": 2.95995182174044e-06,
      "loss": 2.8096,
      "step": 18949
    },
    {
      "epoch": 7.3364305071622145,
      "grad_norm": 12.108246803283691,
      "learning_rate": 2.9595216587086504e-06,
      "loss": 0.0865,
      "step": 18950
    },
    {
      "epoch": 7.336817653890825,
      "grad_norm": 8.867512702941895,
      "learning_rate": 2.959091495676862e-06,
      "loss": 0.0901,
      "step": 18951
    },
    {
      "epoch": 7.337204800619435,
      "grad_norm": 341.078857421875,
      "learning_rate": 2.9586613326450724e-06,
      "loss": 1.0831,
      "step": 18952
    },
    {
      "epoch": 7.337591947348045,
      "grad_norm": 175.59214782714844,
      "learning_rate": 2.958231169613284e-06,
      "loss": 0.5731,
      "step": 18953
    },
    {
      "epoch": 7.337979094076655,
      "grad_norm": 0.637454092502594,
      "learning_rate": 2.9578010065814944e-06,
      "loss": 0.0221,
      "step": 18954
    },
    {
      "epoch": 7.338366240805265,
      "grad_norm": 2.322949171066284,
      "learning_rate": 2.957370843549706e-06,
      "loss": 0.1177,
      "step": 18955
    },
    {
      "epoch": 7.338753387533876,
      "grad_norm": 190.14028930664062,
      "learning_rate": 2.9569406805179164e-06,
      "loss": 1.9433,
      "step": 18956
    },
    {
      "epoch": 7.339140534262485,
      "grad_norm": 109.77774810791016,
      "learning_rate": 2.9565105174861274e-06,
      "loss": 0.7232,
      "step": 18957
    },
    {
      "epoch": 7.339527680991096,
      "grad_norm": 39.24607849121094,
      "learning_rate": 2.9560803544543384e-06,
      "loss": 2.1565,
      "step": 18958
    },
    {
      "epoch": 7.3399148277197055,
      "grad_norm": 73.3327407836914,
      "learning_rate": 2.9556501914225494e-06,
      "loss": 0.9283,
      "step": 18959
    },
    {
      "epoch": 7.340301974448316,
      "grad_norm": 1.9107986688613892,
      "learning_rate": 2.9552200283907608e-06,
      "loss": 0.0662,
      "step": 18960
    },
    {
      "epoch": 7.340689121176926,
      "grad_norm": 62.10010528564453,
      "learning_rate": 2.9547898653589713e-06,
      "loss": 1.9738,
      "step": 18961
    },
    {
      "epoch": 7.341076267905536,
      "grad_norm": 1.9211252927780151,
      "learning_rate": 2.9543597023271823e-06,
      "loss": 0.0743,
      "step": 18962
    },
    {
      "epoch": 7.341463414634147,
      "grad_norm": 17.336076736450195,
      "learning_rate": 2.9539295392953933e-06,
      "loss": 1.7506,
      "step": 18963
    },
    {
      "epoch": 7.341850561362756,
      "grad_norm": 54.13178253173828,
      "learning_rate": 2.9534993762636043e-06,
      "loss": 0.3177,
      "step": 18964
    },
    {
      "epoch": 7.342237708091367,
      "grad_norm": 7.82820463180542,
      "learning_rate": 2.953069213231815e-06,
      "loss": 0.1267,
      "step": 18965
    },
    {
      "epoch": 7.342624854819976,
      "grad_norm": 20.629594802856445,
      "learning_rate": 2.9526390502000263e-06,
      "loss": 0.3044,
      "step": 18966
    },
    {
      "epoch": 7.343012001548587,
      "grad_norm": 43.76325988769531,
      "learning_rate": 2.952208887168237e-06,
      "loss": 1.2361,
      "step": 18967
    },
    {
      "epoch": 7.343399148277197,
      "grad_norm": 57.159912109375,
      "learning_rate": 2.9517787241364483e-06,
      "loss": 0.2778,
      "step": 18968
    },
    {
      "epoch": 7.343786295005807,
      "grad_norm": 5.0557684898376465,
      "learning_rate": 2.951348561104659e-06,
      "loss": 0.2495,
      "step": 18969
    },
    {
      "epoch": 7.3441734417344176,
      "grad_norm": 114.65380859375,
      "learning_rate": 2.9509183980728703e-06,
      "loss": 1.1769,
      "step": 18970
    },
    {
      "epoch": 7.344560588463027,
      "grad_norm": 25.314395904541016,
      "learning_rate": 2.950488235041081e-06,
      "loss": 1.6203,
      "step": 18971
    },
    {
      "epoch": 7.344947735191638,
      "grad_norm": 23.182092666625977,
      "learning_rate": 2.950058072009292e-06,
      "loss": 0.1028,
      "step": 18972
    },
    {
      "epoch": 7.345334881920248,
      "grad_norm": 78.51146697998047,
      "learning_rate": 2.949627908977503e-06,
      "loss": 1.9764,
      "step": 18973
    },
    {
      "epoch": 7.345722028648858,
      "grad_norm": 122.82727813720703,
      "learning_rate": 2.949197745945714e-06,
      "loss": 1.4618,
      "step": 18974
    },
    {
      "epoch": 7.346109175377468,
      "grad_norm": 53.709659576416016,
      "learning_rate": 2.9487675829139244e-06,
      "loss": 1.5813,
      "step": 18975
    },
    {
      "epoch": 7.346496322106078,
      "grad_norm": 52.51701736450195,
      "learning_rate": 2.948337419882136e-06,
      "loss": 1.5632,
      "step": 18976
    },
    {
      "epoch": 7.3468834688346885,
      "grad_norm": 65.58502960205078,
      "learning_rate": 2.9479072568503464e-06,
      "loss": 2.4038,
      "step": 18977
    },
    {
      "epoch": 7.347270615563298,
      "grad_norm": 35.68693542480469,
      "learning_rate": 2.9474770938185578e-06,
      "loss": 2.9358,
      "step": 18978
    },
    {
      "epoch": 7.347657762291909,
      "grad_norm": 5.075628280639648,
      "learning_rate": 2.9470469307867683e-06,
      "loss": 0.1574,
      "step": 18979
    },
    {
      "epoch": 7.348044909020519,
      "grad_norm": 62.91834259033203,
      "learning_rate": 2.9466167677549793e-06,
      "loss": 0.368,
      "step": 18980
    },
    {
      "epoch": 7.348432055749129,
      "grad_norm": 83.098388671875,
      "learning_rate": 2.9461866047231903e-06,
      "loss": 3.6109,
      "step": 18981
    },
    {
      "epoch": 7.348819202477739,
      "grad_norm": 57.3802604675293,
      "learning_rate": 2.9457564416914013e-06,
      "loss": 0.5878,
      "step": 18982
    },
    {
      "epoch": 7.349206349206349,
      "grad_norm": 20.89815902709961,
      "learning_rate": 2.945326278659612e-06,
      "loss": 1.9882,
      "step": 18983
    },
    {
      "epoch": 7.349593495934959,
      "grad_norm": 47.02217483520508,
      "learning_rate": 2.9448961156278233e-06,
      "loss": 2.3009,
      "step": 18984
    },
    {
      "epoch": 7.34998064266357,
      "grad_norm": 363.890869140625,
      "learning_rate": 2.944465952596034e-06,
      "loss": 3.3939,
      "step": 18985
    },
    {
      "epoch": 7.3503677893921795,
      "grad_norm": 106.53126525878906,
      "learning_rate": 2.9440357895642453e-06,
      "loss": 0.6883,
      "step": 18986
    },
    {
      "epoch": 7.35075493612079,
      "grad_norm": 108.87577819824219,
      "learning_rate": 2.943605626532456e-06,
      "loss": 2.0435,
      "step": 18987
    },
    {
      "epoch": 7.3511420828494,
      "grad_norm": 120.89501953125,
      "learning_rate": 2.9431754635006673e-06,
      "loss": 0.9135,
      "step": 18988
    },
    {
      "epoch": 7.35152922957801,
      "grad_norm": 55.00006103515625,
      "learning_rate": 2.942745300468878e-06,
      "loss": 0.1949,
      "step": 18989
    },
    {
      "epoch": 7.351916376306621,
      "grad_norm": 52.2342529296875,
      "learning_rate": 2.942315137437089e-06,
      "loss": 0.6673,
      "step": 18990
    },
    {
      "epoch": 7.35230352303523,
      "grad_norm": 21.614078521728516,
      "learning_rate": 2.9418849744053e-06,
      "loss": 2.4464,
      "step": 18991
    },
    {
      "epoch": 7.352690669763841,
      "grad_norm": 19.722238540649414,
      "learning_rate": 2.941454811373511e-06,
      "loss": 0.0574,
      "step": 18992
    },
    {
      "epoch": 7.35307781649245,
      "grad_norm": 42.377323150634766,
      "learning_rate": 2.9410246483417214e-06,
      "loss": 1.1602,
      "step": 18993
    },
    {
      "epoch": 7.353464963221061,
      "grad_norm": 136.1551971435547,
      "learning_rate": 2.940594485309933e-06,
      "loss": 2.3052,
      "step": 18994
    },
    {
      "epoch": 7.3538521099496705,
      "grad_norm": 27.34714698791504,
      "learning_rate": 2.9401643222781434e-06,
      "loss": 0.7236,
      "step": 18995
    },
    {
      "epoch": 7.354239256678281,
      "grad_norm": 84.66553497314453,
      "learning_rate": 2.939734159246355e-06,
      "loss": 1.6027,
      "step": 18996
    },
    {
      "epoch": 7.3546264034068916,
      "grad_norm": 27.280210494995117,
      "learning_rate": 2.9393039962145654e-06,
      "loss": 1.4608,
      "step": 18997
    },
    {
      "epoch": 7.355013550135501,
      "grad_norm": 57.325626373291016,
      "learning_rate": 2.9388738331827764e-06,
      "loss": 1.6529,
      "step": 18998
    },
    {
      "epoch": 7.355400696864112,
      "grad_norm": 44.71725082397461,
      "learning_rate": 2.9384436701509873e-06,
      "loss": 1.8487,
      "step": 18999
    },
    {
      "epoch": 7.355787843592721,
      "grad_norm": 124.54094696044922,
      "learning_rate": 2.9380135071191983e-06,
      "loss": 0.669,
      "step": 19000
    },
    {
      "epoch": 7.356174990321332,
      "grad_norm": 1.3498538732528687,
      "learning_rate": 2.9375833440874098e-06,
      "loss": 0.0588,
      "step": 19001
    },
    {
      "epoch": 7.356562137049942,
      "grad_norm": 4.027067184448242,
      "learning_rate": 2.9371531810556203e-06,
      "loss": 0.0919,
      "step": 19002
    },
    {
      "epoch": 7.356949283778552,
      "grad_norm": 2.5285251140594482,
      "learning_rate": 2.9367230180238317e-06,
      "loss": 0.0753,
      "step": 19003
    },
    {
      "epoch": 7.3573364305071625,
      "grad_norm": 45.91748046875,
      "learning_rate": 2.9362928549920423e-06,
      "loss": 1.4285,
      "step": 19004
    },
    {
      "epoch": 7.357723577235772,
      "grad_norm": 34.34244155883789,
      "learning_rate": 2.9358626919602533e-06,
      "loss": 0.4924,
      "step": 19005
    },
    {
      "epoch": 7.358110723964383,
      "grad_norm": 81.95697021484375,
      "learning_rate": 2.9354325289284643e-06,
      "loss": 1.7846,
      "step": 19006
    },
    {
      "epoch": 7.358497870692993,
      "grad_norm": 108.85664367675781,
      "learning_rate": 2.9350023658966753e-06,
      "loss": 3.4709,
      "step": 19007
    },
    {
      "epoch": 7.358885017421603,
      "grad_norm": 96.94385528564453,
      "learning_rate": 2.934572202864886e-06,
      "loss": 1.9222,
      "step": 19008
    },
    {
      "epoch": 7.359272164150213,
      "grad_norm": 30.384363174438477,
      "learning_rate": 2.9341420398330973e-06,
      "loss": 0.3014,
      "step": 19009
    },
    {
      "epoch": 7.359659310878823,
      "grad_norm": 173.82684326171875,
      "learning_rate": 2.933711876801308e-06,
      "loss": 2.2029,
      "step": 19010
    },
    {
      "epoch": 7.360046457607433,
      "grad_norm": 27.578250885009766,
      "learning_rate": 2.9332817137695193e-06,
      "loss": 2.9698,
      "step": 19011
    },
    {
      "epoch": 7.360433604336043,
      "grad_norm": 44.199398040771484,
      "learning_rate": 2.93285155073773e-06,
      "loss": 1.9349,
      "step": 19012
    },
    {
      "epoch": 7.3608207510646535,
      "grad_norm": 6.8746137619018555,
      "learning_rate": 2.932421387705941e-06,
      "loss": 0.0964,
      "step": 19013
    },
    {
      "epoch": 7.361207897793264,
      "grad_norm": 73.958740234375,
      "learning_rate": 2.931991224674152e-06,
      "loss": 1.4793,
      "step": 19014
    },
    {
      "epoch": 7.361595044521874,
      "grad_norm": 68.53176879882812,
      "learning_rate": 2.931561061642363e-06,
      "loss": 1.0472,
      "step": 19015
    },
    {
      "epoch": 7.361982191250484,
      "grad_norm": 22.153493881225586,
      "learning_rate": 2.9311308986105734e-06,
      "loss": 0.1141,
      "step": 19016
    },
    {
      "epoch": 7.362369337979094,
      "grad_norm": 3.0017788410186768,
      "learning_rate": 2.9307007355787848e-06,
      "loss": 0.1102,
      "step": 19017
    },
    {
      "epoch": 7.362756484707704,
      "grad_norm": 93.35590362548828,
      "learning_rate": 2.9302705725469954e-06,
      "loss": 0.1784,
      "step": 19018
    },
    {
      "epoch": 7.363143631436314,
      "grad_norm": 286.969970703125,
      "learning_rate": 2.9298404095152068e-06,
      "loss": 1.4332,
      "step": 19019
    },
    {
      "epoch": 7.363530778164924,
      "grad_norm": 19.64332389831543,
      "learning_rate": 2.9294102464834173e-06,
      "loss": 0.3044,
      "step": 19020
    },
    {
      "epoch": 7.363917924893535,
      "grad_norm": 13.177739143371582,
      "learning_rate": 2.9289800834516288e-06,
      "loss": 0.0926,
      "step": 19021
    },
    {
      "epoch": 7.3643050716221445,
      "grad_norm": 80.08616638183594,
      "learning_rate": 2.9285499204198393e-06,
      "loss": 0.6721,
      "step": 19022
    },
    {
      "epoch": 7.364692218350755,
      "grad_norm": 261.5739440917969,
      "learning_rate": 2.9281197573880503e-06,
      "loss": 1.5401,
      "step": 19023
    },
    {
      "epoch": 7.365079365079365,
      "grad_norm": 29.00559425354004,
      "learning_rate": 2.9276895943562613e-06,
      "loss": 0.2265,
      "step": 19024
    },
    {
      "epoch": 7.365466511807975,
      "grad_norm": 1.4909756183624268,
      "learning_rate": 2.9272594313244723e-06,
      "loss": 0.0424,
      "step": 19025
    },
    {
      "epoch": 7.365853658536586,
      "grad_norm": 85.22650146484375,
      "learning_rate": 2.926829268292683e-06,
      "loss": 1.3726,
      "step": 19026
    },
    {
      "epoch": 7.366240805265195,
      "grad_norm": 50.10620880126953,
      "learning_rate": 2.9263991052608943e-06,
      "loss": 0.4534,
      "step": 19027
    },
    {
      "epoch": 7.366627951993806,
      "grad_norm": 150.97286987304688,
      "learning_rate": 2.925968942229105e-06,
      "loss": 1.335,
      "step": 19028
    },
    {
      "epoch": 7.3670150987224154,
      "grad_norm": 317.35040283203125,
      "learning_rate": 2.9255387791973163e-06,
      "loss": 2.965,
      "step": 19029
    },
    {
      "epoch": 7.367402245451026,
      "grad_norm": 4.9473652839660645,
      "learning_rate": 2.925108616165527e-06,
      "loss": 0.0635,
      "step": 19030
    },
    {
      "epoch": 7.3677893921796365,
      "grad_norm": 56.873443603515625,
      "learning_rate": 2.924678453133738e-06,
      "loss": 1.282,
      "step": 19031
    },
    {
      "epoch": 7.368176538908246,
      "grad_norm": 121.38824462890625,
      "learning_rate": 2.924248290101949e-06,
      "loss": 0.2798,
      "step": 19032
    },
    {
      "epoch": 7.368563685636857,
      "grad_norm": 1.2204878330230713,
      "learning_rate": 2.92381812707016e-06,
      "loss": 0.0467,
      "step": 19033
    },
    {
      "epoch": 7.368950832365466,
      "grad_norm": 10.625041961669922,
      "learning_rate": 2.9233879640383704e-06,
      "loss": 0.2656,
      "step": 19034
    },
    {
      "epoch": 7.369337979094077,
      "grad_norm": 54.650428771972656,
      "learning_rate": 2.922957801006582e-06,
      "loss": 2.6201,
      "step": 19035
    },
    {
      "epoch": 7.369725125822686,
      "grad_norm": 34.013553619384766,
      "learning_rate": 2.9225276379747924e-06,
      "loss": 2.1105,
      "step": 19036
    },
    {
      "epoch": 7.370112272551297,
      "grad_norm": 42.48681640625,
      "learning_rate": 2.9220974749430038e-06,
      "loss": 1.2318,
      "step": 19037
    },
    {
      "epoch": 7.370499419279907,
      "grad_norm": 5.5627312660217285,
      "learning_rate": 2.9216673119112143e-06,
      "loss": 0.2179,
      "step": 19038
    },
    {
      "epoch": 7.370886566008517,
      "grad_norm": 71.59306335449219,
      "learning_rate": 2.9212371488794258e-06,
      "loss": 0.4892,
      "step": 19039
    },
    {
      "epoch": 7.3712737127371275,
      "grad_norm": 39.1159782409668,
      "learning_rate": 2.9208069858476363e-06,
      "loss": 0.7127,
      "step": 19040
    },
    {
      "epoch": 7.371660859465737,
      "grad_norm": 1.6053203344345093,
      "learning_rate": 2.9203768228158473e-06,
      "loss": 0.0678,
      "step": 19041
    },
    {
      "epoch": 7.372048006194348,
      "grad_norm": 98.23846435546875,
      "learning_rate": 2.9199466597840587e-06,
      "loss": 0.3572,
      "step": 19042
    },
    {
      "epoch": 7.372435152922958,
      "grad_norm": 5.221762180328369,
      "learning_rate": 2.9195164967522693e-06,
      "loss": 0.1677,
      "step": 19043
    },
    {
      "epoch": 7.372822299651568,
      "grad_norm": 3.2765543460845947,
      "learning_rate": 2.9190863337204807e-06,
      "loss": 0.0567,
      "step": 19044
    },
    {
      "epoch": 7.373209446380178,
      "grad_norm": 109.35013580322266,
      "learning_rate": 2.9186561706886913e-06,
      "loss": 0.8642,
      "step": 19045
    },
    {
      "epoch": 7.373596593108788,
      "grad_norm": 2.358877182006836,
      "learning_rate": 2.9182260076569023e-06,
      "loss": 0.0777,
      "step": 19046
    },
    {
      "epoch": 7.373983739837398,
      "grad_norm": 26.392976760864258,
      "learning_rate": 2.9177958446251133e-06,
      "loss": 1.2668,
      "step": 19047
    },
    {
      "epoch": 7.374370886566009,
      "grad_norm": 214.62664794921875,
      "learning_rate": 2.9173656815933243e-06,
      "loss": 4.7735,
      "step": 19048
    },
    {
      "epoch": 7.3747580332946185,
      "grad_norm": 80.39009857177734,
      "learning_rate": 2.916935518561535e-06,
      "loss": 0.1966,
      "step": 19049
    },
    {
      "epoch": 7.375145180023229,
      "grad_norm": 83.26620483398438,
      "learning_rate": 2.9165053555297463e-06,
      "loss": 0.981,
      "step": 19050
    },
    {
      "epoch": 7.375532326751839,
      "grad_norm": 65.3046646118164,
      "learning_rate": 2.916075192497957e-06,
      "loss": 1.6547,
      "step": 19051
    },
    {
      "epoch": 7.375919473480449,
      "grad_norm": 7.464778423309326,
      "learning_rate": 2.9156450294661682e-06,
      "loss": 0.208,
      "step": 19052
    },
    {
      "epoch": 7.376306620209059,
      "grad_norm": 43.7580680847168,
      "learning_rate": 2.915214866434379e-06,
      "loss": 1.5056,
      "step": 19053
    },
    {
      "epoch": 7.376693766937669,
      "grad_norm": 82.59983825683594,
      "learning_rate": 2.91478470340259e-06,
      "loss": 0.6618,
      "step": 19054
    },
    {
      "epoch": 7.37708091366628,
      "grad_norm": 86.28189849853516,
      "learning_rate": 2.914354540370801e-06,
      "loss": 0.3229,
      "step": 19055
    },
    {
      "epoch": 7.3774680603948894,
      "grad_norm": 80.04248046875,
      "learning_rate": 2.9139243773390118e-06,
      "loss": 1.9,
      "step": 19056
    },
    {
      "epoch": 7.3778552071235,
      "grad_norm": 29.03139305114746,
      "learning_rate": 2.9134942143072228e-06,
      "loss": 2.0653,
      "step": 19057
    },
    {
      "epoch": 7.37824235385211,
      "grad_norm": 2.784586191177368,
      "learning_rate": 2.9130640512754338e-06,
      "loss": 0.0919,
      "step": 19058
    },
    {
      "epoch": 7.37862950058072,
      "grad_norm": 123.6755599975586,
      "learning_rate": 2.9126338882436443e-06,
      "loss": 0.4326,
      "step": 19059
    },
    {
      "epoch": 7.379016647309331,
      "grad_norm": 95.96524810791016,
      "learning_rate": 2.9122037252118558e-06,
      "loss": 2.8603,
      "step": 19060
    },
    {
      "epoch": 7.37940379403794,
      "grad_norm": 34.73833084106445,
      "learning_rate": 2.9117735621800663e-06,
      "loss": 1.9211,
      "step": 19061
    },
    {
      "epoch": 7.379790940766551,
      "grad_norm": 120.00912475585938,
      "learning_rate": 2.9113433991482777e-06,
      "loss": 3.0454,
      "step": 19062
    },
    {
      "epoch": 7.38017808749516,
      "grad_norm": 74.54151153564453,
      "learning_rate": 2.9109132361164883e-06,
      "loss": 3.1166,
      "step": 19063
    },
    {
      "epoch": 7.380565234223771,
      "grad_norm": 423.2093505859375,
      "learning_rate": 2.9104830730846993e-06,
      "loss": 1.3907,
      "step": 19064
    },
    {
      "epoch": 7.380952380952381,
      "grad_norm": 172.17514038085938,
      "learning_rate": 2.9100529100529103e-06,
      "loss": 1.1241,
      "step": 19065
    },
    {
      "epoch": 7.381339527680991,
      "grad_norm": 175.8078155517578,
      "learning_rate": 2.9096227470211213e-06,
      "loss": 2.3349,
      "step": 19066
    },
    {
      "epoch": 7.3817266744096015,
      "grad_norm": 5.520447731018066,
      "learning_rate": 2.909192583989332e-06,
      "loss": 0.1244,
      "step": 19067
    },
    {
      "epoch": 7.382113821138211,
      "grad_norm": 45.046714782714844,
      "learning_rate": 2.9087624209575433e-06,
      "loss": 1.9111,
      "step": 19068
    },
    {
      "epoch": 7.382500967866822,
      "grad_norm": 398.8982238769531,
      "learning_rate": 2.908332257925754e-06,
      "loss": 1.0603,
      "step": 19069
    },
    {
      "epoch": 7.382888114595431,
      "grad_norm": 65.2525405883789,
      "learning_rate": 2.9079020948939653e-06,
      "loss": 3.3594,
      "step": 19070
    },
    {
      "epoch": 7.383275261324042,
      "grad_norm": 6.759668350219727,
      "learning_rate": 2.907471931862176e-06,
      "loss": 0.0933,
      "step": 19071
    },
    {
      "epoch": 7.383662408052652,
      "grad_norm": 32.43689727783203,
      "learning_rate": 2.907041768830387e-06,
      "loss": 1.5378,
      "step": 19072
    },
    {
      "epoch": 7.384049554781262,
      "grad_norm": 59.04943084716797,
      "learning_rate": 2.906611605798598e-06,
      "loss": 1.8779,
      "step": 19073
    },
    {
      "epoch": 7.384436701509872,
      "grad_norm": 99.44520568847656,
      "learning_rate": 2.906181442766809e-06,
      "loss": 3.1425,
      "step": 19074
    },
    {
      "epoch": 7.384823848238482,
      "grad_norm": 24.66706085205078,
      "learning_rate": 2.90575127973502e-06,
      "loss": 0.1951,
      "step": 19075
    },
    {
      "epoch": 7.3852109949670925,
      "grad_norm": 140.70680236816406,
      "learning_rate": 2.9053211167032308e-06,
      "loss": 3.4416,
      "step": 19076
    },
    {
      "epoch": 7.385598141695703,
      "grad_norm": 1.4649463891983032,
      "learning_rate": 2.9048909536714414e-06,
      "loss": 0.0567,
      "step": 19077
    },
    {
      "epoch": 7.385985288424313,
      "grad_norm": 68.81524658203125,
      "learning_rate": 2.9044607906396528e-06,
      "loss": 1.8779,
      "step": 19078
    },
    {
      "epoch": 7.386372435152923,
      "grad_norm": 92.2596206665039,
      "learning_rate": 2.9040306276078633e-06,
      "loss": 2.9174,
      "step": 19079
    },
    {
      "epoch": 7.386759581881533,
      "grad_norm": 7.01392936706543,
      "learning_rate": 2.9036004645760748e-06,
      "loss": 0.1878,
      "step": 19080
    },
    {
      "epoch": 7.387146728610143,
      "grad_norm": 127.0230941772461,
      "learning_rate": 2.9031703015442853e-06,
      "loss": 1.1072,
      "step": 19081
    },
    {
      "epoch": 7.387533875338754,
      "grad_norm": 134.5094451904297,
      "learning_rate": 2.9027401385124963e-06,
      "loss": 0.4223,
      "step": 19082
    },
    {
      "epoch": 7.3879210220673635,
      "grad_norm": 201.55043029785156,
      "learning_rate": 2.9023099754807077e-06,
      "loss": 1.145,
      "step": 19083
    },
    {
      "epoch": 7.388308168795974,
      "grad_norm": 90.12519073486328,
      "learning_rate": 2.9018798124489183e-06,
      "loss": 0.8833,
      "step": 19084
    },
    {
      "epoch": 7.388695315524584,
      "grad_norm": 77.44286346435547,
      "learning_rate": 2.9014496494171297e-06,
      "loss": 1.3836,
      "step": 19085
    },
    {
      "epoch": 7.389082462253194,
      "grad_norm": 2.2978248596191406,
      "learning_rate": 2.9010194863853403e-06,
      "loss": 0.1358,
      "step": 19086
    },
    {
      "epoch": 7.389469608981804,
      "grad_norm": 3.431971549987793,
      "learning_rate": 2.9005893233535513e-06,
      "loss": 0.0956,
      "step": 19087
    },
    {
      "epoch": 7.389856755710414,
      "grad_norm": 83.80396270751953,
      "learning_rate": 2.9001591603217623e-06,
      "loss": 1.0993,
      "step": 19088
    },
    {
      "epoch": 7.390243902439025,
      "grad_norm": 3.9040472507476807,
      "learning_rate": 2.8997289972899733e-06,
      "loss": 0.0658,
      "step": 19089
    },
    {
      "epoch": 7.390631049167634,
      "grad_norm": 65.86552429199219,
      "learning_rate": 2.899298834258184e-06,
      "loss": 2.1669,
      "step": 19090
    },
    {
      "epoch": 7.391018195896245,
      "grad_norm": 1.3887110948562622,
      "learning_rate": 2.8988686712263952e-06,
      "loss": 0.0756,
      "step": 19091
    },
    {
      "epoch": 7.3914053426248545,
      "grad_norm": 44.320533752441406,
      "learning_rate": 2.898438508194606e-06,
      "loss": 0.1211,
      "step": 19092
    },
    {
      "epoch": 7.391792489353465,
      "grad_norm": 17.66670799255371,
      "learning_rate": 2.8980083451628172e-06,
      "loss": 1.6644,
      "step": 19093
    },
    {
      "epoch": 7.3921796360820755,
      "grad_norm": 144.4598388671875,
      "learning_rate": 2.897578182131028e-06,
      "loss": 2.8244,
      "step": 19094
    },
    {
      "epoch": 7.392566782810685,
      "grad_norm": 53.95233917236328,
      "learning_rate": 2.8971480190992392e-06,
      "loss": 1.1313,
      "step": 19095
    },
    {
      "epoch": 7.392953929539296,
      "grad_norm": 1.4555317163467407,
      "learning_rate": 2.8967178560674498e-06,
      "loss": 0.0638,
      "step": 19096
    },
    {
      "epoch": 7.393341076267905,
      "grad_norm": 145.84133911132812,
      "learning_rate": 2.8962876930356608e-06,
      "loss": 0.7048,
      "step": 19097
    },
    {
      "epoch": 7.393728222996516,
      "grad_norm": 81.45825958251953,
      "learning_rate": 2.8958575300038718e-06,
      "loss": 3.272,
      "step": 19098
    },
    {
      "epoch": 7.394115369725126,
      "grad_norm": 133.04144287109375,
      "learning_rate": 2.8954273669720828e-06,
      "loss": 0.6117,
      "step": 19099
    },
    {
      "epoch": 7.394502516453736,
      "grad_norm": 187.01541137695312,
      "learning_rate": 2.8949972039402933e-06,
      "loss": 1.2037,
      "step": 19100
    },
    {
      "epoch": 7.394889663182346,
      "grad_norm": 1.563757061958313,
      "learning_rate": 2.8945670409085047e-06,
      "loss": 0.0264,
      "step": 19101
    },
    {
      "epoch": 7.395276809910956,
      "grad_norm": 125.22382354736328,
      "learning_rate": 2.8941368778767153e-06,
      "loss": 3.4157,
      "step": 19102
    },
    {
      "epoch": 7.3956639566395665,
      "grad_norm": 131.34146118164062,
      "learning_rate": 2.8937067148449267e-06,
      "loss": 3.1535,
      "step": 19103
    },
    {
      "epoch": 7.396051103368176,
      "grad_norm": 62.2646484375,
      "learning_rate": 2.8932765518131373e-06,
      "loss": 2.4371,
      "step": 19104
    },
    {
      "epoch": 7.396438250096787,
      "grad_norm": 3.7356088161468506,
      "learning_rate": 2.8928463887813483e-06,
      "loss": 0.165,
      "step": 19105
    },
    {
      "epoch": 7.396825396825397,
      "grad_norm": 5.4947614669799805,
      "learning_rate": 2.8924162257495593e-06,
      "loss": 0.2046,
      "step": 19106
    },
    {
      "epoch": 7.397212543554007,
      "grad_norm": 26.858789443969727,
      "learning_rate": 2.8919860627177703e-06,
      "loss": 0.3161,
      "step": 19107
    },
    {
      "epoch": 7.397599690282617,
      "grad_norm": 51.55829620361328,
      "learning_rate": 2.891555899685981e-06,
      "loss": 3.2903,
      "step": 19108
    },
    {
      "epoch": 7.397986837011227,
      "grad_norm": 44.60878372192383,
      "learning_rate": 2.8911257366541923e-06,
      "loss": 1.027,
      "step": 19109
    },
    {
      "epoch": 7.3983739837398375,
      "grad_norm": 222.43441772460938,
      "learning_rate": 2.890695573622403e-06,
      "loss": 4.1465,
      "step": 19110
    },
    {
      "epoch": 7.398761130468447,
      "grad_norm": 39.01207733154297,
      "learning_rate": 2.8902654105906142e-06,
      "loss": 3.4333,
      "step": 19111
    },
    {
      "epoch": 7.399148277197058,
      "grad_norm": 113.8080062866211,
      "learning_rate": 2.889835247558825e-06,
      "loss": 3.4183,
      "step": 19112
    },
    {
      "epoch": 7.399535423925668,
      "grad_norm": 109.48468017578125,
      "learning_rate": 2.8894050845270362e-06,
      "loss": 0.3939,
      "step": 19113
    },
    {
      "epoch": 7.399922570654278,
      "grad_norm": 30.556285858154297,
      "learning_rate": 2.888974921495247e-06,
      "loss": 0.6399,
      "step": 19114
    },
    {
      "epoch": 7.400309717382888,
      "grad_norm": 48.07108688354492,
      "learning_rate": 2.8885447584634578e-06,
      "loss": 1.3211,
      "step": 19115
    },
    {
      "epoch": 7.400696864111498,
      "grad_norm": 64.1441421508789,
      "learning_rate": 2.8881145954316688e-06,
      "loss": 1.4673,
      "step": 19116
    },
    {
      "epoch": 7.401084010840108,
      "grad_norm": 108.83290100097656,
      "learning_rate": 2.8876844323998798e-06,
      "loss": 1.3037,
      "step": 19117
    },
    {
      "epoch": 7.401471157568719,
      "grad_norm": 5.128231525421143,
      "learning_rate": 2.8872542693680903e-06,
      "loss": 0.2113,
      "step": 19118
    },
    {
      "epoch": 7.4018583042973285,
      "grad_norm": 88.26551055908203,
      "learning_rate": 2.8868241063363018e-06,
      "loss": 0.6467,
      "step": 19119
    },
    {
      "epoch": 7.402245451025939,
      "grad_norm": 2.534001588821411,
      "learning_rate": 2.8863939433045123e-06,
      "loss": 0.1239,
      "step": 19120
    },
    {
      "epoch": 7.402632597754549,
      "grad_norm": 1.6220791339874268,
      "learning_rate": 2.8859637802727237e-06,
      "loss": 0.0687,
      "step": 19121
    },
    {
      "epoch": 7.403019744483159,
      "grad_norm": 13.936790466308594,
      "learning_rate": 2.8855336172409343e-06,
      "loss": 0.2293,
      "step": 19122
    },
    {
      "epoch": 7.40340689121177,
      "grad_norm": 92.06659698486328,
      "learning_rate": 2.8851034542091453e-06,
      "loss": 0.828,
      "step": 19123
    },
    {
      "epoch": 7.403794037940379,
      "grad_norm": 2.928927183151245,
      "learning_rate": 2.8846732911773567e-06,
      "loss": 0.0887,
      "step": 19124
    },
    {
      "epoch": 7.40418118466899,
      "grad_norm": 39.4739875793457,
      "learning_rate": 2.8842431281455673e-06,
      "loss": 1.676,
      "step": 19125
    },
    {
      "epoch": 7.404568331397599,
      "grad_norm": 12.0990629196167,
      "learning_rate": 2.8838129651137787e-06,
      "loss": 0.2377,
      "step": 19126
    },
    {
      "epoch": 7.40495547812621,
      "grad_norm": 54.616180419921875,
      "learning_rate": 2.8833828020819893e-06,
      "loss": 2.1699,
      "step": 19127
    },
    {
      "epoch": 7.4053426248548195,
      "grad_norm": 132.4935760498047,
      "learning_rate": 2.8829526390502007e-06,
      "loss": 2.3001,
      "step": 19128
    },
    {
      "epoch": 7.40572977158343,
      "grad_norm": 29.03292465209961,
      "learning_rate": 2.8825224760184113e-06,
      "loss": 0.2272,
      "step": 19129
    },
    {
      "epoch": 7.4061169183120406,
      "grad_norm": 1.8038660287857056,
      "learning_rate": 2.8820923129866222e-06,
      "loss": 0.059,
      "step": 19130
    },
    {
      "epoch": 7.40650406504065,
      "grad_norm": 42.84695053100586,
      "learning_rate": 2.8816621499548332e-06,
      "loss": 1.9932,
      "step": 19131
    },
    {
      "epoch": 7.406891211769261,
      "grad_norm": 42.24988555908203,
      "learning_rate": 2.8812319869230442e-06,
      "loss": 0.9185,
      "step": 19132
    },
    {
      "epoch": 7.40727835849787,
      "grad_norm": 25.231794357299805,
      "learning_rate": 2.880801823891255e-06,
      "loss": 0.1249,
      "step": 19133
    },
    {
      "epoch": 7.407665505226481,
      "grad_norm": 97.1747817993164,
      "learning_rate": 2.8803716608594662e-06,
      "loss": 0.7597,
      "step": 19134
    },
    {
      "epoch": 7.408052651955091,
      "grad_norm": 31.65444564819336,
      "learning_rate": 2.8799414978276768e-06,
      "loss": 0.158,
      "step": 19135
    },
    {
      "epoch": 7.408439798683701,
      "grad_norm": 26.725696563720703,
      "learning_rate": 2.879511334795888e-06,
      "loss": 2.6168,
      "step": 19136
    },
    {
      "epoch": 7.4088269454123115,
      "grad_norm": 43.11448669433594,
      "learning_rate": 2.8790811717640988e-06,
      "loss": 2.343,
      "step": 19137
    },
    {
      "epoch": 7.409214092140921,
      "grad_norm": 161.9661407470703,
      "learning_rate": 2.8786510087323098e-06,
      "loss": 2.5108,
      "step": 19138
    },
    {
      "epoch": 7.409601238869532,
      "grad_norm": 79.42288970947266,
      "learning_rate": 2.8782208457005208e-06,
      "loss": 0.4749,
      "step": 19139
    },
    {
      "epoch": 7.409988385598142,
      "grad_norm": 325.60162353515625,
      "learning_rate": 2.8777906826687317e-06,
      "loss": 1.0996,
      "step": 19140
    },
    {
      "epoch": 7.410375532326752,
      "grad_norm": 31.110496520996094,
      "learning_rate": 2.8773605196369423e-06,
      "loss": 0.3586,
      "step": 19141
    },
    {
      "epoch": 7.410762679055362,
      "grad_norm": 1.7461600303649902,
      "learning_rate": 2.8769303566051537e-06,
      "loss": 0.0699,
      "step": 19142
    },
    {
      "epoch": 7.411149825783972,
      "grad_norm": 97.82930755615234,
      "learning_rate": 2.8765001935733643e-06,
      "loss": 1.6374,
      "step": 19143
    },
    {
      "epoch": 7.411536972512582,
      "grad_norm": 1.425467610359192,
      "learning_rate": 2.8760700305415757e-06,
      "loss": 0.0688,
      "step": 19144
    },
    {
      "epoch": 7.411924119241192,
      "grad_norm": 1.479496955871582,
      "learning_rate": 2.8756398675097863e-06,
      "loss": 0.0363,
      "step": 19145
    },
    {
      "epoch": 7.4123112659698025,
      "grad_norm": 5.847238540649414,
      "learning_rate": 2.8752097044779977e-06,
      "loss": 0.197,
      "step": 19146
    },
    {
      "epoch": 7.412698412698413,
      "grad_norm": 0.8943700790405273,
      "learning_rate": 2.8747795414462083e-06,
      "loss": 0.0318,
      "step": 19147
    },
    {
      "epoch": 7.413085559427023,
      "grad_norm": 63.30482864379883,
      "learning_rate": 2.8743493784144193e-06,
      "loss": 0.901,
      "step": 19148
    },
    {
      "epoch": 7.413472706155633,
      "grad_norm": 0.5152228474617004,
      "learning_rate": 2.8739192153826303e-06,
      "loss": 0.0178,
      "step": 19149
    },
    {
      "epoch": 7.413859852884243,
      "grad_norm": 62.74656677246094,
      "learning_rate": 2.8734890523508412e-06,
      "loss": 1.5393,
      "step": 19150
    },
    {
      "epoch": 7.414246999612853,
      "grad_norm": 2.7781636714935303,
      "learning_rate": 2.873058889319052e-06,
      "loss": 0.1469,
      "step": 19151
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 85.63713836669922,
      "learning_rate": 2.8726287262872632e-06,
      "loss": 1.3011,
      "step": 19152
    },
    {
      "epoch": 7.415021293070073,
      "grad_norm": 70.2683334350586,
      "learning_rate": 2.872198563255474e-06,
      "loss": 2.3872,
      "step": 19153
    },
    {
      "epoch": 7.415408439798684,
      "grad_norm": 81.37106323242188,
      "learning_rate": 2.8717684002236852e-06,
      "loss": 1.2646,
      "step": 19154
    },
    {
      "epoch": 7.4157955865272935,
      "grad_norm": 99.01783752441406,
      "learning_rate": 2.8713382371918958e-06,
      "loss": 3.0761,
      "step": 19155
    },
    {
      "epoch": 7.416182733255904,
      "grad_norm": 76.80902099609375,
      "learning_rate": 2.8709080741601068e-06,
      "loss": 0.2942,
      "step": 19156
    },
    {
      "epoch": 7.416569879984515,
      "grad_norm": 324.8804626464844,
      "learning_rate": 2.8704779111283178e-06,
      "loss": 3.5504,
      "step": 19157
    },
    {
      "epoch": 7.416957026713124,
      "grad_norm": 65.81168365478516,
      "learning_rate": 2.8700477480965288e-06,
      "loss": 2.5076,
      "step": 19158
    },
    {
      "epoch": 7.417344173441735,
      "grad_norm": 72.66593933105469,
      "learning_rate": 2.8696175850647393e-06,
      "loss": 0.7994,
      "step": 19159
    },
    {
      "epoch": 7.417731320170344,
      "grad_norm": 4.2737507820129395,
      "learning_rate": 2.8691874220329507e-06,
      "loss": 0.1167,
      "step": 19160
    },
    {
      "epoch": 7.418118466898955,
      "grad_norm": 147.79083251953125,
      "learning_rate": 2.8687572590011613e-06,
      "loss": 1.063,
      "step": 19161
    },
    {
      "epoch": 7.418505613627564,
      "grad_norm": 3.0240838527679443,
      "learning_rate": 2.8683270959693727e-06,
      "loss": 0.1253,
      "step": 19162
    },
    {
      "epoch": 7.418892760356175,
      "grad_norm": 3.566122055053711,
      "learning_rate": 2.8678969329375833e-06,
      "loss": 0.131,
      "step": 19163
    },
    {
      "epoch": 7.4192799070847855,
      "grad_norm": 55.16581726074219,
      "learning_rate": 2.8674667699057947e-06,
      "loss": 1.7251,
      "step": 19164
    },
    {
      "epoch": 7.419667053813395,
      "grad_norm": 36.91531753540039,
      "learning_rate": 2.8670366068740057e-06,
      "loss": 1.2175,
      "step": 19165
    },
    {
      "epoch": 7.420054200542006,
      "grad_norm": 48.35588455200195,
      "learning_rate": 2.8666064438422163e-06,
      "loss": 0.3846,
      "step": 19166
    },
    {
      "epoch": 7.420441347270615,
      "grad_norm": 5.153799057006836,
      "learning_rate": 2.8661762808104277e-06,
      "loss": 0.1291,
      "step": 19167
    },
    {
      "epoch": 7.420828493999226,
      "grad_norm": 54.345462799072266,
      "learning_rate": 2.8657461177786383e-06,
      "loss": 1.9399,
      "step": 19168
    },
    {
      "epoch": 7.421215640727836,
      "grad_norm": 116.53077697753906,
      "learning_rate": 2.8653159547468497e-06,
      "loss": 2.8764,
      "step": 19169
    },
    {
      "epoch": 7.421602787456446,
      "grad_norm": 8.183826446533203,
      "learning_rate": 2.8648857917150602e-06,
      "loss": 0.1496,
      "step": 19170
    },
    {
      "epoch": 7.421989934185056,
      "grad_norm": 81.23201751708984,
      "learning_rate": 2.8644556286832712e-06,
      "loss": 1.5401,
      "step": 19171
    },
    {
      "epoch": 7.422377080913666,
      "grad_norm": 5.262897968292236,
      "learning_rate": 2.8640254656514822e-06,
      "loss": 0.064,
      "step": 19172
    },
    {
      "epoch": 7.4227642276422765,
      "grad_norm": 3.164389133453369,
      "learning_rate": 2.8635953026196932e-06,
      "loss": 0.1685,
      "step": 19173
    },
    {
      "epoch": 7.423151374370887,
      "grad_norm": 12.257545471191406,
      "learning_rate": 2.8631651395879038e-06,
      "loss": 0.1551,
      "step": 19174
    },
    {
      "epoch": 7.423538521099497,
      "grad_norm": 142.0382080078125,
      "learning_rate": 2.862734976556115e-06,
      "loss": 2.3106,
      "step": 19175
    },
    {
      "epoch": 7.423925667828107,
      "grad_norm": 32.34377670288086,
      "learning_rate": 2.8623048135243258e-06,
      "loss": 2.1525,
      "step": 19176
    },
    {
      "epoch": 7.424312814556717,
      "grad_norm": 192.56103515625,
      "learning_rate": 2.861874650492537e-06,
      "loss": 1.0733,
      "step": 19177
    },
    {
      "epoch": 7.424699961285327,
      "grad_norm": 1.1756932735443115,
      "learning_rate": 2.8614444874607478e-06,
      "loss": 0.0475,
      "step": 19178
    },
    {
      "epoch": 7.425087108013937,
      "grad_norm": 142.92213439941406,
      "learning_rate": 2.861014324428959e-06,
      "loss": 0.6248,
      "step": 19179
    },
    {
      "epoch": 7.425474254742547,
      "grad_norm": 49.55796432495117,
      "learning_rate": 2.8605841613971697e-06,
      "loss": 0.6409,
      "step": 19180
    },
    {
      "epoch": 7.425861401471158,
      "grad_norm": 12.855506896972656,
      "learning_rate": 2.8601539983653807e-06,
      "loss": 0.3542,
      "step": 19181
    },
    {
      "epoch": 7.4262485481997675,
      "grad_norm": 76.0754623413086,
      "learning_rate": 2.8597238353335917e-06,
      "loss": 0.6101,
      "step": 19182
    },
    {
      "epoch": 7.426635694928378,
      "grad_norm": 19.386676788330078,
      "learning_rate": 2.8592936723018027e-06,
      "loss": 0.2805,
      "step": 19183
    },
    {
      "epoch": 7.427022841656988,
      "grad_norm": 38.86371994018555,
      "learning_rate": 2.8588635092700133e-06,
      "loss": 0.2254,
      "step": 19184
    },
    {
      "epoch": 7.427409988385598,
      "grad_norm": 87.37948608398438,
      "learning_rate": 2.8584333462382247e-06,
      "loss": 0.6858,
      "step": 19185
    },
    {
      "epoch": 7.427797135114209,
      "grad_norm": 29.19015121459961,
      "learning_rate": 2.8580031832064353e-06,
      "loss": 1.9546,
      "step": 19186
    },
    {
      "epoch": 7.428184281842818,
      "grad_norm": 6.875840663909912,
      "learning_rate": 2.8575730201746467e-06,
      "loss": 0.1822,
      "step": 19187
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 3.8668739795684814,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 0.0624,
      "step": 19188
    },
    {
      "epoch": 7.4289585753000384,
      "grad_norm": 117.7028579711914,
      "learning_rate": 2.8567126941110682e-06,
      "loss": 1.6156,
      "step": 19189
    },
    {
      "epoch": 7.429345722028649,
      "grad_norm": 99.15451049804688,
      "learning_rate": 2.8562825310792792e-06,
      "loss": 1.0658,
      "step": 19190
    },
    {
      "epoch": 7.4297328687572595,
      "grad_norm": 2.8033273220062256,
      "learning_rate": 2.8558523680474902e-06,
      "loss": 0.0674,
      "step": 19191
    },
    {
      "epoch": 7.430120015485869,
      "grad_norm": 20.354372024536133,
      "learning_rate": 2.855422205015701e-06,
      "loss": 3.7656,
      "step": 19192
    },
    {
      "epoch": 7.43050716221448,
      "grad_norm": 118.06412506103516,
      "learning_rate": 2.8549920419839122e-06,
      "loss": 1.6796,
      "step": 19193
    },
    {
      "epoch": 7.430894308943089,
      "grad_norm": 142.43896484375,
      "learning_rate": 2.8545618789521228e-06,
      "loss": 0.7038,
      "step": 19194
    },
    {
      "epoch": 7.4312814556717,
      "grad_norm": 1.594979166984558,
      "learning_rate": 2.854131715920334e-06,
      "loss": 0.066,
      "step": 19195
    },
    {
      "epoch": 7.431668602400309,
      "grad_norm": 109.5324935913086,
      "learning_rate": 2.8537015528885448e-06,
      "loss": 1.6499,
      "step": 19196
    },
    {
      "epoch": 7.43205574912892,
      "grad_norm": 3.347238779067993,
      "learning_rate": 2.853271389856756e-06,
      "loss": 0.1235,
      "step": 19197
    },
    {
      "epoch": 7.43244289585753,
      "grad_norm": 90.45809173583984,
      "learning_rate": 2.8528412268249668e-06,
      "loss": 0.8316,
      "step": 19198
    },
    {
      "epoch": 7.43283004258614,
      "grad_norm": 72.57913970947266,
      "learning_rate": 2.8524110637931777e-06,
      "loss": 0.7414,
      "step": 19199
    },
    {
      "epoch": 7.4332171893147505,
      "grad_norm": 8.580843925476074,
      "learning_rate": 2.8519809007613887e-06,
      "loss": 0.0851,
      "step": 19200
    },
    {
      "epoch": 7.43360433604336,
      "grad_norm": 5.325290679931641,
      "learning_rate": 2.8515507377295997e-06,
      "loss": 0.0913,
      "step": 19201
    },
    {
      "epoch": 7.433991482771971,
      "grad_norm": 6.802062034606934,
      "learning_rate": 2.8511205746978103e-06,
      "loss": 0.0933,
      "step": 19202
    },
    {
      "epoch": 7.43437862950058,
      "grad_norm": 10.488840103149414,
      "learning_rate": 2.8506904116660217e-06,
      "loss": 0.0587,
      "step": 19203
    },
    {
      "epoch": 7.434765776229191,
      "grad_norm": 308.6936340332031,
      "learning_rate": 2.8502602486342323e-06,
      "loss": 2.4101,
      "step": 19204
    },
    {
      "epoch": 7.435152922957801,
      "grad_norm": 1.8284481763839722,
      "learning_rate": 2.8498300856024437e-06,
      "loss": 0.0359,
      "step": 19205
    },
    {
      "epoch": 7.435540069686411,
      "grad_norm": 135.77615356445312,
      "learning_rate": 2.8493999225706547e-06,
      "loss": 0.5318,
      "step": 19206
    },
    {
      "epoch": 7.435927216415021,
      "grad_norm": 39.15492248535156,
      "learning_rate": 2.8489697595388653e-06,
      "loss": 2.2449,
      "step": 19207
    },
    {
      "epoch": 7.436314363143631,
      "grad_norm": 20.36922836303711,
      "learning_rate": 2.8485395965070767e-06,
      "loss": 0.2629,
      "step": 19208
    },
    {
      "epoch": 7.4367015098722415,
      "grad_norm": 193.60813903808594,
      "learning_rate": 2.8481094334752872e-06,
      "loss": 0.9436,
      "step": 19209
    },
    {
      "epoch": 7.437088656600852,
      "grad_norm": 74.19647216796875,
      "learning_rate": 2.8476792704434987e-06,
      "loss": 0.8086,
      "step": 19210
    },
    {
      "epoch": 7.437475803329462,
      "grad_norm": 119.9948959350586,
      "learning_rate": 2.8472491074117092e-06,
      "loss": 0.4796,
      "step": 19211
    },
    {
      "epoch": 7.437862950058072,
      "grad_norm": 110.53364562988281,
      "learning_rate": 2.8468189443799206e-06,
      "loss": 2.5036,
      "step": 19212
    },
    {
      "epoch": 7.438250096786682,
      "grad_norm": 3.188668966293335,
      "learning_rate": 2.8463887813481312e-06,
      "loss": 0.0924,
      "step": 19213
    },
    {
      "epoch": 7.438637243515292,
      "grad_norm": 73.60259246826172,
      "learning_rate": 2.845958618316342e-06,
      "loss": 1.0117,
      "step": 19214
    },
    {
      "epoch": 7.439024390243903,
      "grad_norm": 1.1093153953552246,
      "learning_rate": 2.845528455284553e-06,
      "loss": 0.0305,
      "step": 19215
    },
    {
      "epoch": 7.4394115369725125,
      "grad_norm": 86.10450744628906,
      "learning_rate": 2.845098292252764e-06,
      "loss": 1.5244,
      "step": 19216
    },
    {
      "epoch": 7.439798683701123,
      "grad_norm": 1.6950349807739258,
      "learning_rate": 2.8446681292209748e-06,
      "loss": 0.0767,
      "step": 19217
    },
    {
      "epoch": 7.440185830429733,
      "grad_norm": 188.28469848632812,
      "learning_rate": 2.844237966189186e-06,
      "loss": 0.7913,
      "step": 19218
    },
    {
      "epoch": 7.440572977158343,
      "grad_norm": 185.33631896972656,
      "learning_rate": 2.8438078031573967e-06,
      "loss": 0.6173,
      "step": 19219
    },
    {
      "epoch": 7.440960123886953,
      "grad_norm": 28.73759651184082,
      "learning_rate": 2.843377640125608e-06,
      "loss": 1.4369,
      "step": 19220
    },
    {
      "epoch": 7.441347270615563,
      "grad_norm": 141.44178771972656,
      "learning_rate": 2.8429474770938187e-06,
      "loss": 1.4587,
      "step": 19221
    },
    {
      "epoch": 7.441734417344174,
      "grad_norm": 84.74010467529297,
      "learning_rate": 2.8425173140620297e-06,
      "loss": 2.142,
      "step": 19222
    },
    {
      "epoch": 7.442121564072783,
      "grad_norm": 37.65889358520508,
      "learning_rate": 2.8420871510302407e-06,
      "loss": 0.1132,
      "step": 19223
    },
    {
      "epoch": 7.442508710801394,
      "grad_norm": 18.46832275390625,
      "learning_rate": 2.8416569879984517e-06,
      "loss": 0.5796,
      "step": 19224
    },
    {
      "epoch": 7.4428958575300035,
      "grad_norm": 31.215593338012695,
      "learning_rate": 2.8412268249666623e-06,
      "loss": 2.9801,
      "step": 19225
    },
    {
      "epoch": 7.443283004258614,
      "grad_norm": 88.73957061767578,
      "learning_rate": 2.8407966619348737e-06,
      "loss": 1.0502,
      "step": 19226
    },
    {
      "epoch": 7.4436701509872245,
      "grad_norm": 5.254355430603027,
      "learning_rate": 2.8403664989030843e-06,
      "loss": 0.1389,
      "step": 19227
    },
    {
      "epoch": 7.444057297715834,
      "grad_norm": 141.29910278320312,
      "learning_rate": 2.8399363358712957e-06,
      "loss": 0.2462,
      "step": 19228
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 67.8782730102539,
      "learning_rate": 2.8395061728395062e-06,
      "loss": 1.82,
      "step": 19229
    },
    {
      "epoch": 7.444831591173054,
      "grad_norm": 60.80195617675781,
      "learning_rate": 2.8390760098077177e-06,
      "loss": 1.8845,
      "step": 19230
    },
    {
      "epoch": 7.445218737901665,
      "grad_norm": 10.599907875061035,
      "learning_rate": 2.8386458467759282e-06,
      "loss": 0.107,
      "step": 19231
    },
    {
      "epoch": 7.445605884630275,
      "grad_norm": 175.68612670898438,
      "learning_rate": 2.8382156837441392e-06,
      "loss": 0.9534,
      "step": 19232
    },
    {
      "epoch": 7.445993031358885,
      "grad_norm": 111.57659149169922,
      "learning_rate": 2.8377855207123502e-06,
      "loss": 2.7384,
      "step": 19233
    },
    {
      "epoch": 7.446380178087495,
      "grad_norm": 2.968564748764038,
      "learning_rate": 2.837355357680561e-06,
      "loss": 0.1191,
      "step": 19234
    },
    {
      "epoch": 7.446767324816105,
      "grad_norm": 25.759130477905273,
      "learning_rate": 2.8369251946487718e-06,
      "loss": 1.6583,
      "step": 19235
    },
    {
      "epoch": 7.4471544715447155,
      "grad_norm": 64.56924438476562,
      "learning_rate": 2.836495031616983e-06,
      "loss": 0.4056,
      "step": 19236
    },
    {
      "epoch": 7.447541618273325,
      "grad_norm": 119.44127655029297,
      "learning_rate": 2.8360648685851938e-06,
      "loss": 2.5966,
      "step": 19237
    },
    {
      "epoch": 7.447928765001936,
      "grad_norm": 51.107078552246094,
      "learning_rate": 2.835634705553405e-06,
      "loss": 1.6588,
      "step": 19238
    },
    {
      "epoch": 7.448315911730546,
      "grad_norm": 222.83457946777344,
      "learning_rate": 2.8352045425216157e-06,
      "loss": 2.5699,
      "step": 19239
    },
    {
      "epoch": 7.448703058459156,
      "grad_norm": 92.40704345703125,
      "learning_rate": 2.8347743794898267e-06,
      "loss": 2.9716,
      "step": 19240
    },
    {
      "epoch": 7.449090205187766,
      "grad_norm": 109.27909851074219,
      "learning_rate": 2.8343442164580377e-06,
      "loss": 2.2078,
      "step": 19241
    },
    {
      "epoch": 7.449477351916376,
      "grad_norm": 22.709264755249023,
      "learning_rate": 2.8339140534262487e-06,
      "loss": 1.9717,
      "step": 19242
    },
    {
      "epoch": 7.4498644986449865,
      "grad_norm": 93.61980438232422,
      "learning_rate": 2.8334838903944593e-06,
      "loss": 2.063,
      "step": 19243
    },
    {
      "epoch": 7.450251645373597,
      "grad_norm": 106.13986206054688,
      "learning_rate": 2.8330537273626707e-06,
      "loss": 2.5825,
      "step": 19244
    },
    {
      "epoch": 7.450638792102207,
      "grad_norm": 25.26671028137207,
      "learning_rate": 2.8326235643308813e-06,
      "loss": 0.4555,
      "step": 19245
    },
    {
      "epoch": 7.451025938830817,
      "grad_norm": 53.75122833251953,
      "learning_rate": 2.8321934012990927e-06,
      "loss": 1.4589,
      "step": 19246
    },
    {
      "epoch": 7.451413085559427,
      "grad_norm": 3.4640064239501953,
      "learning_rate": 2.8317632382673037e-06,
      "loss": 0.0694,
      "step": 19247
    },
    {
      "epoch": 7.451800232288037,
      "grad_norm": 0.9417096376419067,
      "learning_rate": 2.8313330752355147e-06,
      "loss": 0.029,
      "step": 19248
    },
    {
      "epoch": 7.452187379016648,
      "grad_norm": 128.89369201660156,
      "learning_rate": 2.8309029122037257e-06,
      "loss": 0.5484,
      "step": 19249
    },
    {
      "epoch": 7.452574525745257,
      "grad_norm": 88.5616683959961,
      "learning_rate": 2.8304727491719362e-06,
      "loss": 0.4283,
      "step": 19250
    },
    {
      "epoch": 7.452961672473868,
      "grad_norm": 36.45771408081055,
      "learning_rate": 2.8300425861401476e-06,
      "loss": 0.2136,
      "step": 19251
    },
    {
      "epoch": 7.4533488192024775,
      "grad_norm": 127.33964538574219,
      "learning_rate": 2.8296124231083582e-06,
      "loss": 1.1933,
      "step": 19252
    },
    {
      "epoch": 7.453735965931088,
      "grad_norm": 32.256221771240234,
      "learning_rate": 2.8291822600765696e-06,
      "loss": 2.2853,
      "step": 19253
    },
    {
      "epoch": 7.454123112659698,
      "grad_norm": 25.35879898071289,
      "learning_rate": 2.82875209704478e-06,
      "loss": 2.6795,
      "step": 19254
    },
    {
      "epoch": 7.454510259388308,
      "grad_norm": 27.615131378173828,
      "learning_rate": 2.828321934012991e-06,
      "loss": 1.0197,
      "step": 19255
    },
    {
      "epoch": 7.454897406116919,
      "grad_norm": 1.154162049293518,
      "learning_rate": 2.827891770981202e-06,
      "loss": 0.0616,
      "step": 19256
    },
    {
      "epoch": 7.455284552845528,
      "grad_norm": 18.047883987426758,
      "learning_rate": 2.827461607949413e-06,
      "loss": 0.238,
      "step": 19257
    },
    {
      "epoch": 7.455671699574139,
      "grad_norm": 163.6635284423828,
      "learning_rate": 2.8270314449176237e-06,
      "loss": 0.472,
      "step": 19258
    },
    {
      "epoch": 7.456058846302748,
      "grad_norm": 79.52206420898438,
      "learning_rate": 2.826601281885835e-06,
      "loss": 0.2082,
      "step": 19259
    },
    {
      "epoch": 7.456445993031359,
      "grad_norm": 75.26602935791016,
      "learning_rate": 2.8261711188540457e-06,
      "loss": 0.5436,
      "step": 19260
    },
    {
      "epoch": 7.456833139759969,
      "grad_norm": 119.2668228149414,
      "learning_rate": 2.825740955822257e-06,
      "loss": 0.3565,
      "step": 19261
    },
    {
      "epoch": 7.457220286488579,
      "grad_norm": 23.23910140991211,
      "learning_rate": 2.8253107927904677e-06,
      "loss": 2.0279,
      "step": 19262
    },
    {
      "epoch": 7.4576074332171896,
      "grad_norm": 26.469385147094727,
      "learning_rate": 2.824880629758679e-06,
      "loss": 0.1551,
      "step": 19263
    },
    {
      "epoch": 7.457994579945799,
      "grad_norm": 245.6215362548828,
      "learning_rate": 2.8244504667268897e-06,
      "loss": 2.5512,
      "step": 19264
    },
    {
      "epoch": 7.45838172667441,
      "grad_norm": 70.9837646484375,
      "learning_rate": 2.8240203036951007e-06,
      "loss": 1.2037,
      "step": 19265
    },
    {
      "epoch": 7.45876887340302,
      "grad_norm": 75.51519775390625,
      "learning_rate": 2.8235901406633117e-06,
      "loss": 1.0759,
      "step": 19266
    },
    {
      "epoch": 7.45915602013163,
      "grad_norm": 33.9082145690918,
      "learning_rate": 2.8231599776315227e-06,
      "loss": 0.2955,
      "step": 19267
    },
    {
      "epoch": 7.45954316686024,
      "grad_norm": 27.612730026245117,
      "learning_rate": 2.8227298145997332e-06,
      "loss": 0.232,
      "step": 19268
    },
    {
      "epoch": 7.45993031358885,
      "grad_norm": 30.757144927978516,
      "learning_rate": 2.8222996515679447e-06,
      "loss": 2.1139,
      "step": 19269
    },
    {
      "epoch": 7.4603174603174605,
      "grad_norm": 1.2255388498306274,
      "learning_rate": 2.8218694885361552e-06,
      "loss": 0.0418,
      "step": 19270
    },
    {
      "epoch": 7.46070460704607,
      "grad_norm": 2.0377418994903564,
      "learning_rate": 2.8214393255043666e-06,
      "loss": 0.0875,
      "step": 19271
    },
    {
      "epoch": 7.461091753774681,
      "grad_norm": 0.48869097232818604,
      "learning_rate": 2.8210091624725772e-06,
      "loss": 0.0168,
      "step": 19272
    },
    {
      "epoch": 7.461478900503291,
      "grad_norm": 16.427654266357422,
      "learning_rate": 2.820578999440788e-06,
      "loss": 0.262,
      "step": 19273
    },
    {
      "epoch": 7.461866047231901,
      "grad_norm": 36.584571838378906,
      "learning_rate": 2.820148836408999e-06,
      "loss": 1.8254,
      "step": 19274
    },
    {
      "epoch": 7.462253193960511,
      "grad_norm": 124.84868621826172,
      "learning_rate": 2.81971867337721e-06,
      "loss": 4.0928,
      "step": 19275
    },
    {
      "epoch": 7.462640340689121,
      "grad_norm": 305.8800354003906,
      "learning_rate": 2.8192885103454208e-06,
      "loss": 1.8923,
      "step": 19276
    },
    {
      "epoch": 7.463027487417731,
      "grad_norm": 45.52975845336914,
      "learning_rate": 2.818858347313632e-06,
      "loss": 0.7257,
      "step": 19277
    },
    {
      "epoch": 7.463414634146342,
      "grad_norm": 26.437118530273438,
      "learning_rate": 2.8184281842818427e-06,
      "loss": 2.2896,
      "step": 19278
    },
    {
      "epoch": 7.4638017808749515,
      "grad_norm": 106.68550872802734,
      "learning_rate": 2.817998021250054e-06,
      "loss": 1.3461,
      "step": 19279
    },
    {
      "epoch": 7.464188927603562,
      "grad_norm": 4.901785850524902,
      "learning_rate": 2.8175678582182647e-06,
      "loss": 0.0759,
      "step": 19280
    },
    {
      "epoch": 7.464576074332172,
      "grad_norm": 25.343059539794922,
      "learning_rate": 2.817137695186476e-06,
      "loss": 0.0745,
      "step": 19281
    },
    {
      "epoch": 7.464963221060782,
      "grad_norm": 2.7637951374053955,
      "learning_rate": 2.8167075321546867e-06,
      "loss": 0.0526,
      "step": 19282
    },
    {
      "epoch": 7.465350367789393,
      "grad_norm": 1.0281866788864136,
      "learning_rate": 2.8162773691228977e-06,
      "loss": 0.029,
      "step": 19283
    },
    {
      "epoch": 7.465737514518002,
      "grad_norm": 164.3677520751953,
      "learning_rate": 2.8158472060911087e-06,
      "loss": 2.6431,
      "step": 19284
    },
    {
      "epoch": 7.466124661246613,
      "grad_norm": 0.9473169445991516,
      "learning_rate": 2.8154170430593197e-06,
      "loss": 0.0276,
      "step": 19285
    },
    {
      "epoch": 7.466511807975222,
      "grad_norm": 8.971973419189453,
      "learning_rate": 2.8149868800275303e-06,
      "loss": 0.0411,
      "step": 19286
    },
    {
      "epoch": 7.466898954703833,
      "grad_norm": 1.4192219972610474,
      "learning_rate": 2.8145567169957417e-06,
      "loss": 0.0619,
      "step": 19287
    },
    {
      "epoch": 7.4672861014324425,
      "grad_norm": 1.7865954637527466,
      "learning_rate": 2.8141265539639527e-06,
      "loss": 0.046,
      "step": 19288
    },
    {
      "epoch": 7.467673248161053,
      "grad_norm": 93.80296325683594,
      "learning_rate": 2.8136963909321637e-06,
      "loss": 0.8617,
      "step": 19289
    },
    {
      "epoch": 7.4680603948896636,
      "grad_norm": 42.60185241699219,
      "learning_rate": 2.8132662279003747e-06,
      "loss": 1.0208,
      "step": 19290
    },
    {
      "epoch": 7.468447541618273,
      "grad_norm": 39.1584358215332,
      "learning_rate": 2.8128360648685852e-06,
      "loss": 1.9397,
      "step": 19291
    },
    {
      "epoch": 7.468834688346884,
      "grad_norm": 54.99921417236328,
      "learning_rate": 2.8124059018367966e-06,
      "loss": 0.7291,
      "step": 19292
    },
    {
      "epoch": 7.469221835075493,
      "grad_norm": 185.09869384765625,
      "learning_rate": 2.811975738805007e-06,
      "loss": 1.3165,
      "step": 19293
    },
    {
      "epoch": 7.469608981804104,
      "grad_norm": 170.1732177734375,
      "learning_rate": 2.8115455757732186e-06,
      "loss": 0.6569,
      "step": 19294
    },
    {
      "epoch": 7.469996128532713,
      "grad_norm": 34.98933029174805,
      "learning_rate": 2.811115412741429e-06,
      "loss": 0.328,
      "step": 19295
    },
    {
      "epoch": 7.470383275261324,
      "grad_norm": 1.0712629556655884,
      "learning_rate": 2.8106852497096406e-06,
      "loss": 0.0362,
      "step": 19296
    },
    {
      "epoch": 7.4707704219899345,
      "grad_norm": 79.85284423828125,
      "learning_rate": 2.810255086677851e-06,
      "loss": 3.4482,
      "step": 19297
    },
    {
      "epoch": 7.471157568718544,
      "grad_norm": 64.51433563232422,
      "learning_rate": 2.809824923646062e-06,
      "loss": 1.4144,
      "step": 19298
    },
    {
      "epoch": 7.471544715447155,
      "grad_norm": 130.625,
      "learning_rate": 2.809394760614273e-06,
      "loss": 3.0516,
      "step": 19299
    },
    {
      "epoch": 7.471931862175764,
      "grad_norm": 146.6846923828125,
      "learning_rate": 2.808964597582484e-06,
      "loss": 2.5652,
      "step": 19300
    },
    {
      "epoch": 7.472319008904375,
      "grad_norm": 163.03343200683594,
      "learning_rate": 2.8085344345506947e-06,
      "loss": 1.4962,
      "step": 19301
    },
    {
      "epoch": 7.472706155632985,
      "grad_norm": 5.654497146606445,
      "learning_rate": 2.808104271518906e-06,
      "loss": 0.0523,
      "step": 19302
    },
    {
      "epoch": 7.473093302361595,
      "grad_norm": 1.0671980381011963,
      "learning_rate": 2.8076741084871167e-06,
      "loss": 0.0566,
      "step": 19303
    },
    {
      "epoch": 7.473480449090205,
      "grad_norm": 24.744813919067383,
      "learning_rate": 2.807243945455328e-06,
      "loss": 2.1402,
      "step": 19304
    },
    {
      "epoch": 7.473867595818815,
      "grad_norm": 59.04369354248047,
      "learning_rate": 2.8068137824235387e-06,
      "loss": 1.9022,
      "step": 19305
    },
    {
      "epoch": 7.4742547425474255,
      "grad_norm": 219.73294067382812,
      "learning_rate": 2.8063836193917497e-06,
      "loss": 1.8791,
      "step": 19306
    },
    {
      "epoch": 7.474641889276036,
      "grad_norm": 43.62929916381836,
      "learning_rate": 2.8059534563599607e-06,
      "loss": 6.6204,
      "step": 19307
    },
    {
      "epoch": 7.475029036004646,
      "grad_norm": 19.514772415161133,
      "learning_rate": 2.8055232933281717e-06,
      "loss": 1.3725,
      "step": 19308
    },
    {
      "epoch": 7.475416182733256,
      "grad_norm": 1.4603214263916016,
      "learning_rate": 2.8050931302963822e-06,
      "loss": 0.0589,
      "step": 19309
    },
    {
      "epoch": 7.475803329461866,
      "grad_norm": 16.818527221679688,
      "learning_rate": 2.8046629672645936e-06,
      "loss": 0.0667,
      "step": 19310
    },
    {
      "epoch": 7.476190476190476,
      "grad_norm": 99.7534408569336,
      "learning_rate": 2.8042328042328042e-06,
      "loss": 0.6883,
      "step": 19311
    },
    {
      "epoch": 7.476577622919086,
      "grad_norm": 3.5082616806030273,
      "learning_rate": 2.8038026412010156e-06,
      "loss": 0.1323,
      "step": 19312
    },
    {
      "epoch": 7.476964769647696,
      "grad_norm": 184.27108764648438,
      "learning_rate": 2.803372478169226e-06,
      "loss": 1.9586,
      "step": 19313
    },
    {
      "epoch": 7.477351916376307,
      "grad_norm": 63.81550979614258,
      "learning_rate": 2.8029423151374376e-06,
      "loss": 3.0897,
      "step": 19314
    },
    {
      "epoch": 7.4777390631049165,
      "grad_norm": 3.101274013519287,
      "learning_rate": 2.802512152105648e-06,
      "loss": 0.0614,
      "step": 19315
    },
    {
      "epoch": 7.478126209833527,
      "grad_norm": 353.9857177734375,
      "learning_rate": 2.802081989073859e-06,
      "loss": 1.8157,
      "step": 19316
    },
    {
      "epoch": 7.478513356562137,
      "grad_norm": 7.399727821350098,
      "learning_rate": 2.80165182604207e-06,
      "loss": 0.0869,
      "step": 19317
    },
    {
      "epoch": 7.478900503290747,
      "grad_norm": 24.677173614501953,
      "learning_rate": 2.801221663010281e-06,
      "loss": 0.3326,
      "step": 19318
    },
    {
      "epoch": 7.479287650019358,
      "grad_norm": 194.88809204101562,
      "learning_rate": 2.8007914999784917e-06,
      "loss": 2.6676,
      "step": 19319
    },
    {
      "epoch": 7.479674796747967,
      "grad_norm": 181.78460693359375,
      "learning_rate": 2.800361336946703e-06,
      "loss": 2.4106,
      "step": 19320
    },
    {
      "epoch": 7.480061943476578,
      "grad_norm": 72.80406951904297,
      "learning_rate": 2.7999311739149137e-06,
      "loss": 1.1409,
      "step": 19321
    },
    {
      "epoch": 7.4804490902051874,
      "grad_norm": 181.08453369140625,
      "learning_rate": 2.799501010883125e-06,
      "loss": 0.7783,
      "step": 19322
    },
    {
      "epoch": 7.480836236933798,
      "grad_norm": 4.006433010101318,
      "learning_rate": 2.7990708478513357e-06,
      "loss": 0.0886,
      "step": 19323
    },
    {
      "epoch": 7.4812233836624085,
      "grad_norm": 1.0752507448196411,
      "learning_rate": 2.7986406848195467e-06,
      "loss": 0.0288,
      "step": 19324
    },
    {
      "epoch": 7.481610530391018,
      "grad_norm": 94.81818389892578,
      "learning_rate": 2.7982105217877577e-06,
      "loss": 1.0839,
      "step": 19325
    },
    {
      "epoch": 7.481997677119629,
      "grad_norm": 50.69674301147461,
      "learning_rate": 2.7977803587559687e-06,
      "loss": 1.5098,
      "step": 19326
    },
    {
      "epoch": 7.482384823848238,
      "grad_norm": 30.200254440307617,
      "learning_rate": 2.7973501957241792e-06,
      "loss": 0.1526,
      "step": 19327
    },
    {
      "epoch": 7.482771970576849,
      "grad_norm": 174.78338623046875,
      "learning_rate": 2.7969200326923907e-06,
      "loss": 0.7084,
      "step": 19328
    },
    {
      "epoch": 7.483159117305458,
      "grad_norm": 3.58577036857605,
      "learning_rate": 2.7964898696606012e-06,
      "loss": 0.0477,
      "step": 19329
    },
    {
      "epoch": 7.483546264034069,
      "grad_norm": 145.1086883544922,
      "learning_rate": 2.7960597066288126e-06,
      "loss": 3.745,
      "step": 19330
    },
    {
      "epoch": 7.483933410762679,
      "grad_norm": 28.453344345092773,
      "learning_rate": 2.7956295435970236e-06,
      "loss": 1.0027,
      "step": 19331
    },
    {
      "epoch": 7.484320557491289,
      "grad_norm": 89.0200424194336,
      "learning_rate": 2.7951993805652346e-06,
      "loss": 0.2363,
      "step": 19332
    },
    {
      "epoch": 7.4847077042198995,
      "grad_norm": 87.81964874267578,
      "learning_rate": 2.7947692175334456e-06,
      "loss": 1.6592,
      "step": 19333
    },
    {
      "epoch": 7.485094850948509,
      "grad_norm": 15.00308609008789,
      "learning_rate": 2.794339054501656e-06,
      "loss": 0.2581,
      "step": 19334
    },
    {
      "epoch": 7.48548199767712,
      "grad_norm": 30.888944625854492,
      "learning_rate": 2.7939088914698676e-06,
      "loss": 1.7135,
      "step": 19335
    },
    {
      "epoch": 7.48586914440573,
      "grad_norm": 45.02968215942383,
      "learning_rate": 2.793478728438078e-06,
      "loss": 0.3784,
      "step": 19336
    },
    {
      "epoch": 7.48625629113434,
      "grad_norm": 10.638540267944336,
      "learning_rate": 2.7930485654062896e-06,
      "loss": 0.1849,
      "step": 19337
    },
    {
      "epoch": 7.48664343786295,
      "grad_norm": 50.56660079956055,
      "learning_rate": 2.7926184023745e-06,
      "loss": 0.6849,
      "step": 19338
    },
    {
      "epoch": 7.48703058459156,
      "grad_norm": 53.013572692871094,
      "learning_rate": 2.792188239342711e-06,
      "loss": 1.9959,
      "step": 19339
    },
    {
      "epoch": 7.48741773132017,
      "grad_norm": 13.289393424987793,
      "learning_rate": 2.791758076310922e-06,
      "loss": 0.2075,
      "step": 19340
    },
    {
      "epoch": 7.487804878048781,
      "grad_norm": 149.51551818847656,
      "learning_rate": 2.791327913279133e-06,
      "loss": 1.9365,
      "step": 19341
    },
    {
      "epoch": 7.4881920247773905,
      "grad_norm": 274.10235595703125,
      "learning_rate": 2.7908977502473437e-06,
      "loss": 4.8305,
      "step": 19342
    },
    {
      "epoch": 7.488579171506001,
      "grad_norm": 200.25991821289062,
      "learning_rate": 2.790467587215555e-06,
      "loss": 3.9472,
      "step": 19343
    },
    {
      "epoch": 7.488966318234611,
      "grad_norm": 36.769989013671875,
      "learning_rate": 2.7900374241837657e-06,
      "loss": 0.525,
      "step": 19344
    },
    {
      "epoch": 7.489353464963221,
      "grad_norm": 68.41522979736328,
      "learning_rate": 2.789607261151977e-06,
      "loss": 0.5233,
      "step": 19345
    },
    {
      "epoch": 7.489740611691831,
      "grad_norm": 154.7455596923828,
      "learning_rate": 2.7891770981201877e-06,
      "loss": 1.435,
      "step": 19346
    },
    {
      "epoch": 7.490127758420441,
      "grad_norm": 151.21279907226562,
      "learning_rate": 2.788746935088399e-06,
      "loss": 1.4309,
      "step": 19347
    },
    {
      "epoch": 7.490514905149052,
      "grad_norm": 6.780185222625732,
      "learning_rate": 2.7883167720566097e-06,
      "loss": 0.1748,
      "step": 19348
    },
    {
      "epoch": 7.4909020518776614,
      "grad_norm": 10.107806205749512,
      "learning_rate": 2.7878866090248207e-06,
      "loss": 0.2673,
      "step": 19349
    },
    {
      "epoch": 7.491289198606272,
      "grad_norm": 108.27001953125,
      "learning_rate": 2.7874564459930316e-06,
      "loss": 2.3693,
      "step": 19350
    },
    {
      "epoch": 7.491676345334882,
      "grad_norm": 40.851924896240234,
      "learning_rate": 2.7870262829612426e-06,
      "loss": 0.4645,
      "step": 19351
    },
    {
      "epoch": 7.492063492063492,
      "grad_norm": 126.31659698486328,
      "learning_rate": 2.786596119929453e-06,
      "loss": 0.4883,
      "step": 19352
    },
    {
      "epoch": 7.492450638792103,
      "grad_norm": 40.67901611328125,
      "learning_rate": 2.7861659568976646e-06,
      "loss": 0.2557,
      "step": 19353
    },
    {
      "epoch": 7.492837785520712,
      "grad_norm": 157.90187072753906,
      "learning_rate": 2.785735793865875e-06,
      "loss": 0.6431,
      "step": 19354
    },
    {
      "epoch": 7.493224932249323,
      "grad_norm": 83.3421630859375,
      "learning_rate": 2.7853056308340866e-06,
      "loss": 2.3363,
      "step": 19355
    },
    {
      "epoch": 7.493612078977932,
      "grad_norm": 402.0458679199219,
      "learning_rate": 2.784875467802297e-06,
      "loss": 2.602,
      "step": 19356
    },
    {
      "epoch": 7.493999225706543,
      "grad_norm": 0.8746397495269775,
      "learning_rate": 2.784445304770508e-06,
      "loss": 0.0246,
      "step": 19357
    },
    {
      "epoch": 7.494386372435153,
      "grad_norm": 65.90570068359375,
      "learning_rate": 2.784015141738719e-06,
      "loss": 2.7421,
      "step": 19358
    },
    {
      "epoch": 7.494773519163763,
      "grad_norm": 43.550167083740234,
      "learning_rate": 2.78358497870693e-06,
      "loss": 2.1616,
      "step": 19359
    },
    {
      "epoch": 7.4951606658923735,
      "grad_norm": 33.671783447265625,
      "learning_rate": 2.7831548156751407e-06,
      "loss": 2.3315,
      "step": 19360
    },
    {
      "epoch": 7.495547812620983,
      "grad_norm": 45.460731506347656,
      "learning_rate": 2.782724652643352e-06,
      "loss": 0.3363,
      "step": 19361
    },
    {
      "epoch": 7.495934959349594,
      "grad_norm": 5.4826483726501465,
      "learning_rate": 2.7822944896115627e-06,
      "loss": 0.1489,
      "step": 19362
    },
    {
      "epoch": 7.496322106078203,
      "grad_norm": 158.9112548828125,
      "learning_rate": 2.781864326579774e-06,
      "loss": 0.5893,
      "step": 19363
    },
    {
      "epoch": 7.496709252806814,
      "grad_norm": 104.91493225097656,
      "learning_rate": 2.7814341635479847e-06,
      "loss": 3.5861,
      "step": 19364
    },
    {
      "epoch": 7.497096399535424,
      "grad_norm": 117.51676177978516,
      "learning_rate": 2.781004000516196e-06,
      "loss": 1.1428,
      "step": 19365
    },
    {
      "epoch": 7.497483546264034,
      "grad_norm": 49.93227767944336,
      "learning_rate": 2.7805738374844067e-06,
      "loss": 2.3874,
      "step": 19366
    },
    {
      "epoch": 7.497870692992644,
      "grad_norm": 38.0992431640625,
      "learning_rate": 2.7801436744526177e-06,
      "loss": 1.4724,
      "step": 19367
    },
    {
      "epoch": 7.498257839721254,
      "grad_norm": 0.59600830078125,
      "learning_rate": 2.7797135114208287e-06,
      "loss": 0.0196,
      "step": 19368
    },
    {
      "epoch": 7.4986449864498645,
      "grad_norm": 256.05206298828125,
      "learning_rate": 2.7792833483890396e-06,
      "loss": 1.146,
      "step": 19369
    },
    {
      "epoch": 7.499032133178475,
      "grad_norm": 49.10117721557617,
      "learning_rate": 2.7788531853572502e-06,
      "loss": 1.6754,
      "step": 19370
    },
    {
      "epoch": 7.499419279907085,
      "grad_norm": 83.5488052368164,
      "learning_rate": 2.7784230223254616e-06,
      "loss": 1.3329,
      "step": 19371
    },
    {
      "epoch": 7.499806426635695,
      "grad_norm": 52.234214782714844,
      "learning_rate": 2.7779928592936726e-06,
      "loss": 1.2711,
      "step": 19372
    },
    {
      "epoch": 7.500193573364305,
      "grad_norm": 99.24103546142578,
      "learning_rate": 2.7775626962618836e-06,
      "loss": 1.3968,
      "step": 19373
    },
    {
      "epoch": 7.500580720092915,
      "grad_norm": 2.5036251544952393,
      "learning_rate": 2.7771325332300946e-06,
      "loss": 0.0527,
      "step": 19374
    },
    {
      "epoch": 7.500967866821526,
      "grad_norm": 69.59879302978516,
      "learning_rate": 2.776702370198305e-06,
      "loss": 2.1349,
      "step": 19375
    },
    {
      "epoch": 7.5013550135501355,
      "grad_norm": 6.277820587158203,
      "learning_rate": 2.7762722071665166e-06,
      "loss": 0.0406,
      "step": 19376
    },
    {
      "epoch": 7.501742160278746,
      "grad_norm": 1.2481259107589722,
      "learning_rate": 2.775842044134727e-06,
      "loss": 0.05,
      "step": 19377
    },
    {
      "epoch": 7.502129307007356,
      "grad_norm": 32.003334045410156,
      "learning_rate": 2.7754118811029386e-06,
      "loss": 0.3779,
      "step": 19378
    },
    {
      "epoch": 7.502516453735966,
      "grad_norm": 60.531951904296875,
      "learning_rate": 2.774981718071149e-06,
      "loss": 1.0935,
      "step": 19379
    },
    {
      "epoch": 7.502903600464576,
      "grad_norm": 17.87665557861328,
      "learning_rate": 2.7745515550393606e-06,
      "loss": 1.4071,
      "step": 19380
    },
    {
      "epoch": 7.503290747193186,
      "grad_norm": 31.988386154174805,
      "learning_rate": 2.774121392007571e-06,
      "loss": 0.131,
      "step": 19381
    },
    {
      "epoch": 7.503677893921797,
      "grad_norm": 157.5321044921875,
      "learning_rate": 2.773691228975782e-06,
      "loss": 0.3343,
      "step": 19382
    },
    {
      "epoch": 7.504065040650406,
      "grad_norm": 6.7948198318481445,
      "learning_rate": 2.773261065943993e-06,
      "loss": 0.0592,
      "step": 19383
    },
    {
      "epoch": 7.504452187379017,
      "grad_norm": 50.91340637207031,
      "learning_rate": 2.772830902912204e-06,
      "loss": 0.3695,
      "step": 19384
    },
    {
      "epoch": 7.5048393341076265,
      "grad_norm": 16.325023651123047,
      "learning_rate": 2.7724007398804147e-06,
      "loss": 0.2401,
      "step": 19385
    },
    {
      "epoch": 7.505226480836237,
      "grad_norm": 32.64832305908203,
      "learning_rate": 2.771970576848626e-06,
      "loss": 0.2354,
      "step": 19386
    },
    {
      "epoch": 7.505613627564847,
      "grad_norm": 91.13780212402344,
      "learning_rate": 2.7715404138168367e-06,
      "loss": 1.1772,
      "step": 19387
    },
    {
      "epoch": 7.506000774293457,
      "grad_norm": 219.44503784179688,
      "learning_rate": 2.771110250785048e-06,
      "loss": 0.7753,
      "step": 19388
    },
    {
      "epoch": 7.506387921022068,
      "grad_norm": 41.935848236083984,
      "learning_rate": 2.7706800877532586e-06,
      "loss": 0.1388,
      "step": 19389
    },
    {
      "epoch": 7.506775067750677,
      "grad_norm": 51.435707092285156,
      "learning_rate": 2.7702499247214696e-06,
      "loss": 1.7467,
      "step": 19390
    },
    {
      "epoch": 7.507162214479288,
      "grad_norm": 7.029212474822998,
      "learning_rate": 2.7698197616896806e-06,
      "loss": 0.2146,
      "step": 19391
    },
    {
      "epoch": 7.507549361207898,
      "grad_norm": 105.58068084716797,
      "learning_rate": 2.7693895986578916e-06,
      "loss": 1.0293,
      "step": 19392
    },
    {
      "epoch": 7.507936507936508,
      "grad_norm": 3.518495798110962,
      "learning_rate": 2.768959435626102e-06,
      "loss": 0.1344,
      "step": 19393
    },
    {
      "epoch": 7.508323654665118,
      "grad_norm": 46.73965072631836,
      "learning_rate": 2.7685292725943136e-06,
      "loss": 0.4636,
      "step": 19394
    },
    {
      "epoch": 7.508710801393728,
      "grad_norm": 207.86183166503906,
      "learning_rate": 2.768099109562524e-06,
      "loss": 0.9657,
      "step": 19395
    },
    {
      "epoch": 7.5090979481223386,
      "grad_norm": 94.39859771728516,
      "learning_rate": 2.7676689465307356e-06,
      "loss": 3.3806,
      "step": 19396
    },
    {
      "epoch": 7.509485094850948,
      "grad_norm": 174.81370544433594,
      "learning_rate": 2.767238783498946e-06,
      "loss": 1.0787,
      "step": 19397
    },
    {
      "epoch": 7.509872241579559,
      "grad_norm": 2.800774335861206,
      "learning_rate": 2.7668086204671576e-06,
      "loss": 0.1154,
      "step": 19398
    },
    {
      "epoch": 7.510259388308169,
      "grad_norm": 86.17644500732422,
      "learning_rate": 2.766378457435368e-06,
      "loss": 3.5695,
      "step": 19399
    },
    {
      "epoch": 7.510646535036779,
      "grad_norm": 48.48950958251953,
      "learning_rate": 2.765948294403579e-06,
      "loss": 1.7437,
      "step": 19400
    },
    {
      "epoch": 7.511033681765389,
      "grad_norm": 9.84267520904541,
      "learning_rate": 2.76551813137179e-06,
      "loss": 0.2551,
      "step": 19401
    },
    {
      "epoch": 7.511420828493999,
      "grad_norm": 3.4645938873291016,
      "learning_rate": 2.765087968340001e-06,
      "loss": 0.0623,
      "step": 19402
    },
    {
      "epoch": 7.5118079752226095,
      "grad_norm": 64.13870239257812,
      "learning_rate": 2.7646578053082117e-06,
      "loss": 1.8541,
      "step": 19403
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 46.37643051147461,
      "learning_rate": 2.764227642276423e-06,
      "loss": 0.9954,
      "step": 19404
    },
    {
      "epoch": 7.51258226867983,
      "grad_norm": 45.92664337158203,
      "learning_rate": 2.7637974792446337e-06,
      "loss": 0.2626,
      "step": 19405
    },
    {
      "epoch": 7.51296941540844,
      "grad_norm": 13.506699562072754,
      "learning_rate": 2.763367316212845e-06,
      "loss": 0.2582,
      "step": 19406
    },
    {
      "epoch": 7.51335656213705,
      "grad_norm": 84.13296508789062,
      "learning_rate": 2.7629371531810557e-06,
      "loss": 0.4543,
      "step": 19407
    },
    {
      "epoch": 7.51374370886566,
      "grad_norm": 0.6304948329925537,
      "learning_rate": 2.7625069901492667e-06,
      "loss": 0.0215,
      "step": 19408
    },
    {
      "epoch": 7.51413085559427,
      "grad_norm": 70.13615417480469,
      "learning_rate": 2.7620768271174776e-06,
      "loss": 0.3993,
      "step": 19409
    },
    {
      "epoch": 7.51451800232288,
      "grad_norm": 36.22430419921875,
      "learning_rate": 2.7616466640856886e-06,
      "loss": 4.8815,
      "step": 19410
    },
    {
      "epoch": 7.514905149051491,
      "grad_norm": 6.818832874298096,
      "learning_rate": 2.761216501053899e-06,
      "loss": 0.1848,
      "step": 19411
    },
    {
      "epoch": 7.5152922957801005,
      "grad_norm": 125.83676147460938,
      "learning_rate": 2.7607863380221106e-06,
      "loss": 0.5108,
      "step": 19412
    },
    {
      "epoch": 7.515679442508711,
      "grad_norm": 133.0667724609375,
      "learning_rate": 2.7603561749903216e-06,
      "loss": 0.4838,
      "step": 19413
    },
    {
      "epoch": 7.516066589237321,
      "grad_norm": 11.693008422851562,
      "learning_rate": 2.7599260119585326e-06,
      "loss": 0.1344,
      "step": 19414
    },
    {
      "epoch": 7.516453735965931,
      "grad_norm": 15.356544494628906,
      "learning_rate": 2.7594958489267436e-06,
      "loss": 0.0843,
      "step": 19415
    },
    {
      "epoch": 7.516840882694542,
      "grad_norm": 3.657200336456299,
      "learning_rate": 2.7590656858949546e-06,
      "loss": 0.0769,
      "step": 19416
    },
    {
      "epoch": 7.517228029423151,
      "grad_norm": 3.1287102699279785,
      "learning_rate": 2.7586355228631656e-06,
      "loss": 0.0386,
      "step": 19417
    },
    {
      "epoch": 7.517615176151762,
      "grad_norm": 13.437094688415527,
      "learning_rate": 2.758205359831376e-06,
      "loss": 0.0679,
      "step": 19418
    },
    {
      "epoch": 7.518002322880371,
      "grad_norm": 1.344118595123291,
      "learning_rate": 2.7577751967995876e-06,
      "loss": 0.0596,
      "step": 19419
    },
    {
      "epoch": 7.518389469608982,
      "grad_norm": 99.44902801513672,
      "learning_rate": 2.757345033767798e-06,
      "loss": 3.1224,
      "step": 19420
    },
    {
      "epoch": 7.5187766163375915,
      "grad_norm": 131.20370483398438,
      "learning_rate": 2.7569148707360096e-06,
      "loss": 0.4121,
      "step": 19421
    },
    {
      "epoch": 7.519163763066202,
      "grad_norm": 28.82975196838379,
      "learning_rate": 2.75648470770422e-06,
      "loss": 1.7973,
      "step": 19422
    },
    {
      "epoch": 7.5195509097948126,
      "grad_norm": 24.879497528076172,
      "learning_rate": 2.756054544672431e-06,
      "loss": 0.2018,
      "step": 19423
    },
    {
      "epoch": 7.519938056523422,
      "grad_norm": 2.6987316608428955,
      "learning_rate": 2.755624381640642e-06,
      "loss": 0.1161,
      "step": 19424
    },
    {
      "epoch": 7.520325203252033,
      "grad_norm": 72.34062194824219,
      "learning_rate": 2.755194218608853e-06,
      "loss": 2.7042,
      "step": 19425
    },
    {
      "epoch": 7.520712349980642,
      "grad_norm": 4.434853553771973,
      "learning_rate": 2.7547640555770637e-06,
      "loss": 0.0818,
      "step": 19426
    },
    {
      "epoch": 7.521099496709253,
      "grad_norm": 33.29619216918945,
      "learning_rate": 2.754333892545275e-06,
      "loss": 0.1304,
      "step": 19427
    },
    {
      "epoch": 7.521486643437863,
      "grad_norm": 7.440388202667236,
      "learning_rate": 2.7539037295134857e-06,
      "loss": 0.2461,
      "step": 19428
    },
    {
      "epoch": 7.521873790166473,
      "grad_norm": 2.804112672805786,
      "learning_rate": 2.753473566481697e-06,
      "loss": 0.0714,
      "step": 19429
    },
    {
      "epoch": 7.5222609368950835,
      "grad_norm": 96.07705688476562,
      "learning_rate": 2.7530434034499076e-06,
      "loss": 1.3475,
      "step": 19430
    },
    {
      "epoch": 7.522648083623693,
      "grad_norm": 117.29656982421875,
      "learning_rate": 2.7526132404181186e-06,
      "loss": 0.6726,
      "step": 19431
    },
    {
      "epoch": 7.523035230352304,
      "grad_norm": 40.18048858642578,
      "learning_rate": 2.7521830773863296e-06,
      "loss": 1.6166,
      "step": 19432
    },
    {
      "epoch": 7.523422377080914,
      "grad_norm": 5.479305744171143,
      "learning_rate": 2.7517529143545406e-06,
      "loss": 0.1921,
      "step": 19433
    },
    {
      "epoch": 7.523809523809524,
      "grad_norm": 27.18758201599121,
      "learning_rate": 2.7513227513227516e-06,
      "loss": 1.9318,
      "step": 19434
    },
    {
      "epoch": 7.524196670538134,
      "grad_norm": 117.09529876708984,
      "learning_rate": 2.7508925882909626e-06,
      "loss": 0.9807,
      "step": 19435
    },
    {
      "epoch": 7.524583817266744,
      "grad_norm": 75.704345703125,
      "learning_rate": 2.750462425259173e-06,
      "loss": 1.5569,
      "step": 19436
    },
    {
      "epoch": 7.524970963995354,
      "grad_norm": 35.147342681884766,
      "learning_rate": 2.7500322622273846e-06,
      "loss": 0.1323,
      "step": 19437
    },
    {
      "epoch": 7.525358110723964,
      "grad_norm": 0.5416216254234314,
      "learning_rate": 2.749602099195595e-06,
      "loss": 0.0192,
      "step": 19438
    },
    {
      "epoch": 7.5257452574525745,
      "grad_norm": 8.322463035583496,
      "learning_rate": 2.7491719361638066e-06,
      "loss": 0.1123,
      "step": 19439
    },
    {
      "epoch": 7.526132404181185,
      "grad_norm": 184.43191528320312,
      "learning_rate": 2.748741773132017e-06,
      "loss": 3.2718,
      "step": 19440
    },
    {
      "epoch": 7.526519550909795,
      "grad_norm": 22.777542114257812,
      "learning_rate": 2.748311610100228e-06,
      "loss": 0.1834,
      "step": 19441
    },
    {
      "epoch": 7.526906697638405,
      "grad_norm": 61.21296691894531,
      "learning_rate": 2.747881447068439e-06,
      "loss": 1.9198,
      "step": 19442
    },
    {
      "epoch": 7.527293844367015,
      "grad_norm": 120.14800262451172,
      "learning_rate": 2.74745128403665e-06,
      "loss": 0.7545,
      "step": 19443
    },
    {
      "epoch": 7.527680991095625,
      "grad_norm": 29.65496253967285,
      "learning_rate": 2.7470211210048607e-06,
      "loss": 3.4834,
      "step": 19444
    },
    {
      "epoch": 7.528068137824235,
      "grad_norm": 196.3985137939453,
      "learning_rate": 2.746590957973072e-06,
      "loss": 0.6391,
      "step": 19445
    },
    {
      "epoch": 7.528455284552845,
      "grad_norm": 29.269073486328125,
      "learning_rate": 2.7461607949412827e-06,
      "loss": 2.2711,
      "step": 19446
    },
    {
      "epoch": 7.528842431281456,
      "grad_norm": 98.20448303222656,
      "learning_rate": 2.745730631909494e-06,
      "loss": 2.0255,
      "step": 19447
    },
    {
      "epoch": 7.5292295780100655,
      "grad_norm": 3.432318687438965,
      "learning_rate": 2.7453004688777046e-06,
      "loss": 0.0914,
      "step": 19448
    },
    {
      "epoch": 7.529616724738676,
      "grad_norm": 9.152792930603027,
      "learning_rate": 2.7448703058459156e-06,
      "loss": 0.0732,
      "step": 19449
    },
    {
      "epoch": 7.530003871467287,
      "grad_norm": 3.293295383453369,
      "learning_rate": 2.7444401428141266e-06,
      "loss": 0.1238,
      "step": 19450
    },
    {
      "epoch": 7.530391018195896,
      "grad_norm": 87.57803344726562,
      "learning_rate": 2.7440099797823376e-06,
      "loss": 1.393,
      "step": 19451
    },
    {
      "epoch": 7.530778164924507,
      "grad_norm": 184.8362274169922,
      "learning_rate": 2.7435798167505486e-06,
      "loss": 0.4922,
      "step": 19452
    },
    {
      "epoch": 7.531165311653116,
      "grad_norm": 35.770198822021484,
      "learning_rate": 2.7431496537187596e-06,
      "loss": 0.2104,
      "step": 19453
    },
    {
      "epoch": 7.531552458381727,
      "grad_norm": 30.75751495361328,
      "learning_rate": 2.742719490686971e-06,
      "loss": 1.6274,
      "step": 19454
    },
    {
      "epoch": 7.5319396051103364,
      "grad_norm": 1.574095606803894,
      "learning_rate": 2.7422893276551816e-06,
      "loss": 0.0584,
      "step": 19455
    },
    {
      "epoch": 7.532326751838947,
      "grad_norm": 77.67758178710938,
      "learning_rate": 2.7418591646233926e-06,
      "loss": 0.1691,
      "step": 19456
    },
    {
      "epoch": 7.5327138985675575,
      "grad_norm": 22.57822036743164,
      "learning_rate": 2.7414290015916036e-06,
      "loss": 0.3461,
      "step": 19457
    },
    {
      "epoch": 7.533101045296167,
      "grad_norm": 104.50759887695312,
      "learning_rate": 2.7409988385598146e-06,
      "loss": 0.8558,
      "step": 19458
    },
    {
      "epoch": 7.533488192024778,
      "grad_norm": 23.67717170715332,
      "learning_rate": 2.740568675528025e-06,
      "loss": 2.364,
      "step": 19459
    },
    {
      "epoch": 7.533875338753387,
      "grad_norm": 10.22160816192627,
      "learning_rate": 2.7401385124962366e-06,
      "loss": 0.1117,
      "step": 19460
    },
    {
      "epoch": 7.534262485481998,
      "grad_norm": 40.63286209106445,
      "learning_rate": 2.739708349464447e-06,
      "loss": 1.8637,
      "step": 19461
    },
    {
      "epoch": 7.534649632210607,
      "grad_norm": 34.105445861816406,
      "learning_rate": 2.7392781864326585e-06,
      "loss": 0.1798,
      "step": 19462
    },
    {
      "epoch": 7.535036778939218,
      "grad_norm": 11.051321029663086,
      "learning_rate": 2.738848023400869e-06,
      "loss": 0.1404,
      "step": 19463
    },
    {
      "epoch": 7.535423925667828,
      "grad_norm": 24.171051025390625,
      "learning_rate": 2.73841786036908e-06,
      "loss": 3.3712,
      "step": 19464
    },
    {
      "epoch": 7.535811072396438,
      "grad_norm": 81.80322265625,
      "learning_rate": 2.737987697337291e-06,
      "loss": 1.1225,
      "step": 19465
    },
    {
      "epoch": 7.5361982191250485,
      "grad_norm": 210.58226013183594,
      "learning_rate": 2.737557534305502e-06,
      "loss": 0.8161,
      "step": 19466
    },
    {
      "epoch": 7.536585365853659,
      "grad_norm": 64.43814849853516,
      "learning_rate": 2.7371273712737127e-06,
      "loss": 0.2654,
      "step": 19467
    },
    {
      "epoch": 7.536972512582269,
      "grad_norm": 83.78380584716797,
      "learning_rate": 2.736697208241924e-06,
      "loss": 0.4141,
      "step": 19468
    },
    {
      "epoch": 7.537359659310879,
      "grad_norm": 81.46741485595703,
      "learning_rate": 2.7362670452101346e-06,
      "loss": 3.6913,
      "step": 19469
    },
    {
      "epoch": 7.537746806039489,
      "grad_norm": 1.6033108234405518,
      "learning_rate": 2.735836882178346e-06,
      "loss": 0.0675,
      "step": 19470
    },
    {
      "epoch": 7.538133952768099,
      "grad_norm": 47.765899658203125,
      "learning_rate": 2.7354067191465566e-06,
      "loss": 0.174,
      "step": 19471
    },
    {
      "epoch": 7.538521099496709,
      "grad_norm": 49.76838684082031,
      "learning_rate": 2.734976556114768e-06,
      "loss": 1.7853,
      "step": 19472
    },
    {
      "epoch": 7.538908246225319,
      "grad_norm": 1.9504586458206177,
      "learning_rate": 2.7345463930829786e-06,
      "loss": 0.0606,
      "step": 19473
    },
    {
      "epoch": 7.53929539295393,
      "grad_norm": 4.675563335418701,
      "learning_rate": 2.7341162300511896e-06,
      "loss": 0.1657,
      "step": 19474
    },
    {
      "epoch": 7.5396825396825395,
      "grad_norm": 194.99624633789062,
      "learning_rate": 2.7336860670194006e-06,
      "loss": 0.7004,
      "step": 19475
    },
    {
      "epoch": 7.54006968641115,
      "grad_norm": 29.451330184936523,
      "learning_rate": 2.7332559039876116e-06,
      "loss": 2.212,
      "step": 19476
    },
    {
      "epoch": 7.54045683313976,
      "grad_norm": 24.946353912353516,
      "learning_rate": 2.732825740955822e-06,
      "loss": 0.3188,
      "step": 19477
    },
    {
      "epoch": 7.54084397986837,
      "grad_norm": 13.409333229064941,
      "learning_rate": 2.7323955779240336e-06,
      "loss": 0.225,
      "step": 19478
    },
    {
      "epoch": 7.54123112659698,
      "grad_norm": 86.31256103515625,
      "learning_rate": 2.731965414892244e-06,
      "loss": 2.0173,
      "step": 19479
    },
    {
      "epoch": 7.54161827332559,
      "grad_norm": 59.6356086730957,
      "learning_rate": 2.7315352518604556e-06,
      "loss": 2.4233,
      "step": 19480
    },
    {
      "epoch": 7.542005420054201,
      "grad_norm": 135.6077117919922,
      "learning_rate": 2.731105088828666e-06,
      "loss": 1.0221,
      "step": 19481
    },
    {
      "epoch": 7.5423925667828104,
      "grad_norm": 30.401567459106445,
      "learning_rate": 2.730674925796877e-06,
      "loss": 1.9181,
      "step": 19482
    },
    {
      "epoch": 7.542779713511421,
      "grad_norm": 26.11809539794922,
      "learning_rate": 2.730244762765088e-06,
      "loss": 1.7327,
      "step": 19483
    },
    {
      "epoch": 7.5431668602400315,
      "grad_norm": 4.768009185791016,
      "learning_rate": 2.729814599733299e-06,
      "loss": 0.1475,
      "step": 19484
    },
    {
      "epoch": 7.543554006968641,
      "grad_norm": 7.158691883087158,
      "learning_rate": 2.7293844367015097e-06,
      "loss": 0.1331,
      "step": 19485
    },
    {
      "epoch": 7.543941153697252,
      "grad_norm": 49.92268371582031,
      "learning_rate": 2.728954273669721e-06,
      "loss": 0.5693,
      "step": 19486
    },
    {
      "epoch": 7.544328300425861,
      "grad_norm": 89.9288330078125,
      "learning_rate": 2.7285241106379317e-06,
      "loss": 1.6309,
      "step": 19487
    },
    {
      "epoch": 7.544715447154472,
      "grad_norm": 39.892818450927734,
      "learning_rate": 2.728093947606143e-06,
      "loss": 1.1619,
      "step": 19488
    },
    {
      "epoch": 7.545102593883081,
      "grad_norm": 52.91264724731445,
      "learning_rate": 2.7276637845743536e-06,
      "loss": 1.3951,
      "step": 19489
    },
    {
      "epoch": 7.545489740611692,
      "grad_norm": 135.55145263671875,
      "learning_rate": 2.727233621542565e-06,
      "loss": 1.5955,
      "step": 19490
    },
    {
      "epoch": 7.545876887340302,
      "grad_norm": 18.88762855529785,
      "learning_rate": 2.7268034585107756e-06,
      "loss": 0.1292,
      "step": 19491
    },
    {
      "epoch": 7.546264034068912,
      "grad_norm": 1.9825639724731445,
      "learning_rate": 2.7263732954789866e-06,
      "loss": 0.0523,
      "step": 19492
    },
    {
      "epoch": 7.5466511807975225,
      "grad_norm": 28.636550903320312,
      "learning_rate": 2.7259431324471976e-06,
      "loss": 2.1532,
      "step": 19493
    },
    {
      "epoch": 7.547038327526132,
      "grad_norm": 85.07410430908203,
      "learning_rate": 2.7255129694154086e-06,
      "loss": 1.939,
      "step": 19494
    },
    {
      "epoch": 7.547425474254743,
      "grad_norm": 139.32763671875,
      "learning_rate": 2.72508280638362e-06,
      "loss": 1.37,
      "step": 19495
    },
    {
      "epoch": 7.547812620983352,
      "grad_norm": 209.0874481201172,
      "learning_rate": 2.7246526433518306e-06,
      "loss": 1.4885,
      "step": 19496
    },
    {
      "epoch": 7.548199767711963,
      "grad_norm": 96.87199401855469,
      "learning_rate": 2.7242224803200416e-06,
      "loss": 2.0783,
      "step": 19497
    },
    {
      "epoch": 7.548586914440573,
      "grad_norm": 2.898736000061035,
      "learning_rate": 2.7237923172882526e-06,
      "loss": 0.0853,
      "step": 19498
    },
    {
      "epoch": 7.548974061169183,
      "grad_norm": 9.367198944091797,
      "learning_rate": 2.7233621542564636e-06,
      "loss": 0.0854,
      "step": 19499
    },
    {
      "epoch": 7.549361207897793,
      "grad_norm": 20.3196964263916,
      "learning_rate": 2.722931991224674e-06,
      "loss": 1.9448,
      "step": 19500
    },
    {
      "epoch": 7.549748354626403,
      "grad_norm": 72.46015930175781,
      "learning_rate": 2.7225018281928855e-06,
      "loss": 0.5842,
      "step": 19501
    },
    {
      "epoch": 7.5501355013550135,
      "grad_norm": 5.849232196807861,
      "learning_rate": 2.722071665161096e-06,
      "loss": 0.2013,
      "step": 19502
    },
    {
      "epoch": 7.550522648083624,
      "grad_norm": 24.741382598876953,
      "learning_rate": 2.7216415021293075e-06,
      "loss": 2.0828,
      "step": 19503
    },
    {
      "epoch": 7.550909794812234,
      "grad_norm": 12.443010330200195,
      "learning_rate": 2.721211339097518e-06,
      "loss": 0.2438,
      "step": 19504
    },
    {
      "epoch": 7.551296941540844,
      "grad_norm": 79.83294677734375,
      "learning_rate": 2.7207811760657295e-06,
      "loss": 0.4117,
      "step": 19505
    },
    {
      "epoch": 7.551684088269454,
      "grad_norm": 8.98196792602539,
      "learning_rate": 2.72035101303394e-06,
      "loss": 0.1272,
      "step": 19506
    },
    {
      "epoch": 7.552071234998064,
      "grad_norm": 65.1794662475586,
      "learning_rate": 2.719920850002151e-06,
      "loss": 3.1842,
      "step": 19507
    },
    {
      "epoch": 7.552458381726675,
      "grad_norm": 63.881385803222656,
      "learning_rate": 2.719490686970362e-06,
      "loss": 0.807,
      "step": 19508
    },
    {
      "epoch": 7.5528455284552845,
      "grad_norm": 63.330322265625,
      "learning_rate": 2.719060523938573e-06,
      "loss": 0.2047,
      "step": 19509
    },
    {
      "epoch": 7.553232675183895,
      "grad_norm": 99.2628173828125,
      "learning_rate": 2.7186303609067836e-06,
      "loss": 0.6852,
      "step": 19510
    },
    {
      "epoch": 7.553619821912505,
      "grad_norm": 107.17140197753906,
      "learning_rate": 2.718200197874995e-06,
      "loss": 1.0749,
      "step": 19511
    },
    {
      "epoch": 7.554006968641115,
      "grad_norm": 4.4789557456970215,
      "learning_rate": 2.7177700348432056e-06,
      "loss": 0.0611,
      "step": 19512
    },
    {
      "epoch": 7.554394115369725,
      "grad_norm": 44.991943359375,
      "learning_rate": 2.717339871811417e-06,
      "loss": 1.7335,
      "step": 19513
    },
    {
      "epoch": 7.554781262098335,
      "grad_norm": 0.3120022416114807,
      "learning_rate": 2.7169097087796276e-06,
      "loss": 0.0088,
      "step": 19514
    },
    {
      "epoch": 7.555168408826946,
      "grad_norm": 16.43096351623535,
      "learning_rate": 2.7164795457478386e-06,
      "loss": 0.0881,
      "step": 19515
    },
    {
      "epoch": 7.555555555555555,
      "grad_norm": 132.16432189941406,
      "learning_rate": 2.7160493827160496e-06,
      "loss": 0.4802,
      "step": 19516
    },
    {
      "epoch": 7.555942702284166,
      "grad_norm": 1.0073707103729248,
      "learning_rate": 2.7156192196842606e-06,
      "loss": 0.0435,
      "step": 19517
    },
    {
      "epoch": 7.5563298490127755,
      "grad_norm": 0.9515972137451172,
      "learning_rate": 2.715189056652471e-06,
      "loss": 0.0274,
      "step": 19518
    },
    {
      "epoch": 7.556716995741386,
      "grad_norm": 30.869447708129883,
      "learning_rate": 2.7147588936206826e-06,
      "loss": 2.0738,
      "step": 19519
    },
    {
      "epoch": 7.5571041424699965,
      "grad_norm": 42.310279846191406,
      "learning_rate": 2.714328730588893e-06,
      "loss": 0.7558,
      "step": 19520
    },
    {
      "epoch": 7.557491289198606,
      "grad_norm": 37.106380462646484,
      "learning_rate": 2.7138985675571045e-06,
      "loss": 1.0428,
      "step": 19521
    },
    {
      "epoch": 7.557878435927217,
      "grad_norm": 5.214056015014648,
      "learning_rate": 2.713468404525315e-06,
      "loss": 0.041,
      "step": 19522
    },
    {
      "epoch": 7.558265582655826,
      "grad_norm": 156.40235900878906,
      "learning_rate": 2.7130382414935265e-06,
      "loss": 1.6191,
      "step": 19523
    },
    {
      "epoch": 7.558652729384437,
      "grad_norm": 1.3675658702850342,
      "learning_rate": 2.712608078461737e-06,
      "loss": 0.028,
      "step": 19524
    },
    {
      "epoch": 7.559039876113047,
      "grad_norm": 0.7971075773239136,
      "learning_rate": 2.712177915429948e-06,
      "loss": 0.034,
      "step": 19525
    },
    {
      "epoch": 7.559427022841657,
      "grad_norm": 102.49654388427734,
      "learning_rate": 2.711747752398159e-06,
      "loss": 0.3298,
      "step": 19526
    },
    {
      "epoch": 7.559814169570267,
      "grad_norm": 206.90286254882812,
      "learning_rate": 2.71131758936637e-06,
      "loss": 0.6171,
      "step": 19527
    },
    {
      "epoch": 7.560201316298877,
      "grad_norm": 79.98322296142578,
      "learning_rate": 2.7108874263345806e-06,
      "loss": 1.9341,
      "step": 19528
    },
    {
      "epoch": 7.5605884630274875,
      "grad_norm": 2.10163950920105,
      "learning_rate": 2.710457263302792e-06,
      "loss": 0.0717,
      "step": 19529
    },
    {
      "epoch": 7.560975609756097,
      "grad_norm": 1.4432307481765747,
      "learning_rate": 2.7100271002710026e-06,
      "loss": 0.0411,
      "step": 19530
    },
    {
      "epoch": 7.561362756484708,
      "grad_norm": 3.093350887298584,
      "learning_rate": 2.709596937239214e-06,
      "loss": 0.0571,
      "step": 19531
    },
    {
      "epoch": 7.561749903213318,
      "grad_norm": 16.25406265258789,
      "learning_rate": 2.7091667742074246e-06,
      "loss": 0.2363,
      "step": 19532
    },
    {
      "epoch": 7.562137049941928,
      "grad_norm": 80.75670623779297,
      "learning_rate": 2.7087366111756356e-06,
      "loss": 3.2638,
      "step": 19533
    },
    {
      "epoch": 7.562524196670538,
      "grad_norm": 155.03445434570312,
      "learning_rate": 2.7083064481438466e-06,
      "loss": 0.8489,
      "step": 19534
    },
    {
      "epoch": 7.562911343399148,
      "grad_norm": 5.257167816162109,
      "learning_rate": 2.7078762851120576e-06,
      "loss": 0.0603,
      "step": 19535
    },
    {
      "epoch": 7.5632984901277585,
      "grad_norm": 89.8910903930664,
      "learning_rate": 2.707446122080269e-06,
      "loss": 2.1493,
      "step": 19536
    },
    {
      "epoch": 7.563685636856368,
      "grad_norm": 127.20040893554688,
      "learning_rate": 2.7070159590484796e-06,
      "loss": 0.7842,
      "step": 19537
    },
    {
      "epoch": 7.564072783584979,
      "grad_norm": 141.25486755371094,
      "learning_rate": 2.706585796016691e-06,
      "loss": 0.9206,
      "step": 19538
    },
    {
      "epoch": 7.564459930313589,
      "grad_norm": 56.70915985107422,
      "learning_rate": 2.7061556329849016e-06,
      "loss": 0.7077,
      "step": 19539
    },
    {
      "epoch": 7.564847077042199,
      "grad_norm": 0.746865451335907,
      "learning_rate": 2.7057254699531125e-06,
      "loss": 0.0347,
      "step": 19540
    },
    {
      "epoch": 7.565234223770809,
      "grad_norm": 165.90731811523438,
      "learning_rate": 2.7052953069213235e-06,
      "loss": 2.8184,
      "step": 19541
    },
    {
      "epoch": 7.56562137049942,
      "grad_norm": 43.15428161621094,
      "learning_rate": 2.7048651438895345e-06,
      "loss": 1.9235,
      "step": 19542
    },
    {
      "epoch": 7.566008517228029,
      "grad_norm": 79.48628234863281,
      "learning_rate": 2.704434980857745e-06,
      "loss": 1.2364,
      "step": 19543
    },
    {
      "epoch": 7.56639566395664,
      "grad_norm": 124.75079345703125,
      "learning_rate": 2.7040048178259565e-06,
      "loss": 2.1082,
      "step": 19544
    },
    {
      "epoch": 7.5667828106852495,
      "grad_norm": 1.4932236671447754,
      "learning_rate": 2.703574654794167e-06,
      "loss": 0.0726,
      "step": 19545
    },
    {
      "epoch": 7.56716995741386,
      "grad_norm": 41.06035614013672,
      "learning_rate": 2.7031444917623785e-06,
      "loss": 1.3119,
      "step": 19546
    },
    {
      "epoch": 7.56755710414247,
      "grad_norm": 1.397465467453003,
      "learning_rate": 2.702714328730589e-06,
      "loss": 0.0584,
      "step": 19547
    },
    {
      "epoch": 7.56794425087108,
      "grad_norm": 51.0866584777832,
      "learning_rate": 2.7022841656988e-06,
      "loss": 0.3974,
      "step": 19548
    },
    {
      "epoch": 7.568331397599691,
      "grad_norm": 1.351772427558899,
      "learning_rate": 2.701854002667011e-06,
      "loss": 0.0389,
      "step": 19549
    },
    {
      "epoch": 7.5687185443283,
      "grad_norm": 11.016931533813477,
      "learning_rate": 2.701423839635222e-06,
      "loss": 0.2453,
      "step": 19550
    },
    {
      "epoch": 7.569105691056911,
      "grad_norm": 25.246341705322266,
      "learning_rate": 2.7009936766034326e-06,
      "loss": 1.4448,
      "step": 19551
    },
    {
      "epoch": 7.56949283778552,
      "grad_norm": 29.45286750793457,
      "learning_rate": 2.700563513571644e-06,
      "loss": 0.1156,
      "step": 19552
    },
    {
      "epoch": 7.569879984514131,
      "grad_norm": 3.737031936645508,
      "learning_rate": 2.7001333505398546e-06,
      "loss": 0.0854,
      "step": 19553
    },
    {
      "epoch": 7.5702671312427405,
      "grad_norm": 93.3041763305664,
      "learning_rate": 2.699703187508066e-06,
      "loss": 1.1513,
      "step": 19554
    },
    {
      "epoch": 7.570654277971351,
      "grad_norm": 149.63760375976562,
      "learning_rate": 2.6992730244762766e-06,
      "loss": 1.8624,
      "step": 19555
    },
    {
      "epoch": 7.5710414246999616,
      "grad_norm": 0.8751581907272339,
      "learning_rate": 2.698842861444488e-06,
      "loss": 0.038,
      "step": 19556
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 215.84027099609375,
      "learning_rate": 2.6984126984126986e-06,
      "loss": 1.4665,
      "step": 19557
    },
    {
      "epoch": 7.571815718157182,
      "grad_norm": 35.19997024536133,
      "learning_rate": 2.6979825353809096e-06,
      "loss": 0.2555,
      "step": 19558
    },
    {
      "epoch": 7.572202864885792,
      "grad_norm": 110.34888458251953,
      "learning_rate": 2.6975523723491206e-06,
      "loss": 3.827,
      "step": 19559
    },
    {
      "epoch": 7.572590011614402,
      "grad_norm": 10.302125930786133,
      "learning_rate": 2.6971222093173315e-06,
      "loss": 0.1752,
      "step": 19560
    },
    {
      "epoch": 7.572977158343012,
      "grad_norm": 145.90574645996094,
      "learning_rate": 2.696692046285542e-06,
      "loss": 0.3347,
      "step": 19561
    },
    {
      "epoch": 7.573364305071622,
      "grad_norm": 203.2946319580078,
      "learning_rate": 2.6962618832537535e-06,
      "loss": 2.6397,
      "step": 19562
    },
    {
      "epoch": 7.5737514518002325,
      "grad_norm": 3.736154794692993,
      "learning_rate": 2.695831720221964e-06,
      "loss": 0.1218,
      "step": 19563
    },
    {
      "epoch": 7.574138598528842,
      "grad_norm": 50.17854690551758,
      "learning_rate": 2.6954015571901755e-06,
      "loss": 2.2486,
      "step": 19564
    },
    {
      "epoch": 7.574525745257453,
      "grad_norm": 101.24369812011719,
      "learning_rate": 2.694971394158386e-06,
      "loss": 3.7458,
      "step": 19565
    },
    {
      "epoch": 7.574912891986063,
      "grad_norm": 13.840483665466309,
      "learning_rate": 2.694541231126597e-06,
      "loss": 0.179,
      "step": 19566
    },
    {
      "epoch": 7.575300038714673,
      "grad_norm": 40.18803405761719,
      "learning_rate": 2.694111068094808e-06,
      "loss": 0.451,
      "step": 19567
    },
    {
      "epoch": 7.575687185443283,
      "grad_norm": 118.2728500366211,
      "learning_rate": 2.693680905063019e-06,
      "loss": 0.8592,
      "step": 19568
    },
    {
      "epoch": 7.576074332171893,
      "grad_norm": 1.049035668373108,
      "learning_rate": 2.6932507420312296e-06,
      "loss": 0.0368,
      "step": 19569
    },
    {
      "epoch": 7.576461478900503,
      "grad_norm": 14.73154354095459,
      "learning_rate": 2.692820578999441e-06,
      "loss": 0.2269,
      "step": 19570
    },
    {
      "epoch": 7.576848625629113,
      "grad_norm": 8.540778160095215,
      "learning_rate": 2.6923904159676516e-06,
      "loss": 0.2197,
      "step": 19571
    },
    {
      "epoch": 7.5772357723577235,
      "grad_norm": 10.912725448608398,
      "learning_rate": 2.691960252935863e-06,
      "loss": 0.034,
      "step": 19572
    },
    {
      "epoch": 7.577622919086334,
      "grad_norm": 0.24183355271816254,
      "learning_rate": 2.6915300899040736e-06,
      "loss": 0.0065,
      "step": 19573
    },
    {
      "epoch": 7.578010065814944,
      "grad_norm": 1.681106448173523,
      "learning_rate": 2.691099926872285e-06,
      "loss": 0.0315,
      "step": 19574
    },
    {
      "epoch": 7.578397212543554,
      "grad_norm": 86.42398834228516,
      "learning_rate": 2.6906697638404956e-06,
      "loss": 0.5603,
      "step": 19575
    },
    {
      "epoch": 7.578784359272164,
      "grad_norm": 89.55277252197266,
      "learning_rate": 2.6902396008087066e-06,
      "loss": 2.3486,
      "step": 19576
    },
    {
      "epoch": 7.579171506000774,
      "grad_norm": 28.797029495239258,
      "learning_rate": 2.689809437776918e-06,
      "loss": 1.1747,
      "step": 19577
    },
    {
      "epoch": 7.579558652729385,
      "grad_norm": 23.837108612060547,
      "learning_rate": 2.6893792747451286e-06,
      "loss": 1.7283,
      "step": 19578
    },
    {
      "epoch": 7.579945799457994,
      "grad_norm": 78.2532958984375,
      "learning_rate": 2.68894911171334e-06,
      "loss": 0.9762,
      "step": 19579
    },
    {
      "epoch": 7.580332946186605,
      "grad_norm": 24.702428817749023,
      "learning_rate": 2.6885189486815505e-06,
      "loss": 0.7164,
      "step": 19580
    },
    {
      "epoch": 7.5807200929152145,
      "grad_norm": 7.823390960693359,
      "learning_rate": 2.6880887856497615e-06,
      "loss": 0.1168,
      "step": 19581
    },
    {
      "epoch": 7.581107239643825,
      "grad_norm": 3.6157596111297607,
      "learning_rate": 2.6876586226179725e-06,
      "loss": 0.0431,
      "step": 19582
    },
    {
      "epoch": 7.581494386372436,
      "grad_norm": 115.77967834472656,
      "learning_rate": 2.6872284595861835e-06,
      "loss": 1.4086,
      "step": 19583
    },
    {
      "epoch": 7.581881533101045,
      "grad_norm": 79.35472869873047,
      "learning_rate": 2.686798296554394e-06,
      "loss": 1.2167,
      "step": 19584
    },
    {
      "epoch": 7.582268679829656,
      "grad_norm": 43.08088684082031,
      "learning_rate": 2.6863681335226055e-06,
      "loss": 0.3426,
      "step": 19585
    },
    {
      "epoch": 7.582655826558265,
      "grad_norm": 76.7692642211914,
      "learning_rate": 2.685937970490816e-06,
      "loss": 1.1398,
      "step": 19586
    },
    {
      "epoch": 7.583042973286876,
      "grad_norm": 109.11922454833984,
      "learning_rate": 2.6855078074590275e-06,
      "loss": 3.494,
      "step": 19587
    },
    {
      "epoch": 7.583430120015485,
      "grad_norm": 29.912385940551758,
      "learning_rate": 2.685077644427238e-06,
      "loss": 0.3234,
      "step": 19588
    },
    {
      "epoch": 7.583817266744096,
      "grad_norm": 16.826574325561523,
      "learning_rate": 2.6846474813954495e-06,
      "loss": 0.1266,
      "step": 19589
    },
    {
      "epoch": 7.5842044134727065,
      "grad_norm": 155.8328094482422,
      "learning_rate": 2.68421731836366e-06,
      "loss": 1.9435,
      "step": 19590
    },
    {
      "epoch": 7.584591560201316,
      "grad_norm": 52.53033447265625,
      "learning_rate": 2.683787155331871e-06,
      "loss": 0.4492,
      "step": 19591
    },
    {
      "epoch": 7.584978706929927,
      "grad_norm": 4.803460121154785,
      "learning_rate": 2.683356992300082e-06,
      "loss": 0.0484,
      "step": 19592
    },
    {
      "epoch": 7.585365853658536,
      "grad_norm": 220.62237548828125,
      "learning_rate": 2.682926829268293e-06,
      "loss": 0.7083,
      "step": 19593
    },
    {
      "epoch": 7.585753000387147,
      "grad_norm": 112.87324523925781,
      "learning_rate": 2.6824966662365036e-06,
      "loss": 0.3088,
      "step": 19594
    },
    {
      "epoch": 7.586140147115757,
      "grad_norm": 220.1155548095703,
      "learning_rate": 2.682066503204715e-06,
      "loss": 2.1662,
      "step": 19595
    },
    {
      "epoch": 7.586527293844367,
      "grad_norm": 6.666286945343018,
      "learning_rate": 2.6816363401729256e-06,
      "loss": 0.1448,
      "step": 19596
    },
    {
      "epoch": 7.586914440572977,
      "grad_norm": 27.446468353271484,
      "learning_rate": 2.681206177141137e-06,
      "loss": 2.1805,
      "step": 19597
    },
    {
      "epoch": 7.587301587301587,
      "grad_norm": 29.540462493896484,
      "learning_rate": 2.6807760141093476e-06,
      "loss": 3.4499,
      "step": 19598
    },
    {
      "epoch": 7.5876887340301975,
      "grad_norm": 0.2373020350933075,
      "learning_rate": 2.6803458510775585e-06,
      "loss": 0.0062,
      "step": 19599
    },
    {
      "epoch": 7.588075880758808,
      "grad_norm": 255.7121124267578,
      "learning_rate": 2.6799156880457695e-06,
      "loss": 1.629,
      "step": 19600
    },
    {
      "epoch": 7.588463027487418,
      "grad_norm": 180.94119262695312,
      "learning_rate": 2.6794855250139805e-06,
      "loss": 1.959,
      "step": 19601
    },
    {
      "epoch": 7.588850174216028,
      "grad_norm": 38.653568267822266,
      "learning_rate": 2.679055361982191e-06,
      "loss": 0.3147,
      "step": 19602
    },
    {
      "epoch": 7.589237320944638,
      "grad_norm": 49.10684585571289,
      "learning_rate": 2.6786251989504025e-06,
      "loss": 0.2179,
      "step": 19603
    },
    {
      "epoch": 7.589624467673248,
      "grad_norm": 37.2220458984375,
      "learning_rate": 2.678195035918613e-06,
      "loss": 0.2848,
      "step": 19604
    },
    {
      "epoch": 7.590011614401858,
      "grad_norm": 94.7375717163086,
      "learning_rate": 2.6777648728868245e-06,
      "loss": 1.351,
      "step": 19605
    },
    {
      "epoch": 7.590398761130468,
      "grad_norm": 133.79466247558594,
      "learning_rate": 2.677334709855035e-06,
      "loss": 1.3503,
      "step": 19606
    },
    {
      "epoch": 7.590785907859079,
      "grad_norm": 30.7752742767334,
      "learning_rate": 2.6769045468232465e-06,
      "loss": 2.2568,
      "step": 19607
    },
    {
      "epoch": 7.5911730545876885,
      "grad_norm": 31.5694522857666,
      "learning_rate": 2.676474383791457e-06,
      "loss": 4.7834,
      "step": 19608
    },
    {
      "epoch": 7.591560201316299,
      "grad_norm": 27.807573318481445,
      "learning_rate": 2.676044220759668e-06,
      "loss": 1.7757,
      "step": 19609
    },
    {
      "epoch": 7.591947348044909,
      "grad_norm": 99.34320068359375,
      "learning_rate": 2.675614057727879e-06,
      "loss": 0.7537,
      "step": 19610
    },
    {
      "epoch": 7.592334494773519,
      "grad_norm": 6.731624126434326,
      "learning_rate": 2.67518389469609e-06,
      "loss": 0.0458,
      "step": 19611
    },
    {
      "epoch": 7.59272164150213,
      "grad_norm": 5.882907867431641,
      "learning_rate": 2.6747537316643006e-06,
      "loss": 0.093,
      "step": 19612
    },
    {
      "epoch": 7.593108788230739,
      "grad_norm": 77.62406158447266,
      "learning_rate": 2.674323568632512e-06,
      "loss": 0.582,
      "step": 19613
    },
    {
      "epoch": 7.59349593495935,
      "grad_norm": 111.04692840576172,
      "learning_rate": 2.6738934056007226e-06,
      "loss": 0.719,
      "step": 19614
    },
    {
      "epoch": 7.5938830816879594,
      "grad_norm": 29.89431381225586,
      "learning_rate": 2.673463242568934e-06,
      "loss": 0.1109,
      "step": 19615
    },
    {
      "epoch": 7.59427022841657,
      "grad_norm": 32.27998352050781,
      "learning_rate": 2.6730330795371446e-06,
      "loss": 0.1598,
      "step": 19616
    },
    {
      "epoch": 7.5946573751451805,
      "grad_norm": 3.6400704383850098,
      "learning_rate": 2.6726029165053556e-06,
      "loss": 0.1555,
      "step": 19617
    },
    {
      "epoch": 7.59504452187379,
      "grad_norm": 132.4791717529297,
      "learning_rate": 2.672172753473567e-06,
      "loss": 0.9319,
      "step": 19618
    },
    {
      "epoch": 7.595431668602401,
      "grad_norm": 98.33629608154297,
      "learning_rate": 2.6717425904417775e-06,
      "loss": 1.0495,
      "step": 19619
    },
    {
      "epoch": 7.59581881533101,
      "grad_norm": 124.67723083496094,
      "learning_rate": 2.671312427409989e-06,
      "loss": 0.8469,
      "step": 19620
    },
    {
      "epoch": 7.596205962059621,
      "grad_norm": 85.3383560180664,
      "learning_rate": 2.6708822643781995e-06,
      "loss": 1.2889,
      "step": 19621
    },
    {
      "epoch": 7.59659310878823,
      "grad_norm": 1.7427223920822144,
      "learning_rate": 2.670452101346411e-06,
      "loss": 0.0289,
      "step": 19622
    },
    {
      "epoch": 7.596980255516841,
      "grad_norm": 115.66781616210938,
      "learning_rate": 2.6700219383146215e-06,
      "loss": 0.7955,
      "step": 19623
    },
    {
      "epoch": 7.597367402245451,
      "grad_norm": 0.24992504715919495,
      "learning_rate": 2.6695917752828325e-06,
      "loss": 0.0061,
      "step": 19624
    },
    {
      "epoch": 7.597754548974061,
      "grad_norm": 25.503402709960938,
      "learning_rate": 2.6691616122510435e-06,
      "loss": 1.4402,
      "step": 19625
    },
    {
      "epoch": 7.5981416957026715,
      "grad_norm": 30.43878173828125,
      "learning_rate": 2.6687314492192545e-06,
      "loss": 0.1117,
      "step": 19626
    },
    {
      "epoch": 7.598528842431281,
      "grad_norm": 155.25827026367188,
      "learning_rate": 2.668301286187465e-06,
      "loss": 1.0944,
      "step": 19627
    },
    {
      "epoch": 7.598915989159892,
      "grad_norm": 36.70843505859375,
      "learning_rate": 2.6678711231556765e-06,
      "loss": 0.1483,
      "step": 19628
    },
    {
      "epoch": 7.599303135888501,
      "grad_norm": 95.48306274414062,
      "learning_rate": 2.667440960123887e-06,
      "loss": 1.091,
      "step": 19629
    },
    {
      "epoch": 7.599690282617112,
      "grad_norm": 0.9645459055900574,
      "learning_rate": 2.6670107970920985e-06,
      "loss": 0.028,
      "step": 19630
    },
    {
      "epoch": 7.600077429345722,
      "grad_norm": 113.84915161132812,
      "learning_rate": 2.666580634060309e-06,
      "loss": 3.2323,
      "step": 19631
    },
    {
      "epoch": 7.600464576074332,
      "grad_norm": 267.9242248535156,
      "learning_rate": 2.66615047102852e-06,
      "loss": 2.301,
      "step": 19632
    },
    {
      "epoch": 7.600851722802942,
      "grad_norm": 138.58412170410156,
      "learning_rate": 2.665720307996731e-06,
      "loss": 0.484,
      "step": 19633
    },
    {
      "epoch": 7.601238869531553,
      "grad_norm": 85.52418518066406,
      "learning_rate": 2.665290144964942e-06,
      "loss": 0.6884,
      "step": 19634
    },
    {
      "epoch": 7.6016260162601625,
      "grad_norm": 249.08316040039062,
      "learning_rate": 2.6648599819331526e-06,
      "loss": 2.0,
      "step": 19635
    },
    {
      "epoch": 7.602013162988773,
      "grad_norm": 1.7817399501800537,
      "learning_rate": 2.664429818901364e-06,
      "loss": 0.0586,
      "step": 19636
    },
    {
      "epoch": 7.602400309717383,
      "grad_norm": 174.82228088378906,
      "learning_rate": 2.6639996558695746e-06,
      "loss": 2.6956,
      "step": 19637
    },
    {
      "epoch": 7.602787456445993,
      "grad_norm": 324.0508728027344,
      "learning_rate": 2.663569492837786e-06,
      "loss": 0.5789,
      "step": 19638
    },
    {
      "epoch": 7.603174603174603,
      "grad_norm": 101.9309310913086,
      "learning_rate": 2.6631393298059965e-06,
      "loss": 0.3624,
      "step": 19639
    },
    {
      "epoch": 7.603561749903213,
      "grad_norm": 303.9513854980469,
      "learning_rate": 2.662709166774208e-06,
      "loss": 2.9051,
      "step": 19640
    },
    {
      "epoch": 7.603948896631824,
      "grad_norm": 39.01936340332031,
      "learning_rate": 2.6622790037424185e-06,
      "loss": 1.6819,
      "step": 19641
    },
    {
      "epoch": 7.6043360433604335,
      "grad_norm": 86.5322036743164,
      "learning_rate": 2.6618488407106295e-06,
      "loss": 2.5691,
      "step": 19642
    },
    {
      "epoch": 7.604723190089044,
      "grad_norm": 101.52819061279297,
      "learning_rate": 2.6614186776788405e-06,
      "loss": 2.6974,
      "step": 19643
    },
    {
      "epoch": 7.605110336817654,
      "grad_norm": 127.7264175415039,
      "learning_rate": 2.6609885146470515e-06,
      "loss": 0.5242,
      "step": 19644
    },
    {
      "epoch": 7.605497483546264,
      "grad_norm": 10.698125839233398,
      "learning_rate": 2.660558351615262e-06,
      "loss": 0.1372,
      "step": 19645
    },
    {
      "epoch": 7.605884630274874,
      "grad_norm": 106.89244079589844,
      "learning_rate": 2.6601281885834735e-06,
      "loss": 2.642,
      "step": 19646
    },
    {
      "epoch": 7.606271777003484,
      "grad_norm": 1.3299356698989868,
      "learning_rate": 2.659698025551684e-06,
      "loss": 0.0667,
      "step": 19647
    },
    {
      "epoch": 7.606658923732095,
      "grad_norm": 75.26726531982422,
      "learning_rate": 2.6592678625198955e-06,
      "loss": 1.9351,
      "step": 19648
    },
    {
      "epoch": 7.607046070460704,
      "grad_norm": 93.97222137451172,
      "learning_rate": 2.658837699488106e-06,
      "loss": 1.4546,
      "step": 19649
    },
    {
      "epoch": 7.607433217189315,
      "grad_norm": 0.8259429335594177,
      "learning_rate": 2.658407536456317e-06,
      "loss": 0.036,
      "step": 19650
    },
    {
      "epoch": 7.607820363917925,
      "grad_norm": 15.34428596496582,
      "learning_rate": 2.657977373424528e-06,
      "loss": 0.1166,
      "step": 19651
    },
    {
      "epoch": 7.608207510646535,
      "grad_norm": 11.589128494262695,
      "learning_rate": 2.657547210392739e-06,
      "loss": 0.1085,
      "step": 19652
    },
    {
      "epoch": 7.6085946573751455,
      "grad_norm": 136.14187622070312,
      "learning_rate": 2.6571170473609496e-06,
      "loss": 4.4209,
      "step": 19653
    },
    {
      "epoch": 7.608981804103755,
      "grad_norm": 207.7733917236328,
      "learning_rate": 2.656686884329161e-06,
      "loss": 2.4363,
      "step": 19654
    },
    {
      "epoch": 7.609368950832366,
      "grad_norm": 25.613401412963867,
      "learning_rate": 2.6562567212973716e-06,
      "loss": 2.6264,
      "step": 19655
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 10.963842391967773,
      "learning_rate": 2.655826558265583e-06,
      "loss": 0.1049,
      "step": 19656
    },
    {
      "epoch": 7.610143244289586,
      "grad_norm": 60.51484298706055,
      "learning_rate": 2.6553963952337936e-06,
      "loss": 1.9364,
      "step": 19657
    },
    {
      "epoch": 7.610530391018196,
      "grad_norm": 100.27253723144531,
      "learning_rate": 2.654966232202005e-06,
      "loss": 3.2232,
      "step": 19658
    },
    {
      "epoch": 7.610917537746806,
      "grad_norm": 39.753379821777344,
      "learning_rate": 2.654536069170216e-06,
      "loss": 1.7072,
      "step": 19659
    },
    {
      "epoch": 7.611304684475416,
      "grad_norm": 78.40563201904297,
      "learning_rate": 2.6541059061384265e-06,
      "loss": 1.1099,
      "step": 19660
    },
    {
      "epoch": 7.611691831204026,
      "grad_norm": 59.305198669433594,
      "learning_rate": 2.653675743106638e-06,
      "loss": 0.5181,
      "step": 19661
    },
    {
      "epoch": 7.6120789779326365,
      "grad_norm": 104.30419921875,
      "learning_rate": 2.6532455800748485e-06,
      "loss": 1.9359,
      "step": 19662
    },
    {
      "epoch": 7.612466124661246,
      "grad_norm": 131.54812622070312,
      "learning_rate": 2.65281541704306e-06,
      "loss": 0.726,
      "step": 19663
    },
    {
      "epoch": 7.612853271389857,
      "grad_norm": 1.7652257680892944,
      "learning_rate": 2.6523852540112705e-06,
      "loss": 0.0569,
      "step": 19664
    },
    {
      "epoch": 7.613240418118467,
      "grad_norm": 17.41352653503418,
      "learning_rate": 2.6519550909794815e-06,
      "loss": 0.2024,
      "step": 19665
    },
    {
      "epoch": 7.613627564847077,
      "grad_norm": 2.5509326457977295,
      "learning_rate": 2.6515249279476925e-06,
      "loss": 0.1051,
      "step": 19666
    },
    {
      "epoch": 7.614014711575687,
      "grad_norm": 109.89544677734375,
      "learning_rate": 2.6510947649159035e-06,
      "loss": 1.3567,
      "step": 19667
    },
    {
      "epoch": 7.614401858304297,
      "grad_norm": 1.3712886571884155,
      "learning_rate": 2.650664601884114e-06,
      "loss": 0.0505,
      "step": 19668
    },
    {
      "epoch": 7.6147890050329075,
      "grad_norm": 21.02235221862793,
      "learning_rate": 2.6502344388523255e-06,
      "loss": 0.5245,
      "step": 19669
    },
    {
      "epoch": 7.615176151761518,
      "grad_norm": 26.552898406982422,
      "learning_rate": 2.649804275820536e-06,
      "loss": 0.1884,
      "step": 19670
    },
    {
      "epoch": 7.615563298490128,
      "grad_norm": 112.44430541992188,
      "learning_rate": 2.6493741127887474e-06,
      "loss": 1.0419,
      "step": 19671
    },
    {
      "epoch": 7.615950445218738,
      "grad_norm": 188.04058837890625,
      "learning_rate": 2.648943949756958e-06,
      "loss": 2.1631,
      "step": 19672
    },
    {
      "epoch": 7.616337591947348,
      "grad_norm": 2.4863297939300537,
      "learning_rate": 2.6485137867251694e-06,
      "loss": 0.1106,
      "step": 19673
    },
    {
      "epoch": 7.616724738675958,
      "grad_norm": 25.160295486450195,
      "learning_rate": 2.64808362369338e-06,
      "loss": 1.264,
      "step": 19674
    },
    {
      "epoch": 7.617111885404569,
      "grad_norm": 3.004920721054077,
      "learning_rate": 2.647653460661591e-06,
      "loss": 0.123,
      "step": 19675
    },
    {
      "epoch": 7.617499032133178,
      "grad_norm": 118.4071044921875,
      "learning_rate": 2.647223297629802e-06,
      "loss": 2.2024,
      "step": 19676
    },
    {
      "epoch": 7.617886178861789,
      "grad_norm": 60.8548698425293,
      "learning_rate": 2.646793134598013e-06,
      "loss": 1.6164,
      "step": 19677
    },
    {
      "epoch": 7.6182733255903985,
      "grad_norm": 59.29731369018555,
      "learning_rate": 2.6463629715662235e-06,
      "loss": 0.8539,
      "step": 19678
    },
    {
      "epoch": 7.618660472319009,
      "grad_norm": 93.91876220703125,
      "learning_rate": 2.645932808534435e-06,
      "loss": 0.3595,
      "step": 19679
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 172.7183837890625,
      "learning_rate": 2.6455026455026455e-06,
      "loss": 4.0679,
      "step": 19680
    },
    {
      "epoch": 7.619434765776229,
      "grad_norm": 79.64353942871094,
      "learning_rate": 2.645072482470857e-06,
      "loss": 1.7167,
      "step": 19681
    },
    {
      "epoch": 7.61982191250484,
      "grad_norm": 79.09410858154297,
      "learning_rate": 2.6446423194390675e-06,
      "loss": 1.0375,
      "step": 19682
    },
    {
      "epoch": 7.620209059233449,
      "grad_norm": 48.208194732666016,
      "learning_rate": 2.6442121564072785e-06,
      "loss": 1.302,
      "step": 19683
    },
    {
      "epoch": 7.62059620596206,
      "grad_norm": 0.2631237208843231,
      "learning_rate": 2.6437819933754895e-06,
      "loss": 0.0071,
      "step": 19684
    },
    {
      "epoch": 7.620983352690669,
      "grad_norm": 112.67185974121094,
      "learning_rate": 2.6433518303437005e-06,
      "loss": 1.2251,
      "step": 19685
    },
    {
      "epoch": 7.62137049941928,
      "grad_norm": 83.01891326904297,
      "learning_rate": 2.642921667311911e-06,
      "loss": 0.4887,
      "step": 19686
    },
    {
      "epoch": 7.62175764614789,
      "grad_norm": 62.74169921875,
      "learning_rate": 2.6424915042801225e-06,
      "loss": 0.7488,
      "step": 19687
    },
    {
      "epoch": 7.6221447928765,
      "grad_norm": 7.411532878875732,
      "learning_rate": 2.642061341248333e-06,
      "loss": 0.0256,
      "step": 19688
    },
    {
      "epoch": 7.6225319396051106,
      "grad_norm": 0.7242648601531982,
      "learning_rate": 2.6416311782165445e-06,
      "loss": 0.0156,
      "step": 19689
    },
    {
      "epoch": 7.62291908633372,
      "grad_norm": 1.2811927795410156,
      "learning_rate": 2.641201015184755e-06,
      "loss": 0.0433,
      "step": 19690
    },
    {
      "epoch": 7.623306233062331,
      "grad_norm": 98.80635833740234,
      "learning_rate": 2.6407708521529664e-06,
      "loss": 0.5303,
      "step": 19691
    },
    {
      "epoch": 7.623693379790941,
      "grad_norm": 11.39009952545166,
      "learning_rate": 2.640340689121177e-06,
      "loss": 0.1217,
      "step": 19692
    },
    {
      "epoch": 7.624080526519551,
      "grad_norm": 78.09617614746094,
      "learning_rate": 2.639910526089388e-06,
      "loss": 2.1257,
      "step": 19693
    },
    {
      "epoch": 7.624467673248161,
      "grad_norm": 4.965622901916504,
      "learning_rate": 2.639480363057599e-06,
      "loss": 0.0776,
      "step": 19694
    },
    {
      "epoch": 7.624854819976771,
      "grad_norm": 107.90635681152344,
      "learning_rate": 2.63905020002581e-06,
      "loss": 1.4652,
      "step": 19695
    },
    {
      "epoch": 7.6252419667053815,
      "grad_norm": 31.902130126953125,
      "learning_rate": 2.6386200369940206e-06,
      "loss": 0.4897,
      "step": 19696
    },
    {
      "epoch": 7.625629113433991,
      "grad_norm": 38.81056213378906,
      "learning_rate": 2.638189873962232e-06,
      "loss": 1.5371,
      "step": 19697
    },
    {
      "epoch": 7.626016260162602,
      "grad_norm": 138.13540649414062,
      "learning_rate": 2.6377597109304425e-06,
      "loss": 0.265,
      "step": 19698
    },
    {
      "epoch": 7.626403406891212,
      "grad_norm": 41.80864715576172,
      "learning_rate": 2.637329547898654e-06,
      "loss": 1.1692,
      "step": 19699
    },
    {
      "epoch": 7.626790553619822,
      "grad_norm": 100.79273986816406,
      "learning_rate": 2.636899384866865e-06,
      "loss": 1.6904,
      "step": 19700
    },
    {
      "epoch": 7.627177700348432,
      "grad_norm": 115.46341705322266,
      "learning_rate": 2.6364692218350755e-06,
      "loss": 1.148,
      "step": 19701
    },
    {
      "epoch": 7.627564847077042,
      "grad_norm": 1.6461797952651978,
      "learning_rate": 2.636039058803287e-06,
      "loss": 0.062,
      "step": 19702
    },
    {
      "epoch": 7.627951993805652,
      "grad_norm": 122.48402404785156,
      "learning_rate": 2.6356088957714975e-06,
      "loss": 0.8893,
      "step": 19703
    },
    {
      "epoch": 7.628339140534262,
      "grad_norm": 202.9234619140625,
      "learning_rate": 2.635178732739709e-06,
      "loss": 2.4237,
      "step": 19704
    },
    {
      "epoch": 7.6287262872628725,
      "grad_norm": 122.41683197021484,
      "learning_rate": 2.6347485697079195e-06,
      "loss": 0.6931,
      "step": 19705
    },
    {
      "epoch": 7.629113433991483,
      "grad_norm": 90.93421173095703,
      "learning_rate": 2.634318406676131e-06,
      "loss": 1.7649,
      "step": 19706
    },
    {
      "epoch": 7.629500580720093,
      "grad_norm": 1.1916909217834473,
      "learning_rate": 2.6338882436443415e-06,
      "loss": 0.027,
      "step": 19707
    },
    {
      "epoch": 7.629887727448703,
      "grad_norm": 25.209749221801758,
      "learning_rate": 2.6334580806125525e-06,
      "loss": 2.9647,
      "step": 19708
    },
    {
      "epoch": 7.630274874177314,
      "grad_norm": 4.482353687286377,
      "learning_rate": 2.6330279175807635e-06,
      "loss": 0.0408,
      "step": 19709
    },
    {
      "epoch": 7.630662020905923,
      "grad_norm": 47.617549896240234,
      "learning_rate": 2.6325977545489744e-06,
      "loss": 1.4629,
      "step": 19710
    },
    {
      "epoch": 7.631049167634534,
      "grad_norm": 17.768341064453125,
      "learning_rate": 2.632167591517185e-06,
      "loss": 0.2462,
      "step": 19711
    },
    {
      "epoch": 7.631436314363143,
      "grad_norm": 39.398460388183594,
      "learning_rate": 2.6317374284853964e-06,
      "loss": 2.037,
      "step": 19712
    },
    {
      "epoch": 7.631823461091754,
      "grad_norm": 5.084169864654541,
      "learning_rate": 2.631307265453607e-06,
      "loss": 0.0537,
      "step": 19713
    },
    {
      "epoch": 7.6322106078203635,
      "grad_norm": 0.9092472195625305,
      "learning_rate": 2.6308771024218184e-06,
      "loss": 0.0258,
      "step": 19714
    },
    {
      "epoch": 7.632597754548974,
      "grad_norm": 1.7028695344924927,
      "learning_rate": 2.630446939390029e-06,
      "loss": 0.0621,
      "step": 19715
    },
    {
      "epoch": 7.6329849012775846,
      "grad_norm": 10.550552368164062,
      "learning_rate": 2.63001677635824e-06,
      "loss": 0.0779,
      "step": 19716
    },
    {
      "epoch": 7.633372048006194,
      "grad_norm": 86.31706237792969,
      "learning_rate": 2.629586613326451e-06,
      "loss": 1.1947,
      "step": 19717
    },
    {
      "epoch": 7.633759194734805,
      "grad_norm": 114.48402404785156,
      "learning_rate": 2.629156450294662e-06,
      "loss": 3.8479,
      "step": 19718
    },
    {
      "epoch": 7.634146341463414,
      "grad_norm": 87.65032196044922,
      "learning_rate": 2.6287262872628725e-06,
      "loss": 4.1048,
      "step": 19719
    },
    {
      "epoch": 7.634533488192025,
      "grad_norm": 17.605796813964844,
      "learning_rate": 2.628296124231084e-06,
      "loss": 0.4149,
      "step": 19720
    },
    {
      "epoch": 7.634920634920634,
      "grad_norm": 80.07810974121094,
      "learning_rate": 2.6278659611992945e-06,
      "loss": 1.8848,
      "step": 19721
    },
    {
      "epoch": 7.635307781649245,
      "grad_norm": 47.031715393066406,
      "learning_rate": 2.627435798167506e-06,
      "loss": 1.8758,
      "step": 19722
    },
    {
      "epoch": 7.6356949283778555,
      "grad_norm": 98.00362396240234,
      "learning_rate": 2.6270056351357165e-06,
      "loss": 0.9553,
      "step": 19723
    },
    {
      "epoch": 7.636082075106465,
      "grad_norm": 153.0751190185547,
      "learning_rate": 2.626575472103928e-06,
      "loss": 0.5664,
      "step": 19724
    },
    {
      "epoch": 7.636469221835076,
      "grad_norm": 2.591214895248413,
      "learning_rate": 2.6261453090721385e-06,
      "loss": 0.0646,
      "step": 19725
    },
    {
      "epoch": 7.636856368563686,
      "grad_norm": 283.0400390625,
      "learning_rate": 2.6257151460403495e-06,
      "loss": 2.0917,
      "step": 19726
    },
    {
      "epoch": 7.637243515292296,
      "grad_norm": 115.50224304199219,
      "learning_rate": 2.6252849830085605e-06,
      "loss": 0.9236,
      "step": 19727
    },
    {
      "epoch": 7.637630662020906,
      "grad_norm": 3.8267741203308105,
      "learning_rate": 2.6248548199767715e-06,
      "loss": 0.0576,
      "step": 19728
    },
    {
      "epoch": 7.638017808749516,
      "grad_norm": 41.36357879638672,
      "learning_rate": 2.624424656944982e-06,
      "loss": 0.2871,
      "step": 19729
    },
    {
      "epoch": 7.638404955478126,
      "grad_norm": 57.91990280151367,
      "learning_rate": 2.6239944939131934e-06,
      "loss": 1.5858,
      "step": 19730
    },
    {
      "epoch": 7.638792102206736,
      "grad_norm": 25.796815872192383,
      "learning_rate": 2.623564330881404e-06,
      "loss": 3.8346,
      "step": 19731
    },
    {
      "epoch": 7.6391792489353465,
      "grad_norm": 6.666734218597412,
      "learning_rate": 2.6231341678496154e-06,
      "loss": 0.1534,
      "step": 19732
    },
    {
      "epoch": 7.639566395663957,
      "grad_norm": 1.597562551498413,
      "learning_rate": 2.622704004817826e-06,
      "loss": 0.0348,
      "step": 19733
    },
    {
      "epoch": 7.639953542392567,
      "grad_norm": 4.431304931640625,
      "learning_rate": 2.622273841786037e-06,
      "loss": 0.1706,
      "step": 19734
    },
    {
      "epoch": 7.640340689121177,
      "grad_norm": 3.329477310180664,
      "learning_rate": 2.621843678754248e-06,
      "loss": 0.0688,
      "step": 19735
    },
    {
      "epoch": 7.640727835849787,
      "grad_norm": 21.66156005859375,
      "learning_rate": 2.621413515722459e-06,
      "loss": 2.345,
      "step": 19736
    },
    {
      "epoch": 7.641114982578397,
      "grad_norm": 23.577308654785156,
      "learning_rate": 2.6209833526906695e-06,
      "loss": 0.3805,
      "step": 19737
    },
    {
      "epoch": 7.641502129307007,
      "grad_norm": 32.419490814208984,
      "learning_rate": 2.620553189658881e-06,
      "loss": 2.0535,
      "step": 19738
    },
    {
      "epoch": 7.641889276035617,
      "grad_norm": 40.864830017089844,
      "learning_rate": 2.6201230266270915e-06,
      "loss": 2.3421,
      "step": 19739
    },
    {
      "epoch": 7.642276422764228,
      "grad_norm": 36.39202117919922,
      "learning_rate": 2.619692863595303e-06,
      "loss": 1.5019,
      "step": 19740
    },
    {
      "epoch": 7.6426635694928375,
      "grad_norm": 40.931827545166016,
      "learning_rate": 2.619262700563514e-06,
      "loss": 0.3499,
      "step": 19741
    },
    {
      "epoch": 7.643050716221448,
      "grad_norm": 136.60867309570312,
      "learning_rate": 2.618832537531725e-06,
      "loss": 1.2157,
      "step": 19742
    },
    {
      "epoch": 7.643437862950059,
      "grad_norm": 219.39999389648438,
      "learning_rate": 2.618402374499936e-06,
      "loss": 1.2603,
      "step": 19743
    },
    {
      "epoch": 7.643825009678668,
      "grad_norm": 40.82986831665039,
      "learning_rate": 2.6179722114681465e-06,
      "loss": 1.8129,
      "step": 19744
    },
    {
      "epoch": 7.644212156407279,
      "grad_norm": 4.455231189727783,
      "learning_rate": 2.617542048436358e-06,
      "loss": 0.1535,
      "step": 19745
    },
    {
      "epoch": 7.644599303135888,
      "grad_norm": 6.716268539428711,
      "learning_rate": 2.6171118854045685e-06,
      "loss": 0.1573,
      "step": 19746
    },
    {
      "epoch": 7.644986449864499,
      "grad_norm": 6.592199802398682,
      "learning_rate": 2.61668172237278e-06,
      "loss": 0.0818,
      "step": 19747
    },
    {
      "epoch": 7.6453735965931084,
      "grad_norm": 223.73904418945312,
      "learning_rate": 2.6162515593409905e-06,
      "loss": 1.361,
      "step": 19748
    },
    {
      "epoch": 7.645760743321719,
      "grad_norm": 189.25833129882812,
      "learning_rate": 2.6158213963092015e-06,
      "loss": 2.6286,
      "step": 19749
    },
    {
      "epoch": 7.6461478900503295,
      "grad_norm": 2.2015113830566406,
      "learning_rate": 2.6153912332774124e-06,
      "loss": 0.0591,
      "step": 19750
    },
    {
      "epoch": 7.646535036778939,
      "grad_norm": 17.462875366210938,
      "learning_rate": 2.6149610702456234e-06,
      "loss": 0.2206,
      "step": 19751
    },
    {
      "epoch": 7.64692218350755,
      "grad_norm": 4.948250770568848,
      "learning_rate": 2.614530907213834e-06,
      "loss": 0.1404,
      "step": 19752
    },
    {
      "epoch": 7.647309330236159,
      "grad_norm": 48.462013244628906,
      "learning_rate": 2.6141007441820454e-06,
      "loss": 1.4691,
      "step": 19753
    },
    {
      "epoch": 7.64769647696477,
      "grad_norm": 1.3340178728103638,
      "learning_rate": 2.613670581150256e-06,
      "loss": 0.0307,
      "step": 19754
    },
    {
      "epoch": 7.648083623693379,
      "grad_norm": 85.544677734375,
      "learning_rate": 2.6132404181184674e-06,
      "loss": 1.6913,
      "step": 19755
    },
    {
      "epoch": 7.64847077042199,
      "grad_norm": 231.16635131835938,
      "learning_rate": 2.612810255086678e-06,
      "loss": 0.6735,
      "step": 19756
    },
    {
      "epoch": 7.6488579171506,
      "grad_norm": 47.31120681762695,
      "learning_rate": 2.6123800920548894e-06,
      "loss": 1.7348,
      "step": 19757
    },
    {
      "epoch": 7.64924506387921,
      "grad_norm": 49.584598541259766,
      "learning_rate": 2.6119499290231e-06,
      "loss": 2.1047,
      "step": 19758
    },
    {
      "epoch": 7.6496322106078205,
      "grad_norm": 65.07525634765625,
      "learning_rate": 2.611519765991311e-06,
      "loss": 0.162,
      "step": 19759
    },
    {
      "epoch": 7.65001935733643,
      "grad_norm": 36.719024658203125,
      "learning_rate": 2.611089602959522e-06,
      "loss": 0.2068,
      "step": 19760
    },
    {
      "epoch": 7.650406504065041,
      "grad_norm": 57.88560104370117,
      "learning_rate": 2.610659439927733e-06,
      "loss": 1.273,
      "step": 19761
    },
    {
      "epoch": 7.650793650793651,
      "grad_norm": 86.66990661621094,
      "learning_rate": 2.6102292768959435e-06,
      "loss": 0.298,
      "step": 19762
    },
    {
      "epoch": 7.651180797522261,
      "grad_norm": 121.55917358398438,
      "learning_rate": 2.609799113864155e-06,
      "loss": 1.5785,
      "step": 19763
    },
    {
      "epoch": 7.651567944250871,
      "grad_norm": 243.50421142578125,
      "learning_rate": 2.6093689508323655e-06,
      "loss": 1.0827,
      "step": 19764
    },
    {
      "epoch": 7.651955090979481,
      "grad_norm": 155.90689086914062,
      "learning_rate": 2.608938787800577e-06,
      "loss": 3.3224,
      "step": 19765
    },
    {
      "epoch": 7.652342237708091,
      "grad_norm": 232.4081573486328,
      "learning_rate": 2.6085086247687875e-06,
      "loss": 0.4683,
      "step": 19766
    },
    {
      "epoch": 7.652729384436702,
      "grad_norm": 42.23971939086914,
      "learning_rate": 2.6080784617369985e-06,
      "loss": 1.5095,
      "step": 19767
    },
    {
      "epoch": 7.6531165311653115,
      "grad_norm": 25.870878219604492,
      "learning_rate": 2.6076482987052095e-06,
      "loss": 0.7768,
      "step": 19768
    },
    {
      "epoch": 7.653503677893922,
      "grad_norm": 248.48406982421875,
      "learning_rate": 2.6072181356734204e-06,
      "loss": 1.2893,
      "step": 19769
    },
    {
      "epoch": 7.653890824622532,
      "grad_norm": 27.479124069213867,
      "learning_rate": 2.606787972641631e-06,
      "loss": 1.9167,
      "step": 19770
    },
    {
      "epoch": 7.654277971351142,
      "grad_norm": 435.52001953125,
      "learning_rate": 2.6063578096098424e-06,
      "loss": 1.2049,
      "step": 19771
    },
    {
      "epoch": 7.654665118079752,
      "grad_norm": 42.921104431152344,
      "learning_rate": 2.605927646578053e-06,
      "loss": 1.5035,
      "step": 19772
    },
    {
      "epoch": 7.655052264808362,
      "grad_norm": 90.31998443603516,
      "learning_rate": 2.6054974835462644e-06,
      "loss": 0.8367,
      "step": 19773
    },
    {
      "epoch": 7.655439411536973,
      "grad_norm": 92.50180053710938,
      "learning_rate": 2.605067320514475e-06,
      "loss": 0.2832,
      "step": 19774
    },
    {
      "epoch": 7.6558265582655824,
      "grad_norm": 258.8793029785156,
      "learning_rate": 2.6046371574826864e-06,
      "loss": 1.9546,
      "step": 19775
    },
    {
      "epoch": 7.656213704994193,
      "grad_norm": 28.23933982849121,
      "learning_rate": 2.604206994450897e-06,
      "loss": 1.8337,
      "step": 19776
    },
    {
      "epoch": 7.656600851722803,
      "grad_norm": 29.898265838623047,
      "learning_rate": 2.603776831419108e-06,
      "loss": 0.107,
      "step": 19777
    },
    {
      "epoch": 7.656987998451413,
      "grad_norm": 3.4339005947113037,
      "learning_rate": 2.603346668387319e-06,
      "loss": 0.0181,
      "step": 19778
    },
    {
      "epoch": 7.657375145180024,
      "grad_norm": 69.01265716552734,
      "learning_rate": 2.60291650535553e-06,
      "loss": 1.3681,
      "step": 19779
    },
    {
      "epoch": 7.657762291908633,
      "grad_norm": 0.5310189723968506,
      "learning_rate": 2.6024863423237405e-06,
      "loss": 0.0169,
      "step": 19780
    },
    {
      "epoch": 7.658149438637244,
      "grad_norm": 1.629401683807373,
      "learning_rate": 2.602056179291952e-06,
      "loss": 0.0661,
      "step": 19781
    },
    {
      "epoch": 7.658536585365853,
      "grad_norm": 26.344507217407227,
      "learning_rate": 2.601626016260163e-06,
      "loss": 4.2739,
      "step": 19782
    },
    {
      "epoch": 7.658923732094464,
      "grad_norm": 23.063335418701172,
      "learning_rate": 2.601195853228374e-06,
      "loss": 0.1916,
      "step": 19783
    },
    {
      "epoch": 7.659310878823074,
      "grad_norm": 352.7802734375,
      "learning_rate": 2.600765690196585e-06,
      "loss": 0.9399,
      "step": 19784
    },
    {
      "epoch": 7.659698025551684,
      "grad_norm": 144.59835815429688,
      "learning_rate": 2.6003355271647955e-06,
      "loss": 1.6673,
      "step": 19785
    },
    {
      "epoch": 7.6600851722802945,
      "grad_norm": 112.5913314819336,
      "learning_rate": 2.599905364133007e-06,
      "loss": 1.7214,
      "step": 19786
    },
    {
      "epoch": 7.660472319008904,
      "grad_norm": 8.186596870422363,
      "learning_rate": 2.5994752011012175e-06,
      "loss": 0.1117,
      "step": 19787
    },
    {
      "epoch": 7.660859465737515,
      "grad_norm": 211.7388153076172,
      "learning_rate": 2.599045038069429e-06,
      "loss": 1.9083,
      "step": 19788
    },
    {
      "epoch": 7.661246612466124,
      "grad_norm": 76.0536880493164,
      "learning_rate": 2.5986148750376394e-06,
      "loss": 1.3536,
      "step": 19789
    },
    {
      "epoch": 7.661633759194735,
      "grad_norm": 3.7382352352142334,
      "learning_rate": 2.598184712005851e-06,
      "loss": 0.0689,
      "step": 19790
    },
    {
      "epoch": 7.662020905923345,
      "grad_norm": 2.539247989654541,
      "learning_rate": 2.5977545489740614e-06,
      "loss": 0.0429,
      "step": 19791
    },
    {
      "epoch": 7.662408052651955,
      "grad_norm": 83.69597625732422,
      "learning_rate": 2.5973243859422724e-06,
      "loss": 0.267,
      "step": 19792
    },
    {
      "epoch": 7.662795199380565,
      "grad_norm": 1.538769006729126,
      "learning_rate": 2.5968942229104834e-06,
      "loss": 0.0461,
      "step": 19793
    },
    {
      "epoch": 7.663182346109175,
      "grad_norm": 62.99418258666992,
      "learning_rate": 2.5964640598786944e-06,
      "loss": 0.2743,
      "step": 19794
    },
    {
      "epoch": 7.6635694928377855,
      "grad_norm": 99.52371215820312,
      "learning_rate": 2.596033896846905e-06,
      "loss": 1.1982,
      "step": 19795
    },
    {
      "epoch": 7.663956639566395,
      "grad_norm": 56.154781341552734,
      "learning_rate": 2.5956037338151164e-06,
      "loss": 0.239,
      "step": 19796
    },
    {
      "epoch": 7.664343786295006,
      "grad_norm": 114.21985626220703,
      "learning_rate": 2.595173570783327e-06,
      "loss": 2.6031,
      "step": 19797
    },
    {
      "epoch": 7.664730933023616,
      "grad_norm": 128.9370880126953,
      "learning_rate": 2.5947434077515384e-06,
      "loss": 0.9841,
      "step": 19798
    },
    {
      "epoch": 7.665118079752226,
      "grad_norm": 3.234875440597534,
      "learning_rate": 2.594313244719749e-06,
      "loss": 0.0514,
      "step": 19799
    },
    {
      "epoch": 7.665505226480836,
      "grad_norm": 21.009578704833984,
      "learning_rate": 2.59388308168796e-06,
      "loss": 2.1793,
      "step": 19800
    },
    {
      "epoch": 7.665892373209447,
      "grad_norm": 108.3235855102539,
      "learning_rate": 2.593452918656171e-06,
      "loss": 1.3941,
      "step": 19801
    },
    {
      "epoch": 7.6662795199380565,
      "grad_norm": 87.60688781738281,
      "learning_rate": 2.593022755624382e-06,
      "loss": 0.6004,
      "step": 19802
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 1.715933918952942,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 0.0642,
      "step": 19803
    },
    {
      "epoch": 7.667053813395277,
      "grad_norm": 60.73344421386719,
      "learning_rate": 2.592162429560804e-06,
      "loss": 2.0079,
      "step": 19804
    },
    {
      "epoch": 7.667440960123887,
      "grad_norm": 5.531106948852539,
      "learning_rate": 2.5917322665290145e-06,
      "loss": 0.0604,
      "step": 19805
    },
    {
      "epoch": 7.667828106852497,
      "grad_norm": 95.708740234375,
      "learning_rate": 2.591302103497226e-06,
      "loss": 2.3853,
      "step": 19806
    },
    {
      "epoch": 7.668215253581107,
      "grad_norm": 2.555873155593872,
      "learning_rate": 2.5908719404654365e-06,
      "loss": 0.0605,
      "step": 19807
    },
    {
      "epoch": 7.668602400309718,
      "grad_norm": 6.094076156616211,
      "learning_rate": 2.590441777433648e-06,
      "loss": 0.0975,
      "step": 19808
    },
    {
      "epoch": 7.668989547038327,
      "grad_norm": 39.99940490722656,
      "learning_rate": 2.5900116144018584e-06,
      "loss": 4.7121,
      "step": 19809
    },
    {
      "epoch": 7.669376693766938,
      "grad_norm": 11.427103996276855,
      "learning_rate": 2.5895814513700694e-06,
      "loss": 0.1884,
      "step": 19810
    },
    {
      "epoch": 7.6697638404955475,
      "grad_norm": 123.10089874267578,
      "learning_rate": 2.5891512883382804e-06,
      "loss": 2.5463,
      "step": 19811
    },
    {
      "epoch": 7.670150987224158,
      "grad_norm": 152.51177978515625,
      "learning_rate": 2.5887211253064914e-06,
      "loss": 1.1481,
      "step": 19812
    },
    {
      "epoch": 7.670538133952768,
      "grad_norm": 85.37714385986328,
      "learning_rate": 2.588290962274702e-06,
      "loss": 2.0496,
      "step": 19813
    },
    {
      "epoch": 7.670925280681378,
      "grad_norm": 1.4242854118347168,
      "learning_rate": 2.5878607992429134e-06,
      "loss": 0.0346,
      "step": 19814
    },
    {
      "epoch": 7.671312427409989,
      "grad_norm": 125.27127838134766,
      "learning_rate": 2.587430636211124e-06,
      "loss": 0.4722,
      "step": 19815
    },
    {
      "epoch": 7.671699574138598,
      "grad_norm": 88.08484649658203,
      "learning_rate": 2.5870004731793354e-06,
      "loss": 1.0381,
      "step": 19816
    },
    {
      "epoch": 7.672086720867209,
      "grad_norm": 185.1652069091797,
      "learning_rate": 2.586570310147546e-06,
      "loss": 1.062,
      "step": 19817
    },
    {
      "epoch": 7.672473867595819,
      "grad_norm": 134.81051635742188,
      "learning_rate": 2.586140147115757e-06,
      "loss": 2.3141,
      "step": 19818
    },
    {
      "epoch": 7.672861014324429,
      "grad_norm": 90.75686645507812,
      "learning_rate": 2.585709984083968e-06,
      "loss": 0.4226,
      "step": 19819
    },
    {
      "epoch": 7.673248161053039,
      "grad_norm": 45.912315368652344,
      "learning_rate": 2.585279821052179e-06,
      "loss": 0.3015,
      "step": 19820
    },
    {
      "epoch": 7.673635307781649,
      "grad_norm": 166.13388061523438,
      "learning_rate": 2.5848496580203895e-06,
      "loss": 0.4718,
      "step": 19821
    },
    {
      "epoch": 7.6740224545102595,
      "grad_norm": 0.8101147413253784,
      "learning_rate": 2.584419494988601e-06,
      "loss": 0.0231,
      "step": 19822
    },
    {
      "epoch": 7.674409601238869,
      "grad_norm": 1.6915605068206787,
      "learning_rate": 2.583989331956812e-06,
      "loss": 0.0751,
      "step": 19823
    },
    {
      "epoch": 7.67479674796748,
      "grad_norm": 2.2665414810180664,
      "learning_rate": 2.583559168925023e-06,
      "loss": 0.0314,
      "step": 19824
    },
    {
      "epoch": 7.67518389469609,
      "grad_norm": 0.7742766737937927,
      "learning_rate": 2.583129005893234e-06,
      "loss": 0.0288,
      "step": 19825
    },
    {
      "epoch": 7.6755710414247,
      "grad_norm": 103.92550659179688,
      "learning_rate": 2.582698842861445e-06,
      "loss": 1.0078,
      "step": 19826
    },
    {
      "epoch": 7.67595818815331,
      "grad_norm": 84.66980743408203,
      "learning_rate": 2.582268679829656e-06,
      "loss": 0.3069,
      "step": 19827
    },
    {
      "epoch": 7.67634533488192,
      "grad_norm": 11.153701782226562,
      "learning_rate": 2.5818385167978664e-06,
      "loss": 0.1814,
      "step": 19828
    },
    {
      "epoch": 7.6767324816105305,
      "grad_norm": 24.146724700927734,
      "learning_rate": 2.581408353766078e-06,
      "loss": 1.8252,
      "step": 19829
    },
    {
      "epoch": 7.67711962833914,
      "grad_norm": 0.6854714751243591,
      "learning_rate": 2.5809781907342884e-06,
      "loss": 0.0202,
      "step": 19830
    },
    {
      "epoch": 7.677506775067751,
      "grad_norm": 3.1384308338165283,
      "learning_rate": 2.5805480277025e-06,
      "loss": 0.0589,
      "step": 19831
    },
    {
      "epoch": 7.677893921796361,
      "grad_norm": 6.243844509124756,
      "learning_rate": 2.5801178646707104e-06,
      "loss": 0.0654,
      "step": 19832
    },
    {
      "epoch": 7.678281068524971,
      "grad_norm": 28.912755966186523,
      "learning_rate": 2.5796877016389214e-06,
      "loss": 0.3324,
      "step": 19833
    },
    {
      "epoch": 7.678668215253581,
      "grad_norm": 68.47209167480469,
      "learning_rate": 2.5792575386071324e-06,
      "loss": 1.492,
      "step": 19834
    },
    {
      "epoch": 7.679055361982192,
      "grad_norm": 81.37067413330078,
      "learning_rate": 2.5788273755753434e-06,
      "loss": 3.1382,
      "step": 19835
    },
    {
      "epoch": 7.679442508710801,
      "grad_norm": 3.9565913677215576,
      "learning_rate": 2.578397212543554e-06,
      "loss": 0.1173,
      "step": 19836
    },
    {
      "epoch": 7.679829655439412,
      "grad_norm": 11.986225128173828,
      "learning_rate": 2.5779670495117654e-06,
      "loss": 0.0945,
      "step": 19837
    },
    {
      "epoch": 7.6802168021680215,
      "grad_norm": 88.91007232666016,
      "learning_rate": 2.577536886479976e-06,
      "loss": 4.0289,
      "step": 19838
    },
    {
      "epoch": 7.680603948896632,
      "grad_norm": 131.75448608398438,
      "learning_rate": 2.5771067234481874e-06,
      "loss": 0.1526,
      "step": 19839
    },
    {
      "epoch": 7.680991095625242,
      "grad_norm": 2.7080578804016113,
      "learning_rate": 2.576676560416398e-06,
      "loss": 0.1553,
      "step": 19840
    },
    {
      "epoch": 7.681378242353852,
      "grad_norm": 47.02180480957031,
      "learning_rate": 2.576246397384609e-06,
      "loss": 0.3346,
      "step": 19841
    },
    {
      "epoch": 7.681765389082463,
      "grad_norm": 36.05534362792969,
      "learning_rate": 2.57581623435282e-06,
      "loss": 1.9507,
      "step": 19842
    },
    {
      "epoch": 7.682152535811072,
      "grad_norm": 32.355384826660156,
      "learning_rate": 2.575386071321031e-06,
      "loss": 0.2082,
      "step": 19843
    },
    {
      "epoch": 7.682539682539683,
      "grad_norm": 2.134554862976074,
      "learning_rate": 2.574955908289242e-06,
      "loss": 0.0233,
      "step": 19844
    },
    {
      "epoch": 7.682926829268292,
      "grad_norm": 2.5759329795837402,
      "learning_rate": 2.574525745257453e-06,
      "loss": 0.1195,
      "step": 19845
    },
    {
      "epoch": 7.683313975996903,
      "grad_norm": 80.97323608398438,
      "learning_rate": 2.5740955822256635e-06,
      "loss": 2.7293,
      "step": 19846
    },
    {
      "epoch": 7.6837011227255125,
      "grad_norm": 2.1840150356292725,
      "learning_rate": 2.573665419193875e-06,
      "loss": 0.1215,
      "step": 19847
    },
    {
      "epoch": 7.684088269454123,
      "grad_norm": 86.78492736816406,
      "learning_rate": 2.5732352561620854e-06,
      "loss": 1.8642,
      "step": 19848
    },
    {
      "epoch": 7.6844754161827336,
      "grad_norm": 98.19884490966797,
      "learning_rate": 2.572805093130297e-06,
      "loss": 1.4543,
      "step": 19849
    },
    {
      "epoch": 7.684862562911343,
      "grad_norm": 48.057701110839844,
      "learning_rate": 2.5723749300985074e-06,
      "loss": 4.5295,
      "step": 19850
    },
    {
      "epoch": 7.685249709639954,
      "grad_norm": 2.777820110321045,
      "learning_rate": 2.5719447670667184e-06,
      "loss": 0.1267,
      "step": 19851
    },
    {
      "epoch": 7.685636856368563,
      "grad_norm": 23.146940231323242,
      "learning_rate": 2.5715146040349294e-06,
      "loss": 1.7661,
      "step": 19852
    },
    {
      "epoch": 7.686024003097174,
      "grad_norm": 1.4632974863052368,
      "learning_rate": 2.5710844410031404e-06,
      "loss": 0.0316,
      "step": 19853
    },
    {
      "epoch": 7.686411149825784,
      "grad_norm": 1.2356973886489868,
      "learning_rate": 2.570654277971351e-06,
      "loss": 0.0445,
      "step": 19854
    },
    {
      "epoch": 7.686798296554394,
      "grad_norm": 0.3377029597759247,
      "learning_rate": 2.5702241149395624e-06,
      "loss": 0.0091,
      "step": 19855
    },
    {
      "epoch": 7.6871854432830045,
      "grad_norm": 168.38946533203125,
      "learning_rate": 2.569793951907773e-06,
      "loss": 3.8948,
      "step": 19856
    },
    {
      "epoch": 7.687572590011614,
      "grad_norm": 245.11062622070312,
      "learning_rate": 2.5693637888759844e-06,
      "loss": 2.768,
      "step": 19857
    },
    {
      "epoch": 7.687959736740225,
      "grad_norm": 3.95200514793396,
      "learning_rate": 2.568933625844195e-06,
      "loss": 0.0734,
      "step": 19858
    },
    {
      "epoch": 7.688346883468835,
      "grad_norm": 53.75254821777344,
      "learning_rate": 2.568503462812406e-06,
      "loss": 0.1885,
      "step": 19859
    },
    {
      "epoch": 7.688734030197445,
      "grad_norm": 23.787410736083984,
      "learning_rate": 2.568073299780617e-06,
      "loss": 1.834,
      "step": 19860
    },
    {
      "epoch": 7.689121176926055,
      "grad_norm": 38.86702346801758,
      "learning_rate": 2.567643136748828e-06,
      "loss": 1.917,
      "step": 19861
    },
    {
      "epoch": 7.689508323654665,
      "grad_norm": 50.263187408447266,
      "learning_rate": 2.567212973717039e-06,
      "loss": 2.7163,
      "step": 19862
    },
    {
      "epoch": 7.689895470383275,
      "grad_norm": 1.436249017715454,
      "learning_rate": 2.56678281068525e-06,
      "loss": 0.0617,
      "step": 19863
    },
    {
      "epoch": 7.690282617111885,
      "grad_norm": 1.1646764278411865,
      "learning_rate": 2.5663526476534613e-06,
      "loss": 0.0316,
      "step": 19864
    },
    {
      "epoch": 7.6906697638404955,
      "grad_norm": 181.59912109375,
      "learning_rate": 2.565922484621672e-06,
      "loss": 1.3753,
      "step": 19865
    },
    {
      "epoch": 7.691056910569106,
      "grad_norm": 155.40391540527344,
      "learning_rate": 2.565492321589883e-06,
      "loss": 1.0536,
      "step": 19866
    },
    {
      "epoch": 7.691444057297716,
      "grad_norm": 2.5260777473449707,
      "learning_rate": 2.565062158558094e-06,
      "loss": 0.0553,
      "step": 19867
    },
    {
      "epoch": 7.691831204026326,
      "grad_norm": 89.90316009521484,
      "learning_rate": 2.564631995526305e-06,
      "loss": 1.2037,
      "step": 19868
    },
    {
      "epoch": 7.692218350754936,
      "grad_norm": 82.57227325439453,
      "learning_rate": 2.5642018324945154e-06,
      "loss": 4.3062,
      "step": 19869
    },
    {
      "epoch": 7.692605497483546,
      "grad_norm": 90.52716064453125,
      "learning_rate": 2.563771669462727e-06,
      "loss": 1.0648,
      "step": 19870
    },
    {
      "epoch": 7.692992644212157,
      "grad_norm": 2.7204747200012207,
      "learning_rate": 2.5633415064309374e-06,
      "loss": 0.0504,
      "step": 19871
    },
    {
      "epoch": 7.693379790940766,
      "grad_norm": 88.61117553710938,
      "learning_rate": 2.562911343399149e-06,
      "loss": 1.739,
      "step": 19872
    },
    {
      "epoch": 7.693766937669377,
      "grad_norm": 3.1518025398254395,
      "learning_rate": 2.5624811803673594e-06,
      "loss": 0.1429,
      "step": 19873
    },
    {
      "epoch": 7.6941540843979865,
      "grad_norm": 31.366214752197266,
      "learning_rate": 2.5620510173355704e-06,
      "loss": 2.3917,
      "step": 19874
    },
    {
      "epoch": 7.694541231126597,
      "grad_norm": 24.664234161376953,
      "learning_rate": 2.5616208543037814e-06,
      "loss": 3.3216,
      "step": 19875
    },
    {
      "epoch": 7.694928377855208,
      "grad_norm": 72.2793960571289,
      "learning_rate": 2.5611906912719924e-06,
      "loss": 0.2497,
      "step": 19876
    },
    {
      "epoch": 7.695315524583817,
      "grad_norm": 39.51766586303711,
      "learning_rate": 2.560760528240203e-06,
      "loss": 1.4671,
      "step": 19877
    },
    {
      "epoch": 7.695702671312428,
      "grad_norm": 28.34014892578125,
      "learning_rate": 2.5603303652084144e-06,
      "loss": 0.6947,
      "step": 19878
    },
    {
      "epoch": 7.696089818041037,
      "grad_norm": 183.88340759277344,
      "learning_rate": 2.559900202176625e-06,
      "loss": 0.6997,
      "step": 19879
    },
    {
      "epoch": 7.696476964769648,
      "grad_norm": 0.8154794573783875,
      "learning_rate": 2.5594700391448364e-06,
      "loss": 0.0106,
      "step": 19880
    },
    {
      "epoch": 7.696864111498257,
      "grad_norm": 96.92518615722656,
      "learning_rate": 2.559039876113047e-06,
      "loss": 1.3627,
      "step": 19881
    },
    {
      "epoch": 7.697251258226868,
      "grad_norm": 352.0096130371094,
      "learning_rate": 2.5586097130812583e-06,
      "loss": 0.4718,
      "step": 19882
    },
    {
      "epoch": 7.6976384049554785,
      "grad_norm": 92.03443145751953,
      "learning_rate": 2.558179550049469e-06,
      "loss": 1.184,
      "step": 19883
    },
    {
      "epoch": 7.698025551684088,
      "grad_norm": 56.20294952392578,
      "learning_rate": 2.55774938701768e-06,
      "loss": 1.7133,
      "step": 19884
    },
    {
      "epoch": 7.698412698412699,
      "grad_norm": 138.3201446533203,
      "learning_rate": 2.557319223985891e-06,
      "loss": 0.9456,
      "step": 19885
    },
    {
      "epoch": 7.698799845141308,
      "grad_norm": 343.8437805175781,
      "learning_rate": 2.556889060954102e-06,
      "loss": 0.8557,
      "step": 19886
    },
    {
      "epoch": 7.699186991869919,
      "grad_norm": 24.17984962463379,
      "learning_rate": 2.5564588979223125e-06,
      "loss": 0.2192,
      "step": 19887
    },
    {
      "epoch": 7.699574138598528,
      "grad_norm": 63.80608367919922,
      "learning_rate": 2.556028734890524e-06,
      "loss": 0.2848,
      "step": 19888
    },
    {
      "epoch": 7.699961285327139,
      "grad_norm": 1.027697205543518,
      "learning_rate": 2.5555985718587344e-06,
      "loss": 0.039,
      "step": 19889
    },
    {
      "epoch": 7.700348432055749,
      "grad_norm": 47.08804702758789,
      "learning_rate": 2.555168408826946e-06,
      "loss": 1.6655,
      "step": 19890
    },
    {
      "epoch": 7.700735578784359,
      "grad_norm": 9.825021743774414,
      "learning_rate": 2.5547382457951564e-06,
      "loss": 0.1218,
      "step": 19891
    },
    {
      "epoch": 7.7011227255129695,
      "grad_norm": 37.660587310791016,
      "learning_rate": 2.5543080827633674e-06,
      "loss": 0.5691,
      "step": 19892
    },
    {
      "epoch": 7.70150987224158,
      "grad_norm": 88.06705474853516,
      "learning_rate": 2.5538779197315784e-06,
      "loss": 1.9049,
      "step": 19893
    },
    {
      "epoch": 7.70189701897019,
      "grad_norm": 222.28453063964844,
      "learning_rate": 2.5534477566997894e-06,
      "loss": 2.5171,
      "step": 19894
    },
    {
      "epoch": 7.7022841656988,
      "grad_norm": 117.0782241821289,
      "learning_rate": 2.553017593668e-06,
      "loss": 2.837,
      "step": 19895
    },
    {
      "epoch": 7.70267131242741,
      "grad_norm": 29.754568099975586,
      "learning_rate": 2.5525874306362114e-06,
      "loss": 2.1144,
      "step": 19896
    },
    {
      "epoch": 7.70305845915602,
      "grad_norm": 75.7184066772461,
      "learning_rate": 2.552157267604422e-06,
      "loss": 1.44,
      "step": 19897
    },
    {
      "epoch": 7.70344560588463,
      "grad_norm": 0.7683099508285522,
      "learning_rate": 2.5517271045726334e-06,
      "loss": 0.0273,
      "step": 19898
    },
    {
      "epoch": 7.70383275261324,
      "grad_norm": 137.7460174560547,
      "learning_rate": 2.551296941540844e-06,
      "loss": 0.2257,
      "step": 19899
    },
    {
      "epoch": 7.704219899341851,
      "grad_norm": 0.9738373756408691,
      "learning_rate": 2.5508667785090553e-06,
      "loss": 0.0237,
      "step": 19900
    },
    {
      "epoch": 7.7046070460704605,
      "grad_norm": 19.415468215942383,
      "learning_rate": 2.550436615477266e-06,
      "loss": 0.208,
      "step": 19901
    },
    {
      "epoch": 7.704994192799071,
      "grad_norm": 42.06406021118164,
      "learning_rate": 2.550006452445477e-06,
      "loss": 0.1838,
      "step": 19902
    },
    {
      "epoch": 7.705381339527681,
      "grad_norm": 40.92039489746094,
      "learning_rate": 2.549576289413688e-06,
      "loss": 0.2499,
      "step": 19903
    },
    {
      "epoch": 7.705768486256291,
      "grad_norm": 90.2249526977539,
      "learning_rate": 2.549146126381899e-06,
      "loss": 0.1622,
      "step": 19904
    },
    {
      "epoch": 7.706155632984901,
      "grad_norm": 34.20662307739258,
      "learning_rate": 2.5487159633501095e-06,
      "loss": 0.323,
      "step": 19905
    },
    {
      "epoch": 7.706542779713511,
      "grad_norm": 0.31812208890914917,
      "learning_rate": 2.548285800318321e-06,
      "loss": 0.0073,
      "step": 19906
    },
    {
      "epoch": 7.706929926442122,
      "grad_norm": 0.871062159538269,
      "learning_rate": 2.547855637286532e-06,
      "loss": 0.0347,
      "step": 19907
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 0.6340179443359375,
      "learning_rate": 2.547425474254743e-06,
      "loss": 0.0204,
      "step": 19908
    },
    {
      "epoch": 7.707704219899342,
      "grad_norm": 244.91114807128906,
      "learning_rate": 2.546995311222954e-06,
      "loss": 0.6409,
      "step": 19909
    },
    {
      "epoch": 7.7080913666279525,
      "grad_norm": 18.29109001159668,
      "learning_rate": 2.5465651481911644e-06,
      "loss": 0.2298,
      "step": 19910
    },
    {
      "epoch": 7.708478513356562,
      "grad_norm": 65.86735534667969,
      "learning_rate": 2.546134985159376e-06,
      "loss": 0.5298,
      "step": 19911
    },
    {
      "epoch": 7.708865660085173,
      "grad_norm": 1.8791885375976562,
      "learning_rate": 2.5457048221275864e-06,
      "loss": 0.0671,
      "step": 19912
    },
    {
      "epoch": 7.709252806813782,
      "grad_norm": 24.26107406616211,
      "learning_rate": 2.545274659095798e-06,
      "loss": 0.2188,
      "step": 19913
    },
    {
      "epoch": 7.709639953542393,
      "grad_norm": 41.49660873413086,
      "learning_rate": 2.5448444960640084e-06,
      "loss": 0.9238,
      "step": 19914
    },
    {
      "epoch": 7.710027100271002,
      "grad_norm": 178.0537567138672,
      "learning_rate": 2.54441433303222e-06,
      "loss": 2.4769,
      "step": 19915
    },
    {
      "epoch": 7.710414246999613,
      "grad_norm": 3.1436171531677246,
      "learning_rate": 2.5439841700004304e-06,
      "loss": 0.0418,
      "step": 19916
    },
    {
      "epoch": 7.710801393728223,
      "grad_norm": 3.265957832336426,
      "learning_rate": 2.5435540069686414e-06,
      "loss": 0.0498,
      "step": 19917
    },
    {
      "epoch": 7.711188540456833,
      "grad_norm": 55.65855407714844,
      "learning_rate": 2.5431238439368524e-06,
      "loss": 2.6791,
      "step": 19918
    },
    {
      "epoch": 7.7115756871854435,
      "grad_norm": 30.34937858581543,
      "learning_rate": 2.5426936809050634e-06,
      "loss": 1.7305,
      "step": 19919
    },
    {
      "epoch": 7.711962833914053,
      "grad_norm": 234.0595703125,
      "learning_rate": 2.542263517873274e-06,
      "loss": 2.7344,
      "step": 19920
    },
    {
      "epoch": 7.712349980642664,
      "grad_norm": 2.745692014694214,
      "learning_rate": 2.5418333548414853e-06,
      "loss": 0.0504,
      "step": 19921
    },
    {
      "epoch": 7.712737127371273,
      "grad_norm": 17.519758224487305,
      "learning_rate": 2.541403191809696e-06,
      "loss": 1.8352,
      "step": 19922
    },
    {
      "epoch": 7.713124274099884,
      "grad_norm": 4.495227336883545,
      "learning_rate": 2.5409730287779073e-06,
      "loss": 0.0446,
      "step": 19923
    },
    {
      "epoch": 7.713511420828494,
      "grad_norm": 130.25181579589844,
      "learning_rate": 2.540542865746118e-06,
      "loss": 2.1352,
      "step": 19924
    },
    {
      "epoch": 7.713898567557104,
      "grad_norm": 53.98339080810547,
      "learning_rate": 2.540112702714329e-06,
      "loss": 4.5495,
      "step": 19925
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 33.421142578125,
      "learning_rate": 2.53968253968254e-06,
      "loss": 0.2099,
      "step": 19926
    },
    {
      "epoch": 7.714672861014325,
      "grad_norm": 103.70417785644531,
      "learning_rate": 2.539252376650751e-06,
      "loss": 0.5812,
      "step": 19927
    },
    {
      "epoch": 7.7150600077429345,
      "grad_norm": 60.111724853515625,
      "learning_rate": 2.5388222136189614e-06,
      "loss": 0.7,
      "step": 19928
    },
    {
      "epoch": 7.715447154471545,
      "grad_norm": 1.8274719715118408,
      "learning_rate": 2.538392050587173e-06,
      "loss": 0.0488,
      "step": 19929
    },
    {
      "epoch": 7.715834301200155,
      "grad_norm": 10.52828598022461,
      "learning_rate": 2.5379618875553834e-06,
      "loss": 0.1293,
      "step": 19930
    },
    {
      "epoch": 7.716221447928765,
      "grad_norm": 2.1777169704437256,
      "learning_rate": 2.537531724523595e-06,
      "loss": 0.1083,
      "step": 19931
    },
    {
      "epoch": 7.716608594657375,
      "grad_norm": 2.716787576675415,
      "learning_rate": 2.5371015614918054e-06,
      "loss": 0.0943,
      "step": 19932
    },
    {
      "epoch": 7.716995741385985,
      "grad_norm": 83.91454315185547,
      "learning_rate": 2.536671398460017e-06,
      "loss": 1.3343,
      "step": 19933
    },
    {
      "epoch": 7.717382888114596,
      "grad_norm": 24.002878189086914,
      "learning_rate": 2.5362412354282274e-06,
      "loss": 0.1191,
      "step": 19934
    },
    {
      "epoch": 7.7177700348432055,
      "grad_norm": 94.81553649902344,
      "learning_rate": 2.5358110723964384e-06,
      "loss": 0.4177,
      "step": 19935
    },
    {
      "epoch": 7.718157181571816,
      "grad_norm": 57.7447395324707,
      "learning_rate": 2.5353809093646494e-06,
      "loss": 0.9145,
      "step": 19936
    },
    {
      "epoch": 7.718544328300426,
      "grad_norm": 4.806535720825195,
      "learning_rate": 2.5349507463328604e-06,
      "loss": 0.0954,
      "step": 19937
    },
    {
      "epoch": 7.718931475029036,
      "grad_norm": 1.1090103387832642,
      "learning_rate": 2.534520583301071e-06,
      "loss": 0.0345,
      "step": 19938
    },
    {
      "epoch": 7.719318621757646,
      "grad_norm": 70.85346984863281,
      "learning_rate": 2.5340904202692824e-06,
      "loss": 1.552,
      "step": 19939
    },
    {
      "epoch": 7.719705768486256,
      "grad_norm": 10.53643798828125,
      "learning_rate": 2.533660257237493e-06,
      "loss": 0.5999,
      "step": 19940
    },
    {
      "epoch": 7.720092915214867,
      "grad_norm": 9.237854957580566,
      "learning_rate": 2.5332300942057043e-06,
      "loss": 0.1519,
      "step": 19941
    },
    {
      "epoch": 7.720480061943476,
      "grad_norm": 165.12445068359375,
      "learning_rate": 2.532799931173915e-06,
      "loss": 0.7757,
      "step": 19942
    },
    {
      "epoch": 7.720867208672087,
      "grad_norm": 45.93695068359375,
      "learning_rate": 2.532369768142126e-06,
      "loss": 2.5123,
      "step": 19943
    },
    {
      "epoch": 7.7212543554006965,
      "grad_norm": 5.685864448547363,
      "learning_rate": 2.531939605110337e-06,
      "loss": 0.0836,
      "step": 19944
    },
    {
      "epoch": 7.721641502129307,
      "grad_norm": 1.0313043594360352,
      "learning_rate": 2.531509442078548e-06,
      "loss": 0.0437,
      "step": 19945
    },
    {
      "epoch": 7.7220286488579175,
      "grad_norm": 1.0113370418548584,
      "learning_rate": 2.5310792790467585e-06,
      "loss": 0.024,
      "step": 19946
    },
    {
      "epoch": 7.722415795586527,
      "grad_norm": 131.21372985839844,
      "learning_rate": 2.53064911601497e-06,
      "loss": 4.4897,
      "step": 19947
    },
    {
      "epoch": 7.722802942315138,
      "grad_norm": 295.46051025390625,
      "learning_rate": 2.5302189529831813e-06,
      "loss": 3.8448,
      "step": 19948
    },
    {
      "epoch": 7.723190089043747,
      "grad_norm": 437.8092956542969,
      "learning_rate": 2.529788789951392e-06,
      "loss": 2.8522,
      "step": 19949
    },
    {
      "epoch": 7.723577235772358,
      "grad_norm": 21.225238800048828,
      "learning_rate": 2.529358626919603e-06,
      "loss": 0.1554,
      "step": 19950
    },
    {
      "epoch": 7.723964382500968,
      "grad_norm": 360.4608459472656,
      "learning_rate": 2.528928463887814e-06,
      "loss": 2.9822,
      "step": 19951
    },
    {
      "epoch": 7.724351529229578,
      "grad_norm": 24.43385887145996,
      "learning_rate": 2.528498300856025e-06,
      "loss": 3.3312,
      "step": 19952
    },
    {
      "epoch": 7.724738675958188,
      "grad_norm": 79.64579010009766,
      "learning_rate": 2.5280681378242354e-06,
      "loss": 1.5453,
      "step": 19953
    },
    {
      "epoch": 7.725125822686798,
      "grad_norm": 220.3320770263672,
      "learning_rate": 2.527637974792447e-06,
      "loss": 1.1361,
      "step": 19954
    },
    {
      "epoch": 7.7255129694154085,
      "grad_norm": 2.776224136352539,
      "learning_rate": 2.5272078117606574e-06,
      "loss": 0.0524,
      "step": 19955
    },
    {
      "epoch": 7.725900116144018,
      "grad_norm": 77.06631469726562,
      "learning_rate": 2.526777648728869e-06,
      "loss": 0.8065,
      "step": 19956
    },
    {
      "epoch": 7.726287262872629,
      "grad_norm": 0.6098511219024658,
      "learning_rate": 2.5263474856970794e-06,
      "loss": 0.0226,
      "step": 19957
    },
    {
      "epoch": 7.726674409601239,
      "grad_norm": 137.03533935546875,
      "learning_rate": 2.5259173226652904e-06,
      "loss": 3.132,
      "step": 19958
    },
    {
      "epoch": 7.727061556329849,
      "grad_norm": 49.473175048828125,
      "learning_rate": 2.5254871596335013e-06,
      "loss": 0.2312,
      "step": 19959
    },
    {
      "epoch": 7.727448703058459,
      "grad_norm": 0.8358714580535889,
      "learning_rate": 2.5250569966017123e-06,
      "loss": 0.0239,
      "step": 19960
    },
    {
      "epoch": 7.727835849787069,
      "grad_norm": 0.5384597182273865,
      "learning_rate": 2.524626833569923e-06,
      "loss": 0.0193,
      "step": 19961
    },
    {
      "epoch": 7.7282229965156795,
      "grad_norm": 1.3007019758224487,
      "learning_rate": 2.5241966705381343e-06,
      "loss": 0.0302,
      "step": 19962
    },
    {
      "epoch": 7.72861014324429,
      "grad_norm": 39.435890197753906,
      "learning_rate": 2.523766507506345e-06,
      "loss": 1.305,
      "step": 19963
    },
    {
      "epoch": 7.7289972899729,
      "grad_norm": 48.3021354675293,
      "learning_rate": 2.5233363444745563e-06,
      "loss": 0.4908,
      "step": 19964
    },
    {
      "epoch": 7.72938443670151,
      "grad_norm": 151.63279724121094,
      "learning_rate": 2.522906181442767e-06,
      "loss": 2.8554,
      "step": 19965
    },
    {
      "epoch": 7.72977158343012,
      "grad_norm": 39.911624908447266,
      "learning_rate": 2.5224760184109783e-06,
      "loss": 0.185,
      "step": 19966
    },
    {
      "epoch": 7.73015873015873,
      "grad_norm": 52.066165924072266,
      "learning_rate": 2.522045855379189e-06,
      "loss": 2.6174,
      "step": 19967
    },
    {
      "epoch": 7.730545876887341,
      "grad_norm": 67.41925811767578,
      "learning_rate": 2.5216156923474e-06,
      "loss": 1.6831,
      "step": 19968
    },
    {
      "epoch": 7.73093302361595,
      "grad_norm": 275.10888671875,
      "learning_rate": 2.521185529315611e-06,
      "loss": 0.9084,
      "step": 19969
    },
    {
      "epoch": 7.731320170344561,
      "grad_norm": 0.1735173612833023,
      "learning_rate": 2.520755366283822e-06,
      "loss": 0.0048,
      "step": 19970
    },
    {
      "epoch": 7.7317073170731705,
      "grad_norm": 0.4924049973487854,
      "learning_rate": 2.5203252032520324e-06,
      "loss": 0.0165,
      "step": 19971
    },
    {
      "epoch": 7.732094463801781,
      "grad_norm": 1.6795985698699951,
      "learning_rate": 2.519895040220244e-06,
      "loss": 0.0635,
      "step": 19972
    },
    {
      "epoch": 7.732481610530391,
      "grad_norm": 61.24217987060547,
      "learning_rate": 2.5194648771884544e-06,
      "loss": 1.8392,
      "step": 19973
    },
    {
      "epoch": 7.732868757259001,
      "grad_norm": 56.176578521728516,
      "learning_rate": 2.519034714156666e-06,
      "loss": 1.2453,
      "step": 19974
    },
    {
      "epoch": 7.733255903987612,
      "grad_norm": 84.22652435302734,
      "learning_rate": 2.5186045511248764e-06,
      "loss": 2.3097,
      "step": 19975
    },
    {
      "epoch": 7.733643050716221,
      "grad_norm": 52.06098556518555,
      "learning_rate": 2.5181743880930874e-06,
      "loss": 1.5184,
      "step": 19976
    },
    {
      "epoch": 7.734030197444832,
      "grad_norm": 114.7271957397461,
      "learning_rate": 2.5177442250612984e-06,
      "loss": 0.9514,
      "step": 19977
    },
    {
      "epoch": 7.734417344173441,
      "grad_norm": 2.1088271141052246,
      "learning_rate": 2.5173140620295094e-06,
      "loss": 0.0764,
      "step": 19978
    },
    {
      "epoch": 7.734804490902052,
      "grad_norm": 95.42259216308594,
      "learning_rate": 2.51688389899772e-06,
      "loss": 3.3617,
      "step": 19979
    },
    {
      "epoch": 7.7351916376306615,
      "grad_norm": 2.711747884750366,
      "learning_rate": 2.5164537359659313e-06,
      "loss": 0.0552,
      "step": 19980
    },
    {
      "epoch": 7.735578784359272,
      "grad_norm": 52.00223159790039,
      "learning_rate": 2.516023572934142e-06,
      "loss": 1.1291,
      "step": 19981
    },
    {
      "epoch": 7.7359659310878826,
      "grad_norm": 87.66927337646484,
      "learning_rate": 2.5155934099023533e-06,
      "loss": 0.6091,
      "step": 19982
    },
    {
      "epoch": 7.736353077816492,
      "grad_norm": 62.91242980957031,
      "learning_rate": 2.515163246870564e-06,
      "loss": 1.8213,
      "step": 19983
    },
    {
      "epoch": 7.736740224545103,
      "grad_norm": 0.23287461698055267,
      "learning_rate": 2.5147330838387753e-06,
      "loss": 0.0058,
      "step": 19984
    },
    {
      "epoch": 7.737127371273713,
      "grad_norm": 308.26776123046875,
      "learning_rate": 2.514302920806986e-06,
      "loss": 1.5471,
      "step": 19985
    },
    {
      "epoch": 7.737514518002323,
      "grad_norm": 4.501607894897461,
      "learning_rate": 2.513872757775197e-06,
      "loss": 0.1202,
      "step": 19986
    },
    {
      "epoch": 7.737901664730933,
      "grad_norm": 44.29444122314453,
      "learning_rate": 2.513442594743408e-06,
      "loss": 1.325,
      "step": 19987
    },
    {
      "epoch": 7.738288811459543,
      "grad_norm": 2.5171988010406494,
      "learning_rate": 2.513012431711619e-06,
      "loss": 0.0125,
      "step": 19988
    },
    {
      "epoch": 7.7386759581881535,
      "grad_norm": 113.70854187011719,
      "learning_rate": 2.5125822686798303e-06,
      "loss": 6.3375,
      "step": 19989
    },
    {
      "epoch": 7.739063104916763,
      "grad_norm": 122.47932434082031,
      "learning_rate": 2.512152105648041e-06,
      "loss": 2.9588,
      "step": 19990
    },
    {
      "epoch": 7.739450251645374,
      "grad_norm": 87.46282196044922,
      "learning_rate": 2.511721942616252e-06,
      "loss": 0.4511,
      "step": 19991
    },
    {
      "epoch": 7.739837398373984,
      "grad_norm": 24.602163314819336,
      "learning_rate": 2.511291779584463e-06,
      "loss": 0.2243,
      "step": 19992
    },
    {
      "epoch": 7.740224545102594,
      "grad_norm": 85.35183715820312,
      "learning_rate": 2.510861616552674e-06,
      "loss": 0.3804,
      "step": 19993
    },
    {
      "epoch": 7.740611691831204,
      "grad_norm": 118.66520690917969,
      "learning_rate": 2.5104314535208844e-06,
      "loss": 0.817,
      "step": 19994
    },
    {
      "epoch": 7.740998838559814,
      "grad_norm": 188.9500732421875,
      "learning_rate": 2.510001290489096e-06,
      "loss": 2.1828,
      "step": 19995
    },
    {
      "epoch": 7.741385985288424,
      "grad_norm": 116.18630981445312,
      "learning_rate": 2.5095711274573064e-06,
      "loss": 1.3658,
      "step": 19996
    },
    {
      "epoch": 7.741773132017034,
      "grad_norm": 1.5351927280426025,
      "learning_rate": 2.5091409644255178e-06,
      "loss": 0.0468,
      "step": 19997
    },
    {
      "epoch": 7.7421602787456445,
      "grad_norm": 36.8563232421875,
      "learning_rate": 2.5087108013937284e-06,
      "loss": 0.4618,
      "step": 19998
    },
    {
      "epoch": 7.742547425474255,
      "grad_norm": 58.5308837890625,
      "learning_rate": 2.5082806383619398e-06,
      "loss": 2.1581,
      "step": 19999
    },
    {
      "epoch": 7.742934572202865,
      "grad_norm": 204.12620544433594,
      "learning_rate": 2.5078504753301503e-06,
      "loss": 0.8611,
      "step": 20000
    },
    {
      "epoch": 7.743321718931475,
      "grad_norm": 3.338765859603882,
      "learning_rate": 2.5074203122983613e-06,
      "loss": 0.0881,
      "step": 20001
    },
    {
      "epoch": 7.743708865660086,
      "grad_norm": 82.86030578613281,
      "learning_rate": 2.5069901492665723e-06,
      "loss": 0.3564,
      "step": 20002
    },
    {
      "epoch": 7.744096012388695,
      "grad_norm": 1.6724332571029663,
      "learning_rate": 2.5065599862347833e-06,
      "loss": 0.014,
      "step": 20003
    },
    {
      "epoch": 7.744483159117306,
      "grad_norm": 1.537947416305542,
      "learning_rate": 2.506129823202994e-06,
      "loss": 0.0514,
      "step": 20004
    },
    {
      "epoch": 7.744870305845915,
      "grad_norm": 21.001768112182617,
      "learning_rate": 2.5056996601712053e-06,
      "loss": 2.2457,
      "step": 20005
    },
    {
      "epoch": 7.745257452574526,
      "grad_norm": 146.23892211914062,
      "learning_rate": 2.505269497139416e-06,
      "loss": 0.7224,
      "step": 20006
    },
    {
      "epoch": 7.7456445993031355,
      "grad_norm": 2.1763076782226562,
      "learning_rate": 2.5048393341076273e-06,
      "loss": 0.1008,
      "step": 20007
    },
    {
      "epoch": 7.746031746031746,
      "grad_norm": 61.341148376464844,
      "learning_rate": 2.504409171075838e-06,
      "loss": 0.582,
      "step": 20008
    },
    {
      "epoch": 7.746418892760357,
      "grad_norm": 2.0436151027679443,
      "learning_rate": 2.503979008044049e-06,
      "loss": 0.0292,
      "step": 20009
    },
    {
      "epoch": 7.746806039488966,
      "grad_norm": 173.70339965820312,
      "learning_rate": 2.50354884501226e-06,
      "loss": 1.1652,
      "step": 20010
    },
    {
      "epoch": 7.747193186217577,
      "grad_norm": 241.04920959472656,
      "learning_rate": 2.503118681980471e-06,
      "loss": 0.8094,
      "step": 20011
    },
    {
      "epoch": 7.747580332946186,
      "grad_norm": 8.14713191986084,
      "learning_rate": 2.5026885189486814e-06,
      "loss": 0.1704,
      "step": 20012
    },
    {
      "epoch": 7.747967479674797,
      "grad_norm": 83.08746337890625,
      "learning_rate": 2.502258355916893e-06,
      "loss": 0.6055,
      "step": 20013
    },
    {
      "epoch": 7.748354626403406,
      "grad_norm": 9.181270599365234,
      "learning_rate": 2.5018281928851034e-06,
      "loss": 0.1326,
      "step": 20014
    },
    {
      "epoch": 7.748741773132017,
      "grad_norm": 1.7642226219177246,
      "learning_rate": 2.501398029853315e-06,
      "loss": 0.0426,
      "step": 20015
    },
    {
      "epoch": 7.7491289198606275,
      "grad_norm": 76.8821029663086,
      "learning_rate": 2.5009678668215254e-06,
      "loss": 0.3256,
      "step": 20016
    },
    {
      "epoch": 7.749516066589237,
      "grad_norm": 0.958890438079834,
      "learning_rate": 2.5005377037897368e-06,
      "loss": 0.0371,
      "step": 20017
    },
    {
      "epoch": 7.749903213317848,
      "grad_norm": 8.555896759033203,
      "learning_rate": 2.5001075407579474e-06,
      "loss": 0.1394,
      "step": 20018
    },
    {
      "epoch": 7.750290360046458,
      "grad_norm": 23.146656036376953,
      "learning_rate": 2.4996773777261583e-06,
      "loss": 0.1247,
      "step": 20019
    },
    {
      "epoch": 7.750677506775068,
      "grad_norm": 31.38654899597168,
      "learning_rate": 2.4992472146943693e-06,
      "loss": 2.5021,
      "step": 20020
    },
    {
      "epoch": 7.751064653503678,
      "grad_norm": 5.279070854187012,
      "learning_rate": 2.4988170516625803e-06,
      "loss": 0.067,
      "step": 20021
    },
    {
      "epoch": 7.751451800232288,
      "grad_norm": 143.32907104492188,
      "learning_rate": 2.4983868886307913e-06,
      "loss": 3.4086,
      "step": 20022
    },
    {
      "epoch": 7.751838946960898,
      "grad_norm": 12.521788597106934,
      "learning_rate": 2.4979567255990023e-06,
      "loss": 0.2125,
      "step": 20023
    },
    {
      "epoch": 7.752226093689508,
      "grad_norm": 117.39283752441406,
      "learning_rate": 2.4975265625672133e-06,
      "loss": 0.511,
      "step": 20024
    },
    {
      "epoch": 7.7526132404181185,
      "grad_norm": 5.656827926635742,
      "learning_rate": 2.4970963995354243e-06,
      "loss": 0.0596,
      "step": 20025
    },
    {
      "epoch": 7.753000387146729,
      "grad_norm": 41.857669830322266,
      "learning_rate": 2.4966662365036353e-06,
      "loss": 0.1915,
      "step": 20026
    },
    {
      "epoch": 7.753387533875339,
      "grad_norm": 0.818098783493042,
      "learning_rate": 2.496236073471846e-06,
      "loss": 0.028,
      "step": 20027
    },
    {
      "epoch": 7.753774680603949,
      "grad_norm": 0.3527737557888031,
      "learning_rate": 2.495805910440057e-06,
      "loss": 0.0119,
      "step": 20028
    },
    {
      "epoch": 7.754161827332559,
      "grad_norm": 4.348302841186523,
      "learning_rate": 2.495375747408268e-06,
      "loss": 0.1479,
      "step": 20029
    },
    {
      "epoch": 7.754548974061169,
      "grad_norm": 276.5231628417969,
      "learning_rate": 2.494945584376479e-06,
      "loss": 0.6934,
      "step": 20030
    },
    {
      "epoch": 7.754936120789779,
      "grad_norm": 128.22219848632812,
      "learning_rate": 2.49451542134469e-06,
      "loss": 4.0135,
      "step": 20031
    },
    {
      "epoch": 7.755323267518389,
      "grad_norm": 130.07948303222656,
      "learning_rate": 2.494085258312901e-06,
      "loss": 0.8624,
      "step": 20032
    },
    {
      "epoch": 7.755710414247,
      "grad_norm": 86.15962982177734,
      "learning_rate": 2.493655095281112e-06,
      "loss": 1.9481,
      "step": 20033
    },
    {
      "epoch": 7.7560975609756095,
      "grad_norm": 70.51011657714844,
      "learning_rate": 2.493224932249323e-06,
      "loss": 2.0781,
      "step": 20034
    },
    {
      "epoch": 7.75648470770422,
      "grad_norm": 7.556073188781738,
      "learning_rate": 2.492794769217534e-06,
      "loss": 0.0833,
      "step": 20035
    },
    {
      "epoch": 7.75687185443283,
      "grad_norm": 147.0018310546875,
      "learning_rate": 2.4923646061857444e-06,
      "loss": 1.4404,
      "step": 20036
    },
    {
      "epoch": 7.75725900116144,
      "grad_norm": 125.96542358398438,
      "learning_rate": 2.4919344431539554e-06,
      "loss": 0.6591,
      "step": 20037
    },
    {
      "epoch": 7.757646147890051,
      "grad_norm": 0.5070687532424927,
      "learning_rate": 2.4915042801221663e-06,
      "loss": 0.0092,
      "step": 20038
    },
    {
      "epoch": 7.75803329461866,
      "grad_norm": 1.958152413368225,
      "learning_rate": 2.4910741170903773e-06,
      "loss": 0.0514,
      "step": 20039
    },
    {
      "epoch": 7.758420441347271,
      "grad_norm": 4.600716590881348,
      "learning_rate": 2.4906439540585883e-06,
      "loss": 0.0547,
      "step": 20040
    },
    {
      "epoch": 7.7588075880758804,
      "grad_norm": 11.728869438171387,
      "learning_rate": 2.4902137910267993e-06,
      "loss": 0.1006,
      "step": 20041
    },
    {
      "epoch": 7.759194734804491,
      "grad_norm": 36.682525634765625,
      "learning_rate": 2.4897836279950103e-06,
      "loss": 1.6294,
      "step": 20042
    },
    {
      "epoch": 7.7595818815331015,
      "grad_norm": 363.02960205078125,
      "learning_rate": 2.4893534649632213e-06,
      "loss": 1.1114,
      "step": 20043
    },
    {
      "epoch": 7.759969028261711,
      "grad_norm": 2.187281370162964,
      "learning_rate": 2.4889233019314323e-06,
      "loss": 0.0901,
      "step": 20044
    },
    {
      "epoch": 7.760356174990322,
      "grad_norm": 116.67536163330078,
      "learning_rate": 2.488493138899643e-06,
      "loss": 0.8006,
      "step": 20045
    },
    {
      "epoch": 7.760743321718931,
      "grad_norm": 187.17636108398438,
      "learning_rate": 2.488062975867854e-06,
      "loss": 3.6083,
      "step": 20046
    },
    {
      "epoch": 7.761130468447542,
      "grad_norm": 227.78526306152344,
      "learning_rate": 2.487632812836065e-06,
      "loss": 2.7725,
      "step": 20047
    },
    {
      "epoch": 7.761517615176151,
      "grad_norm": 82.25275421142578,
      "learning_rate": 2.487202649804276e-06,
      "loss": 3.1114,
      "step": 20048
    },
    {
      "epoch": 7.761904761904762,
      "grad_norm": 0.5423712134361267,
      "learning_rate": 2.486772486772487e-06,
      "loss": 0.0191,
      "step": 20049
    },
    {
      "epoch": 7.762291908633372,
      "grad_norm": 1.4099469184875488,
      "learning_rate": 2.4863423237406983e-06,
      "loss": 0.0566,
      "step": 20050
    },
    {
      "epoch": 7.762679055361982,
      "grad_norm": 26.245222091674805,
      "learning_rate": 2.485912160708909e-06,
      "loss": 2.5986,
      "step": 20051
    },
    {
      "epoch": 7.7630662020905925,
      "grad_norm": 49.329612731933594,
      "learning_rate": 2.48548199767712e-06,
      "loss": 0.5144,
      "step": 20052
    },
    {
      "epoch": 7.763453348819202,
      "grad_norm": 188.24688720703125,
      "learning_rate": 2.485051834645331e-06,
      "loss": 1.7135,
      "step": 20053
    },
    {
      "epoch": 7.763840495547813,
      "grad_norm": 36.132022857666016,
      "learning_rate": 2.484621671613542e-06,
      "loss": 2.2918,
      "step": 20054
    },
    {
      "epoch": 7.764227642276423,
      "grad_norm": 253.64894104003906,
      "learning_rate": 2.484191508581753e-06,
      "loss": 2.6878,
      "step": 20055
    },
    {
      "epoch": 7.764614789005033,
      "grad_norm": 2.5950074195861816,
      "learning_rate": 2.4837613455499638e-06,
      "loss": 0.1438,
      "step": 20056
    },
    {
      "epoch": 7.765001935733643,
      "grad_norm": 47.22578811645508,
      "learning_rate": 2.4833311825181748e-06,
      "loss": 1.3456,
      "step": 20057
    },
    {
      "epoch": 7.765389082462253,
      "grad_norm": 54.01319885253906,
      "learning_rate": 2.4829010194863858e-06,
      "loss": 2.0483,
      "step": 20058
    },
    {
      "epoch": 7.765776229190863,
      "grad_norm": 17.5714054107666,
      "learning_rate": 2.4824708564545968e-06,
      "loss": 0.1505,
      "step": 20059
    },
    {
      "epoch": 7.766163375919474,
      "grad_norm": 548.2777709960938,
      "learning_rate": 2.4820406934228073e-06,
      "loss": 0.9966,
      "step": 20060
    },
    {
      "epoch": 7.7665505226480835,
      "grad_norm": 141.95504760742188,
      "learning_rate": 2.4816105303910183e-06,
      "loss": 1.5572,
      "step": 20061
    },
    {
      "epoch": 7.766937669376694,
      "grad_norm": 96.45885467529297,
      "learning_rate": 2.4811803673592293e-06,
      "loss": 0.5658,
      "step": 20062
    },
    {
      "epoch": 7.767324816105304,
      "grad_norm": 172.58644104003906,
      "learning_rate": 2.4807502043274403e-06,
      "loss": 2.2794,
      "step": 20063
    },
    {
      "epoch": 7.767711962833914,
      "grad_norm": 61.14670181274414,
      "learning_rate": 2.4803200412956513e-06,
      "loss": 1.1571,
      "step": 20064
    },
    {
      "epoch": 7.768099109562524,
      "grad_norm": 73.24051666259766,
      "learning_rate": 2.4798898782638623e-06,
      "loss": 1.5479,
      "step": 20065
    },
    {
      "epoch": 7.768486256291134,
      "grad_norm": 80.90442657470703,
      "learning_rate": 2.4794597152320733e-06,
      "loss": 2.7284,
      "step": 20066
    },
    {
      "epoch": 7.768873403019745,
      "grad_norm": 31.058347702026367,
      "learning_rate": 2.4790295522002843e-06,
      "loss": 2.4831,
      "step": 20067
    },
    {
      "epoch": 7.7692605497483544,
      "grad_norm": 2.7182087898254395,
      "learning_rate": 2.4785993891684953e-06,
      "loss": 0.1033,
      "step": 20068
    },
    {
      "epoch": 7.769647696476965,
      "grad_norm": 3.298269748687744,
      "learning_rate": 2.478169226136706e-06,
      "loss": 0.0774,
      "step": 20069
    },
    {
      "epoch": 7.770034843205575,
      "grad_norm": 59.61498260498047,
      "learning_rate": 2.477739063104917e-06,
      "loss": 2.9021,
      "step": 20070
    },
    {
      "epoch": 7.770421989934185,
      "grad_norm": 12.218683242797852,
      "learning_rate": 2.477308900073128e-06,
      "loss": 0.0551,
      "step": 20071
    },
    {
      "epoch": 7.770809136662795,
      "grad_norm": 2.8003084659576416,
      "learning_rate": 2.476878737041339e-06,
      "loss": 0.0828,
      "step": 20072
    },
    {
      "epoch": 7.771196283391405,
      "grad_norm": 179.8188018798828,
      "learning_rate": 2.47644857400955e-06,
      "loss": 1.2399,
      "step": 20073
    },
    {
      "epoch": 7.771583430120016,
      "grad_norm": 32.582218170166016,
      "learning_rate": 2.476018410977761e-06,
      "loss": 0.0996,
      "step": 20074
    },
    {
      "epoch": 7.771970576848625,
      "grad_norm": 117.27288818359375,
      "learning_rate": 2.475588247945972e-06,
      "loss": 0.6403,
      "step": 20075
    },
    {
      "epoch": 7.772357723577236,
      "grad_norm": 26.38488006591797,
      "learning_rate": 2.4751580849141828e-06,
      "loss": 1.1208,
      "step": 20076
    },
    {
      "epoch": 7.772744870305846,
      "grad_norm": 35.4489860534668,
      "learning_rate": 2.4747279218823938e-06,
      "loss": 4.4492,
      "step": 20077
    },
    {
      "epoch": 7.773132017034456,
      "grad_norm": 34.79051971435547,
      "learning_rate": 2.4742977588506043e-06,
      "loss": 1.9629,
      "step": 20078
    },
    {
      "epoch": 7.7735191637630665,
      "grad_norm": 136.0715789794922,
      "learning_rate": 2.4738675958188153e-06,
      "loss": 3.2063,
      "step": 20079
    },
    {
      "epoch": 7.773906310491676,
      "grad_norm": 6.893307685852051,
      "learning_rate": 2.4734374327870263e-06,
      "loss": 0.1249,
      "step": 20080
    },
    {
      "epoch": 7.774293457220287,
      "grad_norm": 98.22532653808594,
      "learning_rate": 2.4730072697552373e-06,
      "loss": 0.1275,
      "step": 20081
    },
    {
      "epoch": 7.774680603948896,
      "grad_norm": 3.2592387199401855,
      "learning_rate": 2.4725771067234483e-06,
      "loss": 0.1215,
      "step": 20082
    },
    {
      "epoch": 7.775067750677507,
      "grad_norm": 90.72940063476562,
      "learning_rate": 2.4721469436916593e-06,
      "loss": 1.3969,
      "step": 20083
    },
    {
      "epoch": 7.775454897406117,
      "grad_norm": 8.90733528137207,
      "learning_rate": 2.4717167806598703e-06,
      "loss": 0.204,
      "step": 20084
    },
    {
      "epoch": 7.775842044134727,
      "grad_norm": 111.04818725585938,
      "learning_rate": 2.4712866176280813e-06,
      "loss": 1.7484,
      "step": 20085
    },
    {
      "epoch": 7.776229190863337,
      "grad_norm": 44.57596206665039,
      "learning_rate": 2.4708564545962923e-06,
      "loss": 2.0463,
      "step": 20086
    },
    {
      "epoch": 7.776616337591947,
      "grad_norm": 137.66395568847656,
      "learning_rate": 2.470426291564503e-06,
      "loss": 3.1527,
      "step": 20087
    },
    {
      "epoch": 7.7770034843205575,
      "grad_norm": 1.3683823347091675,
      "learning_rate": 2.469996128532714e-06,
      "loss": 0.0235,
      "step": 20088
    },
    {
      "epoch": 7.777390631049167,
      "grad_norm": 7.838514804840088,
      "learning_rate": 2.469565965500925e-06,
      "loss": 0.1595,
      "step": 20089
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 80.86263275146484,
      "learning_rate": 2.469135802469136e-06,
      "loss": 2.4272,
      "step": 20090
    },
    {
      "epoch": 7.778164924506388,
      "grad_norm": 58.559932708740234,
      "learning_rate": 2.4687056394373472e-06,
      "loss": 0.6008,
      "step": 20091
    },
    {
      "epoch": 7.778552071234998,
      "grad_norm": 141.5758819580078,
      "learning_rate": 2.4682754764055582e-06,
      "loss": 0.5374,
      "step": 20092
    },
    {
      "epoch": 7.778939217963608,
      "grad_norm": 174.27989196777344,
      "learning_rate": 2.467845313373769e-06,
      "loss": 1.35,
      "step": 20093
    },
    {
      "epoch": 7.779326364692219,
      "grad_norm": 80.97987365722656,
      "learning_rate": 2.46741515034198e-06,
      "loss": 0.4962,
      "step": 20094
    },
    {
      "epoch": 7.7797135114208285,
      "grad_norm": 7.557552337646484,
      "learning_rate": 2.4669849873101908e-06,
      "loss": 0.2008,
      "step": 20095
    },
    {
      "epoch": 7.780100658149439,
      "grad_norm": 28.456892013549805,
      "learning_rate": 2.4665548242784018e-06,
      "loss": 1.7749,
      "step": 20096
    },
    {
      "epoch": 7.780487804878049,
      "grad_norm": 190.44906616210938,
      "learning_rate": 2.4661246612466128e-06,
      "loss": 2.6331,
      "step": 20097
    },
    {
      "epoch": 7.780874951606659,
      "grad_norm": 1.7482550144195557,
      "learning_rate": 2.4656944982148238e-06,
      "loss": 0.0448,
      "step": 20098
    },
    {
      "epoch": 7.781262098335269,
      "grad_norm": 8.360208511352539,
      "learning_rate": 2.4652643351830348e-06,
      "loss": 0.1285,
      "step": 20099
    },
    {
      "epoch": 7.781649245063879,
      "grad_norm": 45.45309066772461,
      "learning_rate": 2.4648341721512457e-06,
      "loss": 1.3428,
      "step": 20100
    },
    {
      "epoch": 7.78203639179249,
      "grad_norm": 99.96734619140625,
      "learning_rate": 2.4644040091194567e-06,
      "loss": 0.1722,
      "step": 20101
    },
    {
      "epoch": 7.782423538521099,
      "grad_norm": 76.1609878540039,
      "learning_rate": 2.4639738460876673e-06,
      "loss": 1.6885,
      "step": 20102
    },
    {
      "epoch": 7.78281068524971,
      "grad_norm": 57.913108825683594,
      "learning_rate": 2.4635436830558783e-06,
      "loss": 1.9109,
      "step": 20103
    },
    {
      "epoch": 7.7831978319783195,
      "grad_norm": 54.06593704223633,
      "learning_rate": 2.4631135200240893e-06,
      "loss": 0.1527,
      "step": 20104
    },
    {
      "epoch": 7.78358497870693,
      "grad_norm": 113.10224914550781,
      "learning_rate": 2.4626833569923003e-06,
      "loss": 0.7858,
      "step": 20105
    },
    {
      "epoch": 7.78397212543554,
      "grad_norm": 9.725211143493652,
      "learning_rate": 2.4622531939605113e-06,
      "loss": 0.1477,
      "step": 20106
    },
    {
      "epoch": 7.78435927216415,
      "grad_norm": 1.210245966911316,
      "learning_rate": 2.4618230309287223e-06,
      "loss": 0.0521,
      "step": 20107
    },
    {
      "epoch": 7.784746418892761,
      "grad_norm": 99.4665756225586,
      "learning_rate": 2.4613928678969333e-06,
      "loss": 1.1896,
      "step": 20108
    },
    {
      "epoch": 7.78513356562137,
      "grad_norm": 48.035911560058594,
      "learning_rate": 2.4609627048651443e-06,
      "loss": 3.6232,
      "step": 20109
    },
    {
      "epoch": 7.785520712349981,
      "grad_norm": 2.5351459980010986,
      "learning_rate": 2.4605325418333552e-06,
      "loss": 0.1161,
      "step": 20110
    },
    {
      "epoch": 7.78590785907859,
      "grad_norm": 2.3366713523864746,
      "learning_rate": 2.460102378801566e-06,
      "loss": 0.1194,
      "step": 20111
    },
    {
      "epoch": 7.786295005807201,
      "grad_norm": 34.83998489379883,
      "learning_rate": 2.459672215769777e-06,
      "loss": 0.1641,
      "step": 20112
    },
    {
      "epoch": 7.786682152535811,
      "grad_norm": 50.90658950805664,
      "learning_rate": 2.459242052737988e-06,
      "loss": 0.8631,
      "step": 20113
    },
    {
      "epoch": 7.787069299264421,
      "grad_norm": 70.21470642089844,
      "learning_rate": 2.458811889706199e-06,
      "loss": 0.4841,
      "step": 20114
    },
    {
      "epoch": 7.7874564459930316,
      "grad_norm": 57.788482666015625,
      "learning_rate": 2.4583817266744098e-06,
      "loss": 1.6054,
      "step": 20115
    },
    {
      "epoch": 7.787843592721641,
      "grad_norm": 69.0838851928711,
      "learning_rate": 2.4579515636426208e-06,
      "loss": 1.48,
      "step": 20116
    },
    {
      "epoch": 7.788230739450252,
      "grad_norm": 54.36589431762695,
      "learning_rate": 2.4575214006108318e-06,
      "loss": 0.164,
      "step": 20117
    },
    {
      "epoch": 7.788617886178862,
      "grad_norm": 48.12059020996094,
      "learning_rate": 2.4570912375790428e-06,
      "loss": 1.3334,
      "step": 20118
    },
    {
      "epoch": 7.789005032907472,
      "grad_norm": 20.31854248046875,
      "learning_rate": 2.4566610745472538e-06,
      "loss": 0.1532,
      "step": 20119
    },
    {
      "epoch": 7.789392179636082,
      "grad_norm": 5.96306848526001,
      "learning_rate": 2.4562309115154643e-06,
      "loss": 0.1471,
      "step": 20120
    },
    {
      "epoch": 7.789779326364692,
      "grad_norm": 99.15217590332031,
      "learning_rate": 2.4558007484836753e-06,
      "loss": 4.0549,
      "step": 20121
    },
    {
      "epoch": 7.7901664730933025,
      "grad_norm": 43.715179443359375,
      "learning_rate": 2.4553705854518863e-06,
      "loss": 1.4869,
      "step": 20122
    },
    {
      "epoch": 7.790553619821912,
      "grad_norm": 13.477279663085938,
      "learning_rate": 2.4549404224200973e-06,
      "loss": 0.1523,
      "step": 20123
    },
    {
      "epoch": 7.790940766550523,
      "grad_norm": 23.4208984375,
      "learning_rate": 2.4545102593883083e-06,
      "loss": 0.1419,
      "step": 20124
    },
    {
      "epoch": 7.791327913279133,
      "grad_norm": 69.16239929199219,
      "learning_rate": 2.4540800963565193e-06,
      "loss": 0.9446,
      "step": 20125
    },
    {
      "epoch": 7.791715060007743,
      "grad_norm": 574.8917846679688,
      "learning_rate": 2.4536499333247303e-06,
      "loss": 2.1304,
      "step": 20126
    },
    {
      "epoch": 7.792102206736353,
      "grad_norm": 62.50947570800781,
      "learning_rate": 2.4532197702929413e-06,
      "loss": 3.9176,
      "step": 20127
    },
    {
      "epoch": 7.792489353464963,
      "grad_norm": 142.24708557128906,
      "learning_rate": 2.4527896072611523e-06,
      "loss": 2.5972,
      "step": 20128
    },
    {
      "epoch": 7.792876500193573,
      "grad_norm": 336.85186767578125,
      "learning_rate": 2.452359444229363e-06,
      "loss": 3.0624,
      "step": 20129
    },
    {
      "epoch": 7.793263646922184,
      "grad_norm": 62.618133544921875,
      "learning_rate": 2.451929281197574e-06,
      "loss": 1.8663,
      "step": 20130
    },
    {
      "epoch": 7.7936507936507935,
      "grad_norm": 108.1707992553711,
      "learning_rate": 2.451499118165785e-06,
      "loss": 0.7236,
      "step": 20131
    },
    {
      "epoch": 7.794037940379404,
      "grad_norm": 14.889813423156738,
      "learning_rate": 2.4510689551339962e-06,
      "loss": 0.1758,
      "step": 20132
    },
    {
      "epoch": 7.794425087108014,
      "grad_norm": 26.604713439941406,
      "learning_rate": 2.4506387921022072e-06,
      "loss": 2.3834,
      "step": 20133
    },
    {
      "epoch": 7.794812233836624,
      "grad_norm": 1.5395437479019165,
      "learning_rate": 2.4502086290704182e-06,
      "loss": 0.032,
      "step": 20134
    },
    {
      "epoch": 7.795199380565235,
      "grad_norm": 5.407809257507324,
      "learning_rate": 2.4497784660386288e-06,
      "loss": 0.1313,
      "step": 20135
    },
    {
      "epoch": 7.795586527293844,
      "grad_norm": 170.06454467773438,
      "learning_rate": 2.4493483030068398e-06,
      "loss": 1.4983,
      "step": 20136
    },
    {
      "epoch": 7.795973674022455,
      "grad_norm": 57.77640151977539,
      "learning_rate": 2.4489181399750508e-06,
      "loss": 0.2306,
      "step": 20137
    },
    {
      "epoch": 7.796360820751064,
      "grad_norm": 28.944599151611328,
      "learning_rate": 2.4484879769432618e-06,
      "loss": 2.1495,
      "step": 20138
    },
    {
      "epoch": 7.796747967479675,
      "grad_norm": 8.837225914001465,
      "learning_rate": 2.4480578139114728e-06,
      "loss": 0.0766,
      "step": 20139
    },
    {
      "epoch": 7.7971351142082845,
      "grad_norm": 54.00862503051758,
      "learning_rate": 2.4476276508796837e-06,
      "loss": 2.2556,
      "step": 20140
    },
    {
      "epoch": 7.797522260936895,
      "grad_norm": 0.7782221436500549,
      "learning_rate": 2.4471974878478947e-06,
      "loss": 0.0288,
      "step": 20141
    },
    {
      "epoch": 7.7979094076655056,
      "grad_norm": 12.550577163696289,
      "learning_rate": 2.4467673248161057e-06,
      "loss": 0.1928,
      "step": 20142
    },
    {
      "epoch": 7.798296554394115,
      "grad_norm": 40.07796859741211,
      "learning_rate": 2.4463371617843167e-06,
      "loss": 3.8224,
      "step": 20143
    },
    {
      "epoch": 7.798683701122726,
      "grad_norm": 32.64139175415039,
      "learning_rate": 2.4459069987525273e-06,
      "loss": 0.1735,
      "step": 20144
    },
    {
      "epoch": 7.799070847851335,
      "grad_norm": 265.53131103515625,
      "learning_rate": 2.4454768357207383e-06,
      "loss": 2.1141,
      "step": 20145
    },
    {
      "epoch": 7.799457994579946,
      "grad_norm": 32.36507797241211,
      "learning_rate": 2.4450466726889493e-06,
      "loss": 0.6022,
      "step": 20146
    },
    {
      "epoch": 7.799845141308556,
      "grad_norm": 20.259096145629883,
      "learning_rate": 2.4446165096571603e-06,
      "loss": 2.6414,
      "step": 20147
    },
    {
      "epoch": 7.800232288037166,
      "grad_norm": 2.8076415061950684,
      "learning_rate": 2.4441863466253713e-06,
      "loss": 0.046,
      "step": 20148
    },
    {
      "epoch": 7.8006194347657765,
      "grad_norm": 120.43379974365234,
      "learning_rate": 2.4437561835935823e-06,
      "loss": 0.6903,
      "step": 20149
    },
    {
      "epoch": 7.801006581494386,
      "grad_norm": 27.540239334106445,
      "learning_rate": 2.4433260205617932e-06,
      "loss": 2.0431,
      "step": 20150
    },
    {
      "epoch": 7.801393728222997,
      "grad_norm": 133.34707641601562,
      "learning_rate": 2.4428958575300042e-06,
      "loss": 0.2872,
      "step": 20151
    },
    {
      "epoch": 7.801780874951607,
      "grad_norm": 71.28265380859375,
      "learning_rate": 2.4424656944982152e-06,
      "loss": 1.041,
      "step": 20152
    },
    {
      "epoch": 7.802168021680217,
      "grad_norm": 196.76327514648438,
      "learning_rate": 2.442035531466426e-06,
      "loss": 1.4767,
      "step": 20153
    },
    {
      "epoch": 7.802555168408827,
      "grad_norm": 142.30319213867188,
      "learning_rate": 2.441605368434637e-06,
      "loss": 2.8206,
      "step": 20154
    },
    {
      "epoch": 7.802942315137437,
      "grad_norm": 141.75094604492188,
      "learning_rate": 2.4411752054028478e-06,
      "loss": 0.4586,
      "step": 20155
    },
    {
      "epoch": 7.803329461866047,
      "grad_norm": 90.26435852050781,
      "learning_rate": 2.4407450423710588e-06,
      "loss": 3.9147,
      "step": 20156
    },
    {
      "epoch": 7.803716608594657,
      "grad_norm": 82.44597625732422,
      "learning_rate": 2.4403148793392698e-06,
      "loss": 0.9424,
      "step": 20157
    },
    {
      "epoch": 7.8041037553232675,
      "grad_norm": 31.516765594482422,
      "learning_rate": 2.4398847163074808e-06,
      "loss": 1.4136,
      "step": 20158
    },
    {
      "epoch": 7.804490902051878,
      "grad_norm": 73.56424713134766,
      "learning_rate": 2.4394545532756917e-06,
      "loss": 1.8061,
      "step": 20159
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 4.835511684417725,
      "learning_rate": 2.4390243902439027e-06,
      "loss": 0.0788,
      "step": 20160
    },
    {
      "epoch": 7.805265195509098,
      "grad_norm": 18.958391189575195,
      "learning_rate": 2.4385942272121137e-06,
      "loss": 0.1023,
      "step": 20161
    },
    {
      "epoch": 7.805652342237708,
      "grad_norm": 62.09685516357422,
      "learning_rate": 2.4381640641803243e-06,
      "loss": 2.9137,
      "step": 20162
    },
    {
      "epoch": 7.806039488966318,
      "grad_norm": 203.2649383544922,
      "learning_rate": 2.4377339011485353e-06,
      "loss": 0.2449,
      "step": 20163
    },
    {
      "epoch": 7.806426635694928,
      "grad_norm": 1.545106291770935,
      "learning_rate": 2.4373037381167463e-06,
      "loss": 0.0341,
      "step": 20164
    },
    {
      "epoch": 7.806813782423538,
      "grad_norm": 9.345935821533203,
      "learning_rate": 2.4368735750849573e-06,
      "loss": 0.1738,
      "step": 20165
    },
    {
      "epoch": 7.807200929152149,
      "grad_norm": 38.751487731933594,
      "learning_rate": 2.4364434120531683e-06,
      "loss": 1.9748,
      "step": 20166
    },
    {
      "epoch": 7.8075880758807585,
      "grad_norm": 43.31400680541992,
      "learning_rate": 2.4360132490213793e-06,
      "loss": 3.3404,
      "step": 20167
    },
    {
      "epoch": 7.807975222609369,
      "grad_norm": 172.10400390625,
      "learning_rate": 2.4355830859895903e-06,
      "loss": 0.7895,
      "step": 20168
    },
    {
      "epoch": 7.80836236933798,
      "grad_norm": 88.1939697265625,
      "learning_rate": 2.4351529229578012e-06,
      "loss": 1.6739,
      "step": 20169
    },
    {
      "epoch": 7.808749516066589,
      "grad_norm": 393.5451965332031,
      "learning_rate": 2.4347227599260122e-06,
      "loss": 2.3585,
      "step": 20170
    },
    {
      "epoch": 7.8091366627952,
      "grad_norm": 4.590921401977539,
      "learning_rate": 2.434292596894223e-06,
      "loss": 0.1032,
      "step": 20171
    },
    {
      "epoch": 7.809523809523809,
      "grad_norm": 89.71419525146484,
      "learning_rate": 2.433862433862434e-06,
      "loss": 1.5681,
      "step": 20172
    },
    {
      "epoch": 7.80991095625242,
      "grad_norm": 8.695878982543945,
      "learning_rate": 2.4334322708306452e-06,
      "loss": 0.1223,
      "step": 20173
    },
    {
      "epoch": 7.8102981029810294,
      "grad_norm": 2.4972286224365234,
      "learning_rate": 2.433002107798856e-06,
      "loss": 0.1029,
      "step": 20174
    },
    {
      "epoch": 7.81068524970964,
      "grad_norm": 160.67205810546875,
      "learning_rate": 2.432571944767067e-06,
      "loss": 1.5982,
      "step": 20175
    },
    {
      "epoch": 7.8110723964382505,
      "grad_norm": 179.8491973876953,
      "learning_rate": 2.432141781735278e-06,
      "loss": 3.4574,
      "step": 20176
    },
    {
      "epoch": 7.81145954316686,
      "grad_norm": 106.52886962890625,
      "learning_rate": 2.4317116187034888e-06,
      "loss": 0.2877,
      "step": 20177
    },
    {
      "epoch": 7.811846689895471,
      "grad_norm": 197.37442016601562,
      "learning_rate": 2.4312814556716998e-06,
      "loss": 2.8956,
      "step": 20178
    },
    {
      "epoch": 7.81223383662408,
      "grad_norm": 2.9395604133605957,
      "learning_rate": 2.4308512926399107e-06,
      "loss": 0.0568,
      "step": 20179
    },
    {
      "epoch": 7.812620983352691,
      "grad_norm": 15.556378364562988,
      "learning_rate": 2.4304211296081217e-06,
      "loss": 0.2497,
      "step": 20180
    },
    {
      "epoch": 7.8130081300813,
      "grad_norm": 6.632260799407959,
      "learning_rate": 2.4299909665763327e-06,
      "loss": 0.064,
      "step": 20181
    },
    {
      "epoch": 7.813395276809911,
      "grad_norm": 33.691402435302734,
      "learning_rate": 2.4295608035445437e-06,
      "loss": 1.8814,
      "step": 20182
    },
    {
      "epoch": 7.813782423538521,
      "grad_norm": 81.73625183105469,
      "learning_rate": 2.4291306405127547e-06,
      "loss": 3.0125,
      "step": 20183
    },
    {
      "epoch": 7.814169570267131,
      "grad_norm": 108.38565063476562,
      "learning_rate": 2.4287004774809657e-06,
      "loss": 2.1543,
      "step": 20184
    },
    {
      "epoch": 7.8145567169957415,
      "grad_norm": 1.485650658607483,
      "learning_rate": 2.4282703144491767e-06,
      "loss": 0.0521,
      "step": 20185
    },
    {
      "epoch": 7.814943863724352,
      "grad_norm": 5.812991142272949,
      "learning_rate": 2.4278401514173873e-06,
      "loss": 0.0668,
      "step": 20186
    },
    {
      "epoch": 7.815331010452962,
      "grad_norm": 43.489288330078125,
      "learning_rate": 2.4274099883855983e-06,
      "loss": 0.3254,
      "step": 20187
    },
    {
      "epoch": 7.815718157181572,
      "grad_norm": 97.60359954833984,
      "learning_rate": 2.4269798253538093e-06,
      "loss": 3.9541,
      "step": 20188
    },
    {
      "epoch": 7.816105303910182,
      "grad_norm": 49.89023971557617,
      "learning_rate": 2.4265496623220202e-06,
      "loss": 0.3873,
      "step": 20189
    },
    {
      "epoch": 7.816492450638792,
      "grad_norm": 41.76511764526367,
      "learning_rate": 2.4261194992902312e-06,
      "loss": 0.1043,
      "step": 20190
    },
    {
      "epoch": 7.816879597367402,
      "grad_norm": 6.424953460693359,
      "learning_rate": 2.4256893362584422e-06,
      "loss": 0.1007,
      "step": 20191
    },
    {
      "epoch": 7.817266744096012,
      "grad_norm": 67.7168197631836,
      "learning_rate": 2.4252591732266532e-06,
      "loss": 1.2861,
      "step": 20192
    },
    {
      "epoch": 7.817653890824623,
      "grad_norm": 9.267775535583496,
      "learning_rate": 2.4248290101948642e-06,
      "loss": 0.0951,
      "step": 20193
    },
    {
      "epoch": 7.8180410375532325,
      "grad_norm": 78.23632049560547,
      "learning_rate": 2.424398847163075e-06,
      "loss": 1.8437,
      "step": 20194
    },
    {
      "epoch": 7.818428184281843,
      "grad_norm": 1.7967047691345215,
      "learning_rate": 2.4239686841312858e-06,
      "loss": 0.0609,
      "step": 20195
    },
    {
      "epoch": 7.818815331010453,
      "grad_norm": 40.67022705078125,
      "learning_rate": 2.4235385210994968e-06,
      "loss": 1.9535,
      "step": 20196
    },
    {
      "epoch": 7.819202477739063,
      "grad_norm": 5.684370994567871,
      "learning_rate": 2.4231083580677078e-06,
      "loss": 0.1268,
      "step": 20197
    },
    {
      "epoch": 7.819589624467673,
      "grad_norm": 3.4896228313446045,
      "learning_rate": 2.4226781950359188e-06,
      "loss": 0.0635,
      "step": 20198
    },
    {
      "epoch": 7.819976771196283,
      "grad_norm": 2.7978131771087646,
      "learning_rate": 2.4222480320041297e-06,
      "loss": 0.0277,
      "step": 20199
    },
    {
      "epoch": 7.820363917924894,
      "grad_norm": 11.61347484588623,
      "learning_rate": 2.4218178689723407e-06,
      "loss": 0.1173,
      "step": 20200
    },
    {
      "epoch": 7.8207510646535034,
      "grad_norm": 114.4886245727539,
      "learning_rate": 2.4213877059405517e-06,
      "loss": 1.3955,
      "step": 20201
    },
    {
      "epoch": 7.821138211382114,
      "grad_norm": 79.5090103149414,
      "learning_rate": 2.4209575429087627e-06,
      "loss": 2.713,
      "step": 20202
    },
    {
      "epoch": 7.821525358110724,
      "grad_norm": 1.7453607320785522,
      "learning_rate": 2.4205273798769737e-06,
      "loss": 0.0642,
      "step": 20203
    },
    {
      "epoch": 7.821912504839334,
      "grad_norm": 28.113710403442383,
      "learning_rate": 2.4200972168451843e-06,
      "loss": 2.2019,
      "step": 20204
    },
    {
      "epoch": 7.822299651567945,
      "grad_norm": 65.97428131103516,
      "learning_rate": 2.4196670538133953e-06,
      "loss": 0.2904,
      "step": 20205
    },
    {
      "epoch": 7.822686798296554,
      "grad_norm": 36.83809280395508,
      "learning_rate": 2.4192368907816063e-06,
      "loss": 0.2851,
      "step": 20206
    },
    {
      "epoch": 7.823073945025165,
      "grad_norm": 123.39805603027344,
      "learning_rate": 2.4188067277498173e-06,
      "loss": 1.6038,
      "step": 20207
    },
    {
      "epoch": 7.823461091753774,
      "grad_norm": 97.47889709472656,
      "learning_rate": 2.4183765647180283e-06,
      "loss": 4.5397,
      "step": 20208
    },
    {
      "epoch": 7.823848238482385,
      "grad_norm": 108.82679748535156,
      "learning_rate": 2.4179464016862392e-06,
      "loss": 2.1066,
      "step": 20209
    },
    {
      "epoch": 7.824235385210995,
      "grad_norm": 25.78021812438965,
      "learning_rate": 2.4175162386544502e-06,
      "loss": 0.3522,
      "step": 20210
    },
    {
      "epoch": 7.824622531939605,
      "grad_norm": 2.259751796722412,
      "learning_rate": 2.4170860756226612e-06,
      "loss": 0.0371,
      "step": 20211
    },
    {
      "epoch": 7.8250096786682155,
      "grad_norm": 2.35732364654541,
      "learning_rate": 2.4166559125908722e-06,
      "loss": 0.0702,
      "step": 20212
    },
    {
      "epoch": 7.825396825396825,
      "grad_norm": 30.96133804321289,
      "learning_rate": 2.416225749559083e-06,
      "loss": 2.3297,
      "step": 20213
    },
    {
      "epoch": 7.825783972125436,
      "grad_norm": 45.81420135498047,
      "learning_rate": 2.415795586527294e-06,
      "loss": 0.5425,
      "step": 20214
    },
    {
      "epoch": 7.826171118854045,
      "grad_norm": 1.288676381111145,
      "learning_rate": 2.415365423495505e-06,
      "loss": 0.0428,
      "step": 20215
    },
    {
      "epoch": 7.826558265582656,
      "grad_norm": 123.74082946777344,
      "learning_rate": 2.414935260463716e-06,
      "loss": 1.3152,
      "step": 20216
    },
    {
      "epoch": 7.826945412311266,
      "grad_norm": 136.1461944580078,
      "learning_rate": 2.414505097431927e-06,
      "loss": 0.6558,
      "step": 20217
    },
    {
      "epoch": 7.827332559039876,
      "grad_norm": 33.492591857910156,
      "learning_rate": 2.4140749344001378e-06,
      "loss": 0.0643,
      "step": 20218
    },
    {
      "epoch": 7.827719705768486,
      "grad_norm": 40.461666107177734,
      "learning_rate": 2.4136447713683487e-06,
      "loss": 0.1775,
      "step": 20219
    },
    {
      "epoch": 7.828106852497096,
      "grad_norm": 152.81883239746094,
      "learning_rate": 2.4132146083365597e-06,
      "loss": 2.8235,
      "step": 20220
    },
    {
      "epoch": 7.8284939992257065,
      "grad_norm": 102.38320922851562,
      "learning_rate": 2.4127844453047707e-06,
      "loss": 1.7398,
      "step": 20221
    },
    {
      "epoch": 7.828881145954317,
      "grad_norm": 94.01151275634766,
      "learning_rate": 2.4123542822729817e-06,
      "loss": 1.5868,
      "step": 20222
    },
    {
      "epoch": 7.829268292682927,
      "grad_norm": 58.071990966796875,
      "learning_rate": 2.4119241192411927e-06,
      "loss": 2.441,
      "step": 20223
    },
    {
      "epoch": 7.829655439411537,
      "grad_norm": 125.46820831298828,
      "learning_rate": 2.4114939562094037e-06,
      "loss": 3.1704,
      "step": 20224
    },
    {
      "epoch": 7.830042586140147,
      "grad_norm": 28.70368194580078,
      "learning_rate": 2.4110637931776147e-06,
      "loss": 0.277,
      "step": 20225
    },
    {
      "epoch": 7.830429732868757,
      "grad_norm": 52.12203598022461,
      "learning_rate": 2.4106336301458257e-06,
      "loss": 2.1385,
      "step": 20226
    },
    {
      "epoch": 7.830816879597368,
      "grad_norm": 8.352142333984375,
      "learning_rate": 2.4102034671140363e-06,
      "loss": 0.0927,
      "step": 20227
    },
    {
      "epoch": 7.8312040263259775,
      "grad_norm": 135.70205688476562,
      "learning_rate": 2.4097733040822472e-06,
      "loss": 2.8327,
      "step": 20228
    },
    {
      "epoch": 7.831591173054588,
      "grad_norm": 88.85427856445312,
      "learning_rate": 2.4093431410504582e-06,
      "loss": 1.2629,
      "step": 20229
    },
    {
      "epoch": 7.831978319783198,
      "grad_norm": 139.63754272460938,
      "learning_rate": 2.4089129780186692e-06,
      "loss": 2.6383,
      "step": 20230
    },
    {
      "epoch": 7.832365466511808,
      "grad_norm": 57.948577880859375,
      "learning_rate": 2.4084828149868802e-06,
      "loss": 1.9374,
      "step": 20231
    },
    {
      "epoch": 7.832752613240418,
      "grad_norm": 82.53407287597656,
      "learning_rate": 2.4080526519550912e-06,
      "loss": 4.0154,
      "step": 20232
    },
    {
      "epoch": 7.833139759969028,
      "grad_norm": 1.913022518157959,
      "learning_rate": 2.407622488923302e-06,
      "loss": 0.0691,
      "step": 20233
    },
    {
      "epoch": 7.833526906697639,
      "grad_norm": 2.252603530883789,
      "learning_rate": 2.407192325891513e-06,
      "loss": 0.0877,
      "step": 20234
    },
    {
      "epoch": 7.833914053426248,
      "grad_norm": 124.61572265625,
      "learning_rate": 2.406762162859724e-06,
      "loss": 3.0394,
      "step": 20235
    },
    {
      "epoch": 7.834301200154859,
      "grad_norm": 120.24654388427734,
      "learning_rate": 2.4063319998279348e-06,
      "loss": 4.1293,
      "step": 20236
    },
    {
      "epoch": 7.8346883468834685,
      "grad_norm": 45.3392448425293,
      "learning_rate": 2.4059018367961458e-06,
      "loss": 2.3989,
      "step": 20237
    },
    {
      "epoch": 7.835075493612079,
      "grad_norm": 24.245031356811523,
      "learning_rate": 2.4054716737643567e-06,
      "loss": 0.1142,
      "step": 20238
    },
    {
      "epoch": 7.835462640340689,
      "grad_norm": 28.850940704345703,
      "learning_rate": 2.4050415107325677e-06,
      "loss": 2.1372,
      "step": 20239
    },
    {
      "epoch": 7.835849787069299,
      "grad_norm": 119.21522521972656,
      "learning_rate": 2.4046113477007787e-06,
      "loss": 0.6294,
      "step": 20240
    },
    {
      "epoch": 7.83623693379791,
      "grad_norm": 14.89829158782959,
      "learning_rate": 2.4041811846689897e-06,
      "loss": 0.2544,
      "step": 20241
    },
    {
      "epoch": 7.836624080526519,
      "grad_norm": 5.827844619750977,
      "learning_rate": 2.4037510216372007e-06,
      "loss": 0.1891,
      "step": 20242
    },
    {
      "epoch": 7.83701122725513,
      "grad_norm": 71.04212188720703,
      "learning_rate": 2.4033208586054117e-06,
      "loss": 1.3293,
      "step": 20243
    },
    {
      "epoch": 7.83739837398374,
      "grad_norm": 15.611294746398926,
      "learning_rate": 2.4028906955736227e-06,
      "loss": 0.0848,
      "step": 20244
    },
    {
      "epoch": 7.83778552071235,
      "grad_norm": 49.51962661743164,
      "learning_rate": 2.4024605325418333e-06,
      "loss": 1.6455,
      "step": 20245
    },
    {
      "epoch": 7.83817266744096,
      "grad_norm": 5.512781143188477,
      "learning_rate": 2.4020303695100443e-06,
      "loss": 0.0472,
      "step": 20246
    },
    {
      "epoch": 7.83855981416957,
      "grad_norm": 3.7512001991271973,
      "learning_rate": 2.4016002064782553e-06,
      "loss": 0.0429,
      "step": 20247
    },
    {
      "epoch": 7.8389469608981805,
      "grad_norm": 28.480695724487305,
      "learning_rate": 2.4011700434464662e-06,
      "loss": 0.1959,
      "step": 20248
    },
    {
      "epoch": 7.83933410762679,
      "grad_norm": 70.82481384277344,
      "learning_rate": 2.4007398804146772e-06,
      "loss": 2.5668,
      "step": 20249
    },
    {
      "epoch": 7.839721254355401,
      "grad_norm": 0.8900238275527954,
      "learning_rate": 2.4003097173828882e-06,
      "loss": 0.0241,
      "step": 20250
    },
    {
      "epoch": 7.840108401084011,
      "grad_norm": 42.58873748779297,
      "learning_rate": 2.3998795543510992e-06,
      "loss": 3.9315,
      "step": 20251
    },
    {
      "epoch": 7.840495547812621,
      "grad_norm": 7.585798740386963,
      "learning_rate": 2.3994493913193102e-06,
      "loss": 0.1163,
      "step": 20252
    },
    {
      "epoch": 7.840882694541231,
      "grad_norm": 385.01873779296875,
      "learning_rate": 2.399019228287521e-06,
      "loss": 1.0103,
      "step": 20253
    },
    {
      "epoch": 7.841269841269841,
      "grad_norm": 63.81374740600586,
      "learning_rate": 2.3985890652557318e-06,
      "loss": 0.2441,
      "step": 20254
    },
    {
      "epoch": 7.8416569879984515,
      "grad_norm": 5.045778751373291,
      "learning_rate": 2.398158902223943e-06,
      "loss": 0.1554,
      "step": 20255
    },
    {
      "epoch": 7.842044134727061,
      "grad_norm": 36.31621170043945,
      "learning_rate": 2.397728739192154e-06,
      "loss": 2.033,
      "step": 20256
    },
    {
      "epoch": 7.842431281455672,
      "grad_norm": 12.773649215698242,
      "learning_rate": 2.397298576160365e-06,
      "loss": 0.1256,
      "step": 20257
    },
    {
      "epoch": 7.842818428184282,
      "grad_norm": 100.9388656616211,
      "learning_rate": 2.396868413128576e-06,
      "loss": 1.1487,
      "step": 20258
    },
    {
      "epoch": 7.843205574912892,
      "grad_norm": 1.33622145652771,
      "learning_rate": 2.396438250096787e-06,
      "loss": 0.0345,
      "step": 20259
    },
    {
      "epoch": 7.843592721641502,
      "grad_norm": 2.1921913623809814,
      "learning_rate": 2.3960080870649977e-06,
      "loss": 0.0331,
      "step": 20260
    },
    {
      "epoch": 7.843979868370113,
      "grad_norm": 60.04656219482422,
      "learning_rate": 2.3955779240332087e-06,
      "loss": 2.1058,
      "step": 20261
    },
    {
      "epoch": 7.844367015098722,
      "grad_norm": 74.02874755859375,
      "learning_rate": 2.3951477610014197e-06,
      "loss": 1.4326,
      "step": 20262
    },
    {
      "epoch": 7.844754161827333,
      "grad_norm": 40.80305862426758,
      "learning_rate": 2.3947175979696307e-06,
      "loss": 0.2008,
      "step": 20263
    },
    {
      "epoch": 7.8451413085559425,
      "grad_norm": 15.852376937866211,
      "learning_rate": 2.3942874349378417e-06,
      "loss": 0.0897,
      "step": 20264
    },
    {
      "epoch": 7.845528455284553,
      "grad_norm": 269.114013671875,
      "learning_rate": 2.3938572719060527e-06,
      "loss": 3.3656,
      "step": 20265
    },
    {
      "epoch": 7.845915602013163,
      "grad_norm": 3.8297555446624756,
      "learning_rate": 2.3934271088742637e-06,
      "loss": 0.1479,
      "step": 20266
    },
    {
      "epoch": 7.846302748741773,
      "grad_norm": 327.09033203125,
      "learning_rate": 2.3929969458424747e-06,
      "loss": 0.9552,
      "step": 20267
    },
    {
      "epoch": 7.846689895470384,
      "grad_norm": 3.261744976043701,
      "learning_rate": 2.3925667828106857e-06,
      "loss": 0.1031,
      "step": 20268
    },
    {
      "epoch": 7.847077042198993,
      "grad_norm": 10.92357349395752,
      "learning_rate": 2.3921366197788962e-06,
      "loss": 0.0499,
      "step": 20269
    },
    {
      "epoch": 7.847464188927604,
      "grad_norm": 86.50714874267578,
      "learning_rate": 2.3917064567471072e-06,
      "loss": 0.3773,
      "step": 20270
    },
    {
      "epoch": 7.847851335656213,
      "grad_norm": 32.422916412353516,
      "learning_rate": 2.3912762937153182e-06,
      "loss": 0.3353,
      "step": 20271
    },
    {
      "epoch": 7.848238482384824,
      "grad_norm": 19.9556827545166,
      "learning_rate": 2.3908461306835292e-06,
      "loss": 0.0648,
      "step": 20272
    },
    {
      "epoch": 7.8486256291134335,
      "grad_norm": 353.3702087402344,
      "learning_rate": 2.39041596765174e-06,
      "loss": 1.1116,
      "step": 20273
    },
    {
      "epoch": 7.849012775842044,
      "grad_norm": 134.75994873046875,
      "learning_rate": 2.389985804619951e-06,
      "loss": 1.4485,
      "step": 20274
    },
    {
      "epoch": 7.8493999225706546,
      "grad_norm": 2.1783154010772705,
      "learning_rate": 2.389555641588162e-06,
      "loss": 0.0521,
      "step": 20275
    },
    {
      "epoch": 7.849787069299264,
      "grad_norm": 91.84381103515625,
      "learning_rate": 2.389125478556373e-06,
      "loss": 1.7759,
      "step": 20276
    },
    {
      "epoch": 7.850174216027875,
      "grad_norm": 353.6701965332031,
      "learning_rate": 2.388695315524584e-06,
      "loss": 0.6692,
      "step": 20277
    },
    {
      "epoch": 7.850561362756485,
      "grad_norm": 32.710662841796875,
      "learning_rate": 2.3882651524927947e-06,
      "loss": 0.154,
      "step": 20278
    },
    {
      "epoch": 7.850948509485095,
      "grad_norm": 3.5672149658203125,
      "learning_rate": 2.3878349894610057e-06,
      "loss": 0.0779,
      "step": 20279
    },
    {
      "epoch": 7.851335656213705,
      "grad_norm": 119.45516967773438,
      "learning_rate": 2.3874048264292167e-06,
      "loss": 0.6761,
      "step": 20280
    },
    {
      "epoch": 7.851722802942315,
      "grad_norm": 13.778646469116211,
      "learning_rate": 2.3869746633974277e-06,
      "loss": 0.1103,
      "step": 20281
    },
    {
      "epoch": 7.8521099496709255,
      "grad_norm": 42.2825927734375,
      "learning_rate": 2.3865445003656387e-06,
      "loss": 0.1694,
      "step": 20282
    },
    {
      "epoch": 7.852497096399535,
      "grad_norm": 59.10871887207031,
      "learning_rate": 2.3861143373338497e-06,
      "loss": 2.1203,
      "step": 20283
    },
    {
      "epoch": 7.852884243128146,
      "grad_norm": 267.62237548828125,
      "learning_rate": 2.3856841743020607e-06,
      "loss": 1.4578,
      "step": 20284
    },
    {
      "epoch": 7.853271389856756,
      "grad_norm": 5.154130935668945,
      "learning_rate": 2.3852540112702717e-06,
      "loss": 0.1102,
      "step": 20285
    },
    {
      "epoch": 7.853658536585366,
      "grad_norm": 15.436983108520508,
      "learning_rate": 2.3848238482384827e-06,
      "loss": 0.1106,
      "step": 20286
    },
    {
      "epoch": 7.854045683313976,
      "grad_norm": 47.81187057495117,
      "learning_rate": 2.3843936852066932e-06,
      "loss": 1.6503,
      "step": 20287
    },
    {
      "epoch": 7.854432830042586,
      "grad_norm": 18.638601303100586,
      "learning_rate": 2.3839635221749042e-06,
      "loss": 0.1123,
      "step": 20288
    },
    {
      "epoch": 7.854819976771196,
      "grad_norm": 81.39015197753906,
      "learning_rate": 2.3835333591431152e-06,
      "loss": 1.5095,
      "step": 20289
    },
    {
      "epoch": 7.855207123499806,
      "grad_norm": 0.4784701466560364,
      "learning_rate": 2.3831031961113262e-06,
      "loss": 0.0081,
      "step": 20290
    },
    {
      "epoch": 7.8555942702284165,
      "grad_norm": 10.03074836730957,
      "learning_rate": 2.3826730330795372e-06,
      "loss": 0.1102,
      "step": 20291
    },
    {
      "epoch": 7.855981416957027,
      "grad_norm": 1.9100911617279053,
      "learning_rate": 2.382242870047748e-06,
      "loss": 0.107,
      "step": 20292
    },
    {
      "epoch": 7.856368563685637,
      "grad_norm": 57.67583465576172,
      "learning_rate": 2.381812707015959e-06,
      "loss": 0.1442,
      "step": 20293
    },
    {
      "epoch": 7.856755710414247,
      "grad_norm": 53.91947555541992,
      "learning_rate": 2.38138254398417e-06,
      "loss": 0.8041,
      "step": 20294
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 1.3622938394546509,
      "learning_rate": 2.380952380952381e-06,
      "loss": 0.0547,
      "step": 20295
    },
    {
      "epoch": 7.857530003871467,
      "grad_norm": 95.7437973022461,
      "learning_rate": 2.380522217920592e-06,
      "loss": 0.558,
      "step": 20296
    },
    {
      "epoch": 7.857917150600078,
      "grad_norm": 133.7532501220703,
      "learning_rate": 2.380092054888803e-06,
      "loss": 0.6694,
      "step": 20297
    },
    {
      "epoch": 7.858304297328687,
      "grad_norm": 15.276660919189453,
      "learning_rate": 2.379661891857014e-06,
      "loss": 0.2565,
      "step": 20298
    },
    {
      "epoch": 7.858691444057298,
      "grad_norm": 1.5762816667556763,
      "learning_rate": 2.379231728825225e-06,
      "loss": 0.0661,
      "step": 20299
    },
    {
      "epoch": 7.8590785907859075,
      "grad_norm": 51.915977478027344,
      "learning_rate": 2.378801565793436e-06,
      "loss": 2.1781,
      "step": 20300
    },
    {
      "epoch": 7.859465737514518,
      "grad_norm": 18.28013038635254,
      "learning_rate": 2.378371402761647e-06,
      "loss": 0.1267,
      "step": 20301
    },
    {
      "epoch": 7.859852884243129,
      "grad_norm": 139.52088928222656,
      "learning_rate": 2.3779412397298577e-06,
      "loss": 2.0963,
      "step": 20302
    },
    {
      "epoch": 7.860240030971738,
      "grad_norm": 64.6101303100586,
      "learning_rate": 2.3775110766980687e-06,
      "loss": 1.5968,
      "step": 20303
    },
    {
      "epoch": 7.860627177700349,
      "grad_norm": 132.1900177001953,
      "learning_rate": 2.3770809136662797e-06,
      "loss": 2.1497,
      "step": 20304
    },
    {
      "epoch": 7.861014324428958,
      "grad_norm": 12.452381134033203,
      "learning_rate": 2.3766507506344907e-06,
      "loss": 0.2322,
      "step": 20305
    },
    {
      "epoch": 7.861401471157569,
      "grad_norm": 64.28160095214844,
      "learning_rate": 2.3762205876027017e-06,
      "loss": 0.4127,
      "step": 20306
    },
    {
      "epoch": 7.861788617886178,
      "grad_norm": 162.0465850830078,
      "learning_rate": 2.3757904245709127e-06,
      "loss": 1.2216,
      "step": 20307
    },
    {
      "epoch": 7.862175764614789,
      "grad_norm": 241.1265106201172,
      "learning_rate": 2.3753602615391237e-06,
      "loss": 1.711,
      "step": 20308
    },
    {
      "epoch": 7.8625629113433995,
      "grad_norm": 23.249006271362305,
      "learning_rate": 2.3749300985073347e-06,
      "loss": 1.828,
      "step": 20309
    },
    {
      "epoch": 7.862950058072009,
      "grad_norm": 145.3975067138672,
      "learning_rate": 2.3744999354755456e-06,
      "loss": 0.9478,
      "step": 20310
    },
    {
      "epoch": 7.86333720480062,
      "grad_norm": 80.65836334228516,
      "learning_rate": 2.3740697724437562e-06,
      "loss": 0.6317,
      "step": 20311
    },
    {
      "epoch": 7.863724351529229,
      "grad_norm": 160.48162841796875,
      "learning_rate": 2.373639609411967e-06,
      "loss": 1.2038,
      "step": 20312
    },
    {
      "epoch": 7.86411149825784,
      "grad_norm": 8.51412582397461,
      "learning_rate": 2.373209446380178e-06,
      "loss": 0.1506,
      "step": 20313
    },
    {
      "epoch": 7.86449864498645,
      "grad_norm": 27.774791717529297,
      "learning_rate": 2.372779283348389e-06,
      "loss": 2.6748,
      "step": 20314
    },
    {
      "epoch": 7.86488579171506,
      "grad_norm": 20.38025665283203,
      "learning_rate": 2.3723491203166e-06,
      "loss": 0.1112,
      "step": 20315
    },
    {
      "epoch": 7.86527293844367,
      "grad_norm": 32.70541000366211,
      "learning_rate": 2.371918957284811e-06,
      "loss": 0.2008,
      "step": 20316
    },
    {
      "epoch": 7.86566008517228,
      "grad_norm": 6.4902544021606445,
      "learning_rate": 2.371488794253022e-06,
      "loss": 0.1146,
      "step": 20317
    },
    {
      "epoch": 7.8660472319008905,
      "grad_norm": 1.563873291015625,
      "learning_rate": 2.371058631221233e-06,
      "loss": 0.0523,
      "step": 20318
    },
    {
      "epoch": 7.866434378629501,
      "grad_norm": 139.69898986816406,
      "learning_rate": 2.370628468189444e-06,
      "loss": 0.9794,
      "step": 20319
    },
    {
      "epoch": 7.866821525358111,
      "grad_norm": 36.62693786621094,
      "learning_rate": 2.3701983051576547e-06,
      "loss": 1.4899,
      "step": 20320
    },
    {
      "epoch": 7.867208672086721,
      "grad_norm": 1.501128911972046,
      "learning_rate": 2.3697681421258657e-06,
      "loss": 0.0225,
      "step": 20321
    },
    {
      "epoch": 7.867595818815331,
      "grad_norm": 48.62833023071289,
      "learning_rate": 2.3693379790940767e-06,
      "loss": 0.1309,
      "step": 20322
    },
    {
      "epoch": 7.867982965543941,
      "grad_norm": 118.41678619384766,
      "learning_rate": 2.3689078160622877e-06,
      "loss": 0.3142,
      "step": 20323
    },
    {
      "epoch": 7.868370112272551,
      "grad_norm": 53.02676010131836,
      "learning_rate": 2.3684776530304987e-06,
      "loss": 1.3443,
      "step": 20324
    },
    {
      "epoch": 7.868757259001161,
      "grad_norm": 0.32879090309143066,
      "learning_rate": 2.3680474899987097e-06,
      "loss": 0.0076,
      "step": 20325
    },
    {
      "epoch": 7.869144405729772,
      "grad_norm": 48.53895568847656,
      "learning_rate": 2.3676173269669207e-06,
      "loss": 0.1788,
      "step": 20326
    },
    {
      "epoch": 7.8695315524583815,
      "grad_norm": 86.00119018554688,
      "learning_rate": 2.3671871639351317e-06,
      "loss": 0.2084,
      "step": 20327
    },
    {
      "epoch": 7.869918699186992,
      "grad_norm": 113.86103820800781,
      "learning_rate": 2.3667570009033427e-06,
      "loss": 1.2201,
      "step": 20328
    },
    {
      "epoch": 7.870305845915602,
      "grad_norm": 57.61820983886719,
      "learning_rate": 2.3663268378715532e-06,
      "loss": 1.923,
      "step": 20329
    },
    {
      "epoch": 7.870692992644212,
      "grad_norm": 14.560577392578125,
      "learning_rate": 2.3658966748397642e-06,
      "loss": 0.0908,
      "step": 20330
    },
    {
      "epoch": 7.871080139372822,
      "grad_norm": 154.53009033203125,
      "learning_rate": 2.3654665118079752e-06,
      "loss": 1.3593,
      "step": 20331
    },
    {
      "epoch": 7.871467286101432,
      "grad_norm": 477.8395080566406,
      "learning_rate": 2.365036348776186e-06,
      "loss": 1.0835,
      "step": 20332
    },
    {
      "epoch": 7.871854432830043,
      "grad_norm": 22.37746238708496,
      "learning_rate": 2.364606185744397e-06,
      "loss": 4.4799,
      "step": 20333
    },
    {
      "epoch": 7.8722415795586524,
      "grad_norm": 181.41798400878906,
      "learning_rate": 2.364176022712608e-06,
      "loss": 2.3141,
      "step": 20334
    },
    {
      "epoch": 7.872628726287263,
      "grad_norm": 13.068609237670898,
      "learning_rate": 2.363745859680819e-06,
      "loss": 0.2567,
      "step": 20335
    },
    {
      "epoch": 7.8730158730158735,
      "grad_norm": 136.43283081054688,
      "learning_rate": 2.36331569664903e-06,
      "loss": 3.1912,
      "step": 20336
    },
    {
      "epoch": 7.873403019744483,
      "grad_norm": 108.97529602050781,
      "learning_rate": 2.362885533617241e-06,
      "loss": 0.2919,
      "step": 20337
    },
    {
      "epoch": 7.873790166473094,
      "grad_norm": 66.10543823242188,
      "learning_rate": 2.362455370585452e-06,
      "loss": 1.8978,
      "step": 20338
    },
    {
      "epoch": 7.874177313201703,
      "grad_norm": 128.97413635253906,
      "learning_rate": 2.362025207553663e-06,
      "loss": 1.2308,
      "step": 20339
    },
    {
      "epoch": 7.874564459930314,
      "grad_norm": 1.0152130126953125,
      "learning_rate": 2.361595044521874e-06,
      "loss": 0.0275,
      "step": 20340
    },
    {
      "epoch": 7.874951606658923,
      "grad_norm": 62.545806884765625,
      "learning_rate": 2.361164881490085e-06,
      "loss": 0.2238,
      "step": 20341
    },
    {
      "epoch": 7.875338753387534,
      "grad_norm": 2.0184078216552734,
      "learning_rate": 2.360734718458296e-06,
      "loss": 0.0831,
      "step": 20342
    },
    {
      "epoch": 7.875725900116144,
      "grad_norm": 0.811004102230072,
      "learning_rate": 2.360304555426507e-06,
      "loss": 0.0289,
      "step": 20343
    },
    {
      "epoch": 7.876113046844754,
      "grad_norm": 15.892788887023926,
      "learning_rate": 2.3598743923947177e-06,
      "loss": 0.1813,
      "step": 20344
    },
    {
      "epoch": 7.8765001935733645,
      "grad_norm": 255.6324462890625,
      "learning_rate": 2.3594442293629287e-06,
      "loss": 1.0998,
      "step": 20345
    },
    {
      "epoch": 7.876887340301974,
      "grad_norm": 234.0694580078125,
      "learning_rate": 2.3590140663311397e-06,
      "loss": 5.0029,
      "step": 20346
    },
    {
      "epoch": 7.877274487030585,
      "grad_norm": 1.160610318183899,
      "learning_rate": 2.3585839032993507e-06,
      "loss": 0.0308,
      "step": 20347
    },
    {
      "epoch": 7.877661633759194,
      "grad_norm": 6.265442371368408,
      "learning_rate": 2.3581537402675617e-06,
      "loss": 0.0501,
      "step": 20348
    },
    {
      "epoch": 7.878048780487805,
      "grad_norm": 116.49383544921875,
      "learning_rate": 2.3577235772357727e-06,
      "loss": 3.3174,
      "step": 20349
    },
    {
      "epoch": 7.878435927216415,
      "grad_norm": 2.469618320465088,
      "learning_rate": 2.3572934142039836e-06,
      "loss": 0.0379,
      "step": 20350
    },
    {
      "epoch": 7.878823073945025,
      "grad_norm": 47.46149444580078,
      "learning_rate": 2.3568632511721946e-06,
      "loss": 0.1985,
      "step": 20351
    },
    {
      "epoch": 7.879210220673635,
      "grad_norm": 233.16329956054688,
      "learning_rate": 2.3564330881404056e-06,
      "loss": 3.2851,
      "step": 20352
    },
    {
      "epoch": 7.879597367402246,
      "grad_norm": 91.48412322998047,
      "learning_rate": 2.356002925108616e-06,
      "loss": 3.5338,
      "step": 20353
    },
    {
      "epoch": 7.8799845141308555,
      "grad_norm": 16.43353843688965,
      "learning_rate": 2.355572762076827e-06,
      "loss": 0.0774,
      "step": 20354
    },
    {
      "epoch": 7.880371660859466,
      "grad_norm": 71.72456359863281,
      "learning_rate": 2.355142599045038e-06,
      "loss": 3.6877,
      "step": 20355
    },
    {
      "epoch": 7.880758807588076,
      "grad_norm": 155.49658203125,
      "learning_rate": 2.354712436013249e-06,
      "loss": 0.6004,
      "step": 20356
    },
    {
      "epoch": 7.881145954316686,
      "grad_norm": 103.14993286132812,
      "learning_rate": 2.35428227298146e-06,
      "loss": 1.9817,
      "step": 20357
    },
    {
      "epoch": 7.881533101045296,
      "grad_norm": 21.887500762939453,
      "learning_rate": 2.353852109949671e-06,
      "loss": 0.1096,
      "step": 20358
    },
    {
      "epoch": 7.881920247773906,
      "grad_norm": 2.0821261405944824,
      "learning_rate": 2.353421946917882e-06,
      "loss": 0.032,
      "step": 20359
    },
    {
      "epoch": 7.882307394502517,
      "grad_norm": 2.550346612930298,
      "learning_rate": 2.352991783886093e-06,
      "loss": 0.1113,
      "step": 20360
    },
    {
      "epoch": 7.8826945412311265,
      "grad_norm": 29.197242736816406,
      "learning_rate": 2.352561620854304e-06,
      "loss": 0.1382,
      "step": 20361
    },
    {
      "epoch": 7.883081687959737,
      "grad_norm": 67.08374786376953,
      "learning_rate": 2.3521314578225147e-06,
      "loss": 0.1883,
      "step": 20362
    },
    {
      "epoch": 7.883468834688347,
      "grad_norm": 259.3245544433594,
      "learning_rate": 2.3517012947907257e-06,
      "loss": 0.8407,
      "step": 20363
    },
    {
      "epoch": 7.883855981416957,
      "grad_norm": 0.8731616735458374,
      "learning_rate": 2.3512711317589367e-06,
      "loss": 0.0247,
      "step": 20364
    },
    {
      "epoch": 7.884243128145567,
      "grad_norm": 108.11994171142578,
      "learning_rate": 2.3508409687271477e-06,
      "loss": 1.5237,
      "step": 20365
    },
    {
      "epoch": 7.884630274874177,
      "grad_norm": 71.99356079101562,
      "learning_rate": 2.3504108056953587e-06,
      "loss": 4.4475,
      "step": 20366
    },
    {
      "epoch": 7.885017421602788,
      "grad_norm": 147.63706970214844,
      "learning_rate": 2.3499806426635697e-06,
      "loss": 1.2262,
      "step": 20367
    },
    {
      "epoch": 7.885404568331397,
      "grad_norm": 67.82819366455078,
      "learning_rate": 2.3495504796317807e-06,
      "loss": 2.7419,
      "step": 20368
    },
    {
      "epoch": 7.885791715060008,
      "grad_norm": 25.604001998901367,
      "learning_rate": 2.3491203165999916e-06,
      "loss": 0.116,
      "step": 20369
    },
    {
      "epoch": 7.886178861788618,
      "grad_norm": 87.86863708496094,
      "learning_rate": 2.3486901535682026e-06,
      "loss": 1.1192,
      "step": 20370
    },
    {
      "epoch": 7.886566008517228,
      "grad_norm": 80.1817398071289,
      "learning_rate": 2.348259990536413e-06,
      "loss": 3.2455,
      "step": 20371
    },
    {
      "epoch": 7.8869531552458385,
      "grad_norm": 123.39696502685547,
      "learning_rate": 2.347829827504624e-06,
      "loss": 0.3816,
      "step": 20372
    },
    {
      "epoch": 7.887340301974448,
      "grad_norm": 1.2195451259613037,
      "learning_rate": 2.347399664472835e-06,
      "loss": 0.0419,
      "step": 20373
    },
    {
      "epoch": 7.887727448703059,
      "grad_norm": 1.2009596824645996,
      "learning_rate": 2.346969501441046e-06,
      "loss": 0.0103,
      "step": 20374
    },
    {
      "epoch": 7.888114595431668,
      "grad_norm": 0.9287985563278198,
      "learning_rate": 2.346539338409257e-06,
      "loss": 0.0323,
      "step": 20375
    },
    {
      "epoch": 7.888501742160279,
      "grad_norm": 4.023707866668701,
      "learning_rate": 2.346109175377468e-06,
      "loss": 0.0431,
      "step": 20376
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 32.93541717529297,
      "learning_rate": 2.345679012345679e-06,
      "loss": 2.0909,
      "step": 20377
    },
    {
      "epoch": 7.889276035617499,
      "grad_norm": 93.01315307617188,
      "learning_rate": 2.34524884931389e-06,
      "loss": 1.4609,
      "step": 20378
    },
    {
      "epoch": 7.889663182346109,
      "grad_norm": 1.3954342603683472,
      "learning_rate": 2.344818686282101e-06,
      "loss": 0.0265,
      "step": 20379
    },
    {
      "epoch": 7.890050329074719,
      "grad_norm": 82.02468872070312,
      "learning_rate": 2.344388523250312e-06,
      "loss": 1.9179,
      "step": 20380
    },
    {
      "epoch": 7.8904374758033295,
      "grad_norm": 69.34151458740234,
      "learning_rate": 2.343958360218523e-06,
      "loss": 1.5774,
      "step": 20381
    },
    {
      "epoch": 7.890824622531939,
      "grad_norm": 289.0635986328125,
      "learning_rate": 2.343528197186734e-06,
      "loss": 0.5119,
      "step": 20382
    },
    {
      "epoch": 7.89121176926055,
      "grad_norm": 98.09193420410156,
      "learning_rate": 2.343098034154945e-06,
      "loss": 0.7307,
      "step": 20383
    },
    {
      "epoch": 7.89159891598916,
      "grad_norm": 231.047119140625,
      "learning_rate": 2.342667871123156e-06,
      "loss": 2.3165,
      "step": 20384
    },
    {
      "epoch": 7.89198606271777,
      "grad_norm": 84.2180404663086,
      "learning_rate": 2.342237708091367e-06,
      "loss": 0.2124,
      "step": 20385
    },
    {
      "epoch": 7.89237320944638,
      "grad_norm": 106.17635345458984,
      "learning_rate": 2.3418075450595777e-06,
      "loss": 1.4032,
      "step": 20386
    },
    {
      "epoch": 7.89276035617499,
      "grad_norm": 107.52332305908203,
      "learning_rate": 2.3413773820277887e-06,
      "loss": 0.6016,
      "step": 20387
    },
    {
      "epoch": 7.8931475029036005,
      "grad_norm": 174.76876831054688,
      "learning_rate": 2.3409472189959997e-06,
      "loss": 1.2733,
      "step": 20388
    },
    {
      "epoch": 7.893534649632211,
      "grad_norm": 1.9411431550979614,
      "learning_rate": 2.3405170559642106e-06,
      "loss": 0.0931,
      "step": 20389
    },
    {
      "epoch": 7.893921796360821,
      "grad_norm": 0.47614800930023193,
      "learning_rate": 2.3400868929324216e-06,
      "loss": 0.0155,
      "step": 20390
    },
    {
      "epoch": 7.894308943089431,
      "grad_norm": 65.46110534667969,
      "learning_rate": 2.3396567299006326e-06,
      "loss": 3.8138,
      "step": 20391
    },
    {
      "epoch": 7.894696089818041,
      "grad_norm": 6.317297458648682,
      "learning_rate": 2.3392265668688436e-06,
      "loss": 0.033,
      "step": 20392
    },
    {
      "epoch": 7.895083236546651,
      "grad_norm": 76.05266571044922,
      "learning_rate": 2.3387964038370546e-06,
      "loss": 1.516,
      "step": 20393
    },
    {
      "epoch": 7.895470383275262,
      "grad_norm": 2.7242753505706787,
      "learning_rate": 2.3383662408052656e-06,
      "loss": 0.0349,
      "step": 20394
    },
    {
      "epoch": 7.895857530003871,
      "grad_norm": 142.18202209472656,
      "learning_rate": 2.337936077773476e-06,
      "loss": 0.3632,
      "step": 20395
    },
    {
      "epoch": 7.896244676732482,
      "grad_norm": 2.4478702545166016,
      "learning_rate": 2.337505914741687e-06,
      "loss": 0.0443,
      "step": 20396
    },
    {
      "epoch": 7.8966318234610915,
      "grad_norm": 79.14344787597656,
      "learning_rate": 2.337075751709898e-06,
      "loss": 1.858,
      "step": 20397
    },
    {
      "epoch": 7.897018970189702,
      "grad_norm": 466.1171569824219,
      "learning_rate": 2.336645588678109e-06,
      "loss": 0.9142,
      "step": 20398
    },
    {
      "epoch": 7.897406116918312,
      "grad_norm": 3.8994195461273193,
      "learning_rate": 2.33621542564632e-06,
      "loss": 0.0991,
      "step": 20399
    },
    {
      "epoch": 7.897793263646922,
      "grad_norm": 125.49991607666016,
      "learning_rate": 2.335785262614531e-06,
      "loss": 1.911,
      "step": 20400
    },
    {
      "epoch": 7.898180410375533,
      "grad_norm": 2.5968024730682373,
      "learning_rate": 2.335355099582742e-06,
      "loss": 0.0363,
      "step": 20401
    },
    {
      "epoch": 7.898567557104142,
      "grad_norm": 3.3073184490203857,
      "learning_rate": 2.334924936550953e-06,
      "loss": 0.0933,
      "step": 20402
    },
    {
      "epoch": 7.898954703832753,
      "grad_norm": 141.6222381591797,
      "learning_rate": 2.334494773519164e-06,
      "loss": 1.3512,
      "step": 20403
    },
    {
      "epoch": 7.899341850561362,
      "grad_norm": 93.26539611816406,
      "learning_rate": 2.3340646104873747e-06,
      "loss": 1.1719,
      "step": 20404
    },
    {
      "epoch": 7.899728997289973,
      "grad_norm": 29.620189666748047,
      "learning_rate": 2.3336344474555857e-06,
      "loss": 2.4876,
      "step": 20405
    },
    {
      "epoch": 7.900116144018583,
      "grad_norm": 147.2442626953125,
      "learning_rate": 2.3332042844237967e-06,
      "loss": 1.5822,
      "step": 20406
    },
    {
      "epoch": 7.900503290747193,
      "grad_norm": 192.414794921875,
      "learning_rate": 2.3327741213920077e-06,
      "loss": 1.7698,
      "step": 20407
    },
    {
      "epoch": 7.9008904374758036,
      "grad_norm": 7.7849578857421875,
      "learning_rate": 2.3323439583602187e-06,
      "loss": 0.0556,
      "step": 20408
    },
    {
      "epoch": 7.901277584204413,
      "grad_norm": 18.258089065551758,
      "learning_rate": 2.3319137953284296e-06,
      "loss": 0.2874,
      "step": 20409
    },
    {
      "epoch": 7.901664730933024,
      "grad_norm": 45.80833053588867,
      "learning_rate": 2.3314836322966406e-06,
      "loss": 1.7102,
      "step": 20410
    },
    {
      "epoch": 7.902051877661634,
      "grad_norm": 20.09004783630371,
      "learning_rate": 2.3310534692648516e-06,
      "loss": 0.1754,
      "step": 20411
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 3.7659387588500977,
      "learning_rate": 2.3306233062330626e-06,
      "loss": 0.0641,
      "step": 20412
    },
    {
      "epoch": 7.902826171118854,
      "grad_norm": 1.990814208984375,
      "learning_rate": 2.330193143201273e-06,
      "loss": 0.0601,
      "step": 20413
    },
    {
      "epoch": 7.903213317847464,
      "grad_norm": 58.662960052490234,
      "learning_rate": 2.329762980169484e-06,
      "loss": 1.6236,
      "step": 20414
    },
    {
      "epoch": 7.9036004645760745,
      "grad_norm": 137.0106964111328,
      "learning_rate": 2.329332817137695e-06,
      "loss": 1.2037,
      "step": 20415
    },
    {
      "epoch": 7.903987611304684,
      "grad_norm": 64.16912078857422,
      "learning_rate": 2.328902654105906e-06,
      "loss": 2.1945,
      "step": 20416
    },
    {
      "epoch": 7.904374758033295,
      "grad_norm": 4.837998390197754,
      "learning_rate": 2.328472491074117e-06,
      "loss": 0.0364,
      "step": 20417
    },
    {
      "epoch": 7.904761904761905,
      "grad_norm": 56.879493713378906,
      "learning_rate": 2.328042328042328e-06,
      "loss": 1.3425,
      "step": 20418
    },
    {
      "epoch": 7.905149051490515,
      "grad_norm": 1.985342025756836,
      "learning_rate": 2.327612165010539e-06,
      "loss": 0.1034,
      "step": 20419
    },
    {
      "epoch": 7.905536198219125,
      "grad_norm": 82.80644989013672,
      "learning_rate": 2.32718200197875e-06,
      "loss": 0.4227,
      "step": 20420
    },
    {
      "epoch": 7.905923344947735,
      "grad_norm": 113.85134887695312,
      "learning_rate": 2.326751838946961e-06,
      "loss": 1.8713,
      "step": 20421
    },
    {
      "epoch": 7.906310491676345,
      "grad_norm": 41.50799560546875,
      "learning_rate": 2.326321675915172e-06,
      "loss": 2.3117,
      "step": 20422
    },
    {
      "epoch": 7.906697638404955,
      "grad_norm": 90.35820770263672,
      "learning_rate": 2.325891512883383e-06,
      "loss": 2.4734,
      "step": 20423
    },
    {
      "epoch": 7.9070847851335655,
      "grad_norm": 154.15170288085938,
      "learning_rate": 2.325461349851594e-06,
      "loss": 2.695,
      "step": 20424
    },
    {
      "epoch": 7.907471931862176,
      "grad_norm": 72.13683319091797,
      "learning_rate": 2.325031186819805e-06,
      "loss": 4.0053,
      "step": 20425
    },
    {
      "epoch": 7.907859078590786,
      "grad_norm": 1.4403355121612549,
      "learning_rate": 2.324601023788016e-06,
      "loss": 0.0486,
      "step": 20426
    },
    {
      "epoch": 7.908246225319396,
      "grad_norm": 0.8985313177108765,
      "learning_rate": 2.324170860756227e-06,
      "loss": 0.0334,
      "step": 20427
    },
    {
      "epoch": 7.908633372048007,
      "grad_norm": 165.5183868408203,
      "learning_rate": 2.3237406977244376e-06,
      "loss": 0.3756,
      "step": 20428
    },
    {
      "epoch": 7.909020518776616,
      "grad_norm": 68.22005462646484,
      "learning_rate": 2.3233105346926486e-06,
      "loss": 3.1317,
      "step": 20429
    },
    {
      "epoch": 7.909407665505227,
      "grad_norm": 129.75804138183594,
      "learning_rate": 2.3228803716608596e-06,
      "loss": 0.5544,
      "step": 20430
    },
    {
      "epoch": 7.909794812233836,
      "grad_norm": 63.06715774536133,
      "learning_rate": 2.3224502086290706e-06,
      "loss": 3.5766,
      "step": 20431
    },
    {
      "epoch": 7.910181958962447,
      "grad_norm": 41.20192337036133,
      "learning_rate": 2.3220200455972816e-06,
      "loss": 0.1121,
      "step": 20432
    },
    {
      "epoch": 7.9105691056910565,
      "grad_norm": 5.28855562210083,
      "learning_rate": 2.3215898825654926e-06,
      "loss": 0.1146,
      "step": 20433
    },
    {
      "epoch": 7.910956252419667,
      "grad_norm": 203.29733276367188,
      "learning_rate": 2.3211597195337036e-06,
      "loss": 1.7366,
      "step": 20434
    },
    {
      "epoch": 7.9113433991482776,
      "grad_norm": 1.7016136646270752,
      "learning_rate": 2.3207295565019146e-06,
      "loss": 0.0463,
      "step": 20435
    },
    {
      "epoch": 7.911730545876887,
      "grad_norm": 90.58706665039062,
      "learning_rate": 2.3202993934701256e-06,
      "loss": 1.942,
      "step": 20436
    },
    {
      "epoch": 7.912117692605498,
      "grad_norm": 160.2318878173828,
      "learning_rate": 2.319869230438336e-06,
      "loss": 1.7581,
      "step": 20437
    },
    {
      "epoch": 7.912504839334107,
      "grad_norm": 0.9722126126289368,
      "learning_rate": 2.319439067406547e-06,
      "loss": 0.0161,
      "step": 20438
    },
    {
      "epoch": 7.912891986062718,
      "grad_norm": 194.93231201171875,
      "learning_rate": 2.319008904374758e-06,
      "loss": 3.4694,
      "step": 20439
    },
    {
      "epoch": 7.913279132791327,
      "grad_norm": 0.1990107297897339,
      "learning_rate": 2.318578741342969e-06,
      "loss": 0.0052,
      "step": 20440
    },
    {
      "epoch": 7.913666279519938,
      "grad_norm": 114.58414459228516,
      "learning_rate": 2.31814857831118e-06,
      "loss": 0.4342,
      "step": 20441
    },
    {
      "epoch": 7.9140534262485485,
      "grad_norm": 113.2376480102539,
      "learning_rate": 2.317718415279391e-06,
      "loss": 1.0244,
      "step": 20442
    },
    {
      "epoch": 7.914440572977158,
      "grad_norm": 1.6344366073608398,
      "learning_rate": 2.317288252247602e-06,
      "loss": 0.0471,
      "step": 20443
    },
    {
      "epoch": 7.914827719705769,
      "grad_norm": 175.78359985351562,
      "learning_rate": 2.316858089215813e-06,
      "loss": 0.516,
      "step": 20444
    },
    {
      "epoch": 7.915214866434379,
      "grad_norm": 29.658079147338867,
      "learning_rate": 2.316427926184024e-06,
      "loss": 2.144,
      "step": 20445
    },
    {
      "epoch": 7.915602013162989,
      "grad_norm": 94.77000427246094,
      "learning_rate": 2.3159977631522347e-06,
      "loss": 2.0184,
      "step": 20446
    },
    {
      "epoch": 7.915989159891599,
      "grad_norm": 230.75257873535156,
      "learning_rate": 2.3155676001204457e-06,
      "loss": 0.3845,
      "step": 20447
    },
    {
      "epoch": 7.916376306620209,
      "grad_norm": 254.78627014160156,
      "learning_rate": 2.3151374370886566e-06,
      "loss": 5.5898,
      "step": 20448
    },
    {
      "epoch": 7.916763453348819,
      "grad_norm": 145.9250030517578,
      "learning_rate": 2.3147072740568676e-06,
      "loss": 2.542,
      "step": 20449
    },
    {
      "epoch": 7.917150600077429,
      "grad_norm": 82.01261138916016,
      "learning_rate": 2.3142771110250786e-06,
      "loss": 1.2294,
      "step": 20450
    },
    {
      "epoch": 7.9175377468060395,
      "grad_norm": 25.176145553588867,
      "learning_rate": 2.3138469479932896e-06,
      "loss": 1.9817,
      "step": 20451
    },
    {
      "epoch": 7.91792489353465,
      "grad_norm": 220.6875762939453,
      "learning_rate": 2.3134167849615006e-06,
      "loss": 0.6036,
      "step": 20452
    },
    {
      "epoch": 7.91831204026326,
      "grad_norm": 7.092053413391113,
      "learning_rate": 2.3129866219297116e-06,
      "loss": 0.1473,
      "step": 20453
    },
    {
      "epoch": 7.91869918699187,
      "grad_norm": 24.134695053100586,
      "learning_rate": 2.3125564588979226e-06,
      "loss": 0.4137,
      "step": 20454
    },
    {
      "epoch": 7.91908633372048,
      "grad_norm": 96.32361602783203,
      "learning_rate": 2.312126295866133e-06,
      "loss": 0.5249,
      "step": 20455
    },
    {
      "epoch": 7.91947348044909,
      "grad_norm": 245.35848999023438,
      "learning_rate": 2.311696132834344e-06,
      "loss": 1.1451,
      "step": 20456
    },
    {
      "epoch": 7.9198606271777,
      "grad_norm": 104.27784729003906,
      "learning_rate": 2.311265969802555e-06,
      "loss": 0.4475,
      "step": 20457
    },
    {
      "epoch": 7.92024777390631,
      "grad_norm": 191.5358428955078,
      "learning_rate": 2.310835806770766e-06,
      "loss": 1.6257,
      "step": 20458
    },
    {
      "epoch": 7.920634920634921,
      "grad_norm": 31.973438262939453,
      "learning_rate": 2.310405643738977e-06,
      "loss": 1.4379,
      "step": 20459
    },
    {
      "epoch": 7.9210220673635305,
      "grad_norm": 0.5122244358062744,
      "learning_rate": 2.309975480707188e-06,
      "loss": 0.0165,
      "step": 20460
    },
    {
      "epoch": 7.921409214092141,
      "grad_norm": 1.2655596733093262,
      "learning_rate": 2.309545317675399e-06,
      "loss": 0.0324,
      "step": 20461
    },
    {
      "epoch": 7.921796360820752,
      "grad_norm": 24.213855743408203,
      "learning_rate": 2.30911515464361e-06,
      "loss": 3.3851,
      "step": 20462
    },
    {
      "epoch": 7.922183507549361,
      "grad_norm": 40.28523635864258,
      "learning_rate": 2.308684991611821e-06,
      "loss": 0.2486,
      "step": 20463
    },
    {
      "epoch": 7.922570654277972,
      "grad_norm": 38.67137908935547,
      "learning_rate": 2.308254828580032e-06,
      "loss": 2.6065,
      "step": 20464
    },
    {
      "epoch": 7.922957801006581,
      "grad_norm": 1.6463552713394165,
      "learning_rate": 2.307824665548243e-06,
      "loss": 0.0462,
      "step": 20465
    },
    {
      "epoch": 7.923344947735192,
      "grad_norm": 44.00708770751953,
      "learning_rate": 2.307394502516454e-06,
      "loss": 0.1893,
      "step": 20466
    },
    {
      "epoch": 7.9237320944638014,
      "grad_norm": 9.268776893615723,
      "learning_rate": 2.306964339484665e-06,
      "loss": 0.0808,
      "step": 20467
    },
    {
      "epoch": 7.924119241192412,
      "grad_norm": 233.41744995117188,
      "learning_rate": 2.306534176452876e-06,
      "loss": 1.6376,
      "step": 20468
    },
    {
      "epoch": 7.9245063879210225,
      "grad_norm": 203.84161376953125,
      "learning_rate": 2.306104013421087e-06,
      "loss": 0.7629,
      "step": 20469
    },
    {
      "epoch": 7.924893534649632,
      "grad_norm": 183.7722625732422,
      "learning_rate": 2.3056738503892976e-06,
      "loss": 1.4651,
      "step": 20470
    },
    {
      "epoch": 7.925280681378243,
      "grad_norm": 45.797508239746094,
      "learning_rate": 2.3052436873575086e-06,
      "loss": 0.2091,
      "step": 20471
    },
    {
      "epoch": 7.925667828106852,
      "grad_norm": 2.2018351554870605,
      "learning_rate": 2.3048135243257196e-06,
      "loss": 0.1058,
      "step": 20472
    },
    {
      "epoch": 7.926054974835463,
      "grad_norm": 49.202476501464844,
      "learning_rate": 2.3043833612939306e-06,
      "loss": 1.7882,
      "step": 20473
    },
    {
      "epoch": 7.926442121564072,
      "grad_norm": 10.65480899810791,
      "learning_rate": 2.3039531982621416e-06,
      "loss": 0.12,
      "step": 20474
    },
    {
      "epoch": 7.926829268292683,
      "grad_norm": 58.14400863647461,
      "learning_rate": 2.3035230352303526e-06,
      "loss": 0.3812,
      "step": 20475
    },
    {
      "epoch": 7.927216415021293,
      "grad_norm": 1.8874657154083252,
      "learning_rate": 2.3030928721985636e-06,
      "loss": 0.0695,
      "step": 20476
    },
    {
      "epoch": 7.927603561749903,
      "grad_norm": 3.0259435176849365,
      "learning_rate": 2.3026627091667746e-06,
      "loss": 0.108,
      "step": 20477
    },
    {
      "epoch": 7.9279907084785135,
      "grad_norm": 74.24259185791016,
      "learning_rate": 2.3022325461349856e-06,
      "loss": 0.3226,
      "step": 20478
    },
    {
      "epoch": 7.928377855207123,
      "grad_norm": 168.54330444335938,
      "learning_rate": 2.301802383103196e-06,
      "loss": 2.8106,
      "step": 20479
    },
    {
      "epoch": 7.928765001935734,
      "grad_norm": 36.5386962890625,
      "learning_rate": 2.301372220071407e-06,
      "loss": 2.8309,
      "step": 20480
    },
    {
      "epoch": 7.929152148664344,
      "grad_norm": 47.5904426574707,
      "learning_rate": 2.300942057039618e-06,
      "loss": 0.2487,
      "step": 20481
    },
    {
      "epoch": 7.929539295392954,
      "grad_norm": 3.23309326171875,
      "learning_rate": 2.300511894007829e-06,
      "loss": 0.0511,
      "step": 20482
    },
    {
      "epoch": 7.929926442121564,
      "grad_norm": 79.41094207763672,
      "learning_rate": 2.30008173097604e-06,
      "loss": 0.8145,
      "step": 20483
    },
    {
      "epoch": 7.930313588850174,
      "grad_norm": 201.42201232910156,
      "learning_rate": 2.299651567944251e-06,
      "loss": 2.5713,
      "step": 20484
    },
    {
      "epoch": 7.930700735578784,
      "grad_norm": 46.899574279785156,
      "learning_rate": 2.299221404912462e-06,
      "loss": 2.3669,
      "step": 20485
    },
    {
      "epoch": 7.931087882307395,
      "grad_norm": 72.8740005493164,
      "learning_rate": 2.298791241880673e-06,
      "loss": 2.0276,
      "step": 20486
    },
    {
      "epoch": 7.9314750290360045,
      "grad_norm": 57.33574676513672,
      "learning_rate": 2.298361078848884e-06,
      "loss": 0.3097,
      "step": 20487
    },
    {
      "epoch": 7.931862175764615,
      "grad_norm": 62.26060104370117,
      "learning_rate": 2.2979309158170946e-06,
      "loss": 0.3306,
      "step": 20488
    },
    {
      "epoch": 7.932249322493225,
      "grad_norm": 132.59573364257812,
      "learning_rate": 2.2975007527853056e-06,
      "loss": 2.2046,
      "step": 20489
    },
    {
      "epoch": 7.932636469221835,
      "grad_norm": 3.4972264766693115,
      "learning_rate": 2.2970705897535166e-06,
      "loss": 0.0704,
      "step": 20490
    },
    {
      "epoch": 7.933023615950445,
      "grad_norm": 180.1188507080078,
      "learning_rate": 2.2966404267217276e-06,
      "loss": 0.84,
      "step": 20491
    },
    {
      "epoch": 7.933410762679055,
      "grad_norm": 43.20759582519531,
      "learning_rate": 2.2962102636899386e-06,
      "loss": 0.2503,
      "step": 20492
    },
    {
      "epoch": 7.933797909407666,
      "grad_norm": 12.030251502990723,
      "learning_rate": 2.2957801006581496e-06,
      "loss": 0.0598,
      "step": 20493
    },
    {
      "epoch": 7.9341850561362754,
      "grad_norm": 143.20108032226562,
      "learning_rate": 2.2953499376263606e-06,
      "loss": 2.9528,
      "step": 20494
    },
    {
      "epoch": 7.934572202864886,
      "grad_norm": 1.2281309366226196,
      "learning_rate": 2.2949197745945716e-06,
      "loss": 0.0199,
      "step": 20495
    },
    {
      "epoch": 7.934959349593496,
      "grad_norm": 11.811487197875977,
      "learning_rate": 2.2944896115627826e-06,
      "loss": 0.0864,
      "step": 20496
    },
    {
      "epoch": 7.935346496322106,
      "grad_norm": 0.9478408098220825,
      "learning_rate": 2.294059448530993e-06,
      "loss": 0.0237,
      "step": 20497
    },
    {
      "epoch": 7.935733643050717,
      "grad_norm": 1.4114993810653687,
      "learning_rate": 2.293629285499204e-06,
      "loss": 0.0524,
      "step": 20498
    },
    {
      "epoch": 7.936120789779326,
      "grad_norm": 35.692222595214844,
      "learning_rate": 2.293199122467415e-06,
      "loss": 1.8995,
      "step": 20499
    },
    {
      "epoch": 7.936507936507937,
      "grad_norm": 98.81755828857422,
      "learning_rate": 2.292768959435626e-06,
      "loss": 3.9132,
      "step": 20500
    },
    {
      "epoch": 7.936895083236546,
      "grad_norm": 9.95755386352539,
      "learning_rate": 2.292338796403837e-06,
      "loss": 0.0512,
      "step": 20501
    },
    {
      "epoch": 7.937282229965157,
      "grad_norm": 44.38426208496094,
      "learning_rate": 2.2919086333720485e-06,
      "loss": 3.2446,
      "step": 20502
    },
    {
      "epoch": 7.937669376693767,
      "grad_norm": 127.4342269897461,
      "learning_rate": 2.291478470340259e-06,
      "loss": 2.7184,
      "step": 20503
    },
    {
      "epoch": 7.938056523422377,
      "grad_norm": 74.10635375976562,
      "learning_rate": 2.29104830730847e-06,
      "loss": 0.8534,
      "step": 20504
    },
    {
      "epoch": 7.9384436701509875,
      "grad_norm": 158.5083770751953,
      "learning_rate": 2.290618144276681e-06,
      "loss": 2.9616,
      "step": 20505
    },
    {
      "epoch": 7.938830816879597,
      "grad_norm": 72.77493286132812,
      "learning_rate": 2.290187981244892e-06,
      "loss": 0.2696,
      "step": 20506
    },
    {
      "epoch": 7.939217963608208,
      "grad_norm": 49.869361877441406,
      "learning_rate": 2.289757818213103e-06,
      "loss": 0.4543,
      "step": 20507
    },
    {
      "epoch": 7.939605110336817,
      "grad_norm": 0.8397935628890991,
      "learning_rate": 2.289327655181314e-06,
      "loss": 0.0341,
      "step": 20508
    },
    {
      "epoch": 7.939992257065428,
      "grad_norm": 59.51029586791992,
      "learning_rate": 2.288897492149525e-06,
      "loss": 2.3926,
      "step": 20509
    },
    {
      "epoch": 7.940379403794038,
      "grad_norm": 1.5895150899887085,
      "learning_rate": 2.288467329117736e-06,
      "loss": 0.0502,
      "step": 20510
    },
    {
      "epoch": 7.940766550522648,
      "grad_norm": 79.2037582397461,
      "learning_rate": 2.288037166085947e-06,
      "loss": 1.1322,
      "step": 20511
    },
    {
      "epoch": 7.941153697251258,
      "grad_norm": 2.3126957416534424,
      "learning_rate": 2.2876070030541576e-06,
      "loss": 0.0529,
      "step": 20512
    },
    {
      "epoch": 7.941540843979868,
      "grad_norm": 82.79528045654297,
      "learning_rate": 2.2871768400223686e-06,
      "loss": 2.7754,
      "step": 20513
    },
    {
      "epoch": 7.9419279907084785,
      "grad_norm": 108.56605529785156,
      "learning_rate": 2.2867466769905796e-06,
      "loss": 2.4962,
      "step": 20514
    },
    {
      "epoch": 7.942315137437088,
      "grad_norm": 31.615842819213867,
      "learning_rate": 2.2863165139587906e-06,
      "loss": 2.1871,
      "step": 20515
    },
    {
      "epoch": 7.942702284165699,
      "grad_norm": 126.70529174804688,
      "learning_rate": 2.2858863509270016e-06,
      "loss": 2.1489,
      "step": 20516
    },
    {
      "epoch": 7.943089430894309,
      "grad_norm": 111.54956817626953,
      "learning_rate": 2.2854561878952126e-06,
      "loss": 4.6865,
      "step": 20517
    },
    {
      "epoch": 7.943476577622919,
      "grad_norm": 11.472550392150879,
      "learning_rate": 2.2850260248634236e-06,
      "loss": 0.0941,
      "step": 20518
    },
    {
      "epoch": 7.943863724351529,
      "grad_norm": 47.727882385253906,
      "learning_rate": 2.2845958618316346e-06,
      "loss": 2.1536,
      "step": 20519
    },
    {
      "epoch": 7.94425087108014,
      "grad_norm": 25.414934158325195,
      "learning_rate": 2.2841656987998455e-06,
      "loss": 0.3294,
      "step": 20520
    },
    {
      "epoch": 7.9446380178087495,
      "grad_norm": 10.242301940917969,
      "learning_rate": 2.283735535768056e-06,
      "loss": 0.2711,
      "step": 20521
    },
    {
      "epoch": 7.94502516453736,
      "grad_norm": 110.46636199951172,
      "learning_rate": 2.283305372736267e-06,
      "loss": 1.988,
      "step": 20522
    },
    {
      "epoch": 7.94541231126597,
      "grad_norm": 1.4135582447052002,
      "learning_rate": 2.282875209704478e-06,
      "loss": 0.0422,
      "step": 20523
    },
    {
      "epoch": 7.94579945799458,
      "grad_norm": 93.50033569335938,
      "learning_rate": 2.282445046672689e-06,
      "loss": 1.4797,
      "step": 20524
    },
    {
      "epoch": 7.94618660472319,
      "grad_norm": 3.4946606159210205,
      "learning_rate": 2.2820148836409e-06,
      "loss": 0.0437,
      "step": 20525
    },
    {
      "epoch": 7.9465737514518,
      "grad_norm": 225.27911376953125,
      "learning_rate": 2.281584720609111e-06,
      "loss": 0.5469,
      "step": 20526
    },
    {
      "epoch": 7.946960898180411,
      "grad_norm": 27.154268264770508,
      "learning_rate": 2.281154557577322e-06,
      "loss": 1.9257,
      "step": 20527
    },
    {
      "epoch": 7.94734804490902,
      "grad_norm": 68.1109848022461,
      "learning_rate": 2.280724394545533e-06,
      "loss": 3.8351,
      "step": 20528
    },
    {
      "epoch": 7.947735191637631,
      "grad_norm": 2.75742506980896,
      "learning_rate": 2.280294231513744e-06,
      "loss": 0.0659,
      "step": 20529
    },
    {
      "epoch": 7.9481223383662405,
      "grad_norm": 99.37601470947266,
      "learning_rate": 2.2798640684819546e-06,
      "loss": 0.8495,
      "step": 20530
    },
    {
      "epoch": 7.948509485094851,
      "grad_norm": 170.07803344726562,
      "learning_rate": 2.2794339054501656e-06,
      "loss": 0.8553,
      "step": 20531
    },
    {
      "epoch": 7.948896631823461,
      "grad_norm": 17.15423011779785,
      "learning_rate": 2.2790037424183766e-06,
      "loss": 0.2436,
      "step": 20532
    },
    {
      "epoch": 7.949283778552071,
      "grad_norm": 22.233957290649414,
      "learning_rate": 2.2785735793865876e-06,
      "loss": 4.1402,
      "step": 20533
    },
    {
      "epoch": 7.949670925280682,
      "grad_norm": 86.49971771240234,
      "learning_rate": 2.2781434163547986e-06,
      "loss": 2.4518,
      "step": 20534
    },
    {
      "epoch": 7.950058072009291,
      "grad_norm": 38.94560241699219,
      "learning_rate": 2.2777132533230096e-06,
      "loss": 1.588,
      "step": 20535
    },
    {
      "epoch": 7.950445218737902,
      "grad_norm": 2.0444650650024414,
      "learning_rate": 2.2772830902912206e-06,
      "loss": 0.0442,
      "step": 20536
    },
    {
      "epoch": 7.950832365466512,
      "grad_norm": 25.51892852783203,
      "learning_rate": 2.2768529272594316e-06,
      "loss": 0.101,
      "step": 20537
    },
    {
      "epoch": 7.951219512195122,
      "grad_norm": 59.4928092956543,
      "learning_rate": 2.2764227642276426e-06,
      "loss": 0.4552,
      "step": 20538
    },
    {
      "epoch": 7.951606658923732,
      "grad_norm": 5.920461654663086,
      "learning_rate": 2.275992601195853e-06,
      "loss": 0.0297,
      "step": 20539
    },
    {
      "epoch": 7.951993805652342,
      "grad_norm": 71.802490234375,
      "learning_rate": 2.275562438164064e-06,
      "loss": 1.0931,
      "step": 20540
    },
    {
      "epoch": 7.9523809523809526,
      "grad_norm": 77.84963989257812,
      "learning_rate": 2.275132275132275e-06,
      "loss": 1.0205,
      "step": 20541
    },
    {
      "epoch": 7.952768099109562,
      "grad_norm": 0.9681001901626587,
      "learning_rate": 2.274702112100486e-06,
      "loss": 0.0191,
      "step": 20542
    },
    {
      "epoch": 7.953155245838173,
      "grad_norm": 3.0682387351989746,
      "learning_rate": 2.2742719490686975e-06,
      "loss": 0.0561,
      "step": 20543
    },
    {
      "epoch": 7.953542392566783,
      "grad_norm": 111.42892456054688,
      "learning_rate": 2.2738417860369085e-06,
      "loss": 4.1422,
      "step": 20544
    },
    {
      "epoch": 7.953929539295393,
      "grad_norm": 62.35289001464844,
      "learning_rate": 2.273411623005119e-06,
      "loss": 1.7835,
      "step": 20545
    },
    {
      "epoch": 7.954316686024003,
      "grad_norm": 2.1663200855255127,
      "learning_rate": 2.27298145997333e-06,
      "loss": 0.0335,
      "step": 20546
    },
    {
      "epoch": 7.954703832752613,
      "grad_norm": 42.40779113769531,
      "learning_rate": 2.272551296941541e-06,
      "loss": 1.3062,
      "step": 20547
    },
    {
      "epoch": 7.9550909794812235,
      "grad_norm": 25.4873104095459,
      "learning_rate": 2.272121133909752e-06,
      "loss": 2.2019,
      "step": 20548
    },
    {
      "epoch": 7.955478126209833,
      "grad_norm": 180.05990600585938,
      "learning_rate": 2.271690970877963e-06,
      "loss": 1.8235,
      "step": 20549
    },
    {
      "epoch": 7.955865272938444,
      "grad_norm": 27.10693359375,
      "learning_rate": 2.271260807846174e-06,
      "loss": 2.3035,
      "step": 20550
    },
    {
      "epoch": 7.956252419667054,
      "grad_norm": 2.0614428520202637,
      "learning_rate": 2.270830644814385e-06,
      "loss": 0.0102,
      "step": 20551
    },
    {
      "epoch": 7.956639566395664,
      "grad_norm": 363.8272705078125,
      "learning_rate": 2.270400481782596e-06,
      "loss": 1.1036,
      "step": 20552
    },
    {
      "epoch": 7.957026713124274,
      "grad_norm": 81.9173812866211,
      "learning_rate": 2.269970318750807e-06,
      "loss": 2.4868,
      "step": 20553
    },
    {
      "epoch": 7.957413859852885,
      "grad_norm": 64.1603012084961,
      "learning_rate": 2.2695401557190176e-06,
      "loss": 2.0187,
      "step": 20554
    },
    {
      "epoch": 7.957801006581494,
      "grad_norm": 121.9545669555664,
      "learning_rate": 2.2691099926872286e-06,
      "loss": 1.1256,
      "step": 20555
    },
    {
      "epoch": 7.958188153310105,
      "grad_norm": 3.9861834049224854,
      "learning_rate": 2.2686798296554396e-06,
      "loss": 0.1231,
      "step": 20556
    },
    {
      "epoch": 7.9585753000387145,
      "grad_norm": 3.349881887435913,
      "learning_rate": 2.2682496666236506e-06,
      "loss": 0.0336,
      "step": 20557
    },
    {
      "epoch": 7.958962446767325,
      "grad_norm": 129.67796325683594,
      "learning_rate": 2.2678195035918616e-06,
      "loss": 2.2151,
      "step": 20558
    },
    {
      "epoch": 7.959349593495935,
      "grad_norm": 141.6630096435547,
      "learning_rate": 2.2673893405600725e-06,
      "loss": 0.4026,
      "step": 20559
    },
    {
      "epoch": 7.959736740224545,
      "grad_norm": 325.1244201660156,
      "learning_rate": 2.2669591775282835e-06,
      "loss": 1.4836,
      "step": 20560
    },
    {
      "epoch": 7.960123886953156,
      "grad_norm": 338.80157470703125,
      "learning_rate": 2.2665290144964945e-06,
      "loss": 1.1525,
      "step": 20561
    },
    {
      "epoch": 7.960511033681765,
      "grad_norm": 6.869223594665527,
      "learning_rate": 2.2660988514647055e-06,
      "loss": 0.1079,
      "step": 20562
    },
    {
      "epoch": 7.960898180410376,
      "grad_norm": 40.21537399291992,
      "learning_rate": 2.265668688432916e-06,
      "loss": 2.0614,
      "step": 20563
    },
    {
      "epoch": 7.961285327138985,
      "grad_norm": 157.16456604003906,
      "learning_rate": 2.265238525401127e-06,
      "loss": 1.4375,
      "step": 20564
    },
    {
      "epoch": 7.961672473867596,
      "grad_norm": 2.8372573852539062,
      "learning_rate": 2.264808362369338e-06,
      "loss": 0.0992,
      "step": 20565
    },
    {
      "epoch": 7.9620596205962055,
      "grad_norm": 3.259101152420044,
      "learning_rate": 2.264378199337549e-06,
      "loss": 0.0982,
      "step": 20566
    },
    {
      "epoch": 7.962446767324816,
      "grad_norm": 182.60511779785156,
      "learning_rate": 2.26394803630576e-06,
      "loss": 2.0628,
      "step": 20567
    },
    {
      "epoch": 7.9628339140534266,
      "grad_norm": 1.5117778778076172,
      "learning_rate": 2.263517873273971e-06,
      "loss": 0.0364,
      "step": 20568
    },
    {
      "epoch": 7.963221060782036,
      "grad_norm": 30.384021759033203,
      "learning_rate": 2.263087710242182e-06,
      "loss": 1.696,
      "step": 20569
    },
    {
      "epoch": 7.963608207510647,
      "grad_norm": 6.598178863525391,
      "learning_rate": 2.262657547210393e-06,
      "loss": 0.1022,
      "step": 20570
    },
    {
      "epoch": 7.963995354239256,
      "grad_norm": 22.03508186340332,
      "learning_rate": 2.262227384178604e-06,
      "loss": 2.411,
      "step": 20571
    },
    {
      "epoch": 7.964382500967867,
      "grad_norm": 0.6616432666778564,
      "learning_rate": 2.2617972211468146e-06,
      "loss": 0.0074,
      "step": 20572
    },
    {
      "epoch": 7.964769647696477,
      "grad_norm": 36.1337776184082,
      "learning_rate": 2.2613670581150256e-06,
      "loss": 0.569,
      "step": 20573
    },
    {
      "epoch": 7.965156794425087,
      "grad_norm": 196.57061767578125,
      "learning_rate": 2.2609368950832366e-06,
      "loss": 2.6708,
      "step": 20574
    },
    {
      "epoch": 7.9655439411536975,
      "grad_norm": 33.70918273925781,
      "learning_rate": 2.2605067320514476e-06,
      "loss": 1.6145,
      "step": 20575
    },
    {
      "epoch": 7.965931087882307,
      "grad_norm": 133.77877807617188,
      "learning_rate": 2.2600765690196586e-06,
      "loss": 0.7791,
      "step": 20576
    },
    {
      "epoch": 7.966318234610918,
      "grad_norm": 0.8507738709449768,
      "learning_rate": 2.2596464059878696e-06,
      "loss": 0.0322,
      "step": 20577
    },
    {
      "epoch": 7.966705381339528,
      "grad_norm": 150.42808532714844,
      "learning_rate": 2.2592162429560806e-06,
      "loss": 0.8814,
      "step": 20578
    },
    {
      "epoch": 7.967092528068138,
      "grad_norm": 41.28816223144531,
      "learning_rate": 2.2587860799242915e-06,
      "loss": 1.0706,
      "step": 20579
    },
    {
      "epoch": 7.967479674796748,
      "grad_norm": 4.9679975509643555,
      "learning_rate": 2.2583559168925025e-06,
      "loss": 0.0677,
      "step": 20580
    },
    {
      "epoch": 7.967866821525358,
      "grad_norm": 32.47267532348633,
      "learning_rate": 2.257925753860713e-06,
      "loss": 1.508,
      "step": 20581
    },
    {
      "epoch": 7.968253968253968,
      "grad_norm": 0.8510984778404236,
      "learning_rate": 2.257495590828924e-06,
      "loss": 0.028,
      "step": 20582
    },
    {
      "epoch": 7.968641114982578,
      "grad_norm": 116.98107147216797,
      "learning_rate": 2.257065427797135e-06,
      "loss": 6.1172,
      "step": 20583
    },
    {
      "epoch": 7.9690282617111885,
      "grad_norm": 140.0495147705078,
      "learning_rate": 2.2566352647653465e-06,
      "loss": 1.9366,
      "step": 20584
    },
    {
      "epoch": 7.969415408439799,
      "grad_norm": 132.04354858398438,
      "learning_rate": 2.2562051017335575e-06,
      "loss": 3.2623,
      "step": 20585
    },
    {
      "epoch": 7.969802555168409,
      "grad_norm": 24.27680206298828,
      "learning_rate": 2.255774938701768e-06,
      "loss": 1.8009,
      "step": 20586
    },
    {
      "epoch": 7.970189701897019,
      "grad_norm": 4.745234489440918,
      "learning_rate": 2.255344775669979e-06,
      "loss": 0.0537,
      "step": 20587
    },
    {
      "epoch": 7.970576848625629,
      "grad_norm": 1.988525152206421,
      "learning_rate": 2.25491461263819e-06,
      "loss": 0.0382,
      "step": 20588
    },
    {
      "epoch": 7.970963995354239,
      "grad_norm": 227.09231567382812,
      "learning_rate": 2.254484449606401e-06,
      "loss": 0.8483,
      "step": 20589
    },
    {
      "epoch": 7.97135114208285,
      "grad_norm": 109.56648254394531,
      "learning_rate": 2.254054286574612e-06,
      "loss": 1.1323,
      "step": 20590
    },
    {
      "epoch": 7.971738288811459,
      "grad_norm": 27.022499084472656,
      "learning_rate": 2.253624123542823e-06,
      "loss": 2.5339,
      "step": 20591
    },
    {
      "epoch": 7.97212543554007,
      "grad_norm": 37.63838195800781,
      "learning_rate": 2.253193960511034e-06,
      "loss": 0.0804,
      "step": 20592
    },
    {
      "epoch": 7.9725125822686795,
      "grad_norm": 89.35458374023438,
      "learning_rate": 2.252763797479245e-06,
      "loss": 1.4256,
      "step": 20593
    },
    {
      "epoch": 7.97289972899729,
      "grad_norm": 1.8613539934158325,
      "learning_rate": 2.252333634447456e-06,
      "loss": 0.0709,
      "step": 20594
    },
    {
      "epoch": 7.973286875725901,
      "grad_norm": 146.00726318359375,
      "learning_rate": 2.2519034714156666e-06,
      "loss": 1.5827,
      "step": 20595
    },
    {
      "epoch": 7.97367402245451,
      "grad_norm": 236.54937744140625,
      "learning_rate": 2.2514733083838776e-06,
      "loss": 1.3873,
      "step": 20596
    },
    {
      "epoch": 7.974061169183121,
      "grad_norm": 165.54440307617188,
      "learning_rate": 2.2510431453520886e-06,
      "loss": 2.3705,
      "step": 20597
    },
    {
      "epoch": 7.97444831591173,
      "grad_norm": 32.46095275878906,
      "learning_rate": 2.2506129823202996e-06,
      "loss": 1.534,
      "step": 20598
    },
    {
      "epoch": 7.974835462640341,
      "grad_norm": 35.43305969238281,
      "learning_rate": 2.2501828192885105e-06,
      "loss": 2.0445,
      "step": 20599
    },
    {
      "epoch": 7.97522260936895,
      "grad_norm": 114.07562255859375,
      "learning_rate": 2.2497526562567215e-06,
      "loss": 1.0055,
      "step": 20600
    },
    {
      "epoch": 7.975609756097561,
      "grad_norm": 56.99463653564453,
      "learning_rate": 2.2493224932249325e-06,
      "loss": 1.584,
      "step": 20601
    },
    {
      "epoch": 7.9759969028261715,
      "grad_norm": 2.2539312839508057,
      "learning_rate": 2.2488923301931435e-06,
      "loss": 0.1079,
      "step": 20602
    },
    {
      "epoch": 7.976384049554781,
      "grad_norm": 132.79615783691406,
      "learning_rate": 2.2484621671613545e-06,
      "loss": 1.7418,
      "step": 20603
    },
    {
      "epoch": 7.976771196283392,
      "grad_norm": 4.333353042602539,
      "learning_rate": 2.248032004129565e-06,
      "loss": 0.0452,
      "step": 20604
    },
    {
      "epoch": 7.977158343012001,
      "grad_norm": 83.86695098876953,
      "learning_rate": 2.247601841097776e-06,
      "loss": 0.9038,
      "step": 20605
    },
    {
      "epoch": 7.977545489740612,
      "grad_norm": 76.17601013183594,
      "learning_rate": 2.247171678065987e-06,
      "loss": 1.0682,
      "step": 20606
    },
    {
      "epoch": 7.977932636469221,
      "grad_norm": 2.4583017826080322,
      "learning_rate": 2.246741515034198e-06,
      "loss": 0.0682,
      "step": 20607
    },
    {
      "epoch": 7.978319783197832,
      "grad_norm": 6.925343036651611,
      "learning_rate": 2.246311352002409e-06,
      "loss": 0.1353,
      "step": 20608
    },
    {
      "epoch": 7.978706929926442,
      "grad_norm": 116.00419616699219,
      "learning_rate": 2.24588118897062e-06,
      "loss": 1.0839,
      "step": 20609
    },
    {
      "epoch": 7.979094076655052,
      "grad_norm": 28.480283737182617,
      "learning_rate": 2.245451025938831e-06,
      "loss": 0.1835,
      "step": 20610
    },
    {
      "epoch": 7.9794812233836625,
      "grad_norm": 74.27599334716797,
      "learning_rate": 2.245020862907042e-06,
      "loss": 0.3256,
      "step": 20611
    },
    {
      "epoch": 7.979868370112273,
      "grad_norm": 117.56523895263672,
      "learning_rate": 2.244590699875253e-06,
      "loss": 0.4817,
      "step": 20612
    },
    {
      "epoch": 7.980255516840883,
      "grad_norm": 6.403030872344971,
      "learning_rate": 2.2441605368434636e-06,
      "loss": 0.1576,
      "step": 20613
    },
    {
      "epoch": 7.980642663569493,
      "grad_norm": 84.81417846679688,
      "learning_rate": 2.2437303738116746e-06,
      "loss": 2.8652,
      "step": 20614
    },
    {
      "epoch": 7.981029810298103,
      "grad_norm": 82.54972839355469,
      "learning_rate": 2.2433002107798856e-06,
      "loss": 1.0366,
      "step": 20615
    },
    {
      "epoch": 7.981416957026713,
      "grad_norm": 110.56944274902344,
      "learning_rate": 2.2428700477480966e-06,
      "loss": 1.3679,
      "step": 20616
    },
    {
      "epoch": 7.981804103755323,
      "grad_norm": 2.2800920009613037,
      "learning_rate": 2.2424398847163076e-06,
      "loss": 0.108,
      "step": 20617
    },
    {
      "epoch": 7.982191250483933,
      "grad_norm": 2.327888250350952,
      "learning_rate": 2.2420097216845185e-06,
      "loss": 0.0496,
      "step": 20618
    },
    {
      "epoch": 7.982578397212544,
      "grad_norm": 1.1187806129455566,
      "learning_rate": 2.2415795586527295e-06,
      "loss": 0.0442,
      "step": 20619
    },
    {
      "epoch": 7.9829655439411535,
      "grad_norm": 4.873229503631592,
      "learning_rate": 2.2411493956209405e-06,
      "loss": 0.1304,
      "step": 20620
    },
    {
      "epoch": 7.983352690669764,
      "grad_norm": 15.941813468933105,
      "learning_rate": 2.2407192325891515e-06,
      "loss": 0.1072,
      "step": 20621
    },
    {
      "epoch": 7.983739837398374,
      "grad_norm": 11.041500091552734,
      "learning_rate": 2.240289069557362e-06,
      "loss": 0.166,
      "step": 20622
    },
    {
      "epoch": 7.984126984126984,
      "grad_norm": 231.64309692382812,
      "learning_rate": 2.239858906525573e-06,
      "loss": 0.4666,
      "step": 20623
    },
    {
      "epoch": 7.984514130855594,
      "grad_norm": 108.3862075805664,
      "learning_rate": 2.239428743493784e-06,
      "loss": 1.1033,
      "step": 20624
    },
    {
      "epoch": 7.984901277584204,
      "grad_norm": 275.4825439453125,
      "learning_rate": 2.238998580461995e-06,
      "loss": 0.4691,
      "step": 20625
    },
    {
      "epoch": 7.985288424312815,
      "grad_norm": 125.56381225585938,
      "learning_rate": 2.2385684174302065e-06,
      "loss": 1.2079,
      "step": 20626
    },
    {
      "epoch": 7.9856755710414244,
      "grad_norm": 118.99275207519531,
      "learning_rate": 2.2381382543984175e-06,
      "loss": 2.4277,
      "step": 20627
    },
    {
      "epoch": 7.986062717770035,
      "grad_norm": 24.255638122558594,
      "learning_rate": 2.237708091366628e-06,
      "loss": 2.1813,
      "step": 20628
    },
    {
      "epoch": 7.9864498644986455,
      "grad_norm": 52.42988586425781,
      "learning_rate": 2.237277928334839e-06,
      "loss": 1.1411,
      "step": 20629
    },
    {
      "epoch": 7.986837011227255,
      "grad_norm": 1.584043264389038,
      "learning_rate": 2.23684776530305e-06,
      "loss": 0.0502,
      "step": 20630
    },
    {
      "epoch": 7.987224157955866,
      "grad_norm": 143.4170684814453,
      "learning_rate": 2.236417602271261e-06,
      "loss": 1.8691,
      "step": 20631
    },
    {
      "epoch": 7.987611304684475,
      "grad_norm": 29.036022186279297,
      "learning_rate": 2.235987439239472e-06,
      "loss": 2.6956,
      "step": 20632
    },
    {
      "epoch": 7.987998451413086,
      "grad_norm": 1.3892650604248047,
      "learning_rate": 2.235557276207683e-06,
      "loss": 0.0429,
      "step": 20633
    },
    {
      "epoch": 7.988385598141695,
      "grad_norm": 148.48243713378906,
      "learning_rate": 2.235127113175894e-06,
      "loss": 2.3443,
      "step": 20634
    },
    {
      "epoch": 7.988772744870306,
      "grad_norm": 1.7731385231018066,
      "learning_rate": 2.234696950144105e-06,
      "loss": 0.0241,
      "step": 20635
    },
    {
      "epoch": 7.989159891598916,
      "grad_norm": 24.892343521118164,
      "learning_rate": 2.234266787112316e-06,
      "loss": 2.9997,
      "step": 20636
    },
    {
      "epoch": 7.989547038327526,
      "grad_norm": 33.39873123168945,
      "learning_rate": 2.2338366240805266e-06,
      "loss": 0.5046,
      "step": 20637
    },
    {
      "epoch": 7.9899341850561365,
      "grad_norm": 0.5892858505249023,
      "learning_rate": 2.2334064610487375e-06,
      "loss": 0.0172,
      "step": 20638
    },
    {
      "epoch": 7.990321331784746,
      "grad_norm": 121.37000274658203,
      "learning_rate": 2.2329762980169485e-06,
      "loss": 0.8269,
      "step": 20639
    },
    {
      "epoch": 7.990708478513357,
      "grad_norm": 24.30033302307129,
      "learning_rate": 2.2325461349851595e-06,
      "loss": 2.1284,
      "step": 20640
    },
    {
      "epoch": 7.991095625241966,
      "grad_norm": 51.38697814941406,
      "learning_rate": 2.2321159719533705e-06,
      "loss": 0.8814,
      "step": 20641
    },
    {
      "epoch": 7.991482771970577,
      "grad_norm": 113.1433334350586,
      "learning_rate": 2.2316858089215815e-06,
      "loss": 0.9404,
      "step": 20642
    },
    {
      "epoch": 7.991869918699187,
      "grad_norm": 29.957626342773438,
      "learning_rate": 2.2312556458897925e-06,
      "loss": 3.1857,
      "step": 20643
    },
    {
      "epoch": 7.992257065427797,
      "grad_norm": 6.437543869018555,
      "learning_rate": 2.2308254828580035e-06,
      "loss": 0.1754,
      "step": 20644
    },
    {
      "epoch": 7.992644212156407,
      "grad_norm": 1.4830100536346436,
      "learning_rate": 2.2303953198262145e-06,
      "loss": 0.0299,
      "step": 20645
    },
    {
      "epoch": 7.993031358885017,
      "grad_norm": 5.352131366729736,
      "learning_rate": 2.229965156794425e-06,
      "loss": 0.1014,
      "step": 20646
    },
    {
      "epoch": 7.9934185056136275,
      "grad_norm": 240.1554412841797,
      "learning_rate": 2.229534993762636e-06,
      "loss": 6.7683,
      "step": 20647
    },
    {
      "epoch": 7.993805652342238,
      "grad_norm": 70.76085662841797,
      "learning_rate": 2.229104830730847e-06,
      "loss": 0.3007,
      "step": 20648
    },
    {
      "epoch": 7.994192799070848,
      "grad_norm": 92.73004913330078,
      "learning_rate": 2.228674667699058e-06,
      "loss": 2.6852,
      "step": 20649
    },
    {
      "epoch": 7.994579945799458,
      "grad_norm": 3.274658441543579,
      "learning_rate": 2.228244504667269e-06,
      "loss": 0.0966,
      "step": 20650
    },
    {
      "epoch": 7.994967092528068,
      "grad_norm": 342.7117614746094,
      "learning_rate": 2.22781434163548e-06,
      "loss": 1.8477,
      "step": 20651
    },
    {
      "epoch": 7.995354239256678,
      "grad_norm": 28.448511123657227,
      "learning_rate": 2.227384178603691e-06,
      "loss": 0.1113,
      "step": 20652
    },
    {
      "epoch": 7.995741385985289,
      "grad_norm": 3.971457004547119,
      "learning_rate": 2.226954015571902e-06,
      "loss": 0.0776,
      "step": 20653
    },
    {
      "epoch": 7.9961285327138985,
      "grad_norm": 0.7414324283599854,
      "learning_rate": 2.226523852540113e-06,
      "loss": 0.0276,
      "step": 20654
    },
    {
      "epoch": 7.996515679442509,
      "grad_norm": 2.2194178104400635,
      "learning_rate": 2.2260936895083236e-06,
      "loss": 0.0408,
      "step": 20655
    },
    {
      "epoch": 7.996902826171119,
      "grad_norm": 107.24264526367188,
      "learning_rate": 2.2256635264765346e-06,
      "loss": 2.4205,
      "step": 20656
    },
    {
      "epoch": 7.997289972899729,
      "grad_norm": 140.33924865722656,
      "learning_rate": 2.2252333634447456e-06,
      "loss": 2.1118,
      "step": 20657
    },
    {
      "epoch": 7.997677119628339,
      "grad_norm": 74.56083679199219,
      "learning_rate": 2.2248032004129565e-06,
      "loss": 0.3153,
      "step": 20658
    },
    {
      "epoch": 7.998064266356949,
      "grad_norm": 12.845120429992676,
      "learning_rate": 2.2243730373811675e-06,
      "loss": 0.0767,
      "step": 20659
    },
    {
      "epoch": 7.99845141308556,
      "grad_norm": 2.771881103515625,
      "learning_rate": 2.2239428743493785e-06,
      "loss": 0.0719,
      "step": 20660
    },
    {
      "epoch": 7.998838559814169,
      "grad_norm": 87.95288848876953,
      "learning_rate": 2.2235127113175895e-06,
      "loss": 2.4429,
      "step": 20661
    },
    {
      "epoch": 7.99922570654278,
      "grad_norm": 5.705123424530029,
      "learning_rate": 2.2230825482858005e-06,
      "loss": 0.2191,
      "step": 20662
    },
    {
      "epoch": 7.9996128532713895,
      "grad_norm": 11.049728393554688,
      "learning_rate": 2.2226523852540115e-06,
      "loss": 0.0505,
      "step": 20663
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.4039056301116943,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.0457,
      "step": 20664
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.590566037735849,
      "eval_f1": 0.5795620120939399,
      "eval_loss": 1.942228078842163,
      "eval_runtime": 384.7169,
      "eval_samples_per_second": 2.755,
      "eval_steps_per_second": 1.378,
      "step": 20664
    }
  ],
  "logging_steps": 1,
  "max_steps": 25830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.12540274846208e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
