{
  "best_metric": 0.3958868115711651,
  "best_model_checkpoint": "models\\CREMAD_hubert_cls\\checkpoint-7749",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7749,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00038714672861014324,
      "grad_norm": 1.8722529411315918,
      "learning_rate": 3.871467286101433e-09,
      "loss": 1.9665,
      "step": 1
    },
    {
      "epoch": 0.0007742934572202865,
      "grad_norm": 1.3827552795410156,
      "learning_rate": 7.742934572202866e-09,
      "loss": 1.9174,
      "step": 2
    },
    {
      "epoch": 0.0011614401858304297,
      "grad_norm": 1.2626818418502808,
      "learning_rate": 1.1614401858304298e-08,
      "loss": 1.9406,
      "step": 3
    },
    {
      "epoch": 0.001548586914440573,
      "grad_norm": 1.2638156414031982,
      "learning_rate": 1.5485869144405732e-08,
      "loss": 1.9347,
      "step": 4
    },
    {
      "epoch": 0.0019357336430507162,
      "grad_norm": 1.4256352186203003,
      "learning_rate": 1.9357336430507163e-08,
      "loss": 1.941,
      "step": 5
    },
    {
      "epoch": 0.0023228803716608595,
      "grad_norm": 1.1354379653930664,
      "learning_rate": 2.3228803716608597e-08,
      "loss": 1.9343,
      "step": 6
    },
    {
      "epoch": 0.0027100271002710027,
      "grad_norm": 1.5203808546066284,
      "learning_rate": 2.710027100271003e-08,
      "loss": 1.9437,
      "step": 7
    },
    {
      "epoch": 0.003097173828881146,
      "grad_norm": 1.2510936260223389,
      "learning_rate": 3.0971738288811464e-08,
      "loss": 1.9503,
      "step": 8
    },
    {
      "epoch": 0.003484320557491289,
      "grad_norm": 1.2681835889816284,
      "learning_rate": 3.484320557491289e-08,
      "loss": 1.9602,
      "step": 9
    },
    {
      "epoch": 0.0038714672861014324,
      "grad_norm": 1.2337955236434937,
      "learning_rate": 3.8714672861014325e-08,
      "loss": 1.9297,
      "step": 10
    },
    {
      "epoch": 0.004258614014711576,
      "grad_norm": 1.2799679040908813,
      "learning_rate": 4.258614014711576e-08,
      "loss": 1.9389,
      "step": 11
    },
    {
      "epoch": 0.004645760743321719,
      "grad_norm": 1.1558623313903809,
      "learning_rate": 4.645760743321719e-08,
      "loss": 1.9505,
      "step": 12
    },
    {
      "epoch": 0.005032907471931862,
      "grad_norm": 1.2249882221221924,
      "learning_rate": 5.032907471931863e-08,
      "loss": 1.9634,
      "step": 13
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 1.39509117603302,
      "learning_rate": 5.420054200542006e-08,
      "loss": 1.943,
      "step": 14
    },
    {
      "epoch": 0.005807200929152149,
      "grad_norm": 2.282097816467285,
      "learning_rate": 5.807200929152149e-08,
      "loss": 1.9743,
      "step": 15
    },
    {
      "epoch": 0.006194347657762292,
      "grad_norm": 1.5924534797668457,
      "learning_rate": 6.194347657762293e-08,
      "loss": 1.9034,
      "step": 16
    },
    {
      "epoch": 0.006581494386372435,
      "grad_norm": 1.3129477500915527,
      "learning_rate": 6.581494386372436e-08,
      "loss": 1.9399,
      "step": 17
    },
    {
      "epoch": 0.006968641114982578,
      "grad_norm": 1.948675274848938,
      "learning_rate": 6.968641114982578e-08,
      "loss": 1.921,
      "step": 18
    },
    {
      "epoch": 0.007355787843592722,
      "grad_norm": 1.161292552947998,
      "learning_rate": 7.355787843592722e-08,
      "loss": 1.9295,
      "step": 19
    },
    {
      "epoch": 0.007742934572202865,
      "grad_norm": 1.602184772491455,
      "learning_rate": 7.742934572202865e-08,
      "loss": 1.9164,
      "step": 20
    },
    {
      "epoch": 0.008130081300813009,
      "grad_norm": 1.4578857421875,
      "learning_rate": 8.130081300813009e-08,
      "loss": 1.9829,
      "step": 21
    },
    {
      "epoch": 0.008517228029423151,
      "grad_norm": 1.7535805702209473,
      "learning_rate": 8.517228029423152e-08,
      "loss": 2.0017,
      "step": 22
    },
    {
      "epoch": 0.008904374758033295,
      "grad_norm": 1.3390156030654907,
      "learning_rate": 8.904374758033296e-08,
      "loss": 1.9736,
      "step": 23
    },
    {
      "epoch": 0.009291521486643438,
      "grad_norm": 1.3785299062728882,
      "learning_rate": 9.291521486643439e-08,
      "loss": 1.9085,
      "step": 24
    },
    {
      "epoch": 0.009678668215253582,
      "grad_norm": 1.6784433126449585,
      "learning_rate": 9.678668215253583e-08,
      "loss": 1.9873,
      "step": 25
    },
    {
      "epoch": 0.010065814943863724,
      "grad_norm": 1.1994949579238892,
      "learning_rate": 1.0065814943863725e-07,
      "loss": 1.9368,
      "step": 26
    },
    {
      "epoch": 0.010452961672473868,
      "grad_norm": 2.8482437133789062,
      "learning_rate": 1.045296167247387e-07,
      "loss": 2.0242,
      "step": 27
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 1.3148632049560547,
      "learning_rate": 1.0840108401084012e-07,
      "loss": 1.9134,
      "step": 28
    },
    {
      "epoch": 0.011227255129694155,
      "grad_norm": 1.808053731918335,
      "learning_rate": 1.1227255129694156e-07,
      "loss": 1.9751,
      "step": 29
    },
    {
      "epoch": 0.011614401858304297,
      "grad_norm": 1.7089858055114746,
      "learning_rate": 1.1614401858304298e-07,
      "loss": 1.9722,
      "step": 30
    },
    {
      "epoch": 0.012001548586914441,
      "grad_norm": 1.437395453453064,
      "learning_rate": 1.2001548586914442e-07,
      "loss": 1.9501,
      "step": 31
    },
    {
      "epoch": 0.012388695315524584,
      "grad_norm": 1.1888842582702637,
      "learning_rate": 1.2388695315524586e-07,
      "loss": 1.9494,
      "step": 32
    },
    {
      "epoch": 0.012775842044134728,
      "grad_norm": 1.641129732131958,
      "learning_rate": 1.277584204413473e-07,
      "loss": 1.9476,
      "step": 33
    },
    {
      "epoch": 0.01316298877274487,
      "grad_norm": 1.4256542921066284,
      "learning_rate": 1.316298877274487e-07,
      "loss": 1.9882,
      "step": 34
    },
    {
      "epoch": 0.013550135501355014,
      "grad_norm": 1.4232455492019653,
      "learning_rate": 1.3550135501355015e-07,
      "loss": 1.9382,
      "step": 35
    },
    {
      "epoch": 0.013937282229965157,
      "grad_norm": 1.3072998523712158,
      "learning_rate": 1.3937282229965157e-07,
      "loss": 1.9046,
      "step": 36
    },
    {
      "epoch": 0.014324428958575301,
      "grad_norm": 1.658265471458435,
      "learning_rate": 1.4324428958575303e-07,
      "loss": 1.9523,
      "step": 37
    },
    {
      "epoch": 0.014711575687185443,
      "grad_norm": 2.552384853363037,
      "learning_rate": 1.4711575687185445e-07,
      "loss": 1.9937,
      "step": 38
    },
    {
      "epoch": 0.015098722415795587,
      "grad_norm": 1.3056846857070923,
      "learning_rate": 1.509872241579559e-07,
      "loss": 1.9564,
      "step": 39
    },
    {
      "epoch": 0.01548586914440573,
      "grad_norm": 1.5663657188415527,
      "learning_rate": 1.548586914440573e-07,
      "loss": 1.9578,
      "step": 40
    },
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 1.4655976295471191,
      "learning_rate": 1.5873015873015874e-07,
      "loss": 1.9609,
      "step": 41
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 1.363230586051941,
      "learning_rate": 1.6260162601626018e-07,
      "loss": 1.9456,
      "step": 42
    },
    {
      "epoch": 0.01664730933023616,
      "grad_norm": 1.4018268585205078,
      "learning_rate": 1.6647309330236162e-07,
      "loss": 1.974,
      "step": 43
    },
    {
      "epoch": 0.017034456058846303,
      "grad_norm": 1.313948631286621,
      "learning_rate": 1.7034456058846304e-07,
      "loss": 1.9217,
      "step": 44
    },
    {
      "epoch": 0.017421602787456445,
      "grad_norm": 1.3514381647109985,
      "learning_rate": 1.7421602787456448e-07,
      "loss": 1.9363,
      "step": 45
    },
    {
      "epoch": 0.01780874951606659,
      "grad_norm": 1.3341065645217896,
      "learning_rate": 1.7808749516066592e-07,
      "loss": 1.947,
      "step": 46
    },
    {
      "epoch": 0.018195896244676733,
      "grad_norm": 1.6700693368911743,
      "learning_rate": 1.8195896244676736e-07,
      "loss": 1.9013,
      "step": 47
    },
    {
      "epoch": 0.018583042973286876,
      "grad_norm": 1.316781997680664,
      "learning_rate": 1.8583042973286877e-07,
      "loss": 1.9705,
      "step": 48
    },
    {
      "epoch": 0.018970189701897018,
      "grad_norm": 1.6118602752685547,
      "learning_rate": 1.897018970189702e-07,
      "loss": 1.945,
      "step": 49
    },
    {
      "epoch": 0.019357336430507164,
      "grad_norm": 1.5270217657089233,
      "learning_rate": 1.9357336430507165e-07,
      "loss": 1.9544,
      "step": 50
    },
    {
      "epoch": 0.019744483159117306,
      "grad_norm": 1.4832324981689453,
      "learning_rate": 1.9744483159117307e-07,
      "loss": 1.9545,
      "step": 51
    },
    {
      "epoch": 0.02013162988772745,
      "grad_norm": 1.3447526693344116,
      "learning_rate": 2.013162988772745e-07,
      "loss": 1.9503,
      "step": 52
    },
    {
      "epoch": 0.02051877661633759,
      "grad_norm": 1.3612908124923706,
      "learning_rate": 2.0518776616337592e-07,
      "loss": 1.9433,
      "step": 53
    },
    {
      "epoch": 0.020905923344947737,
      "grad_norm": 1.390224814414978,
      "learning_rate": 2.090592334494774e-07,
      "loss": 1.94,
      "step": 54
    },
    {
      "epoch": 0.02129307007355788,
      "grad_norm": 1.323393702507019,
      "learning_rate": 2.129307007355788e-07,
      "loss": 1.9492,
      "step": 55
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 1.2235162258148193,
      "learning_rate": 2.1680216802168024e-07,
      "loss": 1.9142,
      "step": 56
    },
    {
      "epoch": 0.022067363530778164,
      "grad_norm": 1.4587013721466064,
      "learning_rate": 2.2067363530778166e-07,
      "loss": 1.9667,
      "step": 57
    },
    {
      "epoch": 0.02245451025938831,
      "grad_norm": 1.5683465003967285,
      "learning_rate": 2.2454510259388312e-07,
      "loss": 1.9902,
      "step": 58
    },
    {
      "epoch": 0.022841656987998452,
      "grad_norm": 1.1734521389007568,
      "learning_rate": 2.2841656987998454e-07,
      "loss": 1.9399,
      "step": 59
    },
    {
      "epoch": 0.023228803716608595,
      "grad_norm": 1.223766803741455,
      "learning_rate": 2.3228803716608595e-07,
      "loss": 1.9228,
      "step": 60
    },
    {
      "epoch": 0.023615950445218737,
      "grad_norm": 1.4083489179611206,
      "learning_rate": 2.361595044521874e-07,
      "loss": 1.9386,
      "step": 61
    },
    {
      "epoch": 0.024003097173828883,
      "grad_norm": 1.222882628440857,
      "learning_rate": 2.4003097173828883e-07,
      "loss": 1.9436,
      "step": 62
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 1.1779512166976929,
      "learning_rate": 2.439024390243903e-07,
      "loss": 1.9587,
      "step": 63
    },
    {
      "epoch": 0.024777390631049168,
      "grad_norm": 1.3427867889404297,
      "learning_rate": 2.477739063104917e-07,
      "loss": 1.9423,
      "step": 64
    },
    {
      "epoch": 0.02516453735965931,
      "grad_norm": 1.3166260719299316,
      "learning_rate": 2.516453735965931e-07,
      "loss": 1.9297,
      "step": 65
    },
    {
      "epoch": 0.025551684088269456,
      "grad_norm": 1.241493582725525,
      "learning_rate": 2.555168408826946e-07,
      "loss": 1.9211,
      "step": 66
    },
    {
      "epoch": 0.025938830816879598,
      "grad_norm": 1.08889901638031,
      "learning_rate": 2.59388308168796e-07,
      "loss": 1.9327,
      "step": 67
    },
    {
      "epoch": 0.02632597754548974,
      "grad_norm": 1.323351502418518,
      "learning_rate": 2.632597754548974e-07,
      "loss": 1.9433,
      "step": 68
    },
    {
      "epoch": 0.026713124274099883,
      "grad_norm": 1.6102895736694336,
      "learning_rate": 2.6713124274099886e-07,
      "loss": 1.9331,
      "step": 69
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 1.2116812467575073,
      "learning_rate": 2.710027100271003e-07,
      "loss": 1.9339,
      "step": 70
    },
    {
      "epoch": 0.02748741773132017,
      "grad_norm": 1.355094075202942,
      "learning_rate": 2.7487417731320175e-07,
      "loss": 1.9562,
      "step": 71
    },
    {
      "epoch": 0.027874564459930314,
      "grad_norm": 1.508041262626648,
      "learning_rate": 2.7874564459930313e-07,
      "loss": 1.9267,
      "step": 72
    },
    {
      "epoch": 0.028261711188540456,
      "grad_norm": 1.884343147277832,
      "learning_rate": 2.8261711188540457e-07,
      "loss": 1.9387,
      "step": 73
    },
    {
      "epoch": 0.028648857917150602,
      "grad_norm": 1.177363395690918,
      "learning_rate": 2.8648857917150607e-07,
      "loss": 1.9102,
      "step": 74
    },
    {
      "epoch": 0.029036004645760744,
      "grad_norm": 1.6059622764587402,
      "learning_rate": 2.9036004645760745e-07,
      "loss": 1.966,
      "step": 75
    },
    {
      "epoch": 0.029423151374370887,
      "grad_norm": 1.9946407079696655,
      "learning_rate": 2.942315137437089e-07,
      "loss": 1.9223,
      "step": 76
    },
    {
      "epoch": 0.02981029810298103,
      "grad_norm": 1.584816575050354,
      "learning_rate": 2.9810298102981034e-07,
      "loss": 1.9353,
      "step": 77
    },
    {
      "epoch": 0.030197444831591175,
      "grad_norm": 1.761682152748108,
      "learning_rate": 3.019744483159118e-07,
      "loss": 1.9959,
      "step": 78
    },
    {
      "epoch": 0.030584591560201317,
      "grad_norm": 1.2681838274002075,
      "learning_rate": 3.058459156020132e-07,
      "loss": 1.9638,
      "step": 79
    },
    {
      "epoch": 0.03097173828881146,
      "grad_norm": 1.2599982023239136,
      "learning_rate": 3.097173828881146e-07,
      "loss": 1.9294,
      "step": 80
    },
    {
      "epoch": 0.0313588850174216,
      "grad_norm": 1.2641021013259888,
      "learning_rate": 3.1358885017421604e-07,
      "loss": 1.9599,
      "step": 81
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 1.806406855583191,
      "learning_rate": 3.174603174603175e-07,
      "loss": 1.9721,
      "step": 82
    },
    {
      "epoch": 0.03213317847464189,
      "grad_norm": 1.8139973878860474,
      "learning_rate": 3.2133178474641887e-07,
      "loss": 1.9249,
      "step": 83
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 1.4414734840393066,
      "learning_rate": 3.2520325203252037e-07,
      "loss": 1.9418,
      "step": 84
    },
    {
      "epoch": 0.03290747193186218,
      "grad_norm": 1.3878464698791504,
      "learning_rate": 3.290747193186218e-07,
      "loss": 1.9529,
      "step": 85
    },
    {
      "epoch": 0.03329461866047232,
      "grad_norm": 1.3376766443252563,
      "learning_rate": 3.3294618660472325e-07,
      "loss": 1.9536,
      "step": 86
    },
    {
      "epoch": 0.03368176538908246,
      "grad_norm": 1.1098401546478271,
      "learning_rate": 3.3681765389082463e-07,
      "loss": 1.9283,
      "step": 87
    },
    {
      "epoch": 0.034068912117692605,
      "grad_norm": 1.6624932289123535,
      "learning_rate": 3.406891211769261e-07,
      "loss": 1.9864,
      "step": 88
    },
    {
      "epoch": 0.03445605884630275,
      "grad_norm": 1.6171804666519165,
      "learning_rate": 3.445605884630275e-07,
      "loss": 1.9468,
      "step": 89
    },
    {
      "epoch": 0.03484320557491289,
      "grad_norm": 1.2598140239715576,
      "learning_rate": 3.4843205574912896e-07,
      "loss": 1.9234,
      "step": 90
    },
    {
      "epoch": 0.03523035230352303,
      "grad_norm": 1.2662670612335205,
      "learning_rate": 3.5230352303523034e-07,
      "loss": 1.9764,
      "step": 91
    },
    {
      "epoch": 0.03561749903213318,
      "grad_norm": 1.4466278553009033,
      "learning_rate": 3.5617499032133184e-07,
      "loss": 1.9249,
      "step": 92
    },
    {
      "epoch": 0.036004645760743324,
      "grad_norm": 1.3765774965286255,
      "learning_rate": 3.600464576074333e-07,
      "loss": 1.9697,
      "step": 93
    },
    {
      "epoch": 0.03639179248935347,
      "grad_norm": 1.7024292945861816,
      "learning_rate": 3.639179248935347e-07,
      "loss": 1.9322,
      "step": 94
    },
    {
      "epoch": 0.03677893921796361,
      "grad_norm": 1.2493489980697632,
      "learning_rate": 3.677893921796361e-07,
      "loss": 1.9433,
      "step": 95
    },
    {
      "epoch": 0.03716608594657375,
      "grad_norm": 1.2947214841842651,
      "learning_rate": 3.7166085946573755e-07,
      "loss": 1.9419,
      "step": 96
    },
    {
      "epoch": 0.037553232675183894,
      "grad_norm": 1.1667033433914185,
      "learning_rate": 3.75532326751839e-07,
      "loss": 1.9536,
      "step": 97
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 1.2410893440246582,
      "learning_rate": 3.794037940379404e-07,
      "loss": 1.9438,
      "step": 98
    },
    {
      "epoch": 0.03832752613240418,
      "grad_norm": 1.519470453262329,
      "learning_rate": 3.832752613240418e-07,
      "loss": 1.9765,
      "step": 99
    },
    {
      "epoch": 0.03871467286101433,
      "grad_norm": 1.4211254119873047,
      "learning_rate": 3.871467286101433e-07,
      "loss": 1.9347,
      "step": 100
    },
    {
      "epoch": 0.03910181958962447,
      "grad_norm": 1.6876616477966309,
      "learning_rate": 3.9101819589624475e-07,
      "loss": 1.9288,
      "step": 101
    },
    {
      "epoch": 0.03948896631823461,
      "grad_norm": 1.8199079036712646,
      "learning_rate": 3.9488966318234614e-07,
      "loss": 1.9188,
      "step": 102
    },
    {
      "epoch": 0.039876113046844755,
      "grad_norm": 1.3045974969863892,
      "learning_rate": 3.987611304684476e-07,
      "loss": 1.9071,
      "step": 103
    },
    {
      "epoch": 0.0402632597754549,
      "grad_norm": 1.272830605506897,
      "learning_rate": 4.02632597754549e-07,
      "loss": 1.9353,
      "step": 104
    },
    {
      "epoch": 0.04065040650406504,
      "grad_norm": 1.3459570407867432,
      "learning_rate": 4.0650406504065046e-07,
      "loss": 1.9307,
      "step": 105
    },
    {
      "epoch": 0.04103755323267518,
      "grad_norm": 1.138325572013855,
      "learning_rate": 4.1037553232675184e-07,
      "loss": 1.9277,
      "step": 106
    },
    {
      "epoch": 0.041424699961285325,
      "grad_norm": 1.2090156078338623,
      "learning_rate": 4.142469996128533e-07,
      "loss": 1.9336,
      "step": 107
    },
    {
      "epoch": 0.041811846689895474,
      "grad_norm": 1.7125427722930908,
      "learning_rate": 4.181184668989548e-07,
      "loss": 1.9698,
      "step": 108
    },
    {
      "epoch": 0.042198993418505616,
      "grad_norm": 1.2674199342727661,
      "learning_rate": 4.219899341850562e-07,
      "loss": 1.9546,
      "step": 109
    },
    {
      "epoch": 0.04258614014711576,
      "grad_norm": 1.2682015895843506,
      "learning_rate": 4.258614014711576e-07,
      "loss": 1.9724,
      "step": 110
    },
    {
      "epoch": 0.0429732868757259,
      "grad_norm": 1.6381535530090332,
      "learning_rate": 4.2973286875725905e-07,
      "loss": 1.9327,
      "step": 111
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 1.373987078666687,
      "learning_rate": 4.336043360433605e-07,
      "loss": 1.9685,
      "step": 112
    },
    {
      "epoch": 0.043747580332946186,
      "grad_norm": 1.551061987876892,
      "learning_rate": 4.374758033294619e-07,
      "loss": 1.9154,
      "step": 113
    },
    {
      "epoch": 0.04413472706155633,
      "grad_norm": 1.1360738277435303,
      "learning_rate": 4.413472706155633e-07,
      "loss": 1.9266,
      "step": 114
    },
    {
      "epoch": 0.04452187379016647,
      "grad_norm": 1.3859291076660156,
      "learning_rate": 4.4521873790166476e-07,
      "loss": 1.9895,
      "step": 115
    },
    {
      "epoch": 0.04490902051877662,
      "grad_norm": 1.1683683395385742,
      "learning_rate": 4.4909020518776625e-07,
      "loss": 1.9103,
      "step": 116
    },
    {
      "epoch": 0.04529616724738676,
      "grad_norm": 1.4732296466827393,
      "learning_rate": 4.5296167247386764e-07,
      "loss": 1.9578,
      "step": 117
    },
    {
      "epoch": 0.045683313975996905,
      "grad_norm": 1.3103270530700684,
      "learning_rate": 4.568331397599691e-07,
      "loss": 1.9643,
      "step": 118
    },
    {
      "epoch": 0.04607046070460705,
      "grad_norm": 1.541742205619812,
      "learning_rate": 4.607046070460705e-07,
      "loss": 1.9323,
      "step": 119
    },
    {
      "epoch": 0.04645760743321719,
      "grad_norm": 1.4031747579574585,
      "learning_rate": 4.645760743321719e-07,
      "loss": 1.9424,
      "step": 120
    },
    {
      "epoch": 0.04684475416182733,
      "grad_norm": 1.2545462846755981,
      "learning_rate": 4.6844754161827335e-07,
      "loss": 1.9171,
      "step": 121
    },
    {
      "epoch": 0.047231900890437474,
      "grad_norm": 1.7230219841003418,
      "learning_rate": 4.723190089043748e-07,
      "loss": 1.9284,
      "step": 122
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.366240382194519,
      "learning_rate": 4.7619047619047623e-07,
      "loss": 1.9595,
      "step": 123
    },
    {
      "epoch": 0.048006194347657766,
      "grad_norm": 1.2956231832504272,
      "learning_rate": 4.800619434765777e-07,
      "loss": 1.9364,
      "step": 124
    },
    {
      "epoch": 0.04839334107626791,
      "grad_norm": 1.6009840965270996,
      "learning_rate": 4.839334107626792e-07,
      "loss": 1.9576,
      "step": 125
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 1.8668384552001953,
      "learning_rate": 4.878048780487805e-07,
      "loss": 1.9437,
      "step": 126
    },
    {
      "epoch": 0.04916763453348819,
      "grad_norm": 1.2493070363998413,
      "learning_rate": 4.916763453348819e-07,
      "loss": 1.9069,
      "step": 127
    },
    {
      "epoch": 0.049554781262098335,
      "grad_norm": 1.4693695306777954,
      "learning_rate": 4.955478126209834e-07,
      "loss": 1.9378,
      "step": 128
    },
    {
      "epoch": 0.04994192799070848,
      "grad_norm": 1.3185383081436157,
      "learning_rate": 4.994192799070848e-07,
      "loss": 1.9629,
      "step": 129
    },
    {
      "epoch": 0.05032907471931862,
      "grad_norm": 1.330763339996338,
      "learning_rate": 5.032907471931862e-07,
      "loss": 1.9106,
      "step": 130
    },
    {
      "epoch": 0.05071622144792876,
      "grad_norm": 1.402917742729187,
      "learning_rate": 5.071622144792877e-07,
      "loss": 1.9535,
      "step": 131
    },
    {
      "epoch": 0.05110336817653891,
      "grad_norm": 1.5598068237304688,
      "learning_rate": 5.110336817653892e-07,
      "loss": 1.9177,
      "step": 132
    },
    {
      "epoch": 0.051490514905149054,
      "grad_norm": 1.5118436813354492,
      "learning_rate": 5.149051490514906e-07,
      "loss": 1.9613,
      "step": 133
    },
    {
      "epoch": 0.051877661633759196,
      "grad_norm": 1.3305963277816772,
      "learning_rate": 5.18776616337592e-07,
      "loss": 1.9311,
      "step": 134
    },
    {
      "epoch": 0.05226480836236934,
      "grad_norm": 1.5102858543395996,
      "learning_rate": 5.226480836236935e-07,
      "loss": 1.9032,
      "step": 135
    },
    {
      "epoch": 0.05265195509097948,
      "grad_norm": 1.3898465633392334,
      "learning_rate": 5.265195509097948e-07,
      "loss": 1.9022,
      "step": 136
    },
    {
      "epoch": 0.053039101819589624,
      "grad_norm": 1.5764615535736084,
      "learning_rate": 5.303910181958962e-07,
      "loss": 1.9854,
      "step": 137
    },
    {
      "epoch": 0.053426248548199766,
      "grad_norm": 1.2301346063613892,
      "learning_rate": 5.342624854819977e-07,
      "loss": 1.9294,
      "step": 138
    },
    {
      "epoch": 0.05381339527680991,
      "grad_norm": 1.7448948621749878,
      "learning_rate": 5.381339527680991e-07,
      "loss": 1.9606,
      "step": 139
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 1.4438631534576416,
      "learning_rate": 5.420054200542006e-07,
      "loss": 1.9549,
      "step": 140
    },
    {
      "epoch": 0.0545876887340302,
      "grad_norm": 1.266018033027649,
      "learning_rate": 5.45876887340302e-07,
      "loss": 1.9444,
      "step": 141
    },
    {
      "epoch": 0.05497483546264034,
      "grad_norm": 1.7858085632324219,
      "learning_rate": 5.497483546264035e-07,
      "loss": 1.965,
      "step": 142
    },
    {
      "epoch": 0.055361982191250485,
      "grad_norm": 1.4917521476745605,
      "learning_rate": 5.536198219125049e-07,
      "loss": 1.9479,
      "step": 143
    },
    {
      "epoch": 0.05574912891986063,
      "grad_norm": 1.7303102016448975,
      "learning_rate": 5.574912891986063e-07,
      "loss": 1.9638,
      "step": 144
    },
    {
      "epoch": 0.05613627564847077,
      "grad_norm": 1.435689091682434,
      "learning_rate": 5.613627564847078e-07,
      "loss": 1.914,
      "step": 145
    },
    {
      "epoch": 0.05652342237708091,
      "grad_norm": 1.178758978843689,
      "learning_rate": 5.652342237708091e-07,
      "loss": 1.9394,
      "step": 146
    },
    {
      "epoch": 0.056910569105691054,
      "grad_norm": 1.3719794750213623,
      "learning_rate": 5.691056910569106e-07,
      "loss": 1.9192,
      "step": 147
    },
    {
      "epoch": 0.057297715834301204,
      "grad_norm": 1.689508318901062,
      "learning_rate": 5.729771583430121e-07,
      "loss": 1.9621,
      "step": 148
    },
    {
      "epoch": 0.057684862562911346,
      "grad_norm": 1.4846851825714111,
      "learning_rate": 5.768486256291135e-07,
      "loss": 1.9338,
      "step": 149
    },
    {
      "epoch": 0.05807200929152149,
      "grad_norm": 2.1773600578308105,
      "learning_rate": 5.807200929152149e-07,
      "loss": 1.954,
      "step": 150
    },
    {
      "epoch": 0.05845915602013163,
      "grad_norm": 1.3258416652679443,
      "learning_rate": 5.845915602013164e-07,
      "loss": 1.9391,
      "step": 151
    },
    {
      "epoch": 0.05884630274874177,
      "grad_norm": 1.0761454105377197,
      "learning_rate": 5.884630274874178e-07,
      "loss": 1.9304,
      "step": 152
    },
    {
      "epoch": 0.059233449477351915,
      "grad_norm": 1.9608542919158936,
      "learning_rate": 5.923344947735192e-07,
      "loss": 1.9314,
      "step": 153
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 1.4622232913970947,
      "learning_rate": 5.962059620596207e-07,
      "loss": 1.9449,
      "step": 154
    },
    {
      "epoch": 0.0600077429345722,
      "grad_norm": 1.386207103729248,
      "learning_rate": 6.000774293457221e-07,
      "loss": 1.9475,
      "step": 155
    },
    {
      "epoch": 0.06039488966318235,
      "grad_norm": 1.374375343322754,
      "learning_rate": 6.039488966318236e-07,
      "loss": 1.9563,
      "step": 156
    },
    {
      "epoch": 0.06078203639179249,
      "grad_norm": 1.9986577033996582,
      "learning_rate": 6.078203639179249e-07,
      "loss": 1.8984,
      "step": 157
    },
    {
      "epoch": 0.061169183120402634,
      "grad_norm": 1.337410807609558,
      "learning_rate": 6.116918312040264e-07,
      "loss": 1.9413,
      "step": 158
    },
    {
      "epoch": 0.06155632984901278,
      "grad_norm": 1.569469928741455,
      "learning_rate": 6.155632984901278e-07,
      "loss": 1.9382,
      "step": 159
    },
    {
      "epoch": 0.06194347657762292,
      "grad_norm": 1.4656800031661987,
      "learning_rate": 6.194347657762292e-07,
      "loss": 1.9587,
      "step": 160
    },
    {
      "epoch": 0.06233062330623306,
      "grad_norm": 1.5173181295394897,
      "learning_rate": 6.233062330623307e-07,
      "loss": 1.8969,
      "step": 161
    },
    {
      "epoch": 0.0627177700348432,
      "grad_norm": 1.3920223712921143,
      "learning_rate": 6.271777003484321e-07,
      "loss": 1.9535,
      "step": 162
    },
    {
      "epoch": 0.06310491676345335,
      "grad_norm": 1.5692684650421143,
      "learning_rate": 6.310491676345336e-07,
      "loss": 1.964,
      "step": 163
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 1.865328073501587,
      "learning_rate": 6.34920634920635e-07,
      "loss": 1.9307,
      "step": 164
    },
    {
      "epoch": 0.06387921022067364,
      "grad_norm": 1.199167013168335,
      "learning_rate": 6.387921022067365e-07,
      "loss": 1.9431,
      "step": 165
    },
    {
      "epoch": 0.06426635694928377,
      "grad_norm": 1.1244466304779053,
      "learning_rate": 6.426635694928377e-07,
      "loss": 1.961,
      "step": 166
    },
    {
      "epoch": 0.06465350367789392,
      "grad_norm": 1.2851150035858154,
      "learning_rate": 6.465350367789392e-07,
      "loss": 1.9451,
      "step": 167
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 1.5376777648925781,
      "learning_rate": 6.504065040650407e-07,
      "loss": 1.9772,
      "step": 168
    },
    {
      "epoch": 0.06542779713511421,
      "grad_norm": 1.279203176498413,
      "learning_rate": 6.542779713511421e-07,
      "loss": 1.9595,
      "step": 169
    },
    {
      "epoch": 0.06581494386372436,
      "grad_norm": 1.3015878200531006,
      "learning_rate": 6.581494386372436e-07,
      "loss": 1.9246,
      "step": 170
    },
    {
      "epoch": 0.06620209059233449,
      "grad_norm": 1.3563429117202759,
      "learning_rate": 6.62020905923345e-07,
      "loss": 1.9266,
      "step": 171
    },
    {
      "epoch": 0.06658923732094464,
      "grad_norm": 1.6971479654312134,
      "learning_rate": 6.658923732094465e-07,
      "loss": 1.9665,
      "step": 172
    },
    {
      "epoch": 0.06697638404955478,
      "grad_norm": 1.6323630809783936,
      "learning_rate": 6.697638404955478e-07,
      "loss": 1.9093,
      "step": 173
    },
    {
      "epoch": 0.06736353077816493,
      "grad_norm": 2.0553784370422363,
      "learning_rate": 6.736353077816493e-07,
      "loss": 1.9728,
      "step": 174
    },
    {
      "epoch": 0.06775067750677506,
      "grad_norm": 1.456234335899353,
      "learning_rate": 6.775067750677507e-07,
      "loss": 1.9113,
      "step": 175
    },
    {
      "epoch": 0.06813782423538521,
      "grad_norm": 1.291272759437561,
      "learning_rate": 6.813782423538521e-07,
      "loss": 1.9161,
      "step": 176
    },
    {
      "epoch": 0.06852497096399536,
      "grad_norm": 2.527912139892578,
      "learning_rate": 6.852497096399536e-07,
      "loss": 2.01,
      "step": 177
    },
    {
      "epoch": 0.0689121176926055,
      "grad_norm": 1.593970537185669,
      "learning_rate": 6.89121176926055e-07,
      "loss": 1.9579,
      "step": 178
    },
    {
      "epoch": 0.06929926442121565,
      "grad_norm": 1.4862563610076904,
      "learning_rate": 6.929926442121565e-07,
      "loss": 1.9594,
      "step": 179
    },
    {
      "epoch": 0.06968641114982578,
      "grad_norm": 1.3331727981567383,
      "learning_rate": 6.968641114982579e-07,
      "loss": 1.9385,
      "step": 180
    },
    {
      "epoch": 0.07007355787843593,
      "grad_norm": 1.440492868423462,
      "learning_rate": 7.007355787843594e-07,
      "loss": 1.9491,
      "step": 181
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 1.2666289806365967,
      "learning_rate": 7.046070460704607e-07,
      "loss": 1.9182,
      "step": 182
    },
    {
      "epoch": 0.07084785133565621,
      "grad_norm": 2.116395950317383,
      "learning_rate": 7.084785133565622e-07,
      "loss": 1.8837,
      "step": 183
    },
    {
      "epoch": 0.07123499806426636,
      "grad_norm": 1.2691996097564697,
      "learning_rate": 7.123499806426637e-07,
      "loss": 1.9147,
      "step": 184
    },
    {
      "epoch": 0.0716221447928765,
      "grad_norm": 1.5789631605148315,
      "learning_rate": 7.162214479287651e-07,
      "loss": 1.9514,
      "step": 185
    },
    {
      "epoch": 0.07200929152148665,
      "grad_norm": 1.7871671915054321,
      "learning_rate": 7.200929152148666e-07,
      "loss": 1.9124,
      "step": 186
    },
    {
      "epoch": 0.07239643825009678,
      "grad_norm": 1.3387380838394165,
      "learning_rate": 7.239643825009679e-07,
      "loss": 1.9585,
      "step": 187
    },
    {
      "epoch": 0.07278358497870693,
      "grad_norm": 1.8699023723602295,
      "learning_rate": 7.278358497870694e-07,
      "loss": 2.0036,
      "step": 188
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 1.2714886665344238,
      "learning_rate": 7.317073170731707e-07,
      "loss": 1.9415,
      "step": 189
    },
    {
      "epoch": 0.07355787843592722,
      "grad_norm": 1.475311517715454,
      "learning_rate": 7.355787843592722e-07,
      "loss": 1.9552,
      "step": 190
    },
    {
      "epoch": 0.07394502516453735,
      "grad_norm": 2.807035446166992,
      "learning_rate": 7.394502516453736e-07,
      "loss": 2.012,
      "step": 191
    },
    {
      "epoch": 0.0743321718931475,
      "grad_norm": 1.132474422454834,
      "learning_rate": 7.433217189314751e-07,
      "loss": 1.9424,
      "step": 192
    },
    {
      "epoch": 0.07471931862175765,
      "grad_norm": 1.295541524887085,
      "learning_rate": 7.471931862175766e-07,
      "loss": 1.923,
      "step": 193
    },
    {
      "epoch": 0.07510646535036779,
      "grad_norm": 1.4100650548934937,
      "learning_rate": 7.51064653503678e-07,
      "loss": 1.9477,
      "step": 194
    },
    {
      "epoch": 0.07549361207897794,
      "grad_norm": 2.203207492828369,
      "learning_rate": 7.549361207897795e-07,
      "loss": 1.9149,
      "step": 195
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 1.3338545560836792,
      "learning_rate": 7.588075880758807e-07,
      "loss": 1.9155,
      "step": 196
    },
    {
      "epoch": 0.07626790553619822,
      "grad_norm": 1.1864159107208252,
      "learning_rate": 7.626790553619822e-07,
      "loss": 1.9412,
      "step": 197
    },
    {
      "epoch": 0.07665505226480836,
      "grad_norm": 1.3234816789627075,
      "learning_rate": 7.665505226480836e-07,
      "loss": 1.9523,
      "step": 198
    },
    {
      "epoch": 0.0770421989934185,
      "grad_norm": 1.321771502494812,
      "learning_rate": 7.704219899341851e-07,
      "loss": 1.9225,
      "step": 199
    },
    {
      "epoch": 0.07742934572202866,
      "grad_norm": 1.8656835556030273,
      "learning_rate": 7.742934572202866e-07,
      "loss": 1.9862,
      "step": 200
    },
    {
      "epoch": 0.07781649245063879,
      "grad_norm": 1.1836506128311157,
      "learning_rate": 7.78164924506388e-07,
      "loss": 1.9598,
      "step": 201
    },
    {
      "epoch": 0.07820363917924894,
      "grad_norm": 1.8203198909759521,
      "learning_rate": 7.820363917924895e-07,
      "loss": 1.9778,
      "step": 202
    },
    {
      "epoch": 0.07859078590785908,
      "grad_norm": 1.5249191522598267,
      "learning_rate": 7.859078590785908e-07,
      "loss": 1.9573,
      "step": 203
    },
    {
      "epoch": 0.07897793263646923,
      "grad_norm": 3.275322437286377,
      "learning_rate": 7.897793263646923e-07,
      "loss": 2.0267,
      "step": 204
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 1.334331750869751,
      "learning_rate": 7.936507936507937e-07,
      "loss": 1.9509,
      "step": 205
    },
    {
      "epoch": 0.07975222609368951,
      "grad_norm": 1.411104440689087,
      "learning_rate": 7.975222609368952e-07,
      "loss": 1.9031,
      "step": 206
    },
    {
      "epoch": 0.08013937282229965,
      "grad_norm": 1.5724178552627563,
      "learning_rate": 8.013937282229965e-07,
      "loss": 1.9363,
      "step": 207
    },
    {
      "epoch": 0.0805265195509098,
      "grad_norm": 1.317803144454956,
      "learning_rate": 8.05265195509098e-07,
      "loss": 1.8998,
      "step": 208
    },
    {
      "epoch": 0.08091366627951994,
      "grad_norm": 1.4644187688827515,
      "learning_rate": 8.091366627951995e-07,
      "loss": 1.9177,
      "step": 209
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 1.3745898008346558,
      "learning_rate": 8.130081300813009e-07,
      "loss": 1.9465,
      "step": 210
    },
    {
      "epoch": 0.08168795973674023,
      "grad_norm": 1.160555124282837,
      "learning_rate": 8.168795973674024e-07,
      "loss": 1.9458,
      "step": 211
    },
    {
      "epoch": 0.08207510646535036,
      "grad_norm": 1.3809980154037476,
      "learning_rate": 8.207510646535037e-07,
      "loss": 1.9491,
      "step": 212
    },
    {
      "epoch": 0.08246225319396051,
      "grad_norm": 1.1839203834533691,
      "learning_rate": 8.246225319396052e-07,
      "loss": 1.9471,
      "step": 213
    },
    {
      "epoch": 0.08284939992257065,
      "grad_norm": 1.617287516593933,
      "learning_rate": 8.284939992257066e-07,
      "loss": 1.9789,
      "step": 214
    },
    {
      "epoch": 0.0832365466511808,
      "grad_norm": 1.344551920890808,
      "learning_rate": 8.323654665118081e-07,
      "loss": 1.9604,
      "step": 215
    },
    {
      "epoch": 0.08362369337979095,
      "grad_norm": 1.2424730062484741,
      "learning_rate": 8.362369337979096e-07,
      "loss": 1.9451,
      "step": 216
    },
    {
      "epoch": 0.08401084010840108,
      "grad_norm": 2.60916805267334,
      "learning_rate": 8.401084010840109e-07,
      "loss": 2.007,
      "step": 217
    },
    {
      "epoch": 0.08439798683701123,
      "grad_norm": 1.6512129306793213,
      "learning_rate": 8.439798683701124e-07,
      "loss": 1.9516,
      "step": 218
    },
    {
      "epoch": 0.08478513356562137,
      "grad_norm": 1.2789881229400635,
      "learning_rate": 8.478513356562137e-07,
      "loss": 1.9559,
      "step": 219
    },
    {
      "epoch": 0.08517228029423152,
      "grad_norm": 1.200788140296936,
      "learning_rate": 8.517228029423152e-07,
      "loss": 1.9446,
      "step": 220
    },
    {
      "epoch": 0.08555942702284165,
      "grad_norm": 2.3394548892974854,
      "learning_rate": 8.555942702284166e-07,
      "loss": 1.9619,
      "step": 221
    },
    {
      "epoch": 0.0859465737514518,
      "grad_norm": 1.5540934801101685,
      "learning_rate": 8.594657375145181e-07,
      "loss": 1.9772,
      "step": 222
    },
    {
      "epoch": 0.08633372048006194,
      "grad_norm": 1.2303707599639893,
      "learning_rate": 8.633372048006195e-07,
      "loss": 1.9447,
      "step": 223
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 2.3388192653656006,
      "learning_rate": 8.67208672086721e-07,
      "loss": 1.9759,
      "step": 224
    },
    {
      "epoch": 0.08710801393728224,
      "grad_norm": 1.3377221822738647,
      "learning_rate": 8.710801393728225e-07,
      "loss": 1.938,
      "step": 225
    },
    {
      "epoch": 0.08749516066589237,
      "grad_norm": 1.5286308526992798,
      "learning_rate": 8.749516066589237e-07,
      "loss": 1.967,
      "step": 226
    },
    {
      "epoch": 0.08788230739450252,
      "grad_norm": 1.6421114206314087,
      "learning_rate": 8.788230739450252e-07,
      "loss": 1.9575,
      "step": 227
    },
    {
      "epoch": 0.08826945412311266,
      "grad_norm": 1.849982500076294,
      "learning_rate": 8.826945412311266e-07,
      "loss": 1.8846,
      "step": 228
    },
    {
      "epoch": 0.0886566008517228,
      "grad_norm": 1.2569493055343628,
      "learning_rate": 8.865660085172281e-07,
      "loss": 1.9301,
      "step": 229
    },
    {
      "epoch": 0.08904374758033294,
      "grad_norm": 1.2114485502243042,
      "learning_rate": 8.904374758033295e-07,
      "loss": 1.9306,
      "step": 230
    },
    {
      "epoch": 0.08943089430894309,
      "grad_norm": 1.7603652477264404,
      "learning_rate": 8.94308943089431e-07,
      "loss": 1.9647,
      "step": 231
    },
    {
      "epoch": 0.08981804103755324,
      "grad_norm": 2.010676145553589,
      "learning_rate": 8.981804103755325e-07,
      "loss": 1.8844,
      "step": 232
    },
    {
      "epoch": 0.09020518776616337,
      "grad_norm": 2.038668394088745,
      "learning_rate": 9.020518776616338e-07,
      "loss": 1.9139,
      "step": 233
    },
    {
      "epoch": 0.09059233449477352,
      "grad_norm": 1.405318260192871,
      "learning_rate": 9.059233449477353e-07,
      "loss": 1.9412,
      "step": 234
    },
    {
      "epoch": 0.09097948122338366,
      "grad_norm": 1.33861243724823,
      "learning_rate": 9.097948122338367e-07,
      "loss": 1.9402,
      "step": 235
    },
    {
      "epoch": 0.09136662795199381,
      "grad_norm": 1.2831127643585205,
      "learning_rate": 9.136662795199382e-07,
      "loss": 1.9616,
      "step": 236
    },
    {
      "epoch": 0.09175377468060394,
      "grad_norm": 1.9888291358947754,
      "learning_rate": 9.175377468060395e-07,
      "loss": 1.955,
      "step": 237
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 1.2296912670135498,
      "learning_rate": 9.21409214092141e-07,
      "loss": 1.9453,
      "step": 238
    },
    {
      "epoch": 0.09252806813782423,
      "grad_norm": 1.3826355934143066,
      "learning_rate": 9.252806813782423e-07,
      "loss": 1.914,
      "step": 239
    },
    {
      "epoch": 0.09291521486643438,
      "grad_norm": 1.311548113822937,
      "learning_rate": 9.291521486643438e-07,
      "loss": 1.9442,
      "step": 240
    },
    {
      "epoch": 0.09330236159504453,
      "grad_norm": 1.4872076511383057,
      "learning_rate": 9.330236159504453e-07,
      "loss": 1.9136,
      "step": 241
    },
    {
      "epoch": 0.09368950832365466,
      "grad_norm": 1.64584481716156,
      "learning_rate": 9.368950832365467e-07,
      "loss": 1.8918,
      "step": 242
    },
    {
      "epoch": 0.09407665505226481,
      "grad_norm": 1.434674859046936,
      "learning_rate": 9.407665505226482e-07,
      "loss": 1.9021,
      "step": 243
    },
    {
      "epoch": 0.09446380178087495,
      "grad_norm": 1.2531490325927734,
      "learning_rate": 9.446380178087496e-07,
      "loss": 1.9471,
      "step": 244
    },
    {
      "epoch": 0.0948509485094851,
      "grad_norm": 1.3403780460357666,
      "learning_rate": 9.485094850948511e-07,
      "loss": 1.957,
      "step": 245
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 1.2302957773208618,
      "learning_rate": 9.523809523809525e-07,
      "loss": 1.9588,
      "step": 246
    },
    {
      "epoch": 0.09562524196670538,
      "grad_norm": 2.386007308959961,
      "learning_rate": 9.562524196670538e-07,
      "loss": 1.9031,
      "step": 247
    },
    {
      "epoch": 0.09601238869531553,
      "grad_norm": 2.1216280460357666,
      "learning_rate": 9.601238869531553e-07,
      "loss": 1.8761,
      "step": 248
    },
    {
      "epoch": 0.09639953542392567,
      "grad_norm": 1.5414080619812012,
      "learning_rate": 9.639953542392568e-07,
      "loss": 1.9508,
      "step": 249
    },
    {
      "epoch": 0.09678668215253582,
      "grad_norm": 1.1540870666503906,
      "learning_rate": 9.678668215253583e-07,
      "loss": 1.9154,
      "step": 250
    },
    {
      "epoch": 0.09717382888114595,
      "grad_norm": 1.7975988388061523,
      "learning_rate": 9.717382888114596e-07,
      "loss": 1.8962,
      "step": 251
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.2319954633712769,
      "learning_rate": 9.75609756097561e-07,
      "loss": 1.9131,
      "step": 252
    },
    {
      "epoch": 0.09794812233836624,
      "grad_norm": 1.3950631618499756,
      "learning_rate": 9.794812233836624e-07,
      "loss": 1.9454,
      "step": 253
    },
    {
      "epoch": 0.09833526906697639,
      "grad_norm": 1.2658807039260864,
      "learning_rate": 9.833526906697639e-07,
      "loss": 1.9204,
      "step": 254
    },
    {
      "epoch": 0.09872241579558652,
      "grad_norm": 1.4267122745513916,
      "learning_rate": 9.872241579558654e-07,
      "loss": 1.9249,
      "step": 255
    },
    {
      "epoch": 0.09910956252419667,
      "grad_norm": 1.1770508289337158,
      "learning_rate": 9.910956252419669e-07,
      "loss": 1.9455,
      "step": 256
    },
    {
      "epoch": 0.09949670925280682,
      "grad_norm": 1.8326971530914307,
      "learning_rate": 9.949670925280684e-07,
      "loss": 1.9653,
      "step": 257
    },
    {
      "epoch": 0.09988385598141696,
      "grad_norm": 1.2412647008895874,
      "learning_rate": 9.988385598141696e-07,
      "loss": 1.9064,
      "step": 258
    },
    {
      "epoch": 0.1002710027100271,
      "grad_norm": 1.4522029161453247,
      "learning_rate": 1.0027100271002711e-06,
      "loss": 1.9688,
      "step": 259
    },
    {
      "epoch": 0.10065814943863724,
      "grad_norm": 1.323971152305603,
      "learning_rate": 1.0065814943863724e-06,
      "loss": 1.9651,
      "step": 260
    },
    {
      "epoch": 0.10104529616724739,
      "grad_norm": 1.4504698514938354,
      "learning_rate": 1.010452961672474e-06,
      "loss": 1.968,
      "step": 261
    },
    {
      "epoch": 0.10143244289585752,
      "grad_norm": 1.3116633892059326,
      "learning_rate": 1.0143244289585754e-06,
      "loss": 1.9339,
      "step": 262
    },
    {
      "epoch": 0.10181958962446767,
      "grad_norm": 1.3598530292510986,
      "learning_rate": 1.0181958962446769e-06,
      "loss": 1.9549,
      "step": 263
    },
    {
      "epoch": 0.10220673635307782,
      "grad_norm": 1.6924139261245728,
      "learning_rate": 1.0220673635307784e-06,
      "loss": 1.9765,
      "step": 264
    },
    {
      "epoch": 0.10259388308168796,
      "grad_norm": 1.5049668550491333,
      "learning_rate": 1.0259388308168797e-06,
      "loss": 1.9353,
      "step": 265
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 1.8106191158294678,
      "learning_rate": 1.0298102981029812e-06,
      "loss": 1.9957,
      "step": 266
    },
    {
      "epoch": 0.10336817653890824,
      "grad_norm": 1.6184200048446655,
      "learning_rate": 1.0336817653890824e-06,
      "loss": 1.9578,
      "step": 267
    },
    {
      "epoch": 0.10375532326751839,
      "grad_norm": 1.5791020393371582,
      "learning_rate": 1.037553232675184e-06,
      "loss": 1.9382,
      "step": 268
    },
    {
      "epoch": 0.10414246999612853,
      "grad_norm": 2.790430784225464,
      "learning_rate": 1.0414246999612854e-06,
      "loss": 2.014,
      "step": 269
    },
    {
      "epoch": 0.10452961672473868,
      "grad_norm": 1.7857729196548462,
      "learning_rate": 1.045296167247387e-06,
      "loss": 1.9737,
      "step": 270
    },
    {
      "epoch": 0.10491676345334881,
      "grad_norm": 1.3903334140777588,
      "learning_rate": 1.0491676345334882e-06,
      "loss": 1.9677,
      "step": 271
    },
    {
      "epoch": 0.10530391018195896,
      "grad_norm": 1.6178395748138428,
      "learning_rate": 1.0530391018195897e-06,
      "loss": 1.9698,
      "step": 272
    },
    {
      "epoch": 0.10569105691056911,
      "grad_norm": 1.5775846242904663,
      "learning_rate": 1.0569105691056912e-06,
      "loss": 1.9566,
      "step": 273
    },
    {
      "epoch": 0.10607820363917925,
      "grad_norm": 1.633928894996643,
      "learning_rate": 1.0607820363917925e-06,
      "loss": 1.9469,
      "step": 274
    },
    {
      "epoch": 0.1064653503677894,
      "grad_norm": 1.3794456720352173,
      "learning_rate": 1.064653503677894e-06,
      "loss": 1.9286,
      "step": 275
    },
    {
      "epoch": 0.10685249709639953,
      "grad_norm": 1.7416026592254639,
      "learning_rate": 1.0685249709639955e-06,
      "loss": 1.9669,
      "step": 276
    },
    {
      "epoch": 0.10723964382500968,
      "grad_norm": 1.34681236743927,
      "learning_rate": 1.072396438250097e-06,
      "loss": 1.9763,
      "step": 277
    },
    {
      "epoch": 0.10762679055361982,
      "grad_norm": 1.1881003379821777,
      "learning_rate": 1.0762679055361982e-06,
      "loss": 1.9483,
      "step": 278
    },
    {
      "epoch": 0.10801393728222997,
      "grad_norm": 1.3797640800476074,
      "learning_rate": 1.0801393728222997e-06,
      "loss": 1.9368,
      "step": 279
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 1.714133620262146,
      "learning_rate": 1.0840108401084012e-06,
      "loss": 1.959,
      "step": 280
    },
    {
      "epoch": 0.10878823073945025,
      "grad_norm": 1.6516321897506714,
      "learning_rate": 1.0878823073945025e-06,
      "loss": 1.9496,
      "step": 281
    },
    {
      "epoch": 0.1091753774680604,
      "grad_norm": 1.2354130744934082,
      "learning_rate": 1.091753774680604e-06,
      "loss": 1.911,
      "step": 282
    },
    {
      "epoch": 0.10956252419667054,
      "grad_norm": 1.9756921529769897,
      "learning_rate": 1.0956252419667055e-06,
      "loss": 1.9926,
      "step": 283
    },
    {
      "epoch": 0.10994967092528068,
      "grad_norm": 1.754900574684143,
      "learning_rate": 1.099496709252807e-06,
      "loss": 1.9661,
      "step": 284
    },
    {
      "epoch": 0.11033681765389082,
      "grad_norm": 1.5594189167022705,
      "learning_rate": 1.1033681765389083e-06,
      "loss": 1.9244,
      "step": 285
    },
    {
      "epoch": 0.11072396438250097,
      "grad_norm": 1.324192762374878,
      "learning_rate": 1.1072396438250098e-06,
      "loss": 1.9389,
      "step": 286
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.5862911939620972,
      "learning_rate": 1.111111111111111e-06,
      "loss": 1.9498,
      "step": 287
    },
    {
      "epoch": 0.11149825783972125,
      "grad_norm": 1.4328017234802246,
      "learning_rate": 1.1149825783972125e-06,
      "loss": 1.9242,
      "step": 288
    },
    {
      "epoch": 0.1118854045683314,
      "grad_norm": 1.3288697004318237,
      "learning_rate": 1.118854045683314e-06,
      "loss": 1.9258,
      "step": 289
    },
    {
      "epoch": 0.11227255129694154,
      "grad_norm": 1.9579726457595825,
      "learning_rate": 1.1227255129694155e-06,
      "loss": 1.9294,
      "step": 290
    },
    {
      "epoch": 0.11265969802555169,
      "grad_norm": 1.5679636001586914,
      "learning_rate": 1.126596980255517e-06,
      "loss": 1.9553,
      "step": 291
    },
    {
      "epoch": 0.11304684475416182,
      "grad_norm": 1.6586045026779175,
      "learning_rate": 1.1304684475416183e-06,
      "loss": 1.9197,
      "step": 292
    },
    {
      "epoch": 0.11343399148277197,
      "grad_norm": 1.574398398399353,
      "learning_rate": 1.1343399148277198e-06,
      "loss": 1.9067,
      "step": 293
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 1.2593274116516113,
      "learning_rate": 1.1382113821138213e-06,
      "loss": 1.9165,
      "step": 294
    },
    {
      "epoch": 0.11420828493999226,
      "grad_norm": 1.6787760257720947,
      "learning_rate": 1.1420828493999228e-06,
      "loss": 1.9367,
      "step": 295
    },
    {
      "epoch": 0.11459543166860241,
      "grad_norm": 1.301345705986023,
      "learning_rate": 1.1459543166860243e-06,
      "loss": 1.931,
      "step": 296
    },
    {
      "epoch": 0.11498257839721254,
      "grad_norm": 1.7856166362762451,
      "learning_rate": 1.1498257839721255e-06,
      "loss": 1.9596,
      "step": 297
    },
    {
      "epoch": 0.11536972512582269,
      "grad_norm": 2.10019588470459,
      "learning_rate": 1.153697251258227e-06,
      "loss": 1.9118,
      "step": 298
    },
    {
      "epoch": 0.11575687185443283,
      "grad_norm": 1.2561169862747192,
      "learning_rate": 1.1575687185443283e-06,
      "loss": 1.9156,
      "step": 299
    },
    {
      "epoch": 0.11614401858304298,
      "grad_norm": 1.9176281690597534,
      "learning_rate": 1.1614401858304298e-06,
      "loss": 2.0002,
      "step": 300
    },
    {
      "epoch": 0.11653116531165311,
      "grad_norm": 1.355483889579773,
      "learning_rate": 1.1653116531165313e-06,
      "loss": 1.9141,
      "step": 301
    },
    {
      "epoch": 0.11691831204026326,
      "grad_norm": 1.5007543563842773,
      "learning_rate": 1.1691831204026328e-06,
      "loss": 1.9266,
      "step": 302
    },
    {
      "epoch": 0.1173054587688734,
      "grad_norm": 1.4830043315887451,
      "learning_rate": 1.173054587688734e-06,
      "loss": 1.9098,
      "step": 303
    },
    {
      "epoch": 0.11769260549748355,
      "grad_norm": 1.8125677108764648,
      "learning_rate": 1.1769260549748356e-06,
      "loss": 1.9666,
      "step": 304
    },
    {
      "epoch": 0.1180797522260937,
      "grad_norm": 1.7718329429626465,
      "learning_rate": 1.180797522260937e-06,
      "loss": 1.9011,
      "step": 305
    },
    {
      "epoch": 0.11846689895470383,
      "grad_norm": 1.7778793573379517,
      "learning_rate": 1.1846689895470384e-06,
      "loss": 1.9549,
      "step": 306
    },
    {
      "epoch": 0.11885404568331398,
      "grad_norm": 1.3464010953903198,
      "learning_rate": 1.1885404568331398e-06,
      "loss": 1.9262,
      "step": 307
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 1.5550071001052856,
      "learning_rate": 1.1924119241192413e-06,
      "loss": 1.9432,
      "step": 308
    },
    {
      "epoch": 0.11962833914053426,
      "grad_norm": 1.3861435651779175,
      "learning_rate": 1.1962833914053428e-06,
      "loss": 1.9498,
      "step": 309
    },
    {
      "epoch": 0.1200154858691444,
      "grad_norm": 1.558093547821045,
      "learning_rate": 1.2001548586914441e-06,
      "loss": 1.9309,
      "step": 310
    },
    {
      "epoch": 0.12040263259775455,
      "grad_norm": 1.7558469772338867,
      "learning_rate": 1.2040263259775456e-06,
      "loss": 1.9673,
      "step": 311
    },
    {
      "epoch": 0.1207897793263647,
      "grad_norm": 1.3375784158706665,
      "learning_rate": 1.207897793263647e-06,
      "loss": 1.9214,
      "step": 312
    },
    {
      "epoch": 0.12117692605497483,
      "grad_norm": 1.4449307918548584,
      "learning_rate": 1.2117692605497484e-06,
      "loss": 1.9161,
      "step": 313
    },
    {
      "epoch": 0.12156407278358498,
      "grad_norm": 2.441638708114624,
      "learning_rate": 1.2156407278358499e-06,
      "loss": 1.9059,
      "step": 314
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 1.353731632232666,
      "learning_rate": 1.2195121951219514e-06,
      "loss": 1.9085,
      "step": 315
    },
    {
      "epoch": 0.12233836624080527,
      "grad_norm": 3.030303716659546,
      "learning_rate": 1.2233836624080529e-06,
      "loss": 1.994,
      "step": 316
    },
    {
      "epoch": 0.1227255129694154,
      "grad_norm": 1.4710289239883423,
      "learning_rate": 1.2272551296941541e-06,
      "loss": 1.9479,
      "step": 317
    },
    {
      "epoch": 0.12311265969802555,
      "grad_norm": 2.2766623497009277,
      "learning_rate": 1.2311265969802556e-06,
      "loss": 1.9691,
      "step": 318
    },
    {
      "epoch": 0.12349980642663569,
      "grad_norm": 1.7645599842071533,
      "learning_rate": 1.234998064266357e-06,
      "loss": 1.9268,
      "step": 319
    },
    {
      "epoch": 0.12388695315524584,
      "grad_norm": 1.4728820323944092,
      "learning_rate": 1.2388695315524584e-06,
      "loss": 1.9227,
      "step": 320
    },
    {
      "epoch": 0.12427409988385599,
      "grad_norm": 1.4844386577606201,
      "learning_rate": 1.24274099883856e-06,
      "loss": 1.9095,
      "step": 321
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 1.8163331747055054,
      "learning_rate": 1.2466124661246614e-06,
      "loss": 1.9141,
      "step": 322
    },
    {
      "epoch": 0.12504839334107626,
      "grad_norm": 1.5501534938812256,
      "learning_rate": 1.2504839334107627e-06,
      "loss": 1.9188,
      "step": 323
    },
    {
      "epoch": 0.1254355400696864,
      "grad_norm": 1.6929230690002441,
      "learning_rate": 1.2543554006968642e-06,
      "loss": 1.9111,
      "step": 324
    },
    {
      "epoch": 0.12582268679829656,
      "grad_norm": 1.8505001068115234,
      "learning_rate": 1.2582268679829657e-06,
      "loss": 1.9889,
      "step": 325
    },
    {
      "epoch": 0.1262098335269067,
      "grad_norm": 1.5359790325164795,
      "learning_rate": 1.2620983352690672e-06,
      "loss": 1.9704,
      "step": 326
    },
    {
      "epoch": 0.12659698025551683,
      "grad_norm": 2.2799971103668213,
      "learning_rate": 1.2659698025551684e-06,
      "loss": 1.9299,
      "step": 327
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 1.4610644578933716,
      "learning_rate": 1.26984126984127e-06,
      "loss": 1.9008,
      "step": 328
    },
    {
      "epoch": 0.12737127371273713,
      "grad_norm": 1.7973026037216187,
      "learning_rate": 1.2737127371273714e-06,
      "loss": 1.9837,
      "step": 329
    },
    {
      "epoch": 0.12775842044134728,
      "grad_norm": 1.6464828252792358,
      "learning_rate": 1.277584204413473e-06,
      "loss": 1.9364,
      "step": 330
    },
    {
      "epoch": 0.12814556716995743,
      "grad_norm": 1.63655686378479,
      "learning_rate": 1.2814556716995744e-06,
      "loss": 1.9066,
      "step": 331
    },
    {
      "epoch": 0.12853271389856755,
      "grad_norm": 1.439244270324707,
      "learning_rate": 1.2853271389856755e-06,
      "loss": 1.9623,
      "step": 332
    },
    {
      "epoch": 0.1289198606271777,
      "grad_norm": 1.2896493673324585,
      "learning_rate": 1.289198606271777e-06,
      "loss": 1.9274,
      "step": 333
    },
    {
      "epoch": 0.12930700735578785,
      "grad_norm": 1.627769947052002,
      "learning_rate": 1.2930700735578785e-06,
      "loss": 1.9523,
      "step": 334
    },
    {
      "epoch": 0.129694154084398,
      "grad_norm": 1.3762844800949097,
      "learning_rate": 1.29694154084398e-06,
      "loss": 1.9705,
      "step": 335
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 2.214961051940918,
      "learning_rate": 1.3008130081300815e-06,
      "loss": 1.9149,
      "step": 336
    },
    {
      "epoch": 0.13046844754161827,
      "grad_norm": 1.923017144203186,
      "learning_rate": 1.3046844754161827e-06,
      "loss": 1.9748,
      "step": 337
    },
    {
      "epoch": 0.13085559427022841,
      "grad_norm": 1.5595788955688477,
      "learning_rate": 1.3085559427022842e-06,
      "loss": 1.9196,
      "step": 338
    },
    {
      "epoch": 0.13124274099883856,
      "grad_norm": 1.6769872903823853,
      "learning_rate": 1.3124274099883857e-06,
      "loss": 1.9044,
      "step": 339
    },
    {
      "epoch": 0.1316298877274487,
      "grad_norm": 1.5886876583099365,
      "learning_rate": 1.3162988772744872e-06,
      "loss": 1.9429,
      "step": 340
    },
    {
      "epoch": 0.13201703445605883,
      "grad_norm": 1.8425449132919312,
      "learning_rate": 1.3201703445605885e-06,
      "loss": 1.9779,
      "step": 341
    },
    {
      "epoch": 0.13240418118466898,
      "grad_norm": 2.574467897415161,
      "learning_rate": 1.32404181184669e-06,
      "loss": 1.9092,
      "step": 342
    },
    {
      "epoch": 0.13279132791327913,
      "grad_norm": 1.966910719871521,
      "learning_rate": 1.3279132791327915e-06,
      "loss": 1.8938,
      "step": 343
    },
    {
      "epoch": 0.13317847464188928,
      "grad_norm": 1.4091365337371826,
      "learning_rate": 1.331784746418893e-06,
      "loss": 1.9338,
      "step": 344
    },
    {
      "epoch": 0.13356562137049943,
      "grad_norm": 1.697117805480957,
      "learning_rate": 1.3356562137049945e-06,
      "loss": 1.9069,
      "step": 345
    },
    {
      "epoch": 0.13395276809910955,
      "grad_norm": 1.5179247856140137,
      "learning_rate": 1.3395276809910955e-06,
      "loss": 1.9147,
      "step": 346
    },
    {
      "epoch": 0.1343399148277197,
      "grad_norm": 2.0698840618133545,
      "learning_rate": 1.343399148277197e-06,
      "loss": 1.9749,
      "step": 347
    },
    {
      "epoch": 0.13472706155632985,
      "grad_norm": 1.9285082817077637,
      "learning_rate": 1.3472706155632985e-06,
      "loss": 1.8833,
      "step": 348
    },
    {
      "epoch": 0.13511420828494,
      "grad_norm": 1.9182261228561401,
      "learning_rate": 1.3511420828494e-06,
      "loss": 1.8858,
      "step": 349
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 1.635923981666565,
      "learning_rate": 1.3550135501355013e-06,
      "loss": 1.9989,
      "step": 350
    },
    {
      "epoch": 0.13588850174216027,
      "grad_norm": 2.0719873905181885,
      "learning_rate": 1.3588850174216028e-06,
      "loss": 1.9787,
      "step": 351
    },
    {
      "epoch": 0.13627564847077042,
      "grad_norm": 1.3286869525909424,
      "learning_rate": 1.3627564847077043e-06,
      "loss": 1.9266,
      "step": 352
    },
    {
      "epoch": 0.13666279519938057,
      "grad_norm": 1.9464876651763916,
      "learning_rate": 1.3666279519938058e-06,
      "loss": 1.9644,
      "step": 353
    },
    {
      "epoch": 0.13704994192799072,
      "grad_norm": 1.4783718585968018,
      "learning_rate": 1.3704994192799073e-06,
      "loss": 1.9031,
      "step": 354
    },
    {
      "epoch": 0.13743708865660084,
      "grad_norm": 1.8775720596313477,
      "learning_rate": 1.3743708865660086e-06,
      "loss": 1.9766,
      "step": 355
    },
    {
      "epoch": 0.137824235385211,
      "grad_norm": 1.3642946481704712,
      "learning_rate": 1.37824235385211e-06,
      "loss": 1.9622,
      "step": 356
    },
    {
      "epoch": 0.13821138211382114,
      "grad_norm": 1.9857633113861084,
      "learning_rate": 1.3821138211382116e-06,
      "loss": 1.9673,
      "step": 357
    },
    {
      "epoch": 0.1385985288424313,
      "grad_norm": 2.044436454772949,
      "learning_rate": 1.385985288424313e-06,
      "loss": 1.9704,
      "step": 358
    },
    {
      "epoch": 0.1389856755710414,
      "grad_norm": 1.88657546043396,
      "learning_rate": 1.3898567557104143e-06,
      "loss": 1.9593,
      "step": 359
    },
    {
      "epoch": 0.13937282229965156,
      "grad_norm": 1.629560112953186,
      "learning_rate": 1.3937282229965158e-06,
      "loss": 1.9291,
      "step": 360
    },
    {
      "epoch": 0.1397599690282617,
      "grad_norm": 2.096426010131836,
      "learning_rate": 1.3975996902826173e-06,
      "loss": 1.8883,
      "step": 361
    },
    {
      "epoch": 0.14014711575687186,
      "grad_norm": 1.6367053985595703,
      "learning_rate": 1.4014711575687188e-06,
      "loss": 1.9506,
      "step": 362
    },
    {
      "epoch": 0.140534262485482,
      "grad_norm": 1.7556641101837158,
      "learning_rate": 1.4053426248548203e-06,
      "loss": 1.882,
      "step": 363
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 2.1869797706604004,
      "learning_rate": 1.4092140921409214e-06,
      "loss": 1.9229,
      "step": 364
    },
    {
      "epoch": 0.14130855594270228,
      "grad_norm": 1.9259355068206787,
      "learning_rate": 1.4130855594270229e-06,
      "loss": 1.8881,
      "step": 365
    },
    {
      "epoch": 0.14169570267131243,
      "grad_norm": 1.702422857284546,
      "learning_rate": 1.4169570267131244e-06,
      "loss": 1.8975,
      "step": 366
    },
    {
      "epoch": 0.14208284939992258,
      "grad_norm": 1.7902804613113403,
      "learning_rate": 1.4208284939992259e-06,
      "loss": 1.9469,
      "step": 367
    },
    {
      "epoch": 0.14246999612853273,
      "grad_norm": 1.5810487270355225,
      "learning_rate": 1.4246999612853273e-06,
      "loss": 1.9212,
      "step": 368
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.1189210414886475,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 1.9797,
      "step": 369
    },
    {
      "epoch": 0.143244289585753,
      "grad_norm": 1.8121310472488403,
      "learning_rate": 1.4324428958575301e-06,
      "loss": 1.9687,
      "step": 370
    },
    {
      "epoch": 0.14363143631436315,
      "grad_norm": 1.6179672479629517,
      "learning_rate": 1.4363143631436316e-06,
      "loss": 1.9583,
      "step": 371
    },
    {
      "epoch": 0.1440185830429733,
      "grad_norm": 1.5939662456512451,
      "learning_rate": 1.4401858304297331e-06,
      "loss": 1.8932,
      "step": 372
    },
    {
      "epoch": 0.14440572977158342,
      "grad_norm": 1.6596579551696777,
      "learning_rate": 1.4440572977158344e-06,
      "loss": 1.9112,
      "step": 373
    },
    {
      "epoch": 0.14479287650019357,
      "grad_norm": 2.0239057540893555,
      "learning_rate": 1.4479287650019359e-06,
      "loss": 1.9901,
      "step": 374
    },
    {
      "epoch": 0.14518002322880372,
      "grad_norm": 1.7644284963607788,
      "learning_rate": 1.4518002322880374e-06,
      "loss": 1.9437,
      "step": 375
    },
    {
      "epoch": 0.14556716995741387,
      "grad_norm": 1.8518065214157104,
      "learning_rate": 1.4556716995741389e-06,
      "loss": 1.9919,
      "step": 376
    },
    {
      "epoch": 0.14595431668602402,
      "grad_norm": 1.8030755519866943,
      "learning_rate": 1.4595431668602404e-06,
      "loss": 1.9337,
      "step": 377
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 1.7847497463226318,
      "learning_rate": 1.4634146341463414e-06,
      "loss": 1.958,
      "step": 378
    },
    {
      "epoch": 0.1467286101432443,
      "grad_norm": 1.6623131036758423,
      "learning_rate": 1.467286101432443e-06,
      "loss": 1.946,
      "step": 379
    },
    {
      "epoch": 0.14711575687185444,
      "grad_norm": 1.6354752779006958,
      "learning_rate": 1.4711575687185444e-06,
      "loss": 1.8925,
      "step": 380
    },
    {
      "epoch": 0.14750290360046459,
      "grad_norm": 2.0448474884033203,
      "learning_rate": 1.475029036004646e-06,
      "loss": 1.9933,
      "step": 381
    },
    {
      "epoch": 0.1478900503290747,
      "grad_norm": 1.1962838172912598,
      "learning_rate": 1.4789005032907472e-06,
      "loss": 1.9323,
      "step": 382
    },
    {
      "epoch": 0.14827719705768486,
      "grad_norm": 1.7342759370803833,
      "learning_rate": 1.4827719705768487e-06,
      "loss": 1.9459,
      "step": 383
    },
    {
      "epoch": 0.148664343786295,
      "grad_norm": 2.4976603984832764,
      "learning_rate": 1.4866434378629502e-06,
      "loss": 1.9026,
      "step": 384
    },
    {
      "epoch": 0.14905149051490515,
      "grad_norm": 2.0613508224487305,
      "learning_rate": 1.4905149051490517e-06,
      "loss": 1.9633,
      "step": 385
    },
    {
      "epoch": 0.1494386372435153,
      "grad_norm": 1.9321515560150146,
      "learning_rate": 1.4943863724351532e-06,
      "loss": 1.9258,
      "step": 386
    },
    {
      "epoch": 0.14982578397212543,
      "grad_norm": 1.7624300718307495,
      "learning_rate": 1.4982578397212545e-06,
      "loss": 1.9071,
      "step": 387
    },
    {
      "epoch": 0.15021293070073558,
      "grad_norm": 1.7151979207992554,
      "learning_rate": 1.502129307007356e-06,
      "loss": 1.9735,
      "step": 388
    },
    {
      "epoch": 0.15060007742934572,
      "grad_norm": 1.619200587272644,
      "learning_rate": 1.5060007742934574e-06,
      "loss": 1.8988,
      "step": 389
    },
    {
      "epoch": 0.15098722415795587,
      "grad_norm": 1.7529816627502441,
      "learning_rate": 1.509872241579559e-06,
      "loss": 1.8931,
      "step": 390
    },
    {
      "epoch": 0.151374370886566,
      "grad_norm": 1.6528289318084717,
      "learning_rate": 1.51374370886566e-06,
      "loss": 1.9409,
      "step": 391
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 1.6155864000320435,
      "learning_rate": 1.5176151761517615e-06,
      "loss": 1.9482,
      "step": 392
    },
    {
      "epoch": 0.1521486643437863,
      "grad_norm": 2.1943047046661377,
      "learning_rate": 1.521486643437863e-06,
      "loss": 1.9432,
      "step": 393
    },
    {
      "epoch": 0.15253581107239644,
      "grad_norm": 2.2073628902435303,
      "learning_rate": 1.5253581107239645e-06,
      "loss": 1.9736,
      "step": 394
    },
    {
      "epoch": 0.1529229578010066,
      "grad_norm": 2.9721994400024414,
      "learning_rate": 1.529229578010066e-06,
      "loss": 1.8918,
      "step": 395
    },
    {
      "epoch": 0.15331010452961671,
      "grad_norm": 2.8253636360168457,
      "learning_rate": 1.5331010452961673e-06,
      "loss": 1.9,
      "step": 396
    },
    {
      "epoch": 0.15369725125822686,
      "grad_norm": 1.879274606704712,
      "learning_rate": 1.5369725125822687e-06,
      "loss": 1.8827,
      "step": 397
    },
    {
      "epoch": 0.154084397986837,
      "grad_norm": 1.8416578769683838,
      "learning_rate": 1.5408439798683702e-06,
      "loss": 1.9564,
      "step": 398
    },
    {
      "epoch": 0.15447154471544716,
      "grad_norm": 2.139995813369751,
      "learning_rate": 1.5447154471544717e-06,
      "loss": 1.948,
      "step": 399
    },
    {
      "epoch": 0.1548586914440573,
      "grad_norm": 1.5404483079910278,
      "learning_rate": 1.5485869144405732e-06,
      "loss": 1.8985,
      "step": 400
    },
    {
      "epoch": 0.15524583817266743,
      "grad_norm": 1.5433573722839355,
      "learning_rate": 1.5524583817266745e-06,
      "loss": 1.887,
      "step": 401
    },
    {
      "epoch": 0.15563298490127758,
      "grad_norm": 1.6600062847137451,
      "learning_rate": 1.556329849012776e-06,
      "loss": 1.8934,
      "step": 402
    },
    {
      "epoch": 0.15602013162988773,
      "grad_norm": 1.8858287334442139,
      "learning_rate": 1.5602013162988775e-06,
      "loss": 1.9519,
      "step": 403
    },
    {
      "epoch": 0.15640727835849788,
      "grad_norm": 3.4226059913635254,
      "learning_rate": 1.564072783584979e-06,
      "loss": 2.0058,
      "step": 404
    },
    {
      "epoch": 0.156794425087108,
      "grad_norm": 1.5807850360870361,
      "learning_rate": 1.56794425087108e-06,
      "loss": 1.9268,
      "step": 405
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 1.7989859580993652,
      "learning_rate": 1.5718157181571816e-06,
      "loss": 1.9184,
      "step": 406
    },
    {
      "epoch": 0.1575687185443283,
      "grad_norm": 1.710532546043396,
      "learning_rate": 1.575687185443283e-06,
      "loss": 1.9577,
      "step": 407
    },
    {
      "epoch": 0.15795586527293845,
      "grad_norm": 2.220198154449463,
      "learning_rate": 1.5795586527293845e-06,
      "loss": 1.9777,
      "step": 408
    },
    {
      "epoch": 0.1583430120015486,
      "grad_norm": 1.4623024463653564,
      "learning_rate": 1.583430120015486e-06,
      "loss": 1.9544,
      "step": 409
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 1.648970365524292,
      "learning_rate": 1.5873015873015873e-06,
      "loss": 1.8919,
      "step": 410
    },
    {
      "epoch": 0.15911730545876887,
      "grad_norm": 3.689943552017212,
      "learning_rate": 1.5911730545876888e-06,
      "loss": 2.0226,
      "step": 411
    },
    {
      "epoch": 0.15950445218737902,
      "grad_norm": 1.7296627759933472,
      "learning_rate": 1.5950445218737903e-06,
      "loss": 1.9595,
      "step": 412
    },
    {
      "epoch": 0.15989159891598917,
      "grad_norm": 1.6155670881271362,
      "learning_rate": 1.5989159891598918e-06,
      "loss": 1.9146,
      "step": 413
    },
    {
      "epoch": 0.1602787456445993,
      "grad_norm": 1.8420119285583496,
      "learning_rate": 1.602787456445993e-06,
      "loss": 1.8759,
      "step": 414
    },
    {
      "epoch": 0.16066589237320944,
      "grad_norm": 2.0066492557525635,
      "learning_rate": 1.6066589237320946e-06,
      "loss": 1.9496,
      "step": 415
    },
    {
      "epoch": 0.1610530391018196,
      "grad_norm": 1.871625304222107,
      "learning_rate": 1.610530391018196e-06,
      "loss": 1.9185,
      "step": 416
    },
    {
      "epoch": 0.16144018583042974,
      "grad_norm": 1.4670833349227905,
      "learning_rate": 1.6144018583042976e-06,
      "loss": 1.9508,
      "step": 417
    },
    {
      "epoch": 0.1618273325590399,
      "grad_norm": 3.789684772491455,
      "learning_rate": 1.618273325590399e-06,
      "loss": 2.0372,
      "step": 418
    },
    {
      "epoch": 0.16221447928765,
      "grad_norm": 2.0267770290374756,
      "learning_rate": 1.6221447928765003e-06,
      "loss": 1.9542,
      "step": 419
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 2.1309025287628174,
      "learning_rate": 1.6260162601626018e-06,
      "loss": 1.93,
      "step": 420
    },
    {
      "epoch": 0.1629887727448703,
      "grad_norm": 1.7066789865493774,
      "learning_rate": 1.6298877274487033e-06,
      "loss": 1.9887,
      "step": 421
    },
    {
      "epoch": 0.16337591947348046,
      "grad_norm": 1.6124634742736816,
      "learning_rate": 1.6337591947348048e-06,
      "loss": 1.9384,
      "step": 422
    },
    {
      "epoch": 0.16376306620209058,
      "grad_norm": 2.008396863937378,
      "learning_rate": 1.6376306620209059e-06,
      "loss": 1.9237,
      "step": 423
    },
    {
      "epoch": 0.16415021293070073,
      "grad_norm": 3.0317580699920654,
      "learning_rate": 1.6415021293070074e-06,
      "loss": 1.9999,
      "step": 424
    },
    {
      "epoch": 0.16453735965931088,
      "grad_norm": 1.720217227935791,
      "learning_rate": 1.6453735965931089e-06,
      "loss": 1.8905,
      "step": 425
    },
    {
      "epoch": 0.16492450638792103,
      "grad_norm": 1.7010760307312012,
      "learning_rate": 1.6492450638792104e-06,
      "loss": 1.8893,
      "step": 426
    },
    {
      "epoch": 0.16531165311653118,
      "grad_norm": 1.8502633571624756,
      "learning_rate": 1.6531165311653119e-06,
      "loss": 1.8761,
      "step": 427
    },
    {
      "epoch": 0.1656987998451413,
      "grad_norm": 1.9518791437149048,
      "learning_rate": 1.6569879984514131e-06,
      "loss": 1.9272,
      "step": 428
    },
    {
      "epoch": 0.16608594657375145,
      "grad_norm": 1.5597469806671143,
      "learning_rate": 1.6608594657375146e-06,
      "loss": 1.9212,
      "step": 429
    },
    {
      "epoch": 0.1664730933023616,
      "grad_norm": 1.6594756841659546,
      "learning_rate": 1.6647309330236161e-06,
      "loss": 1.9082,
      "step": 430
    },
    {
      "epoch": 0.16686024003097175,
      "grad_norm": 2.568570137023926,
      "learning_rate": 1.6686024003097176e-06,
      "loss": 1.9329,
      "step": 431
    },
    {
      "epoch": 0.1672473867595819,
      "grad_norm": 1.8203858137130737,
      "learning_rate": 1.6724738675958191e-06,
      "loss": 1.977,
      "step": 432
    },
    {
      "epoch": 0.16763453348819202,
      "grad_norm": 1.8662112951278687,
      "learning_rate": 1.6763453348819204e-06,
      "loss": 1.9073,
      "step": 433
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 2.7789127826690674,
      "learning_rate": 1.6802168021680219e-06,
      "loss": 1.9687,
      "step": 434
    },
    {
      "epoch": 0.16840882694541232,
      "grad_norm": 1.8848450183868408,
      "learning_rate": 1.6840882694541234e-06,
      "loss": 1.9468,
      "step": 435
    },
    {
      "epoch": 0.16879597367402246,
      "grad_norm": 1.9737122058868408,
      "learning_rate": 1.6879597367402249e-06,
      "loss": 1.9407,
      "step": 436
    },
    {
      "epoch": 0.1691831204026326,
      "grad_norm": 1.8369290828704834,
      "learning_rate": 1.691831204026326e-06,
      "loss": 1.8792,
      "step": 437
    },
    {
      "epoch": 0.16957026713124274,
      "grad_norm": 2.665700912475586,
      "learning_rate": 1.6957026713124274e-06,
      "loss": 1.898,
      "step": 438
    },
    {
      "epoch": 0.16995741385985288,
      "grad_norm": 2.0381131172180176,
      "learning_rate": 1.699574138598529e-06,
      "loss": 1.9852,
      "step": 439
    },
    {
      "epoch": 0.17034456058846303,
      "grad_norm": 2.0582683086395264,
      "learning_rate": 1.7034456058846304e-06,
      "loss": 1.8832,
      "step": 440
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 2.215968370437622,
      "learning_rate": 1.707317073170732e-06,
      "loss": 1.9686,
      "step": 441
    },
    {
      "epoch": 0.1711188540456833,
      "grad_norm": 2.136892318725586,
      "learning_rate": 1.7111885404568332e-06,
      "loss": 1.9417,
      "step": 442
    },
    {
      "epoch": 0.17150600077429345,
      "grad_norm": 1.8776793479919434,
      "learning_rate": 1.7150600077429347e-06,
      "loss": 1.8849,
      "step": 443
    },
    {
      "epoch": 0.1718931475029036,
      "grad_norm": 2.1167948246002197,
      "learning_rate": 1.7189314750290362e-06,
      "loss": 1.9597,
      "step": 444
    },
    {
      "epoch": 0.17228029423151375,
      "grad_norm": 1.548703908920288,
      "learning_rate": 1.7228029423151377e-06,
      "loss": 1.9166,
      "step": 445
    },
    {
      "epoch": 0.17266744096012387,
      "grad_norm": 1.735309362411499,
      "learning_rate": 1.726674409601239e-06,
      "loss": 1.9208,
      "step": 446
    },
    {
      "epoch": 0.17305458768873402,
      "grad_norm": 2.2364118099212646,
      "learning_rate": 1.7305458768873405e-06,
      "loss": 1.9475,
      "step": 447
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 1.9087903499603271,
      "learning_rate": 1.734417344173442e-06,
      "loss": 1.9559,
      "step": 448
    },
    {
      "epoch": 0.17382888114595432,
      "grad_norm": 1.683915376663208,
      "learning_rate": 1.7382888114595434e-06,
      "loss": 1.8875,
      "step": 449
    },
    {
      "epoch": 0.17421602787456447,
      "grad_norm": 1.8590646982192993,
      "learning_rate": 1.742160278745645e-06,
      "loss": 1.9165,
      "step": 450
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 1.775416374206543,
      "learning_rate": 1.746031746031746e-06,
      "loss": 1.9376,
      "step": 451
    },
    {
      "epoch": 0.17499032133178474,
      "grad_norm": 1.5585452318191528,
      "learning_rate": 1.7499032133178475e-06,
      "loss": 1.899,
      "step": 452
    },
    {
      "epoch": 0.1753774680603949,
      "grad_norm": 1.9003162384033203,
      "learning_rate": 1.753774680603949e-06,
      "loss": 1.9676,
      "step": 453
    },
    {
      "epoch": 0.17576461478900504,
      "grad_norm": 2.2428629398345947,
      "learning_rate": 1.7576461478900505e-06,
      "loss": 1.9091,
      "step": 454
    },
    {
      "epoch": 0.17615176151761516,
      "grad_norm": 2.1124215126037598,
      "learning_rate": 1.7615176151761518e-06,
      "loss": 1.9249,
      "step": 455
    },
    {
      "epoch": 0.1765389082462253,
      "grad_norm": 2.023526430130005,
      "learning_rate": 1.7653890824622533e-06,
      "loss": 1.9182,
      "step": 456
    },
    {
      "epoch": 0.17692605497483546,
      "grad_norm": 2.263871192932129,
      "learning_rate": 1.7692605497483548e-06,
      "loss": 1.8959,
      "step": 457
    },
    {
      "epoch": 0.1773132017034456,
      "grad_norm": 1.727168083190918,
      "learning_rate": 1.7731320170344562e-06,
      "loss": 1.9249,
      "step": 458
    },
    {
      "epoch": 0.17770034843205576,
      "grad_norm": 2.3711769580841064,
      "learning_rate": 1.7770034843205577e-06,
      "loss": 1.89,
      "step": 459
    },
    {
      "epoch": 0.17808749516066588,
      "grad_norm": 1.6670210361480713,
      "learning_rate": 1.780874951606659e-06,
      "loss": 1.915,
      "step": 460
    },
    {
      "epoch": 0.17847464188927603,
      "grad_norm": 3.0449299812316895,
      "learning_rate": 1.7847464188927605e-06,
      "loss": 1.8581,
      "step": 461
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 2.191378116607666,
      "learning_rate": 1.788617886178862e-06,
      "loss": 1.8564,
      "step": 462
    },
    {
      "epoch": 0.17924893534649633,
      "grad_norm": 1.9549640417099,
      "learning_rate": 1.7924893534649635e-06,
      "loss": 1.9251,
      "step": 463
    },
    {
      "epoch": 0.17963608207510648,
      "grad_norm": 2.2771153450012207,
      "learning_rate": 1.796360820751065e-06,
      "loss": 1.9291,
      "step": 464
    },
    {
      "epoch": 0.1800232288037166,
      "grad_norm": 1.8234041929244995,
      "learning_rate": 1.800232288037166e-06,
      "loss": 1.8824,
      "step": 465
    },
    {
      "epoch": 0.18041037553232675,
      "grad_norm": 2.7391409873962402,
      "learning_rate": 1.8041037553232676e-06,
      "loss": 1.9647,
      "step": 466
    },
    {
      "epoch": 0.1807975222609369,
      "grad_norm": 2.333094358444214,
      "learning_rate": 1.807975222609369e-06,
      "loss": 1.9167,
      "step": 467
    },
    {
      "epoch": 0.18118466898954705,
      "grad_norm": 1.7818106412887573,
      "learning_rate": 1.8118466898954705e-06,
      "loss": 1.9099,
      "step": 468
    },
    {
      "epoch": 0.18157181571815717,
      "grad_norm": 2.2323849201202393,
      "learning_rate": 1.8157181571815718e-06,
      "loss": 1.9488,
      "step": 469
    },
    {
      "epoch": 0.18195896244676732,
      "grad_norm": 2.204543352127075,
      "learning_rate": 1.8195896244676733e-06,
      "loss": 1.9422,
      "step": 470
    },
    {
      "epoch": 0.18234610917537747,
      "grad_norm": 3.3988256454467773,
      "learning_rate": 1.8234610917537748e-06,
      "loss": 1.8778,
      "step": 471
    },
    {
      "epoch": 0.18273325590398762,
      "grad_norm": 1.9499329328536987,
      "learning_rate": 1.8273325590398763e-06,
      "loss": 1.8762,
      "step": 472
    },
    {
      "epoch": 0.18312040263259777,
      "grad_norm": 2.0696651935577393,
      "learning_rate": 1.8312040263259778e-06,
      "loss": 1.8734,
      "step": 473
    },
    {
      "epoch": 0.1835075493612079,
      "grad_norm": 1.943961501121521,
      "learning_rate": 1.835075493612079e-06,
      "loss": 1.8755,
      "step": 474
    },
    {
      "epoch": 0.18389469608981804,
      "grad_norm": 2.222642660140991,
      "learning_rate": 1.8389469608981806e-06,
      "loss": 1.8614,
      "step": 475
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 1.7302738428115845,
      "learning_rate": 1.842818428184282e-06,
      "loss": 1.95,
      "step": 476
    },
    {
      "epoch": 0.18466898954703834,
      "grad_norm": 2.9227631092071533,
      "learning_rate": 1.8466898954703836e-06,
      "loss": 1.8966,
      "step": 477
    },
    {
      "epoch": 0.18505613627564846,
      "grad_norm": 2.488569498062134,
      "learning_rate": 1.8505613627564846e-06,
      "loss": 1.8509,
      "step": 478
    },
    {
      "epoch": 0.1854432830042586,
      "grad_norm": 2.4713664054870605,
      "learning_rate": 1.8544328300425861e-06,
      "loss": 1.8599,
      "step": 479
    },
    {
      "epoch": 0.18583042973286876,
      "grad_norm": 2.2355926036834717,
      "learning_rate": 1.8583042973286876e-06,
      "loss": 1.8724,
      "step": 480
    },
    {
      "epoch": 0.1862175764614789,
      "grad_norm": 1.877783179283142,
      "learning_rate": 1.8621757646147891e-06,
      "loss": 1.9127,
      "step": 481
    },
    {
      "epoch": 0.18660472319008906,
      "grad_norm": 1.7124749422073364,
      "learning_rate": 1.8660472319008906e-06,
      "loss": 1.9015,
      "step": 482
    },
    {
      "epoch": 0.18699186991869918,
      "grad_norm": 2.6785898208618164,
      "learning_rate": 1.8699186991869919e-06,
      "loss": 1.9304,
      "step": 483
    },
    {
      "epoch": 0.18737901664730933,
      "grad_norm": 2.280778169631958,
      "learning_rate": 1.8737901664730934e-06,
      "loss": 1.9159,
      "step": 484
    },
    {
      "epoch": 0.18776616337591948,
      "grad_norm": 1.917722463607788,
      "learning_rate": 1.8776616337591949e-06,
      "loss": 1.9146,
      "step": 485
    },
    {
      "epoch": 0.18815331010452963,
      "grad_norm": 1.9566518068313599,
      "learning_rate": 1.8815331010452964e-06,
      "loss": 1.9506,
      "step": 486
    },
    {
      "epoch": 0.18854045683313975,
      "grad_norm": 2.3477766513824463,
      "learning_rate": 1.8854045683313977e-06,
      "loss": 1.9187,
      "step": 487
    },
    {
      "epoch": 0.1889276035617499,
      "grad_norm": 2.4909584522247314,
      "learning_rate": 1.8892760356174991e-06,
      "loss": 1.9382,
      "step": 488
    },
    {
      "epoch": 0.18931475029036005,
      "grad_norm": 2.0873820781707764,
      "learning_rate": 1.8931475029036006e-06,
      "loss": 1.9359,
      "step": 489
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 2.571516990661621,
      "learning_rate": 1.8970189701897021e-06,
      "loss": 1.9586,
      "step": 490
    },
    {
      "epoch": 0.19008904374758034,
      "grad_norm": 2.38154935836792,
      "learning_rate": 1.9008904374758036e-06,
      "loss": 1.947,
      "step": 491
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 2.5822691917419434,
      "learning_rate": 1.904761904761905e-06,
      "loss": 1.8536,
      "step": 492
    },
    {
      "epoch": 0.19086333720480061,
      "grad_norm": 1.8056399822235107,
      "learning_rate": 1.908633372048006e-06,
      "loss": 1.9063,
      "step": 493
    },
    {
      "epoch": 0.19125048393341076,
      "grad_norm": 3.5948235988616943,
      "learning_rate": 1.9125048393341077e-06,
      "loss": 1.967,
      "step": 494
    },
    {
      "epoch": 0.1916376306620209,
      "grad_norm": 2.4224820137023926,
      "learning_rate": 1.916376306620209e-06,
      "loss": 1.9639,
      "step": 495
    },
    {
      "epoch": 0.19202477739063106,
      "grad_norm": 2.342782974243164,
      "learning_rate": 1.9202477739063107e-06,
      "loss": 1.9245,
      "step": 496
    },
    {
      "epoch": 0.19241192411924118,
      "grad_norm": 3.6940810680389404,
      "learning_rate": 1.924119241192412e-06,
      "loss": 2.0253,
      "step": 497
    },
    {
      "epoch": 0.19279907084785133,
      "grad_norm": 3.257767915725708,
      "learning_rate": 1.9279907084785137e-06,
      "loss": 1.8736,
      "step": 498
    },
    {
      "epoch": 0.19318621757646148,
      "grad_norm": 3.9524102210998535,
      "learning_rate": 1.931862175764615e-06,
      "loss": 1.9619,
      "step": 499
    },
    {
      "epoch": 0.19357336430507163,
      "grad_norm": 3.4401321411132812,
      "learning_rate": 1.9357336430507166e-06,
      "loss": 2.0274,
      "step": 500
    },
    {
      "epoch": 0.19396051103368175,
      "grad_norm": 2.4816038608551025,
      "learning_rate": 1.9396051103368177e-06,
      "loss": 1.8443,
      "step": 501
    },
    {
      "epoch": 0.1943476577622919,
      "grad_norm": 3.461838960647583,
      "learning_rate": 1.943476577622919e-06,
      "loss": 1.8317,
      "step": 502
    },
    {
      "epoch": 0.19473480449090205,
      "grad_norm": 2.773894786834717,
      "learning_rate": 1.9473480449090207e-06,
      "loss": 1.9651,
      "step": 503
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 2.1346523761749268,
      "learning_rate": 1.951219512195122e-06,
      "loss": 1.948,
      "step": 504
    },
    {
      "epoch": 0.19550909794812235,
      "grad_norm": 2.639450788497925,
      "learning_rate": 1.9550909794812237e-06,
      "loss": 1.955,
      "step": 505
    },
    {
      "epoch": 0.19589624467673247,
      "grad_norm": 2.8137636184692383,
      "learning_rate": 1.9589624467673248e-06,
      "loss": 1.9477,
      "step": 506
    },
    {
      "epoch": 0.19628339140534262,
      "grad_norm": 2.0274181365966797,
      "learning_rate": 1.9628339140534263e-06,
      "loss": 1.8955,
      "step": 507
    },
    {
      "epoch": 0.19667053813395277,
      "grad_norm": 2.113105058670044,
      "learning_rate": 1.9667053813395277e-06,
      "loss": 1.8924,
      "step": 508
    },
    {
      "epoch": 0.19705768486256292,
      "grad_norm": 2.6175010204315186,
      "learning_rate": 1.9705768486256292e-06,
      "loss": 1.9068,
      "step": 509
    },
    {
      "epoch": 0.19744483159117304,
      "grad_norm": 2.2982449531555176,
      "learning_rate": 1.9744483159117307e-06,
      "loss": 1.9035,
      "step": 510
    },
    {
      "epoch": 0.1978319783197832,
      "grad_norm": 2.0805115699768066,
      "learning_rate": 1.9783197831978322e-06,
      "loss": 1.9404,
      "step": 511
    },
    {
      "epoch": 0.19821912504839334,
      "grad_norm": 2.186197519302368,
      "learning_rate": 1.9821912504839337e-06,
      "loss": 1.8626,
      "step": 512
    },
    {
      "epoch": 0.1986062717770035,
      "grad_norm": 2.702214002609253,
      "learning_rate": 1.986062717770035e-06,
      "loss": 1.8421,
      "step": 513
    },
    {
      "epoch": 0.19899341850561364,
      "grad_norm": 2.377042293548584,
      "learning_rate": 1.9899341850561367e-06,
      "loss": 1.9021,
      "step": 514
    },
    {
      "epoch": 0.19938056523422376,
      "grad_norm": 2.3623898029327393,
      "learning_rate": 1.9938056523422378e-06,
      "loss": 1.8568,
      "step": 515
    },
    {
      "epoch": 0.1997677119628339,
      "grad_norm": 2.3873507976531982,
      "learning_rate": 1.9976771196283393e-06,
      "loss": 1.9689,
      "step": 516
    },
    {
      "epoch": 0.20015485869144406,
      "grad_norm": 2.4235477447509766,
      "learning_rate": 2.0015485869144408e-06,
      "loss": 1.9833,
      "step": 517
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 2.501016139984131,
      "learning_rate": 2.0054200542005423e-06,
      "loss": 1.9063,
      "step": 518
    },
    {
      "epoch": 0.20092915214866433,
      "grad_norm": 2.4915363788604736,
      "learning_rate": 2.0092915214866433e-06,
      "loss": 1.8524,
      "step": 519
    },
    {
      "epoch": 0.20131629887727448,
      "grad_norm": 2.2108585834503174,
      "learning_rate": 2.013162988772745e-06,
      "loss": 1.9087,
      "step": 520
    },
    {
      "epoch": 0.20170344560588463,
      "grad_norm": 2.323448896408081,
      "learning_rate": 2.0170344560588463e-06,
      "loss": 1.9437,
      "step": 521
    },
    {
      "epoch": 0.20209059233449478,
      "grad_norm": 2.4101457595825195,
      "learning_rate": 2.020905923344948e-06,
      "loss": 1.8875,
      "step": 522
    },
    {
      "epoch": 0.20247773906310493,
      "grad_norm": 2.8812882900238037,
      "learning_rate": 2.0247773906310493e-06,
      "loss": 2.0077,
      "step": 523
    },
    {
      "epoch": 0.20286488579171505,
      "grad_norm": 2.8530945777893066,
      "learning_rate": 2.028648857917151e-06,
      "loss": 1.882,
      "step": 524
    },
    {
      "epoch": 0.2032520325203252,
      "grad_norm": 3.3358843326568604,
      "learning_rate": 2.0325203252032523e-06,
      "loss": 1.8882,
      "step": 525
    },
    {
      "epoch": 0.20363917924893535,
      "grad_norm": 2.7170238494873047,
      "learning_rate": 2.0363917924893538e-06,
      "loss": 1.8855,
      "step": 526
    },
    {
      "epoch": 0.2040263259775455,
      "grad_norm": 3.7087786197662354,
      "learning_rate": 2.0402632597754553e-06,
      "loss": 1.882,
      "step": 527
    },
    {
      "epoch": 0.20441347270615565,
      "grad_norm": 2.993332862854004,
      "learning_rate": 2.0441347270615568e-06,
      "loss": 1.9938,
      "step": 528
    },
    {
      "epoch": 0.20480061943476577,
      "grad_norm": 2.101775884628296,
      "learning_rate": 2.048006194347658e-06,
      "loss": 1.921,
      "step": 529
    },
    {
      "epoch": 0.20518776616337592,
      "grad_norm": 2.6562321186065674,
      "learning_rate": 2.0518776616337593e-06,
      "loss": 1.8606,
      "step": 530
    },
    {
      "epoch": 0.20557491289198607,
      "grad_norm": 2.7955009937286377,
      "learning_rate": 2.055749128919861e-06,
      "loss": 1.8412,
      "step": 531
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 3.4236927032470703,
      "learning_rate": 2.0596205962059623e-06,
      "loss": 1.83,
      "step": 532
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 3.0495593547821045,
      "learning_rate": 2.0634920634920634e-06,
      "loss": 2.0067,
      "step": 533
    },
    {
      "epoch": 0.2067363530778165,
      "grad_norm": 2.8688130378723145,
      "learning_rate": 2.067363530778165e-06,
      "loss": 1.9912,
      "step": 534
    },
    {
      "epoch": 0.20712349980642664,
      "grad_norm": 2.4124221801757812,
      "learning_rate": 2.0712349980642664e-06,
      "loss": 1.8411,
      "step": 535
    },
    {
      "epoch": 0.20751064653503679,
      "grad_norm": 3.1382622718811035,
      "learning_rate": 2.075106465350368e-06,
      "loss": 1.9271,
      "step": 536
    },
    {
      "epoch": 0.20789779326364694,
      "grad_norm": 2.927093505859375,
      "learning_rate": 2.0789779326364694e-06,
      "loss": 1.9559,
      "step": 537
    },
    {
      "epoch": 0.20828493999225706,
      "grad_norm": 2.526332139968872,
      "learning_rate": 2.082849399922571e-06,
      "loss": 1.902,
      "step": 538
    },
    {
      "epoch": 0.2086720867208672,
      "grad_norm": 2.562246084213257,
      "learning_rate": 2.0867208672086723e-06,
      "loss": 1.9005,
      "step": 539
    },
    {
      "epoch": 0.20905923344947736,
      "grad_norm": 4.358397483825684,
      "learning_rate": 2.090592334494774e-06,
      "loss": 1.9562,
      "step": 540
    },
    {
      "epoch": 0.2094463801780875,
      "grad_norm": 3.723266839981079,
      "learning_rate": 2.0944638017808753e-06,
      "loss": 1.8856,
      "step": 541
    },
    {
      "epoch": 0.20983352690669763,
      "grad_norm": 3.3886048793792725,
      "learning_rate": 2.0983352690669764e-06,
      "loss": 2.002,
      "step": 542
    },
    {
      "epoch": 0.21022067363530778,
      "grad_norm": 2.269824743270874,
      "learning_rate": 2.102206736353078e-06,
      "loss": 1.9524,
      "step": 543
    },
    {
      "epoch": 0.21060782036391792,
      "grad_norm": 2.7479076385498047,
      "learning_rate": 2.1060782036391794e-06,
      "loss": 1.8612,
      "step": 544
    },
    {
      "epoch": 0.21099496709252807,
      "grad_norm": 2.758011817932129,
      "learning_rate": 2.109949670925281e-06,
      "loss": 1.8876,
      "step": 545
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 2.6552813053131104,
      "learning_rate": 2.1138211382113824e-06,
      "loss": 1.974,
      "step": 546
    },
    {
      "epoch": 0.21176926054974834,
      "grad_norm": 3.232708692550659,
      "learning_rate": 2.1176926054974834e-06,
      "loss": 1.9336,
      "step": 547
    },
    {
      "epoch": 0.2121564072783585,
      "grad_norm": 2.7354931831359863,
      "learning_rate": 2.121564072783585e-06,
      "loss": 1.9484,
      "step": 548
    },
    {
      "epoch": 0.21254355400696864,
      "grad_norm": 2.8682477474212646,
      "learning_rate": 2.1254355400696864e-06,
      "loss": 1.9574,
      "step": 549
    },
    {
      "epoch": 0.2129307007355788,
      "grad_norm": 2.6743104457855225,
      "learning_rate": 2.129307007355788e-06,
      "loss": 1.9425,
      "step": 550
    },
    {
      "epoch": 0.21331784746418891,
      "grad_norm": 2.7308857440948486,
      "learning_rate": 2.1331784746418894e-06,
      "loss": 1.9423,
      "step": 551
    },
    {
      "epoch": 0.21370499419279906,
      "grad_norm": 4.331692218780518,
      "learning_rate": 2.137049941927991e-06,
      "loss": 1.8594,
      "step": 552
    },
    {
      "epoch": 0.2140921409214092,
      "grad_norm": 2.5042083263397217,
      "learning_rate": 2.1409214092140924e-06,
      "loss": 1.9768,
      "step": 553
    },
    {
      "epoch": 0.21447928765001936,
      "grad_norm": 2.335200071334839,
      "learning_rate": 2.144792876500194e-06,
      "loss": 1.8498,
      "step": 554
    },
    {
      "epoch": 0.2148664343786295,
      "grad_norm": 2.799895763397217,
      "learning_rate": 2.1486643437862954e-06,
      "loss": 1.8374,
      "step": 555
    },
    {
      "epoch": 0.21525358110723963,
      "grad_norm": 2.5952205657958984,
      "learning_rate": 2.1525358110723965e-06,
      "loss": 1.8815,
      "step": 556
    },
    {
      "epoch": 0.21564072783584978,
      "grad_norm": 3.546342134475708,
      "learning_rate": 2.156407278358498e-06,
      "loss": 1.9167,
      "step": 557
    },
    {
      "epoch": 0.21602787456445993,
      "grad_norm": 2.8184549808502197,
      "learning_rate": 2.1602787456445995e-06,
      "loss": 1.8496,
      "step": 558
    },
    {
      "epoch": 0.21641502129307008,
      "grad_norm": 2.996811866760254,
      "learning_rate": 2.164150212930701e-06,
      "loss": 1.8741,
      "step": 559
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 2.5355756282806396,
      "learning_rate": 2.1680216802168024e-06,
      "loss": 1.8463,
      "step": 560
    },
    {
      "epoch": 0.21718931475029035,
      "grad_norm": 2.502490282058716,
      "learning_rate": 2.1718931475029035e-06,
      "loss": 1.9196,
      "step": 561
    },
    {
      "epoch": 0.2175764614789005,
      "grad_norm": 2.6291375160217285,
      "learning_rate": 2.175764614789005e-06,
      "loss": 1.8721,
      "step": 562
    },
    {
      "epoch": 0.21796360820751065,
      "grad_norm": 1.9296364784240723,
      "learning_rate": 2.1796360820751065e-06,
      "loss": 1.9146,
      "step": 563
    },
    {
      "epoch": 0.2183507549361208,
      "grad_norm": 2.586912155151367,
      "learning_rate": 2.183507549361208e-06,
      "loss": 1.8551,
      "step": 564
    },
    {
      "epoch": 0.21873790166473092,
      "grad_norm": 3.1770145893096924,
      "learning_rate": 2.1873790166473095e-06,
      "loss": 1.9061,
      "step": 565
    },
    {
      "epoch": 0.21912504839334107,
      "grad_norm": 2.5562517642974854,
      "learning_rate": 2.191250483933411e-06,
      "loss": 1.9023,
      "step": 566
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 3.6946306228637695,
      "learning_rate": 2.1951219512195125e-06,
      "loss": 1.8845,
      "step": 567
    },
    {
      "epoch": 0.21989934185056137,
      "grad_norm": 2.6644959449768066,
      "learning_rate": 2.198993418505614e-06,
      "loss": 1.8888,
      "step": 568
    },
    {
      "epoch": 0.22028648857917152,
      "grad_norm": 3.0481698513031006,
      "learning_rate": 2.2028648857917155e-06,
      "loss": 1.9299,
      "step": 569
    },
    {
      "epoch": 0.22067363530778164,
      "grad_norm": 3.1044862270355225,
      "learning_rate": 2.2067363530778165e-06,
      "loss": 1.9954,
      "step": 570
    },
    {
      "epoch": 0.2210607820363918,
      "grad_norm": 2.7377381324768066,
      "learning_rate": 2.210607820363918e-06,
      "loss": 1.9922,
      "step": 571
    },
    {
      "epoch": 0.22144792876500194,
      "grad_norm": 3.2715463638305664,
      "learning_rate": 2.2144792876500195e-06,
      "loss": 1.8587,
      "step": 572
    },
    {
      "epoch": 0.2218350754936121,
      "grad_norm": 2.9851653575897217,
      "learning_rate": 2.218350754936121e-06,
      "loss": 1.873,
      "step": 573
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 3.0437581539154053,
      "learning_rate": 2.222222222222222e-06,
      "loss": 1.9325,
      "step": 574
    },
    {
      "epoch": 0.22260936895083236,
      "grad_norm": 3.1831600666046143,
      "learning_rate": 2.2260936895083236e-06,
      "loss": 1.8842,
      "step": 575
    },
    {
      "epoch": 0.2229965156794425,
      "grad_norm": 2.4917285442352295,
      "learning_rate": 2.229965156794425e-06,
      "loss": 1.8764,
      "step": 576
    },
    {
      "epoch": 0.22338366240805266,
      "grad_norm": 3.124706745147705,
      "learning_rate": 2.2338366240805266e-06,
      "loss": 1.8457,
      "step": 577
    },
    {
      "epoch": 0.2237708091366628,
      "grad_norm": 2.5710504055023193,
      "learning_rate": 2.237708091366628e-06,
      "loss": 1.867,
      "step": 578
    },
    {
      "epoch": 0.22415795586527293,
      "grad_norm": 3.237506628036499,
      "learning_rate": 2.2415795586527295e-06,
      "loss": 2.0212,
      "step": 579
    },
    {
      "epoch": 0.22454510259388308,
      "grad_norm": 3.0819568634033203,
      "learning_rate": 2.245451025938831e-06,
      "loss": 1.9654,
      "step": 580
    },
    {
      "epoch": 0.22493224932249323,
      "grad_norm": 3.028522253036499,
      "learning_rate": 2.2493224932249325e-06,
      "loss": 1.9505,
      "step": 581
    },
    {
      "epoch": 0.22531939605110338,
      "grad_norm": 3.6128528118133545,
      "learning_rate": 2.253193960511034e-06,
      "loss": 1.8228,
      "step": 582
    },
    {
      "epoch": 0.2257065427797135,
      "grad_norm": 3.3251895904541016,
      "learning_rate": 2.257065427797135e-06,
      "loss": 1.8851,
      "step": 583
    },
    {
      "epoch": 0.22609368950832365,
      "grad_norm": 2.2347934246063232,
      "learning_rate": 2.2609368950832366e-06,
      "loss": 1.9214,
      "step": 584
    },
    {
      "epoch": 0.2264808362369338,
      "grad_norm": 4.391674518585205,
      "learning_rate": 2.264808362369338e-06,
      "loss": 1.9317,
      "step": 585
    },
    {
      "epoch": 0.22686798296554395,
      "grad_norm": 3.3873775005340576,
      "learning_rate": 2.2686798296554396e-06,
      "loss": 1.8221,
      "step": 586
    },
    {
      "epoch": 0.2272551296941541,
      "grad_norm": 3.1601979732513428,
      "learning_rate": 2.272551296941541e-06,
      "loss": 1.8219,
      "step": 587
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 3.4796934127807617,
      "learning_rate": 2.2764227642276426e-06,
      "loss": 1.87,
      "step": 588
    },
    {
      "epoch": 0.22802942315137437,
      "grad_norm": 2.8448221683502197,
      "learning_rate": 2.280294231513744e-06,
      "loss": 1.969,
      "step": 589
    },
    {
      "epoch": 0.22841656987998452,
      "grad_norm": 3.1195778846740723,
      "learning_rate": 2.2841656987998455e-06,
      "loss": 1.9873,
      "step": 590
    },
    {
      "epoch": 0.22880371660859466,
      "grad_norm": 5.096436023712158,
      "learning_rate": 2.288037166085947e-06,
      "loss": 1.9543,
      "step": 591
    },
    {
      "epoch": 0.22919086333720481,
      "grad_norm": 2.8748507499694824,
      "learning_rate": 2.2919086333720485e-06,
      "loss": 1.9509,
      "step": 592
    },
    {
      "epoch": 0.22957801006581494,
      "grad_norm": 2.957690477371216,
      "learning_rate": 2.2957801006581496e-06,
      "loss": 1.8318,
      "step": 593
    },
    {
      "epoch": 0.22996515679442509,
      "grad_norm": 3.318359613418579,
      "learning_rate": 2.299651567944251e-06,
      "loss": 1.906,
      "step": 594
    },
    {
      "epoch": 0.23035230352303523,
      "grad_norm": 3.4155304431915283,
      "learning_rate": 2.3035230352303526e-06,
      "loss": 1.9993,
      "step": 595
    },
    {
      "epoch": 0.23073945025164538,
      "grad_norm": 3.2739977836608887,
      "learning_rate": 2.307394502516454e-06,
      "loss": 1.9274,
      "step": 596
    },
    {
      "epoch": 0.2311265969802555,
      "grad_norm": 3.0603384971618652,
      "learning_rate": 2.311265969802555e-06,
      "loss": 1.8668,
      "step": 597
    },
    {
      "epoch": 0.23151374370886565,
      "grad_norm": 3.2655792236328125,
      "learning_rate": 2.3151374370886566e-06,
      "loss": 1.997,
      "step": 598
    },
    {
      "epoch": 0.2319008904374758,
      "grad_norm": 2.7068700790405273,
      "learning_rate": 2.319008904374758e-06,
      "loss": 1.9316,
      "step": 599
    },
    {
      "epoch": 0.23228803716608595,
      "grad_norm": 2.8865816593170166,
      "learning_rate": 2.3228803716608596e-06,
      "loss": 1.8957,
      "step": 600
    },
    {
      "epoch": 0.2326751838946961,
      "grad_norm": 2.599088668823242,
      "learning_rate": 2.326751838946961e-06,
      "loss": 1.9323,
      "step": 601
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 3.4828672409057617,
      "learning_rate": 2.3306233062330626e-06,
      "loss": 1.9538,
      "step": 602
    },
    {
      "epoch": 0.23344947735191637,
      "grad_norm": 4.671082973480225,
      "learning_rate": 2.334494773519164e-06,
      "loss": 1.8551,
      "step": 603
    },
    {
      "epoch": 0.23383662408052652,
      "grad_norm": 3.0255954265594482,
      "learning_rate": 2.3383662408052656e-06,
      "loss": 1.892,
      "step": 604
    },
    {
      "epoch": 0.23422377080913667,
      "grad_norm": 3.1279845237731934,
      "learning_rate": 2.342237708091367e-06,
      "loss": 1.8258,
      "step": 605
    },
    {
      "epoch": 0.2346109175377468,
      "grad_norm": 3.1772384643554688,
      "learning_rate": 2.346109175377468e-06,
      "loss": 1.8204,
      "step": 606
    },
    {
      "epoch": 0.23499806426635694,
      "grad_norm": 3.6955058574676514,
      "learning_rate": 2.3499806426635697e-06,
      "loss": 1.9051,
      "step": 607
    },
    {
      "epoch": 0.2353852109949671,
      "grad_norm": 3.1887357234954834,
      "learning_rate": 2.353852109949671e-06,
      "loss": 1.8205,
      "step": 608
    },
    {
      "epoch": 0.23577235772357724,
      "grad_norm": 3.2619283199310303,
      "learning_rate": 2.3577235772357727e-06,
      "loss": 2.001,
      "step": 609
    },
    {
      "epoch": 0.2361595044521874,
      "grad_norm": 3.414287805557251,
      "learning_rate": 2.361595044521874e-06,
      "loss": 1.8904,
      "step": 610
    },
    {
      "epoch": 0.2365466511807975,
      "grad_norm": 3.365271806716919,
      "learning_rate": 2.3654665118079752e-06,
      "loss": 1.9885,
      "step": 611
    },
    {
      "epoch": 0.23693379790940766,
      "grad_norm": 2.9154138565063477,
      "learning_rate": 2.3693379790940767e-06,
      "loss": 1.8776,
      "step": 612
    },
    {
      "epoch": 0.2373209446380178,
      "grad_norm": 3.950515031814575,
      "learning_rate": 2.373209446380178e-06,
      "loss": 1.8804,
      "step": 613
    },
    {
      "epoch": 0.23770809136662796,
      "grad_norm": 3.3670907020568848,
      "learning_rate": 2.3770809136662797e-06,
      "loss": 1.896,
      "step": 614
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 3.382739543914795,
      "learning_rate": 2.380952380952381e-06,
      "loss": 1.8724,
      "step": 615
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 3.3065719604492188,
      "learning_rate": 2.3848238482384827e-06,
      "loss": 1.9434,
      "step": 616
    },
    {
      "epoch": 0.23886953155245838,
      "grad_norm": 3.561591148376465,
      "learning_rate": 2.388695315524584e-06,
      "loss": 1.8742,
      "step": 617
    },
    {
      "epoch": 0.23925667828106853,
      "grad_norm": 2.970308780670166,
      "learning_rate": 2.3925667828106857e-06,
      "loss": 1.9409,
      "step": 618
    },
    {
      "epoch": 0.23964382500967868,
      "grad_norm": 2.8836848735809326,
      "learning_rate": 2.396438250096787e-06,
      "loss": 1.8374,
      "step": 619
    },
    {
      "epoch": 0.2400309717382888,
      "grad_norm": 3.968465566635132,
      "learning_rate": 2.4003097173828882e-06,
      "loss": 1.8975,
      "step": 620
    },
    {
      "epoch": 0.24041811846689895,
      "grad_norm": 3.4351775646209717,
      "learning_rate": 2.4041811846689897e-06,
      "loss": 1.922,
      "step": 621
    },
    {
      "epoch": 0.2408052651955091,
      "grad_norm": 3.2049365043640137,
      "learning_rate": 2.4080526519550912e-06,
      "loss": 1.8614,
      "step": 622
    },
    {
      "epoch": 0.24119241192411925,
      "grad_norm": 3.821133852005005,
      "learning_rate": 2.4119241192411927e-06,
      "loss": 1.9453,
      "step": 623
    },
    {
      "epoch": 0.2415795586527294,
      "grad_norm": 3.1494576930999756,
      "learning_rate": 2.415795586527294e-06,
      "loss": 1.9489,
      "step": 624
    },
    {
      "epoch": 0.24196670538133952,
      "grad_norm": 5.582937240600586,
      "learning_rate": 2.4196670538133953e-06,
      "loss": 1.9362,
      "step": 625
    },
    {
      "epoch": 0.24235385210994967,
      "grad_norm": 3.1615118980407715,
      "learning_rate": 2.4235385210994968e-06,
      "loss": 1.9177,
      "step": 626
    },
    {
      "epoch": 0.24274099883855982,
      "grad_norm": 4.917486667633057,
      "learning_rate": 2.4274099883855983e-06,
      "loss": 1.9665,
      "step": 627
    },
    {
      "epoch": 0.24312814556716997,
      "grad_norm": 2.617466688156128,
      "learning_rate": 2.4312814556716998e-06,
      "loss": 1.9039,
      "step": 628
    },
    {
      "epoch": 0.2435152922957801,
      "grad_norm": 3.5294570922851562,
      "learning_rate": 2.4351529229578012e-06,
      "loss": 1.7998,
      "step": 629
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 3.3786113262176514,
      "learning_rate": 2.4390243902439027e-06,
      "loss": 1.8826,
      "step": 630
    },
    {
      "epoch": 0.2442895857530004,
      "grad_norm": 3.3518545627593994,
      "learning_rate": 2.4428958575300042e-06,
      "loss": 1.9907,
      "step": 631
    },
    {
      "epoch": 0.24467673248161054,
      "grad_norm": 3.9130935668945312,
      "learning_rate": 2.4467673248161057e-06,
      "loss": 1.9671,
      "step": 632
    },
    {
      "epoch": 0.2450638792102207,
      "grad_norm": 2.4000244140625,
      "learning_rate": 2.4506387921022072e-06,
      "loss": 1.9032,
      "step": 633
    },
    {
      "epoch": 0.2454510259388308,
      "grad_norm": 3.740856885910034,
      "learning_rate": 2.4545102593883083e-06,
      "loss": 1.9897,
      "step": 634
    },
    {
      "epoch": 0.24583817266744096,
      "grad_norm": 5.551656723022461,
      "learning_rate": 2.4583817266744098e-06,
      "loss": 2.0685,
      "step": 635
    },
    {
      "epoch": 0.2462253193960511,
      "grad_norm": 5.3994293212890625,
      "learning_rate": 2.4622531939605113e-06,
      "loss": 2.0431,
      "step": 636
    },
    {
      "epoch": 0.24661246612466126,
      "grad_norm": 3.578625202178955,
      "learning_rate": 2.4661246612466128e-06,
      "loss": 1.9106,
      "step": 637
    },
    {
      "epoch": 0.24699961285327138,
      "grad_norm": 3.429713010787964,
      "learning_rate": 2.469996128532714e-06,
      "loss": 2.009,
      "step": 638
    },
    {
      "epoch": 0.24738675958188153,
      "grad_norm": 3.784666061401367,
      "learning_rate": 2.4738675958188153e-06,
      "loss": 1.7985,
      "step": 639
    },
    {
      "epoch": 0.24777390631049168,
      "grad_norm": 3.3645927906036377,
      "learning_rate": 2.477739063104917e-06,
      "loss": 1.9087,
      "step": 640
    },
    {
      "epoch": 0.24816105303910183,
      "grad_norm": 3.9199585914611816,
      "learning_rate": 2.4816105303910183e-06,
      "loss": 1.8845,
      "step": 641
    },
    {
      "epoch": 0.24854819976771197,
      "grad_norm": 6.136801719665527,
      "learning_rate": 2.48548199767712e-06,
      "loss": 1.7943,
      "step": 642
    },
    {
      "epoch": 0.2489353464963221,
      "grad_norm": 3.523496389389038,
      "learning_rate": 2.4893534649632213e-06,
      "loss": 1.9407,
      "step": 643
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 3.8842272758483887,
      "learning_rate": 2.493224932249323e-06,
      "loss": 1.8769,
      "step": 644
    },
    {
      "epoch": 0.2497096399535424,
      "grad_norm": 4.059936046600342,
      "learning_rate": 2.4970963995354243e-06,
      "loss": 1.861,
      "step": 645
    },
    {
      "epoch": 0.2500967866821525,
      "grad_norm": 3.360710859298706,
      "learning_rate": 2.5009678668215254e-06,
      "loss": 1.8916,
      "step": 646
    },
    {
      "epoch": 0.2504839334107627,
      "grad_norm": 3.1754322052001953,
      "learning_rate": 2.5048393341076273e-06,
      "loss": 1.8353,
      "step": 647
    },
    {
      "epoch": 0.2508710801393728,
      "grad_norm": 3.7455263137817383,
      "learning_rate": 2.5087108013937284e-06,
      "loss": 1.8163,
      "step": 648
    },
    {
      "epoch": 0.251258226867983,
      "grad_norm": 3.599885940551758,
      "learning_rate": 2.5125822686798303e-06,
      "loss": 1.8072,
      "step": 649
    },
    {
      "epoch": 0.2516453735965931,
      "grad_norm": 4.261420726776123,
      "learning_rate": 2.5164537359659313e-06,
      "loss": 1.8518,
      "step": 650
    },
    {
      "epoch": 0.25203252032520324,
      "grad_norm": 3.8999245166778564,
      "learning_rate": 2.5203252032520324e-06,
      "loss": 1.9896,
      "step": 651
    },
    {
      "epoch": 0.2524196670538134,
      "grad_norm": 3.0409655570983887,
      "learning_rate": 2.5241966705381343e-06,
      "loss": 1.8783,
      "step": 652
    },
    {
      "epoch": 0.25280681378242353,
      "grad_norm": 3.44474720954895,
      "learning_rate": 2.5280681378242354e-06,
      "loss": 1.9036,
      "step": 653
    },
    {
      "epoch": 0.25319396051103366,
      "grad_norm": 3.290738582611084,
      "learning_rate": 2.531939605110337e-06,
      "loss": 1.8864,
      "step": 654
    },
    {
      "epoch": 0.25358110723964383,
      "grad_norm": 3.4059650897979736,
      "learning_rate": 2.5358110723964384e-06,
      "loss": 1.933,
      "step": 655
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 6.91644811630249,
      "learning_rate": 2.53968253968254e-06,
      "loss": 2.1221,
      "step": 656
    },
    {
      "epoch": 0.25435540069686413,
      "grad_norm": 4.318762302398682,
      "learning_rate": 2.5435540069686414e-06,
      "loss": 1.8058,
      "step": 657
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 6.063329696655273,
      "learning_rate": 2.547425474254743e-06,
      "loss": 1.8173,
      "step": 658
    },
    {
      "epoch": 0.2551296941540844,
      "grad_norm": 4.607699871063232,
      "learning_rate": 2.551296941540844e-06,
      "loss": 1.8468,
      "step": 659
    },
    {
      "epoch": 0.25551684088269455,
      "grad_norm": 4.768526077270508,
      "learning_rate": 2.555168408826946e-06,
      "loss": 1.9442,
      "step": 660
    },
    {
      "epoch": 0.2559039876113047,
      "grad_norm": 4.362678050994873,
      "learning_rate": 2.559039876113047e-06,
      "loss": 2.0183,
      "step": 661
    },
    {
      "epoch": 0.25629113433991485,
      "grad_norm": 6.192969799041748,
      "learning_rate": 2.562911343399149e-06,
      "loss": 1.7825,
      "step": 662
    },
    {
      "epoch": 0.25667828106852497,
      "grad_norm": 6.307344913482666,
      "learning_rate": 2.56678281068525e-06,
      "loss": 1.7833,
      "step": 663
    },
    {
      "epoch": 0.2570654277971351,
      "grad_norm": 4.242782115936279,
      "learning_rate": 2.570654277971351e-06,
      "loss": 1.8815,
      "step": 664
    },
    {
      "epoch": 0.25745257452574527,
      "grad_norm": 3.866044521331787,
      "learning_rate": 2.574525745257453e-06,
      "loss": 1.9273,
      "step": 665
    },
    {
      "epoch": 0.2578397212543554,
      "grad_norm": 4.032533168792725,
      "learning_rate": 2.578397212543554e-06,
      "loss": 2.0077,
      "step": 666
    },
    {
      "epoch": 0.25822686798296557,
      "grad_norm": 4.543755054473877,
      "learning_rate": 2.582268679829656e-06,
      "loss": 1.7515,
      "step": 667
    },
    {
      "epoch": 0.2586140147115757,
      "grad_norm": 4.582633972167969,
      "learning_rate": 2.586140147115757e-06,
      "loss": 1.9885,
      "step": 668
    },
    {
      "epoch": 0.2590011614401858,
      "grad_norm": 3.1893250942230225,
      "learning_rate": 2.5900116144018584e-06,
      "loss": 1.8189,
      "step": 669
    },
    {
      "epoch": 0.259388308168796,
      "grad_norm": 4.60728120803833,
      "learning_rate": 2.59388308168796e-06,
      "loss": 1.8458,
      "step": 670
    },
    {
      "epoch": 0.2597754548974061,
      "grad_norm": 4.773448944091797,
      "learning_rate": 2.5977545489740614e-06,
      "loss": 1.8498,
      "step": 671
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 4.272185325622559,
      "learning_rate": 2.601626016260163e-06,
      "loss": 1.7841,
      "step": 672
    },
    {
      "epoch": 0.2605497483546264,
      "grad_norm": 4.759449005126953,
      "learning_rate": 2.6054974835462644e-06,
      "loss": 1.7508,
      "step": 673
    },
    {
      "epoch": 0.26093689508323653,
      "grad_norm": 5.055673599243164,
      "learning_rate": 2.6093689508323655e-06,
      "loss": 2.0055,
      "step": 674
    },
    {
      "epoch": 0.2613240418118467,
      "grad_norm": 6.391668319702148,
      "learning_rate": 2.6132404181184674e-06,
      "loss": 1.7855,
      "step": 675
    },
    {
      "epoch": 0.26171118854045683,
      "grad_norm": 4.756046772003174,
      "learning_rate": 2.6171118854045685e-06,
      "loss": 1.7472,
      "step": 676
    },
    {
      "epoch": 0.26209833526906695,
      "grad_norm": 5.495229244232178,
      "learning_rate": 2.6209833526906695e-06,
      "loss": 2.0584,
      "step": 677
    },
    {
      "epoch": 0.26248548199767713,
      "grad_norm": 5.0100860595703125,
      "learning_rate": 2.6248548199767715e-06,
      "loss": 1.8883,
      "step": 678
    },
    {
      "epoch": 0.26287262872628725,
      "grad_norm": 5.162625312805176,
      "learning_rate": 2.6287262872628725e-06,
      "loss": 1.9899,
      "step": 679
    },
    {
      "epoch": 0.2632597754548974,
      "grad_norm": 3.8375680446624756,
      "learning_rate": 2.6325977545489744e-06,
      "loss": 1.8749,
      "step": 680
    },
    {
      "epoch": 0.26364692218350755,
      "grad_norm": 3.909369707107544,
      "learning_rate": 2.6364692218350755e-06,
      "loss": 1.9139,
      "step": 681
    },
    {
      "epoch": 0.26403406891211767,
      "grad_norm": 5.063611030578613,
      "learning_rate": 2.640340689121177e-06,
      "loss": 2.0069,
      "step": 682
    },
    {
      "epoch": 0.26442121564072785,
      "grad_norm": 4.642796993255615,
      "learning_rate": 2.6442121564072785e-06,
      "loss": 2.0197,
      "step": 683
    },
    {
      "epoch": 0.26480836236933797,
      "grad_norm": 6.2610697746276855,
      "learning_rate": 2.64808362369338e-06,
      "loss": 2.0715,
      "step": 684
    },
    {
      "epoch": 0.26519550909794815,
      "grad_norm": 5.662660598754883,
      "learning_rate": 2.6519550909794815e-06,
      "loss": 1.9479,
      "step": 685
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 4.154530048370361,
      "learning_rate": 2.655826558265583e-06,
      "loss": 1.9376,
      "step": 686
    },
    {
      "epoch": 0.2659698025551684,
      "grad_norm": 4.834386348724365,
      "learning_rate": 2.659698025551684e-06,
      "loss": 1.8541,
      "step": 687
    },
    {
      "epoch": 0.26635694928377857,
      "grad_norm": 3.6495752334594727,
      "learning_rate": 2.663569492837786e-06,
      "loss": 1.896,
      "step": 688
    },
    {
      "epoch": 0.2667440960123887,
      "grad_norm": 4.754018783569336,
      "learning_rate": 2.667440960123887e-06,
      "loss": 2.0248,
      "step": 689
    },
    {
      "epoch": 0.26713124274099886,
      "grad_norm": 4.64987850189209,
      "learning_rate": 2.671312427409989e-06,
      "loss": 1.9435,
      "step": 690
    },
    {
      "epoch": 0.267518389469609,
      "grad_norm": 7.222677230834961,
      "learning_rate": 2.67518389469609e-06,
      "loss": 1.8265,
      "step": 691
    },
    {
      "epoch": 0.2679055361982191,
      "grad_norm": 4.4423933029174805,
      "learning_rate": 2.679055361982191e-06,
      "loss": 2.0162,
      "step": 692
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 7.022895812988281,
      "learning_rate": 2.682926829268293e-06,
      "loss": 1.7694,
      "step": 693
    },
    {
      "epoch": 0.2686798296554394,
      "grad_norm": 4.531471252441406,
      "learning_rate": 2.686798296554394e-06,
      "loss": 1.7688,
      "step": 694
    },
    {
      "epoch": 0.2690669763840495,
      "grad_norm": 5.1545209884643555,
      "learning_rate": 2.6906697638404956e-06,
      "loss": 1.8752,
      "step": 695
    },
    {
      "epoch": 0.2694541231126597,
      "grad_norm": 5.3327412605285645,
      "learning_rate": 2.694541231126597e-06,
      "loss": 2.0328,
      "step": 696
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 4.146062850952148,
      "learning_rate": 2.6984126984126986e-06,
      "loss": 1.9438,
      "step": 697
    },
    {
      "epoch": 0.27022841656988,
      "grad_norm": 4.124353408813477,
      "learning_rate": 2.7022841656988e-06,
      "loss": 1.7708,
      "step": 698
    },
    {
      "epoch": 0.2706155632984901,
      "grad_norm": 4.504242420196533,
      "learning_rate": 2.7061556329849016e-06,
      "loss": 1.9064,
      "step": 699
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 7.229456424713135,
      "learning_rate": 2.7100271002710026e-06,
      "loss": 1.7583,
      "step": 700
    },
    {
      "epoch": 0.2713898567557104,
      "grad_norm": 4.642282485961914,
      "learning_rate": 2.7138985675571045e-06,
      "loss": 1.8314,
      "step": 701
    },
    {
      "epoch": 0.27177700348432055,
      "grad_norm": 5.135335445404053,
      "learning_rate": 2.7177700348432056e-06,
      "loss": 1.9238,
      "step": 702
    },
    {
      "epoch": 0.2721641502129307,
      "grad_norm": 2.8204410076141357,
      "learning_rate": 2.7216415021293075e-06,
      "loss": 1.8897,
      "step": 703
    },
    {
      "epoch": 0.27255129694154084,
      "grad_norm": 6.136299133300781,
      "learning_rate": 2.7255129694154086e-06,
      "loss": 1.9242,
      "step": 704
    },
    {
      "epoch": 0.27293844367015097,
      "grad_norm": 7.137617111206055,
      "learning_rate": 2.7293844367015097e-06,
      "loss": 1.787,
      "step": 705
    },
    {
      "epoch": 0.27332559039876114,
      "grad_norm": 3.5939252376556396,
      "learning_rate": 2.7332559039876116e-06,
      "loss": 1.8043,
      "step": 706
    },
    {
      "epoch": 0.27371273712737126,
      "grad_norm": 4.38499116897583,
      "learning_rate": 2.7371273712737127e-06,
      "loss": 1.858,
      "step": 707
    },
    {
      "epoch": 0.27409988385598144,
      "grad_norm": 4.360669136047363,
      "learning_rate": 2.7409988385598146e-06,
      "loss": 1.8495,
      "step": 708
    },
    {
      "epoch": 0.27448703058459156,
      "grad_norm": 4.928162097930908,
      "learning_rate": 2.7448703058459156e-06,
      "loss": 1.7899,
      "step": 709
    },
    {
      "epoch": 0.2748741773132017,
      "grad_norm": 5.019421577453613,
      "learning_rate": 2.748741773132017e-06,
      "loss": 1.852,
      "step": 710
    },
    {
      "epoch": 0.27526132404181186,
      "grad_norm": 4.517184257507324,
      "learning_rate": 2.7526132404181186e-06,
      "loss": 1.9816,
      "step": 711
    },
    {
      "epoch": 0.275648470770422,
      "grad_norm": 4.655425071716309,
      "learning_rate": 2.75648470770422e-06,
      "loss": 1.7883,
      "step": 712
    },
    {
      "epoch": 0.27603561749903216,
      "grad_norm": 5.299063682556152,
      "learning_rate": 2.7603561749903216e-06,
      "loss": 1.9563,
      "step": 713
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 3.487900495529175,
      "learning_rate": 2.764227642276423e-06,
      "loss": 1.8027,
      "step": 714
    },
    {
      "epoch": 0.2768099109562524,
      "grad_norm": 4.364028453826904,
      "learning_rate": 2.768099109562524e-06,
      "loss": 1.9442,
      "step": 715
    },
    {
      "epoch": 0.2771970576848626,
      "grad_norm": 5.812967300415039,
      "learning_rate": 2.771970576848626e-06,
      "loss": 1.8083,
      "step": 716
    },
    {
      "epoch": 0.2775842044134727,
      "grad_norm": 4.044814109802246,
      "learning_rate": 2.775842044134727e-06,
      "loss": 1.9015,
      "step": 717
    },
    {
      "epoch": 0.2779713511420828,
      "grad_norm": 3.7721736431121826,
      "learning_rate": 2.7797135114208287e-06,
      "loss": 1.8481,
      "step": 718
    },
    {
      "epoch": 0.278358497870693,
      "grad_norm": 4.931928634643555,
      "learning_rate": 2.78358497870693e-06,
      "loss": 1.7782,
      "step": 719
    },
    {
      "epoch": 0.2787456445993031,
      "grad_norm": 5.657633304595947,
      "learning_rate": 2.7874564459930316e-06,
      "loss": 1.8797,
      "step": 720
    },
    {
      "epoch": 0.2791327913279133,
      "grad_norm": 5.048424243927002,
      "learning_rate": 2.791327913279133e-06,
      "loss": 1.725,
      "step": 721
    },
    {
      "epoch": 0.2795199380565234,
      "grad_norm": 6.106997966766357,
      "learning_rate": 2.7951993805652346e-06,
      "loss": 1.8718,
      "step": 722
    },
    {
      "epoch": 0.27990708478513354,
      "grad_norm": 7.286074161529541,
      "learning_rate": 2.7990708478513357e-06,
      "loss": 1.7517,
      "step": 723
    },
    {
      "epoch": 0.2802942315137437,
      "grad_norm": 5.362709045410156,
      "learning_rate": 2.8029423151374376e-06,
      "loss": 1.7559,
      "step": 724
    },
    {
      "epoch": 0.28068137824235384,
      "grad_norm": 4.095361709594727,
      "learning_rate": 2.8068137824235387e-06,
      "loss": 1.8369,
      "step": 725
    },
    {
      "epoch": 0.281068524970964,
      "grad_norm": 4.322763919830322,
      "learning_rate": 2.8106852497096406e-06,
      "loss": 1.882,
      "step": 726
    },
    {
      "epoch": 0.28145567169957414,
      "grad_norm": 6.495409965515137,
      "learning_rate": 2.8145567169957417e-06,
      "loss": 2.022,
      "step": 727
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 6.4468183517456055,
      "learning_rate": 2.8184281842818427e-06,
      "loss": 2.1031,
      "step": 728
    },
    {
      "epoch": 0.28222996515679444,
      "grad_norm": 4.456118583679199,
      "learning_rate": 2.8222996515679447e-06,
      "loss": 1.9985,
      "step": 729
    },
    {
      "epoch": 0.28261711188540456,
      "grad_norm": 6.888878345489502,
      "learning_rate": 2.8261711188540457e-06,
      "loss": 1.9374,
      "step": 730
    },
    {
      "epoch": 0.28300425861401474,
      "grad_norm": 7.9537153244018555,
      "learning_rate": 2.8300425861401476e-06,
      "loss": 1.8008,
      "step": 731
    },
    {
      "epoch": 0.28339140534262486,
      "grad_norm": 5.529542446136475,
      "learning_rate": 2.8339140534262487e-06,
      "loss": 1.9633,
      "step": 732
    },
    {
      "epoch": 0.283778552071235,
      "grad_norm": 6.911525249481201,
      "learning_rate": 2.8377855207123502e-06,
      "loss": 1.7761,
      "step": 733
    },
    {
      "epoch": 0.28416569879984516,
      "grad_norm": 5.577544212341309,
      "learning_rate": 2.8416569879984517e-06,
      "loss": 1.884,
      "step": 734
    },
    {
      "epoch": 0.2845528455284553,
      "grad_norm": 5.366798400878906,
      "learning_rate": 2.845528455284553e-06,
      "loss": 2.029,
      "step": 735
    },
    {
      "epoch": 0.28493999225706546,
      "grad_norm": 9.168179512023926,
      "learning_rate": 2.8493999225706547e-06,
      "loss": 1.9563,
      "step": 736
    },
    {
      "epoch": 0.2853271389856756,
      "grad_norm": 4.902073383331299,
      "learning_rate": 2.853271389856756e-06,
      "loss": 1.984,
      "step": 737
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.704491138458252,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.7238,
      "step": 738
    },
    {
      "epoch": 0.2861014324428959,
      "grad_norm": 3.4613540172576904,
      "learning_rate": 2.861014324428959e-06,
      "loss": 1.9359,
      "step": 739
    },
    {
      "epoch": 0.286488579171506,
      "grad_norm": 5.451022148132324,
      "learning_rate": 2.8648857917150602e-06,
      "loss": 1.9373,
      "step": 740
    },
    {
      "epoch": 0.2868757259001161,
      "grad_norm": 5.204198837280273,
      "learning_rate": 2.8687572590011613e-06,
      "loss": 1.7742,
      "step": 741
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 5.681777000427246,
      "learning_rate": 2.8726287262872632e-06,
      "loss": 1.8316,
      "step": 742
    },
    {
      "epoch": 0.2876500193573364,
      "grad_norm": 4.054718494415283,
      "learning_rate": 2.8765001935733643e-06,
      "loss": 1.8907,
      "step": 743
    },
    {
      "epoch": 0.2880371660859466,
      "grad_norm": 5.634524822235107,
      "learning_rate": 2.8803716608594662e-06,
      "loss": 1.7384,
      "step": 744
    },
    {
      "epoch": 0.2884243128145567,
      "grad_norm": 7.47828483581543,
      "learning_rate": 2.8842431281455673e-06,
      "loss": 1.7447,
      "step": 745
    },
    {
      "epoch": 0.28881145954316684,
      "grad_norm": 4.801217079162598,
      "learning_rate": 2.8881145954316688e-06,
      "loss": 1.8459,
      "step": 746
    },
    {
      "epoch": 0.289198606271777,
      "grad_norm": 4.754260540008545,
      "learning_rate": 2.8919860627177703e-06,
      "loss": 1.8811,
      "step": 747
    },
    {
      "epoch": 0.28958575300038714,
      "grad_norm": 4.147353649139404,
      "learning_rate": 2.8958575300038718e-06,
      "loss": 1.875,
      "step": 748
    },
    {
      "epoch": 0.2899728997289973,
      "grad_norm": 4.6981000900268555,
      "learning_rate": 2.8997289972899733e-06,
      "loss": 1.8323,
      "step": 749
    },
    {
      "epoch": 0.29036004645760743,
      "grad_norm": 5.474301815032959,
      "learning_rate": 2.9036004645760748e-06,
      "loss": 1.8334,
      "step": 750
    },
    {
      "epoch": 0.29074719318621756,
      "grad_norm": 5.662237167358398,
      "learning_rate": 2.907471931862176e-06,
      "loss": 1.8387,
      "step": 751
    },
    {
      "epoch": 0.29113433991482773,
      "grad_norm": 4.950139999389648,
      "learning_rate": 2.9113433991482777e-06,
      "loss": 1.8388,
      "step": 752
    },
    {
      "epoch": 0.29152148664343785,
      "grad_norm": 5.912869930267334,
      "learning_rate": 2.915214866434379e-06,
      "loss": 2.0337,
      "step": 753
    },
    {
      "epoch": 0.29190863337204803,
      "grad_norm": 5.617253303527832,
      "learning_rate": 2.9190863337204807e-06,
      "loss": 2.0504,
      "step": 754
    },
    {
      "epoch": 0.29229578010065815,
      "grad_norm": 6.655825614929199,
      "learning_rate": 2.922957801006582e-06,
      "loss": 2.0543,
      "step": 755
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 4.767932415008545,
      "learning_rate": 2.926829268292683e-06,
      "loss": 1.8532,
      "step": 756
    },
    {
      "epoch": 0.29307007355787845,
      "grad_norm": 4.485767364501953,
      "learning_rate": 2.9307007355787848e-06,
      "loss": 1.798,
      "step": 757
    },
    {
      "epoch": 0.2934572202864886,
      "grad_norm": 4.872833251953125,
      "learning_rate": 2.934572202864886e-06,
      "loss": 1.8822,
      "step": 758
    },
    {
      "epoch": 0.2938443670150987,
      "grad_norm": 5.726745128631592,
      "learning_rate": 2.9384436701509873e-06,
      "loss": 1.8422,
      "step": 759
    },
    {
      "epoch": 0.2942315137437089,
      "grad_norm": 5.475102424621582,
      "learning_rate": 2.942315137437089e-06,
      "loss": 1.8736,
      "step": 760
    },
    {
      "epoch": 0.294618660472319,
      "grad_norm": 4.796927452087402,
      "learning_rate": 2.9461866047231903e-06,
      "loss": 1.8309,
      "step": 761
    },
    {
      "epoch": 0.29500580720092917,
      "grad_norm": 4.532254695892334,
      "learning_rate": 2.950058072009292e-06,
      "loss": 1.8391,
      "step": 762
    },
    {
      "epoch": 0.2953929539295393,
      "grad_norm": 4.085954189300537,
      "learning_rate": 2.9539295392953933e-06,
      "loss": 1.8699,
      "step": 763
    },
    {
      "epoch": 0.2957801006581494,
      "grad_norm": 5.225602626800537,
      "learning_rate": 2.9578010065814944e-06,
      "loss": 1.8301,
      "step": 764
    },
    {
      "epoch": 0.2961672473867596,
      "grad_norm": 5.530776500701904,
      "learning_rate": 2.9616724738675963e-06,
      "loss": 1.8712,
      "step": 765
    },
    {
      "epoch": 0.2965543941153697,
      "grad_norm": 4.6287841796875,
      "learning_rate": 2.9655439411536974e-06,
      "loss": 2.0258,
      "step": 766
    },
    {
      "epoch": 0.2969415408439799,
      "grad_norm": 6.171422004699707,
      "learning_rate": 2.9694154084397993e-06,
      "loss": 2.031,
      "step": 767
    },
    {
      "epoch": 0.29732868757259,
      "grad_norm": 5.255777359008789,
      "learning_rate": 2.9732868757259004e-06,
      "loss": 1.8541,
      "step": 768
    },
    {
      "epoch": 0.29771583430120013,
      "grad_norm": 4.348309516906738,
      "learning_rate": 2.9771583430120014e-06,
      "loss": 1.9124,
      "step": 769
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 5.602612495422363,
      "learning_rate": 2.9810298102981034e-06,
      "loss": 1.8359,
      "step": 770
    },
    {
      "epoch": 0.29849012775842043,
      "grad_norm": 4.787204265594482,
      "learning_rate": 2.9849012775842044e-06,
      "loss": 1.7659,
      "step": 771
    },
    {
      "epoch": 0.2988772744870306,
      "grad_norm": 5.784604072570801,
      "learning_rate": 2.9887727448703063e-06,
      "loss": 1.7458,
      "step": 772
    },
    {
      "epoch": 0.29926442121564073,
      "grad_norm": 7.5158233642578125,
      "learning_rate": 2.9926442121564074e-06,
      "loss": 1.9343,
      "step": 773
    },
    {
      "epoch": 0.29965156794425085,
      "grad_norm": 4.8036603927612305,
      "learning_rate": 2.996515679442509e-06,
      "loss": 1.9368,
      "step": 774
    },
    {
      "epoch": 0.30003871467286103,
      "grad_norm": 4.685634613037109,
      "learning_rate": 3.0003871467286104e-06,
      "loss": 1.7937,
      "step": 775
    },
    {
      "epoch": 0.30042586140147115,
      "grad_norm": 5.344221591949463,
      "learning_rate": 3.004258614014712e-06,
      "loss": 1.9383,
      "step": 776
    },
    {
      "epoch": 0.3008130081300813,
      "grad_norm": 6.42534065246582,
      "learning_rate": 3.0081300813008134e-06,
      "loss": 1.9676,
      "step": 777
    },
    {
      "epoch": 0.30120015485869145,
      "grad_norm": 6.802953720092773,
      "learning_rate": 3.012001548586915e-06,
      "loss": 1.8353,
      "step": 778
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 6.546514511108398,
      "learning_rate": 3.015873015873016e-06,
      "loss": 2.0512,
      "step": 779
    },
    {
      "epoch": 0.30197444831591175,
      "grad_norm": 6.816506862640381,
      "learning_rate": 3.019744483159118e-06,
      "loss": 1.8282,
      "step": 780
    },
    {
      "epoch": 0.30236159504452187,
      "grad_norm": 10.432145118713379,
      "learning_rate": 3.023615950445219e-06,
      "loss": 1.7776,
      "step": 781
    },
    {
      "epoch": 0.302748741773132,
      "grad_norm": 4.962366104125977,
      "learning_rate": 3.02748741773132e-06,
      "loss": 1.9361,
      "step": 782
    },
    {
      "epoch": 0.30313588850174217,
      "grad_norm": 5.64499568939209,
      "learning_rate": 3.031358885017422e-06,
      "loss": 1.9697,
      "step": 783
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 9.46323013305664,
      "learning_rate": 3.035230352303523e-06,
      "loss": 1.9652,
      "step": 784
    },
    {
      "epoch": 0.30391018195896247,
      "grad_norm": 6.745748996734619,
      "learning_rate": 3.039101819589625e-06,
      "loss": 1.7772,
      "step": 785
    },
    {
      "epoch": 0.3042973286875726,
      "grad_norm": 8.02359390258789,
      "learning_rate": 3.042973286875726e-06,
      "loss": 1.7514,
      "step": 786
    },
    {
      "epoch": 0.3046844754161827,
      "grad_norm": 5.675581455230713,
      "learning_rate": 3.0468447541618275e-06,
      "loss": 1.735,
      "step": 787
    },
    {
      "epoch": 0.3050716221447929,
      "grad_norm": 5.587117671966553,
      "learning_rate": 3.050716221447929e-06,
      "loss": 1.7456,
      "step": 788
    },
    {
      "epoch": 0.305458768873403,
      "grad_norm": 5.306910991668701,
      "learning_rate": 3.0545876887340305e-06,
      "loss": 1.988,
      "step": 789
    },
    {
      "epoch": 0.3058459156020132,
      "grad_norm": 4.185196876525879,
      "learning_rate": 3.058459156020132e-06,
      "loss": 1.9034,
      "step": 790
    },
    {
      "epoch": 0.3062330623306233,
      "grad_norm": 6.3735671043396,
      "learning_rate": 3.0623306233062334e-06,
      "loss": 1.8445,
      "step": 791
    },
    {
      "epoch": 0.30662020905923343,
      "grad_norm": 10.124629020690918,
      "learning_rate": 3.0662020905923345e-06,
      "loss": 1.6863,
      "step": 792
    },
    {
      "epoch": 0.3070073557878436,
      "grad_norm": 6.769291877746582,
      "learning_rate": 3.0700735578784364e-06,
      "loss": 2.0166,
      "step": 793
    },
    {
      "epoch": 0.3073945025164537,
      "grad_norm": 5.670444488525391,
      "learning_rate": 3.0739450251645375e-06,
      "loss": 1.8505,
      "step": 794
    },
    {
      "epoch": 0.3077816492450639,
      "grad_norm": 6.0421881675720215,
      "learning_rate": 3.0778164924506394e-06,
      "loss": 1.9222,
      "step": 795
    },
    {
      "epoch": 0.308168795973674,
      "grad_norm": 6.5627570152282715,
      "learning_rate": 3.0816879597367405e-06,
      "loss": 1.8231,
      "step": 796
    },
    {
      "epoch": 0.30855594270228415,
      "grad_norm": 4.4243645668029785,
      "learning_rate": 3.0855594270228416e-06,
      "loss": 1.7902,
      "step": 797
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 4.4525227546691895,
      "learning_rate": 3.0894308943089435e-06,
      "loss": 1.8462,
      "step": 798
    },
    {
      "epoch": 0.30933023615950445,
      "grad_norm": 4.4406514167785645,
      "learning_rate": 3.0933023615950445e-06,
      "loss": 1.8506,
      "step": 799
    },
    {
      "epoch": 0.3097173828881146,
      "grad_norm": 6.788020610809326,
      "learning_rate": 3.0971738288811465e-06,
      "loss": 1.8197,
      "step": 800
    },
    {
      "epoch": 0.31010452961672474,
      "grad_norm": 4.740174770355225,
      "learning_rate": 3.1010452961672475e-06,
      "loss": 1.8689,
      "step": 801
    },
    {
      "epoch": 0.31049167634533487,
      "grad_norm": 5.3511247634887695,
      "learning_rate": 3.104916763453349e-06,
      "loss": 1.8578,
      "step": 802
    },
    {
      "epoch": 0.31087882307394504,
      "grad_norm": 4.1916704177856445,
      "learning_rate": 3.1087882307394505e-06,
      "loss": 1.9015,
      "step": 803
    },
    {
      "epoch": 0.31126596980255516,
      "grad_norm": 5.017251491546631,
      "learning_rate": 3.112659698025552e-06,
      "loss": 1.8689,
      "step": 804
    },
    {
      "epoch": 0.3116531165311653,
      "grad_norm": 4.858780384063721,
      "learning_rate": 3.116531165311653e-06,
      "loss": 1.8752,
      "step": 805
    },
    {
      "epoch": 0.31204026325977546,
      "grad_norm": 7.455528736114502,
      "learning_rate": 3.120402632597755e-06,
      "loss": 2.0778,
      "step": 806
    },
    {
      "epoch": 0.3124274099883856,
      "grad_norm": 6.000278472900391,
      "learning_rate": 3.124274099883856e-06,
      "loss": 1.8841,
      "step": 807
    },
    {
      "epoch": 0.31281455671699576,
      "grad_norm": 6.417995929718018,
      "learning_rate": 3.128145567169958e-06,
      "loss": 1.9384,
      "step": 808
    },
    {
      "epoch": 0.3132017034456059,
      "grad_norm": 5.796879291534424,
      "learning_rate": 3.132017034456059e-06,
      "loss": 1.7489,
      "step": 809
    },
    {
      "epoch": 0.313588850174216,
      "grad_norm": 5.434488296508789,
      "learning_rate": 3.13588850174216e-06,
      "loss": 1.9463,
      "step": 810
    },
    {
      "epoch": 0.3139759969028262,
      "grad_norm": 4.301883697509766,
      "learning_rate": 3.139759969028262e-06,
      "loss": 1.8658,
      "step": 811
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 5.496555328369141,
      "learning_rate": 3.143631436314363e-06,
      "loss": 1.8655,
      "step": 812
    },
    {
      "epoch": 0.3147502903600465,
      "grad_norm": 6.712944507598877,
      "learning_rate": 3.147502903600465e-06,
      "loss": 1.876,
      "step": 813
    },
    {
      "epoch": 0.3151374370886566,
      "grad_norm": 6.929919719696045,
      "learning_rate": 3.151374370886566e-06,
      "loss": 1.942,
      "step": 814
    },
    {
      "epoch": 0.3155245838172667,
      "grad_norm": 7.619915962219238,
      "learning_rate": 3.1552458381726676e-06,
      "loss": 2.0467,
      "step": 815
    },
    {
      "epoch": 0.3159117305458769,
      "grad_norm": 6.238576412200928,
      "learning_rate": 3.159117305458769e-06,
      "loss": 2.0159,
      "step": 816
    },
    {
      "epoch": 0.316298877274487,
      "grad_norm": 6.187562465667725,
      "learning_rate": 3.1629887727448706e-06,
      "loss": 1.8343,
      "step": 817
    },
    {
      "epoch": 0.3166860240030972,
      "grad_norm": 5.7111663818359375,
      "learning_rate": 3.166860240030972e-06,
      "loss": 2.0185,
      "step": 818
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 6.155790328979492,
      "learning_rate": 3.1707317073170736e-06,
      "loss": 1.9357,
      "step": 819
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 6.024950981140137,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 1.8469,
      "step": 820
    },
    {
      "epoch": 0.3178474641889276,
      "grad_norm": 4.766077041625977,
      "learning_rate": 3.1784746418892766e-06,
      "loss": 1.9448,
      "step": 821
    },
    {
      "epoch": 0.31823461091753774,
      "grad_norm": 5.4873504638671875,
      "learning_rate": 3.1823461091753776e-06,
      "loss": 1.7459,
      "step": 822
    },
    {
      "epoch": 0.31862175764614786,
      "grad_norm": 6.2112579345703125,
      "learning_rate": 3.1862175764614787e-06,
      "loss": 1.8522,
      "step": 823
    },
    {
      "epoch": 0.31900890437475804,
      "grad_norm": 5.348487377166748,
      "learning_rate": 3.1900890437475806e-06,
      "loss": 1.8355,
      "step": 824
    },
    {
      "epoch": 0.31939605110336816,
      "grad_norm": 4.652820587158203,
      "learning_rate": 3.1939605110336817e-06,
      "loss": 1.8774,
      "step": 825
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 4.153924942016602,
      "learning_rate": 3.1978319783197836e-06,
      "loss": 1.9156,
      "step": 826
    },
    {
      "epoch": 0.32017034456058846,
      "grad_norm": 4.569142818450928,
      "learning_rate": 3.2017034456058847e-06,
      "loss": 1.7949,
      "step": 827
    },
    {
      "epoch": 0.3205574912891986,
      "grad_norm": 7.637098789215088,
      "learning_rate": 3.205574912891986e-06,
      "loss": 1.9284,
      "step": 828
    },
    {
      "epoch": 0.32094463801780876,
      "grad_norm": 4.78095817565918,
      "learning_rate": 3.2094463801780877e-06,
      "loss": 1.8912,
      "step": 829
    },
    {
      "epoch": 0.3213317847464189,
      "grad_norm": 4.840390205383301,
      "learning_rate": 3.213317847464189e-06,
      "loss": 1.9292,
      "step": 830
    },
    {
      "epoch": 0.32171893147502906,
      "grad_norm": 4.632334232330322,
      "learning_rate": 3.2171893147502906e-06,
      "loss": 1.7963,
      "step": 831
    },
    {
      "epoch": 0.3221060782036392,
      "grad_norm": 7.335258960723877,
      "learning_rate": 3.221060782036392e-06,
      "loss": 2.1439,
      "step": 832
    },
    {
      "epoch": 0.3224932249322493,
      "grad_norm": 6.618497848510742,
      "learning_rate": 3.224932249322493e-06,
      "loss": 1.8431,
      "step": 833
    },
    {
      "epoch": 0.3228803716608595,
      "grad_norm": 6.997554779052734,
      "learning_rate": 3.228803716608595e-06,
      "loss": 1.8194,
      "step": 834
    },
    {
      "epoch": 0.3232675183894696,
      "grad_norm": 7.6938018798828125,
      "learning_rate": 3.232675183894696e-06,
      "loss": 2.0969,
      "step": 835
    },
    {
      "epoch": 0.3236546651180798,
      "grad_norm": 6.193465232849121,
      "learning_rate": 3.236546651180798e-06,
      "loss": 1.8314,
      "step": 836
    },
    {
      "epoch": 0.3240418118466899,
      "grad_norm": 6.705842971801758,
      "learning_rate": 3.240418118466899e-06,
      "loss": 1.9696,
      "step": 837
    },
    {
      "epoch": 0.3244289585753,
      "grad_norm": 6.203885555267334,
      "learning_rate": 3.2442895857530007e-06,
      "loss": 1.8961,
      "step": 838
    },
    {
      "epoch": 0.3248161053039102,
      "grad_norm": 8.54855728149414,
      "learning_rate": 3.248161053039102e-06,
      "loss": 1.9223,
      "step": 839
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 5.910970211029053,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 1.9365,
      "step": 840
    },
    {
      "epoch": 0.3255903987611305,
      "grad_norm": 5.887923717498779,
      "learning_rate": 3.255903987611305e-06,
      "loss": 1.766,
      "step": 841
    },
    {
      "epoch": 0.3259775454897406,
      "grad_norm": 5.040539264678955,
      "learning_rate": 3.2597754548974066e-06,
      "loss": 1.9635,
      "step": 842
    },
    {
      "epoch": 0.32636469221835074,
      "grad_norm": 5.9049391746521,
      "learning_rate": 3.2636469221835077e-06,
      "loss": 1.8503,
      "step": 843
    },
    {
      "epoch": 0.3267518389469609,
      "grad_norm": 5.912818431854248,
      "learning_rate": 3.2675183894696096e-06,
      "loss": 1.7372,
      "step": 844
    },
    {
      "epoch": 0.32713898567557104,
      "grad_norm": 5.692553997039795,
      "learning_rate": 3.2713898567557107e-06,
      "loss": 1.8906,
      "step": 845
    },
    {
      "epoch": 0.32752613240418116,
      "grad_norm": 5.645511150360107,
      "learning_rate": 3.2752613240418118e-06,
      "loss": 1.9932,
      "step": 846
    },
    {
      "epoch": 0.32791327913279134,
      "grad_norm": 4.425200462341309,
      "learning_rate": 3.2791327913279137e-06,
      "loss": 1.7631,
      "step": 847
    },
    {
      "epoch": 0.32830042586140146,
      "grad_norm": 5.873213291168213,
      "learning_rate": 3.2830042586140148e-06,
      "loss": 1.9709,
      "step": 848
    },
    {
      "epoch": 0.32868757259001163,
      "grad_norm": 7.371757984161377,
      "learning_rate": 3.2868757259001167e-06,
      "loss": 1.7789,
      "step": 849
    },
    {
      "epoch": 0.32907471931862176,
      "grad_norm": 10.0683012008667,
      "learning_rate": 3.2907471931862177e-06,
      "loss": 1.7267,
      "step": 850
    },
    {
      "epoch": 0.3294618660472319,
      "grad_norm": 6.804142475128174,
      "learning_rate": 3.2946186604723192e-06,
      "loss": 1.991,
      "step": 851
    },
    {
      "epoch": 0.32984901277584205,
      "grad_norm": 5.716935634613037,
      "learning_rate": 3.2984901277584207e-06,
      "loss": 1.8856,
      "step": 852
    },
    {
      "epoch": 0.3302361595044522,
      "grad_norm": 5.3959059715271,
      "learning_rate": 3.3023615950445222e-06,
      "loss": 1.9214,
      "step": 853
    },
    {
      "epoch": 0.33062330623306235,
      "grad_norm": 8.169224739074707,
      "learning_rate": 3.3062330623306237e-06,
      "loss": 1.9224,
      "step": 854
    },
    {
      "epoch": 0.3310104529616725,
      "grad_norm": 5.549602508544922,
      "learning_rate": 3.310104529616725e-06,
      "loss": 1.7604,
      "step": 855
    },
    {
      "epoch": 0.3313975996902826,
      "grad_norm": 5.1741766929626465,
      "learning_rate": 3.3139759969028263e-06,
      "loss": 1.872,
      "step": 856
    },
    {
      "epoch": 0.3317847464188928,
      "grad_norm": 3.778778314590454,
      "learning_rate": 3.317847464188928e-06,
      "loss": 1.8409,
      "step": 857
    },
    {
      "epoch": 0.3321718931475029,
      "grad_norm": 5.4393439292907715,
      "learning_rate": 3.3217189314750293e-06,
      "loss": 1.8688,
      "step": 858
    },
    {
      "epoch": 0.33255903987611307,
      "grad_norm": 5.699974536895752,
      "learning_rate": 3.325590398761131e-06,
      "loss": 1.841,
      "step": 859
    },
    {
      "epoch": 0.3329461866047232,
      "grad_norm": 5.061371326446533,
      "learning_rate": 3.3294618660472323e-06,
      "loss": 1.9313,
      "step": 860
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 4.864750385284424,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.9648,
      "step": 861
    },
    {
      "epoch": 0.3337204800619435,
      "grad_norm": 7.423203468322754,
      "learning_rate": 3.3372048006194352e-06,
      "loss": 1.9577,
      "step": 862
    },
    {
      "epoch": 0.3341076267905536,
      "grad_norm": 6.4587860107421875,
      "learning_rate": 3.3410762679055363e-06,
      "loss": 1.7395,
      "step": 863
    },
    {
      "epoch": 0.3344947735191638,
      "grad_norm": 6.423622131347656,
      "learning_rate": 3.3449477351916382e-06,
      "loss": 1.8121,
      "step": 864
    },
    {
      "epoch": 0.3348819202477739,
      "grad_norm": 6.226582050323486,
      "learning_rate": 3.3488192024777393e-06,
      "loss": 1.8357,
      "step": 865
    },
    {
      "epoch": 0.33526906697638403,
      "grad_norm": 8.454564094543457,
      "learning_rate": 3.352690669763841e-06,
      "loss": 1.9223,
      "step": 866
    },
    {
      "epoch": 0.3356562137049942,
      "grad_norm": 6.044162750244141,
      "learning_rate": 3.3565621370499423e-06,
      "loss": 1.7265,
      "step": 867
    },
    {
      "epoch": 0.33604336043360433,
      "grad_norm": 4.222441673278809,
      "learning_rate": 3.3604336043360438e-06,
      "loss": 1.8772,
      "step": 868
    },
    {
      "epoch": 0.33643050716221445,
      "grad_norm": 4.663129806518555,
      "learning_rate": 3.364305071622145e-06,
      "loss": 1.9725,
      "step": 869
    },
    {
      "epoch": 0.33681765389082463,
      "grad_norm": 6.317265510559082,
      "learning_rate": 3.3681765389082468e-06,
      "loss": 1.7537,
      "step": 870
    },
    {
      "epoch": 0.33720480061943475,
      "grad_norm": 4.505406856536865,
      "learning_rate": 3.372048006194348e-06,
      "loss": 1.8973,
      "step": 871
    },
    {
      "epoch": 0.33759194734804493,
      "grad_norm": 6.944278240203857,
      "learning_rate": 3.3759194734804498e-06,
      "loss": 1.9139,
      "step": 872
    },
    {
      "epoch": 0.33797909407665505,
      "grad_norm": 5.991332530975342,
      "learning_rate": 3.379790940766551e-06,
      "loss": 2.007,
      "step": 873
    },
    {
      "epoch": 0.3383662408052652,
      "grad_norm": 5.562312126159668,
      "learning_rate": 3.383662408052652e-06,
      "loss": 1.9214,
      "step": 874
    },
    {
      "epoch": 0.33875338753387535,
      "grad_norm": 8.87118911743164,
      "learning_rate": 3.387533875338754e-06,
      "loss": 1.906,
      "step": 875
    },
    {
      "epoch": 0.33914053426248547,
      "grad_norm": 5.414921760559082,
      "learning_rate": 3.391405342624855e-06,
      "loss": 1.8234,
      "step": 876
    },
    {
      "epoch": 0.33952768099109565,
      "grad_norm": 6.100900650024414,
      "learning_rate": 3.395276809910957e-06,
      "loss": 1.8379,
      "step": 877
    },
    {
      "epoch": 0.33991482771970577,
      "grad_norm": 6.632890701293945,
      "learning_rate": 3.399148277197058e-06,
      "loss": 1.7059,
      "step": 878
    },
    {
      "epoch": 0.3403019744483159,
      "grad_norm": 5.403982639312744,
      "learning_rate": 3.4030197444831594e-06,
      "loss": 1.9368,
      "step": 879
    },
    {
      "epoch": 0.34068912117692607,
      "grad_norm": 6.948767185211182,
      "learning_rate": 3.406891211769261e-06,
      "loss": 1.9726,
      "step": 880
    },
    {
      "epoch": 0.3410762679055362,
      "grad_norm": 7.1643571853637695,
      "learning_rate": 3.4107626790553623e-06,
      "loss": 1.8223,
      "step": 881
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 4.706746578216553,
      "learning_rate": 3.414634146341464e-06,
      "loss": 1.8573,
      "step": 882
    },
    {
      "epoch": 0.3418505613627565,
      "grad_norm": 8.068329811096191,
      "learning_rate": 3.4185056136275653e-06,
      "loss": 1.7434,
      "step": 883
    },
    {
      "epoch": 0.3422377080913666,
      "grad_norm": 6.023092746734619,
      "learning_rate": 3.4223770809136664e-06,
      "loss": 1.8223,
      "step": 884
    },
    {
      "epoch": 0.3426248548199768,
      "grad_norm": 7.659391403198242,
      "learning_rate": 3.4262485481997683e-06,
      "loss": 1.8335,
      "step": 885
    },
    {
      "epoch": 0.3430120015485869,
      "grad_norm": 6.32168436050415,
      "learning_rate": 3.4301200154858694e-06,
      "loss": 1.8933,
      "step": 886
    },
    {
      "epoch": 0.34339914827719703,
      "grad_norm": 5.008774280548096,
      "learning_rate": 3.4339914827719705e-06,
      "loss": 1.9749,
      "step": 887
    },
    {
      "epoch": 0.3437862950058072,
      "grad_norm": 3.820782423019409,
      "learning_rate": 3.4378629500580724e-06,
      "loss": 1.8132,
      "step": 888
    },
    {
      "epoch": 0.34417344173441733,
      "grad_norm": 10.133663177490234,
      "learning_rate": 3.4417344173441734e-06,
      "loss": 1.9117,
      "step": 889
    },
    {
      "epoch": 0.3445605884630275,
      "grad_norm": 6.351448059082031,
      "learning_rate": 3.4456058846302754e-06,
      "loss": 1.9423,
      "step": 890
    },
    {
      "epoch": 0.34494773519163763,
      "grad_norm": 5.801458835601807,
      "learning_rate": 3.4494773519163764e-06,
      "loss": 1.9876,
      "step": 891
    },
    {
      "epoch": 0.34533488192024775,
      "grad_norm": 5.4739460945129395,
      "learning_rate": 3.453348819202478e-06,
      "loss": 1.8503,
      "step": 892
    },
    {
      "epoch": 0.3457220286488579,
      "grad_norm": 5.154107093811035,
      "learning_rate": 3.4572202864885794e-06,
      "loss": 1.8281,
      "step": 893
    },
    {
      "epoch": 0.34610917537746805,
      "grad_norm": 5.932253837585449,
      "learning_rate": 3.461091753774681e-06,
      "loss": 1.8432,
      "step": 894
    },
    {
      "epoch": 0.3464963221060782,
      "grad_norm": 10.667448043823242,
      "learning_rate": 3.4649632210607824e-06,
      "loss": 1.7225,
      "step": 895
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 7.351673126220703,
      "learning_rate": 3.468834688346884e-06,
      "loss": 1.7891,
      "step": 896
    },
    {
      "epoch": 0.34727061556329847,
      "grad_norm": 5.1575751304626465,
      "learning_rate": 3.472706155632985e-06,
      "loss": 1.8494,
      "step": 897
    },
    {
      "epoch": 0.34765776229190865,
      "grad_norm": 5.02217960357666,
      "learning_rate": 3.476577622919087e-06,
      "loss": 1.9592,
      "step": 898
    },
    {
      "epoch": 0.34804490902051877,
      "grad_norm": 6.5551886558532715,
      "learning_rate": 3.480449090205188e-06,
      "loss": 1.8134,
      "step": 899
    },
    {
      "epoch": 0.34843205574912894,
      "grad_norm": 10.820032119750977,
      "learning_rate": 3.48432055749129e-06,
      "loss": 1.9588,
      "step": 900
    },
    {
      "epoch": 0.34881920247773907,
      "grad_norm": 6.451162815093994,
      "learning_rate": 3.488192024777391e-06,
      "loss": 2.07,
      "step": 901
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 6.635438919067383,
      "learning_rate": 3.492063492063492e-06,
      "loss": 1.8382,
      "step": 902
    },
    {
      "epoch": 0.34959349593495936,
      "grad_norm": 7.011789798736572,
      "learning_rate": 3.495934959349594e-06,
      "loss": 1.6766,
      "step": 903
    },
    {
      "epoch": 0.3499806426635695,
      "grad_norm": 9.57318115234375,
      "learning_rate": 3.499806426635695e-06,
      "loss": 1.7279,
      "step": 904
    },
    {
      "epoch": 0.35036778939217966,
      "grad_norm": 6.355260372161865,
      "learning_rate": 3.503677893921797e-06,
      "loss": 1.9439,
      "step": 905
    },
    {
      "epoch": 0.3507549361207898,
      "grad_norm": 6.412200927734375,
      "learning_rate": 3.507549361207898e-06,
      "loss": 1.8079,
      "step": 906
    },
    {
      "epoch": 0.3511420828493999,
      "grad_norm": 6.9061479568481445,
      "learning_rate": 3.5114208284939995e-06,
      "loss": 1.7214,
      "step": 907
    },
    {
      "epoch": 0.3515292295780101,
      "grad_norm": 5.9676313400268555,
      "learning_rate": 3.515292295780101e-06,
      "loss": 1.7231,
      "step": 908
    },
    {
      "epoch": 0.3519163763066202,
      "grad_norm": 7.351244926452637,
      "learning_rate": 3.5191637630662025e-06,
      "loss": 1.9573,
      "step": 909
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 9.965896606445312,
      "learning_rate": 3.5230352303523035e-06,
      "loss": 1.9179,
      "step": 910
    },
    {
      "epoch": 0.3526906697638405,
      "grad_norm": 5.996851921081543,
      "learning_rate": 3.5269066976384055e-06,
      "loss": 1.8246,
      "step": 911
    },
    {
      "epoch": 0.3530778164924506,
      "grad_norm": 5.066661357879639,
      "learning_rate": 3.5307781649245065e-06,
      "loss": 1.8176,
      "step": 912
    },
    {
      "epoch": 0.3534649632210608,
      "grad_norm": 7.239879608154297,
      "learning_rate": 3.5346496322106084e-06,
      "loss": 1.7103,
      "step": 913
    },
    {
      "epoch": 0.3538521099496709,
      "grad_norm": 4.652872085571289,
      "learning_rate": 3.5385210994967095e-06,
      "loss": 1.8752,
      "step": 914
    },
    {
      "epoch": 0.35423925667828104,
      "grad_norm": 8.013227462768555,
      "learning_rate": 3.5423925667828106e-06,
      "loss": 1.9386,
      "step": 915
    },
    {
      "epoch": 0.3546264034068912,
      "grad_norm": 6.250703811645508,
      "learning_rate": 3.5462640340689125e-06,
      "loss": 1.8108,
      "step": 916
    },
    {
      "epoch": 0.35501355013550134,
      "grad_norm": 5.24884557723999,
      "learning_rate": 3.5501355013550136e-06,
      "loss": 1.7778,
      "step": 917
    },
    {
      "epoch": 0.3554006968641115,
      "grad_norm": 5.327972412109375,
      "learning_rate": 3.5540069686411155e-06,
      "loss": 1.8662,
      "step": 918
    },
    {
      "epoch": 0.35578784359272164,
      "grad_norm": 5.019577980041504,
      "learning_rate": 3.5578784359272166e-06,
      "loss": 1.8298,
      "step": 919
    },
    {
      "epoch": 0.35617499032133176,
      "grad_norm": 5.474857330322266,
      "learning_rate": 3.561749903213318e-06,
      "loss": 1.826,
      "step": 920
    },
    {
      "epoch": 0.35656213704994194,
      "grad_norm": 4.48022985458374,
      "learning_rate": 3.5656213704994195e-06,
      "loss": 1.9117,
      "step": 921
    },
    {
      "epoch": 0.35694928377855206,
      "grad_norm": 6.3672776222229,
      "learning_rate": 3.569492837785521e-06,
      "loss": 1.8216,
      "step": 922
    },
    {
      "epoch": 0.35733643050716224,
      "grad_norm": 6.9902849197387695,
      "learning_rate": 3.5733643050716225e-06,
      "loss": 2.0655,
      "step": 923
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 5.389644622802734,
      "learning_rate": 3.577235772357724e-06,
      "loss": 1.917,
      "step": 924
    },
    {
      "epoch": 0.3581107239643825,
      "grad_norm": 6.284972667694092,
      "learning_rate": 3.581107239643825e-06,
      "loss": 1.7176,
      "step": 925
    },
    {
      "epoch": 0.35849787069299266,
      "grad_norm": 7.1447954177856445,
      "learning_rate": 3.584978706929927e-06,
      "loss": 1.8124,
      "step": 926
    },
    {
      "epoch": 0.3588850174216028,
      "grad_norm": 9.958982467651367,
      "learning_rate": 3.588850174216028e-06,
      "loss": 1.9154,
      "step": 927
    },
    {
      "epoch": 0.35927216415021296,
      "grad_norm": 4.640240669250488,
      "learning_rate": 3.59272164150213e-06,
      "loss": 1.8529,
      "step": 928
    },
    {
      "epoch": 0.3596593108788231,
      "grad_norm": 10.382489204406738,
      "learning_rate": 3.596593108788231e-06,
      "loss": 1.9181,
      "step": 929
    },
    {
      "epoch": 0.3600464576074332,
      "grad_norm": 7.03015661239624,
      "learning_rate": 3.600464576074332e-06,
      "loss": 1.9232,
      "step": 930
    },
    {
      "epoch": 0.3604336043360434,
      "grad_norm": 5.814809799194336,
      "learning_rate": 3.604336043360434e-06,
      "loss": 1.863,
      "step": 931
    },
    {
      "epoch": 0.3608207510646535,
      "grad_norm": 6.6736860275268555,
      "learning_rate": 3.608207510646535e-06,
      "loss": 1.8235,
      "step": 932
    },
    {
      "epoch": 0.3612078977932636,
      "grad_norm": 5.057463645935059,
      "learning_rate": 3.6120789779326366e-06,
      "loss": 1.8298,
      "step": 933
    },
    {
      "epoch": 0.3615950445218738,
      "grad_norm": 10.918339729309082,
      "learning_rate": 3.615950445218738e-06,
      "loss": 1.6659,
      "step": 934
    },
    {
      "epoch": 0.3619821912504839,
      "grad_norm": 7.8552398681640625,
      "learning_rate": 3.6198219125048396e-06,
      "loss": 1.9181,
      "step": 935
    },
    {
      "epoch": 0.3623693379790941,
      "grad_norm": 6.742218971252441,
      "learning_rate": 3.623693379790941e-06,
      "loss": 1.9649,
      "step": 936
    },
    {
      "epoch": 0.3627564847077042,
      "grad_norm": 10.614937782287598,
      "learning_rate": 3.6275648470770426e-06,
      "loss": 1.658,
      "step": 937
    },
    {
      "epoch": 0.36314363143631434,
      "grad_norm": 6.994784355163574,
      "learning_rate": 3.6314363143631437e-06,
      "loss": 1.7901,
      "step": 938
    },
    {
      "epoch": 0.3635307781649245,
      "grad_norm": 7.651595592498779,
      "learning_rate": 3.6353077816492456e-06,
      "loss": 1.7745,
      "step": 939
    },
    {
      "epoch": 0.36391792489353464,
      "grad_norm": 7.180861949920654,
      "learning_rate": 3.6391792489353466e-06,
      "loss": 1.7663,
      "step": 940
    },
    {
      "epoch": 0.3643050716221448,
      "grad_norm": 7.757514476776123,
      "learning_rate": 3.6430507162214486e-06,
      "loss": 1.9243,
      "step": 941
    },
    {
      "epoch": 0.36469221835075494,
      "grad_norm": 7.771273612976074,
      "learning_rate": 3.6469221835075496e-06,
      "loss": 1.8037,
      "step": 942
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 6.205393314361572,
      "learning_rate": 3.6507936507936507e-06,
      "loss": 1.8512,
      "step": 943
    },
    {
      "epoch": 0.36546651180797524,
      "grad_norm": 7.9298014640808105,
      "learning_rate": 3.6546651180797526e-06,
      "loss": 2.0097,
      "step": 944
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 7.5006103515625,
      "learning_rate": 3.6585365853658537e-06,
      "loss": 2.0315,
      "step": 945
    },
    {
      "epoch": 0.36624080526519553,
      "grad_norm": 6.117129325866699,
      "learning_rate": 3.6624080526519556e-06,
      "loss": 1.8459,
      "step": 946
    },
    {
      "epoch": 0.36662795199380566,
      "grad_norm": 5.192980766296387,
      "learning_rate": 3.6662795199380567e-06,
      "loss": 1.8411,
      "step": 947
    },
    {
      "epoch": 0.3670150987224158,
      "grad_norm": 7.468575954437256,
      "learning_rate": 3.670150987224158e-06,
      "loss": 1.7749,
      "step": 948
    },
    {
      "epoch": 0.36740224545102595,
      "grad_norm": 10.083757400512695,
      "learning_rate": 3.6740224545102597e-06,
      "loss": 1.7704,
      "step": 949
    },
    {
      "epoch": 0.3677893921796361,
      "grad_norm": 6.403713703155518,
      "learning_rate": 3.677893921796361e-06,
      "loss": 1.8017,
      "step": 950
    },
    {
      "epoch": 0.3681765389082462,
      "grad_norm": 5.674198627471924,
      "learning_rate": 3.6817653890824622e-06,
      "loss": 2.0026,
      "step": 951
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 4.889875411987305,
      "learning_rate": 3.685636856368564e-06,
      "loss": 1.8509,
      "step": 952
    },
    {
      "epoch": 0.3689508323654665,
      "grad_norm": 6.990925312042236,
      "learning_rate": 3.6895083236546652e-06,
      "loss": 1.9853,
      "step": 953
    },
    {
      "epoch": 0.3693379790940767,
      "grad_norm": 11.994502067565918,
      "learning_rate": 3.693379790940767e-06,
      "loss": 1.7516,
      "step": 954
    },
    {
      "epoch": 0.3697251258226868,
      "grad_norm": 9.122444152832031,
      "learning_rate": 3.697251258226868e-06,
      "loss": 2.1057,
      "step": 955
    },
    {
      "epoch": 0.3701122725512969,
      "grad_norm": 7.620613098144531,
      "learning_rate": 3.7011227255129693e-06,
      "loss": 1.7136,
      "step": 956
    },
    {
      "epoch": 0.3704994192799071,
      "grad_norm": 6.132121562957764,
      "learning_rate": 3.704994192799071e-06,
      "loss": 1.9759,
      "step": 957
    },
    {
      "epoch": 0.3708865660085172,
      "grad_norm": 7.0659403800964355,
      "learning_rate": 3.7088656600851723e-06,
      "loss": 1.8104,
      "step": 958
    },
    {
      "epoch": 0.3712737127371274,
      "grad_norm": 6.117478370666504,
      "learning_rate": 3.712737127371274e-06,
      "loss": 1.7564,
      "step": 959
    },
    {
      "epoch": 0.3716608594657375,
      "grad_norm": 6.567720413208008,
      "learning_rate": 3.7166085946573752e-06,
      "loss": 1.8593,
      "step": 960
    },
    {
      "epoch": 0.37204800619434764,
      "grad_norm": 7.8149333000183105,
      "learning_rate": 3.7204800619434767e-06,
      "loss": 1.8281,
      "step": 961
    },
    {
      "epoch": 0.3724351529229578,
      "grad_norm": 8.909214973449707,
      "learning_rate": 3.7243515292295782e-06,
      "loss": 2.0803,
      "step": 962
    },
    {
      "epoch": 0.37282229965156793,
      "grad_norm": 5.039905548095703,
      "learning_rate": 3.7282229965156797e-06,
      "loss": 1.7746,
      "step": 963
    },
    {
      "epoch": 0.3732094463801781,
      "grad_norm": 5.5525641441345215,
      "learning_rate": 3.7320944638017812e-06,
      "loss": 1.826,
      "step": 964
    },
    {
      "epoch": 0.37359659310878823,
      "grad_norm": 11.596717834472656,
      "learning_rate": 3.7359659310878827e-06,
      "loss": 2.1175,
      "step": 965
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 6.0371856689453125,
      "learning_rate": 3.7398373983739838e-06,
      "loss": 1.9787,
      "step": 966
    },
    {
      "epoch": 0.37437088656600853,
      "grad_norm": 6.924212455749512,
      "learning_rate": 3.7437088656600857e-06,
      "loss": 1.8907,
      "step": 967
    },
    {
      "epoch": 0.37475803329461865,
      "grad_norm": 7.629073619842529,
      "learning_rate": 3.7475803329461868e-06,
      "loss": 1.9683,
      "step": 968
    },
    {
      "epoch": 0.37514518002322883,
      "grad_norm": 8.482142448425293,
      "learning_rate": 3.7514518002322887e-06,
      "loss": 1.7534,
      "step": 969
    },
    {
      "epoch": 0.37553232675183895,
      "grad_norm": 4.847962379455566,
      "learning_rate": 3.7553232675183898e-06,
      "loss": 1.9211,
      "step": 970
    },
    {
      "epoch": 0.3759194734804491,
      "grad_norm": 8.615931510925293,
      "learning_rate": 3.7591947348044912e-06,
      "loss": 1.9173,
      "step": 971
    },
    {
      "epoch": 0.37630662020905925,
      "grad_norm": 4.532628536224365,
      "learning_rate": 3.7630662020905927e-06,
      "loss": 1.8191,
      "step": 972
    },
    {
      "epoch": 0.37669376693766937,
      "grad_norm": 7.408926486968994,
      "learning_rate": 3.7669376693766942e-06,
      "loss": 1.9242,
      "step": 973
    },
    {
      "epoch": 0.3770809136662795,
      "grad_norm": 6.459501266479492,
      "learning_rate": 3.7708091366627953e-06,
      "loss": 1.8151,
      "step": 974
    },
    {
      "epoch": 0.37746806039488967,
      "grad_norm": 4.728452682495117,
      "learning_rate": 3.7746806039488972e-06,
      "loss": 1.8509,
      "step": 975
    },
    {
      "epoch": 0.3778552071234998,
      "grad_norm": 6.875911712646484,
      "learning_rate": 3.7785520712349983e-06,
      "loss": 1.8357,
      "step": 976
    },
    {
      "epoch": 0.37824235385210997,
      "grad_norm": 6.194357872009277,
      "learning_rate": 3.7824235385211e-06,
      "loss": 1.8507,
      "step": 977
    },
    {
      "epoch": 0.3786295005807201,
      "grad_norm": 6.441167831420898,
      "learning_rate": 3.7862950058072013e-06,
      "loss": 1.8902,
      "step": 978
    },
    {
      "epoch": 0.3790166473093302,
      "grad_norm": 7.9552693367004395,
      "learning_rate": 3.7901664730933023e-06,
      "loss": 1.7467,
      "step": 979
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 4.351224422454834,
      "learning_rate": 3.7940379403794043e-06,
      "loss": 1.9513,
      "step": 980
    },
    {
      "epoch": 0.3797909407665505,
      "grad_norm": 5.030082702636719,
      "learning_rate": 3.7979094076655053e-06,
      "loss": 1.9397,
      "step": 981
    },
    {
      "epoch": 0.3801780874951607,
      "grad_norm": 6.6347737312316895,
      "learning_rate": 3.8017808749516073e-06,
      "loss": 1.826,
      "step": 982
    },
    {
      "epoch": 0.3805652342237708,
      "grad_norm": 5.957674503326416,
      "learning_rate": 3.8056523422377083e-06,
      "loss": 1.8744,
      "step": 983
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 7.484006404876709,
      "learning_rate": 3.80952380952381e-06,
      "loss": 1.9361,
      "step": 984
    },
    {
      "epoch": 0.3813395276809911,
      "grad_norm": 5.486177921295166,
      "learning_rate": 3.8133952768099113e-06,
      "loss": 1.7396,
      "step": 985
    },
    {
      "epoch": 0.38172667440960123,
      "grad_norm": 6.526432514190674,
      "learning_rate": 3.817266744096012e-06,
      "loss": 1.805,
      "step": 986
    },
    {
      "epoch": 0.3821138211382114,
      "grad_norm": 6.485808372497559,
      "learning_rate": 3.821138211382115e-06,
      "loss": 1.8467,
      "step": 987
    },
    {
      "epoch": 0.38250096786682153,
      "grad_norm": 8.187198638916016,
      "learning_rate": 3.825009678668215e-06,
      "loss": 1.8837,
      "step": 988
    },
    {
      "epoch": 0.38288811459543165,
      "grad_norm": 6.070324897766113,
      "learning_rate": 3.828881145954317e-06,
      "loss": 1.8484,
      "step": 989
    },
    {
      "epoch": 0.3832752613240418,
      "grad_norm": 7.754283905029297,
      "learning_rate": 3.832752613240418e-06,
      "loss": 1.7662,
      "step": 990
    },
    {
      "epoch": 0.38366240805265195,
      "grad_norm": 5.667147159576416,
      "learning_rate": 3.83662408052652e-06,
      "loss": 1.7702,
      "step": 991
    },
    {
      "epoch": 0.3840495547812621,
      "grad_norm": 4.315986633300781,
      "learning_rate": 3.840495547812621e-06,
      "loss": 1.8675,
      "step": 992
    },
    {
      "epoch": 0.38443670150987225,
      "grad_norm": 5.951976776123047,
      "learning_rate": 3.844367015098723e-06,
      "loss": 1.8134,
      "step": 993
    },
    {
      "epoch": 0.38482384823848237,
      "grad_norm": 4.930840015411377,
      "learning_rate": 3.848238482384824e-06,
      "loss": 1.7918,
      "step": 994
    },
    {
      "epoch": 0.38521099496709255,
      "grad_norm": 6.285064697265625,
      "learning_rate": 3.852109949670926e-06,
      "loss": 1.9145,
      "step": 995
    },
    {
      "epoch": 0.38559814169570267,
      "grad_norm": 7.371286392211914,
      "learning_rate": 3.855981416957027e-06,
      "loss": 1.9535,
      "step": 996
    },
    {
      "epoch": 0.3859852884243128,
      "grad_norm": 6.694183826446533,
      "learning_rate": 3.859852884243128e-06,
      "loss": 1.7954,
      "step": 997
    },
    {
      "epoch": 0.38637243515292297,
      "grad_norm": 6.17716121673584,
      "learning_rate": 3.86372435152923e-06,
      "loss": 1.8989,
      "step": 998
    },
    {
      "epoch": 0.3867595818815331,
      "grad_norm": 4.890300273895264,
      "learning_rate": 3.867595818815331e-06,
      "loss": 1.8554,
      "step": 999
    },
    {
      "epoch": 0.38714672861014326,
      "grad_norm": 6.994903564453125,
      "learning_rate": 3.871467286101433e-06,
      "loss": 1.755,
      "step": 1000
    },
    {
      "epoch": 0.3875338753387534,
      "grad_norm": 6.921496868133545,
      "learning_rate": 3.875338753387534e-06,
      "loss": 1.8091,
      "step": 1001
    },
    {
      "epoch": 0.3879210220673635,
      "grad_norm": 10.440444946289062,
      "learning_rate": 3.8792102206736354e-06,
      "loss": 1.8656,
      "step": 1002
    },
    {
      "epoch": 0.3883081687959737,
      "grad_norm": 7.185372352600098,
      "learning_rate": 3.883081687959737e-06,
      "loss": 1.8885,
      "step": 1003
    },
    {
      "epoch": 0.3886953155245838,
      "grad_norm": 4.347341060638428,
      "learning_rate": 3.886953155245838e-06,
      "loss": 1.8515,
      "step": 1004
    },
    {
      "epoch": 0.389082462253194,
      "grad_norm": 10.460396766662598,
      "learning_rate": 3.89082462253194e-06,
      "loss": 1.914,
      "step": 1005
    },
    {
      "epoch": 0.3894696089818041,
      "grad_norm": 5.536220550537109,
      "learning_rate": 3.894696089818041e-06,
      "loss": 1.7757,
      "step": 1006
    },
    {
      "epoch": 0.3898567557104142,
      "grad_norm": 4.700542449951172,
      "learning_rate": 3.898567557104143e-06,
      "loss": 1.7862,
      "step": 1007
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 6.4426374435424805,
      "learning_rate": 3.902439024390244e-06,
      "loss": 1.9703,
      "step": 1008
    },
    {
      "epoch": 0.3906310491676345,
      "grad_norm": 7.4468817710876465,
      "learning_rate": 3.906310491676346e-06,
      "loss": 1.9811,
      "step": 1009
    },
    {
      "epoch": 0.3910181958962447,
      "grad_norm": 7.305410861968994,
      "learning_rate": 3.910181958962447e-06,
      "loss": 1.8045,
      "step": 1010
    },
    {
      "epoch": 0.3914053426248548,
      "grad_norm": 7.314209938049316,
      "learning_rate": 3.914053426248549e-06,
      "loss": 1.8096,
      "step": 1011
    },
    {
      "epoch": 0.39179248935346495,
      "grad_norm": 6.130899429321289,
      "learning_rate": 3.9179248935346495e-06,
      "loss": 1.839,
      "step": 1012
    },
    {
      "epoch": 0.3921796360820751,
      "grad_norm": 6.605877876281738,
      "learning_rate": 3.921796360820752e-06,
      "loss": 1.8013,
      "step": 1013
    },
    {
      "epoch": 0.39256678281068524,
      "grad_norm": 9.672568321228027,
      "learning_rate": 3.9256678281068525e-06,
      "loss": 1.8679,
      "step": 1014
    },
    {
      "epoch": 0.39295392953929537,
      "grad_norm": 7.296731948852539,
      "learning_rate": 3.929539295392954e-06,
      "loss": 1.8086,
      "step": 1015
    },
    {
      "epoch": 0.39334107626790554,
      "grad_norm": 7.263525485992432,
      "learning_rate": 3.9334107626790555e-06,
      "loss": 1.7227,
      "step": 1016
    },
    {
      "epoch": 0.39372822299651566,
      "grad_norm": 8.393779754638672,
      "learning_rate": 3.937282229965157e-06,
      "loss": 1.8701,
      "step": 1017
    },
    {
      "epoch": 0.39411536972512584,
      "grad_norm": 7.498469829559326,
      "learning_rate": 3.9411536972512585e-06,
      "loss": 1.9127,
      "step": 1018
    },
    {
      "epoch": 0.39450251645373596,
      "grad_norm": 7.705573558807373,
      "learning_rate": 3.94502516453736e-06,
      "loss": 1.8794,
      "step": 1019
    },
    {
      "epoch": 0.3948896631823461,
      "grad_norm": 5.393834114074707,
      "learning_rate": 3.9488966318234615e-06,
      "loss": 1.776,
      "step": 1020
    },
    {
      "epoch": 0.39527680991095626,
      "grad_norm": 6.81558895111084,
      "learning_rate": 3.952768099109563e-06,
      "loss": 1.9436,
      "step": 1021
    },
    {
      "epoch": 0.3956639566395664,
      "grad_norm": 10.730734825134277,
      "learning_rate": 3.9566395663956644e-06,
      "loss": 1.9126,
      "step": 1022
    },
    {
      "epoch": 0.39605110336817656,
      "grad_norm": 4.824930667877197,
      "learning_rate": 3.960511033681766e-06,
      "loss": 1.9485,
      "step": 1023
    },
    {
      "epoch": 0.3964382500967867,
      "grad_norm": 7.289262771606445,
      "learning_rate": 3.9643825009678674e-06,
      "loss": 1.7659,
      "step": 1024
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 8.324384689331055,
      "learning_rate": 3.968253968253968e-06,
      "loss": 1.822,
      "step": 1025
    },
    {
      "epoch": 0.397212543554007,
      "grad_norm": 7.1331682205200195,
      "learning_rate": 3.97212543554007e-06,
      "loss": 1.7658,
      "step": 1026
    },
    {
      "epoch": 0.3975996902826171,
      "grad_norm": 6.0450873374938965,
      "learning_rate": 3.975996902826171e-06,
      "loss": 1.9733,
      "step": 1027
    },
    {
      "epoch": 0.3979868370112273,
      "grad_norm": 8.693906784057617,
      "learning_rate": 3.979868370112273e-06,
      "loss": 2.041,
      "step": 1028
    },
    {
      "epoch": 0.3983739837398374,
      "grad_norm": 6.79072904586792,
      "learning_rate": 3.983739837398374e-06,
      "loss": 1.8285,
      "step": 1029
    },
    {
      "epoch": 0.3987611304684475,
      "grad_norm": 6.792149066925049,
      "learning_rate": 3.9876113046844755e-06,
      "loss": 1.6992,
      "step": 1030
    },
    {
      "epoch": 0.3991482771970577,
      "grad_norm": 10.180524826049805,
      "learning_rate": 3.991482771970577e-06,
      "loss": 1.7537,
      "step": 1031
    },
    {
      "epoch": 0.3995354239256678,
      "grad_norm": 6.90080451965332,
      "learning_rate": 3.9953542392566785e-06,
      "loss": 1.8543,
      "step": 1032
    },
    {
      "epoch": 0.399922570654278,
      "grad_norm": 8.782181739807129,
      "learning_rate": 3.99922570654278e-06,
      "loss": 1.9359,
      "step": 1033
    },
    {
      "epoch": 0.4003097173828881,
      "grad_norm": 4.588636875152588,
      "learning_rate": 4.0030971738288815e-06,
      "loss": 1.854,
      "step": 1034
    },
    {
      "epoch": 0.40069686411149824,
      "grad_norm": 5.694295406341553,
      "learning_rate": 4.006968641114983e-06,
      "loss": 1.9144,
      "step": 1035
    },
    {
      "epoch": 0.4010840108401084,
      "grad_norm": 5.34428071975708,
      "learning_rate": 4.0108401084010845e-06,
      "loss": 1.9145,
      "step": 1036
    },
    {
      "epoch": 0.40147115756871854,
      "grad_norm": 8.83871078491211,
      "learning_rate": 4.014711575687186e-06,
      "loss": 1.6739,
      "step": 1037
    },
    {
      "epoch": 0.40185830429732866,
      "grad_norm": 5.622236251831055,
      "learning_rate": 4.018583042973287e-06,
      "loss": 1.8569,
      "step": 1038
    },
    {
      "epoch": 0.40224545102593884,
      "grad_norm": 9.09410285949707,
      "learning_rate": 4.022454510259389e-06,
      "loss": 1.7797,
      "step": 1039
    },
    {
      "epoch": 0.40263259775454896,
      "grad_norm": 6.6376495361328125,
      "learning_rate": 4.02632597754549e-06,
      "loss": 1.9282,
      "step": 1040
    },
    {
      "epoch": 0.40301974448315914,
      "grad_norm": 6.3174028396606445,
      "learning_rate": 4.030197444831592e-06,
      "loss": 1.8154,
      "step": 1041
    },
    {
      "epoch": 0.40340689121176926,
      "grad_norm": 5.422191143035889,
      "learning_rate": 4.034068912117693e-06,
      "loss": 1.7916,
      "step": 1042
    },
    {
      "epoch": 0.4037940379403794,
      "grad_norm": 7.130708694458008,
      "learning_rate": 4.037940379403794e-06,
      "loss": 1.7154,
      "step": 1043
    },
    {
      "epoch": 0.40418118466898956,
      "grad_norm": 7.735983371734619,
      "learning_rate": 4.041811846689896e-06,
      "loss": 1.9298,
      "step": 1044
    },
    {
      "epoch": 0.4045683313975997,
      "grad_norm": 7.1909990310668945,
      "learning_rate": 4.045683313975997e-06,
      "loss": 1.7682,
      "step": 1045
    },
    {
      "epoch": 0.40495547812620986,
      "grad_norm": 5.878539562225342,
      "learning_rate": 4.049554781262099e-06,
      "loss": 1.8125,
      "step": 1046
    },
    {
      "epoch": 0.40534262485482,
      "grad_norm": 5.125941276550293,
      "learning_rate": 4.0534262485482e-06,
      "loss": 1.9141,
      "step": 1047
    },
    {
      "epoch": 0.4057297715834301,
      "grad_norm": 7.789982795715332,
      "learning_rate": 4.057297715834302e-06,
      "loss": 1.7673,
      "step": 1048
    },
    {
      "epoch": 0.4061169183120403,
      "grad_norm": 6.927610397338867,
      "learning_rate": 4.061169183120403e-06,
      "loss": 1.7171,
      "step": 1049
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 7.074440956115723,
      "learning_rate": 4.0650406504065046e-06,
      "loss": 1.8205,
      "step": 1050
    },
    {
      "epoch": 0.4068912117692606,
      "grad_norm": 10.519023895263672,
      "learning_rate": 4.068912117692606e-06,
      "loss": 1.9074,
      "step": 1051
    },
    {
      "epoch": 0.4072783584978707,
      "grad_norm": 6.531515121459961,
      "learning_rate": 4.0727835849787076e-06,
      "loss": 1.777,
      "step": 1052
    },
    {
      "epoch": 0.4076655052264808,
      "grad_norm": 5.757457733154297,
      "learning_rate": 4.076655052264808e-06,
      "loss": 1.824,
      "step": 1053
    },
    {
      "epoch": 0.408052651955091,
      "grad_norm": 5.75399923324585,
      "learning_rate": 4.0805265195509105e-06,
      "loss": 1.8079,
      "step": 1054
    },
    {
      "epoch": 0.4084397986837011,
      "grad_norm": 11.61396598815918,
      "learning_rate": 4.084397986837011e-06,
      "loss": 2.0023,
      "step": 1055
    },
    {
      "epoch": 0.4088269454123113,
      "grad_norm": 7.138791084289551,
      "learning_rate": 4.0882694541231135e-06,
      "loss": 1.8936,
      "step": 1056
    },
    {
      "epoch": 0.4092140921409214,
      "grad_norm": 7.854433536529541,
      "learning_rate": 4.092140921409214e-06,
      "loss": 1.8226,
      "step": 1057
    },
    {
      "epoch": 0.40960123886953154,
      "grad_norm": 11.446723937988281,
      "learning_rate": 4.096012388695316e-06,
      "loss": 2.0463,
      "step": 1058
    },
    {
      "epoch": 0.4099883855981417,
      "grad_norm": 6.197576999664307,
      "learning_rate": 4.099883855981417e-06,
      "loss": 1.9028,
      "step": 1059
    },
    {
      "epoch": 0.41037553232675184,
      "grad_norm": 8.371200561523438,
      "learning_rate": 4.103755323267519e-06,
      "loss": 1.9448,
      "step": 1060
    },
    {
      "epoch": 0.41076267905536196,
      "grad_norm": 8.966072082519531,
      "learning_rate": 4.10762679055362e-06,
      "loss": 2.0768,
      "step": 1061
    },
    {
      "epoch": 0.41114982578397213,
      "grad_norm": 5.86836576461792,
      "learning_rate": 4.111498257839722e-06,
      "loss": 1.8453,
      "step": 1062
    },
    {
      "epoch": 0.41153697251258226,
      "grad_norm": 6.762277126312256,
      "learning_rate": 4.115369725125823e-06,
      "loss": 1.6987,
      "step": 1063
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 6.063173294067383,
      "learning_rate": 4.119241192411925e-06,
      "loss": 1.8231,
      "step": 1064
    },
    {
      "epoch": 0.41231126596980255,
      "grad_norm": 6.219878673553467,
      "learning_rate": 4.123112659698026e-06,
      "loss": 1.8949,
      "step": 1065
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 11.741094589233398,
      "learning_rate": 4.126984126984127e-06,
      "loss": 1.8869,
      "step": 1066
    },
    {
      "epoch": 0.41308555942702285,
      "grad_norm": 9.171361923217773,
      "learning_rate": 4.130855594270229e-06,
      "loss": 2.1046,
      "step": 1067
    },
    {
      "epoch": 0.413472706155633,
      "grad_norm": 7.2616729736328125,
      "learning_rate": 4.13472706155633e-06,
      "loss": 1.8042,
      "step": 1068
    },
    {
      "epoch": 0.41385985288424315,
      "grad_norm": 7.731119632720947,
      "learning_rate": 4.138598528842432e-06,
      "loss": 1.833,
      "step": 1069
    },
    {
      "epoch": 0.4142469996128533,
      "grad_norm": 7.547011852264404,
      "learning_rate": 4.142469996128533e-06,
      "loss": 1.8938,
      "step": 1070
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 7.011690616607666,
      "learning_rate": 4.146341463414634e-06,
      "loss": 1.836,
      "step": 1071
    },
    {
      "epoch": 0.41502129307007357,
      "grad_norm": 12.237669944763184,
      "learning_rate": 4.150212930700736e-06,
      "loss": 1.8363,
      "step": 1072
    },
    {
      "epoch": 0.4154084397986837,
      "grad_norm": 5.191030979156494,
      "learning_rate": 4.154084397986837e-06,
      "loss": 1.8278,
      "step": 1073
    },
    {
      "epoch": 0.41579558652729387,
      "grad_norm": 7.0229597091674805,
      "learning_rate": 4.157955865272939e-06,
      "loss": 1.6721,
      "step": 1074
    },
    {
      "epoch": 0.416182733255904,
      "grad_norm": 11.699090003967285,
      "learning_rate": 4.16182733255904e-06,
      "loss": 2.0986,
      "step": 1075
    },
    {
      "epoch": 0.4165698799845141,
      "grad_norm": 8.181157112121582,
      "learning_rate": 4.165698799845142e-06,
      "loss": 1.9333,
      "step": 1076
    },
    {
      "epoch": 0.4169570267131243,
      "grad_norm": 7.733731746673584,
      "learning_rate": 4.169570267131243e-06,
      "loss": 1.8594,
      "step": 1077
    },
    {
      "epoch": 0.4173441734417344,
      "grad_norm": 5.851437568664551,
      "learning_rate": 4.173441734417345e-06,
      "loss": 1.822,
      "step": 1078
    },
    {
      "epoch": 0.41773132017034453,
      "grad_norm": 8.455815315246582,
      "learning_rate": 4.177313201703445e-06,
      "loss": 1.895,
      "step": 1079
    },
    {
      "epoch": 0.4181184668989547,
      "grad_norm": 7.864645957946777,
      "learning_rate": 4.181184668989548e-06,
      "loss": 1.7781,
      "step": 1080
    },
    {
      "epoch": 0.41850561362756483,
      "grad_norm": 8.117196083068848,
      "learning_rate": 4.185056136275648e-06,
      "loss": 1.8597,
      "step": 1081
    },
    {
      "epoch": 0.418892760356175,
      "grad_norm": 5.850574970245361,
      "learning_rate": 4.188927603561751e-06,
      "loss": 1.844,
      "step": 1082
    },
    {
      "epoch": 0.41927990708478513,
      "grad_norm": 7.6419525146484375,
      "learning_rate": 4.192799070847851e-06,
      "loss": 1.7653,
      "step": 1083
    },
    {
      "epoch": 0.41966705381339525,
      "grad_norm": 6.343409538269043,
      "learning_rate": 4.196670538133953e-06,
      "loss": 1.7293,
      "step": 1084
    },
    {
      "epoch": 0.42005420054200543,
      "grad_norm": 5.983706951141357,
      "learning_rate": 4.200542005420054e-06,
      "loss": 1.802,
      "step": 1085
    },
    {
      "epoch": 0.42044134727061555,
      "grad_norm": 7.5377092361450195,
      "learning_rate": 4.204413472706156e-06,
      "loss": 1.8169,
      "step": 1086
    },
    {
      "epoch": 0.42082849399922573,
      "grad_norm": 3.8462092876434326,
      "learning_rate": 4.208284939992257e-06,
      "loss": 1.8385,
      "step": 1087
    },
    {
      "epoch": 0.42121564072783585,
      "grad_norm": 5.379147529602051,
      "learning_rate": 4.212156407278359e-06,
      "loss": 1.7416,
      "step": 1088
    },
    {
      "epoch": 0.42160278745644597,
      "grad_norm": 7.648809432983398,
      "learning_rate": 4.21602787456446e-06,
      "loss": 1.8885,
      "step": 1089
    },
    {
      "epoch": 0.42198993418505615,
      "grad_norm": 7.084758758544922,
      "learning_rate": 4.219899341850562e-06,
      "loss": 1.8204,
      "step": 1090
    },
    {
      "epoch": 0.42237708091366627,
      "grad_norm": 8.979840278625488,
      "learning_rate": 4.223770809136663e-06,
      "loss": 2.0371,
      "step": 1091
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 5.410975456237793,
      "learning_rate": 4.227642276422765e-06,
      "loss": 1.8007,
      "step": 1092
    },
    {
      "epoch": 0.42315137437088657,
      "grad_norm": 6.89345645904541,
      "learning_rate": 4.231513743708866e-06,
      "loss": 1.8088,
      "step": 1093
    },
    {
      "epoch": 0.4235385210994967,
      "grad_norm": 5.9134521484375,
      "learning_rate": 4.235385210994967e-06,
      "loss": 1.7834,
      "step": 1094
    },
    {
      "epoch": 0.42392566782810687,
      "grad_norm": 5.889510631561279,
      "learning_rate": 4.239256678281069e-06,
      "loss": 1.7182,
      "step": 1095
    },
    {
      "epoch": 0.424312814556717,
      "grad_norm": 7.260437965393066,
      "learning_rate": 4.24312814556717e-06,
      "loss": 1.7903,
      "step": 1096
    },
    {
      "epoch": 0.42469996128532717,
      "grad_norm": 7.3325419425964355,
      "learning_rate": 4.246999612853272e-06,
      "loss": 1.7872,
      "step": 1097
    },
    {
      "epoch": 0.4250871080139373,
      "grad_norm": 6.902414321899414,
      "learning_rate": 4.250871080139373e-06,
      "loss": 1.8061,
      "step": 1098
    },
    {
      "epoch": 0.4254742547425474,
      "grad_norm": 9.584883689880371,
      "learning_rate": 4.254742547425474e-06,
      "loss": 1.7024,
      "step": 1099
    },
    {
      "epoch": 0.4258614014711576,
      "grad_norm": 7.092770099639893,
      "learning_rate": 4.258614014711576e-06,
      "loss": 1.8645,
      "step": 1100
    },
    {
      "epoch": 0.4262485481997677,
      "grad_norm": 10.605938911437988,
      "learning_rate": 4.262485481997677e-06,
      "loss": 2.0842,
      "step": 1101
    },
    {
      "epoch": 0.42663569492837783,
      "grad_norm": 7.272816181182861,
      "learning_rate": 4.266356949283779e-06,
      "loss": 1.7644,
      "step": 1102
    },
    {
      "epoch": 0.427022841656988,
      "grad_norm": 7.227027893066406,
      "learning_rate": 4.27022841656988e-06,
      "loss": 1.8143,
      "step": 1103
    },
    {
      "epoch": 0.4274099883855981,
      "grad_norm": 6.540299415588379,
      "learning_rate": 4.274099883855982e-06,
      "loss": 1.8091,
      "step": 1104
    },
    {
      "epoch": 0.4277971351142083,
      "grad_norm": 7.267534255981445,
      "learning_rate": 4.277971351142083e-06,
      "loss": 1.8152,
      "step": 1105
    },
    {
      "epoch": 0.4281842818428184,
      "grad_norm": 7.487703323364258,
      "learning_rate": 4.281842818428185e-06,
      "loss": 1.7344,
      "step": 1106
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.099993705749512,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 1.7852,
      "step": 1107
    },
    {
      "epoch": 0.4289585753000387,
      "grad_norm": 7.036769866943359,
      "learning_rate": 4.289585753000388e-06,
      "loss": 1.8193,
      "step": 1108
    },
    {
      "epoch": 0.42934572202864885,
      "grad_norm": 7.88557767868042,
      "learning_rate": 4.2934572202864884e-06,
      "loss": 1.7807,
      "step": 1109
    },
    {
      "epoch": 0.429732868757259,
      "grad_norm": 6.234097480773926,
      "learning_rate": 4.297328687572591e-06,
      "loss": 1.7211,
      "step": 1110
    },
    {
      "epoch": 0.43012001548586914,
      "grad_norm": 12.439189910888672,
      "learning_rate": 4.3012001548586914e-06,
      "loss": 1.6537,
      "step": 1111
    },
    {
      "epoch": 0.43050716221447927,
      "grad_norm": 8.169246673583984,
      "learning_rate": 4.305071622144793e-06,
      "loss": 1.8592,
      "step": 1112
    },
    {
      "epoch": 0.43089430894308944,
      "grad_norm": 4.973944187164307,
      "learning_rate": 4.308943089430894e-06,
      "loss": 1.8289,
      "step": 1113
    },
    {
      "epoch": 0.43128145567169957,
      "grad_norm": 8.851553916931152,
      "learning_rate": 4.312814556716996e-06,
      "loss": 1.9104,
      "step": 1114
    },
    {
      "epoch": 0.43166860240030974,
      "grad_norm": 8.162639617919922,
      "learning_rate": 4.316686024003097e-06,
      "loss": 1.7266,
      "step": 1115
    },
    {
      "epoch": 0.43205574912891986,
      "grad_norm": 8.516408920288086,
      "learning_rate": 4.320557491289199e-06,
      "loss": 1.8425,
      "step": 1116
    },
    {
      "epoch": 0.43244289585753,
      "grad_norm": 8.054035186767578,
      "learning_rate": 4.3244289585753e-06,
      "loss": 1.6528,
      "step": 1117
    },
    {
      "epoch": 0.43283004258614016,
      "grad_norm": 8.632983207702637,
      "learning_rate": 4.328300425861402e-06,
      "loss": 1.7698,
      "step": 1118
    },
    {
      "epoch": 0.4332171893147503,
      "grad_norm": 7.768477439880371,
      "learning_rate": 4.332171893147503e-06,
      "loss": 1.7886,
      "step": 1119
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 7.118156433105469,
      "learning_rate": 4.336043360433605e-06,
      "loss": 1.8985,
      "step": 1120
    },
    {
      "epoch": 0.4339914827719706,
      "grad_norm": 8.817588806152344,
      "learning_rate": 4.339914827719706e-06,
      "loss": 1.7758,
      "step": 1121
    },
    {
      "epoch": 0.4343786295005807,
      "grad_norm": 6.1306962966918945,
      "learning_rate": 4.343786295005807e-06,
      "loss": 1.7747,
      "step": 1122
    },
    {
      "epoch": 0.4347657762291909,
      "grad_norm": 8.011433601379395,
      "learning_rate": 4.347657762291909e-06,
      "loss": 1.822,
      "step": 1123
    },
    {
      "epoch": 0.435152922957801,
      "grad_norm": 7.646282196044922,
      "learning_rate": 4.35152922957801e-06,
      "loss": 1.7612,
      "step": 1124
    },
    {
      "epoch": 0.4355400696864111,
      "grad_norm": 6.916039943695068,
      "learning_rate": 4.3554006968641115e-06,
      "loss": 1.7448,
      "step": 1125
    },
    {
      "epoch": 0.4359272164150213,
      "grad_norm": 8.355910301208496,
      "learning_rate": 4.359272164150213e-06,
      "loss": 1.8102,
      "step": 1126
    },
    {
      "epoch": 0.4363143631436314,
      "grad_norm": 9.373740196228027,
      "learning_rate": 4.3631436314363145e-06,
      "loss": 1.8227,
      "step": 1127
    },
    {
      "epoch": 0.4367015098722416,
      "grad_norm": 8.126326560974121,
      "learning_rate": 4.367015098722416e-06,
      "loss": 1.8974,
      "step": 1128
    },
    {
      "epoch": 0.4370886566008517,
      "grad_norm": 7.3929033279418945,
      "learning_rate": 4.3708865660085175e-06,
      "loss": 1.7919,
      "step": 1129
    },
    {
      "epoch": 0.43747580332946184,
      "grad_norm": 7.894052505493164,
      "learning_rate": 4.374758033294619e-06,
      "loss": 1.9394,
      "step": 1130
    },
    {
      "epoch": 0.437862950058072,
      "grad_norm": 7.260898590087891,
      "learning_rate": 4.3786295005807205e-06,
      "loss": 1.9232,
      "step": 1131
    },
    {
      "epoch": 0.43825009678668214,
      "grad_norm": 6.294471263885498,
      "learning_rate": 4.382500967866822e-06,
      "loss": 1.7289,
      "step": 1132
    },
    {
      "epoch": 0.4386372435152923,
      "grad_norm": 7.442167282104492,
      "learning_rate": 4.3863724351529234e-06,
      "loss": 1.702,
      "step": 1133
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 10.481618881225586,
      "learning_rate": 4.390243902439025e-06,
      "loss": 1.8236,
      "step": 1134
    },
    {
      "epoch": 0.43941153697251256,
      "grad_norm": 10.772294998168945,
      "learning_rate": 4.394115369725126e-06,
      "loss": 1.9302,
      "step": 1135
    },
    {
      "epoch": 0.43979868370112274,
      "grad_norm": 9.008394241333008,
      "learning_rate": 4.397986837011228e-06,
      "loss": 1.8349,
      "step": 1136
    },
    {
      "epoch": 0.44018583042973286,
      "grad_norm": 10.050360679626465,
      "learning_rate": 4.4018583042973286e-06,
      "loss": 1.9866,
      "step": 1137
    },
    {
      "epoch": 0.44057297715834304,
      "grad_norm": 6.3553080558776855,
      "learning_rate": 4.405729771583431e-06,
      "loss": 1.8056,
      "step": 1138
    },
    {
      "epoch": 0.44096012388695316,
      "grad_norm": 10.872370719909668,
      "learning_rate": 4.4096012388695316e-06,
      "loss": 1.8464,
      "step": 1139
    },
    {
      "epoch": 0.4413472706155633,
      "grad_norm": 6.178465366363525,
      "learning_rate": 4.413472706155633e-06,
      "loss": 1.8874,
      "step": 1140
    },
    {
      "epoch": 0.44173441734417346,
      "grad_norm": 5.847365379333496,
      "learning_rate": 4.4173441734417345e-06,
      "loss": 1.7468,
      "step": 1141
    },
    {
      "epoch": 0.4421215640727836,
      "grad_norm": 5.362544059753418,
      "learning_rate": 4.421215640727836e-06,
      "loss": 1.8735,
      "step": 1142
    },
    {
      "epoch": 0.4425087108013937,
      "grad_norm": 7.363213539123535,
      "learning_rate": 4.4250871080139375e-06,
      "loss": 1.7748,
      "step": 1143
    },
    {
      "epoch": 0.4428958575300039,
      "grad_norm": 7.650078296661377,
      "learning_rate": 4.428958575300039e-06,
      "loss": 1.7209,
      "step": 1144
    },
    {
      "epoch": 0.443283004258614,
      "grad_norm": 9.157793045043945,
      "learning_rate": 4.4328300425861405e-06,
      "loss": 1.8174,
      "step": 1145
    },
    {
      "epoch": 0.4436701509872242,
      "grad_norm": 8.010568618774414,
      "learning_rate": 4.436701509872242e-06,
      "loss": 1.6716,
      "step": 1146
    },
    {
      "epoch": 0.4440572977158343,
      "grad_norm": 7.896255016326904,
      "learning_rate": 4.4405729771583435e-06,
      "loss": 1.9857,
      "step": 1147
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 7.9335432052612305,
      "learning_rate": 4.444444444444444e-06,
      "loss": 1.7966,
      "step": 1148
    },
    {
      "epoch": 0.4448315911730546,
      "grad_norm": 6.553643226623535,
      "learning_rate": 4.4483159117305465e-06,
      "loss": 1.8011,
      "step": 1149
    },
    {
      "epoch": 0.4452187379016647,
      "grad_norm": 7.421691417694092,
      "learning_rate": 4.452187379016647e-06,
      "loss": 1.6947,
      "step": 1150
    },
    {
      "epoch": 0.4456058846302749,
      "grad_norm": 5.758768558502197,
      "learning_rate": 4.4560588463027495e-06,
      "loss": 1.8463,
      "step": 1151
    },
    {
      "epoch": 0.445993031358885,
      "grad_norm": 8.436195373535156,
      "learning_rate": 4.45993031358885e-06,
      "loss": 1.7883,
      "step": 1152
    },
    {
      "epoch": 0.44638017808749514,
      "grad_norm": 6.468707084655762,
      "learning_rate": 4.463801780874952e-06,
      "loss": 1.8275,
      "step": 1153
    },
    {
      "epoch": 0.4467673248161053,
      "grad_norm": 5.621915340423584,
      "learning_rate": 4.467673248161053e-06,
      "loss": 1.8272,
      "step": 1154
    },
    {
      "epoch": 0.44715447154471544,
      "grad_norm": 6.155309677124023,
      "learning_rate": 4.471544715447155e-06,
      "loss": 1.8239,
      "step": 1155
    },
    {
      "epoch": 0.4475416182733256,
      "grad_norm": 16.646562576293945,
      "learning_rate": 4.475416182733256e-06,
      "loss": 1.8473,
      "step": 1156
    },
    {
      "epoch": 0.44792876500193574,
      "grad_norm": 10.270953178405762,
      "learning_rate": 4.479287650019358e-06,
      "loss": 2.0731,
      "step": 1157
    },
    {
      "epoch": 0.44831591173054586,
      "grad_norm": 7.724088668823242,
      "learning_rate": 4.483159117305459e-06,
      "loss": 1.8746,
      "step": 1158
    },
    {
      "epoch": 0.44870305845915603,
      "grad_norm": 7.1062140464782715,
      "learning_rate": 4.487030584591561e-06,
      "loss": 1.7333,
      "step": 1159
    },
    {
      "epoch": 0.44909020518776616,
      "grad_norm": 7.381711006164551,
      "learning_rate": 4.490902051877662e-06,
      "loss": 1.7291,
      "step": 1160
    },
    {
      "epoch": 0.44947735191637633,
      "grad_norm": 10.362436294555664,
      "learning_rate": 4.4947735191637636e-06,
      "loss": 1.9623,
      "step": 1161
    },
    {
      "epoch": 0.44986449864498645,
      "grad_norm": 10.222472190856934,
      "learning_rate": 4.498644986449865e-06,
      "loss": 1.6973,
      "step": 1162
    },
    {
      "epoch": 0.4502516453735966,
      "grad_norm": 7.345465183258057,
      "learning_rate": 4.5025164537359666e-06,
      "loss": 1.8314,
      "step": 1163
    },
    {
      "epoch": 0.45063879210220675,
      "grad_norm": 8.868804931640625,
      "learning_rate": 4.506387921022068e-06,
      "loss": 1.7175,
      "step": 1164
    },
    {
      "epoch": 0.4510259388308169,
      "grad_norm": 6.456099033355713,
      "learning_rate": 4.5102593883081695e-06,
      "loss": 1.8393,
      "step": 1165
    },
    {
      "epoch": 0.451413085559427,
      "grad_norm": 8.33167839050293,
      "learning_rate": 4.51413085559427e-06,
      "loss": 1.9257,
      "step": 1166
    },
    {
      "epoch": 0.4518002322880372,
      "grad_norm": 8.079118728637695,
      "learning_rate": 4.5180023228803725e-06,
      "loss": 1.6983,
      "step": 1167
    },
    {
      "epoch": 0.4521873790166473,
      "grad_norm": 6.416520595550537,
      "learning_rate": 4.521873790166473e-06,
      "loss": 1.7987,
      "step": 1168
    },
    {
      "epoch": 0.45257452574525747,
      "grad_norm": 8.079507827758789,
      "learning_rate": 4.5257452574525755e-06,
      "loss": 1.8103,
      "step": 1169
    },
    {
      "epoch": 0.4529616724738676,
      "grad_norm": 14.616253852844238,
      "learning_rate": 4.529616724738676e-06,
      "loss": 2.0165,
      "step": 1170
    },
    {
      "epoch": 0.4533488192024777,
      "grad_norm": 10.729535102844238,
      "learning_rate": 4.533488192024778e-06,
      "loss": 1.8941,
      "step": 1171
    },
    {
      "epoch": 0.4537359659310879,
      "grad_norm": 6.862905502319336,
      "learning_rate": 4.537359659310879e-06,
      "loss": 1.7868,
      "step": 1172
    },
    {
      "epoch": 0.454123112659698,
      "grad_norm": 7.891745567321777,
      "learning_rate": 4.541231126596981e-06,
      "loss": 1.7899,
      "step": 1173
    },
    {
      "epoch": 0.4545102593883082,
      "grad_norm": 7.428366661071777,
      "learning_rate": 4.545102593883082e-06,
      "loss": 1.718,
      "step": 1174
    },
    {
      "epoch": 0.4548974061169183,
      "grad_norm": 9.85200023651123,
      "learning_rate": 4.548974061169184e-06,
      "loss": 1.7185,
      "step": 1175
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 7.832864761352539,
      "learning_rate": 4.552845528455285e-06,
      "loss": 1.7691,
      "step": 1176
    },
    {
      "epoch": 0.4556716995741386,
      "grad_norm": 8.443107604980469,
      "learning_rate": 4.556716995741387e-06,
      "loss": 2.232,
      "step": 1177
    },
    {
      "epoch": 0.45605884630274873,
      "grad_norm": 8.465107917785645,
      "learning_rate": 4.560588463027488e-06,
      "loss": 1.8328,
      "step": 1178
    },
    {
      "epoch": 0.4564459930313589,
      "grad_norm": 7.087314128875732,
      "learning_rate": 4.56445993031359e-06,
      "loss": 1.7479,
      "step": 1179
    },
    {
      "epoch": 0.45683313975996903,
      "grad_norm": 7.591129779815674,
      "learning_rate": 4.568331397599691e-06,
      "loss": 1.7046,
      "step": 1180
    },
    {
      "epoch": 0.45722028648857915,
      "grad_norm": 5.6059651374816895,
      "learning_rate": 4.572202864885792e-06,
      "loss": 1.7802,
      "step": 1181
    },
    {
      "epoch": 0.45760743321718933,
      "grad_norm": 8.291529655456543,
      "learning_rate": 4.576074332171894e-06,
      "loss": 1.8031,
      "step": 1182
    },
    {
      "epoch": 0.45799457994579945,
      "grad_norm": 7.791135311126709,
      "learning_rate": 4.579945799457995e-06,
      "loss": 1.8618,
      "step": 1183
    },
    {
      "epoch": 0.45838172667440963,
      "grad_norm": 7.984649658203125,
      "learning_rate": 4.583817266744097e-06,
      "loss": 1.7971,
      "step": 1184
    },
    {
      "epoch": 0.45876887340301975,
      "grad_norm": 11.637068748474121,
      "learning_rate": 4.587688734030198e-06,
      "loss": 2.0359,
      "step": 1185
    },
    {
      "epoch": 0.45915602013162987,
      "grad_norm": 11.001853942871094,
      "learning_rate": 4.591560201316299e-06,
      "loss": 1.8609,
      "step": 1186
    },
    {
      "epoch": 0.45954316686024005,
      "grad_norm": 11.171842575073242,
      "learning_rate": 4.595431668602401e-06,
      "loss": 1.7055,
      "step": 1187
    },
    {
      "epoch": 0.45993031358885017,
      "grad_norm": 5.497411251068115,
      "learning_rate": 4.599303135888502e-06,
      "loss": 1.8262,
      "step": 1188
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 6.827698230743408,
      "learning_rate": 4.603174603174604e-06,
      "loss": 1.7899,
      "step": 1189
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 9.068109512329102,
      "learning_rate": 4.607046070460705e-06,
      "loss": 1.8165,
      "step": 1190
    },
    {
      "epoch": 0.4610917537746806,
      "grad_norm": 8.095277786254883,
      "learning_rate": 4.610917537746807e-06,
      "loss": 1.756,
      "step": 1191
    },
    {
      "epoch": 0.46147890050329077,
      "grad_norm": 10.41849136352539,
      "learning_rate": 4.614789005032908e-06,
      "loss": 1.7962,
      "step": 1192
    },
    {
      "epoch": 0.4618660472319009,
      "grad_norm": 7.548772811889648,
      "learning_rate": 4.61866047231901e-06,
      "loss": 1.7278,
      "step": 1193
    },
    {
      "epoch": 0.462253193960511,
      "grad_norm": 6.841695785522461,
      "learning_rate": 4.62253193960511e-06,
      "loss": 1.7784,
      "step": 1194
    },
    {
      "epoch": 0.4626403406891212,
      "grad_norm": 8.158512115478516,
      "learning_rate": 4.626403406891213e-06,
      "loss": 1.8571,
      "step": 1195
    },
    {
      "epoch": 0.4630274874177313,
      "grad_norm": 5.224759578704834,
      "learning_rate": 4.630274874177313e-06,
      "loss": 1.8073,
      "step": 1196
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 7.086298942565918,
      "learning_rate": 4.634146341463416e-06,
      "loss": 1.6694,
      "step": 1197
    },
    {
      "epoch": 0.4638017808749516,
      "grad_norm": 7.74200963973999,
      "learning_rate": 4.638017808749516e-06,
      "loss": 1.7286,
      "step": 1198
    },
    {
      "epoch": 0.46418892760356173,
      "grad_norm": 9.289406776428223,
      "learning_rate": 4.641889276035618e-06,
      "loss": 1.6876,
      "step": 1199
    },
    {
      "epoch": 0.4645760743321719,
      "grad_norm": 6.77566385269165,
      "learning_rate": 4.645760743321719e-06,
      "loss": 1.7614,
      "step": 1200
    },
    {
      "epoch": 0.46496322106078203,
      "grad_norm": 7.472706317901611,
      "learning_rate": 4.649632210607821e-06,
      "loss": 1.8944,
      "step": 1201
    },
    {
      "epoch": 0.4653503677893922,
      "grad_norm": 9.37161636352539,
      "learning_rate": 4.653503677893922e-06,
      "loss": 1.7653,
      "step": 1202
    },
    {
      "epoch": 0.4657375145180023,
      "grad_norm": 8.697610855102539,
      "learning_rate": 4.657375145180024e-06,
      "loss": 1.7986,
      "step": 1203
    },
    {
      "epoch": 0.46612466124661245,
      "grad_norm": 9.745418548583984,
      "learning_rate": 4.661246612466125e-06,
      "loss": 2.2016,
      "step": 1204
    },
    {
      "epoch": 0.4665118079752226,
      "grad_norm": 7.772027015686035,
      "learning_rate": 4.665118079752227e-06,
      "loss": 1.8722,
      "step": 1205
    },
    {
      "epoch": 0.46689895470383275,
      "grad_norm": 8.565866470336914,
      "learning_rate": 4.668989547038328e-06,
      "loss": 1.7909,
      "step": 1206
    },
    {
      "epoch": 0.46728610143244287,
      "grad_norm": 5.965920925140381,
      "learning_rate": 4.672861014324429e-06,
      "loss": 1.76,
      "step": 1207
    },
    {
      "epoch": 0.46767324816105305,
      "grad_norm": 11.530014991760254,
      "learning_rate": 4.676732481610531e-06,
      "loss": 1.8608,
      "step": 1208
    },
    {
      "epoch": 0.46806039488966317,
      "grad_norm": 6.076766490936279,
      "learning_rate": 4.680603948896632e-06,
      "loss": 1.7292,
      "step": 1209
    },
    {
      "epoch": 0.46844754161827334,
      "grad_norm": 8.431111335754395,
      "learning_rate": 4.684475416182734e-06,
      "loss": 1.7818,
      "step": 1210
    },
    {
      "epoch": 0.46883468834688347,
      "grad_norm": 7.297515869140625,
      "learning_rate": 4.688346883468835e-06,
      "loss": 1.7175,
      "step": 1211
    },
    {
      "epoch": 0.4692218350754936,
      "grad_norm": 12.616189956665039,
      "learning_rate": 4.692218350754936e-06,
      "loss": 1.8794,
      "step": 1212
    },
    {
      "epoch": 0.46960898180410376,
      "grad_norm": 6.422956466674805,
      "learning_rate": 4.696089818041038e-06,
      "loss": 1.6784,
      "step": 1213
    },
    {
      "epoch": 0.4699961285327139,
      "grad_norm": 9.145773887634277,
      "learning_rate": 4.699961285327139e-06,
      "loss": 1.8095,
      "step": 1214
    },
    {
      "epoch": 0.47038327526132406,
      "grad_norm": 9.226818084716797,
      "learning_rate": 4.703832752613241e-06,
      "loss": 1.7219,
      "step": 1215
    },
    {
      "epoch": 0.4707704219899342,
      "grad_norm": 6.9555158615112305,
      "learning_rate": 4.707704219899342e-06,
      "loss": 1.7958,
      "step": 1216
    },
    {
      "epoch": 0.4711575687185443,
      "grad_norm": 8.154215812683105,
      "learning_rate": 4.711575687185444e-06,
      "loss": 1.8573,
      "step": 1217
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 5.027792453765869,
      "learning_rate": 4.715447154471545e-06,
      "loss": 1.7501,
      "step": 1218
    },
    {
      "epoch": 0.4719318621757646,
      "grad_norm": 7.6340413093566895,
      "learning_rate": 4.719318621757647e-06,
      "loss": 1.8114,
      "step": 1219
    },
    {
      "epoch": 0.4723190089043748,
      "grad_norm": 8.009791374206543,
      "learning_rate": 4.723190089043748e-06,
      "loss": 1.7774,
      "step": 1220
    },
    {
      "epoch": 0.4727061556329849,
      "grad_norm": 5.859385967254639,
      "learning_rate": 4.72706155632985e-06,
      "loss": 1.7764,
      "step": 1221
    },
    {
      "epoch": 0.473093302361595,
      "grad_norm": 6.946610450744629,
      "learning_rate": 4.7309330236159504e-06,
      "loss": 1.754,
      "step": 1222
    },
    {
      "epoch": 0.4734804490902052,
      "grad_norm": 9.399944305419922,
      "learning_rate": 4.734804490902053e-06,
      "loss": 1.8098,
      "step": 1223
    },
    {
      "epoch": 0.4738675958188153,
      "grad_norm": 6.630936145782471,
      "learning_rate": 4.738675958188153e-06,
      "loss": 1.8501,
      "step": 1224
    },
    {
      "epoch": 0.4742547425474255,
      "grad_norm": 11.695547103881836,
      "learning_rate": 4.742547425474256e-06,
      "loss": 1.6851,
      "step": 1225
    },
    {
      "epoch": 0.4746418892760356,
      "grad_norm": 8.159245491027832,
      "learning_rate": 4.746418892760356e-06,
      "loss": 2.091,
      "step": 1226
    },
    {
      "epoch": 0.47502903600464574,
      "grad_norm": 6.82175350189209,
      "learning_rate": 4.750290360046458e-06,
      "loss": 1.8104,
      "step": 1227
    },
    {
      "epoch": 0.4754161827332559,
      "grad_norm": 6.956649303436279,
      "learning_rate": 4.754161827332559e-06,
      "loss": 1.7584,
      "step": 1228
    },
    {
      "epoch": 0.47580332946186604,
      "grad_norm": 8.740132331848145,
      "learning_rate": 4.758033294618661e-06,
      "loss": 1.8276,
      "step": 1229
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 6.581330299377441,
      "learning_rate": 4.761904761904762e-06,
      "loss": 1.7654,
      "step": 1230
    },
    {
      "epoch": 0.47657762291908634,
      "grad_norm": 6.841709136962891,
      "learning_rate": 4.765776229190864e-06,
      "loss": 1.8021,
      "step": 1231
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 9.53692626953125,
      "learning_rate": 4.769647696476965e-06,
      "loss": 1.8709,
      "step": 1232
    },
    {
      "epoch": 0.47735191637630664,
      "grad_norm": 28.54514503479004,
      "learning_rate": 4.773519163763067e-06,
      "loss": 2.039,
      "step": 1233
    },
    {
      "epoch": 0.47773906310491676,
      "grad_norm": 6.897552490234375,
      "learning_rate": 4.777390631049168e-06,
      "loss": 1.762,
      "step": 1234
    },
    {
      "epoch": 0.4781262098335269,
      "grad_norm": 8.126384735107422,
      "learning_rate": 4.781262098335269e-06,
      "loss": 2.1001,
      "step": 1235
    },
    {
      "epoch": 0.47851335656213706,
      "grad_norm": 11.268472671508789,
      "learning_rate": 4.785133565621371e-06,
      "loss": 1.6657,
      "step": 1236
    },
    {
      "epoch": 0.4789005032907472,
      "grad_norm": 6.6045613288879395,
      "learning_rate": 4.789005032907472e-06,
      "loss": 1.7459,
      "step": 1237
    },
    {
      "epoch": 0.47928765001935736,
      "grad_norm": 7.3218560218811035,
      "learning_rate": 4.792876500193574e-06,
      "loss": 1.7375,
      "step": 1238
    },
    {
      "epoch": 0.4796747967479675,
      "grad_norm": 12.644137382507324,
      "learning_rate": 4.796747967479675e-06,
      "loss": 1.9214,
      "step": 1239
    },
    {
      "epoch": 0.4800619434765776,
      "grad_norm": 12.779243469238281,
      "learning_rate": 4.8006194347657765e-06,
      "loss": 1.9184,
      "step": 1240
    },
    {
      "epoch": 0.4804490902051878,
      "grad_norm": 7.266574859619141,
      "learning_rate": 4.804490902051878e-06,
      "loss": 1.7182,
      "step": 1241
    },
    {
      "epoch": 0.4808362369337979,
      "grad_norm": 8.06868839263916,
      "learning_rate": 4.8083623693379794e-06,
      "loss": 1.7949,
      "step": 1242
    },
    {
      "epoch": 0.4812233836624081,
      "grad_norm": 16.04082679748535,
      "learning_rate": 4.812233836624081e-06,
      "loss": 2.0924,
      "step": 1243
    },
    {
      "epoch": 0.4816105303910182,
      "grad_norm": 7.495399475097656,
      "learning_rate": 4.8161053039101824e-06,
      "loss": 1.6522,
      "step": 1244
    },
    {
      "epoch": 0.4819976771196283,
      "grad_norm": 7.571193695068359,
      "learning_rate": 4.819976771196284e-06,
      "loss": 1.8176,
      "step": 1245
    },
    {
      "epoch": 0.4823848238482385,
      "grad_norm": 9.685017585754395,
      "learning_rate": 4.823848238482385e-06,
      "loss": 1.7813,
      "step": 1246
    },
    {
      "epoch": 0.4827719705768486,
      "grad_norm": 7.221957683563232,
      "learning_rate": 4.827719705768487e-06,
      "loss": 1.7539,
      "step": 1247
    },
    {
      "epoch": 0.4831591173054588,
      "grad_norm": 6.6510186195373535,
      "learning_rate": 4.831591173054588e-06,
      "loss": 1.7172,
      "step": 1248
    },
    {
      "epoch": 0.4835462640340689,
      "grad_norm": 8.394550323486328,
      "learning_rate": 4.83546264034069e-06,
      "loss": 1.8425,
      "step": 1249
    },
    {
      "epoch": 0.48393341076267904,
      "grad_norm": 8.16180419921875,
      "learning_rate": 4.8393341076267905e-06,
      "loss": 1.8468,
      "step": 1250
    },
    {
      "epoch": 0.4843205574912892,
      "grad_norm": 8.013097763061523,
      "learning_rate": 4.843205574912893e-06,
      "loss": 1.7693,
      "step": 1251
    },
    {
      "epoch": 0.48470770421989934,
      "grad_norm": 10.49217414855957,
      "learning_rate": 4.8470770421989935e-06,
      "loss": 1.7618,
      "step": 1252
    },
    {
      "epoch": 0.48509485094850946,
      "grad_norm": 8.79500961303711,
      "learning_rate": 4.850948509485095e-06,
      "loss": 1.8085,
      "step": 1253
    },
    {
      "epoch": 0.48548199767711964,
      "grad_norm": 8.855293273925781,
      "learning_rate": 4.8548199767711965e-06,
      "loss": 1.7781,
      "step": 1254
    },
    {
      "epoch": 0.48586914440572976,
      "grad_norm": 7.3895063400268555,
      "learning_rate": 4.858691444057298e-06,
      "loss": 1.8355,
      "step": 1255
    },
    {
      "epoch": 0.48625629113433994,
      "grad_norm": 9.0484619140625,
      "learning_rate": 4.8625629113433995e-06,
      "loss": 1.6794,
      "step": 1256
    },
    {
      "epoch": 0.48664343786295006,
      "grad_norm": 11.733718872070312,
      "learning_rate": 4.866434378629501e-06,
      "loss": 2.1144,
      "step": 1257
    },
    {
      "epoch": 0.4870305845915602,
      "grad_norm": 8.353759765625,
      "learning_rate": 4.8703058459156025e-06,
      "loss": 1.7139,
      "step": 1258
    },
    {
      "epoch": 0.48741773132017036,
      "grad_norm": 5.8226447105407715,
      "learning_rate": 4.874177313201704e-06,
      "loss": 1.7637,
      "step": 1259
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 7.501962184906006,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 1.7533,
      "step": 1260
    },
    {
      "epoch": 0.48819202477739065,
      "grad_norm": 8.486833572387695,
      "learning_rate": 4.881920247773907e-06,
      "loss": 1.704,
      "step": 1261
    },
    {
      "epoch": 0.4885791715060008,
      "grad_norm": 9.040925979614258,
      "learning_rate": 4.8857917150600085e-06,
      "loss": 1.9838,
      "step": 1262
    },
    {
      "epoch": 0.4889663182346109,
      "grad_norm": 5.881021976470947,
      "learning_rate": 4.889663182346109e-06,
      "loss": 1.7447,
      "step": 1263
    },
    {
      "epoch": 0.4893534649632211,
      "grad_norm": 7.9820098876953125,
      "learning_rate": 4.8935346496322115e-06,
      "loss": 2.048,
      "step": 1264
    },
    {
      "epoch": 0.4897406116918312,
      "grad_norm": 9.066625595092773,
      "learning_rate": 4.897406116918312e-06,
      "loss": 2.149,
      "step": 1265
    },
    {
      "epoch": 0.4901277584204414,
      "grad_norm": 8.0830078125,
      "learning_rate": 4.9012775842044144e-06,
      "loss": 1.8366,
      "step": 1266
    },
    {
      "epoch": 0.4905149051490515,
      "grad_norm": 7.364961624145508,
      "learning_rate": 4.905149051490515e-06,
      "loss": 1.6481,
      "step": 1267
    },
    {
      "epoch": 0.4909020518776616,
      "grad_norm": 10.621599197387695,
      "learning_rate": 4.909020518776617e-06,
      "loss": 1.8789,
      "step": 1268
    },
    {
      "epoch": 0.4912891986062718,
      "grad_norm": 8.669751167297363,
      "learning_rate": 4.912891986062718e-06,
      "loss": 1.8713,
      "step": 1269
    },
    {
      "epoch": 0.4916763453348819,
      "grad_norm": 8.815266609191895,
      "learning_rate": 4.9167634533488196e-06,
      "loss": 1.6935,
      "step": 1270
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 5.5545735359191895,
      "learning_rate": 4.920634920634921e-06,
      "loss": 1.8098,
      "step": 1271
    },
    {
      "epoch": 0.4924506387921022,
      "grad_norm": 6.907401084899902,
      "learning_rate": 4.9245063879210226e-06,
      "loss": 1.7447,
      "step": 1272
    },
    {
      "epoch": 0.49283778552071233,
      "grad_norm": 12.632711410522461,
      "learning_rate": 4.928377855207124e-06,
      "loss": 1.9561,
      "step": 1273
    },
    {
      "epoch": 0.4932249322493225,
      "grad_norm": 9.984132766723633,
      "learning_rate": 4.9322493224932255e-06,
      "loss": 1.7556,
      "step": 1274
    },
    {
      "epoch": 0.49361207897793263,
      "grad_norm": 6.085011959075928,
      "learning_rate": 4.936120789779327e-06,
      "loss": 1.7836,
      "step": 1275
    },
    {
      "epoch": 0.49399922570654275,
      "grad_norm": 6.727043151855469,
      "learning_rate": 4.939992257065428e-06,
      "loss": 1.6737,
      "step": 1276
    },
    {
      "epoch": 0.49438637243515293,
      "grad_norm": 6.552462100982666,
      "learning_rate": 4.94386372435153e-06,
      "loss": 1.8562,
      "step": 1277
    },
    {
      "epoch": 0.49477351916376305,
      "grad_norm": 6.996310710906982,
      "learning_rate": 4.947735191637631e-06,
      "loss": 1.774,
      "step": 1278
    },
    {
      "epoch": 0.49516066589237323,
      "grad_norm": 7.439060211181641,
      "learning_rate": 4.951606658923733e-06,
      "loss": 1.8645,
      "step": 1279
    },
    {
      "epoch": 0.49554781262098335,
      "grad_norm": 8.391182899475098,
      "learning_rate": 4.955478126209834e-06,
      "loss": 1.7161,
      "step": 1280
    },
    {
      "epoch": 0.4959349593495935,
      "grad_norm": 6.773946762084961,
      "learning_rate": 4.959349593495935e-06,
      "loss": 1.7727,
      "step": 1281
    },
    {
      "epoch": 0.49632210607820365,
      "grad_norm": 9.339591026306152,
      "learning_rate": 4.963221060782037e-06,
      "loss": 2.1467,
      "step": 1282
    },
    {
      "epoch": 0.4967092528068138,
      "grad_norm": 13.101534843444824,
      "learning_rate": 4.967092528068138e-06,
      "loss": 1.8156,
      "step": 1283
    },
    {
      "epoch": 0.49709639953542395,
      "grad_norm": 9.072443008422852,
      "learning_rate": 4.97096399535424e-06,
      "loss": 1.7877,
      "step": 1284
    },
    {
      "epoch": 0.49748354626403407,
      "grad_norm": 8.034409523010254,
      "learning_rate": 4.974835462640341e-06,
      "loss": 1.6724,
      "step": 1285
    },
    {
      "epoch": 0.4978706929926442,
      "grad_norm": 9.200090408325195,
      "learning_rate": 4.978706929926443e-06,
      "loss": 1.6717,
      "step": 1286
    },
    {
      "epoch": 0.49825783972125437,
      "grad_norm": 7.619313716888428,
      "learning_rate": 4.982578397212544e-06,
      "loss": 1.7391,
      "step": 1287
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 11.121085166931152,
      "learning_rate": 4.986449864498646e-06,
      "loss": 1.7499,
      "step": 1288
    },
    {
      "epoch": 0.49903213317847467,
      "grad_norm": 10.333406448364258,
      "learning_rate": 4.990321331784747e-06,
      "loss": 1.9308,
      "step": 1289
    },
    {
      "epoch": 0.4994192799070848,
      "grad_norm": 7.297568321228027,
      "learning_rate": 4.994192799070849e-06,
      "loss": 1.6901,
      "step": 1290
    },
    {
      "epoch": 0.4998064266356949,
      "grad_norm": 5.885211944580078,
      "learning_rate": 4.998064266356949e-06,
      "loss": 1.7462,
      "step": 1291
    },
    {
      "epoch": 0.500193573364305,
      "grad_norm": 8.989912033081055,
      "learning_rate": 5.001935733643051e-06,
      "loss": 1.7973,
      "step": 1292
    },
    {
      "epoch": 0.5005807200929152,
      "grad_norm": 14.319707870483398,
      "learning_rate": 5.005807200929152e-06,
      "loss": 1.7254,
      "step": 1293
    },
    {
      "epoch": 0.5009678668215254,
      "grad_norm": 8.638689994812012,
      "learning_rate": 5.0096786682152546e-06,
      "loss": 1.723,
      "step": 1294
    },
    {
      "epoch": 0.5013550135501355,
      "grad_norm": 6.493345260620117,
      "learning_rate": 5.013550135501355e-06,
      "loss": 1.7121,
      "step": 1295
    },
    {
      "epoch": 0.5017421602787456,
      "grad_norm": 7.5784478187561035,
      "learning_rate": 5.017421602787457e-06,
      "loss": 1.6619,
      "step": 1296
    },
    {
      "epoch": 0.5021293070073558,
      "grad_norm": 8.598311424255371,
      "learning_rate": 5.021293070073558e-06,
      "loss": 1.7179,
      "step": 1297
    },
    {
      "epoch": 0.502516453735966,
      "grad_norm": 9.280269622802734,
      "learning_rate": 5.0251645373596605e-06,
      "loss": 1.7426,
      "step": 1298
    },
    {
      "epoch": 0.502903600464576,
      "grad_norm": 7.828007698059082,
      "learning_rate": 5.029036004645761e-06,
      "loss": 1.6942,
      "step": 1299
    },
    {
      "epoch": 0.5032907471931862,
      "grad_norm": 8.416085243225098,
      "learning_rate": 5.032907471931863e-06,
      "loss": 1.7567,
      "step": 1300
    },
    {
      "epoch": 0.5036778939217964,
      "grad_norm": 4.9756622314453125,
      "learning_rate": 5.036778939217964e-06,
      "loss": 1.8594,
      "step": 1301
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 9.59684944152832,
      "learning_rate": 5.040650406504065e-06,
      "loss": 1.769,
      "step": 1302
    },
    {
      "epoch": 0.5044521873790166,
      "grad_norm": 7.622328758239746,
      "learning_rate": 5.044521873790167e-06,
      "loss": 1.7885,
      "step": 1303
    },
    {
      "epoch": 0.5048393341076268,
      "grad_norm": 5.3853759765625,
      "learning_rate": 5.048393341076269e-06,
      "loss": 1.9012,
      "step": 1304
    },
    {
      "epoch": 0.5052264808362369,
      "grad_norm": 12.7770414352417,
      "learning_rate": 5.052264808362369e-06,
      "loss": 1.5756,
      "step": 1305
    },
    {
      "epoch": 0.5056136275648471,
      "grad_norm": 9.566641807556152,
      "learning_rate": 5.056136275648471e-06,
      "loss": 1.8211,
      "step": 1306
    },
    {
      "epoch": 0.5060007742934572,
      "grad_norm": 7.381981372833252,
      "learning_rate": 5.060007742934573e-06,
      "loss": 1.7978,
      "step": 1307
    },
    {
      "epoch": 0.5063879210220673,
      "grad_norm": 13.096182823181152,
      "learning_rate": 5.063879210220674e-06,
      "loss": 1.5714,
      "step": 1308
    },
    {
      "epoch": 0.5067750677506775,
      "grad_norm": 7.9695634841918945,
      "learning_rate": 5.067750677506775e-06,
      "loss": 1.6306,
      "step": 1309
    },
    {
      "epoch": 0.5071622144792877,
      "grad_norm": 6.70554256439209,
      "learning_rate": 5.071622144792877e-06,
      "loss": 1.6707,
      "step": 1310
    },
    {
      "epoch": 0.5075493612078978,
      "grad_norm": 9.293648719787598,
      "learning_rate": 5.075493612078979e-06,
      "loss": 1.7442,
      "step": 1311
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 12.665901184082031,
      "learning_rate": 5.07936507936508e-06,
      "loss": 1.7302,
      "step": 1312
    },
    {
      "epoch": 0.5083236546651181,
      "grad_norm": 7.331841945648193,
      "learning_rate": 5.083236546651181e-06,
      "loss": 1.7705,
      "step": 1313
    },
    {
      "epoch": 0.5087108013937283,
      "grad_norm": 8.053418159484863,
      "learning_rate": 5.087108013937283e-06,
      "loss": 1.723,
      "step": 1314
    },
    {
      "epoch": 0.5090979481223383,
      "grad_norm": 16.229639053344727,
      "learning_rate": 5.090979481223383e-06,
      "loss": 1.7001,
      "step": 1315
    },
    {
      "epoch": 0.5094850948509485,
      "grad_norm": 7.097141742706299,
      "learning_rate": 5.094850948509486e-06,
      "loss": 1.6488,
      "step": 1316
    },
    {
      "epoch": 0.5098722415795587,
      "grad_norm": 8.260337829589844,
      "learning_rate": 5.098722415795587e-06,
      "loss": 1.7674,
      "step": 1317
    },
    {
      "epoch": 0.5102593883081687,
      "grad_norm": 7.707313060760498,
      "learning_rate": 5.102593883081688e-06,
      "loss": 1.6598,
      "step": 1318
    },
    {
      "epoch": 0.5106465350367789,
      "grad_norm": 9.216583251953125,
      "learning_rate": 5.106465350367789e-06,
      "loss": 1.7438,
      "step": 1319
    },
    {
      "epoch": 0.5110336817653891,
      "grad_norm": 8.76384449005127,
      "learning_rate": 5.110336817653892e-06,
      "loss": 1.66,
      "step": 1320
    },
    {
      "epoch": 0.5114208284939993,
      "grad_norm": 9.909137725830078,
      "learning_rate": 5.114208284939993e-06,
      "loss": 1.918,
      "step": 1321
    },
    {
      "epoch": 0.5118079752226093,
      "grad_norm": 15.340450286865234,
      "learning_rate": 5.118079752226094e-06,
      "loss": 1.9091,
      "step": 1322
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 14.217267990112305,
      "learning_rate": 5.121951219512195e-06,
      "loss": 1.6284,
      "step": 1323
    },
    {
      "epoch": 0.5125822686798297,
      "grad_norm": 9.805551528930664,
      "learning_rate": 5.125822686798298e-06,
      "loss": 1.9058,
      "step": 1324
    },
    {
      "epoch": 0.5129694154084398,
      "grad_norm": 7.676059246063232,
      "learning_rate": 5.129694154084398e-06,
      "loss": 1.7423,
      "step": 1325
    },
    {
      "epoch": 0.5133565621370499,
      "grad_norm": 7.661569118499756,
      "learning_rate": 5.1335656213705e-06,
      "loss": 1.8519,
      "step": 1326
    },
    {
      "epoch": 0.5137437088656601,
      "grad_norm": 10.50899600982666,
      "learning_rate": 5.137437088656601e-06,
      "loss": 1.7718,
      "step": 1327
    },
    {
      "epoch": 0.5141308555942702,
      "grad_norm": 9.510676383972168,
      "learning_rate": 5.141308555942702e-06,
      "loss": 1.5858,
      "step": 1328
    },
    {
      "epoch": 0.5145180023228804,
      "grad_norm": 8.900527954101562,
      "learning_rate": 5.145180023228804e-06,
      "loss": 1.7474,
      "step": 1329
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 8.274018287658691,
      "learning_rate": 5.149051490514906e-06,
      "loss": 1.709,
      "step": 1330
    },
    {
      "epoch": 0.5152922957801006,
      "grad_norm": 7.996002197265625,
      "learning_rate": 5.1529229578010064e-06,
      "loss": 1.9953,
      "step": 1331
    },
    {
      "epoch": 0.5156794425087108,
      "grad_norm": 8.977570533752441,
      "learning_rate": 5.156794425087108e-06,
      "loss": 1.6712,
      "step": 1332
    },
    {
      "epoch": 0.516066589237321,
      "grad_norm": 9.667548179626465,
      "learning_rate": 5.16066589237321e-06,
      "loss": 1.8049,
      "step": 1333
    },
    {
      "epoch": 0.5164537359659311,
      "grad_norm": 9.509604454040527,
      "learning_rate": 5.164537359659312e-06,
      "loss": 1.7928,
      "step": 1334
    },
    {
      "epoch": 0.5168408826945412,
      "grad_norm": 9.484648704528809,
      "learning_rate": 5.168408826945412e-06,
      "loss": 1.8007,
      "step": 1335
    },
    {
      "epoch": 0.5172280294231514,
      "grad_norm": 8.456109046936035,
      "learning_rate": 5.172280294231514e-06,
      "loss": 1.6151,
      "step": 1336
    },
    {
      "epoch": 0.5176151761517616,
      "grad_norm": 13.701489448547363,
      "learning_rate": 5.176151761517616e-06,
      "loss": 1.8202,
      "step": 1337
    },
    {
      "epoch": 0.5180023228803716,
      "grad_norm": 10.891124725341797,
      "learning_rate": 5.180023228803717e-06,
      "loss": 1.8602,
      "step": 1338
    },
    {
      "epoch": 0.5183894696089818,
      "grad_norm": 7.9399518966674805,
      "learning_rate": 5.183894696089818e-06,
      "loss": 1.7691,
      "step": 1339
    },
    {
      "epoch": 0.518776616337592,
      "grad_norm": 14.020740509033203,
      "learning_rate": 5.18776616337592e-06,
      "loss": 1.8357,
      "step": 1340
    },
    {
      "epoch": 0.519163763066202,
      "grad_norm": 11.67481803894043,
      "learning_rate": 5.1916376306620205e-06,
      "loss": 1.6927,
      "step": 1341
    },
    {
      "epoch": 0.5195509097948122,
      "grad_norm": 15.899163246154785,
      "learning_rate": 5.195509097948123e-06,
      "loss": 1.9472,
      "step": 1342
    },
    {
      "epoch": 0.5199380565234224,
      "grad_norm": 9.076348304748535,
      "learning_rate": 5.199380565234224e-06,
      "loss": 1.7944,
      "step": 1343
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 12.691703796386719,
      "learning_rate": 5.203252032520326e-06,
      "loss": 1.9228,
      "step": 1344
    },
    {
      "epoch": 0.5207123499806426,
      "grad_norm": 9.287591934204102,
      "learning_rate": 5.2071234998064265e-06,
      "loss": 1.5984,
      "step": 1345
    },
    {
      "epoch": 0.5210994967092528,
      "grad_norm": 7.831995964050293,
      "learning_rate": 5.210994967092529e-06,
      "loss": 1.6131,
      "step": 1346
    },
    {
      "epoch": 0.521486643437863,
      "grad_norm": 6.71788215637207,
      "learning_rate": 5.21486643437863e-06,
      "loss": 1.5869,
      "step": 1347
    },
    {
      "epoch": 0.5218737901664731,
      "grad_norm": 9.345585823059082,
      "learning_rate": 5.218737901664731e-06,
      "loss": 1.6317,
      "step": 1348
    },
    {
      "epoch": 0.5222609368950832,
      "grad_norm": 8.090785026550293,
      "learning_rate": 5.2226093689508325e-06,
      "loss": 1.5941,
      "step": 1349
    },
    {
      "epoch": 0.5226480836236934,
      "grad_norm": 12.546285629272461,
      "learning_rate": 5.226480836236935e-06,
      "loss": 1.7428,
      "step": 1350
    },
    {
      "epoch": 0.5230352303523035,
      "grad_norm": 11.025548934936523,
      "learning_rate": 5.2303523035230355e-06,
      "loss": 1.9032,
      "step": 1351
    },
    {
      "epoch": 0.5234223770809137,
      "grad_norm": 8.071944236755371,
      "learning_rate": 5.234223770809137e-06,
      "loss": 1.7206,
      "step": 1352
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 7.341916084289551,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 1.7799,
      "step": 1353
    },
    {
      "epoch": 0.5241966705381339,
      "grad_norm": 10.793625831604004,
      "learning_rate": 5.241966705381339e-06,
      "loss": 1.703,
      "step": 1354
    },
    {
      "epoch": 0.5245838172667441,
      "grad_norm": 7.954017162322998,
      "learning_rate": 5.2458381726674414e-06,
      "loss": 1.7322,
      "step": 1355
    },
    {
      "epoch": 0.5249709639953543,
      "grad_norm": 7.091125965118408,
      "learning_rate": 5.249709639953543e-06,
      "loss": 1.8452,
      "step": 1356
    },
    {
      "epoch": 0.5253581107239644,
      "grad_norm": 9.655097007751465,
      "learning_rate": 5.253581107239644e-06,
      "loss": 1.6682,
      "step": 1357
    },
    {
      "epoch": 0.5257452574525745,
      "grad_norm": 15.474637985229492,
      "learning_rate": 5.257452574525745e-06,
      "loss": 1.765,
      "step": 1358
    },
    {
      "epoch": 0.5261324041811847,
      "grad_norm": 11.418221473693848,
      "learning_rate": 5.261324041811847e-06,
      "loss": 1.7673,
      "step": 1359
    },
    {
      "epoch": 0.5265195509097949,
      "grad_norm": 9.937798500061035,
      "learning_rate": 5.265195509097949e-06,
      "loss": 1.7564,
      "step": 1360
    },
    {
      "epoch": 0.5269066976384049,
      "grad_norm": 6.057168483734131,
      "learning_rate": 5.2690669763840495e-06,
      "loss": 1.8477,
      "step": 1361
    },
    {
      "epoch": 0.5272938443670151,
      "grad_norm": 8.355338096618652,
      "learning_rate": 5.272938443670151e-06,
      "loss": 1.6224,
      "step": 1362
    },
    {
      "epoch": 0.5276809910956253,
      "grad_norm": 13.478459358215332,
      "learning_rate": 5.276809910956253e-06,
      "loss": 2.0608,
      "step": 1363
    },
    {
      "epoch": 0.5280681378242353,
      "grad_norm": 9.84047794342041,
      "learning_rate": 5.280681378242354e-06,
      "loss": 1.6636,
      "step": 1364
    },
    {
      "epoch": 0.5284552845528455,
      "grad_norm": 10.584559440612793,
      "learning_rate": 5.2845528455284555e-06,
      "loss": 1.7397,
      "step": 1365
    },
    {
      "epoch": 0.5288424312814557,
      "grad_norm": 14.384819030761719,
      "learning_rate": 5.288424312814557e-06,
      "loss": 1.7679,
      "step": 1366
    },
    {
      "epoch": 0.5292295780100658,
      "grad_norm": 12.933555603027344,
      "learning_rate": 5.292295780100658e-06,
      "loss": 1.7447,
      "step": 1367
    },
    {
      "epoch": 0.5296167247386759,
      "grad_norm": 8.521564483642578,
      "learning_rate": 5.29616724738676e-06,
      "loss": 1.7322,
      "step": 1368
    },
    {
      "epoch": 0.5300038714672861,
      "grad_norm": 8.680744171142578,
      "learning_rate": 5.3000387146728615e-06,
      "loss": 1.7035,
      "step": 1369
    },
    {
      "epoch": 0.5303910181958963,
      "grad_norm": 13.195222854614258,
      "learning_rate": 5.303910181958963e-06,
      "loss": 1.6419,
      "step": 1370
    },
    {
      "epoch": 0.5307781649245064,
      "grad_norm": 8.50390911102295,
      "learning_rate": 5.307781649245064e-06,
      "loss": 1.8746,
      "step": 1371
    },
    {
      "epoch": 0.5311653116531165,
      "grad_norm": 9.71081256866455,
      "learning_rate": 5.311653116531166e-06,
      "loss": 1.6766,
      "step": 1372
    },
    {
      "epoch": 0.5315524583817267,
      "grad_norm": 12.901923179626465,
      "learning_rate": 5.3155245838172675e-06,
      "loss": 1.8042,
      "step": 1373
    },
    {
      "epoch": 0.5319396051103368,
      "grad_norm": 7.895209312438965,
      "learning_rate": 5.319396051103368e-06,
      "loss": 1.6994,
      "step": 1374
    },
    {
      "epoch": 0.532326751838947,
      "grad_norm": 7.3983001708984375,
      "learning_rate": 5.32326751838947e-06,
      "loss": 1.7151,
      "step": 1375
    },
    {
      "epoch": 0.5327138985675571,
      "grad_norm": 15.921335220336914,
      "learning_rate": 5.327138985675572e-06,
      "loss": 1.7117,
      "step": 1376
    },
    {
      "epoch": 0.5331010452961672,
      "grad_norm": 9.918570518493652,
      "learning_rate": 5.331010452961673e-06,
      "loss": 1.9024,
      "step": 1377
    },
    {
      "epoch": 0.5334881920247774,
      "grad_norm": 7.98599100112915,
      "learning_rate": 5.334881920247774e-06,
      "loss": 1.6681,
      "step": 1378
    },
    {
      "epoch": 0.5338753387533876,
      "grad_norm": 7.361870288848877,
      "learning_rate": 5.338753387533876e-06,
      "loss": 1.6636,
      "step": 1379
    },
    {
      "epoch": 0.5342624854819977,
      "grad_norm": 8.02617073059082,
      "learning_rate": 5.342624854819978e-06,
      "loss": 1.6671,
      "step": 1380
    },
    {
      "epoch": 0.5346496322106078,
      "grad_norm": 13.056593894958496,
      "learning_rate": 5.3464963221060786e-06,
      "loss": 1.7162,
      "step": 1381
    },
    {
      "epoch": 0.535036778939218,
      "grad_norm": 9.123769760131836,
      "learning_rate": 5.35036778939218e-06,
      "loss": 1.6141,
      "step": 1382
    },
    {
      "epoch": 0.5354239256678281,
      "grad_norm": 7.697166442871094,
      "learning_rate": 5.3542392566782816e-06,
      "loss": 1.7032,
      "step": 1383
    },
    {
      "epoch": 0.5358110723964382,
      "grad_norm": 9.595966339111328,
      "learning_rate": 5.358110723964382e-06,
      "loss": 2.2018,
      "step": 1384
    },
    {
      "epoch": 0.5361982191250484,
      "grad_norm": 12.388055801391602,
      "learning_rate": 5.3619821912504845e-06,
      "loss": 1.791,
      "step": 1385
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 8.258294105529785,
      "learning_rate": 5.365853658536586e-06,
      "loss": 1.7152,
      "step": 1386
    },
    {
      "epoch": 0.5369725125822686,
      "grad_norm": 7.559144973754883,
      "learning_rate": 5.369725125822687e-06,
      "loss": 1.594,
      "step": 1387
    },
    {
      "epoch": 0.5373596593108788,
      "grad_norm": 6.790043354034424,
      "learning_rate": 5.373596593108788e-06,
      "loss": 1.7443,
      "step": 1388
    },
    {
      "epoch": 0.537746806039489,
      "grad_norm": 6.464563369750977,
      "learning_rate": 5.3774680603948905e-06,
      "loss": 1.5464,
      "step": 1389
    },
    {
      "epoch": 0.538133952768099,
      "grad_norm": 8.155656814575195,
      "learning_rate": 5.381339527680991e-06,
      "loss": 1.8746,
      "step": 1390
    },
    {
      "epoch": 0.5385210994967092,
      "grad_norm": 9.205279350280762,
      "learning_rate": 5.385210994967093e-06,
      "loss": 1.6912,
      "step": 1391
    },
    {
      "epoch": 0.5389082462253194,
      "grad_norm": 7.957200527191162,
      "learning_rate": 5.389082462253194e-06,
      "loss": 1.8355,
      "step": 1392
    },
    {
      "epoch": 0.5392953929539296,
      "grad_norm": 8.353617668151855,
      "learning_rate": 5.3929539295392965e-06,
      "loss": 1.7264,
      "step": 1393
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 8.412663459777832,
      "learning_rate": 5.396825396825397e-06,
      "loss": 1.796,
      "step": 1394
    },
    {
      "epoch": 0.5400696864111498,
      "grad_norm": 8.655327796936035,
      "learning_rate": 5.400696864111499e-06,
      "loss": 1.5663,
      "step": 1395
    },
    {
      "epoch": 0.54045683313976,
      "grad_norm": 9.140593528747559,
      "learning_rate": 5.4045683313976e-06,
      "loss": 1.7888,
      "step": 1396
    },
    {
      "epoch": 0.5408439798683701,
      "grad_norm": 12.821698188781738,
      "learning_rate": 5.408439798683701e-06,
      "loss": 1.7683,
      "step": 1397
    },
    {
      "epoch": 0.5412311265969802,
      "grad_norm": 10.31419849395752,
      "learning_rate": 5.412311265969803e-06,
      "loss": 1.8958,
      "step": 1398
    },
    {
      "epoch": 0.5416182733255904,
      "grad_norm": 11.663201332092285,
      "learning_rate": 5.416182733255905e-06,
      "loss": 1.6816,
      "step": 1399
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 17.67310905456543,
      "learning_rate": 5.420054200542005e-06,
      "loss": 1.8997,
      "step": 1400
    },
    {
      "epoch": 0.5423925667828107,
      "grad_norm": 7.681925296783447,
      "learning_rate": 5.423925667828107e-06,
      "loss": 1.6003,
      "step": 1401
    },
    {
      "epoch": 0.5427797135114208,
      "grad_norm": 8.727168083190918,
      "learning_rate": 5.427797135114209e-06,
      "loss": 1.592,
      "step": 1402
    },
    {
      "epoch": 0.543166860240031,
      "grad_norm": 9.692272186279297,
      "learning_rate": 5.4316686024003106e-06,
      "loss": 1.8054,
      "step": 1403
    },
    {
      "epoch": 0.5435540069686411,
      "grad_norm": 8.61725902557373,
      "learning_rate": 5.435540069686411e-06,
      "loss": 1.7554,
      "step": 1404
    },
    {
      "epoch": 0.5439411536972513,
      "grad_norm": 9.115333557128906,
      "learning_rate": 5.439411536972513e-06,
      "loss": 1.539,
      "step": 1405
    },
    {
      "epoch": 0.5443283004258614,
      "grad_norm": 9.260086059570312,
      "learning_rate": 5.443283004258615e-06,
      "loss": 1.6606,
      "step": 1406
    },
    {
      "epoch": 0.5447154471544715,
      "grad_norm": 11.25811767578125,
      "learning_rate": 5.447154471544716e-06,
      "loss": 1.6883,
      "step": 1407
    },
    {
      "epoch": 0.5451025938830817,
      "grad_norm": 9.235289573669434,
      "learning_rate": 5.451025938830817e-06,
      "loss": 1.5065,
      "step": 1408
    },
    {
      "epoch": 0.5454897406116919,
      "grad_norm": 23.70233154296875,
      "learning_rate": 5.454897406116919e-06,
      "loss": 1.9649,
      "step": 1409
    },
    {
      "epoch": 0.5458768873403019,
      "grad_norm": 10.807398796081543,
      "learning_rate": 5.458768873403019e-06,
      "loss": 1.8071,
      "step": 1410
    },
    {
      "epoch": 0.5462640340689121,
      "grad_norm": 9.413725852966309,
      "learning_rate": 5.462640340689122e-06,
      "loss": 1.5424,
      "step": 1411
    },
    {
      "epoch": 0.5466511807975223,
      "grad_norm": 15.844057083129883,
      "learning_rate": 5.466511807975223e-06,
      "loss": 2.2533,
      "step": 1412
    },
    {
      "epoch": 0.5470383275261324,
      "grad_norm": 8.093308448791504,
      "learning_rate": 5.470383275261324e-06,
      "loss": 1.6579,
      "step": 1413
    },
    {
      "epoch": 0.5474254742547425,
      "grad_norm": 8.075512886047363,
      "learning_rate": 5.474254742547425e-06,
      "loss": 1.761,
      "step": 1414
    },
    {
      "epoch": 0.5478126209833527,
      "grad_norm": 8.526487350463867,
      "learning_rate": 5.478126209833528e-06,
      "loss": 1.6375,
      "step": 1415
    },
    {
      "epoch": 0.5481997677119629,
      "grad_norm": 9.4284086227417,
      "learning_rate": 5.481997677119629e-06,
      "loss": 1.8597,
      "step": 1416
    },
    {
      "epoch": 0.548586914440573,
      "grad_norm": 7.141223430633545,
      "learning_rate": 5.48586914440573e-06,
      "loss": 1.4689,
      "step": 1417
    },
    {
      "epoch": 0.5489740611691831,
      "grad_norm": 10.1143217086792,
      "learning_rate": 5.489740611691831e-06,
      "loss": 1.5996,
      "step": 1418
    },
    {
      "epoch": 0.5493612078977933,
      "grad_norm": 9.950176239013672,
      "learning_rate": 5.493612078977934e-06,
      "loss": 1.8674,
      "step": 1419
    },
    {
      "epoch": 0.5497483546264034,
      "grad_norm": 8.04936408996582,
      "learning_rate": 5.497483546264034e-06,
      "loss": 1.6556,
      "step": 1420
    },
    {
      "epoch": 0.5501355013550135,
      "grad_norm": 12.24074649810791,
      "learning_rate": 5.501355013550136e-06,
      "loss": 1.6691,
      "step": 1421
    },
    {
      "epoch": 0.5505226480836237,
      "grad_norm": 11.19809627532959,
      "learning_rate": 5.505226480836237e-06,
      "loss": 1.7494,
      "step": 1422
    },
    {
      "epoch": 0.5509097948122338,
      "grad_norm": 7.17512845993042,
      "learning_rate": 5.509097948122339e-06,
      "loss": 1.7289,
      "step": 1423
    },
    {
      "epoch": 0.551296941540844,
      "grad_norm": 13.313982963562012,
      "learning_rate": 5.51296941540844e-06,
      "loss": 1.4654,
      "step": 1424
    },
    {
      "epoch": 0.5516840882694541,
      "grad_norm": 9.673526763916016,
      "learning_rate": 5.516840882694542e-06,
      "loss": 1.555,
      "step": 1425
    },
    {
      "epoch": 0.5520712349980643,
      "grad_norm": 9.7999906539917,
      "learning_rate": 5.520712349980643e-06,
      "loss": 1.6722,
      "step": 1426
    },
    {
      "epoch": 0.5524583817266744,
      "grad_norm": 8.88854694366455,
      "learning_rate": 5.524583817266745e-06,
      "loss": 1.8199,
      "step": 1427
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 10.987530708312988,
      "learning_rate": 5.528455284552846e-06,
      "loss": 1.6953,
      "step": 1428
    },
    {
      "epoch": 0.5532326751838947,
      "grad_norm": 9.979049682617188,
      "learning_rate": 5.532326751838948e-06,
      "loss": 1.5629,
      "step": 1429
    },
    {
      "epoch": 0.5536198219125048,
      "grad_norm": 7.817266941070557,
      "learning_rate": 5.536198219125048e-06,
      "loss": 1.5905,
      "step": 1430
    },
    {
      "epoch": 0.554006968641115,
      "grad_norm": 7.565865993499756,
      "learning_rate": 5.540069686411151e-06,
      "loss": 1.5959,
      "step": 1431
    },
    {
      "epoch": 0.5543941153697252,
      "grad_norm": 10.322434425354004,
      "learning_rate": 5.543941153697252e-06,
      "loss": 2.0315,
      "step": 1432
    },
    {
      "epoch": 0.5547812620983352,
      "grad_norm": 7.255078315734863,
      "learning_rate": 5.547812620983353e-06,
      "loss": 1.6373,
      "step": 1433
    },
    {
      "epoch": 0.5551684088269454,
      "grad_norm": 8.379776000976562,
      "learning_rate": 5.551684088269454e-06,
      "loss": 1.774,
      "step": 1434
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 9.933935165405273,
      "learning_rate": 5.555555555555557e-06,
      "loss": 2.0162,
      "step": 1435
    },
    {
      "epoch": 0.5559427022841656,
      "grad_norm": 8.29250431060791,
      "learning_rate": 5.559427022841657e-06,
      "loss": 1.7496,
      "step": 1436
    },
    {
      "epoch": 0.5563298490127758,
      "grad_norm": 12.468098640441895,
      "learning_rate": 5.563298490127759e-06,
      "loss": 1.6349,
      "step": 1437
    },
    {
      "epoch": 0.556716995741386,
      "grad_norm": 13.199508666992188,
      "learning_rate": 5.56716995741386e-06,
      "loss": 1.6256,
      "step": 1438
    },
    {
      "epoch": 0.5571041424699962,
      "grad_norm": 10.697770118713379,
      "learning_rate": 5.571041424699963e-06,
      "loss": 1.5869,
      "step": 1439
    },
    {
      "epoch": 0.5574912891986062,
      "grad_norm": 10.315377235412598,
      "learning_rate": 5.574912891986063e-06,
      "loss": 1.9821,
      "step": 1440
    },
    {
      "epoch": 0.5578784359272164,
      "grad_norm": 5.946903228759766,
      "learning_rate": 5.578784359272165e-06,
      "loss": 1.7128,
      "step": 1441
    },
    {
      "epoch": 0.5582655826558266,
      "grad_norm": 8.534420013427734,
      "learning_rate": 5.582655826558266e-06,
      "loss": 1.7344,
      "step": 1442
    },
    {
      "epoch": 0.5586527293844367,
      "grad_norm": 9.231522560119629,
      "learning_rate": 5.586527293844367e-06,
      "loss": 1.5411,
      "step": 1443
    },
    {
      "epoch": 0.5590398761130468,
      "grad_norm": 9.710814476013184,
      "learning_rate": 5.590398761130469e-06,
      "loss": 1.5228,
      "step": 1444
    },
    {
      "epoch": 0.559427022841657,
      "grad_norm": 9.185322761535645,
      "learning_rate": 5.594270228416571e-06,
      "loss": 1.7659,
      "step": 1445
    },
    {
      "epoch": 0.5598141695702671,
      "grad_norm": 9.67588996887207,
      "learning_rate": 5.598141695702671e-06,
      "loss": 1.7684,
      "step": 1446
    },
    {
      "epoch": 0.5602013162988773,
      "grad_norm": 9.422475814819336,
      "learning_rate": 5.602013162988773e-06,
      "loss": 1.7035,
      "step": 1447
    },
    {
      "epoch": 0.5605884630274874,
      "grad_norm": 9.273335456848145,
      "learning_rate": 5.605884630274875e-06,
      "loss": 1.7381,
      "step": 1448
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 10.089031219482422,
      "learning_rate": 5.609756097560977e-06,
      "loss": 1.6133,
      "step": 1449
    },
    {
      "epoch": 0.5613627564847077,
      "grad_norm": 10.175055503845215,
      "learning_rate": 5.613627564847077e-06,
      "loss": 1.7544,
      "step": 1450
    },
    {
      "epoch": 0.5617499032133179,
      "grad_norm": 8.147137641906738,
      "learning_rate": 5.617499032133179e-06,
      "loss": 1.7314,
      "step": 1451
    },
    {
      "epoch": 0.562137049941928,
      "grad_norm": 10.88351821899414,
      "learning_rate": 5.621370499419281e-06,
      "loss": 1.7979,
      "step": 1452
    },
    {
      "epoch": 0.5625241966705381,
      "grad_norm": 10.035901069641113,
      "learning_rate": 5.625241966705382e-06,
      "loss": 1.8173,
      "step": 1453
    },
    {
      "epoch": 0.5629113433991483,
      "grad_norm": 8.756290435791016,
      "learning_rate": 5.629113433991483e-06,
      "loss": 1.704,
      "step": 1454
    },
    {
      "epoch": 0.5632984901277585,
      "grad_norm": 11.939477920532227,
      "learning_rate": 5.632984901277585e-06,
      "loss": 1.8192,
      "step": 1455
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 9.53458023071289,
      "learning_rate": 5.6368563685636855e-06,
      "loss": 1.7223,
      "step": 1456
    },
    {
      "epoch": 0.5640727835849787,
      "grad_norm": 13.202938079833984,
      "learning_rate": 5.640727835849788e-06,
      "loss": 2.1213,
      "step": 1457
    },
    {
      "epoch": 0.5644599303135889,
      "grad_norm": 10.124244689941406,
      "learning_rate": 5.644599303135889e-06,
      "loss": 1.7353,
      "step": 1458
    },
    {
      "epoch": 0.5648470770421989,
      "grad_norm": 12.167207717895508,
      "learning_rate": 5.64847077042199e-06,
      "loss": 2.0167,
      "step": 1459
    },
    {
      "epoch": 0.5652342237708091,
      "grad_norm": 10.377643585205078,
      "learning_rate": 5.6523422377080915e-06,
      "loss": 1.5211,
      "step": 1460
    },
    {
      "epoch": 0.5656213704994193,
      "grad_norm": 13.694287300109863,
      "learning_rate": 5.656213704994194e-06,
      "loss": 1.6214,
      "step": 1461
    },
    {
      "epoch": 0.5660085172280295,
      "grad_norm": 12.551365852355957,
      "learning_rate": 5.660085172280295e-06,
      "loss": 1.6732,
      "step": 1462
    },
    {
      "epoch": 0.5663956639566395,
      "grad_norm": 10.562268257141113,
      "learning_rate": 5.663956639566396e-06,
      "loss": 1.7806,
      "step": 1463
    },
    {
      "epoch": 0.5667828106852497,
      "grad_norm": 7.578174114227295,
      "learning_rate": 5.6678281068524974e-06,
      "loss": 1.6514,
      "step": 1464
    },
    {
      "epoch": 0.5671699574138599,
      "grad_norm": 10.057487487792969,
      "learning_rate": 5.6716995741386e-06,
      "loss": 1.6932,
      "step": 1465
    },
    {
      "epoch": 0.56755710414247,
      "grad_norm": 17.37509536743164,
      "learning_rate": 5.6755710414247004e-06,
      "loss": 1.8797,
      "step": 1466
    },
    {
      "epoch": 0.5679442508710801,
      "grad_norm": 15.189029693603516,
      "learning_rate": 5.679442508710802e-06,
      "loss": 1.62,
      "step": 1467
    },
    {
      "epoch": 0.5683313975996903,
      "grad_norm": 15.259355545043945,
      "learning_rate": 5.683313975996903e-06,
      "loss": 1.8883,
      "step": 1468
    },
    {
      "epoch": 0.5687185443283004,
      "grad_norm": 11.407679557800293,
      "learning_rate": 5.687185443283004e-06,
      "loss": 1.6147,
      "step": 1469
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 13.385226249694824,
      "learning_rate": 5.691056910569106e-06,
      "loss": 2.015,
      "step": 1470
    },
    {
      "epoch": 0.5694928377855207,
      "grad_norm": 13.078021049499512,
      "learning_rate": 5.694928377855208e-06,
      "loss": 2.1355,
      "step": 1471
    },
    {
      "epoch": 0.5698799845141309,
      "grad_norm": 16.56485366821289,
      "learning_rate": 5.698799845141309e-06,
      "loss": 1.6212,
      "step": 1472
    },
    {
      "epoch": 0.570267131242741,
      "grad_norm": 10.44498062133789,
      "learning_rate": 5.70267131242741e-06,
      "loss": 1.5316,
      "step": 1473
    },
    {
      "epoch": 0.5706542779713512,
      "grad_norm": 11.248676300048828,
      "learning_rate": 5.706542779713512e-06,
      "loss": 1.7543,
      "step": 1474
    },
    {
      "epoch": 0.5710414246999613,
      "grad_norm": 8.887187957763672,
      "learning_rate": 5.710414246999614e-06,
      "loss": 1.6853,
      "step": 1475
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 14.102459907531738,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.4686,
      "step": 1476
    },
    {
      "epoch": 0.5718157181571816,
      "grad_norm": 16.885581970214844,
      "learning_rate": 5.718157181571816e-06,
      "loss": 1.8241,
      "step": 1477
    },
    {
      "epoch": 0.5722028648857918,
      "grad_norm": 10.367696762084961,
      "learning_rate": 5.722028648857918e-06,
      "loss": 1.7372,
      "step": 1478
    },
    {
      "epoch": 0.5725900116144018,
      "grad_norm": 10.994058609008789,
      "learning_rate": 5.725900116144019e-06,
      "loss": 1.5818,
      "step": 1479
    },
    {
      "epoch": 0.572977158343012,
      "grad_norm": 9.929997444152832,
      "learning_rate": 5.7297715834301205e-06,
      "loss": 1.596,
      "step": 1480
    },
    {
      "epoch": 0.5733643050716222,
      "grad_norm": 11.595735549926758,
      "learning_rate": 5.733643050716222e-06,
      "loss": 1.7573,
      "step": 1481
    },
    {
      "epoch": 0.5737514518002322,
      "grad_norm": 9.880404472351074,
      "learning_rate": 5.737514518002323e-06,
      "loss": 1.6642,
      "step": 1482
    },
    {
      "epoch": 0.5741385985288424,
      "grad_norm": 13.446292877197266,
      "learning_rate": 5.741385985288425e-06,
      "loss": 1.7607,
      "step": 1483
    },
    {
      "epoch": 0.5745257452574526,
      "grad_norm": 11.114376068115234,
      "learning_rate": 5.7452574525745265e-06,
      "loss": 1.8247,
      "step": 1484
    },
    {
      "epoch": 0.5749128919860628,
      "grad_norm": 11.16783618927002,
      "learning_rate": 5.749128919860628e-06,
      "loss": 1.6077,
      "step": 1485
    },
    {
      "epoch": 0.5753000387146728,
      "grad_norm": 8.862831115722656,
      "learning_rate": 5.753000387146729e-06,
      "loss": 1.7596,
      "step": 1486
    },
    {
      "epoch": 0.575687185443283,
      "grad_norm": 10.285255432128906,
      "learning_rate": 5.756871854432831e-06,
      "loss": 1.8237,
      "step": 1487
    },
    {
      "epoch": 0.5760743321718932,
      "grad_norm": 15.75640869140625,
      "learning_rate": 5.7607433217189324e-06,
      "loss": 1.5814,
      "step": 1488
    },
    {
      "epoch": 0.5764614789005033,
      "grad_norm": 12.1646146774292,
      "learning_rate": 5.764614789005033e-06,
      "loss": 2.1519,
      "step": 1489
    },
    {
      "epoch": 0.5768486256291134,
      "grad_norm": 10.039576530456543,
      "learning_rate": 5.7684862562911346e-06,
      "loss": 1.6119,
      "step": 1490
    },
    {
      "epoch": 0.5772357723577236,
      "grad_norm": 6.684099197387695,
      "learning_rate": 5.772357723577237e-06,
      "loss": 1.8163,
      "step": 1491
    },
    {
      "epoch": 0.5776229190863337,
      "grad_norm": 8.67487907409668,
      "learning_rate": 5.7762291908633376e-06,
      "loss": 1.4125,
      "step": 1492
    },
    {
      "epoch": 0.5780100658149439,
      "grad_norm": 10.82089614868164,
      "learning_rate": 5.780100658149439e-06,
      "loss": 1.5987,
      "step": 1493
    },
    {
      "epoch": 0.578397212543554,
      "grad_norm": 12.447325706481934,
      "learning_rate": 5.7839721254355405e-06,
      "loss": 1.9415,
      "step": 1494
    },
    {
      "epoch": 0.5787843592721641,
      "grad_norm": 11.678605079650879,
      "learning_rate": 5.787843592721641e-06,
      "loss": 1.7508,
      "step": 1495
    },
    {
      "epoch": 0.5791715060007743,
      "grad_norm": 8.295422554016113,
      "learning_rate": 5.7917150600077435e-06,
      "loss": 1.7493,
      "step": 1496
    },
    {
      "epoch": 0.5795586527293844,
      "grad_norm": 11.251540184020996,
      "learning_rate": 5.795586527293845e-06,
      "loss": 1.6122,
      "step": 1497
    },
    {
      "epoch": 0.5799457994579946,
      "grad_norm": 11.07530689239502,
      "learning_rate": 5.7994579945799465e-06,
      "loss": 1.8203,
      "step": 1498
    },
    {
      "epoch": 0.5803329461866047,
      "grad_norm": 7.696312427520752,
      "learning_rate": 5.803329461866047e-06,
      "loss": 1.6412,
      "step": 1499
    },
    {
      "epoch": 0.5807200929152149,
      "grad_norm": 8.422235488891602,
      "learning_rate": 5.8072009291521495e-06,
      "loss": 1.7239,
      "step": 1500
    },
    {
      "epoch": 0.581107239643825,
      "grad_norm": 8.833298683166504,
      "learning_rate": 5.811072396438251e-06,
      "loss": 1.7684,
      "step": 1501
    },
    {
      "epoch": 0.5814943863724351,
      "grad_norm": 12.904641151428223,
      "learning_rate": 5.814943863724352e-06,
      "loss": 1.582,
      "step": 1502
    },
    {
      "epoch": 0.5818815331010453,
      "grad_norm": 10.146073341369629,
      "learning_rate": 5.818815331010453e-06,
      "loss": 1.6466,
      "step": 1503
    },
    {
      "epoch": 0.5822686798296555,
      "grad_norm": 9.032862663269043,
      "learning_rate": 5.8226867982965555e-06,
      "loss": 1.6147,
      "step": 1504
    },
    {
      "epoch": 0.5826558265582655,
      "grad_norm": 13.819158554077148,
      "learning_rate": 5.826558265582656e-06,
      "loss": 1.6945,
      "step": 1505
    },
    {
      "epoch": 0.5830429732868757,
      "grad_norm": 10.75665283203125,
      "learning_rate": 5.830429732868758e-06,
      "loss": 1.9395,
      "step": 1506
    },
    {
      "epoch": 0.5834301200154859,
      "grad_norm": 8.8504638671875,
      "learning_rate": 5.834301200154859e-06,
      "loss": 1.8054,
      "step": 1507
    },
    {
      "epoch": 0.5838172667440961,
      "grad_norm": 18.66946792602539,
      "learning_rate": 5.8381726674409615e-06,
      "loss": 1.7902,
      "step": 1508
    },
    {
      "epoch": 0.5842044134727061,
      "grad_norm": 9.193771362304688,
      "learning_rate": 5.842044134727062e-06,
      "loss": 1.8253,
      "step": 1509
    },
    {
      "epoch": 0.5845915602013163,
      "grad_norm": 9.99756908416748,
      "learning_rate": 5.845915602013164e-06,
      "loss": 1.6649,
      "step": 1510
    },
    {
      "epoch": 0.5849787069299265,
      "grad_norm": 14.317920684814453,
      "learning_rate": 5.849787069299265e-06,
      "loss": 1.6757,
      "step": 1511
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 13.28160285949707,
      "learning_rate": 5.853658536585366e-06,
      "loss": 1.9246,
      "step": 1512
    },
    {
      "epoch": 0.5857530003871467,
      "grad_norm": 9.238507270812988,
      "learning_rate": 5.857530003871468e-06,
      "loss": 1.6773,
      "step": 1513
    },
    {
      "epoch": 0.5861401471157569,
      "grad_norm": 6.352151870727539,
      "learning_rate": 5.8614014711575696e-06,
      "loss": 1.7755,
      "step": 1514
    },
    {
      "epoch": 0.586527293844367,
      "grad_norm": 9.958553314208984,
      "learning_rate": 5.86527293844367e-06,
      "loss": 1.7826,
      "step": 1515
    },
    {
      "epoch": 0.5869144405729771,
      "grad_norm": 14.048624038696289,
      "learning_rate": 5.869144405729772e-06,
      "loss": 1.8408,
      "step": 1516
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 13.656244277954102,
      "learning_rate": 5.873015873015874e-06,
      "loss": 1.5944,
      "step": 1517
    },
    {
      "epoch": 0.5876887340301974,
      "grad_norm": 17.456457138061523,
      "learning_rate": 5.876887340301975e-06,
      "loss": 1.6646,
      "step": 1518
    },
    {
      "epoch": 0.5880758807588076,
      "grad_norm": 41.354862213134766,
      "learning_rate": 5.880758807588076e-06,
      "loss": 1.8757,
      "step": 1519
    },
    {
      "epoch": 0.5884630274874177,
      "grad_norm": 8.14378833770752,
      "learning_rate": 5.884630274874178e-06,
      "loss": 1.6647,
      "step": 1520
    },
    {
      "epoch": 0.5888501742160279,
      "grad_norm": 10.520184516906738,
      "learning_rate": 5.88850174216028e-06,
      "loss": 2.1739,
      "step": 1521
    },
    {
      "epoch": 0.589237320944638,
      "grad_norm": 8.685115814208984,
      "learning_rate": 5.892373209446381e-06,
      "loss": 1.7264,
      "step": 1522
    },
    {
      "epoch": 0.5896244676732482,
      "grad_norm": 10.867173194885254,
      "learning_rate": 5.896244676732482e-06,
      "loss": 1.8826,
      "step": 1523
    },
    {
      "epoch": 0.5900116144018583,
      "grad_norm": 14.543697357177734,
      "learning_rate": 5.900116144018584e-06,
      "loss": 1.5724,
      "step": 1524
    },
    {
      "epoch": 0.5903987611304684,
      "grad_norm": 14.55343246459961,
      "learning_rate": 5.903987611304684e-06,
      "loss": 1.5863,
      "step": 1525
    },
    {
      "epoch": 0.5907859078590786,
      "grad_norm": 22.200260162353516,
      "learning_rate": 5.907859078590787e-06,
      "loss": 2.027,
      "step": 1526
    },
    {
      "epoch": 0.5911730545876888,
      "grad_norm": 15.841262817382812,
      "learning_rate": 5.911730545876888e-06,
      "loss": 1.6265,
      "step": 1527
    },
    {
      "epoch": 0.5915602013162988,
      "grad_norm": 10.919628143310547,
      "learning_rate": 5.915602013162989e-06,
      "loss": 1.8709,
      "step": 1528
    },
    {
      "epoch": 0.591947348044909,
      "grad_norm": 9.772811889648438,
      "learning_rate": 5.91947348044909e-06,
      "loss": 1.7798,
      "step": 1529
    },
    {
      "epoch": 0.5923344947735192,
      "grad_norm": 12.474100112915039,
      "learning_rate": 5.923344947735193e-06,
      "loss": 1.6601,
      "step": 1530
    },
    {
      "epoch": 0.5927216415021294,
      "grad_norm": 12.051456451416016,
      "learning_rate": 5.927216415021294e-06,
      "loss": 1.5838,
      "step": 1531
    },
    {
      "epoch": 0.5931087882307394,
      "grad_norm": 13.413492202758789,
      "learning_rate": 5.931087882307395e-06,
      "loss": 1.8935,
      "step": 1532
    },
    {
      "epoch": 0.5934959349593496,
      "grad_norm": 7.277101516723633,
      "learning_rate": 5.934959349593496e-06,
      "loss": 1.3854,
      "step": 1533
    },
    {
      "epoch": 0.5938830816879598,
      "grad_norm": 18.329099655151367,
      "learning_rate": 5.938830816879599e-06,
      "loss": 1.8802,
      "step": 1534
    },
    {
      "epoch": 0.5942702284165698,
      "grad_norm": 11.395195007324219,
      "learning_rate": 5.942702284165699e-06,
      "loss": 1.7302,
      "step": 1535
    },
    {
      "epoch": 0.59465737514518,
      "grad_norm": 8.492464065551758,
      "learning_rate": 5.946573751451801e-06,
      "loss": 1.6016,
      "step": 1536
    },
    {
      "epoch": 0.5950445218737902,
      "grad_norm": 12.25341796875,
      "learning_rate": 5.950445218737902e-06,
      "loss": 1.8058,
      "step": 1537
    },
    {
      "epoch": 0.5954316686024003,
      "grad_norm": 10.197683334350586,
      "learning_rate": 5.954316686024003e-06,
      "loss": 1.5771,
      "step": 1538
    },
    {
      "epoch": 0.5958188153310104,
      "grad_norm": 10.790886878967285,
      "learning_rate": 5.958188153310105e-06,
      "loss": 1.5338,
      "step": 1539
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 9.665396690368652,
      "learning_rate": 5.962059620596207e-06,
      "loss": 1.5568,
      "step": 1540
    },
    {
      "epoch": 0.5965931087882307,
      "grad_norm": 10.372039794921875,
      "learning_rate": 5.965931087882307e-06,
      "loss": 1.5795,
      "step": 1541
    },
    {
      "epoch": 0.5969802555168409,
      "grad_norm": 9.478906631469727,
      "learning_rate": 5.969802555168409e-06,
      "loss": 1.7045,
      "step": 1542
    },
    {
      "epoch": 0.597367402245451,
      "grad_norm": 10.947084426879883,
      "learning_rate": 5.973674022454511e-06,
      "loss": 1.4489,
      "step": 1543
    },
    {
      "epoch": 0.5977545489740612,
      "grad_norm": 14.078129768371582,
      "learning_rate": 5.977545489740613e-06,
      "loss": 1.6344,
      "step": 1544
    },
    {
      "epoch": 0.5981416957026713,
      "grad_norm": 11.127141952514648,
      "learning_rate": 5.981416957026713e-06,
      "loss": 1.7046,
      "step": 1545
    },
    {
      "epoch": 0.5985288424312815,
      "grad_norm": 10.913918495178223,
      "learning_rate": 5.985288424312815e-06,
      "loss": 1.7891,
      "step": 1546
    },
    {
      "epoch": 0.5989159891598916,
      "grad_norm": 10.396374702453613,
      "learning_rate": 5.989159891598917e-06,
      "loss": 1.4717,
      "step": 1547
    },
    {
      "epoch": 0.5993031358885017,
      "grad_norm": 7.831698894500732,
      "learning_rate": 5.993031358885018e-06,
      "loss": 1.7413,
      "step": 1548
    },
    {
      "epoch": 0.5996902826171119,
      "grad_norm": 14.152691841125488,
      "learning_rate": 5.996902826171119e-06,
      "loss": 1.8132,
      "step": 1549
    },
    {
      "epoch": 0.6000774293457221,
      "grad_norm": 10.332314491271973,
      "learning_rate": 6.000774293457221e-06,
      "loss": 1.5192,
      "step": 1550
    },
    {
      "epoch": 0.6004645760743321,
      "grad_norm": 10.374032974243164,
      "learning_rate": 6.0046457607433214e-06,
      "loss": 1.8334,
      "step": 1551
    },
    {
      "epoch": 0.6008517228029423,
      "grad_norm": 13.717498779296875,
      "learning_rate": 6.008517228029424e-06,
      "loss": 1.8211,
      "step": 1552
    },
    {
      "epoch": 0.6012388695315525,
      "grad_norm": 12.885575294494629,
      "learning_rate": 6.012388695315525e-06,
      "loss": 1.6023,
      "step": 1553
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 8.932714462280273,
      "learning_rate": 6.016260162601627e-06,
      "loss": 1.5866,
      "step": 1554
    },
    {
      "epoch": 0.6020131629887727,
      "grad_norm": 12.031266212463379,
      "learning_rate": 6.020131629887727e-06,
      "loss": 1.3664,
      "step": 1555
    },
    {
      "epoch": 0.6024003097173829,
      "grad_norm": 15.150345802307129,
      "learning_rate": 6.02400309717383e-06,
      "loss": 1.6353,
      "step": 1556
    },
    {
      "epoch": 0.6027874564459931,
      "grad_norm": 14.332188606262207,
      "learning_rate": 6.027874564459931e-06,
      "loss": 1.5895,
      "step": 1557
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 13.231552124023438,
      "learning_rate": 6.031746031746032e-06,
      "loss": 1.7254,
      "step": 1558
    },
    {
      "epoch": 0.6035617499032133,
      "grad_norm": 8.915203094482422,
      "learning_rate": 6.035617499032133e-06,
      "loss": 1.6773,
      "step": 1559
    },
    {
      "epoch": 0.6039488966318235,
      "grad_norm": 8.854516983032227,
      "learning_rate": 6.039488966318236e-06,
      "loss": 1.613,
      "step": 1560
    },
    {
      "epoch": 0.6043360433604336,
      "grad_norm": 38.64569854736328,
      "learning_rate": 6.043360433604336e-06,
      "loss": 1.3755,
      "step": 1561
    },
    {
      "epoch": 0.6047231900890437,
      "grad_norm": 20.608163833618164,
      "learning_rate": 6.047231900890438e-06,
      "loss": 1.7736,
      "step": 1562
    },
    {
      "epoch": 0.6051103368176539,
      "grad_norm": 14.431235313415527,
      "learning_rate": 6.051103368176539e-06,
      "loss": 1.5949,
      "step": 1563
    },
    {
      "epoch": 0.605497483546264,
      "grad_norm": 9.450784683227539,
      "learning_rate": 6.05497483546264e-06,
      "loss": 1.5292,
      "step": 1564
    },
    {
      "epoch": 0.6058846302748742,
      "grad_norm": 9.671939849853516,
      "learning_rate": 6.058846302748742e-06,
      "loss": 1.5512,
      "step": 1565
    },
    {
      "epoch": 0.6062717770034843,
      "grad_norm": 12.98320198059082,
      "learning_rate": 6.062717770034844e-06,
      "loss": 1.577,
      "step": 1566
    },
    {
      "epoch": 0.6066589237320945,
      "grad_norm": 10.783313751220703,
      "learning_rate": 6.066589237320945e-06,
      "loss": 1.8383,
      "step": 1567
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 10.17593002319336,
      "learning_rate": 6.070460704607046e-06,
      "loss": 1.5238,
      "step": 1568
    },
    {
      "epoch": 0.6074332171893148,
      "grad_norm": 16.289405822753906,
      "learning_rate": 6.074332171893148e-06,
      "loss": 2.0461,
      "step": 1569
    },
    {
      "epoch": 0.6078203639179249,
      "grad_norm": 11.299612998962402,
      "learning_rate": 6.07820363917925e-06,
      "loss": 1.8521,
      "step": 1570
    },
    {
      "epoch": 0.608207510646535,
      "grad_norm": 11.739983558654785,
      "learning_rate": 6.0820751064653505e-06,
      "loss": 2.0838,
      "step": 1571
    },
    {
      "epoch": 0.6085946573751452,
      "grad_norm": 7.775533676147461,
      "learning_rate": 6.085946573751452e-06,
      "loss": 1.6605,
      "step": 1572
    },
    {
      "epoch": 0.6089818041037554,
      "grad_norm": 9.274157524108887,
      "learning_rate": 6.089818041037554e-06,
      "loss": 1.6843,
      "step": 1573
    },
    {
      "epoch": 0.6093689508323654,
      "grad_norm": 10.940832138061523,
      "learning_rate": 6.093689508323655e-06,
      "loss": 1.5896,
      "step": 1574
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 10.693075180053711,
      "learning_rate": 6.0975609756097564e-06,
      "loss": 1.7382,
      "step": 1575
    },
    {
      "epoch": 0.6101432442895858,
      "grad_norm": 12.609443664550781,
      "learning_rate": 6.101432442895858e-06,
      "loss": 2.5954,
      "step": 1576
    },
    {
      "epoch": 0.610530391018196,
      "grad_norm": 10.26055908203125,
      "learning_rate": 6.10530391018196e-06,
      "loss": 1.5159,
      "step": 1577
    },
    {
      "epoch": 0.610917537746806,
      "grad_norm": 16.145095825195312,
      "learning_rate": 6.109175377468061e-06,
      "loss": 1.915,
      "step": 1578
    },
    {
      "epoch": 0.6113046844754162,
      "grad_norm": 10.751679420471191,
      "learning_rate": 6.113046844754162e-06,
      "loss": 1.7961,
      "step": 1579
    },
    {
      "epoch": 0.6116918312040264,
      "grad_norm": 10.17103385925293,
      "learning_rate": 6.116918312040264e-06,
      "loss": 1.7543,
      "step": 1580
    },
    {
      "epoch": 0.6120789779326364,
      "grad_norm": 8.905314445495605,
      "learning_rate": 6.1207897793263645e-06,
      "loss": 1.4692,
      "step": 1581
    },
    {
      "epoch": 0.6124661246612466,
      "grad_norm": 12.344270706176758,
      "learning_rate": 6.124661246612467e-06,
      "loss": 1.5914,
      "step": 1582
    },
    {
      "epoch": 0.6128532713898568,
      "grad_norm": 12.150907516479492,
      "learning_rate": 6.128532713898568e-06,
      "loss": 1.6411,
      "step": 1583
    },
    {
      "epoch": 0.6132404181184669,
      "grad_norm": 8.09635066986084,
      "learning_rate": 6.132404181184669e-06,
      "loss": 1.6552,
      "step": 1584
    },
    {
      "epoch": 0.613627564847077,
      "grad_norm": 9.78537368774414,
      "learning_rate": 6.1362756484707705e-06,
      "loss": 1.9092,
      "step": 1585
    },
    {
      "epoch": 0.6140147115756872,
      "grad_norm": 10.028664588928223,
      "learning_rate": 6.140147115756873e-06,
      "loss": 1.4177,
      "step": 1586
    },
    {
      "epoch": 0.6144018583042973,
      "grad_norm": 22.991329193115234,
      "learning_rate": 6.1440185830429735e-06,
      "loss": 1.6709,
      "step": 1587
    },
    {
      "epoch": 0.6147890050329075,
      "grad_norm": 13.437554359436035,
      "learning_rate": 6.147890050329075e-06,
      "loss": 1.535,
      "step": 1588
    },
    {
      "epoch": 0.6151761517615176,
      "grad_norm": 8.078166007995605,
      "learning_rate": 6.1517615176151765e-06,
      "loss": 1.8083,
      "step": 1589
    },
    {
      "epoch": 0.6155632984901278,
      "grad_norm": 7.74685525894165,
      "learning_rate": 6.155632984901279e-06,
      "loss": 1.7143,
      "step": 1590
    },
    {
      "epoch": 0.6159504452187379,
      "grad_norm": 9.635074615478516,
      "learning_rate": 6.1595044521873795e-06,
      "loss": 1.3984,
      "step": 1591
    },
    {
      "epoch": 0.616337591947348,
      "grad_norm": 12.859708786010742,
      "learning_rate": 6.163375919473481e-06,
      "loss": 1.7372,
      "step": 1592
    },
    {
      "epoch": 0.6167247386759582,
      "grad_norm": 24.00034523010254,
      "learning_rate": 6.1672473867595825e-06,
      "loss": 1.494,
      "step": 1593
    },
    {
      "epoch": 0.6171118854045683,
      "grad_norm": 9.76987075805664,
      "learning_rate": 6.171118854045683e-06,
      "loss": 1.831,
      "step": 1594
    },
    {
      "epoch": 0.6174990321331785,
      "grad_norm": 13.735464096069336,
      "learning_rate": 6.1749903213317855e-06,
      "loss": 1.8629,
      "step": 1595
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 12.546796798706055,
      "learning_rate": 6.178861788617887e-06,
      "loss": 1.7484,
      "step": 1596
    },
    {
      "epoch": 0.6182733255903987,
      "grad_norm": 10.250710487365723,
      "learning_rate": 6.182733255903988e-06,
      "loss": 1.8899,
      "step": 1597
    },
    {
      "epoch": 0.6186604723190089,
      "grad_norm": 12.981754302978516,
      "learning_rate": 6.186604723190089e-06,
      "loss": 2.2793,
      "step": 1598
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 11.438450813293457,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 2.1992,
      "step": 1599
    },
    {
      "epoch": 0.6194347657762292,
      "grad_norm": 11.044639587402344,
      "learning_rate": 6.194347657762293e-06,
      "loss": 1.996,
      "step": 1600
    },
    {
      "epoch": 0.6198219125048393,
      "grad_norm": 9.229256629943848,
      "learning_rate": 6.1982191250483936e-06,
      "loss": 1.6605,
      "step": 1601
    },
    {
      "epoch": 0.6202090592334495,
      "grad_norm": 16.97092056274414,
      "learning_rate": 6.202090592334495e-06,
      "loss": 1.6276,
      "step": 1602
    },
    {
      "epoch": 0.6205962059620597,
      "grad_norm": 10.03750991821289,
      "learning_rate": 6.205962059620597e-06,
      "loss": 1.692,
      "step": 1603
    },
    {
      "epoch": 0.6209833526906697,
      "grad_norm": 11.449248313903809,
      "learning_rate": 6.209833526906698e-06,
      "loss": 1.9971,
      "step": 1604
    },
    {
      "epoch": 0.6213704994192799,
      "grad_norm": 9.563312530517578,
      "learning_rate": 6.2137049941927995e-06,
      "loss": 1.6856,
      "step": 1605
    },
    {
      "epoch": 0.6217576461478901,
      "grad_norm": 17.56059455871582,
      "learning_rate": 6.217576461478901e-06,
      "loss": 1.6123,
      "step": 1606
    },
    {
      "epoch": 0.6221447928765002,
      "grad_norm": 14.759915351867676,
      "learning_rate": 6.221447928765002e-06,
      "loss": 1.9823,
      "step": 1607
    },
    {
      "epoch": 0.6225319396051103,
      "grad_norm": 12.17438793182373,
      "learning_rate": 6.225319396051104e-06,
      "loss": 1.9422,
      "step": 1608
    },
    {
      "epoch": 0.6229190863337205,
      "grad_norm": 11.85266399383545,
      "learning_rate": 6.2291908633372055e-06,
      "loss": 1.8395,
      "step": 1609
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 9.19947338104248,
      "learning_rate": 6.233062330623306e-06,
      "loss": 1.6802,
      "step": 1610
    },
    {
      "epoch": 0.6236933797909407,
      "grad_norm": 14.721713066101074,
      "learning_rate": 6.236933797909408e-06,
      "loss": 1.18,
      "step": 1611
    },
    {
      "epoch": 0.6240805265195509,
      "grad_norm": 10.618770599365234,
      "learning_rate": 6.24080526519551e-06,
      "loss": 1.7229,
      "step": 1612
    },
    {
      "epoch": 0.6244676732481611,
      "grad_norm": 15.817943572998047,
      "learning_rate": 6.2446767324816115e-06,
      "loss": 1.5922,
      "step": 1613
    },
    {
      "epoch": 0.6248548199767712,
      "grad_norm": 14.560968399047852,
      "learning_rate": 6.248548199767712e-06,
      "loss": 1.6989,
      "step": 1614
    },
    {
      "epoch": 0.6252419667053813,
      "grad_norm": 10.002652168273926,
      "learning_rate": 6.252419667053814e-06,
      "loss": 1.4555,
      "step": 1615
    },
    {
      "epoch": 0.6256291134339915,
      "grad_norm": 13.65610122680664,
      "learning_rate": 6.256291134339916e-06,
      "loss": 1.4468,
      "step": 1616
    },
    {
      "epoch": 0.6260162601626016,
      "grad_norm": 12.421309471130371,
      "learning_rate": 6.260162601626017e-06,
      "loss": 1.8664,
      "step": 1617
    },
    {
      "epoch": 0.6264034068912118,
      "grad_norm": 9.310602188110352,
      "learning_rate": 6.264034068912118e-06,
      "loss": 1.5122,
      "step": 1618
    },
    {
      "epoch": 0.6267905536198219,
      "grad_norm": 9.932397842407227,
      "learning_rate": 6.26790553619822e-06,
      "loss": 1.552,
      "step": 1619
    },
    {
      "epoch": 0.627177700348432,
      "grad_norm": 11.499909400939941,
      "learning_rate": 6.27177700348432e-06,
      "loss": 1.8281,
      "step": 1620
    },
    {
      "epoch": 0.6275648470770422,
      "grad_norm": 13.386284828186035,
      "learning_rate": 6.275648470770423e-06,
      "loss": 1.5517,
      "step": 1621
    },
    {
      "epoch": 0.6279519938056524,
      "grad_norm": 14.677328109741211,
      "learning_rate": 6.279519938056524e-06,
      "loss": 1.6707,
      "step": 1622
    },
    {
      "epoch": 0.6283391405342624,
      "grad_norm": 9.997922897338867,
      "learning_rate": 6.283391405342625e-06,
      "loss": 1.7789,
      "step": 1623
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 9.93423080444336,
      "learning_rate": 6.287262872628726e-06,
      "loss": 1.5773,
      "step": 1624
    },
    {
      "epoch": 0.6291134339914828,
      "grad_norm": 11.95890998840332,
      "learning_rate": 6.2911343399148286e-06,
      "loss": 1.9148,
      "step": 1625
    },
    {
      "epoch": 0.629500580720093,
      "grad_norm": 9.166473388671875,
      "learning_rate": 6.29500580720093e-06,
      "loss": 1.7835,
      "step": 1626
    },
    {
      "epoch": 0.629887727448703,
      "grad_norm": 9.763961791992188,
      "learning_rate": 6.298877274487031e-06,
      "loss": 1.426,
      "step": 1627
    },
    {
      "epoch": 0.6302748741773132,
      "grad_norm": 11.169343948364258,
      "learning_rate": 6.302748741773132e-06,
      "loss": 1.7174,
      "step": 1628
    },
    {
      "epoch": 0.6306620209059234,
      "grad_norm": 9.525415420532227,
      "learning_rate": 6.3066202090592345e-06,
      "loss": 1.698,
      "step": 1629
    },
    {
      "epoch": 0.6310491676345334,
      "grad_norm": 11.631133079528809,
      "learning_rate": 6.310491676345335e-06,
      "loss": 1.8154,
      "step": 1630
    },
    {
      "epoch": 0.6314363143631436,
      "grad_norm": 12.042900085449219,
      "learning_rate": 6.314363143631437e-06,
      "loss": 1.8916,
      "step": 1631
    },
    {
      "epoch": 0.6318234610917538,
      "grad_norm": 12.48315715789795,
      "learning_rate": 6.318234610917538e-06,
      "loss": 1.7226,
      "step": 1632
    },
    {
      "epoch": 0.6322106078203639,
      "grad_norm": 13.990615844726562,
      "learning_rate": 6.322106078203639e-06,
      "loss": 1.8374,
      "step": 1633
    },
    {
      "epoch": 0.632597754548974,
      "grad_norm": 10.116567611694336,
      "learning_rate": 6.325977545489741e-06,
      "loss": 1.8486,
      "step": 1634
    },
    {
      "epoch": 0.6329849012775842,
      "grad_norm": 16.7943172454834,
      "learning_rate": 6.329849012775843e-06,
      "loss": 1.7482,
      "step": 1635
    },
    {
      "epoch": 0.6333720480061944,
      "grad_norm": 11.196146011352539,
      "learning_rate": 6.333720480061944e-06,
      "loss": 1.543,
      "step": 1636
    },
    {
      "epoch": 0.6337591947348045,
      "grad_norm": 9.938338279724121,
      "learning_rate": 6.337591947348045e-06,
      "loss": 1.5045,
      "step": 1637
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 9.68420124053955,
      "learning_rate": 6.341463414634147e-06,
      "loss": 1.6563,
      "step": 1638
    },
    {
      "epoch": 0.6345334881920248,
      "grad_norm": 15.46877384185791,
      "learning_rate": 6.345334881920249e-06,
      "loss": 1.5631,
      "step": 1639
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 10.867834091186523,
      "learning_rate": 6.349206349206349e-06,
      "loss": 1.7889,
      "step": 1640
    },
    {
      "epoch": 0.6353077816492451,
      "grad_norm": 12.051791191101074,
      "learning_rate": 6.353077816492451e-06,
      "loss": 1.5484,
      "step": 1641
    },
    {
      "epoch": 0.6356949283778552,
      "grad_norm": 9.796772956848145,
      "learning_rate": 6.356949283778553e-06,
      "loss": 2.1745,
      "step": 1642
    },
    {
      "epoch": 0.6360820751064653,
      "grad_norm": 10.983915328979492,
      "learning_rate": 6.360820751064654e-06,
      "loss": 1.3828,
      "step": 1643
    },
    {
      "epoch": 0.6364692218350755,
      "grad_norm": 15.645587921142578,
      "learning_rate": 6.364692218350755e-06,
      "loss": 1.5505,
      "step": 1644
    },
    {
      "epoch": 0.6368563685636857,
      "grad_norm": 15.705018997192383,
      "learning_rate": 6.368563685636857e-06,
      "loss": 1.4453,
      "step": 1645
    },
    {
      "epoch": 0.6372435152922957,
      "grad_norm": 14.376604080200195,
      "learning_rate": 6.372435152922957e-06,
      "loss": 1.7615,
      "step": 1646
    },
    {
      "epoch": 0.6376306620209059,
      "grad_norm": 10.845812797546387,
      "learning_rate": 6.37630662020906e-06,
      "loss": 1.6827,
      "step": 1647
    },
    {
      "epoch": 0.6380178087495161,
      "grad_norm": 10.224095344543457,
      "learning_rate": 6.380178087495161e-06,
      "loss": 1.9021,
      "step": 1648
    },
    {
      "epoch": 0.6384049554781263,
      "grad_norm": 16.573556900024414,
      "learning_rate": 6.384049554781263e-06,
      "loss": 1.7546,
      "step": 1649
    },
    {
      "epoch": 0.6387921022067363,
      "grad_norm": 10.449393272399902,
      "learning_rate": 6.387921022067363e-06,
      "loss": 1.6522,
      "step": 1650
    },
    {
      "epoch": 0.6391792489353465,
      "grad_norm": 18.235862731933594,
      "learning_rate": 6.391792489353466e-06,
      "loss": 1.599,
      "step": 1651
    },
    {
      "epoch": 0.6395663956639567,
      "grad_norm": 11.273093223571777,
      "learning_rate": 6.395663956639567e-06,
      "loss": 1.8063,
      "step": 1652
    },
    {
      "epoch": 0.6399535423925667,
      "grad_norm": 9.501378059387207,
      "learning_rate": 6.399535423925668e-06,
      "loss": 1.5975,
      "step": 1653
    },
    {
      "epoch": 0.6403406891211769,
      "grad_norm": 11.26749324798584,
      "learning_rate": 6.403406891211769e-06,
      "loss": 1.6983,
      "step": 1654
    },
    {
      "epoch": 0.6407278358497871,
      "grad_norm": 8.536458969116211,
      "learning_rate": 6.407278358497872e-06,
      "loss": 1.5714,
      "step": 1655
    },
    {
      "epoch": 0.6411149825783972,
      "grad_norm": 7.421027183532715,
      "learning_rate": 6.411149825783972e-06,
      "loss": 1.6369,
      "step": 1656
    },
    {
      "epoch": 0.6415021293070073,
      "grad_norm": 9.434779167175293,
      "learning_rate": 6.415021293070074e-06,
      "loss": 1.6161,
      "step": 1657
    },
    {
      "epoch": 0.6418892760356175,
      "grad_norm": 7.645019054412842,
      "learning_rate": 6.418892760356175e-06,
      "loss": 1.6453,
      "step": 1658
    },
    {
      "epoch": 0.6422764227642277,
      "grad_norm": 19.036104202270508,
      "learning_rate": 6.422764227642278e-06,
      "loss": 1.8505,
      "step": 1659
    },
    {
      "epoch": 0.6426635694928378,
      "grad_norm": 10.538844108581543,
      "learning_rate": 6.426635694928378e-06,
      "loss": 1.817,
      "step": 1660
    },
    {
      "epoch": 0.6430507162214479,
      "grad_norm": 9.730415344238281,
      "learning_rate": 6.43050716221448e-06,
      "loss": 1.6494,
      "step": 1661
    },
    {
      "epoch": 0.6434378629500581,
      "grad_norm": 11.24897289276123,
      "learning_rate": 6.434378629500581e-06,
      "loss": 1.3921,
      "step": 1662
    },
    {
      "epoch": 0.6438250096786682,
      "grad_norm": 13.17837905883789,
      "learning_rate": 6.438250096786682e-06,
      "loss": 2.4474,
      "step": 1663
    },
    {
      "epoch": 0.6442121564072784,
      "grad_norm": 10.572211265563965,
      "learning_rate": 6.442121564072784e-06,
      "loss": 1.3125,
      "step": 1664
    },
    {
      "epoch": 0.6445993031358885,
      "grad_norm": 10.286937713623047,
      "learning_rate": 6.445993031358886e-06,
      "loss": 1.4291,
      "step": 1665
    },
    {
      "epoch": 0.6449864498644986,
      "grad_norm": 11.037425994873047,
      "learning_rate": 6.449864498644986e-06,
      "loss": 1.7163,
      "step": 1666
    },
    {
      "epoch": 0.6453735965931088,
      "grad_norm": 10.855558395385742,
      "learning_rate": 6.453735965931088e-06,
      "loss": 1.639,
      "step": 1667
    },
    {
      "epoch": 0.645760743321719,
      "grad_norm": 9.719392776489258,
      "learning_rate": 6.45760743321719e-06,
      "loss": 1.6727,
      "step": 1668
    },
    {
      "epoch": 0.646147890050329,
      "grad_norm": 10.755160331726074,
      "learning_rate": 6.461478900503291e-06,
      "loss": 1.8768,
      "step": 1669
    },
    {
      "epoch": 0.6465350367789392,
      "grad_norm": 15.968979835510254,
      "learning_rate": 6.465350367789392e-06,
      "loss": 1.7733,
      "step": 1670
    },
    {
      "epoch": 0.6469221835075494,
      "grad_norm": 41.006656646728516,
      "learning_rate": 6.469221835075494e-06,
      "loss": 1.8769,
      "step": 1671
    },
    {
      "epoch": 0.6473093302361596,
      "grad_norm": 18.741825103759766,
      "learning_rate": 6.473093302361596e-06,
      "loss": 1.7571,
      "step": 1672
    },
    {
      "epoch": 0.6476964769647696,
      "grad_norm": 10.438446998596191,
      "learning_rate": 6.476964769647697e-06,
      "loss": 1.8701,
      "step": 1673
    },
    {
      "epoch": 0.6480836236933798,
      "grad_norm": 10.539752960205078,
      "learning_rate": 6.480836236933798e-06,
      "loss": 1.6691,
      "step": 1674
    },
    {
      "epoch": 0.64847077042199,
      "grad_norm": 9.968301773071289,
      "learning_rate": 6.4847077042199e-06,
      "loss": 1.511,
      "step": 1675
    },
    {
      "epoch": 0.6488579171506,
      "grad_norm": 12.567091941833496,
      "learning_rate": 6.488579171506001e-06,
      "loss": 1.3022,
      "step": 1676
    },
    {
      "epoch": 0.6492450638792102,
      "grad_norm": 9.34203815460205,
      "learning_rate": 6.492450638792103e-06,
      "loss": 1.7131,
      "step": 1677
    },
    {
      "epoch": 0.6496322106078204,
      "grad_norm": 10.73672866821289,
      "learning_rate": 6.496322106078204e-06,
      "loss": 1.4045,
      "step": 1678
    },
    {
      "epoch": 0.6500193573364305,
      "grad_norm": 9.564488410949707,
      "learning_rate": 6.500193573364305e-06,
      "loss": 1.5883,
      "step": 1679
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 16.660350799560547,
      "learning_rate": 6.504065040650407e-06,
      "loss": 2.0124,
      "step": 1680
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 9.59033489227295,
      "learning_rate": 6.507936507936509e-06,
      "loss": 1.6768,
      "step": 1681
    },
    {
      "epoch": 0.651180797522261,
      "grad_norm": 10.19639778137207,
      "learning_rate": 6.51180797522261e-06,
      "loss": 1.3712,
      "step": 1682
    },
    {
      "epoch": 0.6515679442508711,
      "grad_norm": 10.807090759277344,
      "learning_rate": 6.515679442508711e-06,
      "loss": 1.6953,
      "step": 1683
    },
    {
      "epoch": 0.6519550909794812,
      "grad_norm": 16.682161331176758,
      "learning_rate": 6.519550909794813e-06,
      "loss": 1.6858,
      "step": 1684
    },
    {
      "epoch": 0.6523422377080914,
      "grad_norm": 5.795151710510254,
      "learning_rate": 6.523422377080915e-06,
      "loss": 1.8299,
      "step": 1685
    },
    {
      "epoch": 0.6527293844367015,
      "grad_norm": 11.262455940246582,
      "learning_rate": 6.5272938443670154e-06,
      "loss": 1.2982,
      "step": 1686
    },
    {
      "epoch": 0.6531165311653117,
      "grad_norm": 18.409643173217773,
      "learning_rate": 6.531165311653117e-06,
      "loss": 1.8875,
      "step": 1687
    },
    {
      "epoch": 0.6535036778939218,
      "grad_norm": 12.072376251220703,
      "learning_rate": 6.535036778939219e-06,
      "loss": 1.7107,
      "step": 1688
    },
    {
      "epoch": 0.6538908246225319,
      "grad_norm": 9.39417839050293,
      "learning_rate": 6.53890824622532e-06,
      "loss": 1.7486,
      "step": 1689
    },
    {
      "epoch": 0.6542779713511421,
      "grad_norm": 14.508858680725098,
      "learning_rate": 6.542779713511421e-06,
      "loss": 1.4811,
      "step": 1690
    },
    {
      "epoch": 0.6546651180797523,
      "grad_norm": 11.4152250289917,
      "learning_rate": 6.546651180797523e-06,
      "loss": 1.8255,
      "step": 1691
    },
    {
      "epoch": 0.6550522648083623,
      "grad_norm": 14.729401588439941,
      "learning_rate": 6.5505226480836235e-06,
      "loss": 1.9383,
      "step": 1692
    },
    {
      "epoch": 0.6554394115369725,
      "grad_norm": 20.702415466308594,
      "learning_rate": 6.554394115369726e-06,
      "loss": 1.7911,
      "step": 1693
    },
    {
      "epoch": 0.6558265582655827,
      "grad_norm": 11.477478981018066,
      "learning_rate": 6.558265582655827e-06,
      "loss": 1.7735,
      "step": 1694
    },
    {
      "epoch": 0.6562137049941928,
      "grad_norm": 16.803979873657227,
      "learning_rate": 6.562137049941929e-06,
      "loss": 2.2658,
      "step": 1695
    },
    {
      "epoch": 0.6566008517228029,
      "grad_norm": 8.346403121948242,
      "learning_rate": 6.5660085172280295e-06,
      "loss": 1.6516,
      "step": 1696
    },
    {
      "epoch": 0.6569879984514131,
      "grad_norm": 10.942815780639648,
      "learning_rate": 6.569879984514132e-06,
      "loss": 1.3745,
      "step": 1697
    },
    {
      "epoch": 0.6573751451800233,
      "grad_norm": 9.2357177734375,
      "learning_rate": 6.573751451800233e-06,
      "loss": 1.5242,
      "step": 1698
    },
    {
      "epoch": 0.6577622919086333,
      "grad_norm": 9.346148490905762,
      "learning_rate": 6.577622919086334e-06,
      "loss": 1.5722,
      "step": 1699
    },
    {
      "epoch": 0.6581494386372435,
      "grad_norm": 14.438264846801758,
      "learning_rate": 6.5814943863724355e-06,
      "loss": 1.5598,
      "step": 1700
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 25.445663452148438,
      "learning_rate": 6.585365853658538e-06,
      "loss": 2.0345,
      "step": 1701
    },
    {
      "epoch": 0.6589237320944638,
      "grad_norm": 8.108845710754395,
      "learning_rate": 6.5892373209446385e-06,
      "loss": 1.5409,
      "step": 1702
    },
    {
      "epoch": 0.6593108788230739,
      "grad_norm": 9.686677932739258,
      "learning_rate": 6.59310878823074e-06,
      "loss": 1.6921,
      "step": 1703
    },
    {
      "epoch": 0.6596980255516841,
      "grad_norm": 10.099474906921387,
      "learning_rate": 6.5969802555168415e-06,
      "loss": 1.7747,
      "step": 1704
    },
    {
      "epoch": 0.6600851722802943,
      "grad_norm": 9.691216468811035,
      "learning_rate": 6.600851722802944e-06,
      "loss": 1.7017,
      "step": 1705
    },
    {
      "epoch": 0.6604723190089044,
      "grad_norm": 10.665839195251465,
      "learning_rate": 6.6047231900890444e-06,
      "loss": 1.7126,
      "step": 1706
    },
    {
      "epoch": 0.6608594657375145,
      "grad_norm": 9.948758125305176,
      "learning_rate": 6.608594657375146e-06,
      "loss": 1.6053,
      "step": 1707
    },
    {
      "epoch": 0.6612466124661247,
      "grad_norm": 9.160399436950684,
      "learning_rate": 6.6124661246612474e-06,
      "loss": 1.6367,
      "step": 1708
    },
    {
      "epoch": 0.6616337591947348,
      "grad_norm": 12.585317611694336,
      "learning_rate": 6.616337591947348e-06,
      "loss": 1.8175,
      "step": 1709
    },
    {
      "epoch": 0.662020905923345,
      "grad_norm": 9.440046310424805,
      "learning_rate": 6.62020905923345e-06,
      "loss": 1.7824,
      "step": 1710
    },
    {
      "epoch": 0.6624080526519551,
      "grad_norm": 10.115623474121094,
      "learning_rate": 6.624080526519552e-06,
      "loss": 1.2509,
      "step": 1711
    },
    {
      "epoch": 0.6627951993805652,
      "grad_norm": 10.586886405944824,
      "learning_rate": 6.6279519938056526e-06,
      "loss": 1.7563,
      "step": 1712
    },
    {
      "epoch": 0.6631823461091754,
      "grad_norm": 14.908645629882812,
      "learning_rate": 6.631823461091754e-06,
      "loss": 1.6461,
      "step": 1713
    },
    {
      "epoch": 0.6635694928377855,
      "grad_norm": 13.411029815673828,
      "learning_rate": 6.635694928377856e-06,
      "loss": 1.9984,
      "step": 1714
    },
    {
      "epoch": 0.6639566395663956,
      "grad_norm": 9.240623474121094,
      "learning_rate": 6.639566395663957e-06,
      "loss": 1.5382,
      "step": 1715
    },
    {
      "epoch": 0.6643437862950058,
      "grad_norm": 12.124184608459473,
      "learning_rate": 6.6434378629500585e-06,
      "loss": 1.5087,
      "step": 1716
    },
    {
      "epoch": 0.664730933023616,
      "grad_norm": 20.874603271484375,
      "learning_rate": 6.64730933023616e-06,
      "loss": 1.6838,
      "step": 1717
    },
    {
      "epoch": 0.6651180797522261,
      "grad_norm": 10.21602725982666,
      "learning_rate": 6.651180797522262e-06,
      "loss": 1.9602,
      "step": 1718
    },
    {
      "epoch": 0.6655052264808362,
      "grad_norm": 10.535090446472168,
      "learning_rate": 6.655052264808363e-06,
      "loss": 1.7679,
      "step": 1719
    },
    {
      "epoch": 0.6658923732094464,
      "grad_norm": 8.714552879333496,
      "learning_rate": 6.6589237320944645e-06,
      "loss": 1.6743,
      "step": 1720
    },
    {
      "epoch": 0.6662795199380566,
      "grad_norm": 16.960966110229492,
      "learning_rate": 6.662795199380566e-06,
      "loss": 1.7441,
      "step": 1721
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 8.444924354553223,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.2433,
      "step": 1722
    },
    {
      "epoch": 0.6670538133952768,
      "grad_norm": 14.322436332702637,
      "learning_rate": 6.670538133952769e-06,
      "loss": 1.4488,
      "step": 1723
    },
    {
      "epoch": 0.667440960123887,
      "grad_norm": 16.21245002746582,
      "learning_rate": 6.6744096012388705e-06,
      "loss": 1.6602,
      "step": 1724
    },
    {
      "epoch": 0.667828106852497,
      "grad_norm": 15.279275894165039,
      "learning_rate": 6.678281068524971e-06,
      "loss": 1.726,
      "step": 1725
    },
    {
      "epoch": 0.6682152535811072,
      "grad_norm": 9.365312576293945,
      "learning_rate": 6.682152535811073e-06,
      "loss": 1.5979,
      "step": 1726
    },
    {
      "epoch": 0.6686024003097174,
      "grad_norm": 7.099853515625,
      "learning_rate": 6.686024003097175e-06,
      "loss": 1.7253,
      "step": 1727
    },
    {
      "epoch": 0.6689895470383276,
      "grad_norm": 18.042552947998047,
      "learning_rate": 6.6898954703832765e-06,
      "loss": 2.1572,
      "step": 1728
    },
    {
      "epoch": 0.6693766937669376,
      "grad_norm": 12.843842506408691,
      "learning_rate": 6.693766937669377e-06,
      "loss": 1.428,
      "step": 1729
    },
    {
      "epoch": 0.6697638404955478,
      "grad_norm": 8.317930221557617,
      "learning_rate": 6.697638404955479e-06,
      "loss": 1.8253,
      "step": 1730
    },
    {
      "epoch": 0.670150987224158,
      "grad_norm": 12.043935775756836,
      "learning_rate": 6.701509872241581e-06,
      "loss": 1.7482,
      "step": 1731
    },
    {
      "epoch": 0.6705381339527681,
      "grad_norm": 15.44277572631836,
      "learning_rate": 6.705381339527682e-06,
      "loss": 1.6465,
      "step": 1732
    },
    {
      "epoch": 0.6709252806813782,
      "grad_norm": 14.590611457824707,
      "learning_rate": 6.709252806813783e-06,
      "loss": 1.6486,
      "step": 1733
    },
    {
      "epoch": 0.6713124274099884,
      "grad_norm": 12.6495943069458,
      "learning_rate": 6.7131242740998846e-06,
      "loss": 1.5811,
      "step": 1734
    },
    {
      "epoch": 0.6716995741385985,
      "grad_norm": 12.761370658874512,
      "learning_rate": 6.716995741385985e-06,
      "loss": 1.6173,
      "step": 1735
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 28.80975914001465,
      "learning_rate": 6.7208672086720876e-06,
      "loss": 2.128,
      "step": 1736
    },
    {
      "epoch": 0.6724738675958188,
      "grad_norm": 11.561868667602539,
      "learning_rate": 6.724738675958189e-06,
      "loss": 1.7178,
      "step": 1737
    },
    {
      "epoch": 0.6728610143244289,
      "grad_norm": 12.951156616210938,
      "learning_rate": 6.72861014324429e-06,
      "loss": 1.6467,
      "step": 1738
    },
    {
      "epoch": 0.6732481610530391,
      "grad_norm": 17.135419845581055,
      "learning_rate": 6.732481610530391e-06,
      "loss": 1.7315,
      "step": 1739
    },
    {
      "epoch": 0.6736353077816493,
      "grad_norm": 13.040820121765137,
      "learning_rate": 6.7363530778164935e-06,
      "loss": 1.6077,
      "step": 1740
    },
    {
      "epoch": 0.6740224545102594,
      "grad_norm": 9.455711364746094,
      "learning_rate": 6.740224545102595e-06,
      "loss": 1.6266,
      "step": 1741
    },
    {
      "epoch": 0.6744096012388695,
      "grad_norm": 9.62055778503418,
      "learning_rate": 6.744096012388696e-06,
      "loss": 1.6667,
      "step": 1742
    },
    {
      "epoch": 0.6747967479674797,
      "grad_norm": 7.899770736694336,
      "learning_rate": 6.747967479674797e-06,
      "loss": 1.2322,
      "step": 1743
    },
    {
      "epoch": 0.6751838946960899,
      "grad_norm": 9.839421272277832,
      "learning_rate": 6.7518389469608995e-06,
      "loss": 1.5933,
      "step": 1744
    },
    {
      "epoch": 0.6755710414246999,
      "grad_norm": 11.663894653320312,
      "learning_rate": 6.755710414247e-06,
      "loss": 1.6245,
      "step": 1745
    },
    {
      "epoch": 0.6759581881533101,
      "grad_norm": 8.384481430053711,
      "learning_rate": 6.759581881533102e-06,
      "loss": 1.6683,
      "step": 1746
    },
    {
      "epoch": 0.6763453348819203,
      "grad_norm": 16.135602951049805,
      "learning_rate": 6.763453348819203e-06,
      "loss": 1.7899,
      "step": 1747
    },
    {
      "epoch": 0.6767324816105303,
      "grad_norm": 10.426004409790039,
      "learning_rate": 6.767324816105304e-06,
      "loss": 1.3406,
      "step": 1748
    },
    {
      "epoch": 0.6771196283391405,
      "grad_norm": 12.822816848754883,
      "learning_rate": 6.771196283391406e-06,
      "loss": 1.5617,
      "step": 1749
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 14.033377647399902,
      "learning_rate": 6.775067750677508e-06,
      "loss": 2.1645,
      "step": 1750
    },
    {
      "epoch": 0.6778939217963608,
      "grad_norm": 30.764177322387695,
      "learning_rate": 6.778939217963608e-06,
      "loss": 1.9821,
      "step": 1751
    },
    {
      "epoch": 0.6782810685249709,
      "grad_norm": 7.057508945465088,
      "learning_rate": 6.78281068524971e-06,
      "loss": 1.229,
      "step": 1752
    },
    {
      "epoch": 0.6786682152535811,
      "grad_norm": 6.876107215881348,
      "learning_rate": 6.786682152535812e-06,
      "loss": 1.3643,
      "step": 1753
    },
    {
      "epoch": 0.6790553619821913,
      "grad_norm": 10.17329216003418,
      "learning_rate": 6.790553619821914e-06,
      "loss": 1.3381,
      "step": 1754
    },
    {
      "epoch": 0.6794425087108014,
      "grad_norm": 9.891817092895508,
      "learning_rate": 6.794425087108014e-06,
      "loss": 1.6959,
      "step": 1755
    },
    {
      "epoch": 0.6798296554394115,
      "grad_norm": 12.845528602600098,
      "learning_rate": 6.798296554394116e-06,
      "loss": 1.4437,
      "step": 1756
    },
    {
      "epoch": 0.6802168021680217,
      "grad_norm": 14.759862899780273,
      "learning_rate": 6.802168021680218e-06,
      "loss": 1.598,
      "step": 1757
    },
    {
      "epoch": 0.6806039488966318,
      "grad_norm": 14.325324058532715,
      "learning_rate": 6.806039488966319e-06,
      "loss": 1.6625,
      "step": 1758
    },
    {
      "epoch": 0.680991095625242,
      "grad_norm": 8.000545501708984,
      "learning_rate": 6.80991095625242e-06,
      "loss": 1.5641,
      "step": 1759
    },
    {
      "epoch": 0.6813782423538521,
      "grad_norm": 15.356788635253906,
      "learning_rate": 6.813782423538522e-06,
      "loss": 1.7632,
      "step": 1760
    },
    {
      "epoch": 0.6817653890824622,
      "grad_norm": 15.611788749694824,
      "learning_rate": 6.817653890824622e-06,
      "loss": 1.724,
      "step": 1761
    },
    {
      "epoch": 0.6821525358110724,
      "grad_norm": 13.338688850402832,
      "learning_rate": 6.821525358110725e-06,
      "loss": 1.4255,
      "step": 1762
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 9.277602195739746,
      "learning_rate": 6.825396825396826e-06,
      "loss": 1.1634,
      "step": 1763
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 10.77285099029541,
      "learning_rate": 6.829268292682928e-06,
      "loss": 1.2284,
      "step": 1764
    },
    {
      "epoch": 0.6833139759969028,
      "grad_norm": 10.962461471557617,
      "learning_rate": 6.833139759969028e-06,
      "loss": 1.5134,
      "step": 1765
    },
    {
      "epoch": 0.683701122725513,
      "grad_norm": 9.403000831604004,
      "learning_rate": 6.837011227255131e-06,
      "loss": 1.3402,
      "step": 1766
    },
    {
      "epoch": 0.6840882694541232,
      "grad_norm": 12.388096809387207,
      "learning_rate": 6.840882694541232e-06,
      "loss": 2.1403,
      "step": 1767
    },
    {
      "epoch": 0.6844754161827332,
      "grad_norm": 11.67637825012207,
      "learning_rate": 6.844754161827333e-06,
      "loss": 1.4719,
      "step": 1768
    },
    {
      "epoch": 0.6848625629113434,
      "grad_norm": 13.047776222229004,
      "learning_rate": 6.848625629113434e-06,
      "loss": 0.9589,
      "step": 1769
    },
    {
      "epoch": 0.6852497096399536,
      "grad_norm": 13.283334732055664,
      "learning_rate": 6.852497096399537e-06,
      "loss": 0.8937,
      "step": 1770
    },
    {
      "epoch": 0.6856368563685636,
      "grad_norm": 8.076489448547363,
      "learning_rate": 6.856368563685637e-06,
      "loss": 1.5734,
      "step": 1771
    },
    {
      "epoch": 0.6860240030971738,
      "grad_norm": 11.028087615966797,
      "learning_rate": 6.860240030971739e-06,
      "loss": 1.5626,
      "step": 1772
    },
    {
      "epoch": 0.686411149825784,
      "grad_norm": 10.923970222473145,
      "learning_rate": 6.86411149825784e-06,
      "loss": 1.556,
      "step": 1773
    },
    {
      "epoch": 0.6867982965543941,
      "grad_norm": 10.3269681930542,
      "learning_rate": 6.867982965543941e-06,
      "loss": 1.2788,
      "step": 1774
    },
    {
      "epoch": 0.6871854432830042,
      "grad_norm": 29.58184814453125,
      "learning_rate": 6.871854432830043e-06,
      "loss": 2.13,
      "step": 1775
    },
    {
      "epoch": 0.6875725900116144,
      "grad_norm": 13.688886642456055,
      "learning_rate": 6.875725900116145e-06,
      "loss": 1.5384,
      "step": 1776
    },
    {
      "epoch": 0.6879597367402246,
      "grad_norm": 9.623886108398438,
      "learning_rate": 6.879597367402246e-06,
      "loss": 1.6521,
      "step": 1777
    },
    {
      "epoch": 0.6883468834688347,
      "grad_norm": 8.089095115661621,
      "learning_rate": 6.883468834688347e-06,
      "loss": 1.4718,
      "step": 1778
    },
    {
      "epoch": 0.6887340301974448,
      "grad_norm": 13.436201095581055,
      "learning_rate": 6.887340301974449e-06,
      "loss": 1.4377,
      "step": 1779
    },
    {
      "epoch": 0.689121176926055,
      "grad_norm": 12.04425048828125,
      "learning_rate": 6.891211769260551e-06,
      "loss": 1.7137,
      "step": 1780
    },
    {
      "epoch": 0.6895083236546651,
      "grad_norm": 26.844402313232422,
      "learning_rate": 6.895083236546651e-06,
      "loss": 1.511,
      "step": 1781
    },
    {
      "epoch": 0.6898954703832753,
      "grad_norm": 15.213305473327637,
      "learning_rate": 6.898954703832753e-06,
      "loss": 1.3233,
      "step": 1782
    },
    {
      "epoch": 0.6902826171118854,
      "grad_norm": 11.502018928527832,
      "learning_rate": 6.902826171118855e-06,
      "loss": 1.8445,
      "step": 1783
    },
    {
      "epoch": 0.6906697638404955,
      "grad_norm": 9.076035499572754,
      "learning_rate": 6.906697638404956e-06,
      "loss": 1.578,
      "step": 1784
    },
    {
      "epoch": 0.6910569105691057,
      "grad_norm": 16.82877540588379,
      "learning_rate": 6.910569105691057e-06,
      "loss": 1.7182,
      "step": 1785
    },
    {
      "epoch": 0.6914440572977159,
      "grad_norm": 25.262794494628906,
      "learning_rate": 6.914440572977159e-06,
      "loss": 2.0436,
      "step": 1786
    },
    {
      "epoch": 0.691831204026326,
      "grad_norm": 11.96796703338623,
      "learning_rate": 6.918312040263261e-06,
      "loss": 1.9054,
      "step": 1787
    },
    {
      "epoch": 0.6922183507549361,
      "grad_norm": 10.857501029968262,
      "learning_rate": 6.922183507549362e-06,
      "loss": 1.7282,
      "step": 1788
    },
    {
      "epoch": 0.6926054974835463,
      "grad_norm": 13.52420711517334,
      "learning_rate": 6.926054974835463e-06,
      "loss": 1.7775,
      "step": 1789
    },
    {
      "epoch": 0.6929926442121565,
      "grad_norm": 10.676712036132812,
      "learning_rate": 6.929926442121565e-06,
      "loss": 1.2428,
      "step": 1790
    },
    {
      "epoch": 0.6933797909407665,
      "grad_norm": 8.852523803710938,
      "learning_rate": 6.9337979094076655e-06,
      "loss": 1.8614,
      "step": 1791
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 21.561601638793945,
      "learning_rate": 6.937669376693768e-06,
      "loss": 1.6159,
      "step": 1792
    },
    {
      "epoch": 0.6941540843979869,
      "grad_norm": 13.067060470581055,
      "learning_rate": 6.941540843979869e-06,
      "loss": 0.8923,
      "step": 1793
    },
    {
      "epoch": 0.6945412311265969,
      "grad_norm": 11.131730079650879,
      "learning_rate": 6.94541231126597e-06,
      "loss": 1.2655,
      "step": 1794
    },
    {
      "epoch": 0.6949283778552071,
      "grad_norm": 9.820119857788086,
      "learning_rate": 6.9492837785520714e-06,
      "loss": 1.3564,
      "step": 1795
    },
    {
      "epoch": 0.6953155245838173,
      "grad_norm": 10.39277172088623,
      "learning_rate": 6.953155245838174e-06,
      "loss": 1.2747,
      "step": 1796
    },
    {
      "epoch": 0.6957026713124274,
      "grad_norm": 20.97304344177246,
      "learning_rate": 6.957026713124274e-06,
      "loss": 1.9854,
      "step": 1797
    },
    {
      "epoch": 0.6960898180410375,
      "grad_norm": 10.669303894042969,
      "learning_rate": 6.960898180410376e-06,
      "loss": 1.2316,
      "step": 1798
    },
    {
      "epoch": 0.6964769647696477,
      "grad_norm": 10.355815887451172,
      "learning_rate": 6.964769647696477e-06,
      "loss": 1.6321,
      "step": 1799
    },
    {
      "epoch": 0.6968641114982579,
      "grad_norm": 18.02633285522461,
      "learning_rate": 6.96864111498258e-06,
      "loss": 1.7068,
      "step": 1800
    },
    {
      "epoch": 0.697251258226868,
      "grad_norm": 13.13068675994873,
      "learning_rate": 6.97251258226868e-06,
      "loss": 1.5696,
      "step": 1801
    },
    {
      "epoch": 0.6976384049554781,
      "grad_norm": 11.295896530151367,
      "learning_rate": 6.976384049554782e-06,
      "loss": 1.5582,
      "step": 1802
    },
    {
      "epoch": 0.6980255516840883,
      "grad_norm": 15.978182792663574,
      "learning_rate": 6.980255516840883e-06,
      "loss": 2.0441,
      "step": 1803
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 12.968184471130371,
      "learning_rate": 6.984126984126984e-06,
      "loss": 1.5884,
      "step": 1804
    },
    {
      "epoch": 0.6987998451413086,
      "grad_norm": 14.512860298156738,
      "learning_rate": 6.987998451413086e-06,
      "loss": 1.5692,
      "step": 1805
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 23.638874053955078,
      "learning_rate": 6.991869918699188e-06,
      "loss": 2.0531,
      "step": 1806
    },
    {
      "epoch": 0.6995741385985288,
      "grad_norm": 13.984369277954102,
      "learning_rate": 6.9957413859852885e-06,
      "loss": 1.4325,
      "step": 1807
    },
    {
      "epoch": 0.699961285327139,
      "grad_norm": 21.825904846191406,
      "learning_rate": 6.99961285327139e-06,
      "loss": 1.827,
      "step": 1808
    },
    {
      "epoch": 0.7003484320557491,
      "grad_norm": 11.423483848571777,
      "learning_rate": 7.003484320557492e-06,
      "loss": 1.6614,
      "step": 1809
    },
    {
      "epoch": 0.7007355787843593,
      "grad_norm": 11.648202896118164,
      "learning_rate": 7.007355787843594e-06,
      "loss": 1.2566,
      "step": 1810
    },
    {
      "epoch": 0.7011227255129694,
      "grad_norm": 9.72801399230957,
      "learning_rate": 7.0112272551296945e-06,
      "loss": 1.5388,
      "step": 1811
    },
    {
      "epoch": 0.7015098722415796,
      "grad_norm": 11.390835762023926,
      "learning_rate": 7.015098722415796e-06,
      "loss": 1.7097,
      "step": 1812
    },
    {
      "epoch": 0.7018970189701897,
      "grad_norm": 11.721148490905762,
      "learning_rate": 7.018970189701898e-06,
      "loss": 1.8019,
      "step": 1813
    },
    {
      "epoch": 0.7022841656987998,
      "grad_norm": 9.784533500671387,
      "learning_rate": 7.022841656987999e-06,
      "loss": 1.5691,
      "step": 1814
    },
    {
      "epoch": 0.70267131242741,
      "grad_norm": 24.900054931640625,
      "learning_rate": 7.0267131242741005e-06,
      "loss": 1.9059,
      "step": 1815
    },
    {
      "epoch": 0.7030584591560202,
      "grad_norm": 10.221701622009277,
      "learning_rate": 7.030584591560202e-06,
      "loss": 1.1204,
      "step": 1816
    },
    {
      "epoch": 0.7034456058846302,
      "grad_norm": 9.470000267028809,
      "learning_rate": 7.034456058846303e-06,
      "loss": 1.5417,
      "step": 1817
    },
    {
      "epoch": 0.7038327526132404,
      "grad_norm": 10.516840934753418,
      "learning_rate": 7.038327526132405e-06,
      "loss": 1.1856,
      "step": 1818
    },
    {
      "epoch": 0.7042198993418506,
      "grad_norm": 16.654998779296875,
      "learning_rate": 7.0421989934185064e-06,
      "loss": 1.5317,
      "step": 1819
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 21.645463943481445,
      "learning_rate": 7.046070460704607e-06,
      "loss": 2.1117,
      "step": 1820
    },
    {
      "epoch": 0.7049941927990708,
      "grad_norm": 10.257989883422852,
      "learning_rate": 7.0499419279907086e-06,
      "loss": 1.5766,
      "step": 1821
    },
    {
      "epoch": 0.705381339527681,
      "grad_norm": 9.5421724319458,
      "learning_rate": 7.053813395276811e-06,
      "loss": 1.6297,
      "step": 1822
    },
    {
      "epoch": 0.7057684862562912,
      "grad_norm": 9.262897491455078,
      "learning_rate": 7.057684862562912e-06,
      "loss": 1.5097,
      "step": 1823
    },
    {
      "epoch": 0.7061556329849012,
      "grad_norm": 10.464183807373047,
      "learning_rate": 7.061556329849013e-06,
      "loss": 1.6753,
      "step": 1824
    },
    {
      "epoch": 0.7065427797135114,
      "grad_norm": 9.965311050415039,
      "learning_rate": 7.0654277971351145e-06,
      "loss": 1.5756,
      "step": 1825
    },
    {
      "epoch": 0.7069299264421216,
      "grad_norm": 10.674848556518555,
      "learning_rate": 7.069299264421217e-06,
      "loss": 1.8911,
      "step": 1826
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 9.590476036071777,
      "learning_rate": 7.0731707317073175e-06,
      "loss": 1.5828,
      "step": 1827
    },
    {
      "epoch": 0.7077042198993418,
      "grad_norm": 15.461480140686035,
      "learning_rate": 7.077042198993419e-06,
      "loss": 1.8836,
      "step": 1828
    },
    {
      "epoch": 0.708091366627952,
      "grad_norm": 31.93998908996582,
      "learning_rate": 7.0809136662795205e-06,
      "loss": 1.9967,
      "step": 1829
    },
    {
      "epoch": 0.7084785133565621,
      "grad_norm": 12.130507469177246,
      "learning_rate": 7.084785133565621e-06,
      "loss": 1.9402,
      "step": 1830
    },
    {
      "epoch": 0.7088656600851723,
      "grad_norm": 14.148700714111328,
      "learning_rate": 7.0886566008517235e-06,
      "loss": 1.9952,
      "step": 1831
    },
    {
      "epoch": 0.7092528068137824,
      "grad_norm": 10.896751403808594,
      "learning_rate": 7.092528068137825e-06,
      "loss": 1.4159,
      "step": 1832
    },
    {
      "epoch": 0.7096399535423926,
      "grad_norm": 17.4697322845459,
      "learning_rate": 7.0963995354239265e-06,
      "loss": 1.6221,
      "step": 1833
    },
    {
      "epoch": 0.7100271002710027,
      "grad_norm": 13.097932815551758,
      "learning_rate": 7.100271002710027e-06,
      "loss": 2.3555,
      "step": 1834
    },
    {
      "epoch": 0.7104142469996129,
      "grad_norm": 12.204648971557617,
      "learning_rate": 7.1041424699961295e-06,
      "loss": 1.8923,
      "step": 1835
    },
    {
      "epoch": 0.710801393728223,
      "grad_norm": 17.001800537109375,
      "learning_rate": 7.108013937282231e-06,
      "loss": 1.6488,
      "step": 1836
    },
    {
      "epoch": 0.7111885404568331,
      "grad_norm": 12.56925106048584,
      "learning_rate": 7.111885404568332e-06,
      "loss": 0.7874,
      "step": 1837
    },
    {
      "epoch": 0.7115756871854433,
      "grad_norm": 11.63797664642334,
      "learning_rate": 7.115756871854433e-06,
      "loss": 1.2369,
      "step": 1838
    },
    {
      "epoch": 0.7119628339140535,
      "grad_norm": 10.654993057250977,
      "learning_rate": 7.1196283391405354e-06,
      "loss": 1.0972,
      "step": 1839
    },
    {
      "epoch": 0.7123499806426635,
      "grad_norm": 11.287028312683105,
      "learning_rate": 7.123499806426636e-06,
      "loss": 1.4792,
      "step": 1840
    },
    {
      "epoch": 0.7127371273712737,
      "grad_norm": 10.462878227233887,
      "learning_rate": 7.127371273712738e-06,
      "loss": 1.1993,
      "step": 1841
    },
    {
      "epoch": 0.7131242740998839,
      "grad_norm": 18.554828643798828,
      "learning_rate": 7.131242740998839e-06,
      "loss": 1.5575,
      "step": 1842
    },
    {
      "epoch": 0.713511420828494,
      "grad_norm": 12.615894317626953,
      "learning_rate": 7.13511420828494e-06,
      "loss": 1.8971,
      "step": 1843
    },
    {
      "epoch": 0.7138985675571041,
      "grad_norm": 13.771031379699707,
      "learning_rate": 7.138985675571042e-06,
      "loss": 1.5347,
      "step": 1844
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 10.006608963012695,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 1.5088,
      "step": 1845
    },
    {
      "epoch": 0.7146728610143245,
      "grad_norm": 12.236921310424805,
      "learning_rate": 7.146728610143245e-06,
      "loss": 1.6373,
      "step": 1846
    },
    {
      "epoch": 0.7150600077429345,
      "grad_norm": 9.374247550964355,
      "learning_rate": 7.150600077429346e-06,
      "loss": 1.5926,
      "step": 1847
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 17.186115264892578,
      "learning_rate": 7.154471544715448e-06,
      "loss": 1.5349,
      "step": 1848
    },
    {
      "epoch": 0.7158343012001549,
      "grad_norm": 12.181107521057129,
      "learning_rate": 7.1583430120015495e-06,
      "loss": 1.9095,
      "step": 1849
    },
    {
      "epoch": 0.716221447928765,
      "grad_norm": 11.187947273254395,
      "learning_rate": 7.16221447928765e-06,
      "loss": 1.3003,
      "step": 1850
    },
    {
      "epoch": 0.7166085946573751,
      "grad_norm": 12.072371482849121,
      "learning_rate": 7.166085946573752e-06,
      "loss": 2.349,
      "step": 1851
    },
    {
      "epoch": 0.7169957413859853,
      "grad_norm": 13.392343521118164,
      "learning_rate": 7.169957413859854e-06,
      "loss": 1.968,
      "step": 1852
    },
    {
      "epoch": 0.7173828881145954,
      "grad_norm": 30.259838104248047,
      "learning_rate": 7.173828881145955e-06,
      "loss": 2.4742,
      "step": 1853
    },
    {
      "epoch": 0.7177700348432056,
      "grad_norm": 11.19955062866211,
      "learning_rate": 7.177700348432056e-06,
      "loss": 1.1376,
      "step": 1854
    },
    {
      "epoch": 0.7181571815718157,
      "grad_norm": 13.725021362304688,
      "learning_rate": 7.181571815718158e-06,
      "loss": 2.4004,
      "step": 1855
    },
    {
      "epoch": 0.7185443283004259,
      "grad_norm": 9.970237731933594,
      "learning_rate": 7.18544328300426e-06,
      "loss": 1.2084,
      "step": 1856
    },
    {
      "epoch": 0.718931475029036,
      "grad_norm": 18.29294204711914,
      "learning_rate": 7.189314750290361e-06,
      "loss": 2.2405,
      "step": 1857
    },
    {
      "epoch": 0.7193186217576462,
      "grad_norm": 22.2736759185791,
      "learning_rate": 7.193186217576462e-06,
      "loss": 1.7058,
      "step": 1858
    },
    {
      "epoch": 0.7197057684862563,
      "grad_norm": 13.911639213562012,
      "learning_rate": 7.197057684862564e-06,
      "loss": 1.984,
      "step": 1859
    },
    {
      "epoch": 0.7200929152148664,
      "grad_norm": 12.247221946716309,
      "learning_rate": 7.200929152148664e-06,
      "loss": 1.348,
      "step": 1860
    },
    {
      "epoch": 0.7204800619434766,
      "grad_norm": 8.755260467529297,
      "learning_rate": 7.204800619434767e-06,
      "loss": 1.8339,
      "step": 1861
    },
    {
      "epoch": 0.7208672086720868,
      "grad_norm": 11.214261054992676,
      "learning_rate": 7.208672086720868e-06,
      "loss": 1.6176,
      "step": 1862
    },
    {
      "epoch": 0.7212543554006968,
      "grad_norm": 12.732138633728027,
      "learning_rate": 7.212543554006969e-06,
      "loss": 1.3676,
      "step": 1863
    },
    {
      "epoch": 0.721641502129307,
      "grad_norm": 10.964174270629883,
      "learning_rate": 7.21641502129307e-06,
      "loss": 1.1852,
      "step": 1864
    },
    {
      "epoch": 0.7220286488579172,
      "grad_norm": 14.338828086853027,
      "learning_rate": 7.220286488579173e-06,
      "loss": 2.0766,
      "step": 1865
    },
    {
      "epoch": 0.7224157955865272,
      "grad_norm": 10.876822471618652,
      "learning_rate": 7.224157955865273e-06,
      "loss": 1.1081,
      "step": 1866
    },
    {
      "epoch": 0.7228029423151374,
      "grad_norm": 18.84029769897461,
      "learning_rate": 7.228029423151375e-06,
      "loss": 1.7809,
      "step": 1867
    },
    {
      "epoch": 0.7231900890437476,
      "grad_norm": 33.96131134033203,
      "learning_rate": 7.231900890437476e-06,
      "loss": 1.7871,
      "step": 1868
    },
    {
      "epoch": 0.7235772357723578,
      "grad_norm": 17.277557373046875,
      "learning_rate": 7.2357723577235786e-06,
      "loss": 1.5063,
      "step": 1869
    },
    {
      "epoch": 0.7239643825009678,
      "grad_norm": 10.946242332458496,
      "learning_rate": 7.239643825009679e-06,
      "loss": 1.6909,
      "step": 1870
    },
    {
      "epoch": 0.724351529229578,
      "grad_norm": 10.621179580688477,
      "learning_rate": 7.243515292295781e-06,
      "loss": 1.13,
      "step": 1871
    },
    {
      "epoch": 0.7247386759581882,
      "grad_norm": 15.861169815063477,
      "learning_rate": 7.247386759581882e-06,
      "loss": 1.7049,
      "step": 1872
    },
    {
      "epoch": 0.7251258226867983,
      "grad_norm": 13.01651668548584,
      "learning_rate": 7.251258226867983e-06,
      "loss": 0.9608,
      "step": 1873
    },
    {
      "epoch": 0.7255129694154084,
      "grad_norm": 13.291890144348145,
      "learning_rate": 7.255129694154085e-06,
      "loss": 1.4186,
      "step": 1874
    },
    {
      "epoch": 0.7259001161440186,
      "grad_norm": 9.263340950012207,
      "learning_rate": 7.259001161440187e-06,
      "loss": 1.2941,
      "step": 1875
    },
    {
      "epoch": 0.7262872628726287,
      "grad_norm": 12.005663871765137,
      "learning_rate": 7.262872628726287e-06,
      "loss": 1.5537,
      "step": 1876
    },
    {
      "epoch": 0.7266744096012389,
      "grad_norm": 14.243425369262695,
      "learning_rate": 7.266744096012389e-06,
      "loss": 1.1913,
      "step": 1877
    },
    {
      "epoch": 0.727061556329849,
      "grad_norm": 17.449832916259766,
      "learning_rate": 7.270615563298491e-06,
      "loss": 2.2363,
      "step": 1878
    },
    {
      "epoch": 0.7274487030584591,
      "grad_norm": 16.841358184814453,
      "learning_rate": 7.274487030584592e-06,
      "loss": 1.6643,
      "step": 1879
    },
    {
      "epoch": 0.7278358497870693,
      "grad_norm": 26.13020896911621,
      "learning_rate": 7.278358497870693e-06,
      "loss": 2.0316,
      "step": 1880
    },
    {
      "epoch": 0.7282229965156795,
      "grad_norm": 29.318880081176758,
      "learning_rate": 7.282229965156795e-06,
      "loss": 0.8627,
      "step": 1881
    },
    {
      "epoch": 0.7286101432442896,
      "grad_norm": 17.875864028930664,
      "learning_rate": 7.286101432442897e-06,
      "loss": 1.4629,
      "step": 1882
    },
    {
      "epoch": 0.7289972899728997,
      "grad_norm": 11.419010162353516,
      "learning_rate": 7.289972899728998e-06,
      "loss": 1.5076,
      "step": 1883
    },
    {
      "epoch": 0.7293844367015099,
      "grad_norm": 18.90157699584961,
      "learning_rate": 7.293844367015099e-06,
      "loss": 1.829,
      "step": 1884
    },
    {
      "epoch": 0.72977158343012,
      "grad_norm": 9.7317533493042,
      "learning_rate": 7.297715834301201e-06,
      "loss": 1.5391,
      "step": 1885
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 12.53754711151123,
      "learning_rate": 7.301587301587301e-06,
      "loss": 1.5928,
      "step": 1886
    },
    {
      "epoch": 0.7305458768873403,
      "grad_norm": 12.51893424987793,
      "learning_rate": 7.305458768873404e-06,
      "loss": 1.007,
      "step": 1887
    },
    {
      "epoch": 0.7309330236159505,
      "grad_norm": 11.269881248474121,
      "learning_rate": 7.309330236159505e-06,
      "loss": 1.2101,
      "step": 1888
    },
    {
      "epoch": 0.7313201703445605,
      "grad_norm": 9.009419441223145,
      "learning_rate": 7.313201703445606e-06,
      "loss": 1.0086,
      "step": 1889
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 12.65145206451416,
      "learning_rate": 7.317073170731707e-06,
      "loss": 2.235,
      "step": 1890
    },
    {
      "epoch": 0.7320944638017809,
      "grad_norm": 10.209554672241211,
      "learning_rate": 7.32094463801781e-06,
      "loss": 1.6514,
      "step": 1891
    },
    {
      "epoch": 0.7324816105303911,
      "grad_norm": 11.349108695983887,
      "learning_rate": 7.324816105303911e-06,
      "loss": 1.1478,
      "step": 1892
    },
    {
      "epoch": 0.7328687572590011,
      "grad_norm": 9.583183288574219,
      "learning_rate": 7.328687572590012e-06,
      "loss": 1.7413,
      "step": 1893
    },
    {
      "epoch": 0.7332559039876113,
      "grad_norm": 7.993554592132568,
      "learning_rate": 7.332559039876113e-06,
      "loss": 1.7242,
      "step": 1894
    },
    {
      "epoch": 0.7336430507162215,
      "grad_norm": 10.382140159606934,
      "learning_rate": 7.336430507162216e-06,
      "loss": 1.5912,
      "step": 1895
    },
    {
      "epoch": 0.7340301974448316,
      "grad_norm": 15.78878116607666,
      "learning_rate": 7.340301974448316e-06,
      "loss": 2.377,
      "step": 1896
    },
    {
      "epoch": 0.7344173441734417,
      "grad_norm": 27.374418258666992,
      "learning_rate": 7.344173441734418e-06,
      "loss": 2.0899,
      "step": 1897
    },
    {
      "epoch": 0.7348044909020519,
      "grad_norm": 26.64742088317871,
      "learning_rate": 7.348044909020519e-06,
      "loss": 2.1844,
      "step": 1898
    },
    {
      "epoch": 0.735191637630662,
      "grad_norm": 14.361444473266602,
      "learning_rate": 7.35191637630662e-06,
      "loss": 1.3762,
      "step": 1899
    },
    {
      "epoch": 0.7355787843592722,
      "grad_norm": 49.87547302246094,
      "learning_rate": 7.355787843592722e-06,
      "loss": 1.5382,
      "step": 1900
    },
    {
      "epoch": 0.7359659310878823,
      "grad_norm": 11.756628036499023,
      "learning_rate": 7.359659310878824e-06,
      "loss": 1.7408,
      "step": 1901
    },
    {
      "epoch": 0.7363530778164924,
      "grad_norm": 8.663679122924805,
      "learning_rate": 7.3635307781649245e-06,
      "loss": 1.0826,
      "step": 1902
    },
    {
      "epoch": 0.7367402245451026,
      "grad_norm": 12.705883979797363,
      "learning_rate": 7.367402245451026e-06,
      "loss": 2.0997,
      "step": 1903
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 13.711716651916504,
      "learning_rate": 7.371273712737128e-06,
      "loss": 2.0705,
      "step": 1904
    },
    {
      "epoch": 0.7375145180023229,
      "grad_norm": 14.677573204040527,
      "learning_rate": 7.37514518002323e-06,
      "loss": 1.5943,
      "step": 1905
    },
    {
      "epoch": 0.737901664730933,
      "grad_norm": 12.284815788269043,
      "learning_rate": 7.3790166473093304e-06,
      "loss": 0.7107,
      "step": 1906
    },
    {
      "epoch": 0.7382888114595432,
      "grad_norm": 13.438404083251953,
      "learning_rate": 7.382888114595432e-06,
      "loss": 1.174,
      "step": 1907
    },
    {
      "epoch": 0.7386759581881533,
      "grad_norm": 17.576393127441406,
      "learning_rate": 7.386759581881534e-06,
      "loss": 1.816,
      "step": 1908
    },
    {
      "epoch": 0.7390631049167634,
      "grad_norm": 30.27766990661621,
      "learning_rate": 7.390631049167635e-06,
      "loss": 2.1301,
      "step": 1909
    },
    {
      "epoch": 0.7394502516453736,
      "grad_norm": 10.517561912536621,
      "learning_rate": 7.394502516453736e-06,
      "loss": 1.5646,
      "step": 1910
    },
    {
      "epoch": 0.7398373983739838,
      "grad_norm": 9.080708503723145,
      "learning_rate": 7.398373983739838e-06,
      "loss": 1.0292,
      "step": 1911
    },
    {
      "epoch": 0.7402245451025938,
      "grad_norm": 10.82419204711914,
      "learning_rate": 7.4022454510259385e-06,
      "loss": 1.2173,
      "step": 1912
    },
    {
      "epoch": 0.740611691831204,
      "grad_norm": 22.4649600982666,
      "learning_rate": 7.406116918312041e-06,
      "loss": 2.1408,
      "step": 1913
    },
    {
      "epoch": 0.7409988385598142,
      "grad_norm": 10.43198299407959,
      "learning_rate": 7.409988385598142e-06,
      "loss": 1.0836,
      "step": 1914
    },
    {
      "epoch": 0.7413859852884244,
      "grad_norm": 14.869677543640137,
      "learning_rate": 7.413859852884244e-06,
      "loss": 1.7277,
      "step": 1915
    },
    {
      "epoch": 0.7417731320170344,
      "grad_norm": 10.25045394897461,
      "learning_rate": 7.4177313201703445e-06,
      "loss": 1.5877,
      "step": 1916
    },
    {
      "epoch": 0.7421602787456446,
      "grad_norm": 12.514726638793945,
      "learning_rate": 7.421602787456447e-06,
      "loss": 0.7247,
      "step": 1917
    },
    {
      "epoch": 0.7425474254742548,
      "grad_norm": 7.74023962020874,
      "learning_rate": 7.425474254742548e-06,
      "loss": 1.1478,
      "step": 1918
    },
    {
      "epoch": 0.7429345722028649,
      "grad_norm": 16.95342254638672,
      "learning_rate": 7.429345722028649e-06,
      "loss": 2.1886,
      "step": 1919
    },
    {
      "epoch": 0.743321718931475,
      "grad_norm": 11.470868110656738,
      "learning_rate": 7.4332171893147505e-06,
      "loss": 1.3637,
      "step": 1920
    },
    {
      "epoch": 0.7437088656600852,
      "grad_norm": 18.00442886352539,
      "learning_rate": 7.437088656600853e-06,
      "loss": 1.5468,
      "step": 1921
    },
    {
      "epoch": 0.7440960123886953,
      "grad_norm": 13.240797996520996,
      "learning_rate": 7.4409601238869535e-06,
      "loss": 1.8159,
      "step": 1922
    },
    {
      "epoch": 0.7444831591173054,
      "grad_norm": 9.970535278320312,
      "learning_rate": 7.444831591173055e-06,
      "loss": 1.0039,
      "step": 1923
    },
    {
      "epoch": 0.7448703058459156,
      "grad_norm": 19.18270492553711,
      "learning_rate": 7.4487030584591565e-06,
      "loss": 1.7457,
      "step": 1924
    },
    {
      "epoch": 0.7452574525745257,
      "grad_norm": 40.182559967041016,
      "learning_rate": 7.452574525745257e-06,
      "loss": 1.679,
      "step": 1925
    },
    {
      "epoch": 0.7456445993031359,
      "grad_norm": 14.895502090454102,
      "learning_rate": 7.4564459930313594e-06,
      "loss": 1.7317,
      "step": 1926
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 20.992162704467773,
      "learning_rate": 7.460317460317461e-06,
      "loss": 2.1566,
      "step": 1927
    },
    {
      "epoch": 0.7464188927603562,
      "grad_norm": 15.920352935791016,
      "learning_rate": 7.4641889276035624e-06,
      "loss": 1.6818,
      "step": 1928
    },
    {
      "epoch": 0.7468060394889663,
      "grad_norm": 9.74396800994873,
      "learning_rate": 7.468060394889663e-06,
      "loss": 1.643,
      "step": 1929
    },
    {
      "epoch": 0.7471931862175765,
      "grad_norm": 16.795475006103516,
      "learning_rate": 7.471931862175765e-06,
      "loss": 2.1981,
      "step": 1930
    },
    {
      "epoch": 0.7475803329461866,
      "grad_norm": 12.765235900878906,
      "learning_rate": 7.475803329461867e-06,
      "loss": 1.688,
      "step": 1931
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 20.37154197692871,
      "learning_rate": 7.4796747967479676e-06,
      "loss": 1.6172,
      "step": 1932
    },
    {
      "epoch": 0.7483546264034069,
      "grad_norm": 14.455954551696777,
      "learning_rate": 7.483546264034069e-06,
      "loss": 1.4268,
      "step": 1933
    },
    {
      "epoch": 0.7487417731320171,
      "grad_norm": 16.610145568847656,
      "learning_rate": 7.487417731320171e-06,
      "loss": 1.373,
      "step": 1934
    },
    {
      "epoch": 0.7491289198606271,
      "grad_norm": 10.616569519042969,
      "learning_rate": 7.491289198606272e-06,
      "loss": 1.1483,
      "step": 1935
    },
    {
      "epoch": 0.7495160665892373,
      "grad_norm": 12.589751243591309,
      "learning_rate": 7.4951606658923735e-06,
      "loss": 1.6187,
      "step": 1936
    },
    {
      "epoch": 0.7499032133178475,
      "grad_norm": 17.257892608642578,
      "learning_rate": 7.499032133178475e-06,
      "loss": 1.6075,
      "step": 1937
    },
    {
      "epoch": 0.7502903600464577,
      "grad_norm": 11.136000633239746,
      "learning_rate": 7.502903600464577e-06,
      "loss": 1.6087,
      "step": 1938
    },
    {
      "epoch": 0.7506775067750677,
      "grad_norm": 25.43038558959961,
      "learning_rate": 7.506775067750678e-06,
      "loss": 1.8974,
      "step": 1939
    },
    {
      "epoch": 0.7510646535036779,
      "grad_norm": 12.164017677307129,
      "learning_rate": 7.5106465350367795e-06,
      "loss": 1.6371,
      "step": 1940
    },
    {
      "epoch": 0.7514518002322881,
      "grad_norm": 12.394525527954102,
      "learning_rate": 7.514518002322881e-06,
      "loss": 1.5595,
      "step": 1941
    },
    {
      "epoch": 0.7518389469608981,
      "grad_norm": 14.04058837890625,
      "learning_rate": 7.5183894696089825e-06,
      "loss": 1.9507,
      "step": 1942
    },
    {
      "epoch": 0.7522260936895083,
      "grad_norm": 14.971114158630371,
      "learning_rate": 7.522260936895084e-06,
      "loss": 1.7544,
      "step": 1943
    },
    {
      "epoch": 0.7526132404181185,
      "grad_norm": 12.41119384765625,
      "learning_rate": 7.5261324041811855e-06,
      "loss": 1.8071,
      "step": 1944
    },
    {
      "epoch": 0.7530003871467286,
      "grad_norm": 9.41286563873291,
      "learning_rate": 7.530003871467286e-06,
      "loss": 1.5262,
      "step": 1945
    },
    {
      "epoch": 0.7533875338753387,
      "grad_norm": 9.069091796875,
      "learning_rate": 7.5338753387533885e-06,
      "loss": 1.4887,
      "step": 1946
    },
    {
      "epoch": 0.7537746806039489,
      "grad_norm": 15.7218599319458,
      "learning_rate": 7.53774680603949e-06,
      "loss": 0.7631,
      "step": 1947
    },
    {
      "epoch": 0.754161827332559,
      "grad_norm": 15.03204345703125,
      "learning_rate": 7.541618273325591e-06,
      "loss": 1.7159,
      "step": 1948
    },
    {
      "epoch": 0.7545489740611692,
      "grad_norm": 11.664665222167969,
      "learning_rate": 7.545489740611692e-06,
      "loss": 1.3532,
      "step": 1949
    },
    {
      "epoch": 0.7549361207897793,
      "grad_norm": 11.342138290405273,
      "learning_rate": 7.5493612078977944e-06,
      "loss": 1.2463,
      "step": 1950
    },
    {
      "epoch": 0.7553232675183895,
      "grad_norm": 13.915107727050781,
      "learning_rate": 7.553232675183896e-06,
      "loss": 1.8022,
      "step": 1951
    },
    {
      "epoch": 0.7557104142469996,
      "grad_norm": 15.865480422973633,
      "learning_rate": 7.557104142469997e-06,
      "loss": 1.612,
      "step": 1952
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 26.49051856994629,
      "learning_rate": 7.560975609756098e-06,
      "loss": 1.9109,
      "step": 1953
    },
    {
      "epoch": 0.7564847077042199,
      "grad_norm": 8.516701698303223,
      "learning_rate": 7.5648470770422e-06,
      "loss": 1.4265,
      "step": 1954
    },
    {
      "epoch": 0.75687185443283,
      "grad_norm": 13.553561210632324,
      "learning_rate": 7.568718544328301e-06,
      "loss": 1.2738,
      "step": 1955
    },
    {
      "epoch": 0.7572590011614402,
      "grad_norm": 11.326796531677246,
      "learning_rate": 7.5725900116144026e-06,
      "loss": 1.543,
      "step": 1956
    },
    {
      "epoch": 0.7576461478900504,
      "grad_norm": 8.318197250366211,
      "learning_rate": 7.576461478900504e-06,
      "loss": 1.5966,
      "step": 1957
    },
    {
      "epoch": 0.7580332946186604,
      "grad_norm": 18.933923721313477,
      "learning_rate": 7.580332946186605e-06,
      "loss": 1.3129,
      "step": 1958
    },
    {
      "epoch": 0.7584204413472706,
      "grad_norm": 13.768263816833496,
      "learning_rate": 7.584204413472707e-06,
      "loss": 1.6965,
      "step": 1959
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 17.770965576171875,
      "learning_rate": 7.5880758807588085e-06,
      "loss": 1.3627,
      "step": 1960
    },
    {
      "epoch": 0.759194734804491,
      "grad_norm": 9.434476852416992,
      "learning_rate": 7.59194734804491e-06,
      "loss": 1.6,
      "step": 1961
    },
    {
      "epoch": 0.759581881533101,
      "grad_norm": 17.576566696166992,
      "learning_rate": 7.595818815331011e-06,
      "loss": 1.5023,
      "step": 1962
    },
    {
      "epoch": 0.7599690282617112,
      "grad_norm": 10.181289672851562,
      "learning_rate": 7.599690282617113e-06,
      "loss": 1.3008,
      "step": 1963
    },
    {
      "epoch": 0.7603561749903214,
      "grad_norm": 26.192546844482422,
      "learning_rate": 7.6035617499032145e-06,
      "loss": 1.8328,
      "step": 1964
    },
    {
      "epoch": 0.7607433217189314,
      "grad_norm": 15.818461418151855,
      "learning_rate": 7.607433217189315e-06,
      "loss": 1.6455,
      "step": 1965
    },
    {
      "epoch": 0.7611304684475416,
      "grad_norm": 9.226573944091797,
      "learning_rate": 7.611304684475417e-06,
      "loss": 1.0409,
      "step": 1966
    },
    {
      "epoch": 0.7615176151761518,
      "grad_norm": 11.794890403747559,
      "learning_rate": 7.615176151761519e-06,
      "loss": 1.5478,
      "step": 1967
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 10.868616104125977,
      "learning_rate": 7.61904761904762e-06,
      "loss": 1.5269,
      "step": 1968
    },
    {
      "epoch": 0.762291908633372,
      "grad_norm": 11.16672420501709,
      "learning_rate": 7.622919086333721e-06,
      "loss": 1.7585,
      "step": 1969
    },
    {
      "epoch": 0.7626790553619822,
      "grad_norm": 11.509918212890625,
      "learning_rate": 7.626790553619823e-06,
      "loss": 1.2451,
      "step": 1970
    },
    {
      "epoch": 0.7630662020905923,
      "grad_norm": 15.383813858032227,
      "learning_rate": 7.630662020905924e-06,
      "loss": 1.7136,
      "step": 1971
    },
    {
      "epoch": 0.7634533488192025,
      "grad_norm": 22.048492431640625,
      "learning_rate": 7.634533488192025e-06,
      "loss": 2.6516,
      "step": 1972
    },
    {
      "epoch": 0.7638404955478126,
      "grad_norm": 16.15256690979004,
      "learning_rate": 7.638404955478127e-06,
      "loss": 1.5158,
      "step": 1973
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 11.238784790039062,
      "learning_rate": 7.64227642276423e-06,
      "loss": 1.6091,
      "step": 1974
    },
    {
      "epoch": 0.7646147890050329,
      "grad_norm": 12.923303604125977,
      "learning_rate": 7.64614789005033e-06,
      "loss": 0.8492,
      "step": 1975
    },
    {
      "epoch": 0.7650019357336431,
      "grad_norm": 16.92905616760254,
      "learning_rate": 7.65001935733643e-06,
      "loss": 2.2878,
      "step": 1976
    },
    {
      "epoch": 0.7653890824622532,
      "grad_norm": 20.7362117767334,
      "learning_rate": 7.653890824622533e-06,
      "loss": 1.8082,
      "step": 1977
    },
    {
      "epoch": 0.7657762291908633,
      "grad_norm": 10.814109802246094,
      "learning_rate": 7.657762291908634e-06,
      "loss": 1.5725,
      "step": 1978
    },
    {
      "epoch": 0.7661633759194735,
      "grad_norm": 13.205665588378906,
      "learning_rate": 7.661633759194736e-06,
      "loss": 1.297,
      "step": 1979
    },
    {
      "epoch": 0.7665505226480837,
      "grad_norm": 14.721141815185547,
      "learning_rate": 7.665505226480837e-06,
      "loss": 1.5405,
      "step": 1980
    },
    {
      "epoch": 0.7669376693766937,
      "grad_norm": 11.625903129577637,
      "learning_rate": 7.669376693766937e-06,
      "loss": 1.6432,
      "step": 1981
    },
    {
      "epoch": 0.7673248161053039,
      "grad_norm": 22.504732131958008,
      "learning_rate": 7.67324816105304e-06,
      "loss": 2.3335,
      "step": 1982
    },
    {
      "epoch": 0.7677119628339141,
      "grad_norm": 21.97978401184082,
      "learning_rate": 7.677119628339142e-06,
      "loss": 1.7436,
      "step": 1983
    },
    {
      "epoch": 0.7680991095625243,
      "grad_norm": 21.98860740661621,
      "learning_rate": 7.680991095625243e-06,
      "loss": 1.7779,
      "step": 1984
    },
    {
      "epoch": 0.7684862562911343,
      "grad_norm": 10.59450912475586,
      "learning_rate": 7.684862562911343e-06,
      "loss": 1.0224,
      "step": 1985
    },
    {
      "epoch": 0.7688734030197445,
      "grad_norm": 11.559497833251953,
      "learning_rate": 7.688734030197446e-06,
      "loss": 1.5719,
      "step": 1986
    },
    {
      "epoch": 0.7692605497483547,
      "grad_norm": 13.249598503112793,
      "learning_rate": 7.692605497483548e-06,
      "loss": 1.4843,
      "step": 1987
    },
    {
      "epoch": 0.7696476964769647,
      "grad_norm": 9.416162490844727,
      "learning_rate": 7.696476964769649e-06,
      "loss": 0.9553,
      "step": 1988
    },
    {
      "epoch": 0.7700348432055749,
      "grad_norm": 13.389005661010742,
      "learning_rate": 7.70034843205575e-06,
      "loss": 1.5369,
      "step": 1989
    },
    {
      "epoch": 0.7704219899341851,
      "grad_norm": 12.763663291931152,
      "learning_rate": 7.704219899341852e-06,
      "loss": 1.3062,
      "step": 1990
    },
    {
      "epoch": 0.7708091366627952,
      "grad_norm": 11.993646621704102,
      "learning_rate": 7.708091366627952e-06,
      "loss": 1.6829,
      "step": 1991
    },
    {
      "epoch": 0.7711962833914053,
      "grad_norm": 11.38809871673584,
      "learning_rate": 7.711962833914055e-06,
      "loss": 1.5214,
      "step": 1992
    },
    {
      "epoch": 0.7715834301200155,
      "grad_norm": 10.566888809204102,
      "learning_rate": 7.715834301200155e-06,
      "loss": 1.2411,
      "step": 1993
    },
    {
      "epoch": 0.7719705768486256,
      "grad_norm": 8.767128944396973,
      "learning_rate": 7.719705768486256e-06,
      "loss": 1.4924,
      "step": 1994
    },
    {
      "epoch": 0.7723577235772358,
      "grad_norm": 12.787650108337402,
      "learning_rate": 7.723577235772358e-06,
      "loss": 0.6316,
      "step": 1995
    },
    {
      "epoch": 0.7727448703058459,
      "grad_norm": 12.293133735656738,
      "learning_rate": 7.72744870305846e-06,
      "loss": 1.1826,
      "step": 1996
    },
    {
      "epoch": 0.7731320170344561,
      "grad_norm": 12.385100364685059,
      "learning_rate": 7.731320170344561e-06,
      "loss": 1.8622,
      "step": 1997
    },
    {
      "epoch": 0.7735191637630662,
      "grad_norm": 56.399009704589844,
      "learning_rate": 7.735191637630662e-06,
      "loss": 1.9428,
      "step": 1998
    },
    {
      "epoch": 0.7739063104916764,
      "grad_norm": 13.542095184326172,
      "learning_rate": 7.739063104916764e-06,
      "loss": 0.6426,
      "step": 1999
    },
    {
      "epoch": 0.7742934572202865,
      "grad_norm": 14.81740665435791,
      "learning_rate": 7.742934572202867e-06,
      "loss": 1.6861,
      "step": 2000
    },
    {
      "epoch": 0.7746806039488966,
      "grad_norm": 21.877193450927734,
      "learning_rate": 7.746806039488967e-06,
      "loss": 1.9161,
      "step": 2001
    },
    {
      "epoch": 0.7750677506775068,
      "grad_norm": 11.67359447479248,
      "learning_rate": 7.750677506775068e-06,
      "loss": 1.1329,
      "step": 2002
    },
    {
      "epoch": 0.775454897406117,
      "grad_norm": 18.83820343017578,
      "learning_rate": 7.75454897406117e-06,
      "loss": 1.7693,
      "step": 2003
    },
    {
      "epoch": 0.775842044134727,
      "grad_norm": 24.510290145874023,
      "learning_rate": 7.758420441347271e-06,
      "loss": 1.5313,
      "step": 2004
    },
    {
      "epoch": 0.7762291908633372,
      "grad_norm": 15.44631290435791,
      "learning_rate": 7.762291908633373e-06,
      "loss": 2.6878,
      "step": 2005
    },
    {
      "epoch": 0.7766163375919474,
      "grad_norm": 24.364273071289062,
      "learning_rate": 7.766163375919474e-06,
      "loss": 1.29,
      "step": 2006
    },
    {
      "epoch": 0.7770034843205574,
      "grad_norm": 15.23204231262207,
      "learning_rate": 7.770034843205574e-06,
      "loss": 2.1408,
      "step": 2007
    },
    {
      "epoch": 0.7773906310491676,
      "grad_norm": 12.872687339782715,
      "learning_rate": 7.773906310491677e-06,
      "loss": 1.3539,
      "step": 2008
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 16.5174560546875,
      "learning_rate": 7.77777777777778e-06,
      "loss": 2.1497,
      "step": 2009
    },
    {
      "epoch": 0.778164924506388,
      "grad_norm": 16.77091407775879,
      "learning_rate": 7.78164924506388e-06,
      "loss": 1.8685,
      "step": 2010
    },
    {
      "epoch": 0.778552071234998,
      "grad_norm": 11.650288581848145,
      "learning_rate": 7.78552071234998e-06,
      "loss": 2.1979,
      "step": 2011
    },
    {
      "epoch": 0.7789392179636082,
      "grad_norm": 8.83530330657959,
      "learning_rate": 7.789392179636083e-06,
      "loss": 1.3675,
      "step": 2012
    },
    {
      "epoch": 0.7793263646922184,
      "grad_norm": 9.870386123657227,
      "learning_rate": 7.793263646922185e-06,
      "loss": 1.7351,
      "step": 2013
    },
    {
      "epoch": 0.7797135114208285,
      "grad_norm": 13.531003952026367,
      "learning_rate": 7.797135114208286e-06,
      "loss": 1.4164,
      "step": 2014
    },
    {
      "epoch": 0.7801006581494386,
      "grad_norm": 8.458600044250488,
      "learning_rate": 7.801006581494386e-06,
      "loss": 1.5804,
      "step": 2015
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 14.386209487915039,
      "learning_rate": 7.804878048780489e-06,
      "loss": 1.6219,
      "step": 2016
    },
    {
      "epoch": 0.7808749516066589,
      "grad_norm": 8.184931755065918,
      "learning_rate": 7.80874951606659e-06,
      "loss": 1.4114,
      "step": 2017
    },
    {
      "epoch": 0.781262098335269,
      "grad_norm": 19.261884689331055,
      "learning_rate": 7.812620983352692e-06,
      "loss": 1.4959,
      "step": 2018
    },
    {
      "epoch": 0.7816492450638792,
      "grad_norm": 15.873129844665527,
      "learning_rate": 7.816492450638792e-06,
      "loss": 1.8075,
      "step": 2019
    },
    {
      "epoch": 0.7820363917924894,
      "grad_norm": 9.34190845489502,
      "learning_rate": 7.820363917924895e-06,
      "loss": 1.4395,
      "step": 2020
    },
    {
      "epoch": 0.7824235385210995,
      "grad_norm": 21.857839584350586,
      "learning_rate": 7.824235385210995e-06,
      "loss": 1.4694,
      "step": 2021
    },
    {
      "epoch": 0.7828106852497096,
      "grad_norm": 14.552980422973633,
      "learning_rate": 7.828106852497098e-06,
      "loss": 1.3637,
      "step": 2022
    },
    {
      "epoch": 0.7831978319783198,
      "grad_norm": 17.919906616210938,
      "learning_rate": 7.831978319783198e-06,
      "loss": 1.469,
      "step": 2023
    },
    {
      "epoch": 0.7835849787069299,
      "grad_norm": 10.988612174987793,
      "learning_rate": 7.835849787069299e-06,
      "loss": 1.7048,
      "step": 2024
    },
    {
      "epoch": 0.7839721254355401,
      "grad_norm": 20.872093200683594,
      "learning_rate": 7.839721254355401e-06,
      "loss": 1.887,
      "step": 2025
    },
    {
      "epoch": 0.7843592721641502,
      "grad_norm": 13.368658065795898,
      "learning_rate": 7.843592721641504e-06,
      "loss": 1.5149,
      "step": 2026
    },
    {
      "epoch": 0.7847464188927603,
      "grad_norm": 8.91414737701416,
      "learning_rate": 7.847464188927604e-06,
      "loss": 1.6903,
      "step": 2027
    },
    {
      "epoch": 0.7851335656213705,
      "grad_norm": 19.51024055480957,
      "learning_rate": 7.851335656213705e-06,
      "loss": 1.7898,
      "step": 2028
    },
    {
      "epoch": 0.7855207123499807,
      "grad_norm": 16.086170196533203,
      "learning_rate": 7.855207123499807e-06,
      "loss": 1.2394,
      "step": 2029
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 8.621088981628418,
      "learning_rate": 7.859078590785908e-06,
      "loss": 1.0175,
      "step": 2030
    },
    {
      "epoch": 0.7862950058072009,
      "grad_norm": 21.321758270263672,
      "learning_rate": 7.86295005807201e-06,
      "loss": 1.303,
      "step": 2031
    },
    {
      "epoch": 0.7866821525358111,
      "grad_norm": 16.430667877197266,
      "learning_rate": 7.866821525358111e-06,
      "loss": 1.7531,
      "step": 2032
    },
    {
      "epoch": 0.7870692992644213,
      "grad_norm": 16.769088745117188,
      "learning_rate": 7.870692992644213e-06,
      "loss": 1.8649,
      "step": 2033
    },
    {
      "epoch": 0.7874564459930313,
      "grad_norm": 16.180889129638672,
      "learning_rate": 7.874564459930314e-06,
      "loss": 1.6573,
      "step": 2034
    },
    {
      "epoch": 0.7878435927216415,
      "grad_norm": 9.34954833984375,
      "learning_rate": 7.878435927216416e-06,
      "loss": 1.6688,
      "step": 2035
    },
    {
      "epoch": 0.7882307394502517,
      "grad_norm": 11.340555191040039,
      "learning_rate": 7.882307394502517e-06,
      "loss": 1.7212,
      "step": 2036
    },
    {
      "epoch": 0.7886178861788617,
      "grad_norm": 10.588753700256348,
      "learning_rate": 7.886178861788618e-06,
      "loss": 1.2372,
      "step": 2037
    },
    {
      "epoch": 0.7890050329074719,
      "grad_norm": 10.633072853088379,
      "learning_rate": 7.89005032907472e-06,
      "loss": 1.4803,
      "step": 2038
    },
    {
      "epoch": 0.7893921796360821,
      "grad_norm": 18.350297927856445,
      "learning_rate": 7.893921796360822e-06,
      "loss": 1.8844,
      "step": 2039
    },
    {
      "epoch": 0.7897793263646922,
      "grad_norm": 12.915491104125977,
      "learning_rate": 7.897793263646923e-06,
      "loss": 1.4727,
      "step": 2040
    },
    {
      "epoch": 0.7901664730933023,
      "grad_norm": 12.982916831970215,
      "learning_rate": 7.901664730933024e-06,
      "loss": 1.404,
      "step": 2041
    },
    {
      "epoch": 0.7905536198219125,
      "grad_norm": 9.356689453125,
      "learning_rate": 7.905536198219126e-06,
      "loss": 1.5784,
      "step": 2042
    },
    {
      "epoch": 0.7909407665505227,
      "grad_norm": 14.527385711669922,
      "learning_rate": 7.909407665505228e-06,
      "loss": 2.1243,
      "step": 2043
    },
    {
      "epoch": 0.7913279132791328,
      "grad_norm": 8.472180366516113,
      "learning_rate": 7.913279132791329e-06,
      "loss": 1.5957,
      "step": 2044
    },
    {
      "epoch": 0.7917150600077429,
      "grad_norm": 16.374982833862305,
      "learning_rate": 7.91715060007743e-06,
      "loss": 1.7892,
      "step": 2045
    },
    {
      "epoch": 0.7921022067363531,
      "grad_norm": 9.015387535095215,
      "learning_rate": 7.921022067363532e-06,
      "loss": 1.3636,
      "step": 2046
    },
    {
      "epoch": 0.7924893534649632,
      "grad_norm": 10.786067962646484,
      "learning_rate": 7.924893534649633e-06,
      "loss": 2.2642,
      "step": 2047
    },
    {
      "epoch": 0.7928765001935734,
      "grad_norm": 26.038461685180664,
      "learning_rate": 7.928765001935735e-06,
      "loss": 1.1573,
      "step": 2048
    },
    {
      "epoch": 0.7932636469221835,
      "grad_norm": 25.198867797851562,
      "learning_rate": 7.932636469221836e-06,
      "loss": 2.2964,
      "step": 2049
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 13.941287994384766,
      "learning_rate": 7.936507936507936e-06,
      "loss": 1.297,
      "step": 2050
    },
    {
      "epoch": 0.7940379403794038,
      "grad_norm": 9.05423641204834,
      "learning_rate": 7.940379403794039e-06,
      "loss": 1.4229,
      "step": 2051
    },
    {
      "epoch": 0.794425087108014,
      "grad_norm": 10.740323066711426,
      "learning_rate": 7.94425087108014e-06,
      "loss": 1.5449,
      "step": 2052
    },
    {
      "epoch": 0.794812233836624,
      "grad_norm": 10.183048248291016,
      "learning_rate": 7.948122338366241e-06,
      "loss": 1.5528,
      "step": 2053
    },
    {
      "epoch": 0.7951993805652342,
      "grad_norm": 15.2091703414917,
      "learning_rate": 7.951993805652342e-06,
      "loss": 1.3319,
      "step": 2054
    },
    {
      "epoch": 0.7955865272938444,
      "grad_norm": 28.51121711730957,
      "learning_rate": 7.955865272938444e-06,
      "loss": 1.2127,
      "step": 2055
    },
    {
      "epoch": 0.7959736740224546,
      "grad_norm": 10.28612232208252,
      "learning_rate": 7.959736740224547e-06,
      "loss": 1.5281,
      "step": 2056
    },
    {
      "epoch": 0.7963608207510646,
      "grad_norm": 27.027082443237305,
      "learning_rate": 7.963608207510647e-06,
      "loss": 1.8897,
      "step": 2057
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 11.64136028289795,
      "learning_rate": 7.967479674796748e-06,
      "loss": 1.6135,
      "step": 2058
    },
    {
      "epoch": 0.797135114208285,
      "grad_norm": 12.317622184753418,
      "learning_rate": 7.97135114208285e-06,
      "loss": 1.137,
      "step": 2059
    },
    {
      "epoch": 0.797522260936895,
      "grad_norm": 18.41493797302246,
      "learning_rate": 7.975222609368951e-06,
      "loss": 1.428,
      "step": 2060
    },
    {
      "epoch": 0.7979094076655052,
      "grad_norm": 9.622678756713867,
      "learning_rate": 7.979094076655053e-06,
      "loss": 0.9097,
      "step": 2061
    },
    {
      "epoch": 0.7982965543941154,
      "grad_norm": 17.047924041748047,
      "learning_rate": 7.982965543941154e-06,
      "loss": 2.5142,
      "step": 2062
    },
    {
      "epoch": 0.7986837011227255,
      "grad_norm": 18.034547805786133,
      "learning_rate": 7.986837011227255e-06,
      "loss": 1.6548,
      "step": 2063
    },
    {
      "epoch": 0.7990708478513356,
      "grad_norm": 11.33230972290039,
      "learning_rate": 7.990708478513357e-06,
      "loss": 1.1807,
      "step": 2064
    },
    {
      "epoch": 0.7994579945799458,
      "grad_norm": 14.44831371307373,
      "learning_rate": 7.99457994579946e-06,
      "loss": 1.574,
      "step": 2065
    },
    {
      "epoch": 0.799845141308556,
      "grad_norm": 23.73099708557129,
      "learning_rate": 7.99845141308556e-06,
      "loss": 1.827,
      "step": 2066
    },
    {
      "epoch": 0.8002322880371661,
      "grad_norm": 10.230483055114746,
      "learning_rate": 8.00232288037166e-06,
      "loss": 1.4317,
      "step": 2067
    },
    {
      "epoch": 0.8006194347657762,
      "grad_norm": 17.13132095336914,
      "learning_rate": 8.006194347657763e-06,
      "loss": 2.127,
      "step": 2068
    },
    {
      "epoch": 0.8010065814943864,
      "grad_norm": 14.252950668334961,
      "learning_rate": 8.010065814943865e-06,
      "loss": 1.7097,
      "step": 2069
    },
    {
      "epoch": 0.8013937282229965,
      "grad_norm": 10.91816234588623,
      "learning_rate": 8.013937282229966e-06,
      "loss": 1.5602,
      "step": 2070
    },
    {
      "epoch": 0.8017808749516067,
      "grad_norm": 17.23995018005371,
      "learning_rate": 8.017808749516067e-06,
      "loss": 1.8387,
      "step": 2071
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 10.96123218536377,
      "learning_rate": 8.021680216802169e-06,
      "loss": 1.3474,
      "step": 2072
    },
    {
      "epoch": 0.8025551684088269,
      "grad_norm": 59.898433685302734,
      "learning_rate": 8.02555168408827e-06,
      "loss": 2.0066,
      "step": 2073
    },
    {
      "epoch": 0.8029423151374371,
      "grad_norm": 9.275467872619629,
      "learning_rate": 8.029423151374372e-06,
      "loss": 1.5191,
      "step": 2074
    },
    {
      "epoch": 0.8033294618660473,
      "grad_norm": 18.352914810180664,
      "learning_rate": 8.033294618660473e-06,
      "loss": 1.7544,
      "step": 2075
    },
    {
      "epoch": 0.8037166085946573,
      "grad_norm": 10.870415687561035,
      "learning_rate": 8.037166085946573e-06,
      "loss": 1.5455,
      "step": 2076
    },
    {
      "epoch": 0.8041037553232675,
      "grad_norm": 10.928840637207031,
      "learning_rate": 8.041037553232676e-06,
      "loss": 1.4908,
      "step": 2077
    },
    {
      "epoch": 0.8044909020518777,
      "grad_norm": 17.364770889282227,
      "learning_rate": 8.044909020518778e-06,
      "loss": 1.3619,
      "step": 2078
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 13.622734069824219,
      "learning_rate": 8.048780487804879e-06,
      "loss": 1.4025,
      "step": 2079
    },
    {
      "epoch": 0.8052651955090979,
      "grad_norm": 23.949111938476562,
      "learning_rate": 8.05265195509098e-06,
      "loss": 1.2608,
      "step": 2080
    },
    {
      "epoch": 0.8056523422377081,
      "grad_norm": 14.556358337402344,
      "learning_rate": 8.056523422377082e-06,
      "loss": 1.7842,
      "step": 2081
    },
    {
      "epoch": 0.8060394889663183,
      "grad_norm": 20.568449020385742,
      "learning_rate": 8.060394889663184e-06,
      "loss": 1.3438,
      "step": 2082
    },
    {
      "epoch": 0.8064266356949283,
      "grad_norm": 16.196861267089844,
      "learning_rate": 8.064266356949285e-06,
      "loss": 1.5254,
      "step": 2083
    },
    {
      "epoch": 0.8068137824235385,
      "grad_norm": 28.71220588684082,
      "learning_rate": 8.068137824235385e-06,
      "loss": 2.3215,
      "step": 2084
    },
    {
      "epoch": 0.8072009291521487,
      "grad_norm": 15.975860595703125,
      "learning_rate": 8.072009291521488e-06,
      "loss": 1.6948,
      "step": 2085
    },
    {
      "epoch": 0.8075880758807588,
      "grad_norm": 14.02998161315918,
      "learning_rate": 8.075880758807588e-06,
      "loss": 1.717,
      "step": 2086
    },
    {
      "epoch": 0.8079752226093689,
      "grad_norm": 41.80805969238281,
      "learning_rate": 8.07975222609369e-06,
      "loss": 1.6576,
      "step": 2087
    },
    {
      "epoch": 0.8083623693379791,
      "grad_norm": 21.00052261352539,
      "learning_rate": 8.083623693379791e-06,
      "loss": 2.0447,
      "step": 2088
    },
    {
      "epoch": 0.8087495160665893,
      "grad_norm": 44.310699462890625,
      "learning_rate": 8.087495160665894e-06,
      "loss": 1.9461,
      "step": 2089
    },
    {
      "epoch": 0.8091366627951994,
      "grad_norm": 10.55066204071045,
      "learning_rate": 8.091366627951994e-06,
      "loss": 0.9652,
      "step": 2090
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 11.417497634887695,
      "learning_rate": 8.095238095238097e-06,
      "loss": 1.5785,
      "step": 2091
    },
    {
      "epoch": 0.8099109562524197,
      "grad_norm": 11.924347877502441,
      "learning_rate": 8.099109562524197e-06,
      "loss": 1.4805,
      "step": 2092
    },
    {
      "epoch": 0.8102981029810298,
      "grad_norm": 18.078807830810547,
      "learning_rate": 8.102981029810298e-06,
      "loss": 1.6493,
      "step": 2093
    },
    {
      "epoch": 0.81068524970964,
      "grad_norm": 13.243597984313965,
      "learning_rate": 8.1068524970964e-06,
      "loss": 1.5893,
      "step": 2094
    },
    {
      "epoch": 0.8110723964382501,
      "grad_norm": 11.638444900512695,
      "learning_rate": 8.110723964382503e-06,
      "loss": 1.4378,
      "step": 2095
    },
    {
      "epoch": 0.8114595431668602,
      "grad_norm": 19.294845581054688,
      "learning_rate": 8.114595431668603e-06,
      "loss": 1.7136,
      "step": 2096
    },
    {
      "epoch": 0.8118466898954704,
      "grad_norm": 13.344961166381836,
      "learning_rate": 8.118466898954704e-06,
      "loss": 1.2045,
      "step": 2097
    },
    {
      "epoch": 0.8122338366240806,
      "grad_norm": 16.976343154907227,
      "learning_rate": 8.122338366240806e-06,
      "loss": 1.8163,
      "step": 2098
    },
    {
      "epoch": 0.8126209833526906,
      "grad_norm": 14.334086418151855,
      "learning_rate": 8.126209833526907e-06,
      "loss": 1.2376,
      "step": 2099
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 10.489884376525879,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.2154,
      "step": 2100
    },
    {
      "epoch": 0.813395276809911,
      "grad_norm": 24.210803985595703,
      "learning_rate": 8.13395276809911e-06,
      "loss": 2.2648,
      "step": 2101
    },
    {
      "epoch": 0.8137824235385211,
      "grad_norm": 15.344009399414062,
      "learning_rate": 8.137824235385212e-06,
      "loss": 1.7779,
      "step": 2102
    },
    {
      "epoch": 0.8141695702671312,
      "grad_norm": 10.385869026184082,
      "learning_rate": 8.141695702671313e-06,
      "loss": 1.7158,
      "step": 2103
    },
    {
      "epoch": 0.8145567169957414,
      "grad_norm": 16.728370666503906,
      "learning_rate": 8.145567169957415e-06,
      "loss": 2.1645,
      "step": 2104
    },
    {
      "epoch": 0.8149438637243516,
      "grad_norm": 11.507981300354004,
      "learning_rate": 8.149438637243516e-06,
      "loss": 1.6753,
      "step": 2105
    },
    {
      "epoch": 0.8153310104529616,
      "grad_norm": 25.624345779418945,
      "learning_rate": 8.153310104529616e-06,
      "loss": 2.1438,
      "step": 2106
    },
    {
      "epoch": 0.8157181571815718,
      "grad_norm": 16.021713256835938,
      "learning_rate": 8.157181571815719e-06,
      "loss": 1.8879,
      "step": 2107
    },
    {
      "epoch": 0.816105303910182,
      "grad_norm": 14.28944206237793,
      "learning_rate": 8.161053039101821e-06,
      "loss": 1.7357,
      "step": 2108
    },
    {
      "epoch": 0.8164924506387921,
      "grad_norm": 13.740686416625977,
      "learning_rate": 8.164924506387922e-06,
      "loss": 1.1798,
      "step": 2109
    },
    {
      "epoch": 0.8168795973674022,
      "grad_norm": 8.679463386535645,
      "learning_rate": 8.168795973674022e-06,
      "loss": 0.998,
      "step": 2110
    },
    {
      "epoch": 0.8172667440960124,
      "grad_norm": 14.888792037963867,
      "learning_rate": 8.172667440960125e-06,
      "loss": 1.8089,
      "step": 2111
    },
    {
      "epoch": 0.8176538908246226,
      "grad_norm": 12.143692970275879,
      "learning_rate": 8.176538908246227e-06,
      "loss": 1.5368,
      "step": 2112
    },
    {
      "epoch": 0.8180410375532327,
      "grad_norm": 25.47806167602539,
      "learning_rate": 8.180410375532328e-06,
      "loss": 1.699,
      "step": 2113
    },
    {
      "epoch": 0.8184281842818428,
      "grad_norm": 14.56418228149414,
      "learning_rate": 8.184281842818428e-06,
      "loss": 1.8969,
      "step": 2114
    },
    {
      "epoch": 0.818815331010453,
      "grad_norm": 11.470760345458984,
      "learning_rate": 8.18815331010453e-06,
      "loss": 1.2484,
      "step": 2115
    },
    {
      "epoch": 0.8192024777390631,
      "grad_norm": 15.581243515014648,
      "learning_rate": 8.192024777390631e-06,
      "loss": 1.5506,
      "step": 2116
    },
    {
      "epoch": 0.8195896244676733,
      "grad_norm": 17.52492904663086,
      "learning_rate": 8.195896244676734e-06,
      "loss": 1.4061,
      "step": 2117
    },
    {
      "epoch": 0.8199767711962834,
      "grad_norm": 12.266321182250977,
      "learning_rate": 8.199767711962834e-06,
      "loss": 1.6297,
      "step": 2118
    },
    {
      "epoch": 0.8203639179248935,
      "grad_norm": 13.054709434509277,
      "learning_rate": 8.203639179248935e-06,
      "loss": 1.6799,
      "step": 2119
    },
    {
      "epoch": 0.8207510646535037,
      "grad_norm": 10.41582202911377,
      "learning_rate": 8.207510646535037e-06,
      "loss": 1.771,
      "step": 2120
    },
    {
      "epoch": 0.8211382113821138,
      "grad_norm": 11.430129051208496,
      "learning_rate": 8.21138211382114e-06,
      "loss": 1.6314,
      "step": 2121
    },
    {
      "epoch": 0.8215253581107239,
      "grad_norm": 11.911107063293457,
      "learning_rate": 8.21525358110724e-06,
      "loss": 1.3811,
      "step": 2122
    },
    {
      "epoch": 0.8219125048393341,
      "grad_norm": 19.053007125854492,
      "learning_rate": 8.219125048393341e-06,
      "loss": 1.6573,
      "step": 2123
    },
    {
      "epoch": 0.8222996515679443,
      "grad_norm": 14.288761138916016,
      "learning_rate": 8.222996515679443e-06,
      "loss": 1.6902,
      "step": 2124
    },
    {
      "epoch": 0.8226867982965544,
      "grad_norm": 20.283615112304688,
      "learning_rate": 8.226867982965546e-06,
      "loss": 1.5305,
      "step": 2125
    },
    {
      "epoch": 0.8230739450251645,
      "grad_norm": 15.44629192352295,
      "learning_rate": 8.230739450251646e-06,
      "loss": 1.5435,
      "step": 2126
    },
    {
      "epoch": 0.8234610917537747,
      "grad_norm": 13.141427040100098,
      "learning_rate": 8.234610917537747e-06,
      "loss": 1.5959,
      "step": 2127
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 11.349234580993652,
      "learning_rate": 8.23848238482385e-06,
      "loss": 2.1449,
      "step": 2128
    },
    {
      "epoch": 0.8242353852109949,
      "grad_norm": 20.24184226989746,
      "learning_rate": 8.24235385210995e-06,
      "loss": 1.7835,
      "step": 2129
    },
    {
      "epoch": 0.8246225319396051,
      "grad_norm": 7.774609088897705,
      "learning_rate": 8.246225319396052e-06,
      "loss": 1.7144,
      "step": 2130
    },
    {
      "epoch": 0.8250096786682153,
      "grad_norm": 10.912466049194336,
      "learning_rate": 8.250096786682153e-06,
      "loss": 1.7148,
      "step": 2131
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 11.331450462341309,
      "learning_rate": 8.253968253968254e-06,
      "loss": 1.6081,
      "step": 2132
    },
    {
      "epoch": 0.8257839721254355,
      "grad_norm": 10.165979385375977,
      "learning_rate": 8.257839721254356e-06,
      "loss": 1.5764,
      "step": 2133
    },
    {
      "epoch": 0.8261711188540457,
      "grad_norm": 11.920113563537598,
      "learning_rate": 8.261711188540458e-06,
      "loss": 1.6907,
      "step": 2134
    },
    {
      "epoch": 0.8265582655826558,
      "grad_norm": 13.684804916381836,
      "learning_rate": 8.265582655826559e-06,
      "loss": 1.5503,
      "step": 2135
    },
    {
      "epoch": 0.826945412311266,
      "grad_norm": 14.786256790161133,
      "learning_rate": 8.26945412311266e-06,
      "loss": 1.6055,
      "step": 2136
    },
    {
      "epoch": 0.8273325590398761,
      "grad_norm": 19.786827087402344,
      "learning_rate": 8.273325590398762e-06,
      "loss": 2.1582,
      "step": 2137
    },
    {
      "epoch": 0.8277197057684863,
      "grad_norm": 17.46035385131836,
      "learning_rate": 8.277197057684864e-06,
      "loss": 1.3962,
      "step": 2138
    },
    {
      "epoch": 0.8281068524970964,
      "grad_norm": 15.019089698791504,
      "learning_rate": 8.281068524970965e-06,
      "loss": 1.8448,
      "step": 2139
    },
    {
      "epoch": 0.8284939992257065,
      "grad_norm": 13.277210235595703,
      "learning_rate": 8.284939992257065e-06,
      "loss": 1.7468,
      "step": 2140
    },
    {
      "epoch": 0.8288811459543167,
      "grad_norm": 13.310478210449219,
      "learning_rate": 8.288811459543168e-06,
      "loss": 1.6477,
      "step": 2141
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 15.190135955810547,
      "learning_rate": 8.292682926829268e-06,
      "loss": 1.6926,
      "step": 2142
    },
    {
      "epoch": 0.829655439411537,
      "grad_norm": 53.67072296142578,
      "learning_rate": 8.29655439411537e-06,
      "loss": 1.4221,
      "step": 2143
    },
    {
      "epoch": 0.8300425861401471,
      "grad_norm": 13.836723327636719,
      "learning_rate": 8.300425861401471e-06,
      "loss": 1.1746,
      "step": 2144
    },
    {
      "epoch": 0.8304297328687572,
      "grad_norm": 9.867587089538574,
      "learning_rate": 8.304297328687572e-06,
      "loss": 1.1726,
      "step": 2145
    },
    {
      "epoch": 0.8308168795973674,
      "grad_norm": 26.833621978759766,
      "learning_rate": 8.308168795973674e-06,
      "loss": 2.722,
      "step": 2146
    },
    {
      "epoch": 0.8312040263259776,
      "grad_norm": 12.140603065490723,
      "learning_rate": 8.312040263259777e-06,
      "loss": 1.0798,
      "step": 2147
    },
    {
      "epoch": 0.8315911730545877,
      "grad_norm": 10.524012565612793,
      "learning_rate": 8.315911730545877e-06,
      "loss": 1.1014,
      "step": 2148
    },
    {
      "epoch": 0.8319783197831978,
      "grad_norm": 14.24618911743164,
      "learning_rate": 8.319783197831978e-06,
      "loss": 1.598,
      "step": 2149
    },
    {
      "epoch": 0.832365466511808,
      "grad_norm": 10.414746284484863,
      "learning_rate": 8.32365466511808e-06,
      "loss": 1.4065,
      "step": 2150
    },
    {
      "epoch": 0.8327526132404182,
      "grad_norm": 23.890275955200195,
      "learning_rate": 8.327526132404183e-06,
      "loss": 1.6112,
      "step": 2151
    },
    {
      "epoch": 0.8331397599690282,
      "grad_norm": 14.585893630981445,
      "learning_rate": 8.331397599690283e-06,
      "loss": 1.189,
      "step": 2152
    },
    {
      "epoch": 0.8335269066976384,
      "grad_norm": 17.004106521606445,
      "learning_rate": 8.335269066976384e-06,
      "loss": 1.6558,
      "step": 2153
    },
    {
      "epoch": 0.8339140534262486,
      "grad_norm": 18.965782165527344,
      "learning_rate": 8.339140534262486e-06,
      "loss": 1.5459,
      "step": 2154
    },
    {
      "epoch": 0.8343012001548586,
      "grad_norm": 31.639196395874023,
      "learning_rate": 8.343012001548587e-06,
      "loss": 1.8591,
      "step": 2155
    },
    {
      "epoch": 0.8346883468834688,
      "grad_norm": 26.23279571533203,
      "learning_rate": 8.34688346883469e-06,
      "loss": 1.4141,
      "step": 2156
    },
    {
      "epoch": 0.835075493612079,
      "grad_norm": 8.058737754821777,
      "learning_rate": 8.35075493612079e-06,
      "loss": 1.4096,
      "step": 2157
    },
    {
      "epoch": 0.8354626403406891,
      "grad_norm": 19.554719924926758,
      "learning_rate": 8.35462640340689e-06,
      "loss": 1.8641,
      "step": 2158
    },
    {
      "epoch": 0.8358497870692992,
      "grad_norm": 18.35394287109375,
      "learning_rate": 8.358497870692993e-06,
      "loss": 1.591,
      "step": 2159
    },
    {
      "epoch": 0.8362369337979094,
      "grad_norm": 9.319986343383789,
      "learning_rate": 8.362369337979095e-06,
      "loss": 0.9274,
      "step": 2160
    },
    {
      "epoch": 0.8366240805265196,
      "grad_norm": 19.622718811035156,
      "learning_rate": 8.366240805265196e-06,
      "loss": 1.7086,
      "step": 2161
    },
    {
      "epoch": 0.8370112272551297,
      "grad_norm": 15.330463409423828,
      "learning_rate": 8.370112272551297e-06,
      "loss": 2.1132,
      "step": 2162
    },
    {
      "epoch": 0.8373983739837398,
      "grad_norm": 14.79594612121582,
      "learning_rate": 8.373983739837399e-06,
      "loss": 2.1258,
      "step": 2163
    },
    {
      "epoch": 0.83778552071235,
      "grad_norm": 9.78537368774414,
      "learning_rate": 8.377855207123501e-06,
      "loss": 1.1433,
      "step": 2164
    },
    {
      "epoch": 0.8381726674409601,
      "grad_norm": 8.926616668701172,
      "learning_rate": 8.381726674409602e-06,
      "loss": 1.5136,
      "step": 2165
    },
    {
      "epoch": 0.8385598141695703,
      "grad_norm": 15.533157348632812,
      "learning_rate": 8.385598141695703e-06,
      "loss": 1.629,
      "step": 2166
    },
    {
      "epoch": 0.8389469608981804,
      "grad_norm": 17.258729934692383,
      "learning_rate": 8.389469608981805e-06,
      "loss": 2.4779,
      "step": 2167
    },
    {
      "epoch": 0.8393341076267905,
      "grad_norm": 20.144193649291992,
      "learning_rate": 8.393341076267906e-06,
      "loss": 1.7957,
      "step": 2168
    },
    {
      "epoch": 0.8397212543554007,
      "grad_norm": 12.98810863494873,
      "learning_rate": 8.397212543554008e-06,
      "loss": 1.2313,
      "step": 2169
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 54.01066970825195,
      "learning_rate": 8.401084010840109e-06,
      "loss": 1.0123,
      "step": 2170
    },
    {
      "epoch": 0.840495547812621,
      "grad_norm": 14.167572021484375,
      "learning_rate": 8.404955478126211e-06,
      "loss": 0.6615,
      "step": 2171
    },
    {
      "epoch": 0.8408826945412311,
      "grad_norm": 9.89950942993164,
      "learning_rate": 8.408826945412312e-06,
      "loss": 1.4337,
      "step": 2172
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 11.962043762207031,
      "learning_rate": 8.412698412698414e-06,
      "loss": 1.7105,
      "step": 2173
    },
    {
      "epoch": 0.8416569879984515,
      "grad_norm": 13.910822868347168,
      "learning_rate": 8.416569879984515e-06,
      "loss": 1.96,
      "step": 2174
    },
    {
      "epoch": 0.8420441347270615,
      "grad_norm": 19.704757690429688,
      "learning_rate": 8.420441347270615e-06,
      "loss": 3.1724,
      "step": 2175
    },
    {
      "epoch": 0.8424312814556717,
      "grad_norm": 13.390996932983398,
      "learning_rate": 8.424312814556718e-06,
      "loss": 1.7029,
      "step": 2176
    },
    {
      "epoch": 0.8428184281842819,
      "grad_norm": 11.462701797485352,
      "learning_rate": 8.42818428184282e-06,
      "loss": 1.1609,
      "step": 2177
    },
    {
      "epoch": 0.8432055749128919,
      "grad_norm": 20.789403915405273,
      "learning_rate": 8.43205574912892e-06,
      "loss": 2.0266,
      "step": 2178
    },
    {
      "epoch": 0.8435927216415021,
      "grad_norm": 10.42020320892334,
      "learning_rate": 8.435927216415021e-06,
      "loss": 1.6771,
      "step": 2179
    },
    {
      "epoch": 0.8439798683701123,
      "grad_norm": 18.41660499572754,
      "learning_rate": 8.439798683701124e-06,
      "loss": 1.6697,
      "step": 2180
    },
    {
      "epoch": 0.8443670150987224,
      "grad_norm": 14.073445320129395,
      "learning_rate": 8.443670150987224e-06,
      "loss": 1.1859,
      "step": 2181
    },
    {
      "epoch": 0.8447541618273325,
      "grad_norm": 15.706205368041992,
      "learning_rate": 8.447541618273327e-06,
      "loss": 1.4689,
      "step": 2182
    },
    {
      "epoch": 0.8451413085559427,
      "grad_norm": 8.167473793029785,
      "learning_rate": 8.451413085559427e-06,
      "loss": 1.3869,
      "step": 2183
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 12.855470657348633,
      "learning_rate": 8.45528455284553e-06,
      "loss": 1.4704,
      "step": 2184
    },
    {
      "epoch": 0.845915602013163,
      "grad_norm": 37.359683990478516,
      "learning_rate": 8.45915602013163e-06,
      "loss": 2.1863,
      "step": 2185
    },
    {
      "epoch": 0.8463027487417731,
      "grad_norm": 16.507675170898438,
      "learning_rate": 8.463027487417732e-06,
      "loss": 1.2882,
      "step": 2186
    },
    {
      "epoch": 0.8466898954703833,
      "grad_norm": 13.112751007080078,
      "learning_rate": 8.466898954703833e-06,
      "loss": 1.6452,
      "step": 2187
    },
    {
      "epoch": 0.8470770421989934,
      "grad_norm": 18.63865852355957,
      "learning_rate": 8.470770421989934e-06,
      "loss": 2.1938,
      "step": 2188
    },
    {
      "epoch": 0.8474641889276036,
      "grad_norm": 19.7222900390625,
      "learning_rate": 8.474641889276036e-06,
      "loss": 2.2125,
      "step": 2189
    },
    {
      "epoch": 0.8478513356562137,
      "grad_norm": 23.135313034057617,
      "learning_rate": 8.478513356562138e-06,
      "loss": 1.8557,
      "step": 2190
    },
    {
      "epoch": 0.8482384823848238,
      "grad_norm": 9.902103424072266,
      "learning_rate": 8.482384823848239e-06,
      "loss": 1.1417,
      "step": 2191
    },
    {
      "epoch": 0.848625629113434,
      "grad_norm": 11.430632591247559,
      "learning_rate": 8.48625629113434e-06,
      "loss": 1.2018,
      "step": 2192
    },
    {
      "epoch": 0.8490127758420442,
      "grad_norm": 24.98682975769043,
      "learning_rate": 8.490127758420442e-06,
      "loss": 1.4995,
      "step": 2193
    },
    {
      "epoch": 0.8493999225706543,
      "grad_norm": 35.070133209228516,
      "learning_rate": 8.493999225706544e-06,
      "loss": 1.4302,
      "step": 2194
    },
    {
      "epoch": 0.8497870692992644,
      "grad_norm": 29.556665420532227,
      "learning_rate": 8.497870692992645e-06,
      "loss": 1.2417,
      "step": 2195
    },
    {
      "epoch": 0.8501742160278746,
      "grad_norm": 14.488005638122559,
      "learning_rate": 8.501742160278746e-06,
      "loss": 1.3895,
      "step": 2196
    },
    {
      "epoch": 0.8505613627564848,
      "grad_norm": 9.084390640258789,
      "learning_rate": 8.505613627564848e-06,
      "loss": 1.4808,
      "step": 2197
    },
    {
      "epoch": 0.8509485094850948,
      "grad_norm": 9.788613319396973,
      "learning_rate": 8.509485094850949e-06,
      "loss": 1.5267,
      "step": 2198
    },
    {
      "epoch": 0.851335656213705,
      "grad_norm": 11.75755786895752,
      "learning_rate": 8.513356562137051e-06,
      "loss": 1.6979,
      "step": 2199
    },
    {
      "epoch": 0.8517228029423152,
      "grad_norm": 13.124393463134766,
      "learning_rate": 8.517228029423152e-06,
      "loss": 1.6381,
      "step": 2200
    },
    {
      "epoch": 0.8521099496709252,
      "grad_norm": 11.733028411865234,
      "learning_rate": 8.521099496709252e-06,
      "loss": 1.6207,
      "step": 2201
    },
    {
      "epoch": 0.8524970963995354,
      "grad_norm": 19.760021209716797,
      "learning_rate": 8.524970963995355e-06,
      "loss": 1.5988,
      "step": 2202
    },
    {
      "epoch": 0.8528842431281456,
      "grad_norm": 15.45322036743164,
      "learning_rate": 8.528842431281457e-06,
      "loss": 1.612,
      "step": 2203
    },
    {
      "epoch": 0.8532713898567557,
      "grad_norm": 25.96436309814453,
      "learning_rate": 8.532713898567558e-06,
      "loss": 2.0123,
      "step": 2204
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 18.135671615600586,
      "learning_rate": 8.536585365853658e-06,
      "loss": 1.4325,
      "step": 2205
    },
    {
      "epoch": 0.854045683313976,
      "grad_norm": 9.199226379394531,
      "learning_rate": 8.54045683313976e-06,
      "loss": 0.9241,
      "step": 2206
    },
    {
      "epoch": 0.8544328300425862,
      "grad_norm": 31.8306941986084,
      "learning_rate": 8.544328300425863e-06,
      "loss": 1.6128,
      "step": 2207
    },
    {
      "epoch": 0.8548199767711963,
      "grad_norm": 10.949952125549316,
      "learning_rate": 8.548199767711964e-06,
      "loss": 1.2165,
      "step": 2208
    },
    {
      "epoch": 0.8552071234998064,
      "grad_norm": 17.183326721191406,
      "learning_rate": 8.552071234998064e-06,
      "loss": 1.9418,
      "step": 2209
    },
    {
      "epoch": 0.8555942702284166,
      "grad_norm": 17.843435287475586,
      "learning_rate": 8.555942702284167e-06,
      "loss": 1.6941,
      "step": 2210
    },
    {
      "epoch": 0.8559814169570267,
      "grad_norm": 17.529129028320312,
      "learning_rate": 8.559814169570267e-06,
      "loss": 1.7653,
      "step": 2211
    },
    {
      "epoch": 0.8563685636856369,
      "grad_norm": 15.642940521240234,
      "learning_rate": 8.56368563685637e-06,
      "loss": 1.5423,
      "step": 2212
    },
    {
      "epoch": 0.856755710414247,
      "grad_norm": 15.752030372619629,
      "learning_rate": 8.56755710414247e-06,
      "loss": 1.7215,
      "step": 2213
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 23.798702239990234,
      "learning_rate": 8.571428571428571e-06,
      "loss": 1.5234,
      "step": 2214
    },
    {
      "epoch": 0.8575300038714673,
      "grad_norm": 24.3959903717041,
      "learning_rate": 8.575300038714673e-06,
      "loss": 1.217,
      "step": 2215
    },
    {
      "epoch": 0.8579171506000774,
      "grad_norm": 9.18332576751709,
      "learning_rate": 8.579171506000776e-06,
      "loss": 1.6293,
      "step": 2216
    },
    {
      "epoch": 0.8583042973286876,
      "grad_norm": 11.722647666931152,
      "learning_rate": 8.583042973286876e-06,
      "loss": 1.6938,
      "step": 2217
    },
    {
      "epoch": 0.8586914440572977,
      "grad_norm": 14.896014213562012,
      "learning_rate": 8.586914440572977e-06,
      "loss": 2.1324,
      "step": 2218
    },
    {
      "epoch": 0.8590785907859079,
      "grad_norm": 15.223676681518555,
      "learning_rate": 8.59078590785908e-06,
      "loss": 1.2524,
      "step": 2219
    },
    {
      "epoch": 0.859465737514518,
      "grad_norm": 12.06539535522461,
      "learning_rate": 8.594657375145182e-06,
      "loss": 1.5714,
      "step": 2220
    },
    {
      "epoch": 0.8598528842431281,
      "grad_norm": 16.286516189575195,
      "learning_rate": 8.598528842431282e-06,
      "loss": 2.1073,
      "step": 2221
    },
    {
      "epoch": 0.8602400309717383,
      "grad_norm": 13.37855339050293,
      "learning_rate": 8.602400309717383e-06,
      "loss": 1.2769,
      "step": 2222
    },
    {
      "epoch": 0.8606271777003485,
      "grad_norm": 14.750328063964844,
      "learning_rate": 8.606271777003485e-06,
      "loss": 1.1355,
      "step": 2223
    },
    {
      "epoch": 0.8610143244289585,
      "grad_norm": 12.071784973144531,
      "learning_rate": 8.610143244289586e-06,
      "loss": 1.6066,
      "step": 2224
    },
    {
      "epoch": 0.8614014711575687,
      "grad_norm": 16.02212905883789,
      "learning_rate": 8.614014711575688e-06,
      "loss": 1.8746,
      "step": 2225
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 10.740756034851074,
      "learning_rate": 8.617886178861789e-06,
      "loss": 1.1276,
      "step": 2226
    },
    {
      "epoch": 0.862175764614789,
      "grad_norm": 14.076102256774902,
      "learning_rate": 8.62175764614789e-06,
      "loss": 1.3813,
      "step": 2227
    },
    {
      "epoch": 0.8625629113433991,
      "grad_norm": 11.554852485656738,
      "learning_rate": 8.625629113433992e-06,
      "loss": 1.6069,
      "step": 2228
    },
    {
      "epoch": 0.8629500580720093,
      "grad_norm": 13.958612442016602,
      "learning_rate": 8.629500580720094e-06,
      "loss": 1.5389,
      "step": 2229
    },
    {
      "epoch": 0.8633372048006195,
      "grad_norm": 14.800100326538086,
      "learning_rate": 8.633372048006195e-06,
      "loss": 1.1257,
      "step": 2230
    },
    {
      "epoch": 0.8637243515292296,
      "grad_norm": 12.484302520751953,
      "learning_rate": 8.637243515292295e-06,
      "loss": 2.019,
      "step": 2231
    },
    {
      "epoch": 0.8641114982578397,
      "grad_norm": 12.822870254516602,
      "learning_rate": 8.641114982578398e-06,
      "loss": 1.6302,
      "step": 2232
    },
    {
      "epoch": 0.8644986449864499,
      "grad_norm": 16.397939682006836,
      "learning_rate": 8.6449864498645e-06,
      "loss": 1.9165,
      "step": 2233
    },
    {
      "epoch": 0.86488579171506,
      "grad_norm": 17.83855628967285,
      "learning_rate": 8.6488579171506e-06,
      "loss": 1.4446,
      "step": 2234
    },
    {
      "epoch": 0.8652729384436701,
      "grad_norm": 18.972755432128906,
      "learning_rate": 8.652729384436701e-06,
      "loss": 1.8924,
      "step": 2235
    },
    {
      "epoch": 0.8656600851722803,
      "grad_norm": 21.031139373779297,
      "learning_rate": 8.656600851722804e-06,
      "loss": 2.4871,
      "step": 2236
    },
    {
      "epoch": 0.8660472319008904,
      "grad_norm": 17.726720809936523,
      "learning_rate": 8.660472319008904e-06,
      "loss": 2.0166,
      "step": 2237
    },
    {
      "epoch": 0.8664343786295006,
      "grad_norm": 14.419464111328125,
      "learning_rate": 8.664343786295007e-06,
      "loss": 2.0454,
      "step": 2238
    },
    {
      "epoch": 0.8668215253581107,
      "grad_norm": 20.74118995666504,
      "learning_rate": 8.668215253581107e-06,
      "loss": 1.7399,
      "step": 2239
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 20.333431243896484,
      "learning_rate": 8.67208672086721e-06,
      "loss": 1.3718,
      "step": 2240
    },
    {
      "epoch": 0.867595818815331,
      "grad_norm": 20.801889419555664,
      "learning_rate": 8.67595818815331e-06,
      "loss": 1.4769,
      "step": 2241
    },
    {
      "epoch": 0.8679829655439412,
      "grad_norm": 15.907133102416992,
      "learning_rate": 8.679829655439413e-06,
      "loss": 1.4795,
      "step": 2242
    },
    {
      "epoch": 0.8683701122725513,
      "grad_norm": 12.965600967407227,
      "learning_rate": 8.683701122725513e-06,
      "loss": 0.904,
      "step": 2243
    },
    {
      "epoch": 0.8687572590011614,
      "grad_norm": 12.582987785339355,
      "learning_rate": 8.687572590011614e-06,
      "loss": 1.1121,
      "step": 2244
    },
    {
      "epoch": 0.8691444057297716,
      "grad_norm": 23.34198570251465,
      "learning_rate": 8.691444057297716e-06,
      "loss": 1.1769,
      "step": 2245
    },
    {
      "epoch": 0.8695315524583818,
      "grad_norm": 12.891820907592773,
      "learning_rate": 8.695315524583819e-06,
      "loss": 0.7337,
      "step": 2246
    },
    {
      "epoch": 0.8699186991869918,
      "grad_norm": 24.68622398376465,
      "learning_rate": 8.69918699186992e-06,
      "loss": 2.071,
      "step": 2247
    },
    {
      "epoch": 0.870305845915602,
      "grad_norm": 14.691848754882812,
      "learning_rate": 8.70305845915602e-06,
      "loss": 2.1575,
      "step": 2248
    },
    {
      "epoch": 0.8706929926442122,
      "grad_norm": 20.639516830444336,
      "learning_rate": 8.706929926442122e-06,
      "loss": 2.0549,
      "step": 2249
    },
    {
      "epoch": 0.8710801393728222,
      "grad_norm": 12.297811508178711,
      "learning_rate": 8.710801393728223e-06,
      "loss": 1.4962,
      "step": 2250
    },
    {
      "epoch": 0.8714672861014324,
      "grad_norm": 17.77300262451172,
      "learning_rate": 8.714672861014325e-06,
      "loss": 2.23,
      "step": 2251
    },
    {
      "epoch": 0.8718544328300426,
      "grad_norm": 11.60481071472168,
      "learning_rate": 8.718544328300426e-06,
      "loss": 1.6528,
      "step": 2252
    },
    {
      "epoch": 0.8722415795586528,
      "grad_norm": 15.060504913330078,
      "learning_rate": 8.722415795586528e-06,
      "loss": 1.5089,
      "step": 2253
    },
    {
      "epoch": 0.8726287262872628,
      "grad_norm": 14.47146224975586,
      "learning_rate": 8.726287262872629e-06,
      "loss": 1.4909,
      "step": 2254
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 11.070438385009766,
      "learning_rate": 8.730158730158731e-06,
      "loss": 1.4419,
      "step": 2255
    },
    {
      "epoch": 0.8734030197444832,
      "grad_norm": 12.088664054870605,
      "learning_rate": 8.734030197444832e-06,
      "loss": 1.1327,
      "step": 2256
    },
    {
      "epoch": 0.8737901664730933,
      "grad_norm": 17.66997528076172,
      "learning_rate": 8.737901664730933e-06,
      "loss": 1.4289,
      "step": 2257
    },
    {
      "epoch": 0.8741773132017034,
      "grad_norm": 14.155426025390625,
      "learning_rate": 8.741773132017035e-06,
      "loss": 1.535,
      "step": 2258
    },
    {
      "epoch": 0.8745644599303136,
      "grad_norm": 12.567061424255371,
      "learning_rate": 8.745644599303137e-06,
      "loss": 1.4027,
      "step": 2259
    },
    {
      "epoch": 0.8749516066589237,
      "grad_norm": 15.7269926071167,
      "learning_rate": 8.749516066589238e-06,
      "loss": 1.4648,
      "step": 2260
    },
    {
      "epoch": 0.8753387533875339,
      "grad_norm": 20.81868553161621,
      "learning_rate": 8.753387533875339e-06,
      "loss": 1.9668,
      "step": 2261
    },
    {
      "epoch": 0.875725900116144,
      "grad_norm": 12.825963973999023,
      "learning_rate": 8.757259001161441e-06,
      "loss": 1.662,
      "step": 2262
    },
    {
      "epoch": 0.8761130468447541,
      "grad_norm": 13.30033016204834,
      "learning_rate": 8.761130468447542e-06,
      "loss": 1.7352,
      "step": 2263
    },
    {
      "epoch": 0.8765001935733643,
      "grad_norm": 16.659343719482422,
      "learning_rate": 8.765001935733644e-06,
      "loss": 1.3876,
      "step": 2264
    },
    {
      "epoch": 0.8768873403019745,
      "grad_norm": 16.27847671508789,
      "learning_rate": 8.768873403019745e-06,
      "loss": 1.5372,
      "step": 2265
    },
    {
      "epoch": 0.8772744870305846,
      "grad_norm": 19.99696922302246,
      "learning_rate": 8.772744870305847e-06,
      "loss": 0.9447,
      "step": 2266
    },
    {
      "epoch": 0.8776616337591947,
      "grad_norm": 15.020037651062012,
      "learning_rate": 8.776616337591948e-06,
      "loss": 1.401,
      "step": 2267
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 13.22045612335205,
      "learning_rate": 8.78048780487805e-06,
      "loss": 1.5589,
      "step": 2268
    },
    {
      "epoch": 0.8784359272164151,
      "grad_norm": 28.87611198425293,
      "learning_rate": 8.78435927216415e-06,
      "loss": 1.5489,
      "step": 2269
    },
    {
      "epoch": 0.8788230739450251,
      "grad_norm": 14.399734497070312,
      "learning_rate": 8.788230739450251e-06,
      "loss": 1.6196,
      "step": 2270
    },
    {
      "epoch": 0.8792102206736353,
      "grad_norm": 18.10629653930664,
      "learning_rate": 8.792102206736354e-06,
      "loss": 1.4175,
      "step": 2271
    },
    {
      "epoch": 0.8795973674022455,
      "grad_norm": 11.851116180419922,
      "learning_rate": 8.795973674022456e-06,
      "loss": 1.1562,
      "step": 2272
    },
    {
      "epoch": 0.8799845141308555,
      "grad_norm": 16.055912017822266,
      "learning_rate": 8.799845141308556e-06,
      "loss": 1.5998,
      "step": 2273
    },
    {
      "epoch": 0.8803716608594657,
      "grad_norm": 32.25862503051758,
      "learning_rate": 8.803716608594657e-06,
      "loss": 1.5831,
      "step": 2274
    },
    {
      "epoch": 0.8807588075880759,
      "grad_norm": 15.607915878295898,
      "learning_rate": 8.80758807588076e-06,
      "loss": 1.5792,
      "step": 2275
    },
    {
      "epoch": 0.8811459543166861,
      "grad_norm": 18.523290634155273,
      "learning_rate": 8.811459543166862e-06,
      "loss": 1.6403,
      "step": 2276
    },
    {
      "epoch": 0.8815331010452961,
      "grad_norm": 23.760475158691406,
      "learning_rate": 8.815331010452962e-06,
      "loss": 1.4693,
      "step": 2277
    },
    {
      "epoch": 0.8819202477739063,
      "grad_norm": 17.157516479492188,
      "learning_rate": 8.819202477739063e-06,
      "loss": 1.7005,
      "step": 2278
    },
    {
      "epoch": 0.8823073945025165,
      "grad_norm": 17.168176651000977,
      "learning_rate": 8.823073945025165e-06,
      "loss": 1.5906,
      "step": 2279
    },
    {
      "epoch": 0.8826945412311266,
      "grad_norm": 16.34307861328125,
      "learning_rate": 8.826945412311266e-06,
      "loss": 1.4369,
      "step": 2280
    },
    {
      "epoch": 0.8830816879597367,
      "grad_norm": 14.74492073059082,
      "learning_rate": 8.830816879597368e-06,
      "loss": 1.2281,
      "step": 2281
    },
    {
      "epoch": 0.8834688346883469,
      "grad_norm": 11.97536563873291,
      "learning_rate": 8.834688346883469e-06,
      "loss": 1.3218,
      "step": 2282
    },
    {
      "epoch": 0.883855981416957,
      "grad_norm": 24.429309844970703,
      "learning_rate": 8.83855981416957e-06,
      "loss": 1.4704,
      "step": 2283
    },
    {
      "epoch": 0.8842431281455672,
      "grad_norm": 12.320389747619629,
      "learning_rate": 8.842431281455672e-06,
      "loss": 1.1616,
      "step": 2284
    },
    {
      "epoch": 0.8846302748741773,
      "grad_norm": 14.957619667053223,
      "learning_rate": 8.846302748741774e-06,
      "loss": 1.5386,
      "step": 2285
    },
    {
      "epoch": 0.8850174216027874,
      "grad_norm": 12.924386978149414,
      "learning_rate": 8.850174216027875e-06,
      "loss": 1.4798,
      "step": 2286
    },
    {
      "epoch": 0.8854045683313976,
      "grad_norm": 16.73602867126465,
      "learning_rate": 8.854045683313976e-06,
      "loss": 1.3266,
      "step": 2287
    },
    {
      "epoch": 0.8857917150600078,
      "grad_norm": 11.90524959564209,
      "learning_rate": 8.857917150600078e-06,
      "loss": 1.1304,
      "step": 2288
    },
    {
      "epoch": 0.8861788617886179,
      "grad_norm": 13.111261367797852,
      "learning_rate": 8.86178861788618e-06,
      "loss": 1.612,
      "step": 2289
    },
    {
      "epoch": 0.886566008517228,
      "grad_norm": 10.81860065460205,
      "learning_rate": 8.865660085172281e-06,
      "loss": 1.0047,
      "step": 2290
    },
    {
      "epoch": 0.8869531552458382,
      "grad_norm": 11.84177303314209,
      "learning_rate": 8.869531552458382e-06,
      "loss": 0.9812,
      "step": 2291
    },
    {
      "epoch": 0.8873403019744484,
      "grad_norm": 20.840534210205078,
      "learning_rate": 8.873403019744484e-06,
      "loss": 1.7371,
      "step": 2292
    },
    {
      "epoch": 0.8877274487030584,
      "grad_norm": 22.989032745361328,
      "learning_rate": 8.877274487030585e-06,
      "loss": 1.6599,
      "step": 2293
    },
    {
      "epoch": 0.8881145954316686,
      "grad_norm": 12.005914688110352,
      "learning_rate": 8.881145954316687e-06,
      "loss": 1.1745,
      "step": 2294
    },
    {
      "epoch": 0.8885017421602788,
      "grad_norm": 13.745367050170898,
      "learning_rate": 8.885017421602788e-06,
      "loss": 1.7005,
      "step": 2295
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 13.400556564331055,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.4364,
      "step": 2296
    },
    {
      "epoch": 0.889276035617499,
      "grad_norm": 15.58880615234375,
      "learning_rate": 8.89276035617499e-06,
      "loss": 1.2581,
      "step": 2297
    },
    {
      "epoch": 0.8896631823461092,
      "grad_norm": 15.218204498291016,
      "learning_rate": 8.896631823461093e-06,
      "loss": 1.6279,
      "step": 2298
    },
    {
      "epoch": 0.8900503290747194,
      "grad_norm": 10.251004219055176,
      "learning_rate": 8.900503290747194e-06,
      "loss": 1.5468,
      "step": 2299
    },
    {
      "epoch": 0.8904374758033294,
      "grad_norm": 11.572959899902344,
      "learning_rate": 8.904374758033294e-06,
      "loss": 1.119,
      "step": 2300
    },
    {
      "epoch": 0.8908246225319396,
      "grad_norm": 13.885424613952637,
      "learning_rate": 8.908246225319397e-06,
      "loss": 2.0078,
      "step": 2301
    },
    {
      "epoch": 0.8912117692605498,
      "grad_norm": 8.547845840454102,
      "learning_rate": 8.912117692605499e-06,
      "loss": 1.403,
      "step": 2302
    },
    {
      "epoch": 0.8915989159891599,
      "grad_norm": 19.928274154663086,
      "learning_rate": 8.9159891598916e-06,
      "loss": 2.2332,
      "step": 2303
    },
    {
      "epoch": 0.89198606271777,
      "grad_norm": 10.80008602142334,
      "learning_rate": 8.9198606271777e-06,
      "loss": 1.5958,
      "step": 2304
    },
    {
      "epoch": 0.8923732094463802,
      "grad_norm": 10.781627655029297,
      "learning_rate": 8.923732094463803e-06,
      "loss": 1.1257,
      "step": 2305
    },
    {
      "epoch": 0.8927603561749903,
      "grad_norm": 18.424612045288086,
      "learning_rate": 8.927603561749903e-06,
      "loss": 1.6353,
      "step": 2306
    },
    {
      "epoch": 0.8931475029036005,
      "grad_norm": 8.803183555603027,
      "learning_rate": 8.931475029036006e-06,
      "loss": 1.4344,
      "step": 2307
    },
    {
      "epoch": 0.8935346496322106,
      "grad_norm": 10.109408378601074,
      "learning_rate": 8.935346496322106e-06,
      "loss": 1.4473,
      "step": 2308
    },
    {
      "epoch": 0.8939217963608207,
      "grad_norm": 10.411210060119629,
      "learning_rate": 8.939217963608207e-06,
      "loss": 1.6122,
      "step": 2309
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 12.85700798034668,
      "learning_rate": 8.94308943089431e-06,
      "loss": 1.2788,
      "step": 2310
    },
    {
      "epoch": 0.894696089818041,
      "grad_norm": 23.017993927001953,
      "learning_rate": 8.946960898180412e-06,
      "loss": 1.8537,
      "step": 2311
    },
    {
      "epoch": 0.8950832365466512,
      "grad_norm": 23.13499641418457,
      "learning_rate": 8.950832365466512e-06,
      "loss": 3.1293,
      "step": 2312
    },
    {
      "epoch": 0.8954703832752613,
      "grad_norm": 18.346111297607422,
      "learning_rate": 8.954703832752613e-06,
      "loss": 2.1261,
      "step": 2313
    },
    {
      "epoch": 0.8958575300038715,
      "grad_norm": 16.246475219726562,
      "learning_rate": 8.958575300038715e-06,
      "loss": 1.6187,
      "step": 2314
    },
    {
      "epoch": 0.8962446767324816,
      "grad_norm": 14.044034004211426,
      "learning_rate": 8.962446767324818e-06,
      "loss": 2.0409,
      "step": 2315
    },
    {
      "epoch": 0.8966318234610917,
      "grad_norm": 9.911795616149902,
      "learning_rate": 8.966318234610918e-06,
      "loss": 1.4129,
      "step": 2316
    },
    {
      "epoch": 0.8970189701897019,
      "grad_norm": 22.82571029663086,
      "learning_rate": 8.970189701897019e-06,
      "loss": 1.9633,
      "step": 2317
    },
    {
      "epoch": 0.8974061169183121,
      "grad_norm": 12.343034744262695,
      "learning_rate": 8.974061169183121e-06,
      "loss": 1.1637,
      "step": 2318
    },
    {
      "epoch": 0.8977932636469221,
      "grad_norm": 10.994402885437012,
      "learning_rate": 8.977932636469222e-06,
      "loss": 1.592,
      "step": 2319
    },
    {
      "epoch": 0.8981804103755323,
      "grad_norm": 23.1755313873291,
      "learning_rate": 8.981804103755324e-06,
      "loss": 1.6409,
      "step": 2320
    },
    {
      "epoch": 0.8985675571041425,
      "grad_norm": 11.940260887145996,
      "learning_rate": 8.985675571041425e-06,
      "loss": 1.6626,
      "step": 2321
    },
    {
      "epoch": 0.8989547038327527,
      "grad_norm": 19.94672203063965,
      "learning_rate": 8.989547038327527e-06,
      "loss": 1.5697,
      "step": 2322
    },
    {
      "epoch": 0.8993418505613627,
      "grad_norm": 17.600189208984375,
      "learning_rate": 8.993418505613628e-06,
      "loss": 1.1746,
      "step": 2323
    },
    {
      "epoch": 0.8997289972899729,
      "grad_norm": 12.956334114074707,
      "learning_rate": 8.99728997289973e-06,
      "loss": 1.6042,
      "step": 2324
    },
    {
      "epoch": 0.9001161440185831,
      "grad_norm": 23.419052124023438,
      "learning_rate": 9.00116144018583e-06,
      "loss": 1.4831,
      "step": 2325
    },
    {
      "epoch": 0.9005032907471932,
      "grad_norm": 16.746448516845703,
      "learning_rate": 9.005032907471933e-06,
      "loss": 1.9065,
      "step": 2326
    },
    {
      "epoch": 0.9008904374758033,
      "grad_norm": 13.007083892822266,
      "learning_rate": 9.008904374758034e-06,
      "loss": 0.6093,
      "step": 2327
    },
    {
      "epoch": 0.9012775842044135,
      "grad_norm": 12.367864608764648,
      "learning_rate": 9.012775842044136e-06,
      "loss": 1.4398,
      "step": 2328
    },
    {
      "epoch": 0.9016647309330236,
      "grad_norm": 12.033641815185547,
      "learning_rate": 9.016647309330237e-06,
      "loss": 1.1713,
      "step": 2329
    },
    {
      "epoch": 0.9020518776616337,
      "grad_norm": 14.2890043258667,
      "learning_rate": 9.020518776616339e-06,
      "loss": 1.5398,
      "step": 2330
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 13.913548469543457,
      "learning_rate": 9.02439024390244e-06,
      "loss": 1.4976,
      "step": 2331
    },
    {
      "epoch": 0.902826171118854,
      "grad_norm": 15.085039138793945,
      "learning_rate": 9.02826171118854e-06,
      "loss": 1.6067,
      "step": 2332
    },
    {
      "epoch": 0.9032133178474642,
      "grad_norm": 17.061906814575195,
      "learning_rate": 9.032133178474643e-06,
      "loss": 1.9206,
      "step": 2333
    },
    {
      "epoch": 0.9036004645760743,
      "grad_norm": 23.209362030029297,
      "learning_rate": 9.036004645760745e-06,
      "loss": 1.588,
      "step": 2334
    },
    {
      "epoch": 0.9039876113046845,
      "grad_norm": 26.369630813598633,
      "learning_rate": 9.039876113046846e-06,
      "loss": 2.1319,
      "step": 2335
    },
    {
      "epoch": 0.9043747580332946,
      "grad_norm": 13.020764350891113,
      "learning_rate": 9.043747580332946e-06,
      "loss": 1.4821,
      "step": 2336
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 15.711220741271973,
      "learning_rate": 9.047619047619049e-06,
      "loss": 1.5899,
      "step": 2337
    },
    {
      "epoch": 0.9051490514905149,
      "grad_norm": 18.32108497619629,
      "learning_rate": 9.051490514905151e-06,
      "loss": 1.6117,
      "step": 2338
    },
    {
      "epoch": 0.905536198219125,
      "grad_norm": 13.442981719970703,
      "learning_rate": 9.055361982191252e-06,
      "loss": 1.5049,
      "step": 2339
    },
    {
      "epoch": 0.9059233449477352,
      "grad_norm": 12.958024024963379,
      "learning_rate": 9.059233449477352e-06,
      "loss": 1.5989,
      "step": 2340
    },
    {
      "epoch": 0.9063104916763454,
      "grad_norm": 23.134050369262695,
      "learning_rate": 9.063104916763455e-06,
      "loss": 1.7602,
      "step": 2341
    },
    {
      "epoch": 0.9066976384049554,
      "grad_norm": 34.762451171875,
      "learning_rate": 9.066976384049555e-06,
      "loss": 1.477,
      "step": 2342
    },
    {
      "epoch": 0.9070847851335656,
      "grad_norm": 14.378808975219727,
      "learning_rate": 9.070847851335658e-06,
      "loss": 1.0421,
      "step": 2343
    },
    {
      "epoch": 0.9074719318621758,
      "grad_norm": 20.486675262451172,
      "learning_rate": 9.074719318621758e-06,
      "loss": 1.9366,
      "step": 2344
    },
    {
      "epoch": 0.907859078590786,
      "grad_norm": 13.179032325744629,
      "learning_rate": 9.07859078590786e-06,
      "loss": 1.5512,
      "step": 2345
    },
    {
      "epoch": 0.908246225319396,
      "grad_norm": 31.507125854492188,
      "learning_rate": 9.082462253193961e-06,
      "loss": 1.4161,
      "step": 2346
    },
    {
      "epoch": 0.9086333720480062,
      "grad_norm": 20.894813537597656,
      "learning_rate": 9.086333720480064e-06,
      "loss": 1.5576,
      "step": 2347
    },
    {
      "epoch": 0.9090205187766164,
      "grad_norm": 11.242952346801758,
      "learning_rate": 9.090205187766164e-06,
      "loss": 1.4449,
      "step": 2348
    },
    {
      "epoch": 0.9094076655052264,
      "grad_norm": 15.805705070495605,
      "learning_rate": 9.094076655052265e-06,
      "loss": 1.2177,
      "step": 2349
    },
    {
      "epoch": 0.9097948122338366,
      "grad_norm": 23.7022705078125,
      "learning_rate": 9.097948122338367e-06,
      "loss": 1.3178,
      "step": 2350
    },
    {
      "epoch": 0.9101819589624468,
      "grad_norm": 11.574383735656738,
      "learning_rate": 9.10181958962447e-06,
      "loss": 1.6262,
      "step": 2351
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 16.93019676208496,
      "learning_rate": 9.10569105691057e-06,
      "loss": 1.8163,
      "step": 2352
    },
    {
      "epoch": 0.910956252419667,
      "grad_norm": 17.140581130981445,
      "learning_rate": 9.109562524196671e-06,
      "loss": 2.0023,
      "step": 2353
    },
    {
      "epoch": 0.9113433991482772,
      "grad_norm": 19.254098892211914,
      "learning_rate": 9.113433991482773e-06,
      "loss": 1.5814,
      "step": 2354
    },
    {
      "epoch": 0.9117305458768873,
      "grad_norm": 13.242351531982422,
      "learning_rate": 9.117305458768874e-06,
      "loss": 1.7686,
      "step": 2355
    },
    {
      "epoch": 0.9121176926054975,
      "grad_norm": 18.668685913085938,
      "learning_rate": 9.121176926054976e-06,
      "loss": 1.6695,
      "step": 2356
    },
    {
      "epoch": 0.9125048393341076,
      "grad_norm": 13.012821197509766,
      "learning_rate": 9.125048393341077e-06,
      "loss": 0.6488,
      "step": 2357
    },
    {
      "epoch": 0.9128919860627178,
      "grad_norm": 10.107338905334473,
      "learning_rate": 9.12891986062718e-06,
      "loss": 1.4543,
      "step": 2358
    },
    {
      "epoch": 0.9132791327913279,
      "grad_norm": 19.161630630493164,
      "learning_rate": 9.13279132791328e-06,
      "loss": 1.5762,
      "step": 2359
    },
    {
      "epoch": 0.9136662795199381,
      "grad_norm": 22.690185546875,
      "learning_rate": 9.136662795199382e-06,
      "loss": 2.2651,
      "step": 2360
    },
    {
      "epoch": 0.9140534262485482,
      "grad_norm": 11.013131141662598,
      "learning_rate": 9.140534262485483e-06,
      "loss": 1.7743,
      "step": 2361
    },
    {
      "epoch": 0.9144405729771583,
      "grad_norm": 30.30308723449707,
      "learning_rate": 9.144405729771583e-06,
      "loss": 1.5577,
      "step": 2362
    },
    {
      "epoch": 0.9148277197057685,
      "grad_norm": 16.0367374420166,
      "learning_rate": 9.148277197057686e-06,
      "loss": 1.589,
      "step": 2363
    },
    {
      "epoch": 0.9152148664343787,
      "grad_norm": 18.354352951049805,
      "learning_rate": 9.152148664343788e-06,
      "loss": 1.9855,
      "step": 2364
    },
    {
      "epoch": 0.9156020131629887,
      "grad_norm": 12.190423965454102,
      "learning_rate": 9.156020131629889e-06,
      "loss": 1.6008,
      "step": 2365
    },
    {
      "epoch": 0.9159891598915989,
      "grad_norm": 20.238805770874023,
      "learning_rate": 9.15989159891599e-06,
      "loss": 1.4064,
      "step": 2366
    },
    {
      "epoch": 0.9163763066202091,
      "grad_norm": 20.29241943359375,
      "learning_rate": 9.163763066202092e-06,
      "loss": 1.4082,
      "step": 2367
    },
    {
      "epoch": 0.9167634533488193,
      "grad_norm": 13.115478515625,
      "learning_rate": 9.167634533488194e-06,
      "loss": 1.7288,
      "step": 2368
    },
    {
      "epoch": 0.9171506000774293,
      "grad_norm": 13.040194511413574,
      "learning_rate": 9.171506000774295e-06,
      "loss": 1.1587,
      "step": 2369
    },
    {
      "epoch": 0.9175377468060395,
      "grad_norm": 23.43735694885254,
      "learning_rate": 9.175377468060395e-06,
      "loss": 1.3228,
      "step": 2370
    },
    {
      "epoch": 0.9179248935346497,
      "grad_norm": 16.442445755004883,
      "learning_rate": 9.179248935346498e-06,
      "loss": 1.5597,
      "step": 2371
    },
    {
      "epoch": 0.9183120402632597,
      "grad_norm": 13.542684555053711,
      "learning_rate": 9.183120402632598e-06,
      "loss": 0.6425,
      "step": 2372
    },
    {
      "epoch": 0.9186991869918699,
      "grad_norm": 22.42603302001953,
      "learning_rate": 9.1869918699187e-06,
      "loss": 1.9084,
      "step": 2373
    },
    {
      "epoch": 0.9190863337204801,
      "grad_norm": 12.764554023742676,
      "learning_rate": 9.190863337204801e-06,
      "loss": 1.6014,
      "step": 2374
    },
    {
      "epoch": 0.9194734804490902,
      "grad_norm": 15.377429008483887,
      "learning_rate": 9.194734804490902e-06,
      "loss": 1.5523,
      "step": 2375
    },
    {
      "epoch": 0.9198606271777003,
      "grad_norm": 16.361726760864258,
      "learning_rate": 9.198606271777004e-06,
      "loss": 1.6989,
      "step": 2376
    },
    {
      "epoch": 0.9202477739063105,
      "grad_norm": 17.958755493164062,
      "learning_rate": 9.202477739063107e-06,
      "loss": 2.0709,
      "step": 2377
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 13.025574684143066,
      "learning_rate": 9.206349206349207e-06,
      "loss": 1.5105,
      "step": 2378
    },
    {
      "epoch": 0.9210220673635308,
      "grad_norm": 11.80859661102295,
      "learning_rate": 9.210220673635308e-06,
      "loss": 1.0666,
      "step": 2379
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 13.937124252319336,
      "learning_rate": 9.21409214092141e-06,
      "loss": 1.5833,
      "step": 2380
    },
    {
      "epoch": 0.9217963608207511,
      "grad_norm": 16.18181610107422,
      "learning_rate": 9.217963608207513e-06,
      "loss": 1.9268,
      "step": 2381
    },
    {
      "epoch": 0.9221835075493612,
      "grad_norm": 18.005353927612305,
      "learning_rate": 9.221835075493613e-06,
      "loss": 1.9648,
      "step": 2382
    },
    {
      "epoch": 0.9225706542779714,
      "grad_norm": 16.683561325073242,
      "learning_rate": 9.225706542779714e-06,
      "loss": 1.5371,
      "step": 2383
    },
    {
      "epoch": 0.9229578010065815,
      "grad_norm": 19.468503952026367,
      "learning_rate": 9.229578010065816e-06,
      "loss": 2.0823,
      "step": 2384
    },
    {
      "epoch": 0.9233449477351916,
      "grad_norm": 40.522422790527344,
      "learning_rate": 9.233449477351917e-06,
      "loss": 1.791,
      "step": 2385
    },
    {
      "epoch": 0.9237320944638018,
      "grad_norm": 17.683931350708008,
      "learning_rate": 9.23732094463802e-06,
      "loss": 1.5831,
      "step": 2386
    },
    {
      "epoch": 0.924119241192412,
      "grad_norm": 21.762582778930664,
      "learning_rate": 9.24119241192412e-06,
      "loss": 1.8657,
      "step": 2387
    },
    {
      "epoch": 0.924506387921022,
      "grad_norm": 16.8353271484375,
      "learning_rate": 9.24506387921022e-06,
      "loss": 2.2314,
      "step": 2388
    },
    {
      "epoch": 0.9248935346496322,
      "grad_norm": 13.438691139221191,
      "learning_rate": 9.248935346496323e-06,
      "loss": 2.0699,
      "step": 2389
    },
    {
      "epoch": 0.9252806813782424,
      "grad_norm": 9.743907928466797,
      "learning_rate": 9.252806813782425e-06,
      "loss": 1.3659,
      "step": 2390
    },
    {
      "epoch": 0.9256678281068524,
      "grad_norm": 16.128942489624023,
      "learning_rate": 9.256678281068526e-06,
      "loss": 1.666,
      "step": 2391
    },
    {
      "epoch": 0.9260549748354626,
      "grad_norm": 21.764951705932617,
      "learning_rate": 9.260549748354627e-06,
      "loss": 1.3506,
      "step": 2392
    },
    {
      "epoch": 0.9264421215640728,
      "grad_norm": 14.598368644714355,
      "learning_rate": 9.264421215640729e-06,
      "loss": 1.2683,
      "step": 2393
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 19.210384368896484,
      "learning_rate": 9.268292682926831e-06,
      "loss": 1.8057,
      "step": 2394
    },
    {
      "epoch": 0.927216415021293,
      "grad_norm": 16.101091384887695,
      "learning_rate": 9.272164150212932e-06,
      "loss": 1.1264,
      "step": 2395
    },
    {
      "epoch": 0.9276035617499032,
      "grad_norm": 12.29494857788086,
      "learning_rate": 9.276035617499033e-06,
      "loss": 1.5805,
      "step": 2396
    },
    {
      "epoch": 0.9279907084785134,
      "grad_norm": 9.9263334274292,
      "learning_rate": 9.279907084785135e-06,
      "loss": 1.3774,
      "step": 2397
    },
    {
      "epoch": 0.9283778552071235,
      "grad_norm": 13.792848587036133,
      "learning_rate": 9.283778552071236e-06,
      "loss": 1.2696,
      "step": 2398
    },
    {
      "epoch": 0.9287650019357336,
      "grad_norm": 20.09490203857422,
      "learning_rate": 9.287650019357338e-06,
      "loss": 1.885,
      "step": 2399
    },
    {
      "epoch": 0.9291521486643438,
      "grad_norm": 11.56367015838623,
      "learning_rate": 9.291521486643439e-06,
      "loss": 1.0915,
      "step": 2400
    },
    {
      "epoch": 0.9295392953929539,
      "grad_norm": 13.34237289428711,
      "learning_rate": 9.29539295392954e-06,
      "loss": 1.1436,
      "step": 2401
    },
    {
      "epoch": 0.9299264421215641,
      "grad_norm": 11.43200397491455,
      "learning_rate": 9.299264421215642e-06,
      "loss": 1.4022,
      "step": 2402
    },
    {
      "epoch": 0.9303135888501742,
      "grad_norm": 20.76986312866211,
      "learning_rate": 9.303135888501744e-06,
      "loss": 1.4049,
      "step": 2403
    },
    {
      "epoch": 0.9307007355787844,
      "grad_norm": 14.892484664916992,
      "learning_rate": 9.307007355787845e-06,
      "loss": 1.477,
      "step": 2404
    },
    {
      "epoch": 0.9310878823073945,
      "grad_norm": 25.995710372924805,
      "learning_rate": 9.310878823073945e-06,
      "loss": 1.9349,
      "step": 2405
    },
    {
      "epoch": 0.9314750290360047,
      "grad_norm": 27.663976669311523,
      "learning_rate": 9.314750290360047e-06,
      "loss": 1.6966,
      "step": 2406
    },
    {
      "epoch": 0.9318621757646148,
      "grad_norm": 16.943809509277344,
      "learning_rate": 9.31862175764615e-06,
      "loss": 1.4204,
      "step": 2407
    },
    {
      "epoch": 0.9322493224932249,
      "grad_norm": 14.838353157043457,
      "learning_rate": 9.32249322493225e-06,
      "loss": 1.4813,
      "step": 2408
    },
    {
      "epoch": 0.9326364692218351,
      "grad_norm": 10.75448989868164,
      "learning_rate": 9.326364692218351e-06,
      "loss": 1.4801,
      "step": 2409
    },
    {
      "epoch": 0.9330236159504453,
      "grad_norm": 16.601646423339844,
      "learning_rate": 9.330236159504453e-06,
      "loss": 0.7622,
      "step": 2410
    },
    {
      "epoch": 0.9334107626790553,
      "grad_norm": 9.349617958068848,
      "learning_rate": 9.334107626790554e-06,
      "loss": 1.4584,
      "step": 2411
    },
    {
      "epoch": 0.9337979094076655,
      "grad_norm": 31.27602195739746,
      "learning_rate": 9.337979094076656e-06,
      "loss": 1.4303,
      "step": 2412
    },
    {
      "epoch": 0.9341850561362757,
      "grad_norm": 17.315326690673828,
      "learning_rate": 9.341850561362757e-06,
      "loss": 1.7213,
      "step": 2413
    },
    {
      "epoch": 0.9345722028648857,
      "grad_norm": 20.14692497253418,
      "learning_rate": 9.345722028648858e-06,
      "loss": 1.5622,
      "step": 2414
    },
    {
      "epoch": 0.9349593495934959,
      "grad_norm": 12.344671249389648,
      "learning_rate": 9.34959349593496e-06,
      "loss": 1.2256,
      "step": 2415
    },
    {
      "epoch": 0.9353464963221061,
      "grad_norm": 18.859813690185547,
      "learning_rate": 9.353464963221062e-06,
      "loss": 1.0116,
      "step": 2416
    },
    {
      "epoch": 0.9357336430507163,
      "grad_norm": 25.69373893737793,
      "learning_rate": 9.357336430507163e-06,
      "loss": 2.3313,
      "step": 2417
    },
    {
      "epoch": 0.9361207897793263,
      "grad_norm": 10.889843940734863,
      "learning_rate": 9.361207897793264e-06,
      "loss": 1.3481,
      "step": 2418
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 16.764297485351562,
      "learning_rate": 9.365079365079366e-06,
      "loss": 1.8266,
      "step": 2419
    },
    {
      "epoch": 0.9368950832365467,
      "grad_norm": 17.193193435668945,
      "learning_rate": 9.368950832365468e-06,
      "loss": 2.1259,
      "step": 2420
    },
    {
      "epoch": 0.9372822299651568,
      "grad_norm": 14.0260009765625,
      "learning_rate": 9.372822299651569e-06,
      "loss": 1.6465,
      "step": 2421
    },
    {
      "epoch": 0.9376693766937669,
      "grad_norm": 15.246230125427246,
      "learning_rate": 9.37669376693767e-06,
      "loss": 1.2291,
      "step": 2422
    },
    {
      "epoch": 0.9380565234223771,
      "grad_norm": 13.033675193786621,
      "learning_rate": 9.380565234223772e-06,
      "loss": 1.3934,
      "step": 2423
    },
    {
      "epoch": 0.9384436701509872,
      "grad_norm": 21.061824798583984,
      "learning_rate": 9.384436701509873e-06,
      "loss": 1.4074,
      "step": 2424
    },
    {
      "epoch": 0.9388308168795974,
      "grad_norm": 14.433723449707031,
      "learning_rate": 9.388308168795975e-06,
      "loss": 1.3989,
      "step": 2425
    },
    {
      "epoch": 0.9392179636082075,
      "grad_norm": 14.267729759216309,
      "learning_rate": 9.392179636082076e-06,
      "loss": 1.0865,
      "step": 2426
    },
    {
      "epoch": 0.9396051103368177,
      "grad_norm": 24.80900764465332,
      "learning_rate": 9.396051103368178e-06,
      "loss": 2.0322,
      "step": 2427
    },
    {
      "epoch": 0.9399922570654278,
      "grad_norm": 9.134562492370605,
      "learning_rate": 9.399922570654279e-06,
      "loss": 0.9409,
      "step": 2428
    },
    {
      "epoch": 0.940379403794038,
      "grad_norm": 37.82712936401367,
      "learning_rate": 9.403794037940381e-06,
      "loss": 1.7059,
      "step": 2429
    },
    {
      "epoch": 0.9407665505226481,
      "grad_norm": 22.591991424560547,
      "learning_rate": 9.407665505226482e-06,
      "loss": 1.5855,
      "step": 2430
    },
    {
      "epoch": 0.9411536972512582,
      "grad_norm": 13.572936058044434,
      "learning_rate": 9.411536972512582e-06,
      "loss": 1.4127,
      "step": 2431
    },
    {
      "epoch": 0.9415408439798684,
      "grad_norm": 18.473112106323242,
      "learning_rate": 9.415408439798685e-06,
      "loss": 2.1885,
      "step": 2432
    },
    {
      "epoch": 0.9419279907084785,
      "grad_norm": 14.272032737731934,
      "learning_rate": 9.419279907084787e-06,
      "loss": 1.419,
      "step": 2433
    },
    {
      "epoch": 0.9423151374370886,
      "grad_norm": 24.690614700317383,
      "learning_rate": 9.423151374370888e-06,
      "loss": 1.3399,
      "step": 2434
    },
    {
      "epoch": 0.9427022841656988,
      "grad_norm": 7.785721302032471,
      "learning_rate": 9.427022841656988e-06,
      "loss": 1.7068,
      "step": 2435
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 17.01764488220215,
      "learning_rate": 9.43089430894309e-06,
      "loss": 1.4329,
      "step": 2436
    },
    {
      "epoch": 0.943476577622919,
      "grad_norm": 25.782939910888672,
      "learning_rate": 9.434765776229191e-06,
      "loss": 1.5691,
      "step": 2437
    },
    {
      "epoch": 0.9438637243515292,
      "grad_norm": 15.145503044128418,
      "learning_rate": 9.438637243515294e-06,
      "loss": 1.7079,
      "step": 2438
    },
    {
      "epoch": 0.9442508710801394,
      "grad_norm": 15.970056533813477,
      "learning_rate": 9.442508710801394e-06,
      "loss": 1.0651,
      "step": 2439
    },
    {
      "epoch": 0.9446380178087496,
      "grad_norm": 9.232255935668945,
      "learning_rate": 9.446380178087497e-06,
      "loss": 1.3785,
      "step": 2440
    },
    {
      "epoch": 0.9450251645373596,
      "grad_norm": 15.347969055175781,
      "learning_rate": 9.450251645373597e-06,
      "loss": 1.9582,
      "step": 2441
    },
    {
      "epoch": 0.9454123112659698,
      "grad_norm": 13.718271255493164,
      "learning_rate": 9.4541231126597e-06,
      "loss": 1.6878,
      "step": 2442
    },
    {
      "epoch": 0.94579945799458,
      "grad_norm": 12.794452667236328,
      "learning_rate": 9.4579945799458e-06,
      "loss": 1.6623,
      "step": 2443
    },
    {
      "epoch": 0.94618660472319,
      "grad_norm": 12.400466918945312,
      "learning_rate": 9.461866047231901e-06,
      "loss": 1.0793,
      "step": 2444
    },
    {
      "epoch": 0.9465737514518002,
      "grad_norm": 18.931352615356445,
      "learning_rate": 9.465737514518003e-06,
      "loss": 1.6084,
      "step": 2445
    },
    {
      "epoch": 0.9469608981804104,
      "grad_norm": 38.066829681396484,
      "learning_rate": 9.469608981804106e-06,
      "loss": 2.7413,
      "step": 2446
    },
    {
      "epoch": 0.9473480449090205,
      "grad_norm": 25.894744873046875,
      "learning_rate": 9.473480449090206e-06,
      "loss": 1.7575,
      "step": 2447
    },
    {
      "epoch": 0.9477351916376306,
      "grad_norm": 42.95613098144531,
      "learning_rate": 9.477351916376307e-06,
      "loss": 1.134,
      "step": 2448
    },
    {
      "epoch": 0.9481223383662408,
      "grad_norm": 13.600801467895508,
      "learning_rate": 9.48122338366241e-06,
      "loss": 1.3171,
      "step": 2449
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 11.727331161499023,
      "learning_rate": 9.485094850948512e-06,
      "loss": 1.7117,
      "step": 2450
    },
    {
      "epoch": 0.9488966318234611,
      "grad_norm": 15.058943748474121,
      "learning_rate": 9.488966318234612e-06,
      "loss": 1.73,
      "step": 2451
    },
    {
      "epoch": 0.9492837785520712,
      "grad_norm": 19.389455795288086,
      "learning_rate": 9.492837785520713e-06,
      "loss": 1.7607,
      "step": 2452
    },
    {
      "epoch": 0.9496709252806814,
      "grad_norm": 12.034249305725098,
      "learning_rate": 9.496709252806815e-06,
      "loss": 1.7567,
      "step": 2453
    },
    {
      "epoch": 0.9500580720092915,
      "grad_norm": 30.04859161376953,
      "learning_rate": 9.500580720092916e-06,
      "loss": 1.8477,
      "step": 2454
    },
    {
      "epoch": 0.9504452187379017,
      "grad_norm": 17.20099449157715,
      "learning_rate": 9.504452187379018e-06,
      "loss": 1.4302,
      "step": 2455
    },
    {
      "epoch": 0.9508323654665118,
      "grad_norm": 8.845955848693848,
      "learning_rate": 9.508323654665119e-06,
      "loss": 1.3515,
      "step": 2456
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 21.175443649291992,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.3648,
      "step": 2457
    },
    {
      "epoch": 0.9516066589237321,
      "grad_norm": 22.235788345336914,
      "learning_rate": 9.516066589237322e-06,
      "loss": 1.6389,
      "step": 2458
    },
    {
      "epoch": 0.9519938056523423,
      "grad_norm": 16.619800567626953,
      "learning_rate": 9.519938056523424e-06,
      "loss": 2.0895,
      "step": 2459
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 13.149585723876953,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.1648,
      "step": 2460
    },
    {
      "epoch": 0.9527680991095625,
      "grad_norm": 23.319137573242188,
      "learning_rate": 9.527680991095625e-06,
      "loss": 2.8377,
      "step": 2461
    },
    {
      "epoch": 0.9531552458381727,
      "grad_norm": 14.723240852355957,
      "learning_rate": 9.531552458381728e-06,
      "loss": 1.5215,
      "step": 2462
    },
    {
      "epoch": 0.9535423925667829,
      "grad_norm": 19.70477867126465,
      "learning_rate": 9.53542392566783e-06,
      "loss": 1.5722,
      "step": 2463
    },
    {
      "epoch": 0.9539295392953929,
      "grad_norm": 27.534887313842773,
      "learning_rate": 9.53929539295393e-06,
      "loss": 1.6698,
      "step": 2464
    },
    {
      "epoch": 0.9543166860240031,
      "grad_norm": 20.85808753967285,
      "learning_rate": 9.543166860240031e-06,
      "loss": 1.5283,
      "step": 2465
    },
    {
      "epoch": 0.9547038327526133,
      "grad_norm": 19.407377243041992,
      "learning_rate": 9.547038327526134e-06,
      "loss": 1.8617,
      "step": 2466
    },
    {
      "epoch": 0.9550909794812233,
      "grad_norm": 47.65606689453125,
      "learning_rate": 9.550909794812234e-06,
      "loss": 2.9037,
      "step": 2467
    },
    {
      "epoch": 0.9554781262098335,
      "grad_norm": 16.20706558227539,
      "learning_rate": 9.554781262098337e-06,
      "loss": 1.7534,
      "step": 2468
    },
    {
      "epoch": 0.9558652729384437,
      "grad_norm": 12.767754554748535,
      "learning_rate": 9.558652729384437e-06,
      "loss": 1.4429,
      "step": 2469
    },
    {
      "epoch": 0.9562524196670538,
      "grad_norm": 20.498306274414062,
      "learning_rate": 9.562524196670538e-06,
      "loss": 1.3395,
      "step": 2470
    },
    {
      "epoch": 0.9566395663956639,
      "grad_norm": 15.765213012695312,
      "learning_rate": 9.56639566395664e-06,
      "loss": 1.4481,
      "step": 2471
    },
    {
      "epoch": 0.9570267131242741,
      "grad_norm": 15.332927703857422,
      "learning_rate": 9.570267131242743e-06,
      "loss": 1.6043,
      "step": 2472
    },
    {
      "epoch": 0.9574138598528843,
      "grad_norm": 29.2025203704834,
      "learning_rate": 9.574138598528843e-06,
      "loss": 1.1671,
      "step": 2473
    },
    {
      "epoch": 0.9578010065814944,
      "grad_norm": 26.906808853149414,
      "learning_rate": 9.578010065814944e-06,
      "loss": 1.8244,
      "step": 2474
    },
    {
      "epoch": 0.9581881533101045,
      "grad_norm": 22.60091781616211,
      "learning_rate": 9.581881533101046e-06,
      "loss": 1.788,
      "step": 2475
    },
    {
      "epoch": 0.9585753000387147,
      "grad_norm": 15.026811599731445,
      "learning_rate": 9.585753000387149e-06,
      "loss": 1.4295,
      "step": 2476
    },
    {
      "epoch": 0.9589624467673248,
      "grad_norm": 16.645891189575195,
      "learning_rate": 9.58962446767325e-06,
      "loss": 1.5895,
      "step": 2477
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 9.505697250366211,
      "learning_rate": 9.59349593495935e-06,
      "loss": 1.5302,
      "step": 2478
    },
    {
      "epoch": 0.9597367402245451,
      "grad_norm": 14.520769119262695,
      "learning_rate": 9.597367402245452e-06,
      "loss": 1.7381,
      "step": 2479
    },
    {
      "epoch": 0.9601238869531552,
      "grad_norm": 24.15897560119629,
      "learning_rate": 9.601238869531553e-06,
      "loss": 2.2318,
      "step": 2480
    },
    {
      "epoch": 0.9605110336817654,
      "grad_norm": 14.324383735656738,
      "learning_rate": 9.605110336817655e-06,
      "loss": 1.243,
      "step": 2481
    },
    {
      "epoch": 0.9608981804103756,
      "grad_norm": 17.041664123535156,
      "learning_rate": 9.608981804103756e-06,
      "loss": 1.8038,
      "step": 2482
    },
    {
      "epoch": 0.9612853271389856,
      "grad_norm": 13.885501861572266,
      "learning_rate": 9.612853271389857e-06,
      "loss": 1.3225,
      "step": 2483
    },
    {
      "epoch": 0.9616724738675958,
      "grad_norm": 29.35101318359375,
      "learning_rate": 9.616724738675959e-06,
      "loss": 1.497,
      "step": 2484
    },
    {
      "epoch": 0.962059620596206,
      "grad_norm": 10.921612739562988,
      "learning_rate": 9.620596205962061e-06,
      "loss": 1.6965,
      "step": 2485
    },
    {
      "epoch": 0.9624467673248162,
      "grad_norm": 16.019756317138672,
      "learning_rate": 9.624467673248162e-06,
      "loss": 1.3167,
      "step": 2486
    },
    {
      "epoch": 0.9628339140534262,
      "grad_norm": 12.54230785369873,
      "learning_rate": 9.628339140534263e-06,
      "loss": 1.1435,
      "step": 2487
    },
    {
      "epoch": 0.9632210607820364,
      "grad_norm": 9.355463027954102,
      "learning_rate": 9.632210607820365e-06,
      "loss": 1.4365,
      "step": 2488
    },
    {
      "epoch": 0.9636082075106466,
      "grad_norm": 10.55498218536377,
      "learning_rate": 9.636082075106467e-06,
      "loss": 1.6198,
      "step": 2489
    },
    {
      "epoch": 0.9639953542392566,
      "grad_norm": 13.701261520385742,
      "learning_rate": 9.639953542392568e-06,
      "loss": 2.2337,
      "step": 2490
    },
    {
      "epoch": 0.9643825009678668,
      "grad_norm": 40.77080535888672,
      "learning_rate": 9.643825009678669e-06,
      "loss": 1.3366,
      "step": 2491
    },
    {
      "epoch": 0.964769647696477,
      "grad_norm": 21.886022567749023,
      "learning_rate": 9.64769647696477e-06,
      "loss": 1.4343,
      "step": 2492
    },
    {
      "epoch": 0.9651567944250871,
      "grad_norm": 10.636088371276855,
      "learning_rate": 9.651567944250871e-06,
      "loss": 1.1051,
      "step": 2493
    },
    {
      "epoch": 0.9655439411536972,
      "grad_norm": 17.435609817504883,
      "learning_rate": 9.655439411536974e-06,
      "loss": 1.1707,
      "step": 2494
    },
    {
      "epoch": 0.9659310878823074,
      "grad_norm": 9.50867748260498,
      "learning_rate": 9.659310878823074e-06,
      "loss": 1.4348,
      "step": 2495
    },
    {
      "epoch": 0.9663182346109176,
      "grad_norm": 17.800506591796875,
      "learning_rate": 9.663182346109177e-06,
      "loss": 1.6305,
      "step": 2496
    },
    {
      "epoch": 0.9667053813395277,
      "grad_norm": 11.626612663269043,
      "learning_rate": 9.667053813395277e-06,
      "loss": 1.6394,
      "step": 2497
    },
    {
      "epoch": 0.9670925280681378,
      "grad_norm": 14.243045806884766,
      "learning_rate": 9.67092528068138e-06,
      "loss": 1.4224,
      "step": 2498
    },
    {
      "epoch": 0.967479674796748,
      "grad_norm": 13.251462936401367,
      "learning_rate": 9.67479674796748e-06,
      "loss": 1.3466,
      "step": 2499
    },
    {
      "epoch": 0.9678668215253581,
      "grad_norm": 8.359162330627441,
      "learning_rate": 9.678668215253581e-06,
      "loss": 1.0328,
      "step": 2500
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 17.624109268188477,
      "learning_rate": 9.682539682539683e-06,
      "loss": 1.6187,
      "step": 2501
    },
    {
      "epoch": 0.9686411149825784,
      "grad_norm": 15.328519821166992,
      "learning_rate": 9.686411149825786e-06,
      "loss": 1.8545,
      "step": 2502
    },
    {
      "epoch": 0.9690282617111885,
      "grad_norm": 13.258936882019043,
      "learning_rate": 9.690282617111886e-06,
      "loss": 2.1023,
      "step": 2503
    },
    {
      "epoch": 0.9694154084397987,
      "grad_norm": 13.75233268737793,
      "learning_rate": 9.694154084397987e-06,
      "loss": 1.3671,
      "step": 2504
    },
    {
      "epoch": 0.9698025551684089,
      "grad_norm": 9.982054710388184,
      "learning_rate": 9.69802555168409e-06,
      "loss": 0.9629,
      "step": 2505
    },
    {
      "epoch": 0.9701897018970189,
      "grad_norm": 11.263108253479004,
      "learning_rate": 9.70189701897019e-06,
      "loss": 1.6017,
      "step": 2506
    },
    {
      "epoch": 0.9705768486256291,
      "grad_norm": 16.134258270263672,
      "learning_rate": 9.705768486256292e-06,
      "loss": 1.5041,
      "step": 2507
    },
    {
      "epoch": 0.9709639953542393,
      "grad_norm": 14.935892105102539,
      "learning_rate": 9.709639953542393e-06,
      "loss": 1.4109,
      "step": 2508
    },
    {
      "epoch": 0.9713511420828495,
      "grad_norm": 9.89331340789795,
      "learning_rate": 9.713511420828495e-06,
      "loss": 1.601,
      "step": 2509
    },
    {
      "epoch": 0.9717382888114595,
      "grad_norm": 30.166852951049805,
      "learning_rate": 9.717382888114596e-06,
      "loss": 1.6253,
      "step": 2510
    },
    {
      "epoch": 0.9721254355400697,
      "grad_norm": 15.278709411621094,
      "learning_rate": 9.721254355400698e-06,
      "loss": 1.2424,
      "step": 2511
    },
    {
      "epoch": 0.9725125822686799,
      "grad_norm": 16.50385856628418,
      "learning_rate": 9.725125822686799e-06,
      "loss": 1.8546,
      "step": 2512
    },
    {
      "epoch": 0.9728997289972899,
      "grad_norm": 19.038211822509766,
      "learning_rate": 9.7289972899729e-06,
      "loss": 1.5566,
      "step": 2513
    },
    {
      "epoch": 0.9732868757259001,
      "grad_norm": 19.280607223510742,
      "learning_rate": 9.732868757259002e-06,
      "loss": 1.9039,
      "step": 2514
    },
    {
      "epoch": 0.9736740224545103,
      "grad_norm": 27.72847557067871,
      "learning_rate": 9.736740224545104e-06,
      "loss": 1.0504,
      "step": 2515
    },
    {
      "epoch": 0.9740611691831204,
      "grad_norm": 15.415870666503906,
      "learning_rate": 9.740611691831205e-06,
      "loss": 1.4687,
      "step": 2516
    },
    {
      "epoch": 0.9744483159117305,
      "grad_norm": 10.676066398620605,
      "learning_rate": 9.744483159117306e-06,
      "loss": 1.4078,
      "step": 2517
    },
    {
      "epoch": 0.9748354626403407,
      "grad_norm": 13.590201377868652,
      "learning_rate": 9.748354626403408e-06,
      "loss": 1.1165,
      "step": 2518
    },
    {
      "epoch": 0.9752226093689508,
      "grad_norm": 12.787718772888184,
      "learning_rate": 9.752226093689509e-06,
      "loss": 1.5834,
      "step": 2519
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 34.39543533325195,
      "learning_rate": 9.756097560975611e-06,
      "loss": 1.6226,
      "step": 2520
    },
    {
      "epoch": 0.9759969028261711,
      "grad_norm": 22.205814361572266,
      "learning_rate": 9.759969028261712e-06,
      "loss": 1.8868,
      "step": 2521
    },
    {
      "epoch": 0.9763840495547813,
      "grad_norm": 14.953932762145996,
      "learning_rate": 9.763840495547814e-06,
      "loss": 1.5732,
      "step": 2522
    },
    {
      "epoch": 0.9767711962833914,
      "grad_norm": 24.26693344116211,
      "learning_rate": 9.767711962833915e-06,
      "loss": 1.6035,
      "step": 2523
    },
    {
      "epoch": 0.9771583430120016,
      "grad_norm": 25.152528762817383,
      "learning_rate": 9.771583430120017e-06,
      "loss": 1.9118,
      "step": 2524
    },
    {
      "epoch": 0.9775454897406117,
      "grad_norm": 12.62132740020752,
      "learning_rate": 9.775454897406118e-06,
      "loss": 1.565,
      "step": 2525
    },
    {
      "epoch": 0.9779326364692218,
      "grad_norm": 14.87847900390625,
      "learning_rate": 9.779326364692218e-06,
      "loss": 1.5118,
      "step": 2526
    },
    {
      "epoch": 0.978319783197832,
      "grad_norm": 15.64487075805664,
      "learning_rate": 9.78319783197832e-06,
      "loss": 1.3423,
      "step": 2527
    },
    {
      "epoch": 0.9787069299264421,
      "grad_norm": 13.70876407623291,
      "learning_rate": 9.787069299264423e-06,
      "loss": 1.4081,
      "step": 2528
    },
    {
      "epoch": 0.9790940766550522,
      "grad_norm": 15.113752365112305,
      "learning_rate": 9.790940766550524e-06,
      "loss": 1.6401,
      "step": 2529
    },
    {
      "epoch": 0.9794812233836624,
      "grad_norm": 14.120649337768555,
      "learning_rate": 9.794812233836624e-06,
      "loss": 1.1461,
      "step": 2530
    },
    {
      "epoch": 0.9798683701122726,
      "grad_norm": 27.340917587280273,
      "learning_rate": 9.798683701122727e-06,
      "loss": 2.1994,
      "step": 2531
    },
    {
      "epoch": 0.9802555168408827,
      "grad_norm": 17.262170791625977,
      "learning_rate": 9.802555168408829e-06,
      "loss": 1.5305,
      "step": 2532
    },
    {
      "epoch": 0.9806426635694928,
      "grad_norm": 13.281227111816406,
      "learning_rate": 9.80642663569493e-06,
      "loss": 0.9674,
      "step": 2533
    },
    {
      "epoch": 0.981029810298103,
      "grad_norm": 20.432065963745117,
      "learning_rate": 9.81029810298103e-06,
      "loss": 1.5451,
      "step": 2534
    },
    {
      "epoch": 0.9814169570267132,
      "grad_norm": 14.50888729095459,
      "learning_rate": 9.814169570267133e-06,
      "loss": 2.1751,
      "step": 2535
    },
    {
      "epoch": 0.9818041037553232,
      "grad_norm": 7.63320779800415,
      "learning_rate": 9.818041037553233e-06,
      "loss": 0.9587,
      "step": 2536
    },
    {
      "epoch": 0.9821912504839334,
      "grad_norm": 13.159502983093262,
      "learning_rate": 9.821912504839336e-06,
      "loss": 1.6428,
      "step": 2537
    },
    {
      "epoch": 0.9825783972125436,
      "grad_norm": 20.119869232177734,
      "learning_rate": 9.825783972125436e-06,
      "loss": 1.5929,
      "step": 2538
    },
    {
      "epoch": 0.9829655439411537,
      "grad_norm": 8.515982627868652,
      "learning_rate": 9.829655439411537e-06,
      "loss": 0.9254,
      "step": 2539
    },
    {
      "epoch": 0.9833526906697638,
      "grad_norm": 12.554961204528809,
      "learning_rate": 9.833526906697639e-06,
      "loss": 1.1249,
      "step": 2540
    },
    {
      "epoch": 0.983739837398374,
      "grad_norm": 13.83731460571289,
      "learning_rate": 9.837398373983741e-06,
      "loss": 1.428,
      "step": 2541
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 15.132648468017578,
      "learning_rate": 9.841269841269842e-06,
      "loss": 2.0606,
      "step": 2542
    },
    {
      "epoch": 0.9845141308555942,
      "grad_norm": 13.795101165771484,
      "learning_rate": 9.845141308555943e-06,
      "loss": 1.1191,
      "step": 2543
    },
    {
      "epoch": 0.9849012775842044,
      "grad_norm": 32.88582992553711,
      "learning_rate": 9.849012775842045e-06,
      "loss": 2.1854,
      "step": 2544
    },
    {
      "epoch": 0.9852884243128146,
      "grad_norm": 17.08940887451172,
      "learning_rate": 9.852884243128147e-06,
      "loss": 1.5562,
      "step": 2545
    },
    {
      "epoch": 0.9856755710414247,
      "grad_norm": 12.026966094970703,
      "learning_rate": 9.856755710414248e-06,
      "loss": 1.5602,
      "step": 2546
    },
    {
      "epoch": 0.9860627177700348,
      "grad_norm": 19.02543067932129,
      "learning_rate": 9.860627177700349e-06,
      "loss": 2.2267,
      "step": 2547
    },
    {
      "epoch": 0.986449864498645,
      "grad_norm": 16.640535354614258,
      "learning_rate": 9.864498644986451e-06,
      "loss": 1.4911,
      "step": 2548
    },
    {
      "epoch": 0.9868370112272551,
      "grad_norm": 19.505102157592773,
      "learning_rate": 9.868370112272552e-06,
      "loss": 1.8137,
      "step": 2549
    },
    {
      "epoch": 0.9872241579558653,
      "grad_norm": 10.739411354064941,
      "learning_rate": 9.872241579558654e-06,
      "loss": 1.5136,
      "step": 2550
    },
    {
      "epoch": 0.9876113046844754,
      "grad_norm": 13.492147445678711,
      "learning_rate": 9.876113046844755e-06,
      "loss": 1.5452,
      "step": 2551
    },
    {
      "epoch": 0.9879984514130855,
      "grad_norm": 18.027143478393555,
      "learning_rate": 9.879984514130855e-06,
      "loss": 1.6421,
      "step": 2552
    },
    {
      "epoch": 0.9883855981416957,
      "grad_norm": 14.806660652160645,
      "learning_rate": 9.883855981416958e-06,
      "loss": 1.4613,
      "step": 2553
    },
    {
      "epoch": 0.9887727448703059,
      "grad_norm": 10.595080375671387,
      "learning_rate": 9.88772744870306e-06,
      "loss": 1.3687,
      "step": 2554
    },
    {
      "epoch": 0.989159891598916,
      "grad_norm": 16.189510345458984,
      "learning_rate": 9.89159891598916e-06,
      "loss": 1.6571,
      "step": 2555
    },
    {
      "epoch": 0.9895470383275261,
      "grad_norm": 21.16738510131836,
      "learning_rate": 9.895470383275261e-06,
      "loss": 2.5705,
      "step": 2556
    },
    {
      "epoch": 0.9899341850561363,
      "grad_norm": 18.801536560058594,
      "learning_rate": 9.899341850561364e-06,
      "loss": 1.7222,
      "step": 2557
    },
    {
      "epoch": 0.9903213317847465,
      "grad_norm": 14.925885200500488,
      "learning_rate": 9.903213317847466e-06,
      "loss": 1.5635,
      "step": 2558
    },
    {
      "epoch": 0.9907084785133565,
      "grad_norm": 19.014347076416016,
      "learning_rate": 9.907084785133567e-06,
      "loss": 1.6099,
      "step": 2559
    },
    {
      "epoch": 0.9910956252419667,
      "grad_norm": 23.29602813720703,
      "learning_rate": 9.910956252419667e-06,
      "loss": 1.86,
      "step": 2560
    },
    {
      "epoch": 0.9914827719705769,
      "grad_norm": 14.084688186645508,
      "learning_rate": 9.91482771970577e-06,
      "loss": 1.6384,
      "step": 2561
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 14.673308372497559,
      "learning_rate": 9.91869918699187e-06,
      "loss": 2.1152,
      "step": 2562
    },
    {
      "epoch": 0.9922570654277971,
      "grad_norm": 13.005427360534668,
      "learning_rate": 9.922570654277973e-06,
      "loss": 1.5282,
      "step": 2563
    },
    {
      "epoch": 0.9926442121564073,
      "grad_norm": 16.916603088378906,
      "learning_rate": 9.926442121564073e-06,
      "loss": 1.6518,
      "step": 2564
    },
    {
      "epoch": 0.9930313588850174,
      "grad_norm": 17.782567977905273,
      "learning_rate": 9.930313588850174e-06,
      "loss": 1.1994,
      "step": 2565
    },
    {
      "epoch": 0.9934185056136275,
      "grad_norm": 21.232084274291992,
      "learning_rate": 9.934185056136276e-06,
      "loss": 1.4839,
      "step": 2566
    },
    {
      "epoch": 0.9938056523422377,
      "grad_norm": 24.868227005004883,
      "learning_rate": 9.938056523422379e-06,
      "loss": 1.9744,
      "step": 2567
    },
    {
      "epoch": 0.9941927990708479,
      "grad_norm": 17.786909103393555,
      "learning_rate": 9.94192799070848e-06,
      "loss": 2.0374,
      "step": 2568
    },
    {
      "epoch": 0.994579945799458,
      "grad_norm": 15.702722549438477,
      "learning_rate": 9.94579945799458e-06,
      "loss": 1.3156,
      "step": 2569
    },
    {
      "epoch": 0.9949670925280681,
      "grad_norm": 17.65581703186035,
      "learning_rate": 9.949670925280682e-06,
      "loss": 1.6388,
      "step": 2570
    },
    {
      "epoch": 0.9953542392566783,
      "grad_norm": 21.25937843322754,
      "learning_rate": 9.953542392566785e-06,
      "loss": 1.8243,
      "step": 2571
    },
    {
      "epoch": 0.9957413859852884,
      "grad_norm": 16.396568298339844,
      "learning_rate": 9.957413859852885e-06,
      "loss": 1.3511,
      "step": 2572
    },
    {
      "epoch": 0.9961285327138986,
      "grad_norm": 22.33350372314453,
      "learning_rate": 9.961285327138986e-06,
      "loss": 1.5456,
      "step": 2573
    },
    {
      "epoch": 0.9965156794425087,
      "grad_norm": 23.696367263793945,
      "learning_rate": 9.965156794425088e-06,
      "loss": 2.4264,
      "step": 2574
    },
    {
      "epoch": 0.9969028261711188,
      "grad_norm": 19.24443244934082,
      "learning_rate": 9.969028261711189e-06,
      "loss": 1.7842,
      "step": 2575
    },
    {
      "epoch": 0.997289972899729,
      "grad_norm": 17.70773696899414,
      "learning_rate": 9.972899728997291e-06,
      "loss": 1.8099,
      "step": 2576
    },
    {
      "epoch": 0.9976771196283392,
      "grad_norm": 14.81180191040039,
      "learning_rate": 9.976771196283392e-06,
      "loss": 1.4985,
      "step": 2577
    },
    {
      "epoch": 0.9980642663569493,
      "grad_norm": 23.634035110473633,
      "learning_rate": 9.980642663569494e-06,
      "loss": 1.5511,
      "step": 2578
    },
    {
      "epoch": 0.9984514130855594,
      "grad_norm": 14.349963188171387,
      "learning_rate": 9.984514130855595e-06,
      "loss": 1.4931,
      "step": 2579
    },
    {
      "epoch": 0.9988385598141696,
      "grad_norm": 16.540782928466797,
      "learning_rate": 9.988385598141697e-06,
      "loss": 1.6154,
      "step": 2580
    },
    {
      "epoch": 0.9992257065427798,
      "grad_norm": 21.973140716552734,
      "learning_rate": 9.992257065427798e-06,
      "loss": 1.5554,
      "step": 2581
    },
    {
      "epoch": 0.9996128532713898,
      "grad_norm": 14.765721321105957,
      "learning_rate": 9.996128532713898e-06,
      "loss": 1.2196,
      "step": 2582
    },
    {
      "epoch": 1.0,
      "grad_norm": 23.19845962524414,
      "learning_rate": 1e-05,
      "loss": 1.4839,
      "step": 2583
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.30566037735849055,
      "eval_f1": 0.24708313558012496,
      "eval_loss": 1.6493834257125854,
      "eval_runtime": 421.8005,
      "eval_samples_per_second": 2.513,
      "eval_steps_per_second": 1.257,
      "step": 2583
    },
    {
      "epoch": 1.00038714672861,
      "grad_norm": 14.09315299987793,
      "learning_rate": 9.999569836968212e-06,
      "loss": 1.4162,
      "step": 2584
    },
    {
      "epoch": 1.0007742934572204,
      "grad_norm": 16.09914779663086,
      "learning_rate": 9.999139673936424e-06,
      "loss": 1.3967,
      "step": 2585
    },
    {
      "epoch": 1.0011614401858304,
      "grad_norm": 17.114521026611328,
      "learning_rate": 9.998709510904633e-06,
      "loss": 1.6082,
      "step": 2586
    },
    {
      "epoch": 1.0015485869144405,
      "grad_norm": 14.84126091003418,
      "learning_rate": 9.998279347872845e-06,
      "loss": 1.3602,
      "step": 2587
    },
    {
      "epoch": 1.0019357336430508,
      "grad_norm": 13.723974227905273,
      "learning_rate": 9.997849184841055e-06,
      "loss": 1.1899,
      "step": 2588
    },
    {
      "epoch": 1.0023228803716608,
      "grad_norm": 19.255876541137695,
      "learning_rate": 9.997419021809268e-06,
      "loss": 1.9304,
      "step": 2589
    },
    {
      "epoch": 1.002710027100271,
      "grad_norm": 14.713759422302246,
      "learning_rate": 9.996988858777477e-06,
      "loss": 0.9794,
      "step": 2590
    },
    {
      "epoch": 1.0030971738288812,
      "grad_norm": 9.271132469177246,
      "learning_rate": 9.996558695745689e-06,
      "loss": 1.5957,
      "step": 2591
    },
    {
      "epoch": 1.0034843205574913,
      "grad_norm": 12.925171852111816,
      "learning_rate": 9.996128532713898e-06,
      "loss": 1.5091,
      "step": 2592
    },
    {
      "epoch": 1.0038714672861013,
      "grad_norm": 21.85167694091797,
      "learning_rate": 9.995698369682112e-06,
      "loss": 1.46,
      "step": 2593
    },
    {
      "epoch": 1.0042586140147116,
      "grad_norm": 15.142290115356445,
      "learning_rate": 9.995268206650321e-06,
      "loss": 1.852,
      "step": 2594
    },
    {
      "epoch": 1.0046457607433217,
      "grad_norm": 12.539502143859863,
      "learning_rate": 9.994838043618533e-06,
      "loss": 1.468,
      "step": 2595
    },
    {
      "epoch": 1.005032907471932,
      "grad_norm": 17.726245880126953,
      "learning_rate": 9.994407880586742e-06,
      "loss": 1.3229,
      "step": 2596
    },
    {
      "epoch": 1.005420054200542,
      "grad_norm": 14.512673377990723,
      "learning_rate": 9.993977717554954e-06,
      "loss": 1.1577,
      "step": 2597
    },
    {
      "epoch": 1.005807200929152,
      "grad_norm": 12.546249389648438,
      "learning_rate": 9.993547554523165e-06,
      "loss": 1.4362,
      "step": 2598
    },
    {
      "epoch": 1.0061943476577624,
      "grad_norm": 11.196110725402832,
      "learning_rate": 9.993117391491377e-06,
      "loss": 1.5978,
      "step": 2599
    },
    {
      "epoch": 1.0065814943863725,
      "grad_norm": 14.639206886291504,
      "learning_rate": 9.992687228459586e-06,
      "loss": 1.0792,
      "step": 2600
    },
    {
      "epoch": 1.0069686411149825,
      "grad_norm": 14.33542251586914,
      "learning_rate": 9.992257065427798e-06,
      "loss": 1.054,
      "step": 2601
    },
    {
      "epoch": 1.0073557878435928,
      "grad_norm": 23.3632755279541,
      "learning_rate": 9.99182690239601e-06,
      "loss": 1.6689,
      "step": 2602
    },
    {
      "epoch": 1.0077429345722029,
      "grad_norm": 24.14690589904785,
      "learning_rate": 9.991396739364219e-06,
      "loss": 1.9858,
      "step": 2603
    },
    {
      "epoch": 1.008130081300813,
      "grad_norm": 18.774250030517578,
      "learning_rate": 9.99096657633243e-06,
      "loss": 1.6269,
      "step": 2604
    },
    {
      "epoch": 1.0085172280294232,
      "grad_norm": 11.012090682983398,
      "learning_rate": 9.990536413300642e-06,
      "loss": 0.8856,
      "step": 2605
    },
    {
      "epoch": 1.0089043747580333,
      "grad_norm": 39.710941314697266,
      "learning_rate": 9.990106250268853e-06,
      "loss": 1.1031,
      "step": 2606
    },
    {
      "epoch": 1.0092915214866434,
      "grad_norm": 9.749580383300781,
      "learning_rate": 9.989676087237063e-06,
      "loss": 1.3253,
      "step": 2607
    },
    {
      "epoch": 1.0096786682152536,
      "grad_norm": 23.097379684448242,
      "learning_rate": 9.989245924205274e-06,
      "loss": 1.763,
      "step": 2608
    },
    {
      "epoch": 1.0100658149438637,
      "grad_norm": 13.3756685256958,
      "learning_rate": 9.988815761173486e-06,
      "loss": 1.6811,
      "step": 2609
    },
    {
      "epoch": 1.0104529616724738,
      "grad_norm": 12.830037117004395,
      "learning_rate": 9.988385598141697e-06,
      "loss": 0.9569,
      "step": 2610
    },
    {
      "epoch": 1.010840108401084,
      "grad_norm": 18.245988845825195,
      "learning_rate": 9.987955435109907e-06,
      "loss": 1.2864,
      "step": 2611
    },
    {
      "epoch": 1.0112272551296941,
      "grad_norm": 56.722293853759766,
      "learning_rate": 9.987525272078118e-06,
      "loss": 2.1246,
      "step": 2612
    },
    {
      "epoch": 1.0116144018583042,
      "grad_norm": 23.643814086914062,
      "learning_rate": 9.98709510904633e-06,
      "loss": 2.1043,
      "step": 2613
    },
    {
      "epoch": 1.0120015485869145,
      "grad_norm": 13.166501998901367,
      "learning_rate": 9.986664946014541e-06,
      "loss": 1.647,
      "step": 2614
    },
    {
      "epoch": 1.0123886953155246,
      "grad_norm": 9.885017395019531,
      "learning_rate": 9.986234782982751e-06,
      "loss": 1.3324,
      "step": 2615
    },
    {
      "epoch": 1.0127758420441346,
      "grad_norm": 9.467904090881348,
      "learning_rate": 9.985804619950962e-06,
      "loss": 1.301,
      "step": 2616
    },
    {
      "epoch": 1.013162988772745,
      "grad_norm": 19.85218048095703,
      "learning_rate": 9.985374456919174e-06,
      "loss": 1.3001,
      "step": 2617
    },
    {
      "epoch": 1.013550135501355,
      "grad_norm": 7.8051934242248535,
      "learning_rate": 9.984944293887383e-06,
      "loss": 1.3422,
      "step": 2618
    },
    {
      "epoch": 1.0139372822299653,
      "grad_norm": 10.278396606445312,
      "learning_rate": 9.984514130855595e-06,
      "loss": 1.5395,
      "step": 2619
    },
    {
      "epoch": 1.0143244289585753,
      "grad_norm": 21.740522384643555,
      "learning_rate": 9.984083967823806e-06,
      "loss": 1.4721,
      "step": 2620
    },
    {
      "epoch": 1.0147115756871854,
      "grad_norm": 25.700231552124023,
      "learning_rate": 9.983653804792018e-06,
      "loss": 1.658,
      "step": 2621
    },
    {
      "epoch": 1.0150987224157957,
      "grad_norm": 13.382265090942383,
      "learning_rate": 9.983223641760227e-06,
      "loss": 2.305,
      "step": 2622
    },
    {
      "epoch": 1.0154858691444058,
      "grad_norm": 25.143522262573242,
      "learning_rate": 9.982793478728439e-06,
      "loss": 2.4553,
      "step": 2623
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 14.751433372497559,
      "learning_rate": 9.982363315696649e-06,
      "loss": 1.3549,
      "step": 2624
    },
    {
      "epoch": 1.016260162601626,
      "grad_norm": 16.997722625732422,
      "learning_rate": 9.981933152664862e-06,
      "loss": 1.7402,
      "step": 2625
    },
    {
      "epoch": 1.0166473093302362,
      "grad_norm": 12.751065254211426,
      "learning_rate": 9.981502989633071e-06,
      "loss": 1.1985,
      "step": 2626
    },
    {
      "epoch": 1.0170344560588462,
      "grad_norm": 19.965747833251953,
      "learning_rate": 9.981072826601283e-06,
      "loss": 1.1355,
      "step": 2627
    },
    {
      "epoch": 1.0174216027874565,
      "grad_norm": 9.752058029174805,
      "learning_rate": 9.980642663569494e-06,
      "loss": 1.371,
      "step": 2628
    },
    {
      "epoch": 1.0178087495160666,
      "grad_norm": 9.810931205749512,
      "learning_rate": 9.980212500537706e-06,
      "loss": 0.8008,
      "step": 2629
    },
    {
      "epoch": 1.0181958962446767,
      "grad_norm": 8.262858390808105,
      "learning_rate": 9.979782337505915e-06,
      "loss": 1.3404,
      "step": 2630
    },
    {
      "epoch": 1.018583042973287,
      "grad_norm": 12.261771202087402,
      "learning_rate": 9.979352174474127e-06,
      "loss": 0.9901,
      "step": 2631
    },
    {
      "epoch": 1.018970189701897,
      "grad_norm": 10.840831756591797,
      "learning_rate": 9.978922011442338e-06,
      "loss": 1.2846,
      "step": 2632
    },
    {
      "epoch": 1.019357336430507,
      "grad_norm": 19.183612823486328,
      "learning_rate": 9.978491848410548e-06,
      "loss": 1.2528,
      "step": 2633
    },
    {
      "epoch": 1.0197444831591174,
      "grad_norm": 10.848145484924316,
      "learning_rate": 9.97806168537876e-06,
      "loss": 1.1285,
      "step": 2634
    },
    {
      "epoch": 1.0201316298877274,
      "grad_norm": 13.361830711364746,
      "learning_rate": 9.97763152234697e-06,
      "loss": 1.6871,
      "step": 2635
    },
    {
      "epoch": 1.0205187766163375,
      "grad_norm": 30.11357879638672,
      "learning_rate": 9.977201359315182e-06,
      "loss": 1.86,
      "step": 2636
    },
    {
      "epoch": 1.0209059233449478,
      "grad_norm": 16.69076156616211,
      "learning_rate": 9.976771196283392e-06,
      "loss": 1.0964,
      "step": 2637
    },
    {
      "epoch": 1.0212930700735579,
      "grad_norm": 16.316829681396484,
      "learning_rate": 9.976341033251603e-06,
      "loss": 1.39,
      "step": 2638
    },
    {
      "epoch": 1.021680216802168,
      "grad_norm": 8.249836921691895,
      "learning_rate": 9.975910870219813e-06,
      "loss": 1.0287,
      "step": 2639
    },
    {
      "epoch": 1.0220673635307782,
      "grad_norm": 16.128551483154297,
      "learning_rate": 9.975480707188026e-06,
      "loss": 1.0403,
      "step": 2640
    },
    {
      "epoch": 1.0224545102593883,
      "grad_norm": 13.948752403259277,
      "learning_rate": 9.975050544156236e-06,
      "loss": 1.3028,
      "step": 2641
    },
    {
      "epoch": 1.0228416569879986,
      "grad_norm": 16.271055221557617,
      "learning_rate": 9.974620381124447e-06,
      "loss": 1.4251,
      "step": 2642
    },
    {
      "epoch": 1.0232288037166086,
      "grad_norm": 18.572494506835938,
      "learning_rate": 9.974190218092657e-06,
      "loss": 2.0752,
      "step": 2643
    },
    {
      "epoch": 1.0236159504452187,
      "grad_norm": 10.610494613647461,
      "learning_rate": 9.97376005506087e-06,
      "loss": 0.7889,
      "step": 2644
    },
    {
      "epoch": 1.024003097173829,
      "grad_norm": 12.43874454498291,
      "learning_rate": 9.97332989202908e-06,
      "loss": 1.5173,
      "step": 2645
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 12.149894714355469,
      "learning_rate": 9.972899728997291e-06,
      "loss": 1.6833,
      "step": 2646
    },
    {
      "epoch": 1.0247773906310491,
      "grad_norm": 22.250722885131836,
      "learning_rate": 9.972469565965501e-06,
      "loss": 2.1066,
      "step": 2647
    },
    {
      "epoch": 1.0251645373596594,
      "grad_norm": 31.769309997558594,
      "learning_rate": 9.972039402933712e-06,
      "loss": 1.2411,
      "step": 2648
    },
    {
      "epoch": 1.0255516840882695,
      "grad_norm": 23.790058135986328,
      "learning_rate": 9.971609239901924e-06,
      "loss": 1.7298,
      "step": 2649
    },
    {
      "epoch": 1.0259388308168795,
      "grad_norm": 10.287089347839355,
      "learning_rate": 9.971179076870135e-06,
      "loss": 1.2483,
      "step": 2650
    },
    {
      "epoch": 1.0263259775454898,
      "grad_norm": 8.567887306213379,
      "learning_rate": 9.970748913838345e-06,
      "loss": 1.117,
      "step": 2651
    },
    {
      "epoch": 1.0267131242740999,
      "grad_norm": 13.543364524841309,
      "learning_rate": 9.970318750806556e-06,
      "loss": 2.0638,
      "step": 2652
    },
    {
      "epoch": 1.02710027100271,
      "grad_norm": 19.337852478027344,
      "learning_rate": 9.969888587774768e-06,
      "loss": 1.9797,
      "step": 2653
    },
    {
      "epoch": 1.0274874177313202,
      "grad_norm": 19.458457946777344,
      "learning_rate": 9.969458424742977e-06,
      "loss": 2.2493,
      "step": 2654
    },
    {
      "epoch": 1.0278745644599303,
      "grad_norm": 14.663086891174316,
      "learning_rate": 9.969028261711189e-06,
      "loss": 1.8724,
      "step": 2655
    },
    {
      "epoch": 1.0282617111885404,
      "grad_norm": 12.829110145568848,
      "learning_rate": 9.9685980986794e-06,
      "loss": 1.5622,
      "step": 2656
    },
    {
      "epoch": 1.0286488579171507,
      "grad_norm": 26.229982376098633,
      "learning_rate": 9.968167935647612e-06,
      "loss": 1.5588,
      "step": 2657
    },
    {
      "epoch": 1.0290360046457607,
      "grad_norm": 15.808579444885254,
      "learning_rate": 9.967737772615821e-06,
      "loss": 1.4224,
      "step": 2658
    },
    {
      "epoch": 1.0294231513743708,
      "grad_norm": 13.52181339263916,
      "learning_rate": 9.967307609584033e-06,
      "loss": 1.7146,
      "step": 2659
    },
    {
      "epoch": 1.029810298102981,
      "grad_norm": 15.731485366821289,
      "learning_rate": 9.966877446552244e-06,
      "loss": 2.1965,
      "step": 2660
    },
    {
      "epoch": 1.0301974448315911,
      "grad_norm": 25.084585189819336,
      "learning_rate": 9.966447283520456e-06,
      "loss": 1.4525,
      "step": 2661
    },
    {
      "epoch": 1.0305845915602012,
      "grad_norm": 21.00584602355957,
      "learning_rate": 9.966017120488665e-06,
      "loss": 1.7112,
      "step": 2662
    },
    {
      "epoch": 1.0309717382888115,
      "grad_norm": 10.195930480957031,
      "learning_rate": 9.965586957456877e-06,
      "loss": 0.7986,
      "step": 2663
    },
    {
      "epoch": 1.0313588850174216,
      "grad_norm": 11.363364219665527,
      "learning_rate": 9.965156794425088e-06,
      "loss": 1.4594,
      "step": 2664
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 17.544635772705078,
      "learning_rate": 9.9647266313933e-06,
      "loss": 1.8068,
      "step": 2665
    },
    {
      "epoch": 1.032133178474642,
      "grad_norm": 14.959081649780273,
      "learning_rate": 9.96429646836151e-06,
      "loss": 1.3973,
      "step": 2666
    },
    {
      "epoch": 1.032520325203252,
      "grad_norm": 19.163761138916016,
      "learning_rate": 9.96386630532972e-06,
      "loss": 2.2167,
      "step": 2667
    },
    {
      "epoch": 1.0329074719318623,
      "grad_norm": 12.247987747192383,
      "learning_rate": 9.963436142297932e-06,
      "loss": 1.188,
      "step": 2668
    },
    {
      "epoch": 1.0332946186604723,
      "grad_norm": 13.103798866271973,
      "learning_rate": 9.963005979266142e-06,
      "loss": 2.1821,
      "step": 2669
    },
    {
      "epoch": 1.0336817653890824,
      "grad_norm": 26.705350875854492,
      "learning_rate": 9.962575816234353e-06,
      "loss": 3.5965,
      "step": 2670
    },
    {
      "epoch": 1.0340689121176927,
      "grad_norm": 12.05018424987793,
      "learning_rate": 9.962145653202565e-06,
      "loss": 1.0646,
      "step": 2671
    },
    {
      "epoch": 1.0344560588463028,
      "grad_norm": 14.519885063171387,
      "learning_rate": 9.961715490170776e-06,
      "loss": 1.3766,
      "step": 2672
    },
    {
      "epoch": 1.0348432055749128,
      "grad_norm": 27.926054000854492,
      "learning_rate": 9.961285327138986e-06,
      "loss": 1.4593,
      "step": 2673
    },
    {
      "epoch": 1.0352303523035231,
      "grad_norm": 19.111059188842773,
      "learning_rate": 9.960855164107197e-06,
      "loss": 1.7912,
      "step": 2674
    },
    {
      "epoch": 1.0356174990321332,
      "grad_norm": 13.485430717468262,
      "learning_rate": 9.960425001075409e-06,
      "loss": 1.1393,
      "step": 2675
    },
    {
      "epoch": 1.0360046457607432,
      "grad_norm": 16.21665382385254,
      "learning_rate": 9.95999483804362e-06,
      "loss": 1.1484,
      "step": 2676
    },
    {
      "epoch": 1.0363917924893535,
      "grad_norm": 11.625178337097168,
      "learning_rate": 9.95956467501183e-06,
      "loss": 1.4928,
      "step": 2677
    },
    {
      "epoch": 1.0367789392179636,
      "grad_norm": 27.97455596923828,
      "learning_rate": 9.959134511980041e-06,
      "loss": 1.6877,
      "step": 2678
    },
    {
      "epoch": 1.0371660859465737,
      "grad_norm": 13.501774787902832,
      "learning_rate": 9.958704348948253e-06,
      "loss": 1.5828,
      "step": 2679
    },
    {
      "epoch": 1.037553232675184,
      "grad_norm": 14.371302604675293,
      "learning_rate": 9.958274185916464e-06,
      "loss": 1.6827,
      "step": 2680
    },
    {
      "epoch": 1.037940379403794,
      "grad_norm": 29.210491180419922,
      "learning_rate": 9.957844022884674e-06,
      "loss": 2.4055,
      "step": 2681
    },
    {
      "epoch": 1.038327526132404,
      "grad_norm": 13.52111530303955,
      "learning_rate": 9.957413859852885e-06,
      "loss": 1.6646,
      "step": 2682
    },
    {
      "epoch": 1.0387146728610144,
      "grad_norm": 28.332359313964844,
      "learning_rate": 9.956983696821097e-06,
      "loss": 1.5333,
      "step": 2683
    },
    {
      "epoch": 1.0391018195896244,
      "grad_norm": 14.842509269714355,
      "learning_rate": 9.956553533789306e-06,
      "loss": 1.5887,
      "step": 2684
    },
    {
      "epoch": 1.0394889663182345,
      "grad_norm": 29.07134437561035,
      "learning_rate": 9.956123370757518e-06,
      "loss": 1.8383,
      "step": 2685
    },
    {
      "epoch": 1.0398761130468448,
      "grad_norm": 16.8466854095459,
      "learning_rate": 9.95569320772573e-06,
      "loss": 1.5168,
      "step": 2686
    },
    {
      "epoch": 1.0402632597754549,
      "grad_norm": 11.186052322387695,
      "learning_rate": 9.95526304469394e-06,
      "loss": 1.4999,
      "step": 2687
    },
    {
      "epoch": 1.040650406504065,
      "grad_norm": 19.452342987060547,
      "learning_rate": 9.95483288166215e-06,
      "loss": 1.1015,
      "step": 2688
    },
    {
      "epoch": 1.0410375532326752,
      "grad_norm": 18.598880767822266,
      "learning_rate": 9.954402718630362e-06,
      "loss": 1.4323,
      "step": 2689
    },
    {
      "epoch": 1.0414246999612853,
      "grad_norm": 25.457122802734375,
      "learning_rate": 9.953972555598571e-06,
      "loss": 1.6193,
      "step": 2690
    },
    {
      "epoch": 1.0418118466898956,
      "grad_norm": 22.03586196899414,
      "learning_rate": 9.953542392566785e-06,
      "loss": 2.5221,
      "step": 2691
    },
    {
      "epoch": 1.0421989934185056,
      "grad_norm": 14.365385055541992,
      "learning_rate": 9.953112229534994e-06,
      "loss": 1.7201,
      "step": 2692
    },
    {
      "epoch": 1.0425861401471157,
      "grad_norm": 19.65247344970703,
      "learning_rate": 9.952682066503206e-06,
      "loss": 1.9715,
      "step": 2693
    },
    {
      "epoch": 1.042973286875726,
      "grad_norm": 21.079561233520508,
      "learning_rate": 9.952251903471415e-06,
      "loss": 1.2685,
      "step": 2694
    },
    {
      "epoch": 1.043360433604336,
      "grad_norm": 15.313301086425781,
      "learning_rate": 9.951821740439629e-06,
      "loss": 1.5385,
      "step": 2695
    },
    {
      "epoch": 1.0437475803329461,
      "grad_norm": 19.117565155029297,
      "learning_rate": 9.951391577407838e-06,
      "loss": 2.2661,
      "step": 2696
    },
    {
      "epoch": 1.0441347270615564,
      "grad_norm": 12.05198860168457,
      "learning_rate": 9.95096141437605e-06,
      "loss": 1.597,
      "step": 2697
    },
    {
      "epoch": 1.0445218737901665,
      "grad_norm": 16.273447036743164,
      "learning_rate": 9.95053125134426e-06,
      "loss": 1.2871,
      "step": 2698
    },
    {
      "epoch": 1.0449090205187765,
      "grad_norm": 21.990150451660156,
      "learning_rate": 9.95010108831247e-06,
      "loss": 1.7014,
      "step": 2699
    },
    {
      "epoch": 1.0452961672473868,
      "grad_norm": 13.947320938110352,
      "learning_rate": 9.949670925280682e-06,
      "loss": 1.0555,
      "step": 2700
    },
    {
      "epoch": 1.045683313975997,
      "grad_norm": 13.0911865234375,
      "learning_rate": 9.949240762248894e-06,
      "loss": 1.3109,
      "step": 2701
    },
    {
      "epoch": 1.046070460704607,
      "grad_norm": 13.747376441955566,
      "learning_rate": 9.948810599217103e-06,
      "loss": 1.6242,
      "step": 2702
    },
    {
      "epoch": 1.0464576074332173,
      "grad_norm": 11.979917526245117,
      "learning_rate": 9.948380436185315e-06,
      "loss": 1.3778,
      "step": 2703
    },
    {
      "epoch": 1.0468447541618273,
      "grad_norm": 13.424851417541504,
      "learning_rate": 9.947950273153526e-06,
      "loss": 1.4891,
      "step": 2704
    },
    {
      "epoch": 1.0472319008904374,
      "grad_norm": 13.677470207214355,
      "learning_rate": 9.947520110121736e-06,
      "loss": 1.1094,
      "step": 2705
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 15.99482250213623,
      "learning_rate": 9.947089947089947e-06,
      "loss": 1.3605,
      "step": 2706
    },
    {
      "epoch": 1.0480061943476577,
      "grad_norm": 18.929494857788086,
      "learning_rate": 9.946659784058159e-06,
      "loss": 1.7863,
      "step": 2707
    },
    {
      "epoch": 1.0483933410762678,
      "grad_norm": 20.880464553833008,
      "learning_rate": 9.94622962102637e-06,
      "loss": 1.3812,
      "step": 2708
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 11.52778434753418,
      "learning_rate": 9.94579945799458e-06,
      "loss": 1.3256,
      "step": 2709
    },
    {
      "epoch": 1.0491676345334882,
      "grad_norm": 23.00777244567871,
      "learning_rate": 9.945369294962793e-06,
      "loss": 1.9646,
      "step": 2710
    },
    {
      "epoch": 1.0495547812620982,
      "grad_norm": 14.476585388183594,
      "learning_rate": 9.944939131931003e-06,
      "loss": 1.5711,
      "step": 2711
    },
    {
      "epoch": 1.0499419279907085,
      "grad_norm": 14.349151611328125,
      "learning_rate": 9.944508968899214e-06,
      "loss": 1.4947,
      "step": 2712
    },
    {
      "epoch": 1.0503290747193186,
      "grad_norm": 42.917572021484375,
      "learning_rate": 9.944078805867424e-06,
      "loss": 2.1038,
      "step": 2713
    },
    {
      "epoch": 1.0507162214479289,
      "grad_norm": 20.197017669677734,
      "learning_rate": 9.943648642835635e-06,
      "loss": 1.2373,
      "step": 2714
    },
    {
      "epoch": 1.051103368176539,
      "grad_norm": 12.746315956115723,
      "learning_rate": 9.943218479803847e-06,
      "loss": 1.2247,
      "step": 2715
    },
    {
      "epoch": 1.051490514905149,
      "grad_norm": 23.372669219970703,
      "learning_rate": 9.942788316772058e-06,
      "loss": 1.6807,
      "step": 2716
    },
    {
      "epoch": 1.0518776616337593,
      "grad_norm": 21.663854598999023,
      "learning_rate": 9.942358153740268e-06,
      "loss": 2.7335,
      "step": 2717
    },
    {
      "epoch": 1.0522648083623694,
      "grad_norm": 11.27500057220459,
      "learning_rate": 9.94192799070848e-06,
      "loss": 1.4169,
      "step": 2718
    },
    {
      "epoch": 1.0526519550909794,
      "grad_norm": 12.647601127624512,
      "learning_rate": 9.94149782767669e-06,
      "loss": 1.6562,
      "step": 2719
    },
    {
      "epoch": 1.0530391018195897,
      "grad_norm": 13.9559965133667,
      "learning_rate": 9.9410676646449e-06,
      "loss": 1.0289,
      "step": 2720
    },
    {
      "epoch": 1.0534262485481998,
      "grad_norm": 23.833908081054688,
      "learning_rate": 9.940637501613112e-06,
      "loss": 1.9903,
      "step": 2721
    },
    {
      "epoch": 1.0538133952768098,
      "grad_norm": 21.276700973510742,
      "learning_rate": 9.940207338581323e-06,
      "loss": 1.625,
      "step": 2722
    },
    {
      "epoch": 1.0542005420054201,
      "grad_norm": 15.311570167541504,
      "learning_rate": 9.939777175549535e-06,
      "loss": 1.5141,
      "step": 2723
    },
    {
      "epoch": 1.0545876887340302,
      "grad_norm": 16.3049259185791,
      "learning_rate": 9.939347012517744e-06,
      "loss": 1.0908,
      "step": 2724
    },
    {
      "epoch": 1.0549748354626403,
      "grad_norm": 9.163262367248535,
      "learning_rate": 9.938916849485956e-06,
      "loss": 1.4294,
      "step": 2725
    },
    {
      "epoch": 1.0553619821912505,
      "grad_norm": 16.678686141967773,
      "learning_rate": 9.938486686454167e-06,
      "loss": 1.7523,
      "step": 2726
    },
    {
      "epoch": 1.0557491289198606,
      "grad_norm": 34.530879974365234,
      "learning_rate": 9.938056523422379e-06,
      "loss": 1.4687,
      "step": 2727
    },
    {
      "epoch": 1.0561362756484707,
      "grad_norm": 28.6888370513916,
      "learning_rate": 9.937626360390588e-06,
      "loss": 1.2836,
      "step": 2728
    },
    {
      "epoch": 1.056523422377081,
      "grad_norm": 10.500811576843262,
      "learning_rate": 9.9371961973588e-06,
      "loss": 1.2797,
      "step": 2729
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 20.976951599121094,
      "learning_rate": 9.936766034327011e-06,
      "loss": 0.8104,
      "step": 2730
    },
    {
      "epoch": 1.057297715834301,
      "grad_norm": 16.457035064697266,
      "learning_rate": 9.936335871295223e-06,
      "loss": 1.4265,
      "step": 2731
    },
    {
      "epoch": 1.0576848625629114,
      "grad_norm": 16.970029830932617,
      "learning_rate": 9.935905708263432e-06,
      "loss": 2.3172,
      "step": 2732
    },
    {
      "epoch": 1.0580720092915215,
      "grad_norm": 20.946565628051758,
      "learning_rate": 9.935475545231644e-06,
      "loss": 1.1788,
      "step": 2733
    },
    {
      "epoch": 1.0584591560201315,
      "grad_norm": 15.498973846435547,
      "learning_rate": 9.935045382199855e-06,
      "loss": 0.9804,
      "step": 2734
    },
    {
      "epoch": 1.0588463027487418,
      "grad_norm": 19.490032196044922,
      "learning_rate": 9.934615219168065e-06,
      "loss": 1.7958,
      "step": 2735
    },
    {
      "epoch": 1.0592334494773519,
      "grad_norm": 11.397605895996094,
      "learning_rate": 9.934185056136276e-06,
      "loss": 1.2566,
      "step": 2736
    },
    {
      "epoch": 1.0596205962059622,
      "grad_norm": 21.040130615234375,
      "learning_rate": 9.933754893104488e-06,
      "loss": 2.3323,
      "step": 2737
    },
    {
      "epoch": 1.0600077429345722,
      "grad_norm": 12.039347648620605,
      "learning_rate": 9.933324730072699e-06,
      "loss": 1.0577,
      "step": 2738
    },
    {
      "epoch": 1.0603948896631823,
      "grad_norm": 14.440267562866211,
      "learning_rate": 9.932894567040909e-06,
      "loss": 1.1544,
      "step": 2739
    },
    {
      "epoch": 1.0607820363917926,
      "grad_norm": 18.006820678710938,
      "learning_rate": 9.93246440400912e-06,
      "loss": 1.2512,
      "step": 2740
    },
    {
      "epoch": 1.0611691831204026,
      "grad_norm": 22.797887802124023,
      "learning_rate": 9.93203424097733e-06,
      "loss": 1.4821,
      "step": 2741
    },
    {
      "epoch": 1.0615563298490127,
      "grad_norm": 21.490034103393555,
      "learning_rate": 9.931604077945543e-06,
      "loss": 1.4541,
      "step": 2742
    },
    {
      "epoch": 1.061943476577623,
      "grad_norm": 7.195273399353027,
      "learning_rate": 9.931173914913753e-06,
      "loss": 1.0029,
      "step": 2743
    },
    {
      "epoch": 1.062330623306233,
      "grad_norm": 11.953699111938477,
      "learning_rate": 9.930743751881964e-06,
      "loss": 1.3832,
      "step": 2744
    },
    {
      "epoch": 1.0627177700348431,
      "grad_norm": 9.280082702636719,
      "learning_rate": 9.930313588850174e-06,
      "loss": 1.26,
      "step": 2745
    },
    {
      "epoch": 1.0631049167634534,
      "grad_norm": 19.15138053894043,
      "learning_rate": 9.929883425818387e-06,
      "loss": 1.3036,
      "step": 2746
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 12.706775665283203,
      "learning_rate": 9.929453262786597e-06,
      "loss": 1.3731,
      "step": 2747
    },
    {
      "epoch": 1.0638792102206736,
      "grad_norm": 14.212179183959961,
      "learning_rate": 9.929023099754808e-06,
      "loss": 1.4801,
      "step": 2748
    },
    {
      "epoch": 1.0642663569492838,
      "grad_norm": 14.667960166931152,
      "learning_rate": 9.92859293672302e-06,
      "loss": 1.1137,
      "step": 2749
    },
    {
      "epoch": 1.064653503677894,
      "grad_norm": 12.149188995361328,
      "learning_rate": 9.92816277369123e-06,
      "loss": 1.673,
      "step": 2750
    },
    {
      "epoch": 1.065040650406504,
      "grad_norm": 14.388509750366211,
      "learning_rate": 9.92773261065944e-06,
      "loss": 1.4741,
      "step": 2751
    },
    {
      "epoch": 1.0654277971351143,
      "grad_norm": 13.235852241516113,
      "learning_rate": 9.927302447627652e-06,
      "loss": 1.1686,
      "step": 2752
    },
    {
      "epoch": 1.0658149438637243,
      "grad_norm": 13.542532920837402,
      "learning_rate": 9.926872284595864e-06,
      "loss": 1.0848,
      "step": 2753
    },
    {
      "epoch": 1.0662020905923344,
      "grad_norm": 16.028987884521484,
      "learning_rate": 9.926442121564073e-06,
      "loss": 1.5912,
      "step": 2754
    },
    {
      "epoch": 1.0665892373209447,
      "grad_norm": 14.137190818786621,
      "learning_rate": 9.926011958532285e-06,
      "loss": 1.8032,
      "step": 2755
    },
    {
      "epoch": 1.0669763840495547,
      "grad_norm": 26.410377502441406,
      "learning_rate": 9.925581795500494e-06,
      "loss": 1.8921,
      "step": 2756
    },
    {
      "epoch": 1.0673635307781648,
      "grad_norm": 26.358259201049805,
      "learning_rate": 9.925151632468708e-06,
      "loss": 1.6838,
      "step": 2757
    },
    {
      "epoch": 1.067750677506775,
      "grad_norm": 43.467864990234375,
      "learning_rate": 9.924721469436917e-06,
      "loss": 1.6382,
      "step": 2758
    },
    {
      "epoch": 1.0681378242353852,
      "grad_norm": 16.535184860229492,
      "learning_rate": 9.924291306405129e-06,
      "loss": 2.8522,
      "step": 2759
    },
    {
      "epoch": 1.0685249709639955,
      "grad_norm": 16.37344741821289,
      "learning_rate": 9.923861143373338e-06,
      "loss": 1.4407,
      "step": 2760
    },
    {
      "epoch": 1.0689121176926055,
      "grad_norm": 16.149799346923828,
      "learning_rate": 9.923430980341552e-06,
      "loss": 1.4488,
      "step": 2761
    },
    {
      "epoch": 1.0692992644212156,
      "grad_norm": 39.77202606201172,
      "learning_rate": 9.923000817309761e-06,
      "loss": 2.5566,
      "step": 2762
    },
    {
      "epoch": 1.0696864111498259,
      "grad_norm": 13.254947662353516,
      "learning_rate": 9.922570654277973e-06,
      "loss": 1.0573,
      "step": 2763
    },
    {
      "epoch": 1.070073557878436,
      "grad_norm": 21.62392807006836,
      "learning_rate": 9.922140491246182e-06,
      "loss": 2.2617,
      "step": 2764
    },
    {
      "epoch": 1.070460704607046,
      "grad_norm": 16.465484619140625,
      "learning_rate": 9.921710328214394e-06,
      "loss": 1.5905,
      "step": 2765
    },
    {
      "epoch": 1.0708478513356563,
      "grad_norm": 21.469348907470703,
      "learning_rate": 9.921280165182605e-06,
      "loss": 1.5324,
      "step": 2766
    },
    {
      "epoch": 1.0712349980642664,
      "grad_norm": 17.836688995361328,
      "learning_rate": 9.920850002150817e-06,
      "loss": 1.3134,
      "step": 2767
    },
    {
      "epoch": 1.0716221447928764,
      "grad_norm": 14.737174987792969,
      "learning_rate": 9.920419839119026e-06,
      "loss": 1.2941,
      "step": 2768
    },
    {
      "epoch": 1.0720092915214867,
      "grad_norm": 19.31356430053711,
      "learning_rate": 9.919989676087238e-06,
      "loss": 1.1312,
      "step": 2769
    },
    {
      "epoch": 1.0723964382500968,
      "grad_norm": 17.751359939575195,
      "learning_rate": 9.91955951305545e-06,
      "loss": 1.6935,
      "step": 2770
    },
    {
      "epoch": 1.0727835849787069,
      "grad_norm": 11.188058853149414,
      "learning_rate": 9.919129350023659e-06,
      "loss": 1.4623,
      "step": 2771
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 17.781143188476562,
      "learning_rate": 9.91869918699187e-06,
      "loss": 2.043,
      "step": 2772
    },
    {
      "epoch": 1.0735578784359272,
      "grad_norm": 19.81410026550293,
      "learning_rate": 9.918269023960082e-06,
      "loss": 1.4629,
      "step": 2773
    },
    {
      "epoch": 1.0739450251645373,
      "grad_norm": 14.021395683288574,
      "learning_rate": 9.917838860928293e-06,
      "loss": 1.6348,
      "step": 2774
    },
    {
      "epoch": 1.0743321718931476,
      "grad_norm": 9.510770797729492,
      "learning_rate": 9.917408697896503e-06,
      "loss": 1.3489,
      "step": 2775
    },
    {
      "epoch": 1.0747193186217576,
      "grad_norm": 11.668146133422852,
      "learning_rate": 9.916978534864714e-06,
      "loss": 1.2344,
      "step": 2776
    },
    {
      "epoch": 1.0751064653503677,
      "grad_norm": 11.00218391418457,
      "learning_rate": 9.916548371832926e-06,
      "loss": 1.0664,
      "step": 2777
    },
    {
      "epoch": 1.075493612078978,
      "grad_norm": 12.528824806213379,
      "learning_rate": 9.916118208801137e-06,
      "loss": 1.238,
      "step": 2778
    },
    {
      "epoch": 1.075880758807588,
      "grad_norm": 16.28148078918457,
      "learning_rate": 9.915688045769347e-06,
      "loss": 1.4971,
      "step": 2779
    },
    {
      "epoch": 1.076267905536198,
      "grad_norm": 15.889019966125488,
      "learning_rate": 9.915257882737558e-06,
      "loss": 1.5115,
      "step": 2780
    },
    {
      "epoch": 1.0766550522648084,
      "grad_norm": 26.95136070251465,
      "learning_rate": 9.91482771970577e-06,
      "loss": 1.8876,
      "step": 2781
    },
    {
      "epoch": 1.0770421989934185,
      "grad_norm": 11.517533302307129,
      "learning_rate": 9.914397556673981e-06,
      "loss": 1.6444,
      "step": 2782
    },
    {
      "epoch": 1.0774293457220288,
      "grad_norm": 11.656947135925293,
      "learning_rate": 9.91396739364219e-06,
      "loss": 0.9656,
      "step": 2783
    },
    {
      "epoch": 1.0778164924506388,
      "grad_norm": 21.93158531188965,
      "learning_rate": 9.913537230610402e-06,
      "loss": 1.7222,
      "step": 2784
    },
    {
      "epoch": 1.0782036391792489,
      "grad_norm": 19.456613540649414,
      "learning_rate": 9.913107067578614e-06,
      "loss": 1.7828,
      "step": 2785
    },
    {
      "epoch": 1.0785907859078592,
      "grad_norm": 17.169706344604492,
      "learning_rate": 9.912676904546823e-06,
      "loss": 1.2231,
      "step": 2786
    },
    {
      "epoch": 1.0789779326364692,
      "grad_norm": 16.774999618530273,
      "learning_rate": 9.912246741515035e-06,
      "loss": 1.7352,
      "step": 2787
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 17.424175262451172,
      "learning_rate": 9.911816578483246e-06,
      "loss": 1.8645,
      "step": 2788
    },
    {
      "epoch": 1.0797522260936896,
      "grad_norm": 24.920774459838867,
      "learning_rate": 9.911386415451458e-06,
      "loss": 2.1198,
      "step": 2789
    },
    {
      "epoch": 1.0801393728222997,
      "grad_norm": 15.377825736999512,
      "learning_rate": 9.910956252419667e-06,
      "loss": 1.6927,
      "step": 2790
    },
    {
      "epoch": 1.0805265195509097,
      "grad_norm": 7.957217693328857,
      "learning_rate": 9.910526089387879e-06,
      "loss": 1.3961,
      "step": 2791
    },
    {
      "epoch": 1.08091366627952,
      "grad_norm": 19.455642700195312,
      "learning_rate": 9.91009592635609e-06,
      "loss": 1.1531,
      "step": 2792
    },
    {
      "epoch": 1.08130081300813,
      "grad_norm": 13.98209285736084,
      "learning_rate": 9.909665763324302e-06,
      "loss": 1.2244,
      "step": 2793
    },
    {
      "epoch": 1.0816879597367401,
      "grad_norm": 11.010237693786621,
      "learning_rate": 9.909235600292511e-06,
      "loss": 1.3215,
      "step": 2794
    },
    {
      "epoch": 1.0820751064653504,
      "grad_norm": 15.592874526977539,
      "learning_rate": 9.908805437260723e-06,
      "loss": 1.3656,
      "step": 2795
    },
    {
      "epoch": 1.0824622531939605,
      "grad_norm": 31.84793472290039,
      "learning_rate": 9.908375274228934e-06,
      "loss": 1.6123,
      "step": 2796
    },
    {
      "epoch": 1.0828493999225706,
      "grad_norm": 29.248933792114258,
      "learning_rate": 9.907945111197146e-06,
      "loss": 1.9832,
      "step": 2797
    },
    {
      "epoch": 1.0832365466511809,
      "grad_norm": 13.054574012756348,
      "learning_rate": 9.907514948165355e-06,
      "loss": 0.8483,
      "step": 2798
    },
    {
      "epoch": 1.083623693379791,
      "grad_norm": 11.118550300598145,
      "learning_rate": 9.907084785133567e-06,
      "loss": 1.2801,
      "step": 2799
    },
    {
      "epoch": 1.084010840108401,
      "grad_norm": 13.279389381408691,
      "learning_rate": 9.906654622101778e-06,
      "loss": 1.5434,
      "step": 2800
    },
    {
      "epoch": 1.0843979868370113,
      "grad_norm": 12.259542465209961,
      "learning_rate": 9.906224459069988e-06,
      "loss": 1.5604,
      "step": 2801
    },
    {
      "epoch": 1.0847851335656213,
      "grad_norm": 13.227477073669434,
      "learning_rate": 9.9057942960382e-06,
      "loss": 1.2603,
      "step": 2802
    },
    {
      "epoch": 1.0851722802942314,
      "grad_norm": 13.82474136352539,
      "learning_rate": 9.90536413300641e-06,
      "loss": 1.2329,
      "step": 2803
    },
    {
      "epoch": 1.0855594270228417,
      "grad_norm": 20.358135223388672,
      "learning_rate": 9.904933969974622e-06,
      "loss": 1.7101,
      "step": 2804
    },
    {
      "epoch": 1.0859465737514518,
      "grad_norm": 19.31425666809082,
      "learning_rate": 9.904503806942832e-06,
      "loss": 1.3342,
      "step": 2805
    },
    {
      "epoch": 1.086333720480062,
      "grad_norm": 14.565228462219238,
      "learning_rate": 9.904073643911043e-06,
      "loss": 1.0581,
      "step": 2806
    },
    {
      "epoch": 1.0867208672086721,
      "grad_norm": 27.635637283325195,
      "learning_rate": 9.903643480879253e-06,
      "loss": 1.1022,
      "step": 2807
    },
    {
      "epoch": 1.0871080139372822,
      "grad_norm": 15.063994407653809,
      "learning_rate": 9.903213317847466e-06,
      "loss": 1.5473,
      "step": 2808
    },
    {
      "epoch": 1.0874951606658925,
      "grad_norm": 16.790645599365234,
      "learning_rate": 9.902783154815676e-06,
      "loss": 1.6172,
      "step": 2809
    },
    {
      "epoch": 1.0878823073945025,
      "grad_norm": 22.252273559570312,
      "learning_rate": 9.902352991783887e-06,
      "loss": 2.1159,
      "step": 2810
    },
    {
      "epoch": 1.0882694541231126,
      "grad_norm": 44.53988265991211,
      "learning_rate": 9.901922828752097e-06,
      "loss": 1.696,
      "step": 2811
    },
    {
      "epoch": 1.0886566008517229,
      "grad_norm": 14.825989723205566,
      "learning_rate": 9.90149266572031e-06,
      "loss": 1.5576,
      "step": 2812
    },
    {
      "epoch": 1.089043747580333,
      "grad_norm": 19.902507781982422,
      "learning_rate": 9.90106250268852e-06,
      "loss": 1.6071,
      "step": 2813
    },
    {
      "epoch": 1.089430894308943,
      "grad_norm": 18.876441955566406,
      "learning_rate": 9.900632339656731e-06,
      "loss": 1.9451,
      "step": 2814
    },
    {
      "epoch": 1.0898180410375533,
      "grad_norm": 14.324376106262207,
      "learning_rate": 9.900202176624941e-06,
      "loss": 1.5577,
      "step": 2815
    },
    {
      "epoch": 1.0902051877661634,
      "grad_norm": 14.74123477935791,
      "learning_rate": 9.899772013593152e-06,
      "loss": 1.3675,
      "step": 2816
    },
    {
      "epoch": 1.0905923344947734,
      "grad_norm": 11.133639335632324,
      "learning_rate": 9.899341850561364e-06,
      "loss": 0.9762,
      "step": 2817
    },
    {
      "epoch": 1.0909794812233837,
      "grad_norm": 14.743924140930176,
      "learning_rate": 9.898911687529575e-06,
      "loss": 1.3684,
      "step": 2818
    },
    {
      "epoch": 1.0913666279519938,
      "grad_norm": 17.064224243164062,
      "learning_rate": 9.898481524497785e-06,
      "loss": 1.9085,
      "step": 2819
    },
    {
      "epoch": 1.0917537746806039,
      "grad_norm": 13.625917434692383,
      "learning_rate": 9.898051361465996e-06,
      "loss": 1.4753,
      "step": 2820
    },
    {
      "epoch": 1.0921409214092141,
      "grad_norm": 13.991575241088867,
      "learning_rate": 9.897621198434208e-06,
      "loss": 1.0837,
      "step": 2821
    },
    {
      "epoch": 1.0925280681378242,
      "grad_norm": 8.075464248657227,
      "learning_rate": 9.897191035402417e-06,
      "loss": 0.9777,
      "step": 2822
    },
    {
      "epoch": 1.0929152148664343,
      "grad_norm": 14.909154891967773,
      "learning_rate": 9.896760872370629e-06,
      "loss": 1.3451,
      "step": 2823
    },
    {
      "epoch": 1.0933023615950446,
      "grad_norm": 20.100353240966797,
      "learning_rate": 9.89633070933884e-06,
      "loss": 1.4664,
      "step": 2824
    },
    {
      "epoch": 1.0936895083236546,
      "grad_norm": 9.702215194702148,
      "learning_rate": 9.895900546307052e-06,
      "loss": 0.874,
      "step": 2825
    },
    {
      "epoch": 1.0940766550522647,
      "grad_norm": 12.105074882507324,
      "learning_rate": 9.895470383275261e-06,
      "loss": 1.1986,
      "step": 2826
    },
    {
      "epoch": 1.094463801780875,
      "grad_norm": 17.996173858642578,
      "learning_rate": 9.895040220243473e-06,
      "loss": 1.2679,
      "step": 2827
    },
    {
      "epoch": 1.094850948509485,
      "grad_norm": 17.211721420288086,
      "learning_rate": 9.894610057211684e-06,
      "loss": 1.3798,
      "step": 2828
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 18.759727478027344,
      "learning_rate": 9.894179894179896e-06,
      "loss": 2.0501,
      "step": 2829
    },
    {
      "epoch": 1.0956252419667054,
      "grad_norm": 14.94530963897705,
      "learning_rate": 9.893749731148105e-06,
      "loss": 1.109,
      "step": 2830
    },
    {
      "epoch": 1.0960123886953155,
      "grad_norm": 20.430320739746094,
      "learning_rate": 9.893319568116317e-06,
      "loss": 1.41,
      "step": 2831
    },
    {
      "epoch": 1.0963995354239258,
      "grad_norm": 13.926247596740723,
      "learning_rate": 9.892889405084528e-06,
      "loss": 1.0417,
      "step": 2832
    },
    {
      "epoch": 1.0967866821525358,
      "grad_norm": 11.009696960449219,
      "learning_rate": 9.89245924205274e-06,
      "loss": 0.8338,
      "step": 2833
    },
    {
      "epoch": 1.097173828881146,
      "grad_norm": 16.837055206298828,
      "learning_rate": 9.89202907902095e-06,
      "loss": 1.5148,
      "step": 2834
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 16.786354064941406,
      "learning_rate": 9.89159891598916e-06,
      "loss": 2.2301,
      "step": 2835
    },
    {
      "epoch": 1.0979481223383663,
      "grad_norm": 21.383834838867188,
      "learning_rate": 9.891168752957372e-06,
      "loss": 1.5593,
      "step": 2836
    },
    {
      "epoch": 1.0983352690669763,
      "grad_norm": 17.937488555908203,
      "learning_rate": 9.890738589925582e-06,
      "loss": 2.2074,
      "step": 2837
    },
    {
      "epoch": 1.0987224157955866,
      "grad_norm": 21.846654891967773,
      "learning_rate": 9.890308426893793e-06,
      "loss": 2.0217,
      "step": 2838
    },
    {
      "epoch": 1.0991095625241967,
      "grad_norm": 16.236703872680664,
      "learning_rate": 9.889878263862005e-06,
      "loss": 1.436,
      "step": 2839
    },
    {
      "epoch": 1.0994967092528067,
      "grad_norm": 19.087764739990234,
      "learning_rate": 9.889448100830216e-06,
      "loss": 2.0653,
      "step": 2840
    },
    {
      "epoch": 1.099883855981417,
      "grad_norm": 9.46346664428711,
      "learning_rate": 9.889017937798426e-06,
      "loss": 1.2812,
      "step": 2841
    },
    {
      "epoch": 1.100271002710027,
      "grad_norm": 16.659927368164062,
      "learning_rate": 9.888587774766637e-06,
      "loss": 1.5111,
      "step": 2842
    },
    {
      "epoch": 1.1006581494386372,
      "grad_norm": 13.559014320373535,
      "learning_rate": 9.888157611734849e-06,
      "loss": 1.2929,
      "step": 2843
    },
    {
      "epoch": 1.1010452961672474,
      "grad_norm": 35.87084197998047,
      "learning_rate": 9.88772744870306e-06,
      "loss": 2.0532,
      "step": 2844
    },
    {
      "epoch": 1.1014324428958575,
      "grad_norm": 22.946332931518555,
      "learning_rate": 9.88729728567127e-06,
      "loss": 1.7546,
      "step": 2845
    },
    {
      "epoch": 1.1018195896244676,
      "grad_norm": 28.69762420654297,
      "learning_rate": 9.886867122639481e-06,
      "loss": 1.5162,
      "step": 2846
    },
    {
      "epoch": 1.1022067363530779,
      "grad_norm": 16.257526397705078,
      "learning_rate": 9.886436959607693e-06,
      "loss": 1.4686,
      "step": 2847
    },
    {
      "epoch": 1.102593883081688,
      "grad_norm": 11.159704208374023,
      "learning_rate": 9.886006796575904e-06,
      "loss": 0.77,
      "step": 2848
    },
    {
      "epoch": 1.102981029810298,
      "grad_norm": 15.644683837890625,
      "learning_rate": 9.885576633544114e-06,
      "loss": 1.1403,
      "step": 2849
    },
    {
      "epoch": 1.1033681765389083,
      "grad_norm": 14.23741626739502,
      "learning_rate": 9.885146470512325e-06,
      "loss": 1.6198,
      "step": 2850
    },
    {
      "epoch": 1.1037553232675184,
      "grad_norm": 28.121519088745117,
      "learning_rate": 9.884716307480537e-06,
      "loss": 1.9775,
      "step": 2851
    },
    {
      "epoch": 1.1041424699961286,
      "grad_norm": 25.742584228515625,
      "learning_rate": 9.884286144448746e-06,
      "loss": 1.7004,
      "step": 2852
    },
    {
      "epoch": 1.1045296167247387,
      "grad_norm": 17.49236297607422,
      "learning_rate": 9.883855981416958e-06,
      "loss": 1.1611,
      "step": 2853
    },
    {
      "epoch": 1.1049167634533488,
      "grad_norm": 17.130149841308594,
      "learning_rate": 9.883425818385169e-06,
      "loss": 2.3317,
      "step": 2854
    },
    {
      "epoch": 1.105303910181959,
      "grad_norm": 13.101213455200195,
      "learning_rate": 9.88299565535338e-06,
      "loss": 1.4709,
      "step": 2855
    },
    {
      "epoch": 1.1056910569105691,
      "grad_norm": 19.066110610961914,
      "learning_rate": 9.88256549232159e-06,
      "loss": 1.4789,
      "step": 2856
    },
    {
      "epoch": 1.1060782036391792,
      "grad_norm": 15.437875747680664,
      "learning_rate": 9.882135329289802e-06,
      "loss": 1.57,
      "step": 2857
    },
    {
      "epoch": 1.1064653503677895,
      "grad_norm": 18.65119171142578,
      "learning_rate": 9.881705166258011e-06,
      "loss": 1.7021,
      "step": 2858
    },
    {
      "epoch": 1.1068524970963995,
      "grad_norm": 15.455779075622559,
      "learning_rate": 9.881275003226225e-06,
      "loss": 0.9609,
      "step": 2859
    },
    {
      "epoch": 1.1072396438250096,
      "grad_norm": 13.241989135742188,
      "learning_rate": 9.880844840194434e-06,
      "loss": 1.6783,
      "step": 2860
    },
    {
      "epoch": 1.10762679055362,
      "grad_norm": 15.756484985351562,
      "learning_rate": 9.880414677162646e-06,
      "loss": 1.8227,
      "step": 2861
    },
    {
      "epoch": 1.10801393728223,
      "grad_norm": 17.05937957763672,
      "learning_rate": 9.879984514130855e-06,
      "loss": 1.927,
      "step": 2862
    },
    {
      "epoch": 1.10840108401084,
      "grad_norm": 18.8395938873291,
      "learning_rate": 9.879554351099068e-06,
      "loss": 1.5739,
      "step": 2863
    },
    {
      "epoch": 1.1087882307394503,
      "grad_norm": 12.058917999267578,
      "learning_rate": 9.879124188067278e-06,
      "loss": 0.7897,
      "step": 2864
    },
    {
      "epoch": 1.1091753774680604,
      "grad_norm": 15.459847450256348,
      "learning_rate": 9.87869402503549e-06,
      "loss": 1.302,
      "step": 2865
    },
    {
      "epoch": 1.1095625241966705,
      "grad_norm": 14.86146068572998,
      "learning_rate": 9.8782638620037e-06,
      "loss": 1.6483,
      "step": 2866
    },
    {
      "epoch": 1.1099496709252807,
      "grad_norm": 15.276413917541504,
      "learning_rate": 9.87783369897191e-06,
      "loss": 1.627,
      "step": 2867
    },
    {
      "epoch": 1.1103368176538908,
      "grad_norm": 17.290611267089844,
      "learning_rate": 9.877403535940122e-06,
      "loss": 1.1946,
      "step": 2868
    },
    {
      "epoch": 1.1107239643825009,
      "grad_norm": 18.400821685791016,
      "learning_rate": 9.876973372908334e-06,
      "loss": 1.4861,
      "step": 2869
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 23.436397552490234,
      "learning_rate": 9.876543209876543e-06,
      "loss": 1.6987,
      "step": 2870
    },
    {
      "epoch": 1.1114982578397212,
      "grad_norm": 20.829509735107422,
      "learning_rate": 9.876113046844755e-06,
      "loss": 1.5905,
      "step": 2871
    },
    {
      "epoch": 1.1118854045683313,
      "grad_norm": 13.985857009887695,
      "learning_rate": 9.875682883812966e-06,
      "loss": 0.9942,
      "step": 2872
    },
    {
      "epoch": 1.1122725512969416,
      "grad_norm": 16.4593563079834,
      "learning_rate": 9.875252720781176e-06,
      "loss": 2.0655,
      "step": 2873
    },
    {
      "epoch": 1.1126596980255516,
      "grad_norm": 20.540531158447266,
      "learning_rate": 9.874822557749389e-06,
      "loss": 1.2762,
      "step": 2874
    },
    {
      "epoch": 1.113046844754162,
      "grad_norm": 12.928327560424805,
      "learning_rate": 9.874392394717599e-06,
      "loss": 1.6058,
      "step": 2875
    },
    {
      "epoch": 1.113433991482772,
      "grad_norm": 12.820450782775879,
      "learning_rate": 9.87396223168581e-06,
      "loss": 0.8833,
      "step": 2876
    },
    {
      "epoch": 1.113821138211382,
      "grad_norm": 16.101985931396484,
      "learning_rate": 9.87353206865402e-06,
      "loss": 1.5987,
      "step": 2877
    },
    {
      "epoch": 1.1142082849399924,
      "grad_norm": 13.91110897064209,
      "learning_rate": 9.873101905622233e-06,
      "loss": 1.6438,
      "step": 2878
    },
    {
      "epoch": 1.1145954316686024,
      "grad_norm": 21.589624404907227,
      "learning_rate": 9.872671742590443e-06,
      "loss": 2.0941,
      "step": 2879
    },
    {
      "epoch": 1.1149825783972125,
      "grad_norm": 14.72765064239502,
      "learning_rate": 9.872241579558654e-06,
      "loss": 1.1228,
      "step": 2880
    },
    {
      "epoch": 1.1153697251258228,
      "grad_norm": 15.649147987365723,
      "learning_rate": 9.871811416526864e-06,
      "loss": 1.1748,
      "step": 2881
    },
    {
      "epoch": 1.1157568718544328,
      "grad_norm": 34.76520538330078,
      "learning_rate": 9.871381253495075e-06,
      "loss": 2.3709,
      "step": 2882
    },
    {
      "epoch": 1.116144018583043,
      "grad_norm": 32.62091827392578,
      "learning_rate": 9.870951090463287e-06,
      "loss": 2.3695,
      "step": 2883
    },
    {
      "epoch": 1.1165311653116532,
      "grad_norm": 29.130706787109375,
      "learning_rate": 9.870520927431498e-06,
      "loss": 1.9355,
      "step": 2884
    },
    {
      "epoch": 1.1169183120402633,
      "grad_norm": 19.52931022644043,
      "learning_rate": 9.870090764399708e-06,
      "loss": 1.4003,
      "step": 2885
    },
    {
      "epoch": 1.1173054587688733,
      "grad_norm": 14.301163673400879,
      "learning_rate": 9.86966060136792e-06,
      "loss": 1.3021,
      "step": 2886
    },
    {
      "epoch": 1.1176926054974836,
      "grad_norm": 20.859174728393555,
      "learning_rate": 9.86923043833613e-06,
      "loss": 1.327,
      "step": 2887
    },
    {
      "epoch": 1.1180797522260937,
      "grad_norm": 22.406583786010742,
      "learning_rate": 9.86880027530434e-06,
      "loss": 2.6807,
      "step": 2888
    },
    {
      "epoch": 1.1184668989547037,
      "grad_norm": 14.193469047546387,
      "learning_rate": 9.868370112272552e-06,
      "loss": 1.1535,
      "step": 2889
    },
    {
      "epoch": 1.118854045683314,
      "grad_norm": 13.19684886932373,
      "learning_rate": 9.867939949240763e-06,
      "loss": 1.0448,
      "step": 2890
    },
    {
      "epoch": 1.119241192411924,
      "grad_norm": 12.06394100189209,
      "learning_rate": 9.867509786208975e-06,
      "loss": 0.476,
      "step": 2891
    },
    {
      "epoch": 1.1196283391405342,
      "grad_norm": 13.442334175109863,
      "learning_rate": 9.867079623177184e-06,
      "loss": 0.9449,
      "step": 2892
    },
    {
      "epoch": 1.1200154858691445,
      "grad_norm": 12.541753768920898,
      "learning_rate": 9.866649460145396e-06,
      "loss": 1.4878,
      "step": 2893
    },
    {
      "epoch": 1.1204026325977545,
      "grad_norm": 20.570653915405273,
      "learning_rate": 9.866219297113607e-06,
      "loss": 1.4762,
      "step": 2894
    },
    {
      "epoch": 1.1207897793263646,
      "grad_norm": 23.315093994140625,
      "learning_rate": 9.865789134081819e-06,
      "loss": 1.8936,
      "step": 2895
    },
    {
      "epoch": 1.1211769260549749,
      "grad_norm": 12.95217514038086,
      "learning_rate": 9.865358971050028e-06,
      "loss": 1.039,
      "step": 2896
    },
    {
      "epoch": 1.121564072783585,
      "grad_norm": 14.480637550354004,
      "learning_rate": 9.86492880801824e-06,
      "loss": 1.6756,
      "step": 2897
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 18.257648468017578,
      "learning_rate": 9.864498644986451e-06,
      "loss": 1.6468,
      "step": 2898
    },
    {
      "epoch": 1.1223383662408053,
      "grad_norm": 8.871774673461914,
      "learning_rate": 9.864068481954663e-06,
      "loss": 0.9127,
      "step": 2899
    },
    {
      "epoch": 1.1227255129694154,
      "grad_norm": 14.512118339538574,
      "learning_rate": 9.863638318922872e-06,
      "loss": 1.3284,
      "step": 2900
    },
    {
      "epoch": 1.1231126596980257,
      "grad_norm": 22.90131950378418,
      "learning_rate": 9.863208155891084e-06,
      "loss": 1.5087,
      "step": 2901
    },
    {
      "epoch": 1.1234998064266357,
      "grad_norm": 19.218748092651367,
      "learning_rate": 9.862777992859295e-06,
      "loss": 1.9366,
      "step": 2902
    },
    {
      "epoch": 1.1238869531552458,
      "grad_norm": 23.199901580810547,
      "learning_rate": 9.862347829827505e-06,
      "loss": 1.5777,
      "step": 2903
    },
    {
      "epoch": 1.124274099883856,
      "grad_norm": 21.8846378326416,
      "learning_rate": 9.861917666795716e-06,
      "loss": 1.6683,
      "step": 2904
    },
    {
      "epoch": 1.1246612466124661,
      "grad_norm": 31.64393424987793,
      "learning_rate": 9.861487503763928e-06,
      "loss": 1.9385,
      "step": 2905
    },
    {
      "epoch": 1.1250483933410762,
      "grad_norm": 13.708986282348633,
      "learning_rate": 9.861057340732139e-06,
      "loss": 1.0758,
      "step": 2906
    },
    {
      "epoch": 1.1254355400696865,
      "grad_norm": 26.54155731201172,
      "learning_rate": 9.860627177700349e-06,
      "loss": 1.6345,
      "step": 2907
    },
    {
      "epoch": 1.1258226867982966,
      "grad_norm": 20.284221649169922,
      "learning_rate": 9.86019701466856e-06,
      "loss": 1.5414,
      "step": 2908
    },
    {
      "epoch": 1.1262098335269066,
      "grad_norm": 15.819384574890137,
      "learning_rate": 9.85976685163677e-06,
      "loss": 1.5592,
      "step": 2909
    },
    {
      "epoch": 1.126596980255517,
      "grad_norm": 12.518463134765625,
      "learning_rate": 9.859336688604983e-06,
      "loss": 1.4985,
      "step": 2910
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 12.258554458618164,
      "learning_rate": 9.858906525573193e-06,
      "loss": 1.2325,
      "step": 2911
    },
    {
      "epoch": 1.127371273712737,
      "grad_norm": 19.8494815826416,
      "learning_rate": 9.858476362541404e-06,
      "loss": 1.5866,
      "step": 2912
    },
    {
      "epoch": 1.1277584204413473,
      "grad_norm": 19.8845272064209,
      "learning_rate": 9.858046199509616e-06,
      "loss": 1.5697,
      "step": 2913
    },
    {
      "epoch": 1.1281455671699574,
      "grad_norm": 20.78984260559082,
      "learning_rate": 9.857616036477827e-06,
      "loss": 1.0873,
      "step": 2914
    },
    {
      "epoch": 1.1285327138985675,
      "grad_norm": 17.576236724853516,
      "learning_rate": 9.857185873446037e-06,
      "loss": 2.1668,
      "step": 2915
    },
    {
      "epoch": 1.1289198606271778,
      "grad_norm": 39.09327697753906,
      "learning_rate": 9.856755710414248e-06,
      "loss": 2.1146,
      "step": 2916
    },
    {
      "epoch": 1.1293070073557878,
      "grad_norm": 26.03662109375,
      "learning_rate": 9.85632554738246e-06,
      "loss": 1.851,
      "step": 2917
    },
    {
      "epoch": 1.1296941540843979,
      "grad_norm": 22.948375701904297,
      "learning_rate": 9.85589538435067e-06,
      "loss": 1.8922,
      "step": 2918
    },
    {
      "epoch": 1.1300813008130082,
      "grad_norm": 40.99020767211914,
      "learning_rate": 9.85546522131888e-06,
      "loss": 2.5331,
      "step": 2919
    },
    {
      "epoch": 1.1304684475416182,
      "grad_norm": 22.032133102416992,
      "learning_rate": 9.855035058287092e-06,
      "loss": 2.031,
      "step": 2920
    },
    {
      "epoch": 1.1308555942702285,
      "grad_norm": 15.252873420715332,
      "learning_rate": 9.854604895255303e-06,
      "loss": 1.6853,
      "step": 2921
    },
    {
      "epoch": 1.1312427409988386,
      "grad_norm": 21.01805305480957,
      "learning_rate": 9.854174732223513e-06,
      "loss": 1.5046,
      "step": 2922
    },
    {
      "epoch": 1.1316298877274487,
      "grad_norm": 20.58396339416504,
      "learning_rate": 9.853744569191725e-06,
      "loss": 1.9132,
      "step": 2923
    },
    {
      "epoch": 1.1320170344560587,
      "grad_norm": 15.415303230285645,
      "learning_rate": 9.853314406159934e-06,
      "loss": 1.3863,
      "step": 2924
    },
    {
      "epoch": 1.132404181184669,
      "grad_norm": 30.044641494750977,
      "learning_rate": 9.852884243128147e-06,
      "loss": 3.0817,
      "step": 2925
    },
    {
      "epoch": 1.132791327913279,
      "grad_norm": 20.82019805908203,
      "learning_rate": 9.852454080096357e-06,
      "loss": 2.3817,
      "step": 2926
    },
    {
      "epoch": 1.1331784746418894,
      "grad_norm": 11.634734153747559,
      "learning_rate": 9.852023917064569e-06,
      "loss": 1.2372,
      "step": 2927
    },
    {
      "epoch": 1.1335656213704994,
      "grad_norm": 9.812459945678711,
      "learning_rate": 9.851593754032778e-06,
      "loss": 1.2807,
      "step": 2928
    },
    {
      "epoch": 1.1339527680991095,
      "grad_norm": 23.327823638916016,
      "learning_rate": 9.851163591000991e-06,
      "loss": 1.7764,
      "step": 2929
    },
    {
      "epoch": 1.1343399148277198,
      "grad_norm": 21.15706443786621,
      "learning_rate": 9.850733427969201e-06,
      "loss": 1.7085,
      "step": 2930
    },
    {
      "epoch": 1.1347270615563299,
      "grad_norm": 18.809093475341797,
      "learning_rate": 9.850303264937413e-06,
      "loss": 1.8063,
      "step": 2931
    },
    {
      "epoch": 1.13511420828494,
      "grad_norm": 13.963958740234375,
      "learning_rate": 9.849873101905622e-06,
      "loss": 1.4123,
      "step": 2932
    },
    {
      "epoch": 1.1355013550135502,
      "grad_norm": 14.73353385925293,
      "learning_rate": 9.849442938873834e-06,
      "loss": 1.4928,
      "step": 2933
    },
    {
      "epoch": 1.1358885017421603,
      "grad_norm": 11.870315551757812,
      "learning_rate": 9.849012775842045e-06,
      "loss": 1.2289,
      "step": 2934
    },
    {
      "epoch": 1.1362756484707703,
      "grad_norm": 15.009160995483398,
      "learning_rate": 9.848582612810257e-06,
      "loss": 1.5651,
      "step": 2935
    },
    {
      "epoch": 1.1366627951993806,
      "grad_norm": 20.653011322021484,
      "learning_rate": 9.848152449778466e-06,
      "loss": 1.5242,
      "step": 2936
    },
    {
      "epoch": 1.1370499419279907,
      "grad_norm": 18.36014747619629,
      "learning_rate": 9.847722286746678e-06,
      "loss": 1.5932,
      "step": 2937
    },
    {
      "epoch": 1.1374370886566008,
      "grad_norm": 13.839876174926758,
      "learning_rate": 9.847292123714889e-06,
      "loss": 1.4004,
      "step": 2938
    },
    {
      "epoch": 1.137824235385211,
      "grad_norm": 12.33191204071045,
      "learning_rate": 9.846861960683099e-06,
      "loss": 0.8605,
      "step": 2939
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 31.5849609375,
      "learning_rate": 9.84643179765131e-06,
      "loss": 1.5645,
      "step": 2940
    },
    {
      "epoch": 1.1385985288424312,
      "grad_norm": 19.39096450805664,
      "learning_rate": 9.846001634619522e-06,
      "loss": 1.6948,
      "step": 2941
    },
    {
      "epoch": 1.1389856755710415,
      "grad_norm": 13.62159252166748,
      "learning_rate": 9.845571471587733e-06,
      "loss": 1.5285,
      "step": 2942
    },
    {
      "epoch": 1.1393728222996515,
      "grad_norm": 29.118619918823242,
      "learning_rate": 9.845141308555943e-06,
      "loss": 1.8037,
      "step": 2943
    },
    {
      "epoch": 1.1397599690282618,
      "grad_norm": 12.638991355895996,
      "learning_rate": 9.844711145524154e-06,
      "loss": 1.008,
      "step": 2944
    },
    {
      "epoch": 1.1401471157568719,
      "grad_norm": 16.07928466796875,
      "learning_rate": 9.844280982492366e-06,
      "loss": 1.5854,
      "step": 2945
    },
    {
      "epoch": 1.140534262485482,
      "grad_norm": 14.37851333618164,
      "learning_rate": 9.843850819460577e-06,
      "loss": 1.1365,
      "step": 2946
    },
    {
      "epoch": 1.140921409214092,
      "grad_norm": 14.047250747680664,
      "learning_rate": 9.843420656428787e-06,
      "loss": 1.2828,
      "step": 2947
    },
    {
      "epoch": 1.1413085559427023,
      "grad_norm": 14.361780166625977,
      "learning_rate": 9.842990493396998e-06,
      "loss": 1.3068,
      "step": 2948
    },
    {
      "epoch": 1.1416957026713124,
      "grad_norm": 32.172325134277344,
      "learning_rate": 9.84256033036521e-06,
      "loss": 1.2087,
      "step": 2949
    },
    {
      "epoch": 1.1420828493999227,
      "grad_norm": 19.969881057739258,
      "learning_rate": 9.842130167333421e-06,
      "loss": 1.5188,
      "step": 2950
    },
    {
      "epoch": 1.1424699961285327,
      "grad_norm": 16.035797119140625,
      "learning_rate": 9.84170000430163e-06,
      "loss": 1.5234,
      "step": 2951
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 15.759203910827637,
      "learning_rate": 9.841269841269842e-06,
      "loss": 2.3967,
      "step": 2952
    },
    {
      "epoch": 1.143244289585753,
      "grad_norm": 12.339290618896484,
      "learning_rate": 9.840839678238054e-06,
      "loss": 1.2222,
      "step": 2953
    },
    {
      "epoch": 1.1436314363143631,
      "grad_norm": 18.471786499023438,
      "learning_rate": 9.840409515206263e-06,
      "loss": 1.1467,
      "step": 2954
    },
    {
      "epoch": 1.1440185830429732,
      "grad_norm": 32.72879409790039,
      "learning_rate": 9.839979352174475e-06,
      "loss": 1.8418,
      "step": 2955
    },
    {
      "epoch": 1.1444057297715835,
      "grad_norm": 16.96495246887207,
      "learning_rate": 9.839549189142686e-06,
      "loss": 0.9659,
      "step": 2956
    },
    {
      "epoch": 1.1447928765001936,
      "grad_norm": 13.96474838256836,
      "learning_rate": 9.839119026110898e-06,
      "loss": 1.2243,
      "step": 2957
    },
    {
      "epoch": 1.1451800232288036,
      "grad_norm": 14.887704849243164,
      "learning_rate": 9.838688863079107e-06,
      "loss": 1.5392,
      "step": 2958
    },
    {
      "epoch": 1.145567169957414,
      "grad_norm": 17.775535583496094,
      "learning_rate": 9.838258700047319e-06,
      "loss": 1.7224,
      "step": 2959
    },
    {
      "epoch": 1.145954316686024,
      "grad_norm": 9.922630310058594,
      "learning_rate": 9.83782853701553e-06,
      "loss": 0.4296,
      "step": 2960
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 22.245471954345703,
      "learning_rate": 9.837398373983741e-06,
      "loss": 1.1567,
      "step": 2961
    },
    {
      "epoch": 1.1467286101432443,
      "grad_norm": 17.460216522216797,
      "learning_rate": 9.836968210951951e-06,
      "loss": 1.5537,
      "step": 2962
    },
    {
      "epoch": 1.1471157568718544,
      "grad_norm": 13.60256290435791,
      "learning_rate": 9.836538047920163e-06,
      "loss": 0.9501,
      "step": 2963
    },
    {
      "epoch": 1.1475029036004645,
      "grad_norm": 11.705642700195312,
      "learning_rate": 9.836107884888374e-06,
      "loss": 1.4102,
      "step": 2964
    },
    {
      "epoch": 1.1478900503290748,
      "grad_norm": 24.933467864990234,
      "learning_rate": 9.835677721856585e-06,
      "loss": 1.2985,
      "step": 2965
    },
    {
      "epoch": 1.1482771970576848,
      "grad_norm": 9.95095157623291,
      "learning_rate": 9.835247558824795e-06,
      "loss": 0.7327,
      "step": 2966
    },
    {
      "epoch": 1.1486643437862951,
      "grad_norm": 20.407468795776367,
      "learning_rate": 9.834817395793007e-06,
      "loss": 2.2685,
      "step": 2967
    },
    {
      "epoch": 1.1490514905149052,
      "grad_norm": 11.88222885131836,
      "learning_rate": 9.834387232761218e-06,
      "loss": 1.055,
      "step": 2968
    },
    {
      "epoch": 1.1494386372435152,
      "grad_norm": 18.896774291992188,
      "learning_rate": 9.833957069729428e-06,
      "loss": 1.6306,
      "step": 2969
    },
    {
      "epoch": 1.1498257839721253,
      "grad_norm": 17.754701614379883,
      "learning_rate": 9.833526906697639e-06,
      "loss": 1.5751,
      "step": 2970
    },
    {
      "epoch": 1.1502129307007356,
      "grad_norm": 20.714019775390625,
      "learning_rate": 9.83309674366585e-06,
      "loss": 1.8457,
      "step": 2971
    },
    {
      "epoch": 1.1506000774293457,
      "grad_norm": 31.65999984741211,
      "learning_rate": 9.832666580634062e-06,
      "loss": 2.1101,
      "step": 2972
    },
    {
      "epoch": 1.150987224157956,
      "grad_norm": 35.955039978027344,
      "learning_rate": 9.832236417602272e-06,
      "loss": 1.5473,
      "step": 2973
    },
    {
      "epoch": 1.151374370886566,
      "grad_norm": 7.081954002380371,
      "learning_rate": 9.831806254570483e-06,
      "loss": 1.4192,
      "step": 2974
    },
    {
      "epoch": 1.151761517615176,
      "grad_norm": 17.49835205078125,
      "learning_rate": 9.831376091538693e-06,
      "loss": 1.2485,
      "step": 2975
    },
    {
      "epoch": 1.1521486643437864,
      "grad_norm": 28.85236930847168,
      "learning_rate": 9.830945928506906e-06,
      "loss": 1.2437,
      "step": 2976
    },
    {
      "epoch": 1.1525358110723964,
      "grad_norm": 20.845680236816406,
      "learning_rate": 9.830515765475116e-06,
      "loss": 2.4573,
      "step": 2977
    },
    {
      "epoch": 1.1529229578010065,
      "grad_norm": 18.903173446655273,
      "learning_rate": 9.830085602443327e-06,
      "loss": 2.115,
      "step": 2978
    },
    {
      "epoch": 1.1533101045296168,
      "grad_norm": 20.152050018310547,
      "learning_rate": 9.829655439411537e-06,
      "loss": 1.6075,
      "step": 2979
    },
    {
      "epoch": 1.1536972512582269,
      "grad_norm": 15.459424018859863,
      "learning_rate": 9.82922527637975e-06,
      "loss": 1.7338,
      "step": 2980
    },
    {
      "epoch": 1.154084397986837,
      "grad_norm": 25.91725730895996,
      "learning_rate": 9.82879511334796e-06,
      "loss": 2.3755,
      "step": 2981
    },
    {
      "epoch": 1.1544715447154472,
      "grad_norm": 26.847471237182617,
      "learning_rate": 9.828364950316171e-06,
      "loss": 1.3795,
      "step": 2982
    },
    {
      "epoch": 1.1548586914440573,
      "grad_norm": 16.094446182250977,
      "learning_rate": 9.82793478728438e-06,
      "loss": 0.9474,
      "step": 2983
    },
    {
      "epoch": 1.1552458381726673,
      "grad_norm": 15.619918823242188,
      "learning_rate": 9.827504624252592e-06,
      "loss": 1.5938,
      "step": 2984
    },
    {
      "epoch": 1.1556329849012776,
      "grad_norm": 41.57929611206055,
      "learning_rate": 9.827074461220804e-06,
      "loss": 1.8007,
      "step": 2985
    },
    {
      "epoch": 1.1560201316298877,
      "grad_norm": 36.08284378051758,
      "learning_rate": 9.826644298189015e-06,
      "loss": 2.4414,
      "step": 2986
    },
    {
      "epoch": 1.1564072783584978,
      "grad_norm": 21.90114402770996,
      "learning_rate": 9.826214135157225e-06,
      "loss": 1.5616,
      "step": 2987
    },
    {
      "epoch": 1.156794425087108,
      "grad_norm": 15.733972549438477,
      "learning_rate": 9.825783972125436e-06,
      "loss": 1.4788,
      "step": 2988
    },
    {
      "epoch": 1.1571815718157181,
      "grad_norm": 9.461965560913086,
      "learning_rate": 9.825353809093648e-06,
      "loss": 1.4567,
      "step": 2989
    },
    {
      "epoch": 1.1575687185443284,
      "grad_norm": 19.564434051513672,
      "learning_rate": 9.824923646061857e-06,
      "loss": 1.5664,
      "step": 2990
    },
    {
      "epoch": 1.1579558652729385,
      "grad_norm": 26.272539138793945,
      "learning_rate": 9.824493483030069e-06,
      "loss": 2.2861,
      "step": 2991
    },
    {
      "epoch": 1.1583430120015485,
      "grad_norm": 11.656249046325684,
      "learning_rate": 9.82406331999828e-06,
      "loss": 0.8916,
      "step": 2992
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 22.562274932861328,
      "learning_rate": 9.823633156966492e-06,
      "loss": 2.3353,
      "step": 2993
    },
    {
      "epoch": 1.159117305458769,
      "grad_norm": 13.80146598815918,
      "learning_rate": 9.823202993934701e-06,
      "loss": 0.9675,
      "step": 2994
    },
    {
      "epoch": 1.159504452187379,
      "grad_norm": 14.673208236694336,
      "learning_rate": 9.822772830902914e-06,
      "loss": 1.2516,
      "step": 2995
    },
    {
      "epoch": 1.1598915989159893,
      "grad_norm": 16.515405654907227,
      "learning_rate": 9.822342667871124e-06,
      "loss": 1.4274,
      "step": 2996
    },
    {
      "epoch": 1.1602787456445993,
      "grad_norm": 17.443923950195312,
      "learning_rate": 9.821912504839336e-06,
      "loss": 1.6368,
      "step": 2997
    },
    {
      "epoch": 1.1606658923732094,
      "grad_norm": 17.66025733947754,
      "learning_rate": 9.821482341807545e-06,
      "loss": 1.5627,
      "step": 2998
    },
    {
      "epoch": 1.1610530391018197,
      "grad_norm": 18.860763549804688,
      "learning_rate": 9.821052178775757e-06,
      "loss": 1.4924,
      "step": 2999
    },
    {
      "epoch": 1.1614401858304297,
      "grad_norm": 13.486059188842773,
      "learning_rate": 9.820622015743968e-06,
      "loss": 1.456,
      "step": 3000
    },
    {
      "epoch": 1.1618273325590398,
      "grad_norm": 22.92889404296875,
      "learning_rate": 9.82019185271218e-06,
      "loss": 1.445,
      "step": 3001
    },
    {
      "epoch": 1.16221447928765,
      "grad_norm": 16.136220932006836,
      "learning_rate": 9.81976168968039e-06,
      "loss": 1.5168,
      "step": 3002
    },
    {
      "epoch": 1.1626016260162602,
      "grad_norm": 20.222835540771484,
      "learning_rate": 9.8193315266486e-06,
      "loss": 2.1027,
      "step": 3003
    },
    {
      "epoch": 1.1629887727448702,
      "grad_norm": 15.182382583618164,
      "learning_rate": 9.818901363616812e-06,
      "loss": 1.3647,
      "step": 3004
    },
    {
      "epoch": 1.1633759194734805,
      "grad_norm": 11.764989852905273,
      "learning_rate": 9.818471200585022e-06,
      "loss": 0.8838,
      "step": 3005
    },
    {
      "epoch": 1.1637630662020906,
      "grad_norm": 13.789085388183594,
      "learning_rate": 9.818041037553233e-06,
      "loss": 1.0464,
      "step": 3006
    },
    {
      "epoch": 1.1641502129307006,
      "grad_norm": 12.53756046295166,
      "learning_rate": 9.817610874521445e-06,
      "loss": 1.1329,
      "step": 3007
    },
    {
      "epoch": 1.164537359659311,
      "grad_norm": 15.539376258850098,
      "learning_rate": 9.817180711489656e-06,
      "loss": 1.7286,
      "step": 3008
    },
    {
      "epoch": 1.164924506387921,
      "grad_norm": 17.020790100097656,
      "learning_rate": 9.816750548457866e-06,
      "loss": 1.7717,
      "step": 3009
    },
    {
      "epoch": 1.165311653116531,
      "grad_norm": 12.021709442138672,
      "learning_rate": 9.816320385426077e-06,
      "loss": 1.5294,
      "step": 3010
    },
    {
      "epoch": 1.1656987998451414,
      "grad_norm": 16.95293426513672,
      "learning_rate": 9.815890222394289e-06,
      "loss": 1.2216,
      "step": 3011
    },
    {
      "epoch": 1.1660859465737514,
      "grad_norm": 26.857521057128906,
      "learning_rate": 9.8154600593625e-06,
      "loss": 1.9899,
      "step": 3012
    },
    {
      "epoch": 1.1664730933023617,
      "grad_norm": 21.654739379882812,
      "learning_rate": 9.81502989633071e-06,
      "loss": 1.736,
      "step": 3013
    },
    {
      "epoch": 1.1668602400309718,
      "grad_norm": 19.27728271484375,
      "learning_rate": 9.814599733298921e-06,
      "loss": 1.5669,
      "step": 3014
    },
    {
      "epoch": 1.1672473867595818,
      "grad_norm": 21.01061248779297,
      "learning_rate": 9.814169570267133e-06,
      "loss": 1.8742,
      "step": 3015
    },
    {
      "epoch": 1.167634533488192,
      "grad_norm": 18.747554779052734,
      "learning_rate": 9.813739407235344e-06,
      "loss": 1.6276,
      "step": 3016
    },
    {
      "epoch": 1.1680216802168022,
      "grad_norm": 16.065994262695312,
      "learning_rate": 9.813309244203554e-06,
      "loss": 1.3208,
      "step": 3017
    },
    {
      "epoch": 1.1684088269454123,
      "grad_norm": 18.052812576293945,
      "learning_rate": 9.812879081171765e-06,
      "loss": 1.412,
      "step": 3018
    },
    {
      "epoch": 1.1687959736740225,
      "grad_norm": 20.589031219482422,
      "learning_rate": 9.812448918139976e-06,
      "loss": 2.1381,
      "step": 3019
    },
    {
      "epoch": 1.1691831204026326,
      "grad_norm": 18.82927703857422,
      "learning_rate": 9.812018755108186e-06,
      "loss": 2.015,
      "step": 3020
    },
    {
      "epoch": 1.1695702671312427,
      "grad_norm": 14.648150444030762,
      "learning_rate": 9.811588592076398e-06,
      "loss": 1.5797,
      "step": 3021
    },
    {
      "epoch": 1.169957413859853,
      "grad_norm": 16.551654815673828,
      "learning_rate": 9.811158429044609e-06,
      "loss": 1.4224,
      "step": 3022
    },
    {
      "epoch": 1.170344560588463,
      "grad_norm": 26.70111083984375,
      "learning_rate": 9.81072826601282e-06,
      "loss": 1.5828,
      "step": 3023
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 17.38921356201172,
      "learning_rate": 9.81029810298103e-06,
      "loss": 1.48,
      "step": 3024
    },
    {
      "epoch": 1.1711188540456834,
      "grad_norm": 21.90741539001465,
      "learning_rate": 9.809867939949242e-06,
      "loss": 1.6149,
      "step": 3025
    },
    {
      "epoch": 1.1715060007742935,
      "grad_norm": 16.900083541870117,
      "learning_rate": 9.809437776917451e-06,
      "loss": 1.2615,
      "step": 3026
    },
    {
      "epoch": 1.1718931475029035,
      "grad_norm": 17.020925521850586,
      "learning_rate": 9.809007613885664e-06,
      "loss": 2.5166,
      "step": 3027
    },
    {
      "epoch": 1.1722802942315138,
      "grad_norm": 12.226788520812988,
      "learning_rate": 9.808577450853874e-06,
      "loss": 0.4676,
      "step": 3028
    },
    {
      "epoch": 1.1726674409601239,
      "grad_norm": 17.987957000732422,
      "learning_rate": 9.808147287822086e-06,
      "loss": 1.5398,
      "step": 3029
    },
    {
      "epoch": 1.173054587688734,
      "grad_norm": 17.478696823120117,
      "learning_rate": 9.807717124790295e-06,
      "loss": 1.6298,
      "step": 3030
    },
    {
      "epoch": 1.1734417344173442,
      "grad_norm": 14.859460830688477,
      "learning_rate": 9.807286961758508e-06,
      "loss": 1.4204,
      "step": 3031
    },
    {
      "epoch": 1.1738288811459543,
      "grad_norm": 18.234813690185547,
      "learning_rate": 9.806856798726718e-06,
      "loss": 1.5239,
      "step": 3032
    },
    {
      "epoch": 1.1742160278745644,
      "grad_norm": 15.822694778442383,
      "learning_rate": 9.80642663569493e-06,
      "loss": 1.4184,
      "step": 3033
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 15.34094524383545,
      "learning_rate": 9.80599647266314e-06,
      "loss": 1.4459,
      "step": 3034
    },
    {
      "epoch": 1.1749903213317847,
      "grad_norm": 28.835205078125,
      "learning_rate": 9.80556630963135e-06,
      "loss": 2.0387,
      "step": 3035
    },
    {
      "epoch": 1.175377468060395,
      "grad_norm": 25.378496170043945,
      "learning_rate": 9.805136146599562e-06,
      "loss": 1.5501,
      "step": 3036
    },
    {
      "epoch": 1.175764614789005,
      "grad_norm": 15.982877731323242,
      "learning_rate": 9.804705983567774e-06,
      "loss": 0.8811,
      "step": 3037
    },
    {
      "epoch": 1.1761517615176151,
      "grad_norm": 15.605831146240234,
      "learning_rate": 9.804275820535985e-06,
      "loss": 1.3322,
      "step": 3038
    },
    {
      "epoch": 1.1765389082462252,
      "grad_norm": 18.751754760742188,
      "learning_rate": 9.803845657504195e-06,
      "loss": 1.6367,
      "step": 3039
    },
    {
      "epoch": 1.1769260549748355,
      "grad_norm": 22.39113998413086,
      "learning_rate": 9.803415494472406e-06,
      "loss": 1.3702,
      "step": 3040
    },
    {
      "epoch": 1.1773132017034456,
      "grad_norm": 13.966052055358887,
      "learning_rate": 9.802985331440616e-06,
      "loss": 1.3971,
      "step": 3041
    },
    {
      "epoch": 1.1777003484320558,
      "grad_norm": 15.1185302734375,
      "learning_rate": 9.802555168408829e-06,
      "loss": 1.6062,
      "step": 3042
    },
    {
      "epoch": 1.178087495160666,
      "grad_norm": 15.840719223022461,
      "learning_rate": 9.802125005377039e-06,
      "loss": 1.2415,
      "step": 3043
    },
    {
      "epoch": 1.178474641889276,
      "grad_norm": 19.78141975402832,
      "learning_rate": 9.80169484234525e-06,
      "loss": 1.5468,
      "step": 3044
    },
    {
      "epoch": 1.1788617886178863,
      "grad_norm": 20.989940643310547,
      "learning_rate": 9.80126467931346e-06,
      "loss": 1.15,
      "step": 3045
    },
    {
      "epoch": 1.1792489353464963,
      "grad_norm": 10.915839195251465,
      "learning_rate": 9.800834516281673e-06,
      "loss": 1.0367,
      "step": 3046
    },
    {
      "epoch": 1.1796360820751064,
      "grad_norm": 17.73381805419922,
      "learning_rate": 9.800404353249883e-06,
      "loss": 1.0686,
      "step": 3047
    },
    {
      "epoch": 1.1800232288037167,
      "grad_norm": 15.845843315124512,
      "learning_rate": 9.799974190218094e-06,
      "loss": 1.0286,
      "step": 3048
    },
    {
      "epoch": 1.1804103755323267,
      "grad_norm": 17.48198890686035,
      "learning_rate": 9.799544027186304e-06,
      "loss": 1.5204,
      "step": 3049
    },
    {
      "epoch": 1.1807975222609368,
      "grad_norm": 15.380476951599121,
      "learning_rate": 9.799113864154515e-06,
      "loss": 1.6431,
      "step": 3050
    },
    {
      "epoch": 1.181184668989547,
      "grad_norm": 16.611730575561523,
      "learning_rate": 9.798683701122727e-06,
      "loss": 1.0813,
      "step": 3051
    },
    {
      "epoch": 1.1815718157181572,
      "grad_norm": 21.591283798217773,
      "learning_rate": 9.798253538090938e-06,
      "loss": 1.7625,
      "step": 3052
    },
    {
      "epoch": 1.1819589624467672,
      "grad_norm": 8.745270729064941,
      "learning_rate": 9.797823375059148e-06,
      "loss": 0.9132,
      "step": 3053
    },
    {
      "epoch": 1.1823461091753775,
      "grad_norm": 22.69384765625,
      "learning_rate": 9.797393212027359e-06,
      "loss": 1.6537,
      "step": 3054
    },
    {
      "epoch": 1.1827332559039876,
      "grad_norm": 13.326053619384766,
      "learning_rate": 9.79696304899557e-06,
      "loss": 1.1552,
      "step": 3055
    },
    {
      "epoch": 1.1831204026325977,
      "grad_norm": 15.162103652954102,
      "learning_rate": 9.79653288596378e-06,
      "loss": 1.4753,
      "step": 3056
    },
    {
      "epoch": 1.183507549361208,
      "grad_norm": 14.489585876464844,
      "learning_rate": 9.796102722931992e-06,
      "loss": 1.3867,
      "step": 3057
    },
    {
      "epoch": 1.183894696089818,
      "grad_norm": 17.16240692138672,
      "learning_rate": 9.795672559900203e-06,
      "loss": 1.2336,
      "step": 3058
    },
    {
      "epoch": 1.1842818428184283,
      "grad_norm": 11.332317352294922,
      "learning_rate": 9.795242396868414e-06,
      "loss": 1.2895,
      "step": 3059
    },
    {
      "epoch": 1.1846689895470384,
      "grad_norm": 20.84572982788086,
      "learning_rate": 9.794812233836624e-06,
      "loss": 1.4105,
      "step": 3060
    },
    {
      "epoch": 1.1850561362756484,
      "grad_norm": 22.489238739013672,
      "learning_rate": 9.794382070804836e-06,
      "loss": 1.6686,
      "step": 3061
    },
    {
      "epoch": 1.1854432830042585,
      "grad_norm": 14.409152030944824,
      "learning_rate": 9.793951907773047e-06,
      "loss": 1.3506,
      "step": 3062
    },
    {
      "epoch": 1.1858304297328688,
      "grad_norm": 42.8580322265625,
      "learning_rate": 9.793521744741258e-06,
      "loss": 2.4383,
      "step": 3063
    },
    {
      "epoch": 1.1862175764614789,
      "grad_norm": 18.06700325012207,
      "learning_rate": 9.793091581709468e-06,
      "loss": 1.5512,
      "step": 3064
    },
    {
      "epoch": 1.1866047231900891,
      "grad_norm": 28.48997688293457,
      "learning_rate": 9.79266141867768e-06,
      "loss": 2.8746,
      "step": 3065
    },
    {
      "epoch": 1.1869918699186992,
      "grad_norm": 18.089374542236328,
      "learning_rate": 9.792231255645891e-06,
      "loss": 2.0267,
      "step": 3066
    },
    {
      "epoch": 1.1873790166473093,
      "grad_norm": 14.755876541137695,
      "learning_rate": 9.791801092614102e-06,
      "loss": 2.8311,
      "step": 3067
    },
    {
      "epoch": 1.1877661633759196,
      "grad_norm": 29.355398178100586,
      "learning_rate": 9.791370929582312e-06,
      "loss": 1.5332,
      "step": 3068
    },
    {
      "epoch": 1.1881533101045296,
      "grad_norm": 19.715835571289062,
      "learning_rate": 9.790940766550524e-06,
      "loss": 1.6341,
      "step": 3069
    },
    {
      "epoch": 1.1885404568331397,
      "grad_norm": 15.60018539428711,
      "learning_rate": 9.790510603518735e-06,
      "loss": 1.0439,
      "step": 3070
    },
    {
      "epoch": 1.18892760356175,
      "grad_norm": 17.047536849975586,
      "learning_rate": 9.790080440486945e-06,
      "loss": 1.4192,
      "step": 3071
    },
    {
      "epoch": 1.18931475029036,
      "grad_norm": 15.645800590515137,
      "learning_rate": 9.789650277455156e-06,
      "loss": 1.1543,
      "step": 3072
    },
    {
      "epoch": 1.1897018970189701,
      "grad_norm": 9.197578430175781,
      "learning_rate": 9.789220114423368e-06,
      "loss": 0.6776,
      "step": 3073
    },
    {
      "epoch": 1.1900890437475804,
      "grad_norm": 19.637731552124023,
      "learning_rate": 9.788789951391579e-06,
      "loss": 1.9816,
      "step": 3074
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 10.762657165527344,
      "learning_rate": 9.788359788359789e-06,
      "loss": 0.7388,
      "step": 3075
    },
    {
      "epoch": 1.1908633372048005,
      "grad_norm": 14.620525360107422,
      "learning_rate": 9.787929625328e-06,
      "loss": 1.5044,
      "step": 3076
    },
    {
      "epoch": 1.1912504839334108,
      "grad_norm": 17.763824462890625,
      "learning_rate": 9.787499462296211e-06,
      "loss": 1.5536,
      "step": 3077
    },
    {
      "epoch": 1.1916376306620209,
      "grad_norm": 23.60650062561035,
      "learning_rate": 9.787069299264423e-06,
      "loss": 1.9365,
      "step": 3078
    },
    {
      "epoch": 1.192024777390631,
      "grad_norm": 15.560032844543457,
      "learning_rate": 9.786639136232633e-06,
      "loss": 1.2124,
      "step": 3079
    },
    {
      "epoch": 1.1924119241192412,
      "grad_norm": 25.57758331298828,
      "learning_rate": 9.786208973200844e-06,
      "loss": 2.3982,
      "step": 3080
    },
    {
      "epoch": 1.1927990708478513,
      "grad_norm": 21.021339416503906,
      "learning_rate": 9.785778810169055e-06,
      "loss": 3.1495,
      "step": 3081
    },
    {
      "epoch": 1.1931862175764616,
      "grad_norm": 30.362653732299805,
      "learning_rate": 9.785348647137267e-06,
      "loss": 1.8394,
      "step": 3082
    },
    {
      "epoch": 1.1935733643050717,
      "grad_norm": 18.890501022338867,
      "learning_rate": 9.784918484105477e-06,
      "loss": 1.9142,
      "step": 3083
    },
    {
      "epoch": 1.1939605110336817,
      "grad_norm": 14.699313163757324,
      "learning_rate": 9.784488321073688e-06,
      "loss": 1.1503,
      "step": 3084
    },
    {
      "epoch": 1.1943476577622918,
      "grad_norm": 22.52219009399414,
      "learning_rate": 9.7840581580419e-06,
      "loss": 1.2827,
      "step": 3085
    },
    {
      "epoch": 1.194734804490902,
      "grad_norm": 13.142180442810059,
      "learning_rate": 9.78362799501011e-06,
      "loss": 1.1669,
      "step": 3086
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 15.19536018371582,
      "learning_rate": 9.78319783197832e-06,
      "loss": 1.293,
      "step": 3087
    },
    {
      "epoch": 1.1955090979481224,
      "grad_norm": 13.536229133605957,
      "learning_rate": 9.782767668946532e-06,
      "loss": 1.1526,
      "step": 3088
    },
    {
      "epoch": 1.1958962446767325,
      "grad_norm": 26.121841430664062,
      "learning_rate": 9.782337505914743e-06,
      "loss": 1.1007,
      "step": 3089
    },
    {
      "epoch": 1.1962833914053426,
      "grad_norm": 14.07019329071045,
      "learning_rate": 9.781907342882953e-06,
      "loss": 1.7054,
      "step": 3090
    },
    {
      "epoch": 1.1966705381339529,
      "grad_norm": 15.080976486206055,
      "learning_rate": 9.781477179851165e-06,
      "loss": 1.3445,
      "step": 3091
    },
    {
      "epoch": 1.197057684862563,
      "grad_norm": 16.302383422851562,
      "learning_rate": 9.781047016819374e-06,
      "loss": 1.5789,
      "step": 3092
    },
    {
      "epoch": 1.197444831591173,
      "grad_norm": 20.442184448242188,
      "learning_rate": 9.780616853787587e-06,
      "loss": 1.6982,
      "step": 3093
    },
    {
      "epoch": 1.1978319783197833,
      "grad_norm": 25.04652976989746,
      "learning_rate": 9.780186690755797e-06,
      "loss": 1.4145,
      "step": 3094
    },
    {
      "epoch": 1.1982191250483933,
      "grad_norm": 13.898717880249023,
      "learning_rate": 9.779756527724009e-06,
      "loss": 0.8982,
      "step": 3095
    },
    {
      "epoch": 1.1986062717770034,
      "grad_norm": 16.84522247314453,
      "learning_rate": 9.779326364692218e-06,
      "loss": 1.8327,
      "step": 3096
    },
    {
      "epoch": 1.1989934185056137,
      "grad_norm": 25.888500213623047,
      "learning_rate": 9.778896201660431e-06,
      "loss": 1.4141,
      "step": 3097
    },
    {
      "epoch": 1.1993805652342238,
      "grad_norm": 16.591463088989258,
      "learning_rate": 9.778466038628641e-06,
      "loss": 1.1514,
      "step": 3098
    },
    {
      "epoch": 1.1997677119628338,
      "grad_norm": 13.106358528137207,
      "learning_rate": 9.778035875596852e-06,
      "loss": 1.1569,
      "step": 3099
    },
    {
      "epoch": 1.2001548586914441,
      "grad_norm": 13.718827247619629,
      "learning_rate": 9.777605712565062e-06,
      "loss": 1.3266,
      "step": 3100
    },
    {
      "epoch": 1.2005420054200542,
      "grad_norm": 15.039712905883789,
      "learning_rate": 9.777175549533274e-06,
      "loss": 1.1704,
      "step": 3101
    },
    {
      "epoch": 1.2009291521486642,
      "grad_norm": 16.75265121459961,
      "learning_rate": 9.776745386501485e-06,
      "loss": 1.131,
      "step": 3102
    },
    {
      "epoch": 1.2013162988772745,
      "grad_norm": 14.807233810424805,
      "learning_rate": 9.776315223469696e-06,
      "loss": 1.1885,
      "step": 3103
    },
    {
      "epoch": 1.2017034456058846,
      "grad_norm": 17.932580947875977,
      "learning_rate": 9.775885060437906e-06,
      "loss": 1.4412,
      "step": 3104
    },
    {
      "epoch": 1.202090592334495,
      "grad_norm": 13.495857238769531,
      "learning_rate": 9.775454897406118e-06,
      "loss": 1.5403,
      "step": 3105
    },
    {
      "epoch": 1.202477739063105,
      "grad_norm": 15.055131912231445,
      "learning_rate": 9.775024734374329e-06,
      "loss": 1.7669,
      "step": 3106
    },
    {
      "epoch": 1.202864885791715,
      "grad_norm": 17.160120010375977,
      "learning_rate": 9.774594571342539e-06,
      "loss": 2.3819,
      "step": 3107
    },
    {
      "epoch": 1.203252032520325,
      "grad_norm": 13.365689277648926,
      "learning_rate": 9.77416440831075e-06,
      "loss": 1.4016,
      "step": 3108
    },
    {
      "epoch": 1.2036391792489354,
      "grad_norm": 19.044315338134766,
      "learning_rate": 9.773734245278962e-06,
      "loss": 1.8014,
      "step": 3109
    },
    {
      "epoch": 1.2040263259775454,
      "grad_norm": 9.955920219421387,
      "learning_rate": 9.773304082247173e-06,
      "loss": 1.2552,
      "step": 3110
    },
    {
      "epoch": 1.2044134727061557,
      "grad_norm": 30.21571159362793,
      "learning_rate": 9.772873919215383e-06,
      "loss": 1.7888,
      "step": 3111
    },
    {
      "epoch": 1.2048006194347658,
      "grad_norm": 18.008140563964844,
      "learning_rate": 9.772443756183594e-06,
      "loss": 2.645,
      "step": 3112
    },
    {
      "epoch": 1.2051877661633759,
      "grad_norm": 20.486852645874023,
      "learning_rate": 9.772013593151806e-06,
      "loss": 1.7594,
      "step": 3113
    },
    {
      "epoch": 1.2055749128919862,
      "grad_norm": 64.62298583984375,
      "learning_rate": 9.771583430120017e-06,
      "loss": 1.0514,
      "step": 3114
    },
    {
      "epoch": 1.2059620596205962,
      "grad_norm": 13.12026596069336,
      "learning_rate": 9.771153267088227e-06,
      "loss": 0.8795,
      "step": 3115
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 14.375173568725586,
      "learning_rate": 9.770723104056438e-06,
      "loss": 1.394,
      "step": 3116
    },
    {
      "epoch": 1.2067363530778166,
      "grad_norm": 16.634183883666992,
      "learning_rate": 9.77029294102465e-06,
      "loss": 1.5412,
      "step": 3117
    },
    {
      "epoch": 1.2071234998064266,
      "grad_norm": 18.613420486450195,
      "learning_rate": 9.769862777992861e-06,
      "loss": 1.5349,
      "step": 3118
    },
    {
      "epoch": 1.2075106465350367,
      "grad_norm": 12.587372779846191,
      "learning_rate": 9.76943261496107e-06,
      "loss": 1.4336,
      "step": 3119
    },
    {
      "epoch": 1.207897793263647,
      "grad_norm": 20.389205932617188,
      "learning_rate": 9.769002451929282e-06,
      "loss": 1.4968,
      "step": 3120
    },
    {
      "epoch": 1.208284939992257,
      "grad_norm": 22.871414184570312,
      "learning_rate": 9.768572288897493e-06,
      "loss": 2.0417,
      "step": 3121
    },
    {
      "epoch": 1.2086720867208671,
      "grad_norm": 23.616647720336914,
      "learning_rate": 9.768142125865703e-06,
      "loss": 1.4864,
      "step": 3122
    },
    {
      "epoch": 1.2090592334494774,
      "grad_norm": 28.835098266601562,
      "learning_rate": 9.767711962833915e-06,
      "loss": 1.532,
      "step": 3123
    },
    {
      "epoch": 1.2094463801780875,
      "grad_norm": 15.325265884399414,
      "learning_rate": 9.767281799802126e-06,
      "loss": 1.4074,
      "step": 3124
    },
    {
      "epoch": 1.2098335269066975,
      "grad_norm": 19.85487174987793,
      "learning_rate": 9.766851636770337e-06,
      "loss": 2.9544,
      "step": 3125
    },
    {
      "epoch": 1.2102206736353078,
      "grad_norm": 17.612648010253906,
      "learning_rate": 9.766421473738547e-06,
      "loss": 1.2416,
      "step": 3126
    },
    {
      "epoch": 1.210607820363918,
      "grad_norm": 13.179670333862305,
      "learning_rate": 9.765991310706759e-06,
      "loss": 1.1451,
      "step": 3127
    },
    {
      "epoch": 1.2109949670925282,
      "grad_norm": 15.012333869934082,
      "learning_rate": 9.76556114767497e-06,
      "loss": 1.8342,
      "step": 3128
    },
    {
      "epoch": 1.2113821138211383,
      "grad_norm": 23.382232666015625,
      "learning_rate": 9.765130984643181e-06,
      "loss": 2.1935,
      "step": 3129
    },
    {
      "epoch": 1.2117692605497483,
      "grad_norm": 15.78349494934082,
      "learning_rate": 9.764700821611391e-06,
      "loss": 1.0928,
      "step": 3130
    },
    {
      "epoch": 1.2121564072783584,
      "grad_norm": 13.81524658203125,
      "learning_rate": 9.764270658579603e-06,
      "loss": 1.0702,
      "step": 3131
    },
    {
      "epoch": 1.2125435540069687,
      "grad_norm": 19.994224548339844,
      "learning_rate": 9.763840495547814e-06,
      "loss": 2.1858,
      "step": 3132
    },
    {
      "epoch": 1.2129307007355787,
      "grad_norm": 27.11276626586914,
      "learning_rate": 9.763410332516025e-06,
      "loss": 2.5132,
      "step": 3133
    },
    {
      "epoch": 1.213317847464189,
      "grad_norm": 13.535355567932129,
      "learning_rate": 9.762980169484235e-06,
      "loss": 1.5923,
      "step": 3134
    },
    {
      "epoch": 1.213704994192799,
      "grad_norm": 17.355627059936523,
      "learning_rate": 9.762550006452447e-06,
      "loss": 1.7406,
      "step": 3135
    },
    {
      "epoch": 1.2140921409214092,
      "grad_norm": 10.758639335632324,
      "learning_rate": 9.762119843420658e-06,
      "loss": 1.2799,
      "step": 3136
    },
    {
      "epoch": 1.2144792876500194,
      "grad_norm": 23.31247329711914,
      "learning_rate": 9.761689680388868e-06,
      "loss": 1.2103,
      "step": 3137
    },
    {
      "epoch": 1.2148664343786295,
      "grad_norm": 12.592398643493652,
      "learning_rate": 9.761259517357079e-06,
      "loss": 1.0454,
      "step": 3138
    },
    {
      "epoch": 1.2152535811072396,
      "grad_norm": 12.90467643737793,
      "learning_rate": 9.76082935432529e-06,
      "loss": 1.1648,
      "step": 3139
    },
    {
      "epoch": 1.2156407278358499,
      "grad_norm": 9.507275581359863,
      "learning_rate": 9.760399191293502e-06,
      "loss": 1.279,
      "step": 3140
    },
    {
      "epoch": 1.21602787456446,
      "grad_norm": 16.21649932861328,
      "learning_rate": 9.759969028261712e-06,
      "loss": 2.342,
      "step": 3141
    },
    {
      "epoch": 1.21641502129307,
      "grad_norm": 16.131473541259766,
      "learning_rate": 9.759538865229923e-06,
      "loss": 1.339,
      "step": 3142
    },
    {
      "epoch": 1.2168021680216803,
      "grad_norm": 21.813587188720703,
      "learning_rate": 9.759108702198133e-06,
      "loss": 1.7441,
      "step": 3143
    },
    {
      "epoch": 1.2171893147502904,
      "grad_norm": 10.322762489318848,
      "learning_rate": 9.758678539166346e-06,
      "loss": 0.4111,
      "step": 3144
    },
    {
      "epoch": 1.2175764614789004,
      "grad_norm": 13.516968727111816,
      "learning_rate": 9.758248376134556e-06,
      "loss": 0.9542,
      "step": 3145
    },
    {
      "epoch": 1.2179636082075107,
      "grad_norm": 13.72641658782959,
      "learning_rate": 9.757818213102767e-06,
      "loss": 1.1,
      "step": 3146
    },
    {
      "epoch": 1.2183507549361208,
      "grad_norm": 24.433515548706055,
      "learning_rate": 9.757388050070977e-06,
      "loss": 1.3665,
      "step": 3147
    },
    {
      "epoch": 1.2187379016647308,
      "grad_norm": 11.27368450164795,
      "learning_rate": 9.75695788703919e-06,
      "loss": 1.4145,
      "step": 3148
    },
    {
      "epoch": 1.2191250483933411,
      "grad_norm": 17.937541961669922,
      "learning_rate": 9.7565277240074e-06,
      "loss": 1.225,
      "step": 3149
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 22.43718147277832,
      "learning_rate": 9.756097560975611e-06,
      "loss": 1.7906,
      "step": 3150
    },
    {
      "epoch": 1.2198993418505615,
      "grad_norm": 17.17709732055664,
      "learning_rate": 9.75566739794382e-06,
      "loss": 1.3701,
      "step": 3151
    },
    {
      "epoch": 1.2202864885791715,
      "grad_norm": 22.362865447998047,
      "learning_rate": 9.755237234912032e-06,
      "loss": 1.0098,
      "step": 3152
    },
    {
      "epoch": 1.2206736353077816,
      "grad_norm": 33.693851470947266,
      "learning_rate": 9.754807071880244e-06,
      "loss": 1.5259,
      "step": 3153
    },
    {
      "epoch": 1.2210607820363917,
      "grad_norm": 10.855972290039062,
      "learning_rate": 9.754376908848455e-06,
      "loss": 1.2092,
      "step": 3154
    },
    {
      "epoch": 1.221447928765002,
      "grad_norm": 21.034648895263672,
      "learning_rate": 9.753946745816665e-06,
      "loss": 1.6799,
      "step": 3155
    },
    {
      "epoch": 1.221835075493612,
      "grad_norm": 14.020866394042969,
      "learning_rate": 9.753516582784876e-06,
      "loss": 1.2996,
      "step": 3156
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 32.28239440917969,
      "learning_rate": 9.753086419753087e-06,
      "loss": 2.352,
      "step": 3157
    },
    {
      "epoch": 1.2226093689508324,
      "grad_norm": 16.011947631835938,
      "learning_rate": 9.752656256721297e-06,
      "loss": 1.7998,
      "step": 3158
    },
    {
      "epoch": 1.2229965156794425,
      "grad_norm": 17.443132400512695,
      "learning_rate": 9.752226093689509e-06,
      "loss": 1.0641,
      "step": 3159
    },
    {
      "epoch": 1.2233836624080527,
      "grad_norm": 19.176340103149414,
      "learning_rate": 9.75179593065772e-06,
      "loss": 2.1969,
      "step": 3160
    },
    {
      "epoch": 1.2237708091366628,
      "grad_norm": 18.671064376831055,
      "learning_rate": 9.751365767625931e-06,
      "loss": 1.6104,
      "step": 3161
    },
    {
      "epoch": 1.2241579558652729,
      "grad_norm": 9.247101783752441,
      "learning_rate": 9.750935604594141e-06,
      "loss": 0.6934,
      "step": 3162
    },
    {
      "epoch": 1.2245451025938832,
      "grad_norm": 15.999277114868164,
      "learning_rate": 9.750505441562354e-06,
      "loss": 1.4947,
      "step": 3163
    },
    {
      "epoch": 1.2249322493224932,
      "grad_norm": 18.861522674560547,
      "learning_rate": 9.750075278530564e-06,
      "loss": 1.8541,
      "step": 3164
    },
    {
      "epoch": 1.2253193960511033,
      "grad_norm": 38.95787811279297,
      "learning_rate": 9.749645115498775e-06,
      "loss": 2.2634,
      "step": 3165
    },
    {
      "epoch": 1.2257065427797136,
      "grad_norm": 17.901823043823242,
      "learning_rate": 9.749214952466985e-06,
      "loss": 2.1485,
      "step": 3166
    },
    {
      "epoch": 1.2260936895083236,
      "grad_norm": 12.456291198730469,
      "learning_rate": 9.748784789435197e-06,
      "loss": 0.8904,
      "step": 3167
    },
    {
      "epoch": 1.2264808362369337,
      "grad_norm": 14.512628555297852,
      "learning_rate": 9.748354626403408e-06,
      "loss": 1.9656,
      "step": 3168
    },
    {
      "epoch": 1.226867982965544,
      "grad_norm": 12.287969589233398,
      "learning_rate": 9.74792446337162e-06,
      "loss": 1.0424,
      "step": 3169
    },
    {
      "epoch": 1.227255129694154,
      "grad_norm": 9.78237247467041,
      "learning_rate": 9.747494300339829e-06,
      "loss": 1.3905,
      "step": 3170
    },
    {
      "epoch": 1.2276422764227641,
      "grad_norm": 14.63348388671875,
      "learning_rate": 9.74706413730804e-06,
      "loss": 1.0032,
      "step": 3171
    },
    {
      "epoch": 1.2280294231513744,
      "grad_norm": 14.406023979187012,
      "learning_rate": 9.746633974276252e-06,
      "loss": 1.7232,
      "step": 3172
    },
    {
      "epoch": 1.2284165698799845,
      "grad_norm": 7.43907356262207,
      "learning_rate": 9.746203811244462e-06,
      "loss": 0.9673,
      "step": 3173
    },
    {
      "epoch": 1.2288037166085948,
      "grad_norm": 19.971786499023438,
      "learning_rate": 9.745773648212673e-06,
      "loss": 2.1597,
      "step": 3174
    },
    {
      "epoch": 1.2291908633372048,
      "grad_norm": 15.919750213623047,
      "learning_rate": 9.745343485180885e-06,
      "loss": 1.2564,
      "step": 3175
    },
    {
      "epoch": 1.229578010065815,
      "grad_norm": 11.826698303222656,
      "learning_rate": 9.744913322149096e-06,
      "loss": 1.054,
      "step": 3176
    },
    {
      "epoch": 1.229965156794425,
      "grad_norm": 20.63727378845215,
      "learning_rate": 9.744483159117306e-06,
      "loss": 1.687,
      "step": 3177
    },
    {
      "epoch": 1.2303523035230353,
      "grad_norm": 28.185518264770508,
      "learning_rate": 9.744052996085517e-06,
      "loss": 1.2939,
      "step": 3178
    },
    {
      "epoch": 1.2307394502516453,
      "grad_norm": 18.30876350402832,
      "learning_rate": 9.743622833053728e-06,
      "loss": 1.7053,
      "step": 3179
    },
    {
      "epoch": 1.2311265969802556,
      "grad_norm": 33.37335205078125,
      "learning_rate": 9.74319267002194e-06,
      "loss": 1.5716,
      "step": 3180
    },
    {
      "epoch": 1.2315137437088657,
      "grad_norm": 19.982162475585938,
      "learning_rate": 9.74276250699015e-06,
      "loss": 1.6535,
      "step": 3181
    },
    {
      "epoch": 1.2319008904374757,
      "grad_norm": 8.646696090698242,
      "learning_rate": 9.742332343958361e-06,
      "loss": 0.6393,
      "step": 3182
    },
    {
      "epoch": 1.232288037166086,
      "grad_norm": 21.951969146728516,
      "learning_rate": 9.741902180926572e-06,
      "loss": 3.5211,
      "step": 3183
    },
    {
      "epoch": 1.232675183894696,
      "grad_norm": 9.936111450195312,
      "learning_rate": 9.741472017894784e-06,
      "loss": 1.2912,
      "step": 3184
    },
    {
      "epoch": 1.2330623306233062,
      "grad_norm": 17.73468589782715,
      "learning_rate": 9.741041854862994e-06,
      "loss": 0.7507,
      "step": 3185
    },
    {
      "epoch": 1.2334494773519165,
      "grad_norm": 33.8300895690918,
      "learning_rate": 9.740611691831205e-06,
      "loss": 1.083,
      "step": 3186
    },
    {
      "epoch": 1.2338366240805265,
      "grad_norm": 33.93206787109375,
      "learning_rate": 9.740181528799416e-06,
      "loss": 2.5306,
      "step": 3187
    },
    {
      "epoch": 1.2342237708091366,
      "grad_norm": 37.65287780761719,
      "learning_rate": 9.739751365767626e-06,
      "loss": 1.921,
      "step": 3188
    },
    {
      "epoch": 1.2346109175377469,
      "grad_norm": 18.55474090576172,
      "learning_rate": 9.739321202735838e-06,
      "loss": 1.6562,
      "step": 3189
    },
    {
      "epoch": 1.234998064266357,
      "grad_norm": 15.199946403503418,
      "learning_rate": 9.738891039704049e-06,
      "loss": 1.636,
      "step": 3190
    },
    {
      "epoch": 1.235385210994967,
      "grad_norm": 9.118399620056152,
      "learning_rate": 9.73846087667226e-06,
      "loss": 1.0603,
      "step": 3191
    },
    {
      "epoch": 1.2357723577235773,
      "grad_norm": 15.418420791625977,
      "learning_rate": 9.73803071364047e-06,
      "loss": 1.4044,
      "step": 3192
    },
    {
      "epoch": 1.2361595044521874,
      "grad_norm": 22.149795532226562,
      "learning_rate": 9.737600550608682e-06,
      "loss": 1.6233,
      "step": 3193
    },
    {
      "epoch": 1.2365466511807974,
      "grad_norm": 9.965652465820312,
      "learning_rate": 9.737170387576891e-06,
      "loss": 0.8635,
      "step": 3194
    },
    {
      "epoch": 1.2369337979094077,
      "grad_norm": 12.179009437561035,
      "learning_rate": 9.736740224545104e-06,
      "loss": 1.4446,
      "step": 3195
    },
    {
      "epoch": 1.2373209446380178,
      "grad_norm": 29.49188232421875,
      "learning_rate": 9.736310061513314e-06,
      "loss": 3.0161,
      "step": 3196
    },
    {
      "epoch": 1.237708091366628,
      "grad_norm": 41.29104995727539,
      "learning_rate": 9.735879898481525e-06,
      "loss": 2.6921,
      "step": 3197
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 11.022710800170898,
      "learning_rate": 9.735449735449735e-06,
      "loss": 1.074,
      "step": 3198
    },
    {
      "epoch": 1.2384823848238482,
      "grad_norm": 13.800790786743164,
      "learning_rate": 9.735019572417948e-06,
      "loss": 0.9662,
      "step": 3199
    },
    {
      "epoch": 1.2388695315524583,
      "grad_norm": 37.71929931640625,
      "learning_rate": 9.734589409386158e-06,
      "loss": 1.7464,
      "step": 3200
    },
    {
      "epoch": 1.2392566782810686,
      "grad_norm": 16.926761627197266,
      "learning_rate": 9.73415924635437e-06,
      "loss": 1.3916,
      "step": 3201
    },
    {
      "epoch": 1.2396438250096786,
      "grad_norm": 15.729730606079102,
      "learning_rate": 9.733729083322581e-06,
      "loss": 1.6168,
      "step": 3202
    },
    {
      "epoch": 1.240030971738289,
      "grad_norm": 18.318832397460938,
      "learning_rate": 9.73329892029079e-06,
      "loss": 1.5338,
      "step": 3203
    },
    {
      "epoch": 1.240418118466899,
      "grad_norm": 31.654428482055664,
      "learning_rate": 9.732868757259002e-06,
      "loss": 1.7008,
      "step": 3204
    },
    {
      "epoch": 1.240805265195509,
      "grad_norm": 26.801971435546875,
      "learning_rate": 9.732438594227213e-06,
      "loss": 1.6171,
      "step": 3205
    },
    {
      "epoch": 1.2411924119241193,
      "grad_norm": 15.905384063720703,
      "learning_rate": 9.732008431195425e-06,
      "loss": 1.0967,
      "step": 3206
    },
    {
      "epoch": 1.2415795586527294,
      "grad_norm": 46.61048126220703,
      "learning_rate": 9.731578268163635e-06,
      "loss": 2.5442,
      "step": 3207
    },
    {
      "epoch": 1.2419667053813395,
      "grad_norm": 13.990106582641602,
      "learning_rate": 9.731148105131846e-06,
      "loss": 1.4651,
      "step": 3208
    },
    {
      "epoch": 1.2423538521099498,
      "grad_norm": 20.290258407592773,
      "learning_rate": 9.730717942100056e-06,
      "loss": 2.2636,
      "step": 3209
    },
    {
      "epoch": 1.2427409988385598,
      "grad_norm": 14.235318183898926,
      "learning_rate": 9.730287779068269e-06,
      "loss": 1.4295,
      "step": 3210
    },
    {
      "epoch": 1.2431281455671699,
      "grad_norm": 22.32913589477539,
      "learning_rate": 9.729857616036479e-06,
      "loss": 1.0644,
      "step": 3211
    },
    {
      "epoch": 1.2435152922957802,
      "grad_norm": 21.833669662475586,
      "learning_rate": 9.72942745300469e-06,
      "loss": 1.3755,
      "step": 3212
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 14.035148620605469,
      "learning_rate": 9.7289972899729e-06,
      "loss": 1.8859,
      "step": 3213
    },
    {
      "epoch": 1.2442895857530003,
      "grad_norm": 18.820079803466797,
      "learning_rate": 9.728567126941113e-06,
      "loss": 2.1499,
      "step": 3214
    },
    {
      "epoch": 1.2446767324816106,
      "grad_norm": 25.59492301940918,
      "learning_rate": 9.728136963909322e-06,
      "loss": 1.5084,
      "step": 3215
    },
    {
      "epoch": 1.2450638792102207,
      "grad_norm": 13.886759757995605,
      "learning_rate": 9.727706800877534e-06,
      "loss": 1.5441,
      "step": 3216
    },
    {
      "epoch": 1.2454510259388307,
      "grad_norm": 15.113125801086426,
      "learning_rate": 9.727276637845744e-06,
      "loss": 1.5593,
      "step": 3217
    },
    {
      "epoch": 1.245838172667441,
      "grad_norm": 12.288847923278809,
      "learning_rate": 9.726846474813955e-06,
      "loss": 0.9764,
      "step": 3218
    },
    {
      "epoch": 1.246225319396051,
      "grad_norm": 11.05102252960205,
      "learning_rate": 9.726416311782166e-06,
      "loss": 0.7221,
      "step": 3219
    },
    {
      "epoch": 1.2466124661246614,
      "grad_norm": 20.791175842285156,
      "learning_rate": 9.725986148750378e-06,
      "loss": 2.447,
      "step": 3220
    },
    {
      "epoch": 1.2469996128532714,
      "grad_norm": 11.105088233947754,
      "learning_rate": 9.725555985718588e-06,
      "loss": 1.4407,
      "step": 3221
    },
    {
      "epoch": 1.2473867595818815,
      "grad_norm": 24.097166061401367,
      "learning_rate": 9.725125822686799e-06,
      "loss": 1.8421,
      "step": 3222
    },
    {
      "epoch": 1.2477739063104916,
      "grad_norm": 9.503959655761719,
      "learning_rate": 9.72469565965501e-06,
      "loss": 0.7583,
      "step": 3223
    },
    {
      "epoch": 1.2481610530391019,
      "grad_norm": 20.85822296142578,
      "learning_rate": 9.72426549662322e-06,
      "loss": 1.6602,
      "step": 3224
    },
    {
      "epoch": 1.248548199767712,
      "grad_norm": 18.58648681640625,
      "learning_rate": 9.723835333591432e-06,
      "loss": 2.0328,
      "step": 3225
    },
    {
      "epoch": 1.2489353464963222,
      "grad_norm": 14.822598457336426,
      "learning_rate": 9.723405170559643e-06,
      "loss": 1.6914,
      "step": 3226
    },
    {
      "epoch": 1.2493224932249323,
      "grad_norm": 21.907865524291992,
      "learning_rate": 9.722975007527854e-06,
      "loss": 1.6368,
      "step": 3227
    },
    {
      "epoch": 1.2497096399535423,
      "grad_norm": 32.83900451660156,
      "learning_rate": 9.722544844496064e-06,
      "loss": 2.0329,
      "step": 3228
    },
    {
      "epoch": 1.2500967866821524,
      "grad_norm": 25.500627517700195,
      "learning_rate": 9.722114681464276e-06,
      "loss": 2.4779,
      "step": 3229
    },
    {
      "epoch": 1.2504839334107627,
      "grad_norm": 21.167142868041992,
      "learning_rate": 9.721684518432487e-06,
      "loss": 2.3112,
      "step": 3230
    },
    {
      "epoch": 1.2508710801393728,
      "grad_norm": 15.050511360168457,
      "learning_rate": 9.721254355400698e-06,
      "loss": 1.507,
      "step": 3231
    },
    {
      "epoch": 1.251258226867983,
      "grad_norm": 14.130298614501953,
      "learning_rate": 9.720824192368908e-06,
      "loss": 1.4606,
      "step": 3232
    },
    {
      "epoch": 1.2516453735965931,
      "grad_norm": 21.182647705078125,
      "learning_rate": 9.72039402933712e-06,
      "loss": 0.9019,
      "step": 3233
    },
    {
      "epoch": 1.2520325203252032,
      "grad_norm": 10.922479629516602,
      "learning_rate": 9.719963866305331e-06,
      "loss": 0.9021,
      "step": 3234
    },
    {
      "epoch": 1.2524196670538135,
      "grad_norm": 14.72940731048584,
      "learning_rate": 9.719533703273542e-06,
      "loss": 1.6678,
      "step": 3235
    },
    {
      "epoch": 1.2528068137824235,
      "grad_norm": 26.77144432067871,
      "learning_rate": 9.719103540241752e-06,
      "loss": 1.5726,
      "step": 3236
    },
    {
      "epoch": 1.2531939605110336,
      "grad_norm": 16.161155700683594,
      "learning_rate": 9.718673377209963e-06,
      "loss": 1.3943,
      "step": 3237
    },
    {
      "epoch": 1.2535811072396439,
      "grad_norm": 14.914190292358398,
      "learning_rate": 9.718243214178175e-06,
      "loss": 1.5508,
      "step": 3238
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 23.380462646484375,
      "learning_rate": 9.717813051146385e-06,
      "loss": 1.0377,
      "step": 3239
    },
    {
      "epoch": 1.254355400696864,
      "grad_norm": 19.437379837036133,
      "learning_rate": 9.717382888114596e-06,
      "loss": 1.7992,
      "step": 3240
    },
    {
      "epoch": 1.2547425474254743,
      "grad_norm": 18.056739807128906,
      "learning_rate": 9.716952725082807e-06,
      "loss": 1.9771,
      "step": 3241
    },
    {
      "epoch": 1.2551296941540844,
      "grad_norm": 21.15443229675293,
      "learning_rate": 9.716522562051019e-06,
      "loss": 1.6361,
      "step": 3242
    },
    {
      "epoch": 1.2555168408826947,
      "grad_norm": 24.650775909423828,
      "learning_rate": 9.716092399019229e-06,
      "loss": 1.871,
      "step": 3243
    },
    {
      "epoch": 1.2559039876113047,
      "grad_norm": 14.716514587402344,
      "learning_rate": 9.71566223598744e-06,
      "loss": 1.0469,
      "step": 3244
    },
    {
      "epoch": 1.2562911343399148,
      "grad_norm": 15.825467109680176,
      "learning_rate": 9.715232072955651e-06,
      "loss": 1.3215,
      "step": 3245
    },
    {
      "epoch": 1.2566782810685249,
      "grad_norm": 19.16295623779297,
      "learning_rate": 9.714801909923863e-06,
      "loss": 1.9752,
      "step": 3246
    },
    {
      "epoch": 1.2570654277971351,
      "grad_norm": 21.728979110717773,
      "learning_rate": 9.714371746892073e-06,
      "loss": 1.5984,
      "step": 3247
    },
    {
      "epoch": 1.2574525745257452,
      "grad_norm": 15.6837797164917,
      "learning_rate": 9.713941583860284e-06,
      "loss": 1.4993,
      "step": 3248
    },
    {
      "epoch": 1.2578397212543555,
      "grad_norm": 34.62144088745117,
      "learning_rate": 9.713511420828495e-06,
      "loss": 1.4777,
      "step": 3249
    },
    {
      "epoch": 1.2582268679829656,
      "grad_norm": 22.691104888916016,
      "learning_rate": 9.713081257796707e-06,
      "loss": 1.8855,
      "step": 3250
    },
    {
      "epoch": 1.2586140147115756,
      "grad_norm": 13.58126163482666,
      "learning_rate": 9.712651094764917e-06,
      "loss": 1.399,
      "step": 3251
    },
    {
      "epoch": 1.2590011614401857,
      "grad_norm": 9.981210708618164,
      "learning_rate": 9.712220931733128e-06,
      "loss": 0.7136,
      "step": 3252
    },
    {
      "epoch": 1.259388308168796,
      "grad_norm": 19.724308013916016,
      "learning_rate": 9.71179076870134e-06,
      "loss": 1.844,
      "step": 3253
    },
    {
      "epoch": 1.259775454897406,
      "grad_norm": 15.86931324005127,
      "learning_rate": 9.711360605669549e-06,
      "loss": 1.7368,
      "step": 3254
    },
    {
      "epoch": 1.2601626016260163,
      "grad_norm": 20.19759178161621,
      "learning_rate": 9.71093044263776e-06,
      "loss": 1.627,
      "step": 3255
    },
    {
      "epoch": 1.2605497483546264,
      "grad_norm": 16.1435546875,
      "learning_rate": 9.710500279605972e-06,
      "loss": 1.4883,
      "step": 3256
    },
    {
      "epoch": 1.2609368950832365,
      "grad_norm": 13.317451477050781,
      "learning_rate": 9.710070116574183e-06,
      "loss": 1.0772,
      "step": 3257
    },
    {
      "epoch": 1.2613240418118468,
      "grad_norm": 11.130797386169434,
      "learning_rate": 9.709639953542393e-06,
      "loss": 1.1782,
      "step": 3258
    },
    {
      "epoch": 1.2617111885404568,
      "grad_norm": 9.878373146057129,
      "learning_rate": 9.709209790510604e-06,
      "loss": 1.3747,
      "step": 3259
    },
    {
      "epoch": 1.262098335269067,
      "grad_norm": 25.332271575927734,
      "learning_rate": 9.708779627478814e-06,
      "loss": 2.3747,
      "step": 3260
    },
    {
      "epoch": 1.2624854819976772,
      "grad_norm": 12.971724510192871,
      "learning_rate": 9.708349464447027e-06,
      "loss": 1.1049,
      "step": 3261
    },
    {
      "epoch": 1.2628726287262872,
      "grad_norm": 13.382844924926758,
      "learning_rate": 9.707919301415237e-06,
      "loss": 0.8284,
      "step": 3262
    },
    {
      "epoch": 1.2632597754548973,
      "grad_norm": 21.925065994262695,
      "learning_rate": 9.707489138383448e-06,
      "loss": 1.5777,
      "step": 3263
    },
    {
      "epoch": 1.2636469221835076,
      "grad_norm": 12.820685386657715,
      "learning_rate": 9.707058975351658e-06,
      "loss": 1.0806,
      "step": 3264
    },
    {
      "epoch": 1.2640340689121177,
      "grad_norm": 15.547905921936035,
      "learning_rate": 9.706628812319871e-06,
      "loss": 1.8276,
      "step": 3265
    },
    {
      "epoch": 1.264421215640728,
      "grad_norm": 41.567779541015625,
      "learning_rate": 9.706198649288081e-06,
      "loss": 1.8534,
      "step": 3266
    },
    {
      "epoch": 1.264808362369338,
      "grad_norm": 23.72753143310547,
      "learning_rate": 9.705768486256292e-06,
      "loss": 1.5138,
      "step": 3267
    },
    {
      "epoch": 1.265195509097948,
      "grad_norm": 13.613243103027344,
      "learning_rate": 9.705338323224502e-06,
      "loss": 1.2042,
      "step": 3268
    },
    {
      "epoch": 1.2655826558265582,
      "grad_norm": 17.425737380981445,
      "learning_rate": 9.704908160192714e-06,
      "loss": 1.6399,
      "step": 3269
    },
    {
      "epoch": 1.2659698025551684,
      "grad_norm": 12.85645866394043,
      "learning_rate": 9.704477997160925e-06,
      "loss": 1.0971,
      "step": 3270
    },
    {
      "epoch": 1.2663569492837785,
      "grad_norm": 9.166070938110352,
      "learning_rate": 9.704047834129136e-06,
      "loss": 1.4291,
      "step": 3271
    },
    {
      "epoch": 1.2667440960123888,
      "grad_norm": 17.07532501220703,
      "learning_rate": 9.703617671097346e-06,
      "loss": 1.2797,
      "step": 3272
    },
    {
      "epoch": 1.2671312427409989,
      "grad_norm": 17.060678482055664,
      "learning_rate": 9.703187508065558e-06,
      "loss": 1.449,
      "step": 3273
    },
    {
      "epoch": 1.267518389469609,
      "grad_norm": 16.083446502685547,
      "learning_rate": 9.702757345033769e-06,
      "loss": 1.0208,
      "step": 3274
    },
    {
      "epoch": 1.267905536198219,
      "grad_norm": 14.649917602539062,
      "learning_rate": 9.702327182001979e-06,
      "loss": 1.0791,
      "step": 3275
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 13.60891342163086,
      "learning_rate": 9.70189701897019e-06,
      "loss": 1.0106,
      "step": 3276
    },
    {
      "epoch": 1.2686798296554394,
      "grad_norm": 15.313760757446289,
      "learning_rate": 9.701466855938401e-06,
      "loss": 0.794,
      "step": 3277
    },
    {
      "epoch": 1.2690669763840496,
      "grad_norm": 20.49737548828125,
      "learning_rate": 9.701036692906613e-06,
      "loss": 1.8289,
      "step": 3278
    },
    {
      "epoch": 1.2694541231126597,
      "grad_norm": 14.213780403137207,
      "learning_rate": 9.700606529874823e-06,
      "loss": 0.9897,
      "step": 3279
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 25.622934341430664,
      "learning_rate": 9.700176366843034e-06,
      "loss": 1.2772,
      "step": 3280
    },
    {
      "epoch": 1.27022841656988,
      "grad_norm": 20.725723266601562,
      "learning_rate": 9.699746203811245e-06,
      "loss": 2.1614,
      "step": 3281
    },
    {
      "epoch": 1.2706155632984901,
      "grad_norm": 21.326505661010742,
      "learning_rate": 9.699316040779457e-06,
      "loss": 2.2819,
      "step": 3282
    },
    {
      "epoch": 1.2710027100271002,
      "grad_norm": 18.862451553344727,
      "learning_rate": 9.698885877747667e-06,
      "loss": 1.1173,
      "step": 3283
    },
    {
      "epoch": 1.2713898567557105,
      "grad_norm": 31.599777221679688,
      "learning_rate": 9.698455714715878e-06,
      "loss": 3.0348,
      "step": 3284
    },
    {
      "epoch": 1.2717770034843205,
      "grad_norm": 14.505105972290039,
      "learning_rate": 9.69802555168409e-06,
      "loss": 0.5428,
      "step": 3285
    },
    {
      "epoch": 1.2721641502129306,
      "grad_norm": 27.11631965637207,
      "learning_rate": 9.6975953886523e-06,
      "loss": 1.707,
      "step": 3286
    },
    {
      "epoch": 1.272551296941541,
      "grad_norm": 20.468534469604492,
      "learning_rate": 9.69716522562051e-06,
      "loss": 1.9114,
      "step": 3287
    },
    {
      "epoch": 1.272938443670151,
      "grad_norm": 15.965800285339355,
      "learning_rate": 9.696735062588722e-06,
      "loss": 1.5302,
      "step": 3288
    },
    {
      "epoch": 1.2733255903987613,
      "grad_norm": 16.15327262878418,
      "learning_rate": 9.696304899556933e-06,
      "loss": 1.5292,
      "step": 3289
    },
    {
      "epoch": 1.2737127371273713,
      "grad_norm": 58.397972106933594,
      "learning_rate": 9.695874736525143e-06,
      "loss": 3.1395,
      "step": 3290
    },
    {
      "epoch": 1.2740998838559814,
      "grad_norm": 18.919288635253906,
      "learning_rate": 9.695444573493355e-06,
      "loss": 2.255,
      "step": 3291
    },
    {
      "epoch": 1.2744870305845915,
      "grad_norm": 15.345280647277832,
      "learning_rate": 9.695014410461566e-06,
      "loss": 1.5958,
      "step": 3292
    },
    {
      "epoch": 1.2748741773132017,
      "grad_norm": 23.239707946777344,
      "learning_rate": 9.694584247429777e-06,
      "loss": 1.4844,
      "step": 3293
    },
    {
      "epoch": 1.2752613240418118,
      "grad_norm": 33.525611877441406,
      "learning_rate": 9.694154084397987e-06,
      "loss": 2.03,
      "step": 3294
    },
    {
      "epoch": 1.275648470770422,
      "grad_norm": 13.756267547607422,
      "learning_rate": 9.693723921366198e-06,
      "loss": 1.7245,
      "step": 3295
    },
    {
      "epoch": 1.2760356174990322,
      "grad_norm": 40.17377471923828,
      "learning_rate": 9.69329375833441e-06,
      "loss": 1.6383,
      "step": 3296
    },
    {
      "epoch": 1.2764227642276422,
      "grad_norm": 23.345556259155273,
      "learning_rate": 9.692863595302621e-06,
      "loss": 1.2773,
      "step": 3297
    },
    {
      "epoch": 1.2768099109562523,
      "grad_norm": 10.175095558166504,
      "learning_rate": 9.692433432270831e-06,
      "loss": 1.1747,
      "step": 3298
    },
    {
      "epoch": 1.2771970576848626,
      "grad_norm": 13.016672134399414,
      "learning_rate": 9.692003269239042e-06,
      "loss": 0.9885,
      "step": 3299
    },
    {
      "epoch": 1.2775842044134726,
      "grad_norm": 29.733572006225586,
      "learning_rate": 9.691573106207254e-06,
      "loss": 1.9277,
      "step": 3300
    },
    {
      "epoch": 1.277971351142083,
      "grad_norm": 21.714582443237305,
      "learning_rate": 9.691142943175465e-06,
      "loss": 1.6587,
      "step": 3301
    },
    {
      "epoch": 1.278358497870693,
      "grad_norm": 16.177486419677734,
      "learning_rate": 9.690712780143675e-06,
      "loss": 1.8059,
      "step": 3302
    },
    {
      "epoch": 1.278745644599303,
      "grad_norm": 9.931166648864746,
      "learning_rate": 9.690282617111886e-06,
      "loss": 1.0671,
      "step": 3303
    },
    {
      "epoch": 1.2791327913279134,
      "grad_norm": 17.09491729736328,
      "learning_rate": 9.689852454080098e-06,
      "loss": 1.5693,
      "step": 3304
    },
    {
      "epoch": 1.2795199380565234,
      "grad_norm": 24.932231903076172,
      "learning_rate": 9.689422291048308e-06,
      "loss": 1.9352,
      "step": 3305
    },
    {
      "epoch": 1.2799070847851335,
      "grad_norm": 21.566740036010742,
      "learning_rate": 9.688992128016519e-06,
      "loss": 2.1048,
      "step": 3306
    },
    {
      "epoch": 1.2802942315137438,
      "grad_norm": 68.03015899658203,
      "learning_rate": 9.68856196498473e-06,
      "loss": 1.7339,
      "step": 3307
    },
    {
      "epoch": 1.2806813782423538,
      "grad_norm": 11.51016616821289,
      "learning_rate": 9.688131801952942e-06,
      "loss": 1.4832,
      "step": 3308
    },
    {
      "epoch": 1.281068524970964,
      "grad_norm": 15.27468204498291,
      "learning_rate": 9.687701638921152e-06,
      "loss": 0.8499,
      "step": 3309
    },
    {
      "epoch": 1.2814556716995742,
      "grad_norm": 12.367563247680664,
      "learning_rate": 9.687271475889363e-06,
      "loss": 0.9416,
      "step": 3310
    },
    {
      "epoch": 1.2818428184281843,
      "grad_norm": 9.575948715209961,
      "learning_rate": 9.686841312857573e-06,
      "loss": 1.3857,
      "step": 3311
    },
    {
      "epoch": 1.2822299651567945,
      "grad_norm": 14.732059478759766,
      "learning_rate": 9.686411149825786e-06,
      "loss": 0.7887,
      "step": 3312
    },
    {
      "epoch": 1.2826171118854046,
      "grad_norm": 14.909027099609375,
      "learning_rate": 9.685980986793996e-06,
      "loss": 1.0884,
      "step": 3313
    },
    {
      "epoch": 1.2830042586140147,
      "grad_norm": 25.21554183959961,
      "learning_rate": 9.685550823762207e-06,
      "loss": 0.8714,
      "step": 3314
    },
    {
      "epoch": 1.2833914053426247,
      "grad_norm": 17.186092376708984,
      "learning_rate": 9.685120660730417e-06,
      "loss": 0.9391,
      "step": 3315
    },
    {
      "epoch": 1.283778552071235,
      "grad_norm": 15.65897274017334,
      "learning_rate": 9.684690497698628e-06,
      "loss": 1.7216,
      "step": 3316
    },
    {
      "epoch": 1.284165698799845,
      "grad_norm": 21.36167335510254,
      "learning_rate": 9.68426033466684e-06,
      "loss": 1.6416,
      "step": 3317
    },
    {
      "epoch": 1.2845528455284554,
      "grad_norm": 20.911548614501953,
      "learning_rate": 9.683830171635051e-06,
      "loss": 1.3365,
      "step": 3318
    },
    {
      "epoch": 1.2849399922570655,
      "grad_norm": 21.69590187072754,
      "learning_rate": 9.68340000860326e-06,
      "loss": 1.751,
      "step": 3319
    },
    {
      "epoch": 1.2853271389856755,
      "grad_norm": 19.207963943481445,
      "learning_rate": 9.682969845571472e-06,
      "loss": 1.5993,
      "step": 3320
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 13.618947982788086,
      "learning_rate": 9.682539682539683e-06,
      "loss": 1.5634,
      "step": 3321
    },
    {
      "epoch": 1.2861014324428959,
      "grad_norm": 12.111778259277344,
      "learning_rate": 9.682109519507895e-06,
      "loss": 0.9602,
      "step": 3322
    },
    {
      "epoch": 1.286488579171506,
      "grad_norm": 29.309297561645508,
      "learning_rate": 9.681679356476105e-06,
      "loss": 2.0014,
      "step": 3323
    },
    {
      "epoch": 1.2868757259001162,
      "grad_norm": 11.898804664611816,
      "learning_rate": 9.681249193444316e-06,
      "loss": 1.4363,
      "step": 3324
    },
    {
      "epoch": 1.2872628726287263,
      "grad_norm": 22.153390884399414,
      "learning_rate": 9.680819030412527e-06,
      "loss": 1.6389,
      "step": 3325
    },
    {
      "epoch": 1.2876500193573364,
      "grad_norm": 10.679762840270996,
      "learning_rate": 9.680388867380737e-06,
      "loss": 1.0232,
      "step": 3326
    },
    {
      "epoch": 1.2880371660859466,
      "grad_norm": 22.199230194091797,
      "learning_rate": 9.67995870434895e-06,
      "loss": 2.006,
      "step": 3327
    },
    {
      "epoch": 1.2884243128145567,
      "grad_norm": 32.10906982421875,
      "learning_rate": 9.67952854131716e-06,
      "loss": 1.7624,
      "step": 3328
    },
    {
      "epoch": 1.2888114595431668,
      "grad_norm": 17.99242401123047,
      "learning_rate": 9.679098378285371e-06,
      "loss": 0.965,
      "step": 3329
    },
    {
      "epoch": 1.289198606271777,
      "grad_norm": 9.240885734558105,
      "learning_rate": 9.678668215253581e-06,
      "loss": 1.0597,
      "step": 3330
    },
    {
      "epoch": 1.2895857530003871,
      "grad_norm": 9.513111114501953,
      "learning_rate": 9.678238052221793e-06,
      "loss": 1.0424,
      "step": 3331
    },
    {
      "epoch": 1.2899728997289972,
      "grad_norm": 24.48053741455078,
      "learning_rate": 9.677807889190004e-06,
      "loss": 1.6013,
      "step": 3332
    },
    {
      "epoch": 1.2903600464576075,
      "grad_norm": 9.635660171508789,
      "learning_rate": 9.677377726158215e-06,
      "loss": 1.2474,
      "step": 3333
    },
    {
      "epoch": 1.2907471931862176,
      "grad_norm": 25.403959274291992,
      "learning_rate": 9.676947563126425e-06,
      "loss": 3.0401,
      "step": 3334
    },
    {
      "epoch": 1.2911343399148278,
      "grad_norm": 9.964703559875488,
      "learning_rate": 9.676517400094636e-06,
      "loss": 0.3981,
      "step": 3335
    },
    {
      "epoch": 1.291521486643438,
      "grad_norm": 22.03668212890625,
      "learning_rate": 9.676087237062848e-06,
      "loss": 1.4553,
      "step": 3336
    },
    {
      "epoch": 1.291908633372048,
      "grad_norm": 18.518678665161133,
      "learning_rate": 9.67565707403106e-06,
      "loss": 1.1309,
      "step": 3337
    },
    {
      "epoch": 1.292295780100658,
      "grad_norm": 12.104294776916504,
      "learning_rate": 9.675226910999269e-06,
      "loss": 1.2565,
      "step": 3338
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 15.823452949523926,
      "learning_rate": 9.67479674796748e-06,
      "loss": 1.8209,
      "step": 3339
    },
    {
      "epoch": 1.2930700735578784,
      "grad_norm": 12.968160629272461,
      "learning_rate": 9.674366584935692e-06,
      "loss": 1.0514,
      "step": 3340
    },
    {
      "epoch": 1.2934572202864887,
      "grad_norm": 19.80754280090332,
      "learning_rate": 9.673936421903902e-06,
      "loss": 1.8392,
      "step": 3341
    },
    {
      "epoch": 1.2938443670150988,
      "grad_norm": 17.071598052978516,
      "learning_rate": 9.673506258872113e-06,
      "loss": 1.2478,
      "step": 3342
    },
    {
      "epoch": 1.2942315137437088,
      "grad_norm": 19.01749038696289,
      "learning_rate": 9.673076095840324e-06,
      "loss": 1.6027,
      "step": 3343
    },
    {
      "epoch": 1.2946186604723189,
      "grad_norm": 16.308300018310547,
      "learning_rate": 9.672645932808536e-06,
      "loss": 1.495,
      "step": 3344
    },
    {
      "epoch": 1.2950058072009292,
      "grad_norm": 21.174715042114258,
      "learning_rate": 9.672215769776746e-06,
      "loss": 1.5615,
      "step": 3345
    },
    {
      "epoch": 1.2953929539295392,
      "grad_norm": 18.816925048828125,
      "learning_rate": 9.671785606744957e-06,
      "loss": 1.7012,
      "step": 3346
    },
    {
      "epoch": 1.2957801006581495,
      "grad_norm": 17.377412796020508,
      "learning_rate": 9.671355443713168e-06,
      "loss": 1.6281,
      "step": 3347
    },
    {
      "epoch": 1.2961672473867596,
      "grad_norm": 15.943391799926758,
      "learning_rate": 9.67092528068138e-06,
      "loss": 1.6755,
      "step": 3348
    },
    {
      "epoch": 1.2965543941153697,
      "grad_norm": 26.40155601501465,
      "learning_rate": 9.67049511764959e-06,
      "loss": 2.3617,
      "step": 3349
    },
    {
      "epoch": 1.29694154084398,
      "grad_norm": 13.924914360046387,
      "learning_rate": 9.670064954617801e-06,
      "loss": 1.1688,
      "step": 3350
    },
    {
      "epoch": 1.29732868757259,
      "grad_norm": 14.837048530578613,
      "learning_rate": 9.669634791586012e-06,
      "loss": 0.9855,
      "step": 3351
    },
    {
      "epoch": 1.2977158343012,
      "grad_norm": 24.940237045288086,
      "learning_rate": 9.669204628554222e-06,
      "loss": 1.2227,
      "step": 3352
    },
    {
      "epoch": 1.2981029810298104,
      "grad_norm": 13.079712867736816,
      "learning_rate": 9.668774465522433e-06,
      "loss": 1.2094,
      "step": 3353
    },
    {
      "epoch": 1.2984901277584204,
      "grad_norm": 12.964866638183594,
      "learning_rate": 9.668344302490645e-06,
      "loss": 1.4801,
      "step": 3354
    },
    {
      "epoch": 1.2988772744870305,
      "grad_norm": 15.798587799072266,
      "learning_rate": 9.667914139458856e-06,
      "loss": 1.4917,
      "step": 3355
    },
    {
      "epoch": 1.2992644212156408,
      "grad_norm": 19.26820182800293,
      "learning_rate": 9.667483976427066e-06,
      "loss": 1.0846,
      "step": 3356
    },
    {
      "epoch": 1.2996515679442509,
      "grad_norm": 13.980013847351074,
      "learning_rate": 9.667053813395277e-06,
      "loss": 0.9377,
      "step": 3357
    },
    {
      "epoch": 1.3000387146728611,
      "grad_norm": 22.0952091217041,
      "learning_rate": 9.666623650363489e-06,
      "loss": 1.9607,
      "step": 3358
    },
    {
      "epoch": 1.3004258614014712,
      "grad_norm": 22.01922607421875,
      "learning_rate": 9.6661934873317e-06,
      "loss": 2.4066,
      "step": 3359
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 20.779891967773438,
      "learning_rate": 9.66576332429991e-06,
      "loss": 1.1087,
      "step": 3360
    },
    {
      "epoch": 1.3012001548586913,
      "grad_norm": 24.43221664428711,
      "learning_rate": 9.665333161268121e-06,
      "loss": 1.3221,
      "step": 3361
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 17.033061981201172,
      "learning_rate": 9.664902998236331e-06,
      "loss": 1.5171,
      "step": 3362
    },
    {
      "epoch": 1.3019744483159117,
      "grad_norm": 36.26148986816406,
      "learning_rate": 9.664472835204544e-06,
      "loss": 1.6572,
      "step": 3363
    },
    {
      "epoch": 1.302361595044522,
      "grad_norm": 22.503494262695312,
      "learning_rate": 9.664042672172754e-06,
      "loss": 1.5498,
      "step": 3364
    },
    {
      "epoch": 1.302748741773132,
      "grad_norm": 20.758296966552734,
      "learning_rate": 9.663612509140965e-06,
      "loss": 1.6391,
      "step": 3365
    },
    {
      "epoch": 1.3031358885017421,
      "grad_norm": 14.692475318908691,
      "learning_rate": 9.663182346109177e-06,
      "loss": 1.1978,
      "step": 3366
    },
    {
      "epoch": 1.3035230352303522,
      "grad_norm": 17.801555633544922,
      "learning_rate": 9.662752183077387e-06,
      "loss": 1.3728,
      "step": 3367
    },
    {
      "epoch": 1.3039101819589625,
      "grad_norm": 16.596481323242188,
      "learning_rate": 9.662322020045598e-06,
      "loss": 1.6341,
      "step": 3368
    },
    {
      "epoch": 1.3042973286875725,
      "grad_norm": 18.138591766357422,
      "learning_rate": 9.66189185701381e-06,
      "loss": 1.6289,
      "step": 3369
    },
    {
      "epoch": 1.3046844754161828,
      "grad_norm": 16.189727783203125,
      "learning_rate": 9.66146169398202e-06,
      "loss": 1.0237,
      "step": 3370
    },
    {
      "epoch": 1.3050716221447929,
      "grad_norm": 18.230623245239258,
      "learning_rate": 9.66103153095023e-06,
      "loss": 1.7535,
      "step": 3371
    },
    {
      "epoch": 1.305458768873403,
      "grad_norm": 15.612677574157715,
      "learning_rate": 9.660601367918442e-06,
      "loss": 1.4639,
      "step": 3372
    },
    {
      "epoch": 1.3058459156020132,
      "grad_norm": 13.41771125793457,
      "learning_rate": 9.660171204886653e-06,
      "loss": 1.3558,
      "step": 3373
    },
    {
      "epoch": 1.3062330623306233,
      "grad_norm": 24.50428009033203,
      "learning_rate": 9.659741041854865e-06,
      "loss": 1.8957,
      "step": 3374
    },
    {
      "epoch": 1.3066202090592334,
      "grad_norm": 33.353355407714844,
      "learning_rate": 9.659310878823074e-06,
      "loss": 2.5199,
      "step": 3375
    },
    {
      "epoch": 1.3070073557878437,
      "grad_norm": 16.305633544921875,
      "learning_rate": 9.658880715791286e-06,
      "loss": 1.1079,
      "step": 3376
    },
    {
      "epoch": 1.3073945025164537,
      "grad_norm": 24.859350204467773,
      "learning_rate": 9.658450552759496e-06,
      "loss": 1.7337,
      "step": 3377
    },
    {
      "epoch": 1.3077816492450638,
      "grad_norm": 26.84339714050293,
      "learning_rate": 9.658020389727709e-06,
      "loss": 1.5897,
      "step": 3378
    },
    {
      "epoch": 1.308168795973674,
      "grad_norm": 12.134149551391602,
      "learning_rate": 9.657590226695918e-06,
      "loss": 1.2963,
      "step": 3379
    },
    {
      "epoch": 1.3085559427022841,
      "grad_norm": 22.818891525268555,
      "learning_rate": 9.65716006366413e-06,
      "loss": 1.5598,
      "step": 3380
    },
    {
      "epoch": 1.3089430894308944,
      "grad_norm": 18.331636428833008,
      "learning_rate": 9.65672990063234e-06,
      "loss": 1.8373,
      "step": 3381
    },
    {
      "epoch": 1.3093302361595045,
      "grad_norm": 42.82891082763672,
      "learning_rate": 9.656299737600551e-06,
      "loss": 1.292,
      "step": 3382
    },
    {
      "epoch": 1.3097173828881146,
      "grad_norm": 12.89648151397705,
      "learning_rate": 9.655869574568762e-06,
      "loss": 1.2975,
      "step": 3383
    },
    {
      "epoch": 1.3101045296167246,
      "grad_norm": 17.787561416625977,
      "learning_rate": 9.655439411536974e-06,
      "loss": 1.2746,
      "step": 3384
    },
    {
      "epoch": 1.310491676345335,
      "grad_norm": 20.891450881958008,
      "learning_rate": 9.655009248505184e-06,
      "loss": 1.8093,
      "step": 3385
    },
    {
      "epoch": 1.310878823073945,
      "grad_norm": 12.46840763092041,
      "learning_rate": 9.654579085473395e-06,
      "loss": 1.0917,
      "step": 3386
    },
    {
      "epoch": 1.3112659698025553,
      "grad_norm": 14.112421035766602,
      "learning_rate": 9.654148922441606e-06,
      "loss": 1.5805,
      "step": 3387
    },
    {
      "epoch": 1.3116531165311653,
      "grad_norm": 17.769887924194336,
      "learning_rate": 9.653718759409816e-06,
      "loss": 1.6792,
      "step": 3388
    },
    {
      "epoch": 1.3120402632597754,
      "grad_norm": 23.842226028442383,
      "learning_rate": 9.653288596378028e-06,
      "loss": 1.4303,
      "step": 3389
    },
    {
      "epoch": 1.3124274099883855,
      "grad_norm": 9.300182342529297,
      "learning_rate": 9.652858433346239e-06,
      "loss": 0.705,
      "step": 3390
    },
    {
      "epoch": 1.3128145567169958,
      "grad_norm": 18.489015579223633,
      "learning_rate": 9.65242827031445e-06,
      "loss": 1.3403,
      "step": 3391
    },
    {
      "epoch": 1.3132017034456058,
      "grad_norm": 15.720929145812988,
      "learning_rate": 9.65199810728266e-06,
      "loss": 1.6585,
      "step": 3392
    },
    {
      "epoch": 1.3135888501742161,
      "grad_norm": 14.094223022460938,
      "learning_rate": 9.651567944250871e-06,
      "loss": 1.3564,
      "step": 3393
    },
    {
      "epoch": 1.3139759969028262,
      "grad_norm": 28.164201736450195,
      "learning_rate": 9.651137781219083e-06,
      "loss": 1.1983,
      "step": 3394
    },
    {
      "epoch": 1.3143631436314362,
      "grad_norm": 17.560741424560547,
      "learning_rate": 9.650707618187294e-06,
      "loss": 1.3248,
      "step": 3395
    },
    {
      "epoch": 1.3147502903600465,
      "grad_norm": 32.0415153503418,
      "learning_rate": 9.650277455155504e-06,
      "loss": 2.9825,
      "step": 3396
    },
    {
      "epoch": 1.3151374370886566,
      "grad_norm": 20.53676986694336,
      "learning_rate": 9.649847292123715e-06,
      "loss": 0.926,
      "step": 3397
    },
    {
      "epoch": 1.3155245838172667,
      "grad_norm": 15.176612854003906,
      "learning_rate": 9.649417129091927e-06,
      "loss": 1.6402,
      "step": 3398
    },
    {
      "epoch": 1.315911730545877,
      "grad_norm": 16.3530216217041,
      "learning_rate": 9.648986966060138e-06,
      "loss": 1.5216,
      "step": 3399
    },
    {
      "epoch": 1.316298877274487,
      "grad_norm": 32.54514694213867,
      "learning_rate": 9.648556803028348e-06,
      "loss": 2.742,
      "step": 3400
    },
    {
      "epoch": 1.316686024003097,
      "grad_norm": 19.002429962158203,
      "learning_rate": 9.64812663999656e-06,
      "loss": 1.8131,
      "step": 3401
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 16.531545639038086,
      "learning_rate": 9.64769647696477e-06,
      "loss": 1.6578,
      "step": 3402
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 15.888009071350098,
      "learning_rate": 9.64726631393298e-06,
      "loss": 1.6679,
      "step": 3403
    },
    {
      "epoch": 1.3178474641889277,
      "grad_norm": 27.521785736083984,
      "learning_rate": 9.646836150901192e-06,
      "loss": 1.2181,
      "step": 3404
    },
    {
      "epoch": 1.3182346109175378,
      "grad_norm": 18.376510620117188,
      "learning_rate": 9.646405987869403e-06,
      "loss": 1.8687,
      "step": 3405
    },
    {
      "epoch": 1.3186217576461479,
      "grad_norm": 22.532203674316406,
      "learning_rate": 9.645975824837615e-06,
      "loss": 1.6539,
      "step": 3406
    },
    {
      "epoch": 1.319008904374758,
      "grad_norm": 15.198843002319336,
      "learning_rate": 9.645545661805825e-06,
      "loss": 1.1417,
      "step": 3407
    },
    {
      "epoch": 1.3193960511033682,
      "grad_norm": 43.06319046020508,
      "learning_rate": 9.645115498774036e-06,
      "loss": 2.1019,
      "step": 3408
    },
    {
      "epoch": 1.3197831978319783,
      "grad_norm": 24.51716423034668,
      "learning_rate": 9.644685335742247e-06,
      "loss": 1.4524,
      "step": 3409
    },
    {
      "epoch": 1.3201703445605886,
      "grad_norm": 14.62345027923584,
      "learning_rate": 9.644255172710459e-06,
      "loss": 0.5857,
      "step": 3410
    },
    {
      "epoch": 1.3205574912891986,
      "grad_norm": 17.768856048583984,
      "learning_rate": 9.643825009678669e-06,
      "loss": 1.1161,
      "step": 3411
    },
    {
      "epoch": 1.3209446380178087,
      "grad_norm": 14.153352737426758,
      "learning_rate": 9.64339484664688e-06,
      "loss": 1.4665,
      "step": 3412
    },
    {
      "epoch": 1.3213317847464188,
      "grad_norm": 31.423934936523438,
      "learning_rate": 9.642964683615091e-06,
      "loss": 1.8024,
      "step": 3413
    },
    {
      "epoch": 1.321718931475029,
      "grad_norm": 20.573917388916016,
      "learning_rate": 9.642534520583303e-06,
      "loss": 2.0796,
      "step": 3414
    },
    {
      "epoch": 1.3221060782036391,
      "grad_norm": 22.55805015563965,
      "learning_rate": 9.642104357551512e-06,
      "loss": 1.7507,
      "step": 3415
    },
    {
      "epoch": 1.3224932249322494,
      "grad_norm": 15.18310546875,
      "learning_rate": 9.641674194519724e-06,
      "loss": 1.6338,
      "step": 3416
    },
    {
      "epoch": 1.3228803716608595,
      "grad_norm": 12.264091491699219,
      "learning_rate": 9.641244031487935e-06,
      "loss": 1.1504,
      "step": 3417
    },
    {
      "epoch": 1.3232675183894695,
      "grad_norm": 20.935256958007812,
      "learning_rate": 9.640813868456145e-06,
      "loss": 2.467,
      "step": 3418
    },
    {
      "epoch": 1.3236546651180798,
      "grad_norm": 13.690155029296875,
      "learning_rate": 9.640383705424356e-06,
      "loss": 1.4612,
      "step": 3419
    },
    {
      "epoch": 1.32404181184669,
      "grad_norm": 17.28893280029297,
      "learning_rate": 9.639953542392568e-06,
      "loss": 1.8173,
      "step": 3420
    },
    {
      "epoch": 1.3244289585753,
      "grad_norm": 16.320608139038086,
      "learning_rate": 9.63952337936078e-06,
      "loss": 1.6497,
      "step": 3421
    },
    {
      "epoch": 1.3248161053039103,
      "grad_norm": 19.458900451660156,
      "learning_rate": 9.639093216328989e-06,
      "loss": 1.6852,
      "step": 3422
    },
    {
      "epoch": 1.3252032520325203,
      "grad_norm": 14.68132495880127,
      "learning_rate": 9.6386630532972e-06,
      "loss": 1.0371,
      "step": 3423
    },
    {
      "epoch": 1.3255903987611304,
      "grad_norm": 12.477901458740234,
      "learning_rate": 9.63823289026541e-06,
      "loss": 1.4236,
      "step": 3424
    },
    {
      "epoch": 1.3259775454897407,
      "grad_norm": 13.613871574401855,
      "learning_rate": 9.637802727233623e-06,
      "loss": 1.4593,
      "step": 3425
    },
    {
      "epoch": 1.3263646922183507,
      "grad_norm": 12.49155330657959,
      "learning_rate": 9.637372564201833e-06,
      "loss": 1.4276,
      "step": 3426
    },
    {
      "epoch": 1.326751838946961,
      "grad_norm": 13.62575912475586,
      "learning_rate": 9.636942401170044e-06,
      "loss": 1.5996,
      "step": 3427
    },
    {
      "epoch": 1.327138985675571,
      "grad_norm": 11.197802543640137,
      "learning_rate": 9.636512238138254e-06,
      "loss": 0.7781,
      "step": 3428
    },
    {
      "epoch": 1.3275261324041812,
      "grad_norm": 20.936513900756836,
      "learning_rate": 9.636082075106467e-06,
      "loss": 1.1645,
      "step": 3429
    },
    {
      "epoch": 1.3279132791327912,
      "grad_norm": 14.205543518066406,
      "learning_rate": 9.635651912074677e-06,
      "loss": 1.4589,
      "step": 3430
    },
    {
      "epoch": 1.3283004258614015,
      "grad_norm": 32.11210250854492,
      "learning_rate": 9.635221749042888e-06,
      "loss": 1.6508,
      "step": 3431
    },
    {
      "epoch": 1.3286875725900116,
      "grad_norm": 15.868603706359863,
      "learning_rate": 9.634791586011098e-06,
      "loss": 1.5584,
      "step": 3432
    },
    {
      "epoch": 1.3290747193186219,
      "grad_norm": 19.77918243408203,
      "learning_rate": 9.63436142297931e-06,
      "loss": 1.5816,
      "step": 3433
    },
    {
      "epoch": 1.329461866047232,
      "grad_norm": 27.626691818237305,
      "learning_rate": 9.633931259947521e-06,
      "loss": 1.8961,
      "step": 3434
    },
    {
      "epoch": 1.329849012775842,
      "grad_norm": 10.127907752990723,
      "learning_rate": 9.633501096915732e-06,
      "loss": 1.8504,
      "step": 3435
    },
    {
      "epoch": 1.330236159504452,
      "grad_norm": 14.391097068786621,
      "learning_rate": 9.633070933883942e-06,
      "loss": 1.2051,
      "step": 3436
    },
    {
      "epoch": 1.3306233062330624,
      "grad_norm": 25.888399124145508,
      "learning_rate": 9.632640770852153e-06,
      "loss": 1.6564,
      "step": 3437
    },
    {
      "epoch": 1.3310104529616724,
      "grad_norm": 13.01993465423584,
      "learning_rate": 9.632210607820365e-06,
      "loss": 1.3869,
      "step": 3438
    },
    {
      "epoch": 1.3313975996902827,
      "grad_norm": 21.876121520996094,
      "learning_rate": 9.631780444788575e-06,
      "loss": 1.7757,
      "step": 3439
    },
    {
      "epoch": 1.3317847464188928,
      "grad_norm": 14.903050422668457,
      "learning_rate": 9.631350281756786e-06,
      "loss": 0.8845,
      "step": 3440
    },
    {
      "epoch": 1.3321718931475028,
      "grad_norm": 14.575642585754395,
      "learning_rate": 9.630920118724997e-06,
      "loss": 1.5567,
      "step": 3441
    },
    {
      "epoch": 1.3325590398761131,
      "grad_norm": 22.90084457397461,
      "learning_rate": 9.630489955693209e-06,
      "loss": 1.6448,
      "step": 3442
    },
    {
      "epoch": 1.3329461866047232,
      "grad_norm": 17.03199577331543,
      "learning_rate": 9.630059792661419e-06,
      "loss": 1.8449,
      "step": 3443
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 13.455605506896973,
      "learning_rate": 9.62962962962963e-06,
      "loss": 1.5526,
      "step": 3444
    },
    {
      "epoch": 1.3337204800619435,
      "grad_norm": 11.355820655822754,
      "learning_rate": 9.629199466597841e-06,
      "loss": 1.229,
      "step": 3445
    },
    {
      "epoch": 1.3341076267905536,
      "grad_norm": 15.893288612365723,
      "learning_rate": 9.628769303566053e-06,
      "loss": 1.5392,
      "step": 3446
    },
    {
      "epoch": 1.3344947735191637,
      "grad_norm": 20.88265037536621,
      "learning_rate": 9.628339140534263e-06,
      "loss": 1.0838,
      "step": 3447
    },
    {
      "epoch": 1.334881920247774,
      "grad_norm": 25.060264587402344,
      "learning_rate": 9.627908977502474e-06,
      "loss": 1.6364,
      "step": 3448
    },
    {
      "epoch": 1.335269066976384,
      "grad_norm": 20.531225204467773,
      "learning_rate": 9.627478814470685e-06,
      "loss": 2.4238,
      "step": 3449
    },
    {
      "epoch": 1.3356562137049943,
      "grad_norm": 13.812579154968262,
      "learning_rate": 9.627048651438897e-06,
      "loss": 0.9264,
      "step": 3450
    },
    {
      "epoch": 1.3360433604336044,
      "grad_norm": 24.500900268554688,
      "learning_rate": 9.626618488407107e-06,
      "loss": 1.4781,
      "step": 3451
    },
    {
      "epoch": 1.3364305071622145,
      "grad_norm": 15.895552635192871,
      "learning_rate": 9.626188325375318e-06,
      "loss": 1.5544,
      "step": 3452
    },
    {
      "epoch": 1.3368176538908245,
      "grad_norm": 22.024145126342773,
      "learning_rate": 9.62575816234353e-06,
      "loss": 1.8333,
      "step": 3453
    },
    {
      "epoch": 1.3372048006194348,
      "grad_norm": 16.74407196044922,
      "learning_rate": 9.625327999311739e-06,
      "loss": 1.3914,
      "step": 3454
    },
    {
      "epoch": 1.3375919473480449,
      "grad_norm": 23.65851593017578,
      "learning_rate": 9.62489783627995e-06,
      "loss": 1.9918,
      "step": 3455
    },
    {
      "epoch": 1.3379790940766552,
      "grad_norm": 18.055118560791016,
      "learning_rate": 9.624467673248162e-06,
      "loss": 1.7498,
      "step": 3456
    },
    {
      "epoch": 1.3383662408052652,
      "grad_norm": 12.637370109558105,
      "learning_rate": 9.624037510216373e-06,
      "loss": 0.7492,
      "step": 3457
    },
    {
      "epoch": 1.3387533875338753,
      "grad_norm": 19.001380920410156,
      "learning_rate": 9.623607347184583e-06,
      "loss": 2.248,
      "step": 3458
    },
    {
      "epoch": 1.3391405342624854,
      "grad_norm": 14.410921096801758,
      "learning_rate": 9.623177184152794e-06,
      "loss": 1.5533,
      "step": 3459
    },
    {
      "epoch": 1.3395276809910956,
      "grad_norm": 15.341729164123535,
      "learning_rate": 9.622747021121006e-06,
      "loss": 1.5207,
      "step": 3460
    },
    {
      "epoch": 1.3399148277197057,
      "grad_norm": 25.953672409057617,
      "learning_rate": 9.622316858089217e-06,
      "loss": 1.3231,
      "step": 3461
    },
    {
      "epoch": 1.340301974448316,
      "grad_norm": 18.3841552734375,
      "learning_rate": 9.621886695057427e-06,
      "loss": 1.3641,
      "step": 3462
    },
    {
      "epoch": 1.340689121176926,
      "grad_norm": 25.626413345336914,
      "learning_rate": 9.621456532025638e-06,
      "loss": 1.6771,
      "step": 3463
    },
    {
      "epoch": 1.3410762679055361,
      "grad_norm": 13.292607307434082,
      "learning_rate": 9.62102636899385e-06,
      "loss": 0.9426,
      "step": 3464
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 33.015682220458984,
      "learning_rate": 9.620596205962061e-06,
      "loss": 1.689,
      "step": 3465
    },
    {
      "epoch": 1.3418505613627565,
      "grad_norm": 26.144439697265625,
      "learning_rate": 9.620166042930271e-06,
      "loss": 2.0046,
      "step": 3466
    },
    {
      "epoch": 1.3422377080913666,
      "grad_norm": 22.526676177978516,
      "learning_rate": 9.619735879898482e-06,
      "loss": 1.8549,
      "step": 3467
    },
    {
      "epoch": 1.3426248548199768,
      "grad_norm": 17.302200317382812,
      "learning_rate": 9.619305716866694e-06,
      "loss": 1.282,
      "step": 3468
    },
    {
      "epoch": 1.343012001548587,
      "grad_norm": 25.541383743286133,
      "learning_rate": 9.618875553834904e-06,
      "loss": 1.6948,
      "step": 3469
    },
    {
      "epoch": 1.343399148277197,
      "grad_norm": 17.177343368530273,
      "learning_rate": 9.618445390803115e-06,
      "loss": 1.8015,
      "step": 3470
    },
    {
      "epoch": 1.3437862950058073,
      "grad_norm": 16.522708892822266,
      "learning_rate": 9.618015227771326e-06,
      "loss": 1.5567,
      "step": 3471
    },
    {
      "epoch": 1.3441734417344173,
      "grad_norm": 16.339515686035156,
      "learning_rate": 9.617585064739538e-06,
      "loss": 1.6642,
      "step": 3472
    },
    {
      "epoch": 1.3445605884630276,
      "grad_norm": 26.31557846069336,
      "learning_rate": 9.617154901707747e-06,
      "loss": 1.1847,
      "step": 3473
    },
    {
      "epoch": 1.3449477351916377,
      "grad_norm": 25.60638427734375,
      "learning_rate": 9.616724738675959e-06,
      "loss": 2.31,
      "step": 3474
    },
    {
      "epoch": 1.3453348819202477,
      "grad_norm": 18.261266708374023,
      "learning_rate": 9.616294575644169e-06,
      "loss": 1.7167,
      "step": 3475
    },
    {
      "epoch": 1.3457220286488578,
      "grad_norm": 16.537546157836914,
      "learning_rate": 9.615864412612382e-06,
      "loss": 2.445,
      "step": 3476
    },
    {
      "epoch": 1.346109175377468,
      "grad_norm": 15.526817321777344,
      "learning_rate": 9.615434249580591e-06,
      "loss": 1.7366,
      "step": 3477
    },
    {
      "epoch": 1.3464963221060782,
      "grad_norm": 22.298677444458008,
      "learning_rate": 9.615004086548803e-06,
      "loss": 0.9798,
      "step": 3478
    },
    {
      "epoch": 1.3468834688346885,
      "grad_norm": 21.987302780151367,
      "learning_rate": 9.614573923517013e-06,
      "loss": 1.5398,
      "step": 3479
    },
    {
      "epoch": 1.3472706155632985,
      "grad_norm": 16.146724700927734,
      "learning_rate": 9.614143760485226e-06,
      "loss": 1.284,
      "step": 3480
    },
    {
      "epoch": 1.3476577622919086,
      "grad_norm": 42.82249069213867,
      "learning_rate": 9.613713597453435e-06,
      "loss": 2.0976,
      "step": 3481
    },
    {
      "epoch": 1.3480449090205187,
      "grad_norm": 12.40505599975586,
      "learning_rate": 9.613283434421647e-06,
      "loss": 1.557,
      "step": 3482
    },
    {
      "epoch": 1.348432055749129,
      "grad_norm": 19.735118865966797,
      "learning_rate": 9.612853271389857e-06,
      "loss": 1.9154,
      "step": 3483
    },
    {
      "epoch": 1.348819202477739,
      "grad_norm": 9.333877563476562,
      "learning_rate": 9.612423108358068e-06,
      "loss": 0.6606,
      "step": 3484
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 19.917957305908203,
      "learning_rate": 9.61199294532628e-06,
      "loss": 2.1717,
      "step": 3485
    },
    {
      "epoch": 1.3495934959349594,
      "grad_norm": 9.478782653808594,
      "learning_rate": 9.61156278229449e-06,
      "loss": 1.2281,
      "step": 3486
    },
    {
      "epoch": 1.3499806426635694,
      "grad_norm": 16.020320892333984,
      "learning_rate": 9.6111326192627e-06,
      "loss": 1.032,
      "step": 3487
    },
    {
      "epoch": 1.3503677893921797,
      "grad_norm": 23.84636688232422,
      "learning_rate": 9.610702456230912e-06,
      "loss": 2.0645,
      "step": 3488
    },
    {
      "epoch": 1.3507549361207898,
      "grad_norm": 19.698400497436523,
      "learning_rate": 9.610272293199123e-06,
      "loss": 2.2971,
      "step": 3489
    },
    {
      "epoch": 1.3511420828493999,
      "grad_norm": 23.008508682250977,
      "learning_rate": 9.609842130167333e-06,
      "loss": 1.5395,
      "step": 3490
    },
    {
      "epoch": 1.3515292295780101,
      "grad_norm": 32.634765625,
      "learning_rate": 9.609411967135546e-06,
      "loss": 1.4945,
      "step": 3491
    },
    {
      "epoch": 1.3519163763066202,
      "grad_norm": 11.456148147583008,
      "learning_rate": 9.608981804103756e-06,
      "loss": 1.3683,
      "step": 3492
    },
    {
      "epoch": 1.3523035230352303,
      "grad_norm": 25.16816520690918,
      "learning_rate": 9.608551641071967e-06,
      "loss": 2.4135,
      "step": 3493
    },
    {
      "epoch": 1.3526906697638406,
      "grad_norm": 24.68096923828125,
      "learning_rate": 9.608121478040177e-06,
      "loss": 1.7738,
      "step": 3494
    },
    {
      "epoch": 1.3530778164924506,
      "grad_norm": 19.967809677124023,
      "learning_rate": 9.60769131500839e-06,
      "loss": 1.4758,
      "step": 3495
    },
    {
      "epoch": 1.353464963221061,
      "grad_norm": 26.237321853637695,
      "learning_rate": 9.6072611519766e-06,
      "loss": 1.4945,
      "step": 3496
    },
    {
      "epoch": 1.353852109949671,
      "grad_norm": 15.790268898010254,
      "learning_rate": 9.606830988944811e-06,
      "loss": 2.4626,
      "step": 3497
    },
    {
      "epoch": 1.354239256678281,
      "grad_norm": 23.68973731994629,
      "learning_rate": 9.606400825913021e-06,
      "loss": 1.4758,
      "step": 3498
    },
    {
      "epoch": 1.354626403406891,
      "grad_norm": 22.662799835205078,
      "learning_rate": 9.605970662881232e-06,
      "loss": 1.6033,
      "step": 3499
    },
    {
      "epoch": 1.3550135501355014,
      "grad_norm": 14.864972114562988,
      "learning_rate": 9.605540499849444e-06,
      "loss": 1.4721,
      "step": 3500
    },
    {
      "epoch": 1.3554006968641115,
      "grad_norm": 17.347036361694336,
      "learning_rate": 9.605110336817655e-06,
      "loss": 1.0213,
      "step": 3501
    },
    {
      "epoch": 1.3557878435927218,
      "grad_norm": 22.828685760498047,
      "learning_rate": 9.604680173785865e-06,
      "loss": 1.4313,
      "step": 3502
    },
    {
      "epoch": 1.3561749903213318,
      "grad_norm": 13.30090618133545,
      "learning_rate": 9.604250010754076e-06,
      "loss": 1.1683,
      "step": 3503
    },
    {
      "epoch": 1.3565621370499419,
      "grad_norm": 22.427967071533203,
      "learning_rate": 9.603819847722288e-06,
      "loss": 2.3406,
      "step": 3504
    },
    {
      "epoch": 1.356949283778552,
      "grad_norm": 15.385917663574219,
      "learning_rate": 9.603389684690498e-06,
      "loss": 1.59,
      "step": 3505
    },
    {
      "epoch": 1.3573364305071622,
      "grad_norm": 16.466846466064453,
      "learning_rate": 9.602959521658709e-06,
      "loss": 1.0232,
      "step": 3506
    },
    {
      "epoch": 1.3577235772357723,
      "grad_norm": 17.26390266418457,
      "learning_rate": 9.60252935862692e-06,
      "loss": 1.5017,
      "step": 3507
    },
    {
      "epoch": 1.3581107239643826,
      "grad_norm": 12.371686935424805,
      "learning_rate": 9.602099195595132e-06,
      "loss": 1.5026,
      "step": 3508
    },
    {
      "epoch": 1.3584978706929927,
      "grad_norm": 18.153127670288086,
      "learning_rate": 9.601669032563342e-06,
      "loss": 1.5795,
      "step": 3509
    },
    {
      "epoch": 1.3588850174216027,
      "grad_norm": 21.939407348632812,
      "learning_rate": 9.601238869531553e-06,
      "loss": 0.9562,
      "step": 3510
    },
    {
      "epoch": 1.359272164150213,
      "grad_norm": 11.34705924987793,
      "learning_rate": 9.600808706499764e-06,
      "loss": 1.3961,
      "step": 3511
    },
    {
      "epoch": 1.359659310878823,
      "grad_norm": 8.758858680725098,
      "learning_rate": 9.600378543467976e-06,
      "loss": 0.7065,
      "step": 3512
    },
    {
      "epoch": 1.3600464576074331,
      "grad_norm": 34.111167907714844,
      "learning_rate": 9.599948380436185e-06,
      "loss": 1.6705,
      "step": 3513
    },
    {
      "epoch": 1.3604336043360434,
      "grad_norm": 19.796598434448242,
      "learning_rate": 9.599518217404397e-06,
      "loss": 1.9703,
      "step": 3514
    },
    {
      "epoch": 1.3608207510646535,
      "grad_norm": 24.188997268676758,
      "learning_rate": 9.599088054372608e-06,
      "loss": 1.6476,
      "step": 3515
    },
    {
      "epoch": 1.3612078977932636,
      "grad_norm": 16.172489166259766,
      "learning_rate": 9.59865789134082e-06,
      "loss": 1.3066,
      "step": 3516
    },
    {
      "epoch": 1.3615950445218739,
      "grad_norm": 15.83199691772461,
      "learning_rate": 9.59822772830903e-06,
      "loss": 1.6864,
      "step": 3517
    },
    {
      "epoch": 1.361982191250484,
      "grad_norm": 23.04738998413086,
      "learning_rate": 9.597797565277241e-06,
      "loss": 2.1675,
      "step": 3518
    },
    {
      "epoch": 1.3623693379790942,
      "grad_norm": 14.54413890838623,
      "learning_rate": 9.597367402245452e-06,
      "loss": 1.34,
      "step": 3519
    },
    {
      "epoch": 1.3627564847077043,
      "grad_norm": 13.993019104003906,
      "learning_rate": 9.596937239213662e-06,
      "loss": 1.0836,
      "step": 3520
    },
    {
      "epoch": 1.3631436314363143,
      "grad_norm": 15.482339859008789,
      "learning_rate": 9.596507076181873e-06,
      "loss": 1.1896,
      "step": 3521
    },
    {
      "epoch": 1.3635307781649244,
      "grad_norm": 16.267318725585938,
      "learning_rate": 9.596076913150085e-06,
      "loss": 1.7389,
      "step": 3522
    },
    {
      "epoch": 1.3639179248935347,
      "grad_norm": 20.676830291748047,
      "learning_rate": 9.595646750118296e-06,
      "loss": 0.9689,
      "step": 3523
    },
    {
      "epoch": 1.3643050716221448,
      "grad_norm": 14.435134887695312,
      "learning_rate": 9.595216587086506e-06,
      "loss": 1.3037,
      "step": 3524
    },
    {
      "epoch": 1.364692218350755,
      "grad_norm": 10.160842895507812,
      "learning_rate": 9.594786424054717e-06,
      "loss": 1.3814,
      "step": 3525
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 19.350954055786133,
      "learning_rate": 9.594356261022927e-06,
      "loss": 1.3492,
      "step": 3526
    },
    {
      "epoch": 1.3654665118079752,
      "grad_norm": 14.83062744140625,
      "learning_rate": 9.59392609799114e-06,
      "loss": 1.3542,
      "step": 3527
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 26.41290283203125,
      "learning_rate": 9.59349593495935e-06,
      "loss": 2.2811,
      "step": 3528
    },
    {
      "epoch": 1.3662408052651955,
      "grad_norm": 20.683237075805664,
      "learning_rate": 9.593065771927561e-06,
      "loss": 1.0309,
      "step": 3529
    },
    {
      "epoch": 1.3666279519938056,
      "grad_norm": 23.57701873779297,
      "learning_rate": 9.592635608895773e-06,
      "loss": 1.7172,
      "step": 3530
    },
    {
      "epoch": 1.3670150987224159,
      "grad_norm": 23.646635055541992,
      "learning_rate": 9.592205445863984e-06,
      "loss": 1.5853,
      "step": 3531
    },
    {
      "epoch": 1.367402245451026,
      "grad_norm": 24.84259033203125,
      "learning_rate": 9.591775282832194e-06,
      "loss": 0.9029,
      "step": 3532
    },
    {
      "epoch": 1.367789392179636,
      "grad_norm": 20.724918365478516,
      "learning_rate": 9.591345119800405e-06,
      "loss": 1.2161,
      "step": 3533
    },
    {
      "epoch": 1.368176538908246,
      "grad_norm": 22.292753219604492,
      "learning_rate": 9.590914956768617e-06,
      "loss": 1.3782,
      "step": 3534
    },
    {
      "epoch": 1.3685636856368564,
      "grad_norm": 17.008453369140625,
      "learning_rate": 9.590484793736826e-06,
      "loss": 1.5696,
      "step": 3535
    },
    {
      "epoch": 1.3689508323654664,
      "grad_norm": 8.011855125427246,
      "learning_rate": 9.590054630705038e-06,
      "loss": 1.3458,
      "step": 3536
    },
    {
      "epoch": 1.3693379790940767,
      "grad_norm": 7.891359329223633,
      "learning_rate": 9.58962446767325e-06,
      "loss": 1.0885,
      "step": 3537
    },
    {
      "epoch": 1.3697251258226868,
      "grad_norm": 11.614701271057129,
      "learning_rate": 9.58919430464146e-06,
      "loss": 0.4847,
      "step": 3538
    },
    {
      "epoch": 1.3701122725512969,
      "grad_norm": 14.757722854614258,
      "learning_rate": 9.58876414160967e-06,
      "loss": 0.984,
      "step": 3539
    },
    {
      "epoch": 1.3704994192799071,
      "grad_norm": 15.103273391723633,
      "learning_rate": 9.588333978577882e-06,
      "loss": 1.4263,
      "step": 3540
    },
    {
      "epoch": 1.3708865660085172,
      "grad_norm": 16.59102439880371,
      "learning_rate": 9.587903815546092e-06,
      "loss": 1.4788,
      "step": 3541
    },
    {
      "epoch": 1.3712737127371275,
      "grad_norm": 18.490917205810547,
      "learning_rate": 9.587473652514305e-06,
      "loss": 2.8409,
      "step": 3542
    },
    {
      "epoch": 1.3716608594657376,
      "grad_norm": 12.286789894104004,
      "learning_rate": 9.587043489482514e-06,
      "loss": 0.7132,
      "step": 3543
    },
    {
      "epoch": 1.3720480061943476,
      "grad_norm": 8.39908218383789,
      "learning_rate": 9.586613326450726e-06,
      "loss": 0.6447,
      "step": 3544
    },
    {
      "epoch": 1.3724351529229577,
      "grad_norm": 24.145570755004883,
      "learning_rate": 9.586183163418936e-06,
      "loss": 2.0586,
      "step": 3545
    },
    {
      "epoch": 1.372822299651568,
      "grad_norm": 27.597412109375,
      "learning_rate": 9.585753000387149e-06,
      "loss": 1.3586,
      "step": 3546
    },
    {
      "epoch": 1.373209446380178,
      "grad_norm": 29.442134857177734,
      "learning_rate": 9.585322837355358e-06,
      "loss": 2.2221,
      "step": 3547
    },
    {
      "epoch": 1.3735965931087883,
      "grad_norm": 11.36711597442627,
      "learning_rate": 9.58489267432357e-06,
      "loss": 0.9323,
      "step": 3548
    },
    {
      "epoch": 1.3739837398373984,
      "grad_norm": 38.84115219116211,
      "learning_rate": 9.58446251129178e-06,
      "loss": 2.0521,
      "step": 3549
    },
    {
      "epoch": 1.3743708865660085,
      "grad_norm": 17.980995178222656,
      "learning_rate": 9.584032348259991e-06,
      "loss": 1.1918,
      "step": 3550
    },
    {
      "epoch": 1.3747580332946185,
      "grad_norm": 14.392972946166992,
      "learning_rate": 9.583602185228202e-06,
      "loss": 1.5151,
      "step": 3551
    },
    {
      "epoch": 1.3751451800232288,
      "grad_norm": 7.78441047668457,
      "learning_rate": 9.583172022196414e-06,
      "loss": 0.562,
      "step": 3552
    },
    {
      "epoch": 1.375532326751839,
      "grad_norm": 9.173548698425293,
      "learning_rate": 9.582741859164623e-06,
      "loss": 1.3207,
      "step": 3553
    },
    {
      "epoch": 1.3759194734804492,
      "grad_norm": 11.747632026672363,
      "learning_rate": 9.582311696132835e-06,
      "loss": 0.7385,
      "step": 3554
    },
    {
      "epoch": 1.3763066202090593,
      "grad_norm": 14.926589012145996,
      "learning_rate": 9.581881533101046e-06,
      "loss": 1.2158,
      "step": 3555
    },
    {
      "epoch": 1.3766937669376693,
      "grad_norm": 19.055112838745117,
      "learning_rate": 9.581451370069256e-06,
      "loss": 1.7706,
      "step": 3556
    },
    {
      "epoch": 1.3770809136662794,
      "grad_norm": 13.38760757446289,
      "learning_rate": 9.581021207037467e-06,
      "loss": 1.119,
      "step": 3557
    },
    {
      "epoch": 1.3774680603948897,
      "grad_norm": 15.868931770324707,
      "learning_rate": 9.580591044005679e-06,
      "loss": 1.5502,
      "step": 3558
    },
    {
      "epoch": 1.3778552071234997,
      "grad_norm": 12.771061897277832,
      "learning_rate": 9.58016088097389e-06,
      "loss": 1.1551,
      "step": 3559
    },
    {
      "epoch": 1.37824235385211,
      "grad_norm": 21.564416885375977,
      "learning_rate": 9.5797307179421e-06,
      "loss": 1.6548,
      "step": 3560
    },
    {
      "epoch": 1.37862950058072,
      "grad_norm": 21.748104095458984,
      "learning_rate": 9.579300554910311e-06,
      "loss": 1.7672,
      "step": 3561
    },
    {
      "epoch": 1.3790166473093302,
      "grad_norm": 41.533451080322266,
      "learning_rate": 9.578870391878523e-06,
      "loss": 1.2596,
      "step": 3562
    },
    {
      "epoch": 1.3794037940379404,
      "grad_norm": 19.553369522094727,
      "learning_rate": 9.578440228846734e-06,
      "loss": 1.5767,
      "step": 3563
    },
    {
      "epoch": 1.3797909407665505,
      "grad_norm": 47.86676025390625,
      "learning_rate": 9.578010065814944e-06,
      "loss": 1.3266,
      "step": 3564
    },
    {
      "epoch": 1.3801780874951608,
      "grad_norm": 15.96479320526123,
      "learning_rate": 9.577579902783155e-06,
      "loss": 2.3863,
      "step": 3565
    },
    {
      "epoch": 1.3805652342237709,
      "grad_norm": 14.267049789428711,
      "learning_rate": 9.577149739751367e-06,
      "loss": 0.782,
      "step": 3566
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 50.56477355957031,
      "learning_rate": 9.576719576719578e-06,
      "loss": 1.792,
      "step": 3567
    },
    {
      "epoch": 1.381339527680991,
      "grad_norm": 16.52740478515625,
      "learning_rate": 9.576289413687788e-06,
      "loss": 1.4485,
      "step": 3568
    },
    {
      "epoch": 1.3817266744096013,
      "grad_norm": 21.923978805541992,
      "learning_rate": 9.575859250656e-06,
      "loss": 1.1944,
      "step": 3569
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 24.03523063659668,
      "learning_rate": 9.57542908762421e-06,
      "loss": 2.477,
      "step": 3570
    },
    {
      "epoch": 1.3825009678668216,
      "grad_norm": 22.33869743347168,
      "learning_rate": 9.57499892459242e-06,
      "loss": 1.552,
      "step": 3571
    },
    {
      "epoch": 1.3828881145954317,
      "grad_norm": 31.526147842407227,
      "learning_rate": 9.574568761560632e-06,
      "loss": 2.897,
      "step": 3572
    },
    {
      "epoch": 1.3832752613240418,
      "grad_norm": 17.543306350708008,
      "learning_rate": 9.574138598528843e-06,
      "loss": 1.8544,
      "step": 3573
    },
    {
      "epoch": 1.3836624080526518,
      "grad_norm": 37.566139221191406,
      "learning_rate": 9.573708435497055e-06,
      "loss": 1.7974,
      "step": 3574
    },
    {
      "epoch": 1.3840495547812621,
      "grad_norm": 33.36931228637695,
      "learning_rate": 9.573278272465264e-06,
      "loss": 1.715,
      "step": 3575
    },
    {
      "epoch": 1.3844367015098722,
      "grad_norm": 13.667819023132324,
      "learning_rate": 9.572848109433476e-06,
      "loss": 0.9941,
      "step": 3576
    },
    {
      "epoch": 1.3848238482384825,
      "grad_norm": 16.70513916015625,
      "learning_rate": 9.572417946401687e-06,
      "loss": 1.4886,
      "step": 3577
    },
    {
      "epoch": 1.3852109949670925,
      "grad_norm": 16.013887405395508,
      "learning_rate": 9.571987783369899e-06,
      "loss": 1.2787,
      "step": 3578
    },
    {
      "epoch": 1.3855981416957026,
      "grad_norm": 17.46553611755371,
      "learning_rate": 9.571557620338108e-06,
      "loss": 1.7699,
      "step": 3579
    },
    {
      "epoch": 1.3859852884243127,
      "grad_norm": 20.88226318359375,
      "learning_rate": 9.57112745730632e-06,
      "loss": 2.0133,
      "step": 3580
    },
    {
      "epoch": 1.386372435152923,
      "grad_norm": 21.302406311035156,
      "learning_rate": 9.570697294274531e-06,
      "loss": 1.1988,
      "step": 3581
    },
    {
      "epoch": 1.386759581881533,
      "grad_norm": 11.546379089355469,
      "learning_rate": 9.570267131242743e-06,
      "loss": 1.3616,
      "step": 3582
    },
    {
      "epoch": 1.3871467286101433,
      "grad_norm": 42.6988410949707,
      "learning_rate": 9.569836968210952e-06,
      "loss": 1.7959,
      "step": 3583
    },
    {
      "epoch": 1.3875338753387534,
      "grad_norm": 14.121162414550781,
      "learning_rate": 9.569406805179164e-06,
      "loss": 1.0947,
      "step": 3584
    },
    {
      "epoch": 1.3879210220673635,
      "grad_norm": 20.13431167602539,
      "learning_rate": 9.568976642147375e-06,
      "loss": 1.1171,
      "step": 3585
    },
    {
      "epoch": 1.3883081687959737,
      "grad_norm": 19.878847122192383,
      "learning_rate": 9.568546479115585e-06,
      "loss": 1.5404,
      "step": 3586
    },
    {
      "epoch": 1.3886953155245838,
      "grad_norm": 45.44544982910156,
      "learning_rate": 9.568116316083796e-06,
      "loss": 2.3652,
      "step": 3587
    },
    {
      "epoch": 1.389082462253194,
      "grad_norm": 19.73773956298828,
      "learning_rate": 9.567686153052008e-06,
      "loss": 1.3241,
      "step": 3588
    },
    {
      "epoch": 1.3894696089818042,
      "grad_norm": 16.222614288330078,
      "learning_rate": 9.56725599002022e-06,
      "loss": 1.7011,
      "step": 3589
    },
    {
      "epoch": 1.3898567557104142,
      "grad_norm": 12.824857711791992,
      "learning_rate": 9.566825826988429e-06,
      "loss": 1.3135,
      "step": 3590
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 14.66210651397705,
      "learning_rate": 9.56639566395664e-06,
      "loss": 1.4737,
      "step": 3591
    },
    {
      "epoch": 1.3906310491676346,
      "grad_norm": 18.79764747619629,
      "learning_rate": 9.56596550092485e-06,
      "loss": 1.6242,
      "step": 3592
    },
    {
      "epoch": 1.3910181958962446,
      "grad_norm": 10.779136657714844,
      "learning_rate": 9.565535337893063e-06,
      "loss": 0.7418,
      "step": 3593
    },
    {
      "epoch": 1.391405342624855,
      "grad_norm": 13.835148811340332,
      "learning_rate": 9.565105174861273e-06,
      "loss": 0.9928,
      "step": 3594
    },
    {
      "epoch": 1.391792489353465,
      "grad_norm": 23.564531326293945,
      "learning_rate": 9.564675011829484e-06,
      "loss": 1.1466,
      "step": 3595
    },
    {
      "epoch": 1.392179636082075,
      "grad_norm": 14.840373039245605,
      "learning_rate": 9.564244848797694e-06,
      "loss": 1.4599,
      "step": 3596
    },
    {
      "epoch": 1.3925667828106851,
      "grad_norm": 18.769840240478516,
      "learning_rate": 9.563814685765907e-06,
      "loss": 1.6917,
      "step": 3597
    },
    {
      "epoch": 1.3929539295392954,
      "grad_norm": 19.322507858276367,
      "learning_rate": 9.563384522734117e-06,
      "loss": 1.4439,
      "step": 3598
    },
    {
      "epoch": 1.3933410762679055,
      "grad_norm": 10.227241516113281,
      "learning_rate": 9.562954359702328e-06,
      "loss": 0.6598,
      "step": 3599
    },
    {
      "epoch": 1.3937282229965158,
      "grad_norm": 17.111080169677734,
      "learning_rate": 9.562524196670538e-06,
      "loss": 1.5706,
      "step": 3600
    },
    {
      "epoch": 1.3941153697251258,
      "grad_norm": 24.19242286682129,
      "learning_rate": 9.56209403363875e-06,
      "loss": 1.4269,
      "step": 3601
    },
    {
      "epoch": 1.394502516453736,
      "grad_norm": 27.989076614379883,
      "learning_rate": 9.56166387060696e-06,
      "loss": 2.1827,
      "step": 3602
    },
    {
      "epoch": 1.394889663182346,
      "grad_norm": 23.679946899414062,
      "learning_rate": 9.561233707575172e-06,
      "loss": 1.7967,
      "step": 3603
    },
    {
      "epoch": 1.3952768099109563,
      "grad_norm": 17.50737953186035,
      "learning_rate": 9.560803544543382e-06,
      "loss": 1.2811,
      "step": 3604
    },
    {
      "epoch": 1.3956639566395663,
      "grad_norm": 15.845333099365234,
      "learning_rate": 9.560373381511593e-06,
      "loss": 1.6614,
      "step": 3605
    },
    {
      "epoch": 1.3960511033681766,
      "grad_norm": 28.210250854492188,
      "learning_rate": 9.559943218479805e-06,
      "loss": 1.2948,
      "step": 3606
    },
    {
      "epoch": 1.3964382500967867,
      "grad_norm": 25.887392044067383,
      "learning_rate": 9.559513055448015e-06,
      "loss": 1.8591,
      "step": 3607
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 27.38494300842285,
      "learning_rate": 9.559082892416226e-06,
      "loss": 1.7259,
      "step": 3608
    },
    {
      "epoch": 1.397212543554007,
      "grad_norm": 16.731182098388672,
      "learning_rate": 9.558652729384437e-06,
      "loss": 1.7935,
      "step": 3609
    },
    {
      "epoch": 1.397599690282617,
      "grad_norm": 19.483205795288086,
      "learning_rate": 9.558222566352649e-06,
      "loss": 1.0147,
      "step": 3610
    },
    {
      "epoch": 1.3979868370112274,
      "grad_norm": 22.293371200561523,
      "learning_rate": 9.557792403320858e-06,
      "loss": 1.4724,
      "step": 3611
    },
    {
      "epoch": 1.3983739837398375,
      "grad_norm": 21.144023895263672,
      "learning_rate": 9.557362240289072e-06,
      "loss": 1.6779,
      "step": 3612
    },
    {
      "epoch": 1.3987611304684475,
      "grad_norm": 15.23646354675293,
      "learning_rate": 9.556932077257281e-06,
      "loss": 1.1107,
      "step": 3613
    },
    {
      "epoch": 1.3991482771970576,
      "grad_norm": 21.523151397705078,
      "learning_rate": 9.556501914225493e-06,
      "loss": 1.6168,
      "step": 3614
    },
    {
      "epoch": 1.3995354239256679,
      "grad_norm": 14.18027114868164,
      "learning_rate": 9.556071751193702e-06,
      "loss": 0.7514,
      "step": 3615
    },
    {
      "epoch": 1.399922570654278,
      "grad_norm": 33.01518249511719,
      "learning_rate": 9.555641588161914e-06,
      "loss": 1.5116,
      "step": 3616
    },
    {
      "epoch": 1.4003097173828882,
      "grad_norm": 13.545287132263184,
      "learning_rate": 9.555211425130125e-06,
      "loss": 1.1868,
      "step": 3617
    },
    {
      "epoch": 1.4006968641114983,
      "grad_norm": 15.072162628173828,
      "learning_rate": 9.554781262098337e-06,
      "loss": 0.7824,
      "step": 3618
    },
    {
      "epoch": 1.4010840108401084,
      "grad_norm": 21.856178283691406,
      "learning_rate": 9.554351099066546e-06,
      "loss": 2.3681,
      "step": 3619
    },
    {
      "epoch": 1.4014711575687184,
      "grad_norm": 13.244826316833496,
      "learning_rate": 9.553920936034758e-06,
      "loss": 1.206,
      "step": 3620
    },
    {
      "epoch": 1.4018583042973287,
      "grad_norm": 10.352575302124023,
      "learning_rate": 9.55349077300297e-06,
      "loss": 1.3016,
      "step": 3621
    },
    {
      "epoch": 1.4022454510259388,
      "grad_norm": 16.94053840637207,
      "learning_rate": 9.553060609971179e-06,
      "loss": 1.8456,
      "step": 3622
    },
    {
      "epoch": 1.402632597754549,
      "grad_norm": 9.614395141601562,
      "learning_rate": 9.55263044693939e-06,
      "loss": 1.295,
      "step": 3623
    },
    {
      "epoch": 1.4030197444831591,
      "grad_norm": 19.890670776367188,
      "learning_rate": 9.552200283907602e-06,
      "loss": 2.4076,
      "step": 3624
    },
    {
      "epoch": 1.4034068912117692,
      "grad_norm": 15.93909740447998,
      "learning_rate": 9.551770120875813e-06,
      "loss": 1.1148,
      "step": 3625
    },
    {
      "epoch": 1.4037940379403793,
      "grad_norm": 21.39826202392578,
      "learning_rate": 9.551339957844023e-06,
      "loss": 2.1967,
      "step": 3626
    },
    {
      "epoch": 1.4041811846689896,
      "grad_norm": 12.765335083007812,
      "learning_rate": 9.550909794812234e-06,
      "loss": 1.4088,
      "step": 3627
    },
    {
      "epoch": 1.4045683313975996,
      "grad_norm": 47.098960876464844,
      "learning_rate": 9.550479631780446e-06,
      "loss": 3.004,
      "step": 3628
    },
    {
      "epoch": 1.40495547812621,
      "grad_norm": 25.874513626098633,
      "learning_rate": 9.550049468748657e-06,
      "loss": 1.6365,
      "step": 3629
    },
    {
      "epoch": 1.40534262485482,
      "grad_norm": 12.27521800994873,
      "learning_rate": 9.549619305716867e-06,
      "loss": 1.203,
      "step": 3630
    },
    {
      "epoch": 1.40572977158343,
      "grad_norm": 14.119560241699219,
      "learning_rate": 9.549189142685078e-06,
      "loss": 1.5502,
      "step": 3631
    },
    {
      "epoch": 1.4061169183120403,
      "grad_norm": 11.076017379760742,
      "learning_rate": 9.54875897965329e-06,
      "loss": 0.4331,
      "step": 3632
    },
    {
      "epoch": 1.4065040650406504,
      "grad_norm": 15.187954902648926,
      "learning_rate": 9.548328816621501e-06,
      "loss": 1.2142,
      "step": 3633
    },
    {
      "epoch": 1.4068912117692607,
      "grad_norm": 17.457950592041016,
      "learning_rate": 9.547898653589711e-06,
      "loss": 1.4189,
      "step": 3634
    },
    {
      "epoch": 1.4072783584978708,
      "grad_norm": 14.904985427856445,
      "learning_rate": 9.547468490557922e-06,
      "loss": 1.675,
      "step": 3635
    },
    {
      "epoch": 1.4076655052264808,
      "grad_norm": 9.059663772583008,
      "learning_rate": 9.547038327526134e-06,
      "loss": 0.6198,
      "step": 3636
    },
    {
      "epoch": 1.4080526519550909,
      "grad_norm": 18.54106903076172,
      "learning_rate": 9.546608164494343e-06,
      "loss": 1.1544,
      "step": 3637
    },
    {
      "epoch": 1.4084397986837012,
      "grad_norm": 16.67202377319336,
      "learning_rate": 9.546178001462555e-06,
      "loss": 1.8065,
      "step": 3638
    },
    {
      "epoch": 1.4088269454123112,
      "grad_norm": 15.585121154785156,
      "learning_rate": 9.545747838430766e-06,
      "loss": 1.1459,
      "step": 3639
    },
    {
      "epoch": 1.4092140921409215,
      "grad_norm": 13.68992805480957,
      "learning_rate": 9.545317675398978e-06,
      "loss": 0.9093,
      "step": 3640
    },
    {
      "epoch": 1.4096012388695316,
      "grad_norm": 12.294504165649414,
      "learning_rate": 9.544887512367187e-06,
      "loss": 1.047,
      "step": 3641
    },
    {
      "epoch": 1.4099883855981417,
      "grad_norm": 17.756256103515625,
      "learning_rate": 9.544457349335399e-06,
      "loss": 0.8328,
      "step": 3642
    },
    {
      "epoch": 1.4103755323267517,
      "grad_norm": 30.489517211914062,
      "learning_rate": 9.544027186303609e-06,
      "loss": 2.352,
      "step": 3643
    },
    {
      "epoch": 1.410762679055362,
      "grad_norm": 17.20305633544922,
      "learning_rate": 9.543597023271822e-06,
      "loss": 0.9017,
      "step": 3644
    },
    {
      "epoch": 1.411149825783972,
      "grad_norm": 17.982446670532227,
      "learning_rate": 9.543166860240031e-06,
      "loss": 1.3991,
      "step": 3645
    },
    {
      "epoch": 1.4115369725125824,
      "grad_norm": 18.239267349243164,
      "learning_rate": 9.542736697208243e-06,
      "loss": 1.6473,
      "step": 3646
    },
    {
      "epoch": 1.4119241192411924,
      "grad_norm": 12.946782112121582,
      "learning_rate": 9.542306534176453e-06,
      "loss": 0.4725,
      "step": 3647
    },
    {
      "epoch": 1.4123112659698025,
      "grad_norm": 20.65831184387207,
      "learning_rate": 9.541876371144666e-06,
      "loss": 1.1565,
      "step": 3648
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 10.483732223510742,
      "learning_rate": 9.541446208112875e-06,
      "loss": 1.2742,
      "step": 3649
    },
    {
      "epoch": 1.4130855594270229,
      "grad_norm": 31.258920669555664,
      "learning_rate": 9.541016045081087e-06,
      "loss": 2.7283,
      "step": 3650
    },
    {
      "epoch": 1.413472706155633,
      "grad_norm": 12.4324312210083,
      "learning_rate": 9.540585882049296e-06,
      "loss": 1.1082,
      "step": 3651
    },
    {
      "epoch": 1.4138598528842432,
      "grad_norm": 16.60248374938965,
      "learning_rate": 9.540155719017508e-06,
      "loss": 0.9952,
      "step": 3652
    },
    {
      "epoch": 1.4142469996128533,
      "grad_norm": 10.963432312011719,
      "learning_rate": 9.53972555598572e-06,
      "loss": 1.4476,
      "step": 3653
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 9.094389915466309,
      "learning_rate": 9.53929539295393e-06,
      "loss": 1.2848,
      "step": 3654
    },
    {
      "epoch": 1.4150212930700736,
      "grad_norm": 39.477359771728516,
      "learning_rate": 9.538865229922142e-06,
      "loss": 1.294,
      "step": 3655
    },
    {
      "epoch": 1.4154084397986837,
      "grad_norm": 19.827434539794922,
      "learning_rate": 9.538435066890352e-06,
      "loss": 1.6109,
      "step": 3656
    },
    {
      "epoch": 1.415795586527294,
      "grad_norm": 12.565638542175293,
      "learning_rate": 9.538004903858563e-06,
      "loss": 1.1223,
      "step": 3657
    },
    {
      "epoch": 1.416182733255904,
      "grad_norm": 10.880878448486328,
      "learning_rate": 9.537574740826773e-06,
      "loss": 1.0312,
      "step": 3658
    },
    {
      "epoch": 1.4165698799845141,
      "grad_norm": 22.723514556884766,
      "learning_rate": 9.537144577794986e-06,
      "loss": 1.9056,
      "step": 3659
    },
    {
      "epoch": 1.4169570267131242,
      "grad_norm": 23.09491539001465,
      "learning_rate": 9.536714414763196e-06,
      "loss": 1.5379,
      "step": 3660
    },
    {
      "epoch": 1.4173441734417345,
      "grad_norm": 15.963936805725098,
      "learning_rate": 9.536284251731407e-06,
      "loss": 1.5559,
      "step": 3661
    },
    {
      "epoch": 1.4177313201703445,
      "grad_norm": 19.110506057739258,
      "learning_rate": 9.535854088699617e-06,
      "loss": 1.9317,
      "step": 3662
    },
    {
      "epoch": 1.4181184668989548,
      "grad_norm": 39.30975341796875,
      "learning_rate": 9.53542392566783e-06,
      "loss": 1.6755,
      "step": 3663
    },
    {
      "epoch": 1.4185056136275649,
      "grad_norm": 21.616533279418945,
      "learning_rate": 9.53499376263604e-06,
      "loss": 2.074,
      "step": 3664
    },
    {
      "epoch": 1.418892760356175,
      "grad_norm": 16.180273056030273,
      "learning_rate": 9.534563599604251e-06,
      "loss": 1.8374,
      "step": 3665
    },
    {
      "epoch": 1.419279907084785,
      "grad_norm": 32.42030715942383,
      "learning_rate": 9.534133436572461e-06,
      "loss": 1.9887,
      "step": 3666
    },
    {
      "epoch": 1.4196670538133953,
      "grad_norm": 21.925540924072266,
      "learning_rate": 9.533703273540672e-06,
      "loss": 1.6288,
      "step": 3667
    },
    {
      "epoch": 1.4200542005420054,
      "grad_norm": 21.330652236938477,
      "learning_rate": 9.533273110508884e-06,
      "loss": 1.628,
      "step": 3668
    },
    {
      "epoch": 1.4204413472706157,
      "grad_norm": 28.615720748901367,
      "learning_rate": 9.532842947477095e-06,
      "loss": 2.5625,
      "step": 3669
    },
    {
      "epoch": 1.4208284939992257,
      "grad_norm": 26.141355514526367,
      "learning_rate": 9.532412784445305e-06,
      "loss": 1.3038,
      "step": 3670
    },
    {
      "epoch": 1.4212156407278358,
      "grad_norm": 14.833569526672363,
      "learning_rate": 9.531982621413516e-06,
      "loss": 0.9084,
      "step": 3671
    },
    {
      "epoch": 1.4216027874564459,
      "grad_norm": 15.940132141113281,
      "learning_rate": 9.531552458381728e-06,
      "loss": 1.5376,
      "step": 3672
    },
    {
      "epoch": 1.4219899341850561,
      "grad_norm": 12.231499671936035,
      "learning_rate": 9.531122295349937e-06,
      "loss": 0.7937,
      "step": 3673
    },
    {
      "epoch": 1.4223770809136662,
      "grad_norm": 16.32278060913086,
      "learning_rate": 9.530692132318149e-06,
      "loss": 1.4239,
      "step": 3674
    },
    {
      "epoch": 1.4227642276422765,
      "grad_norm": 16.540245056152344,
      "learning_rate": 9.53026196928636e-06,
      "loss": 1.1907,
      "step": 3675
    },
    {
      "epoch": 1.4231513743708866,
      "grad_norm": 16.743633270263672,
      "learning_rate": 9.529831806254572e-06,
      "loss": 1.6008,
      "step": 3676
    },
    {
      "epoch": 1.4235385210994966,
      "grad_norm": 20.99725914001465,
      "learning_rate": 9.529401643222781e-06,
      "loss": 2.3794,
      "step": 3677
    },
    {
      "epoch": 1.423925667828107,
      "grad_norm": 16.698938369750977,
      "learning_rate": 9.528971480190993e-06,
      "loss": 1.3365,
      "step": 3678
    },
    {
      "epoch": 1.424312814556717,
      "grad_norm": 24.623876571655273,
      "learning_rate": 9.528541317159204e-06,
      "loss": 1.316,
      "step": 3679
    },
    {
      "epoch": 1.4246999612853273,
      "grad_norm": 33.833465576171875,
      "learning_rate": 9.528111154127416e-06,
      "loss": 2.2556,
      "step": 3680
    },
    {
      "epoch": 1.4250871080139373,
      "grad_norm": 14.53125286102295,
      "learning_rate": 9.527680991095625e-06,
      "loss": 0.9216,
      "step": 3681
    },
    {
      "epoch": 1.4254742547425474,
      "grad_norm": 12.110595703125,
      "learning_rate": 9.527250828063837e-06,
      "loss": 0.8871,
      "step": 3682
    },
    {
      "epoch": 1.4258614014711575,
      "grad_norm": 21.05938720703125,
      "learning_rate": 9.526820665032048e-06,
      "loss": 1.9293,
      "step": 3683
    },
    {
      "epoch": 1.4262485481997678,
      "grad_norm": 34.108642578125,
      "learning_rate": 9.52639050200026e-06,
      "loss": 1.6278,
      "step": 3684
    },
    {
      "epoch": 1.4266356949283778,
      "grad_norm": 18.04875946044922,
      "learning_rate": 9.52596033896847e-06,
      "loss": 1.5399,
      "step": 3685
    },
    {
      "epoch": 1.4270228416569881,
      "grad_norm": 19.8041934967041,
      "learning_rate": 9.52553017593668e-06,
      "loss": 1.0782,
      "step": 3686
    },
    {
      "epoch": 1.4274099883855982,
      "grad_norm": 20.535764694213867,
      "learning_rate": 9.525100012904892e-06,
      "loss": 1.1176,
      "step": 3687
    },
    {
      "epoch": 1.4277971351142082,
      "grad_norm": 23.34959602355957,
      "learning_rate": 9.524669849873102e-06,
      "loss": 1.1379,
      "step": 3688
    },
    {
      "epoch": 1.4281842818428183,
      "grad_norm": 26.55074691772461,
      "learning_rate": 9.524239686841313e-06,
      "loss": 2.3257,
      "step": 3689
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 21.377174377441406,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.9511,
      "step": 3690
    },
    {
      "epoch": 1.4289585753000387,
      "grad_norm": 24.351844787597656,
      "learning_rate": 9.523379360777736e-06,
      "loss": 1.3327,
      "step": 3691
    },
    {
      "epoch": 1.429345722028649,
      "grad_norm": 18.027124404907227,
      "learning_rate": 9.522949197745946e-06,
      "loss": 1.452,
      "step": 3692
    },
    {
      "epoch": 1.429732868757259,
      "grad_norm": 22.35357093811035,
      "learning_rate": 9.522519034714157e-06,
      "loss": 1.2743,
      "step": 3693
    },
    {
      "epoch": 1.430120015485869,
      "grad_norm": 14.408860206604004,
      "learning_rate": 9.522088871682369e-06,
      "loss": 1.0611,
      "step": 3694
    },
    {
      "epoch": 1.4305071622144792,
      "grad_norm": 25.08940887451172,
      "learning_rate": 9.52165870865058e-06,
      "loss": 1.6741,
      "step": 3695
    },
    {
      "epoch": 1.4308943089430894,
      "grad_norm": 17.658222198486328,
      "learning_rate": 9.52122854561879e-06,
      "loss": 1.5993,
      "step": 3696
    },
    {
      "epoch": 1.4312814556716995,
      "grad_norm": 14.764775276184082,
      "learning_rate": 9.520798382587001e-06,
      "loss": 0.9409,
      "step": 3697
    },
    {
      "epoch": 1.4316686024003098,
      "grad_norm": 17.272924423217773,
      "learning_rate": 9.520368219555213e-06,
      "loss": 1.8102,
      "step": 3698
    },
    {
      "epoch": 1.4320557491289199,
      "grad_norm": 18.6186466217041,
      "learning_rate": 9.519938056523424e-06,
      "loss": 1.9165,
      "step": 3699
    },
    {
      "epoch": 1.43244289585753,
      "grad_norm": 26.37054443359375,
      "learning_rate": 9.519507893491634e-06,
      "loss": 1.4913,
      "step": 3700
    },
    {
      "epoch": 1.4328300425861402,
      "grad_norm": 17.893463134765625,
      "learning_rate": 9.519077730459845e-06,
      "loss": 0.7429,
      "step": 3701
    },
    {
      "epoch": 1.4332171893147503,
      "grad_norm": 47.69562530517578,
      "learning_rate": 9.518647567428057e-06,
      "loss": 1.311,
      "step": 3702
    },
    {
      "epoch": 1.4336043360433606,
      "grad_norm": 17.906286239624023,
      "learning_rate": 9.518217404396266e-06,
      "loss": 2.0256,
      "step": 3703
    },
    {
      "epoch": 1.4339914827719706,
      "grad_norm": 25.152368545532227,
      "learning_rate": 9.517787241364478e-06,
      "loss": 1.5511,
      "step": 3704
    },
    {
      "epoch": 1.4343786295005807,
      "grad_norm": 14.436120986938477,
      "learning_rate": 9.51735707833269e-06,
      "loss": 1.0532,
      "step": 3705
    },
    {
      "epoch": 1.4347657762291908,
      "grad_norm": 27.525239944458008,
      "learning_rate": 9.5169269153009e-06,
      "loss": 1.1882,
      "step": 3706
    },
    {
      "epoch": 1.435152922957801,
      "grad_norm": 22.73465347290039,
      "learning_rate": 9.51649675226911e-06,
      "loss": 1.0503,
      "step": 3707
    },
    {
      "epoch": 1.4355400696864111,
      "grad_norm": 14.905771255493164,
      "learning_rate": 9.516066589237322e-06,
      "loss": 1.4635,
      "step": 3708
    },
    {
      "epoch": 1.4359272164150214,
      "grad_norm": 15.758349418640137,
      "learning_rate": 9.515636426205531e-06,
      "loss": 1.4036,
      "step": 3709
    },
    {
      "epoch": 1.4363143631436315,
      "grad_norm": 25.784732818603516,
      "learning_rate": 9.515206263173745e-06,
      "loss": 1.8916,
      "step": 3710
    },
    {
      "epoch": 1.4367015098722415,
      "grad_norm": 16.767595291137695,
      "learning_rate": 9.514776100141954e-06,
      "loss": 0.7201,
      "step": 3711
    },
    {
      "epoch": 1.4370886566008516,
      "grad_norm": 19.392375946044922,
      "learning_rate": 9.514345937110166e-06,
      "loss": 0.7126,
      "step": 3712
    },
    {
      "epoch": 1.437475803329462,
      "grad_norm": 20.6655330657959,
      "learning_rate": 9.513915774078375e-06,
      "loss": 1.9479,
      "step": 3713
    },
    {
      "epoch": 1.437862950058072,
      "grad_norm": 16.872249603271484,
      "learning_rate": 9.513485611046589e-06,
      "loss": 1.4229,
      "step": 3714
    },
    {
      "epoch": 1.4382500967866823,
      "grad_norm": 12.847192764282227,
      "learning_rate": 9.513055448014798e-06,
      "loss": 1.0242,
      "step": 3715
    },
    {
      "epoch": 1.4386372435152923,
      "grad_norm": 20.74510955810547,
      "learning_rate": 9.51262528498301e-06,
      "loss": 1.0771,
      "step": 3716
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 14.493424415588379,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.0278,
      "step": 3717
    },
    {
      "epoch": 1.4394115369725125,
      "grad_norm": 21.907644271850586,
      "learning_rate": 9.51176495891943e-06,
      "loss": 2.3762,
      "step": 3718
    },
    {
      "epoch": 1.4397986837011227,
      "grad_norm": 7.012363433837891,
      "learning_rate": 9.511334795887642e-06,
      "loss": 0.4538,
      "step": 3719
    },
    {
      "epoch": 1.4401858304297328,
      "grad_norm": 21.908979415893555,
      "learning_rate": 9.510904632855854e-06,
      "loss": 1.2769,
      "step": 3720
    },
    {
      "epoch": 1.440572977158343,
      "grad_norm": 12.86209487915039,
      "learning_rate": 9.510474469824063e-06,
      "loss": 1.1657,
      "step": 3721
    },
    {
      "epoch": 1.4409601238869532,
      "grad_norm": 20.734710693359375,
      "learning_rate": 9.510044306792275e-06,
      "loss": 1.6851,
      "step": 3722
    },
    {
      "epoch": 1.4413472706155632,
      "grad_norm": 13.306396484375,
      "learning_rate": 9.509614143760486e-06,
      "loss": 1.2233,
      "step": 3723
    },
    {
      "epoch": 1.4417344173441735,
      "grad_norm": 20.721832275390625,
      "learning_rate": 9.509183980728696e-06,
      "loss": 1.0985,
      "step": 3724
    },
    {
      "epoch": 1.4421215640727836,
      "grad_norm": 15.491926193237305,
      "learning_rate": 9.508753817696907e-06,
      "loss": 1.3153,
      "step": 3725
    },
    {
      "epoch": 1.4425087108013936,
      "grad_norm": 14.215641975402832,
      "learning_rate": 9.508323654665119e-06,
      "loss": 1.1314,
      "step": 3726
    },
    {
      "epoch": 1.442895857530004,
      "grad_norm": 12.457674026489258,
      "learning_rate": 9.50789349163333e-06,
      "loss": 2.1562,
      "step": 3727
    },
    {
      "epoch": 1.443283004258614,
      "grad_norm": 13.595939636230469,
      "learning_rate": 9.50746332860154e-06,
      "loss": 1.7776,
      "step": 3728
    },
    {
      "epoch": 1.443670150987224,
      "grad_norm": 18.86102294921875,
      "learning_rate": 9.507033165569751e-06,
      "loss": 1.2841,
      "step": 3729
    },
    {
      "epoch": 1.4440572977158344,
      "grad_norm": 22.3024959564209,
      "learning_rate": 9.506603002537963e-06,
      "loss": 1.6669,
      "step": 3730
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 14.186941146850586,
      "learning_rate": 9.506172839506174e-06,
      "loss": 1.0717,
      "step": 3731
    },
    {
      "epoch": 1.4448315911730547,
      "grad_norm": 17.74237823486328,
      "learning_rate": 9.505742676474384e-06,
      "loss": 0.582,
      "step": 3732
    },
    {
      "epoch": 1.4452187379016648,
      "grad_norm": 19.653759002685547,
      "learning_rate": 9.505312513442595e-06,
      "loss": 1.6184,
      "step": 3733
    },
    {
      "epoch": 1.4456058846302748,
      "grad_norm": 17.72967529296875,
      "learning_rate": 9.504882350410807e-06,
      "loss": 1.7947,
      "step": 3734
    },
    {
      "epoch": 1.445993031358885,
      "grad_norm": 9.57983684539795,
      "learning_rate": 9.504452187379018e-06,
      "loss": 1.3496,
      "step": 3735
    },
    {
      "epoch": 1.4463801780874952,
      "grad_norm": 25.59796905517578,
      "learning_rate": 9.504022024347228e-06,
      "loss": 2.5934,
      "step": 3736
    },
    {
      "epoch": 1.4467673248161053,
      "grad_norm": 18.657522201538086,
      "learning_rate": 9.50359186131544e-06,
      "loss": 1.7724,
      "step": 3737
    },
    {
      "epoch": 1.4471544715447155,
      "grad_norm": 11.905975341796875,
      "learning_rate": 9.50316169828365e-06,
      "loss": 0.8264,
      "step": 3738
    },
    {
      "epoch": 1.4475416182733256,
      "grad_norm": 15.931320190429688,
      "learning_rate": 9.50273153525186e-06,
      "loss": 1.762,
      "step": 3739
    },
    {
      "epoch": 1.4479287650019357,
      "grad_norm": 16.423171997070312,
      "learning_rate": 9.502301372220072e-06,
      "loss": 1.8067,
      "step": 3740
    },
    {
      "epoch": 1.4483159117305457,
      "grad_norm": 19.281112670898438,
      "learning_rate": 9.501871209188283e-06,
      "loss": 1.1367,
      "step": 3741
    },
    {
      "epoch": 1.448703058459156,
      "grad_norm": 20.58602523803711,
      "learning_rate": 9.501441046156495e-06,
      "loss": 1.7134,
      "step": 3742
    },
    {
      "epoch": 1.449090205187766,
      "grad_norm": 13.015419960021973,
      "learning_rate": 9.501010883124704e-06,
      "loss": 1.4343,
      "step": 3743
    },
    {
      "epoch": 1.4494773519163764,
      "grad_norm": 23.519901275634766,
      "learning_rate": 9.500580720092916e-06,
      "loss": 1.346,
      "step": 3744
    },
    {
      "epoch": 1.4498644986449865,
      "grad_norm": 15.124411582946777,
      "learning_rate": 9.500150557061127e-06,
      "loss": 1.0134,
      "step": 3745
    },
    {
      "epoch": 1.4502516453735965,
      "grad_norm": 16.432985305786133,
      "learning_rate": 9.499720394029339e-06,
      "loss": 1.7082,
      "step": 3746
    },
    {
      "epoch": 1.4506387921022068,
      "grad_norm": 18.928054809570312,
      "learning_rate": 9.499290230997548e-06,
      "loss": 2.0152,
      "step": 3747
    },
    {
      "epoch": 1.4510259388308169,
      "grad_norm": 18.145671844482422,
      "learning_rate": 9.49886006796576e-06,
      "loss": 1.3595,
      "step": 3748
    },
    {
      "epoch": 1.451413085559427,
      "grad_norm": 34.345829010009766,
      "learning_rate": 9.498429904933971e-06,
      "loss": 1.9037,
      "step": 3749
    },
    {
      "epoch": 1.4518002322880372,
      "grad_norm": 33.20063018798828,
      "learning_rate": 9.497999741902183e-06,
      "loss": 2.0492,
      "step": 3750
    },
    {
      "epoch": 1.4521873790166473,
      "grad_norm": 11.700613021850586,
      "learning_rate": 9.497569578870392e-06,
      "loss": 1.0773,
      "step": 3751
    },
    {
      "epoch": 1.4525745257452574,
      "grad_norm": 13.939396858215332,
      "learning_rate": 9.497139415838604e-06,
      "loss": 1.0199,
      "step": 3752
    },
    {
      "epoch": 1.4529616724738676,
      "grad_norm": 18.99810791015625,
      "learning_rate": 9.496709252806815e-06,
      "loss": 1.8536,
      "step": 3753
    },
    {
      "epoch": 1.4533488192024777,
      "grad_norm": 12.221540451049805,
      "learning_rate": 9.496279089775025e-06,
      "loss": 1.9668,
      "step": 3754
    },
    {
      "epoch": 1.453735965931088,
      "grad_norm": 19.287878036499023,
      "learning_rate": 9.495848926743236e-06,
      "loss": 1.7545,
      "step": 3755
    },
    {
      "epoch": 1.454123112659698,
      "grad_norm": 41.00162887573242,
      "learning_rate": 9.495418763711448e-06,
      "loss": 1.5359,
      "step": 3756
    },
    {
      "epoch": 1.4545102593883081,
      "grad_norm": 17.30933952331543,
      "learning_rate": 9.494988600679659e-06,
      "loss": 1.513,
      "step": 3757
    },
    {
      "epoch": 1.4548974061169182,
      "grad_norm": 16.779428482055664,
      "learning_rate": 9.494558437647869e-06,
      "loss": 1.5719,
      "step": 3758
    },
    {
      "epoch": 1.4552845528455285,
      "grad_norm": 15.078276634216309,
      "learning_rate": 9.49412827461608e-06,
      "loss": 1.6094,
      "step": 3759
    },
    {
      "epoch": 1.4556716995741386,
      "grad_norm": 15.486088752746582,
      "learning_rate": 9.49369811158429e-06,
      "loss": 1.1272,
      "step": 3760
    },
    {
      "epoch": 1.4560588463027488,
      "grad_norm": 13.495430946350098,
      "learning_rate": 9.493267948552503e-06,
      "loss": 1.4662,
      "step": 3761
    },
    {
      "epoch": 1.456445993031359,
      "grad_norm": 17.047365188598633,
      "learning_rate": 9.492837785520713e-06,
      "loss": 1.7809,
      "step": 3762
    },
    {
      "epoch": 1.456833139759969,
      "grad_norm": 13.938576698303223,
      "learning_rate": 9.492407622488924e-06,
      "loss": 1.1026,
      "step": 3763
    },
    {
      "epoch": 1.457220286488579,
      "grad_norm": 21.14166259765625,
      "learning_rate": 9.491977459457134e-06,
      "loss": 0.995,
      "step": 3764
    },
    {
      "epoch": 1.4576074332171893,
      "grad_norm": 23.95529556274414,
      "learning_rate": 9.491547296425347e-06,
      "loss": 1.5835,
      "step": 3765
    },
    {
      "epoch": 1.4579945799457994,
      "grad_norm": 15.530014991760254,
      "learning_rate": 9.491117133393557e-06,
      "loss": 1.0761,
      "step": 3766
    },
    {
      "epoch": 1.4583817266744097,
      "grad_norm": 12.042799949645996,
      "learning_rate": 9.490686970361768e-06,
      "loss": 0.9698,
      "step": 3767
    },
    {
      "epoch": 1.4587688734030198,
      "grad_norm": 13.809616088867188,
      "learning_rate": 9.490256807329978e-06,
      "loss": 1.8818,
      "step": 3768
    },
    {
      "epoch": 1.4591560201316298,
      "grad_norm": 25.756975173950195,
      "learning_rate": 9.48982664429819e-06,
      "loss": 1.9839,
      "step": 3769
    },
    {
      "epoch": 1.45954316686024,
      "grad_norm": 19.921998977661133,
      "learning_rate": 9.4893964812664e-06,
      "loss": 1.9301,
      "step": 3770
    },
    {
      "epoch": 1.4599303135888502,
      "grad_norm": 32.842315673828125,
      "learning_rate": 9.488966318234612e-06,
      "loss": 1.5553,
      "step": 3771
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 8.364005088806152,
      "learning_rate": 9.488536155202822e-06,
      "loss": 0.3347,
      "step": 3772
    },
    {
      "epoch": 1.4607046070460705,
      "grad_norm": 14.907505989074707,
      "learning_rate": 9.488105992171033e-06,
      "loss": 1.1152,
      "step": 3773
    },
    {
      "epoch": 1.4610917537746806,
      "grad_norm": 13.670364379882812,
      "learning_rate": 9.487675829139245e-06,
      "loss": 1.4592,
      "step": 3774
    },
    {
      "epoch": 1.4614789005032907,
      "grad_norm": 20.31901741027832,
      "learning_rate": 9.487245666107454e-06,
      "loss": 1.7564,
      "step": 3775
    },
    {
      "epoch": 1.461866047231901,
      "grad_norm": 26.41939353942871,
      "learning_rate": 9.486815503075666e-06,
      "loss": 1.0709,
      "step": 3776
    },
    {
      "epoch": 1.462253193960511,
      "grad_norm": 17.201210021972656,
      "learning_rate": 9.486385340043877e-06,
      "loss": 2.4491,
      "step": 3777
    },
    {
      "epoch": 1.4626403406891213,
      "grad_norm": 24.953027725219727,
      "learning_rate": 9.485955177012089e-06,
      "loss": 1.5614,
      "step": 3778
    },
    {
      "epoch": 1.4630274874177314,
      "grad_norm": 17.116172790527344,
      "learning_rate": 9.485525013980298e-06,
      "loss": 1.5657,
      "step": 3779
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 21.199731826782227,
      "learning_rate": 9.485094850948512e-06,
      "loss": 1.2007,
      "step": 3780
    },
    {
      "epoch": 1.4638017808749515,
      "grad_norm": 22.08456802368164,
      "learning_rate": 9.484664687916721e-06,
      "loss": 1.8944,
      "step": 3781
    },
    {
      "epoch": 1.4641889276035618,
      "grad_norm": 22.918039321899414,
      "learning_rate": 9.484234524884933e-06,
      "loss": 1.3168,
      "step": 3782
    },
    {
      "epoch": 1.4645760743321719,
      "grad_norm": 22.70132827758789,
      "learning_rate": 9.483804361853142e-06,
      "loss": 1.965,
      "step": 3783
    },
    {
      "epoch": 1.4649632210607821,
      "grad_norm": 18.425886154174805,
      "learning_rate": 9.483374198821354e-06,
      "loss": 1.7478,
      "step": 3784
    },
    {
      "epoch": 1.4653503677893922,
      "grad_norm": 14.878325462341309,
      "learning_rate": 9.482944035789565e-06,
      "loss": 1.3124,
      "step": 3785
    },
    {
      "epoch": 1.4657375145180023,
      "grad_norm": 17.699195861816406,
      "learning_rate": 9.482513872757777e-06,
      "loss": 1.5911,
      "step": 3786
    },
    {
      "epoch": 1.4661246612466123,
      "grad_norm": 14.30493450164795,
      "learning_rate": 9.482083709725986e-06,
      "loss": 1.0362,
      "step": 3787
    },
    {
      "epoch": 1.4665118079752226,
      "grad_norm": 32.10088348388672,
      "learning_rate": 9.481653546694198e-06,
      "loss": 1.5444,
      "step": 3788
    },
    {
      "epoch": 1.4668989547038327,
      "grad_norm": 14.254013061523438,
      "learning_rate": 9.48122338366241e-06,
      "loss": 1.0433,
      "step": 3789
    },
    {
      "epoch": 1.467286101432443,
      "grad_norm": 13.330840110778809,
      "learning_rate": 9.480793220630619e-06,
      "loss": 1.4255,
      "step": 3790
    },
    {
      "epoch": 1.467673248161053,
      "grad_norm": 12.750984191894531,
      "learning_rate": 9.48036305759883e-06,
      "loss": 0.9021,
      "step": 3791
    },
    {
      "epoch": 1.4680603948896631,
      "grad_norm": 13.2025785446167,
      "learning_rate": 9.479932894567042e-06,
      "loss": 1.4881,
      "step": 3792
    },
    {
      "epoch": 1.4684475416182734,
      "grad_norm": 29.27882957458496,
      "learning_rate": 9.479502731535253e-06,
      "loss": 1.4746,
      "step": 3793
    },
    {
      "epoch": 1.4688346883468835,
      "grad_norm": 23.47199249267578,
      "learning_rate": 9.479072568503463e-06,
      "loss": 1.3723,
      "step": 3794
    },
    {
      "epoch": 1.4692218350754935,
      "grad_norm": 15.17221736907959,
      "learning_rate": 9.478642405471674e-06,
      "loss": 1.0551,
      "step": 3795
    },
    {
      "epoch": 1.4696089818041038,
      "grad_norm": 26.623550415039062,
      "learning_rate": 9.478212242439886e-06,
      "loss": 2.2621,
      "step": 3796
    },
    {
      "epoch": 1.4699961285327139,
      "grad_norm": 12.838220596313477,
      "learning_rate": 9.477782079408097e-06,
      "loss": 1.3606,
      "step": 3797
    },
    {
      "epoch": 1.470383275261324,
      "grad_norm": 20.185503005981445,
      "learning_rate": 9.477351916376307e-06,
      "loss": 2.2873,
      "step": 3798
    },
    {
      "epoch": 1.4707704219899342,
      "grad_norm": 20.242918014526367,
      "learning_rate": 9.476921753344518e-06,
      "loss": 1.7351,
      "step": 3799
    },
    {
      "epoch": 1.4711575687185443,
      "grad_norm": 15.935962677001953,
      "learning_rate": 9.47649159031273e-06,
      "loss": 1.4324,
      "step": 3800
    },
    {
      "epoch": 1.4715447154471546,
      "grad_norm": 13.154728889465332,
      "learning_rate": 9.476061427280941e-06,
      "loss": 0.9234,
      "step": 3801
    },
    {
      "epoch": 1.4719318621757647,
      "grad_norm": 19.310834884643555,
      "learning_rate": 9.47563126424915e-06,
      "loss": 1.9493,
      "step": 3802
    },
    {
      "epoch": 1.4723190089043747,
      "grad_norm": 15.071966171264648,
      "learning_rate": 9.475201101217362e-06,
      "loss": 1.6068,
      "step": 3803
    },
    {
      "epoch": 1.4727061556329848,
      "grad_norm": 19.867849349975586,
      "learning_rate": 9.474770938185574e-06,
      "loss": 1.7278,
      "step": 3804
    },
    {
      "epoch": 1.473093302361595,
      "grad_norm": 18.536296844482422,
      "learning_rate": 9.474340775153783e-06,
      "loss": 1.6153,
      "step": 3805
    },
    {
      "epoch": 1.4734804490902051,
      "grad_norm": 14.365636825561523,
      "learning_rate": 9.473910612121995e-06,
      "loss": 0.594,
      "step": 3806
    },
    {
      "epoch": 1.4738675958188154,
      "grad_norm": 12.324121475219727,
      "learning_rate": 9.473480449090206e-06,
      "loss": 0.7482,
      "step": 3807
    },
    {
      "epoch": 1.4742547425474255,
      "grad_norm": 17.62993812561035,
      "learning_rate": 9.473050286058418e-06,
      "loss": 1.3446,
      "step": 3808
    },
    {
      "epoch": 1.4746418892760356,
      "grad_norm": 13.91637134552002,
      "learning_rate": 9.472620123026627e-06,
      "loss": 1.1237,
      "step": 3809
    },
    {
      "epoch": 1.4750290360046456,
      "grad_norm": 23.11749267578125,
      "learning_rate": 9.472189959994839e-06,
      "loss": 1.644,
      "step": 3810
    },
    {
      "epoch": 1.475416182733256,
      "grad_norm": 15.406393051147461,
      "learning_rate": 9.471759796963048e-06,
      "loss": 1.3487,
      "step": 3811
    },
    {
      "epoch": 1.475803329461866,
      "grad_norm": 22.918996810913086,
      "learning_rate": 9.471329633931262e-06,
      "loss": 2.5641,
      "step": 3812
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 19.608362197875977,
      "learning_rate": 9.470899470899471e-06,
      "loss": 1.9891,
      "step": 3813
    },
    {
      "epoch": 1.4765776229190863,
      "grad_norm": 22.134769439697266,
      "learning_rate": 9.470469307867683e-06,
      "loss": 1.7572,
      "step": 3814
    },
    {
      "epoch": 1.4769647696476964,
      "grad_norm": 20.426788330078125,
      "learning_rate": 9.470039144835892e-06,
      "loss": 1.3754,
      "step": 3815
    },
    {
      "epoch": 1.4773519163763067,
      "grad_norm": 13.061929702758789,
      "learning_rate": 9.469608981804106e-06,
      "loss": 1.0661,
      "step": 3816
    },
    {
      "epoch": 1.4777390631049168,
      "grad_norm": 11.091650009155273,
      "learning_rate": 9.469178818772315e-06,
      "loss": 1.3372,
      "step": 3817
    },
    {
      "epoch": 1.4781262098335268,
      "grad_norm": 13.021749496459961,
      "learning_rate": 9.468748655740527e-06,
      "loss": 1.2059,
      "step": 3818
    },
    {
      "epoch": 1.4785133565621371,
      "grad_norm": 18.970430374145508,
      "learning_rate": 9.468318492708738e-06,
      "loss": 1.0455,
      "step": 3819
    },
    {
      "epoch": 1.4789005032907472,
      "grad_norm": 27.089801788330078,
      "learning_rate": 9.467888329676948e-06,
      "loss": 2.2514,
      "step": 3820
    },
    {
      "epoch": 1.4792876500193572,
      "grad_norm": 18.355806350708008,
      "learning_rate": 9.46745816664516e-06,
      "loss": 2.3771,
      "step": 3821
    },
    {
      "epoch": 1.4796747967479675,
      "grad_norm": 20.29918670654297,
      "learning_rate": 9.46702800361337e-06,
      "loss": 2.3622,
      "step": 3822
    },
    {
      "epoch": 1.4800619434765776,
      "grad_norm": 19.230262756347656,
      "learning_rate": 9.466597840581582e-06,
      "loss": 2.1505,
      "step": 3823
    },
    {
      "epoch": 1.480449090205188,
      "grad_norm": 19.211597442626953,
      "learning_rate": 9.466167677549792e-06,
      "loss": 1.9099,
      "step": 3824
    },
    {
      "epoch": 1.480836236933798,
      "grad_norm": 20.236154556274414,
      "learning_rate": 9.465737514518003e-06,
      "loss": 2.2555,
      "step": 3825
    },
    {
      "epoch": 1.481223383662408,
      "grad_norm": 14.391087532043457,
      "learning_rate": 9.465307351486213e-06,
      "loss": 1.8834,
      "step": 3826
    },
    {
      "epoch": 1.481610530391018,
      "grad_norm": 19.4555721282959,
      "learning_rate": 9.464877188454426e-06,
      "loss": 1.4196,
      "step": 3827
    },
    {
      "epoch": 1.4819976771196284,
      "grad_norm": 25.94155502319336,
      "learning_rate": 9.464447025422636e-06,
      "loss": 2.3253,
      "step": 3828
    },
    {
      "epoch": 1.4823848238482384,
      "grad_norm": 12.648688316345215,
      "learning_rate": 9.464016862390847e-06,
      "loss": 0.88,
      "step": 3829
    },
    {
      "epoch": 1.4827719705768487,
      "grad_norm": 14.392012596130371,
      "learning_rate": 9.463586699359057e-06,
      "loss": 1.4664,
      "step": 3830
    },
    {
      "epoch": 1.4831591173054588,
      "grad_norm": 26.383689880371094,
      "learning_rate": 9.46315653632727e-06,
      "loss": 1.5308,
      "step": 3831
    },
    {
      "epoch": 1.4835462640340689,
      "grad_norm": 17.51293182373047,
      "learning_rate": 9.46272637329548e-06,
      "loss": 2.0361,
      "step": 3832
    },
    {
      "epoch": 1.483933410762679,
      "grad_norm": 11.99791145324707,
      "learning_rate": 9.462296210263691e-06,
      "loss": 1.4206,
      "step": 3833
    },
    {
      "epoch": 1.4843205574912892,
      "grad_norm": 12.287755966186523,
      "learning_rate": 9.461866047231901e-06,
      "loss": 1.0482,
      "step": 3834
    },
    {
      "epoch": 1.4847077042198993,
      "grad_norm": 37.76327896118164,
      "learning_rate": 9.461435884200112e-06,
      "loss": 1.4854,
      "step": 3835
    },
    {
      "epoch": 1.4850948509485096,
      "grad_norm": 14.521543502807617,
      "learning_rate": 9.461005721168324e-06,
      "loss": 1.5964,
      "step": 3836
    },
    {
      "epoch": 1.4854819976771196,
      "grad_norm": 11.423442840576172,
      "learning_rate": 9.460575558136535e-06,
      "loss": 0.967,
      "step": 3837
    },
    {
      "epoch": 1.4858691444057297,
      "grad_norm": 11.223771095275879,
      "learning_rate": 9.460145395104745e-06,
      "loss": 1.3517,
      "step": 3838
    },
    {
      "epoch": 1.48625629113434,
      "grad_norm": 33.31816864013672,
      "learning_rate": 9.459715232072956e-06,
      "loss": 1.5945,
      "step": 3839
    },
    {
      "epoch": 1.48664343786295,
      "grad_norm": 12.590106964111328,
      "learning_rate": 9.459285069041168e-06,
      "loss": 0.8432,
      "step": 3840
    },
    {
      "epoch": 1.4870305845915601,
      "grad_norm": 12.973419189453125,
      "learning_rate": 9.458854906009377e-06,
      "loss": 0.9188,
      "step": 3841
    },
    {
      "epoch": 1.4874177313201704,
      "grad_norm": 14.774730682373047,
      "learning_rate": 9.458424742977589e-06,
      "loss": 0.9469,
      "step": 3842
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 14.100372314453125,
      "learning_rate": 9.4579945799458e-06,
      "loss": 1.6444,
      "step": 3843
    },
    {
      "epoch": 1.4881920247773905,
      "grad_norm": 18.767030715942383,
      "learning_rate": 9.457564416914012e-06,
      "loss": 1.8925,
      "step": 3844
    },
    {
      "epoch": 1.4885791715060008,
      "grad_norm": 21.492691040039062,
      "learning_rate": 9.457134253882221e-06,
      "loss": 2.5096,
      "step": 3845
    },
    {
      "epoch": 1.488966318234611,
      "grad_norm": 20.60612678527832,
      "learning_rate": 9.456704090850433e-06,
      "loss": 1.5089,
      "step": 3846
    },
    {
      "epoch": 1.4893534649632212,
      "grad_norm": 15.58771800994873,
      "learning_rate": 9.456273927818644e-06,
      "loss": 1.6018,
      "step": 3847
    },
    {
      "epoch": 1.4897406116918313,
      "grad_norm": 14.651637077331543,
      "learning_rate": 9.455843764786856e-06,
      "loss": 1.7536,
      "step": 3848
    },
    {
      "epoch": 1.4901277584204413,
      "grad_norm": 14.798362731933594,
      "learning_rate": 9.455413601755065e-06,
      "loss": 1.5822,
      "step": 3849
    },
    {
      "epoch": 1.4905149051490514,
      "grad_norm": 23.820688247680664,
      "learning_rate": 9.454983438723277e-06,
      "loss": 1.3638,
      "step": 3850
    },
    {
      "epoch": 1.4909020518776617,
      "grad_norm": 28.43178939819336,
      "learning_rate": 9.454553275691488e-06,
      "loss": 1.5871,
      "step": 3851
    },
    {
      "epoch": 1.4912891986062717,
      "grad_norm": 13.702770233154297,
      "learning_rate": 9.4541231126597e-06,
      "loss": 1.3954,
      "step": 3852
    },
    {
      "epoch": 1.491676345334882,
      "grad_norm": 12.753701210021973,
      "learning_rate": 9.45369294962791e-06,
      "loss": 1.0322,
      "step": 3853
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 15.69642448425293,
      "learning_rate": 9.45326278659612e-06,
      "loss": 1.5612,
      "step": 3854
    },
    {
      "epoch": 1.4924506387921022,
      "grad_norm": 20.41327667236328,
      "learning_rate": 9.452832623564332e-06,
      "loss": 1.7093,
      "step": 3855
    },
    {
      "epoch": 1.4928377855207122,
      "grad_norm": 16.76844024658203,
      "learning_rate": 9.452402460532542e-06,
      "loss": 1.075,
      "step": 3856
    },
    {
      "epoch": 1.4932249322493225,
      "grad_norm": 22.291200637817383,
      "learning_rate": 9.451972297500753e-06,
      "loss": 1.436,
      "step": 3857
    },
    {
      "epoch": 1.4936120789779326,
      "grad_norm": 12.689984321594238,
      "learning_rate": 9.451542134468965e-06,
      "loss": 1.0705,
      "step": 3858
    },
    {
      "epoch": 1.4939992257065429,
      "grad_norm": 16.753707885742188,
      "learning_rate": 9.451111971437176e-06,
      "loss": 1.1939,
      "step": 3859
    },
    {
      "epoch": 1.494386372435153,
      "grad_norm": 15.32621955871582,
      "learning_rate": 9.450681808405386e-06,
      "loss": 1.0978,
      "step": 3860
    },
    {
      "epoch": 1.494773519163763,
      "grad_norm": 24.219873428344727,
      "learning_rate": 9.450251645373597e-06,
      "loss": 1.3453,
      "step": 3861
    },
    {
      "epoch": 1.4951606658923733,
      "grad_norm": 22.278968811035156,
      "learning_rate": 9.449821482341809e-06,
      "loss": 1.5217,
      "step": 3862
    },
    {
      "epoch": 1.4955478126209834,
      "grad_norm": 16.611326217651367,
      "learning_rate": 9.44939131931002e-06,
      "loss": 1.2218,
      "step": 3863
    },
    {
      "epoch": 1.4959349593495934,
      "grad_norm": 16.864238739013672,
      "learning_rate": 9.44896115627823e-06,
      "loss": 0.9536,
      "step": 3864
    },
    {
      "epoch": 1.4963221060782037,
      "grad_norm": 17.514108657836914,
      "learning_rate": 9.448530993246441e-06,
      "loss": 1.7641,
      "step": 3865
    },
    {
      "epoch": 1.4967092528068138,
      "grad_norm": 22.80377197265625,
      "learning_rate": 9.448100830214653e-06,
      "loss": 2.2043,
      "step": 3866
    },
    {
      "epoch": 1.4970963995354238,
      "grad_norm": 15.426070213317871,
      "learning_rate": 9.447670667182864e-06,
      "loss": 1.5306,
      "step": 3867
    },
    {
      "epoch": 1.4974835462640341,
      "grad_norm": 29.72636604309082,
      "learning_rate": 9.447240504151074e-06,
      "loss": 1.5082,
      "step": 3868
    },
    {
      "epoch": 1.4978706929926442,
      "grad_norm": 32.02777099609375,
      "learning_rate": 9.446810341119285e-06,
      "loss": 2.0202,
      "step": 3869
    },
    {
      "epoch": 1.4982578397212545,
      "grad_norm": 15.311335563659668,
      "learning_rate": 9.446380178087497e-06,
      "loss": 1.5723,
      "step": 3870
    },
    {
      "epoch": 1.4986449864498645,
      "grad_norm": 26.624263763427734,
      "learning_rate": 9.445950015055706e-06,
      "loss": 1.9573,
      "step": 3871
    },
    {
      "epoch": 1.4990321331784746,
      "grad_norm": 15.075349807739258,
      "learning_rate": 9.445519852023918e-06,
      "loss": 1.5592,
      "step": 3872
    },
    {
      "epoch": 1.4994192799070847,
      "grad_norm": 12.935165405273438,
      "learning_rate": 9.445089688992129e-06,
      "loss": 1.0046,
      "step": 3873
    },
    {
      "epoch": 1.499806426635695,
      "grad_norm": 38.7092399597168,
      "learning_rate": 9.44465952596034e-06,
      "loss": 2.7376,
      "step": 3874
    },
    {
      "epoch": 1.500193573364305,
      "grad_norm": 19.47774887084961,
      "learning_rate": 9.44422936292855e-06,
      "loss": 2.084,
      "step": 3875
    },
    {
      "epoch": 1.5005807200929153,
      "grad_norm": 14.522294044494629,
      "learning_rate": 9.443799199896762e-06,
      "loss": 1.5467,
      "step": 3876
    },
    {
      "epoch": 1.5009678668215254,
      "grad_norm": 17.018680572509766,
      "learning_rate": 9.443369036864971e-06,
      "loss": 1.6893,
      "step": 3877
    },
    {
      "epoch": 1.5013550135501355,
      "grad_norm": 15.505231857299805,
      "learning_rate": 9.442938873833185e-06,
      "loss": 1.4417,
      "step": 3878
    },
    {
      "epoch": 1.5017421602787455,
      "grad_norm": 36.97850799560547,
      "learning_rate": 9.442508710801394e-06,
      "loss": 1.802,
      "step": 3879
    },
    {
      "epoch": 1.5021293070073558,
      "grad_norm": 24.041025161743164,
      "learning_rate": 9.442078547769606e-06,
      "loss": 1.8935,
      "step": 3880
    },
    {
      "epoch": 1.502516453735966,
      "grad_norm": 22.990543365478516,
      "learning_rate": 9.441648384737815e-06,
      "loss": 2.2231,
      "step": 3881
    },
    {
      "epoch": 1.5029036004645762,
      "grad_norm": 23.483055114746094,
      "learning_rate": 9.441218221706028e-06,
      "loss": 1.3042,
      "step": 3882
    },
    {
      "epoch": 1.5032907471931862,
      "grad_norm": 23.369346618652344,
      "learning_rate": 9.440788058674238e-06,
      "loss": 2.0418,
      "step": 3883
    },
    {
      "epoch": 1.5036778939217963,
      "grad_norm": 23.211944580078125,
      "learning_rate": 9.44035789564245e-06,
      "loss": 1.6773,
      "step": 3884
    },
    {
      "epoch": 1.5040650406504064,
      "grad_norm": 20.211036682128906,
      "learning_rate": 9.43992773261066e-06,
      "loss": 1.3008,
      "step": 3885
    },
    {
      "epoch": 1.5044521873790166,
      "grad_norm": 19.26239013671875,
      "learning_rate": 9.43949756957887e-06,
      "loss": 2.2753,
      "step": 3886
    },
    {
      "epoch": 1.504839334107627,
      "grad_norm": 19.2429141998291,
      "learning_rate": 9.439067406547082e-06,
      "loss": 1.2503,
      "step": 3887
    },
    {
      "epoch": 1.505226480836237,
      "grad_norm": 21.462247848510742,
      "learning_rate": 9.438637243515294e-06,
      "loss": 2.2055,
      "step": 3888
    },
    {
      "epoch": 1.505613627564847,
      "grad_norm": 13.353675842285156,
      "learning_rate": 9.438207080483503e-06,
      "loss": 0.9216,
      "step": 3889
    },
    {
      "epoch": 1.5060007742934571,
      "grad_norm": 11.640013694763184,
      "learning_rate": 9.437776917451715e-06,
      "loss": 1.3952,
      "step": 3890
    },
    {
      "epoch": 1.5063879210220672,
      "grad_norm": 18.978132247924805,
      "learning_rate": 9.437346754419926e-06,
      "loss": 1.1646,
      "step": 3891
    },
    {
      "epoch": 1.5067750677506775,
      "grad_norm": 14.71306324005127,
      "learning_rate": 9.436916591388136e-06,
      "loss": 0.6364,
      "step": 3892
    },
    {
      "epoch": 1.5071622144792878,
      "grad_norm": 23.786497116088867,
      "learning_rate": 9.436486428356347e-06,
      "loss": 1.8545,
      "step": 3893
    },
    {
      "epoch": 1.5075493612078978,
      "grad_norm": 20.55413818359375,
      "learning_rate": 9.436056265324559e-06,
      "loss": 1.7539,
      "step": 3894
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 10.08866024017334,
      "learning_rate": 9.43562610229277e-06,
      "loss": 1.1969,
      "step": 3895
    },
    {
      "epoch": 1.508323654665118,
      "grad_norm": 33.55976104736328,
      "learning_rate": 9.43519593926098e-06,
      "loss": 1.1679,
      "step": 3896
    },
    {
      "epoch": 1.5087108013937283,
      "grad_norm": 26.86595344543457,
      "learning_rate": 9.434765776229191e-06,
      "loss": 0.9961,
      "step": 3897
    },
    {
      "epoch": 1.5090979481223383,
      "grad_norm": 13.73779010772705,
      "learning_rate": 9.434335613197403e-06,
      "loss": 1.0918,
      "step": 3898
    },
    {
      "epoch": 1.5094850948509486,
      "grad_norm": 12.445355415344238,
      "learning_rate": 9.433905450165614e-06,
      "loss": 0.6774,
      "step": 3899
    },
    {
      "epoch": 1.5098722415795587,
      "grad_norm": 13.097793579101562,
      "learning_rate": 9.433475287133824e-06,
      "loss": 1.0742,
      "step": 3900
    },
    {
      "epoch": 1.5102593883081687,
      "grad_norm": 27.882577896118164,
      "learning_rate": 9.433045124102035e-06,
      "loss": 1.7419,
      "step": 3901
    },
    {
      "epoch": 1.5106465350367788,
      "grad_norm": 11.622425079345703,
      "learning_rate": 9.432614961070247e-06,
      "loss": 0.8681,
      "step": 3902
    },
    {
      "epoch": 1.511033681765389,
      "grad_norm": 31.147403717041016,
      "learning_rate": 9.432184798038458e-06,
      "loss": 1.384,
      "step": 3903
    },
    {
      "epoch": 1.5114208284939994,
      "grad_norm": 37.17719268798828,
      "learning_rate": 9.431754635006668e-06,
      "loss": 2.1155,
      "step": 3904
    },
    {
      "epoch": 1.5118079752226095,
      "grad_norm": 16.17116355895996,
      "learning_rate": 9.43132447197488e-06,
      "loss": 1.6359,
      "step": 3905
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 12.613065719604492,
      "learning_rate": 9.43089430894309e-06,
      "loss": 2.1252,
      "step": 3906
    },
    {
      "epoch": 1.5125822686798296,
      "grad_norm": 16.115129470825195,
      "learning_rate": 9.4304641459113e-06,
      "loss": 1.0522,
      "step": 3907
    },
    {
      "epoch": 1.5129694154084397,
      "grad_norm": 13.471986770629883,
      "learning_rate": 9.430033982879512e-06,
      "loss": 1.1583,
      "step": 3908
    },
    {
      "epoch": 1.51335656213705,
      "grad_norm": 17.46193504333496,
      "learning_rate": 9.429603819847723e-06,
      "loss": 1.6651,
      "step": 3909
    },
    {
      "epoch": 1.5137437088656602,
      "grad_norm": 20.066329956054688,
      "learning_rate": 9.429173656815935e-06,
      "loss": 1.4204,
      "step": 3910
    },
    {
      "epoch": 1.5141308555942703,
      "grad_norm": 15.097694396972656,
      "learning_rate": 9.428743493784144e-06,
      "loss": 1.1588,
      "step": 3911
    },
    {
      "epoch": 1.5145180023228804,
      "grad_norm": 23.959217071533203,
      "learning_rate": 9.428313330752356e-06,
      "loss": 2.1686,
      "step": 3912
    },
    {
      "epoch": 1.5149051490514904,
      "grad_norm": 26.256126403808594,
      "learning_rate": 9.427883167720567e-06,
      "loss": 2.4759,
      "step": 3913
    },
    {
      "epoch": 1.5152922957801005,
      "grad_norm": 12.944364547729492,
      "learning_rate": 9.427453004688779e-06,
      "loss": 1.4366,
      "step": 3914
    },
    {
      "epoch": 1.5156794425087108,
      "grad_norm": 86.43086242675781,
      "learning_rate": 9.427022841656988e-06,
      "loss": 1.9852,
      "step": 3915
    },
    {
      "epoch": 1.516066589237321,
      "grad_norm": 13.451542854309082,
      "learning_rate": 9.4265926786252e-06,
      "loss": 1.6871,
      "step": 3916
    },
    {
      "epoch": 1.5164537359659311,
      "grad_norm": 22.11095428466797,
      "learning_rate": 9.426162515593411e-06,
      "loss": 2.0388,
      "step": 3917
    },
    {
      "epoch": 1.5168408826945412,
      "grad_norm": 6.704661846160889,
      "learning_rate": 9.425732352561623e-06,
      "loss": 0.4669,
      "step": 3918
    },
    {
      "epoch": 1.5172280294231513,
      "grad_norm": 16.158058166503906,
      "learning_rate": 9.425302189529832e-06,
      "loss": 1.184,
      "step": 3919
    },
    {
      "epoch": 1.5176151761517616,
      "grad_norm": 21.858524322509766,
      "learning_rate": 9.424872026498044e-06,
      "loss": 1.6067,
      "step": 3920
    },
    {
      "epoch": 1.5180023228803716,
      "grad_norm": 21.612056732177734,
      "learning_rate": 9.424441863466255e-06,
      "loss": 1.4601,
      "step": 3921
    },
    {
      "epoch": 1.518389469608982,
      "grad_norm": 23.2360897064209,
      "learning_rate": 9.424011700434465e-06,
      "loss": 1.561,
      "step": 3922
    },
    {
      "epoch": 1.518776616337592,
      "grad_norm": 16.678388595581055,
      "learning_rate": 9.423581537402676e-06,
      "loss": 1.7277,
      "step": 3923
    },
    {
      "epoch": 1.519163763066202,
      "grad_norm": 13.514790534973145,
      "learning_rate": 9.423151374370888e-06,
      "loss": 1.0079,
      "step": 3924
    },
    {
      "epoch": 1.519550909794812,
      "grad_norm": 11.047492027282715,
      "learning_rate": 9.422721211339099e-06,
      "loss": 1.172,
      "step": 3925
    },
    {
      "epoch": 1.5199380565234224,
      "grad_norm": 21.837682723999023,
      "learning_rate": 9.422291048307309e-06,
      "loss": 2.0483,
      "step": 3926
    },
    {
      "epoch": 1.5203252032520327,
      "grad_norm": 15.495177268981934,
      "learning_rate": 9.42186088527552e-06,
      "loss": 0.9523,
      "step": 3927
    },
    {
      "epoch": 1.5207123499806428,
      "grad_norm": 17.577999114990234,
      "learning_rate": 9.42143072224373e-06,
      "loss": 1.2081,
      "step": 3928
    },
    {
      "epoch": 1.5210994967092528,
      "grad_norm": 14.941057205200195,
      "learning_rate": 9.421000559211943e-06,
      "loss": 1.344,
      "step": 3929
    },
    {
      "epoch": 1.5214866434378629,
      "grad_norm": 13.183587074279785,
      "learning_rate": 9.420570396180153e-06,
      "loss": 0.8979,
      "step": 3930
    },
    {
      "epoch": 1.521873790166473,
      "grad_norm": 15.241125106811523,
      "learning_rate": 9.420140233148364e-06,
      "loss": 1.7059,
      "step": 3931
    },
    {
      "epoch": 1.5222609368950832,
      "grad_norm": 13.749785423278809,
      "learning_rate": 9.419710070116574e-06,
      "loss": 1.3549,
      "step": 3932
    },
    {
      "epoch": 1.5226480836236935,
      "grad_norm": 17.42852210998535,
      "learning_rate": 9.419279907084787e-06,
      "loss": 1.6334,
      "step": 3933
    },
    {
      "epoch": 1.5230352303523036,
      "grad_norm": 13.074681282043457,
      "learning_rate": 9.418849744052997e-06,
      "loss": 0.8865,
      "step": 3934
    },
    {
      "epoch": 1.5234223770809137,
      "grad_norm": 22.834720611572266,
      "learning_rate": 9.418419581021208e-06,
      "loss": 1.7812,
      "step": 3935
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 15.491644859313965,
      "learning_rate": 9.417989417989418e-06,
      "loss": 0.8299,
      "step": 3936
    },
    {
      "epoch": 1.5241966705381338,
      "grad_norm": 14.28906536102295,
      "learning_rate": 9.41755925495763e-06,
      "loss": 0.5199,
      "step": 3937
    },
    {
      "epoch": 1.524583817266744,
      "grad_norm": 16.186254501342773,
      "learning_rate": 9.41712909192584e-06,
      "loss": 1.1357,
      "step": 3938
    },
    {
      "epoch": 1.5249709639953544,
      "grad_norm": 19.905899047851562,
      "learning_rate": 9.416698928894052e-06,
      "loss": 2.288,
      "step": 3939
    },
    {
      "epoch": 1.5253581107239644,
      "grad_norm": 18.550222396850586,
      "learning_rate": 9.416268765862262e-06,
      "loss": 1.8623,
      "step": 3940
    },
    {
      "epoch": 1.5257452574525745,
      "grad_norm": 25.147581100463867,
      "learning_rate": 9.415838602830473e-06,
      "loss": 1.6018,
      "step": 3941
    },
    {
      "epoch": 1.5261324041811846,
      "grad_norm": 22.822059631347656,
      "learning_rate": 9.415408439798685e-06,
      "loss": 2.5317,
      "step": 3942
    },
    {
      "epoch": 1.5265195509097949,
      "grad_norm": 17.51810073852539,
      "learning_rate": 9.414978276766894e-06,
      "loss": 1.5012,
      "step": 3943
    },
    {
      "epoch": 1.526906697638405,
      "grad_norm": 15.397089958190918,
      "learning_rate": 9.414548113735107e-06,
      "loss": 1.6265,
      "step": 3944
    },
    {
      "epoch": 1.5272938443670152,
      "grad_norm": 19.187231063842773,
      "learning_rate": 9.414117950703317e-06,
      "loss": 1.6169,
      "step": 3945
    },
    {
      "epoch": 1.5276809910956253,
      "grad_norm": 16.254732131958008,
      "learning_rate": 9.413687787671529e-06,
      "loss": 1.1416,
      "step": 3946
    },
    {
      "epoch": 1.5280681378242353,
      "grad_norm": 13.0378999710083,
      "learning_rate": 9.413257624639738e-06,
      "loss": 0.8379,
      "step": 3947
    },
    {
      "epoch": 1.5284552845528454,
      "grad_norm": 12.8118257522583,
      "learning_rate": 9.412827461607951e-06,
      "loss": 1.2129,
      "step": 3948
    },
    {
      "epoch": 1.5288424312814557,
      "grad_norm": 14.828121185302734,
      "learning_rate": 9.412397298576161e-06,
      "loss": 0.438,
      "step": 3949
    },
    {
      "epoch": 1.5292295780100658,
      "grad_norm": 11.931756019592285,
      "learning_rate": 9.411967135544373e-06,
      "loss": 0.8421,
      "step": 3950
    },
    {
      "epoch": 1.529616724738676,
      "grad_norm": 14.859469413757324,
      "learning_rate": 9.411536972512582e-06,
      "loss": 1.5207,
      "step": 3951
    },
    {
      "epoch": 1.5300038714672861,
      "grad_norm": 12.964058876037598,
      "learning_rate": 9.411106809480794e-06,
      "loss": 0.8251,
      "step": 3952
    },
    {
      "epoch": 1.5303910181958962,
      "grad_norm": 23.955738067626953,
      "learning_rate": 9.410676646449005e-06,
      "loss": 1.3702,
      "step": 3953
    },
    {
      "epoch": 1.5307781649245062,
      "grad_norm": 14.399362564086914,
      "learning_rate": 9.410246483417217e-06,
      "loss": 1.5559,
      "step": 3954
    },
    {
      "epoch": 1.5311653116531165,
      "grad_norm": 20.977088928222656,
      "learning_rate": 9.409816320385426e-06,
      "loss": 0.9933,
      "step": 3955
    },
    {
      "epoch": 1.5315524583817268,
      "grad_norm": 15.495823860168457,
      "learning_rate": 9.409386157353638e-06,
      "loss": 1.096,
      "step": 3956
    },
    {
      "epoch": 1.5319396051103369,
      "grad_norm": 14.730125427246094,
      "learning_rate": 9.408955994321849e-06,
      "loss": 1.5583,
      "step": 3957
    },
    {
      "epoch": 1.532326751838947,
      "grad_norm": 15.700297355651855,
      "learning_rate": 9.408525831290059e-06,
      "loss": 1.2222,
      "step": 3958
    },
    {
      "epoch": 1.532713898567557,
      "grad_norm": 21.24092674255371,
      "learning_rate": 9.40809566825827e-06,
      "loss": 1.3141,
      "step": 3959
    },
    {
      "epoch": 1.533101045296167,
      "grad_norm": 20.40489387512207,
      "learning_rate": 9.407665505226482e-06,
      "loss": 1.8391,
      "step": 3960
    },
    {
      "epoch": 1.5334881920247774,
      "grad_norm": 16.544797897338867,
      "learning_rate": 9.407235342194693e-06,
      "loss": 1.3968,
      "step": 3961
    },
    {
      "epoch": 1.5338753387533877,
      "grad_norm": 14.599631309509277,
      "learning_rate": 9.406805179162903e-06,
      "loss": 0.9567,
      "step": 3962
    },
    {
      "epoch": 1.5342624854819977,
      "grad_norm": 8.073845863342285,
      "learning_rate": 9.406375016131114e-06,
      "loss": 0.4393,
      "step": 3963
    },
    {
      "epoch": 1.5346496322106078,
      "grad_norm": 29.411725997924805,
      "learning_rate": 9.405944853099326e-06,
      "loss": 2.4947,
      "step": 3964
    },
    {
      "epoch": 1.5350367789392179,
      "grad_norm": 17.32366943359375,
      "learning_rate": 9.405514690067537e-06,
      "loss": 0.788,
      "step": 3965
    },
    {
      "epoch": 1.5354239256678281,
      "grad_norm": 14.597768783569336,
      "learning_rate": 9.405084527035747e-06,
      "loss": 1.4308,
      "step": 3966
    },
    {
      "epoch": 1.5358110723964382,
      "grad_norm": 31.09840202331543,
      "learning_rate": 9.404654364003958e-06,
      "loss": 1.6618,
      "step": 3967
    },
    {
      "epoch": 1.5361982191250485,
      "grad_norm": 18.076019287109375,
      "learning_rate": 9.40422420097217e-06,
      "loss": 1.0749,
      "step": 3968
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 10.876916885375977,
      "learning_rate": 9.403794037940381e-06,
      "loss": 1.1488,
      "step": 3969
    },
    {
      "epoch": 1.5369725125822686,
      "grad_norm": 36.41008758544922,
      "learning_rate": 9.40336387490859e-06,
      "loss": 1.11,
      "step": 3970
    },
    {
      "epoch": 1.5373596593108787,
      "grad_norm": 20.07271385192871,
      "learning_rate": 9.402933711876802e-06,
      "loss": 2.0825,
      "step": 3971
    },
    {
      "epoch": 1.537746806039489,
      "grad_norm": 29.33096694946289,
      "learning_rate": 9.402503548845014e-06,
      "loss": 2.0868,
      "step": 3972
    },
    {
      "epoch": 1.538133952768099,
      "grad_norm": 18.996341705322266,
      "learning_rate": 9.402073385813223e-06,
      "loss": 1.3752,
      "step": 3973
    },
    {
      "epoch": 1.5385210994967093,
      "grad_norm": 23.78791618347168,
      "learning_rate": 9.401643222781435e-06,
      "loss": 0.8033,
      "step": 3974
    },
    {
      "epoch": 1.5389082462253194,
      "grad_norm": 12.820208549499512,
      "learning_rate": 9.401213059749646e-06,
      "loss": 1.1945,
      "step": 3975
    },
    {
      "epoch": 1.5392953929539295,
      "grad_norm": 14.268813133239746,
      "learning_rate": 9.400782896717858e-06,
      "loss": 1.2348,
      "step": 3976
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 8.762625694274902,
      "learning_rate": 9.400352733686067e-06,
      "loss": 1.5827,
      "step": 3977
    },
    {
      "epoch": 1.5400696864111498,
      "grad_norm": 18.558727264404297,
      "learning_rate": 9.399922570654279e-06,
      "loss": 1.199,
      "step": 3978
    },
    {
      "epoch": 1.5404568331397601,
      "grad_norm": 17.79087257385254,
      "learning_rate": 9.399492407622488e-06,
      "loss": 1.8586,
      "step": 3979
    },
    {
      "epoch": 1.5408439798683702,
      "grad_norm": 16.22873878479004,
      "learning_rate": 9.399062244590701e-06,
      "loss": 0.9386,
      "step": 3980
    },
    {
      "epoch": 1.5412311265969802,
      "grad_norm": 15.530988693237305,
      "learning_rate": 9.398632081558911e-06,
      "loss": 1.0475,
      "step": 3981
    },
    {
      "epoch": 1.5416182733255903,
      "grad_norm": 27.44196128845215,
      "learning_rate": 9.398201918527123e-06,
      "loss": 1.2622,
      "step": 3982
    },
    {
      "epoch": 1.5420054200542004,
      "grad_norm": 12.762165069580078,
      "learning_rate": 9.397771755495334e-06,
      "loss": 1.2465,
      "step": 3983
    },
    {
      "epoch": 1.5423925667828107,
      "grad_norm": 33.533870697021484,
      "learning_rate": 9.397341592463545e-06,
      "loss": 1.8584,
      "step": 3984
    },
    {
      "epoch": 1.542779713511421,
      "grad_norm": 14.593361854553223,
      "learning_rate": 9.396911429431755e-06,
      "loss": 1.3712,
      "step": 3985
    },
    {
      "epoch": 1.543166860240031,
      "grad_norm": 21.035036087036133,
      "learning_rate": 9.396481266399967e-06,
      "loss": 1.3071,
      "step": 3986
    },
    {
      "epoch": 1.543554006968641,
      "grad_norm": 25.665224075317383,
      "learning_rate": 9.396051103368178e-06,
      "loss": 1.4736,
      "step": 3987
    },
    {
      "epoch": 1.5439411536972512,
      "grad_norm": 20.98574447631836,
      "learning_rate": 9.395620940336388e-06,
      "loss": 1.1754,
      "step": 3988
    },
    {
      "epoch": 1.5443283004258614,
      "grad_norm": 15.763877868652344,
      "learning_rate": 9.395190777304599e-06,
      "loss": 1.4581,
      "step": 3989
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 25.372386932373047,
      "learning_rate": 9.39476061427281e-06,
      "loss": 1.334,
      "step": 3990
    },
    {
      "epoch": 1.5451025938830818,
      "grad_norm": 12.397263526916504,
      "learning_rate": 9.394330451241022e-06,
      "loss": 0.7921,
      "step": 3991
    },
    {
      "epoch": 1.5454897406116919,
      "grad_norm": 15.087065696716309,
      "learning_rate": 9.393900288209232e-06,
      "loss": 1.2949,
      "step": 3992
    },
    {
      "epoch": 1.545876887340302,
      "grad_norm": 31.227380752563477,
      "learning_rate": 9.393470125177443e-06,
      "loss": 1.5497,
      "step": 3993
    },
    {
      "epoch": 1.546264034068912,
      "grad_norm": 15.098467826843262,
      "learning_rate": 9.393039962145653e-06,
      "loss": 1.4166,
      "step": 3994
    },
    {
      "epoch": 1.5466511807975223,
      "grad_norm": 17.18636703491211,
      "learning_rate": 9.392609799113866e-06,
      "loss": 1.4586,
      "step": 3995
    },
    {
      "epoch": 1.5470383275261324,
      "grad_norm": 23.4821834564209,
      "learning_rate": 9.392179636082076e-06,
      "loss": 2.4765,
      "step": 3996
    },
    {
      "epoch": 1.5474254742547426,
      "grad_norm": 25.781484603881836,
      "learning_rate": 9.391749473050287e-06,
      "loss": 1.4599,
      "step": 3997
    },
    {
      "epoch": 1.5478126209833527,
      "grad_norm": 6.632574081420898,
      "learning_rate": 9.391319310018497e-06,
      "loss": 0.402,
      "step": 3998
    },
    {
      "epoch": 1.5481997677119628,
      "grad_norm": 14.807177543640137,
      "learning_rate": 9.39088914698671e-06,
      "loss": 1.2666,
      "step": 3999
    },
    {
      "epoch": 1.5485869144405728,
      "grad_norm": 19.153217315673828,
      "learning_rate": 9.39045898395492e-06,
      "loss": 1.168,
      "step": 4000
    },
    {
      "epoch": 1.5489740611691831,
      "grad_norm": 8.536092758178711,
      "learning_rate": 9.390028820923131e-06,
      "loss": 0.6696,
      "step": 4001
    },
    {
      "epoch": 1.5493612078977934,
      "grad_norm": 18.136287689208984,
      "learning_rate": 9.38959865789134e-06,
      "loss": 1.079,
      "step": 4002
    },
    {
      "epoch": 1.5497483546264035,
      "grad_norm": 19.77773666381836,
      "learning_rate": 9.389168494859552e-06,
      "loss": 2.4854,
      "step": 4003
    },
    {
      "epoch": 1.5501355013550135,
      "grad_norm": 21.517131805419922,
      "learning_rate": 9.388738331827764e-06,
      "loss": 1.5085,
      "step": 4004
    },
    {
      "epoch": 1.5505226480836236,
      "grad_norm": 25.72987937927246,
      "learning_rate": 9.388308168795975e-06,
      "loss": 1.5498,
      "step": 4005
    },
    {
      "epoch": 1.5509097948122337,
      "grad_norm": 17.059377670288086,
      "learning_rate": 9.387878005764185e-06,
      "loss": 1.7216,
      "step": 4006
    },
    {
      "epoch": 1.551296941540844,
      "grad_norm": 19.799463272094727,
      "learning_rate": 9.387447842732396e-06,
      "loss": 2.0447,
      "step": 4007
    },
    {
      "epoch": 1.5516840882694543,
      "grad_norm": 25.783370971679688,
      "learning_rate": 9.387017679700608e-06,
      "loss": 1.771,
      "step": 4008
    },
    {
      "epoch": 1.5520712349980643,
      "grad_norm": 14.400238037109375,
      "learning_rate": 9.386587516668817e-06,
      "loss": 1.0304,
      "step": 4009
    },
    {
      "epoch": 1.5524583817266744,
      "grad_norm": 16.98451805114746,
      "learning_rate": 9.386157353637029e-06,
      "loss": 0.6912,
      "step": 4010
    },
    {
      "epoch": 1.5528455284552845,
      "grad_norm": 19.803504943847656,
      "learning_rate": 9.38572719060524e-06,
      "loss": 0.9974,
      "step": 4011
    },
    {
      "epoch": 1.5532326751838947,
      "grad_norm": 32.38468933105469,
      "learning_rate": 9.385297027573452e-06,
      "loss": 0.8829,
      "step": 4012
    },
    {
      "epoch": 1.5536198219125048,
      "grad_norm": 20.700450897216797,
      "learning_rate": 9.384866864541661e-06,
      "loss": 1.6671,
      "step": 4013
    },
    {
      "epoch": 1.554006968641115,
      "grad_norm": 17.59571075439453,
      "learning_rate": 9.384436701509873e-06,
      "loss": 1.6044,
      "step": 4014
    },
    {
      "epoch": 1.5543941153697252,
      "grad_norm": 18.17901039123535,
      "learning_rate": 9.384006538478084e-06,
      "loss": 1.2898,
      "step": 4015
    },
    {
      "epoch": 1.5547812620983352,
      "grad_norm": 17.310306549072266,
      "learning_rate": 9.383576375446296e-06,
      "loss": 1.4833,
      "step": 4016
    },
    {
      "epoch": 1.5551684088269453,
      "grad_norm": 22.343280792236328,
      "learning_rate": 9.383146212414505e-06,
      "loss": 1.0518,
      "step": 4017
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 16.205488204956055,
      "learning_rate": 9.382716049382717e-06,
      "loss": 1.0548,
      "step": 4018
    },
    {
      "epoch": 1.5559427022841656,
      "grad_norm": 26.666162490844727,
      "learning_rate": 9.382285886350928e-06,
      "loss": 1.9067,
      "step": 4019
    },
    {
      "epoch": 1.556329849012776,
      "grad_norm": 40.99513626098633,
      "learning_rate": 9.38185572331914e-06,
      "loss": 1.4198,
      "step": 4020
    },
    {
      "epoch": 1.556716995741386,
      "grad_norm": 13.414250373840332,
      "learning_rate": 9.38142556028735e-06,
      "loss": 1.2698,
      "step": 4021
    },
    {
      "epoch": 1.557104142469996,
      "grad_norm": 20.083120346069336,
      "learning_rate": 9.38099539725556e-06,
      "loss": 1.3597,
      "step": 4022
    },
    {
      "epoch": 1.5574912891986061,
      "grad_norm": 7.951657295227051,
      "learning_rate": 9.380565234223772e-06,
      "loss": 1.2255,
      "step": 4023
    },
    {
      "epoch": 1.5578784359272164,
      "grad_norm": 32.37173843383789,
      "learning_rate": 9.380135071191982e-06,
      "loss": 1.4278,
      "step": 4024
    },
    {
      "epoch": 1.5582655826558267,
      "grad_norm": 20.108516693115234,
      "learning_rate": 9.379704908160193e-06,
      "loss": 2.0294,
      "step": 4025
    },
    {
      "epoch": 1.5586527293844368,
      "grad_norm": 25.311010360717773,
      "learning_rate": 9.379274745128405e-06,
      "loss": 1.7543,
      "step": 4026
    },
    {
      "epoch": 1.5590398761130468,
      "grad_norm": 15.930974006652832,
      "learning_rate": 9.378844582096616e-06,
      "loss": 1.1232,
      "step": 4027
    },
    {
      "epoch": 1.559427022841657,
      "grad_norm": 9.953201293945312,
      "learning_rate": 9.378414419064826e-06,
      "loss": 1.3656,
      "step": 4028
    },
    {
      "epoch": 1.559814169570267,
      "grad_norm": 15.369306564331055,
      "learning_rate": 9.377984256033037e-06,
      "loss": 0.8565,
      "step": 4029
    },
    {
      "epoch": 1.5602013162988773,
      "grad_norm": 15.487290382385254,
      "learning_rate": 9.377554093001249e-06,
      "loss": 1.3265,
      "step": 4030
    },
    {
      "epoch": 1.5605884630274875,
      "grad_norm": 21.08793067932129,
      "learning_rate": 9.37712392996946e-06,
      "loss": 2.1202,
      "step": 4031
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 14.423012733459473,
      "learning_rate": 9.37669376693767e-06,
      "loss": 1.3909,
      "step": 4032
    },
    {
      "epoch": 1.5613627564847077,
      "grad_norm": 7.974408149719238,
      "learning_rate": 9.376263603905881e-06,
      "loss": 0.4902,
      "step": 4033
    },
    {
      "epoch": 1.5617499032133177,
      "grad_norm": 47.39072036743164,
      "learning_rate": 9.375833440874093e-06,
      "loss": 1.0332,
      "step": 4034
    },
    {
      "epoch": 1.562137049941928,
      "grad_norm": 18.833789825439453,
      "learning_rate": 9.375403277842304e-06,
      "loss": 1.1982,
      "step": 4035
    },
    {
      "epoch": 1.562524196670538,
      "grad_norm": 16.765975952148438,
      "learning_rate": 9.374973114810514e-06,
      "loss": 1.0216,
      "step": 4036
    },
    {
      "epoch": 1.5629113433991484,
      "grad_norm": 20.335853576660156,
      "learning_rate": 9.374542951778725e-06,
      "loss": 1.1081,
      "step": 4037
    },
    {
      "epoch": 1.5632984901277585,
      "grad_norm": 14.207318305969238,
      "learning_rate": 9.374112788746936e-06,
      "loss": 1.04,
      "step": 4038
    },
    {
      "epoch": 1.5636856368563685,
      "grad_norm": 12.773507118225098,
      "learning_rate": 9.373682625715146e-06,
      "loss": 0.8423,
      "step": 4039
    },
    {
      "epoch": 1.5640727835849786,
      "grad_norm": 7.1028337478637695,
      "learning_rate": 9.373252462683358e-06,
      "loss": 0.473,
      "step": 4040
    },
    {
      "epoch": 1.5644599303135889,
      "grad_norm": 13.864154815673828,
      "learning_rate": 9.372822299651569e-06,
      "loss": 1.2928,
      "step": 4041
    },
    {
      "epoch": 1.564847077042199,
      "grad_norm": 14.700790405273438,
      "learning_rate": 9.37239213661978e-06,
      "loss": 1.6209,
      "step": 4042
    },
    {
      "epoch": 1.5652342237708092,
      "grad_norm": 31.01461410522461,
      "learning_rate": 9.37196197358799e-06,
      "loss": 2.3524,
      "step": 4043
    },
    {
      "epoch": 1.5656213704994193,
      "grad_norm": 16.942543029785156,
      "learning_rate": 9.371531810556202e-06,
      "loss": 1.4835,
      "step": 4044
    },
    {
      "epoch": 1.5660085172280294,
      "grad_norm": 25.15765953063965,
      "learning_rate": 9.371101647524411e-06,
      "loss": 1.8851,
      "step": 4045
    },
    {
      "epoch": 1.5663956639566394,
      "grad_norm": 43.97419357299805,
      "learning_rate": 9.370671484492624e-06,
      "loss": 3.2379,
      "step": 4046
    },
    {
      "epoch": 1.5667828106852497,
      "grad_norm": 24.769939422607422,
      "learning_rate": 9.370241321460834e-06,
      "loss": 1.5716,
      "step": 4047
    },
    {
      "epoch": 1.56716995741386,
      "grad_norm": 22.813940048217773,
      "learning_rate": 9.369811158429046e-06,
      "loss": 1.9538,
      "step": 4048
    },
    {
      "epoch": 1.56755710414247,
      "grad_norm": 13.951101303100586,
      "learning_rate": 9.369380995397255e-06,
      "loss": 1.3656,
      "step": 4049
    },
    {
      "epoch": 1.5679442508710801,
      "grad_norm": 24.223766326904297,
      "learning_rate": 9.368950832365468e-06,
      "loss": 1.3299,
      "step": 4050
    },
    {
      "epoch": 1.5683313975996902,
      "grad_norm": 15.811868667602539,
      "learning_rate": 9.368520669333678e-06,
      "loss": 1.332,
      "step": 4051
    },
    {
      "epoch": 1.5687185443283003,
      "grad_norm": 24.982419967651367,
      "learning_rate": 9.36809050630189e-06,
      "loss": 1.9767,
      "step": 4052
    },
    {
      "epoch": 1.5691056910569106,
      "grad_norm": 21.359968185424805,
      "learning_rate": 9.3676603432701e-06,
      "loss": 1.6121,
      "step": 4053
    },
    {
      "epoch": 1.5694928377855208,
      "grad_norm": 25.785993576049805,
      "learning_rate": 9.36723018023831e-06,
      "loss": 1.6679,
      "step": 4054
    },
    {
      "epoch": 1.569879984514131,
      "grad_norm": 10.838200569152832,
      "learning_rate": 9.366800017206522e-06,
      "loss": 0.6416,
      "step": 4055
    },
    {
      "epoch": 1.570267131242741,
      "grad_norm": 19.75639533996582,
      "learning_rate": 9.366369854174734e-06,
      "loss": 1.5779,
      "step": 4056
    },
    {
      "epoch": 1.570654277971351,
      "grad_norm": 19.546476364135742,
      "learning_rate": 9.365939691142943e-06,
      "loss": 1.288,
      "step": 4057
    },
    {
      "epoch": 1.5710414246999613,
      "grad_norm": 17.13753890991211,
      "learning_rate": 9.365509528111155e-06,
      "loss": 1.71,
      "step": 4058
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 17.607160568237305,
      "learning_rate": 9.365079365079366e-06,
      "loss": 1.4302,
      "step": 4059
    },
    {
      "epoch": 1.5718157181571817,
      "grad_norm": 35.427886962890625,
      "learning_rate": 9.364649202047576e-06,
      "loss": 0.8009,
      "step": 4060
    },
    {
      "epoch": 1.5722028648857918,
      "grad_norm": 15.329849243164062,
      "learning_rate": 9.364219039015787e-06,
      "loss": 1.3925,
      "step": 4061
    },
    {
      "epoch": 1.5725900116144018,
      "grad_norm": 28.808996200561523,
      "learning_rate": 9.363788875983999e-06,
      "loss": 2.4942,
      "step": 4062
    },
    {
      "epoch": 1.5729771583430119,
      "grad_norm": 26.759241104125977,
      "learning_rate": 9.36335871295221e-06,
      "loss": 0.596,
      "step": 4063
    },
    {
      "epoch": 1.5733643050716222,
      "grad_norm": 10.877384185791016,
      "learning_rate": 9.36292854992042e-06,
      "loss": 0.6658,
      "step": 4064
    },
    {
      "epoch": 1.5737514518002322,
      "grad_norm": 23.334280014038086,
      "learning_rate": 9.362498386888633e-06,
      "loss": 2.2794,
      "step": 4065
    },
    {
      "epoch": 1.5741385985288425,
      "grad_norm": 25.14316177368164,
      "learning_rate": 9.362068223856843e-06,
      "loss": 2.0647,
      "step": 4066
    },
    {
      "epoch": 1.5745257452574526,
      "grad_norm": 16.03374481201172,
      "learning_rate": 9.361638060825054e-06,
      "loss": 1.5873,
      "step": 4067
    },
    {
      "epoch": 1.5749128919860627,
      "grad_norm": 27.689512252807617,
      "learning_rate": 9.361207897793264e-06,
      "loss": 1.8737,
      "step": 4068
    },
    {
      "epoch": 1.5753000387146727,
      "grad_norm": 25.662033081054688,
      "learning_rate": 9.360777734761475e-06,
      "loss": 1.3288,
      "step": 4069
    },
    {
      "epoch": 1.575687185443283,
      "grad_norm": 19.914907455444336,
      "learning_rate": 9.360347571729687e-06,
      "loss": 1.8805,
      "step": 4070
    },
    {
      "epoch": 1.5760743321718933,
      "grad_norm": 17.58182716369629,
      "learning_rate": 9.359917408697898e-06,
      "loss": 1.4115,
      "step": 4071
    },
    {
      "epoch": 1.5764614789005034,
      "grad_norm": 15.244479179382324,
      "learning_rate": 9.359487245666108e-06,
      "loss": 0.6416,
      "step": 4072
    },
    {
      "epoch": 1.5768486256291134,
      "grad_norm": 20.227317810058594,
      "learning_rate": 9.359057082634319e-06,
      "loss": 1.6463,
      "step": 4073
    },
    {
      "epoch": 1.5772357723577235,
      "grad_norm": 20.605655670166016,
      "learning_rate": 9.35862691960253e-06,
      "loss": 1.9468,
      "step": 4074
    },
    {
      "epoch": 1.5776229190863336,
      "grad_norm": 30.989871978759766,
      "learning_rate": 9.35819675657074e-06,
      "loss": 1.5456,
      "step": 4075
    },
    {
      "epoch": 1.5780100658149439,
      "grad_norm": 17.801454544067383,
      "learning_rate": 9.357766593538952e-06,
      "loss": 1.7664,
      "step": 4076
    },
    {
      "epoch": 1.5783972125435541,
      "grad_norm": 21.277868270874023,
      "learning_rate": 9.357336430507163e-06,
      "loss": 0.7292,
      "step": 4077
    },
    {
      "epoch": 1.5787843592721642,
      "grad_norm": 45.3848762512207,
      "learning_rate": 9.356906267475374e-06,
      "loss": 1.8764,
      "step": 4078
    },
    {
      "epoch": 1.5791715060007743,
      "grad_norm": 21.616926193237305,
      "learning_rate": 9.356476104443584e-06,
      "loss": 0.6266,
      "step": 4079
    },
    {
      "epoch": 1.5795586527293843,
      "grad_norm": 23.366418838500977,
      "learning_rate": 9.356045941411796e-06,
      "loss": 1.7143,
      "step": 4080
    },
    {
      "epoch": 1.5799457994579946,
      "grad_norm": 22.609373092651367,
      "learning_rate": 9.355615778380007e-06,
      "loss": 1.2512,
      "step": 4081
    },
    {
      "epoch": 1.5803329461866047,
      "grad_norm": 14.313847541809082,
      "learning_rate": 9.355185615348218e-06,
      "loss": 1.0119,
      "step": 4082
    },
    {
      "epoch": 1.580720092915215,
      "grad_norm": 23.20594024658203,
      "learning_rate": 9.354755452316428e-06,
      "loss": 1.4022,
      "step": 4083
    },
    {
      "epoch": 1.581107239643825,
      "grad_norm": 19.019058227539062,
      "learning_rate": 9.35432528928464e-06,
      "loss": 1.7179,
      "step": 4084
    },
    {
      "epoch": 1.5814943863724351,
      "grad_norm": 22.71950340270996,
      "learning_rate": 9.353895126252851e-06,
      "loss": 1.3279,
      "step": 4085
    },
    {
      "epoch": 1.5818815331010452,
      "grad_norm": 16.664169311523438,
      "learning_rate": 9.353464963221062e-06,
      "loss": 1.5676,
      "step": 4086
    },
    {
      "epoch": 1.5822686798296555,
      "grad_norm": 18.03811264038086,
      "learning_rate": 9.353034800189272e-06,
      "loss": 1.2436,
      "step": 4087
    },
    {
      "epoch": 1.5826558265582655,
      "grad_norm": 23.17261505126953,
      "learning_rate": 9.352604637157484e-06,
      "loss": 1.579,
      "step": 4088
    },
    {
      "epoch": 1.5830429732868758,
      "grad_norm": 35.13852310180664,
      "learning_rate": 9.352174474125695e-06,
      "loss": 1.0351,
      "step": 4089
    },
    {
      "epoch": 1.5834301200154859,
      "grad_norm": 12.432600021362305,
      "learning_rate": 9.351744311093905e-06,
      "loss": 2.7188,
      "step": 4090
    },
    {
      "epoch": 1.583817266744096,
      "grad_norm": 30.338642120361328,
      "learning_rate": 9.351314148062116e-06,
      "loss": 1.9378,
      "step": 4091
    },
    {
      "epoch": 1.584204413472706,
      "grad_norm": 37.60462951660156,
      "learning_rate": 9.350883985030328e-06,
      "loss": 2.216,
      "step": 4092
    },
    {
      "epoch": 1.5845915602013163,
      "grad_norm": 16.26364517211914,
      "learning_rate": 9.350453821998539e-06,
      "loss": 1.5507,
      "step": 4093
    },
    {
      "epoch": 1.5849787069299266,
      "grad_norm": 11.724665641784668,
      "learning_rate": 9.350023658966749e-06,
      "loss": 0.769,
      "step": 4094
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 15.551807403564453,
      "learning_rate": 9.34959349593496e-06,
      "loss": 1.7552,
      "step": 4095
    },
    {
      "epoch": 1.5857530003871467,
      "grad_norm": 22.834827423095703,
      "learning_rate": 9.34916333290317e-06,
      "loss": 1.9,
      "step": 4096
    },
    {
      "epoch": 1.5861401471157568,
      "grad_norm": 31.48217010498047,
      "learning_rate": 9.348733169871383e-06,
      "loss": 1.6551,
      "step": 4097
    },
    {
      "epoch": 1.5865272938443669,
      "grad_norm": 22.74690818786621,
      "learning_rate": 9.348303006839593e-06,
      "loss": 1.5176,
      "step": 4098
    },
    {
      "epoch": 1.5869144405729771,
      "grad_norm": 15.243289947509766,
      "learning_rate": 9.347872843807804e-06,
      "loss": 1.1578,
      "step": 4099
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 14.489785194396973,
      "learning_rate": 9.347442680776014e-06,
      "loss": 1.1397,
      "step": 4100
    },
    {
      "epoch": 1.5876887340301975,
      "grad_norm": 24.281557083129883,
      "learning_rate": 9.347012517744227e-06,
      "loss": 2.2119,
      "step": 4101
    },
    {
      "epoch": 1.5880758807588076,
      "grad_norm": 11.3148832321167,
      "learning_rate": 9.346582354712437e-06,
      "loss": 2.0416,
      "step": 4102
    },
    {
      "epoch": 1.5884630274874176,
      "grad_norm": 39.257720947265625,
      "learning_rate": 9.346152191680648e-06,
      "loss": 1.4669,
      "step": 4103
    },
    {
      "epoch": 1.588850174216028,
      "grad_norm": 15.148138999938965,
      "learning_rate": 9.345722028648858e-06,
      "loss": 1.3049,
      "step": 4104
    },
    {
      "epoch": 1.589237320944638,
      "grad_norm": 17.409446716308594,
      "learning_rate": 9.34529186561707e-06,
      "loss": 1.3957,
      "step": 4105
    },
    {
      "epoch": 1.5896244676732483,
      "grad_norm": 16.51162338256836,
      "learning_rate": 9.34486170258528e-06,
      "loss": 1.9267,
      "step": 4106
    },
    {
      "epoch": 1.5900116144018583,
      "grad_norm": 14.548907279968262,
      "learning_rate": 9.344431539553492e-06,
      "loss": 1.014,
      "step": 4107
    },
    {
      "epoch": 1.5903987611304684,
      "grad_norm": 17.87949562072754,
      "learning_rate": 9.344001376521703e-06,
      "loss": 1.3039,
      "step": 4108
    },
    {
      "epoch": 1.5907859078590785,
      "grad_norm": 12.701443672180176,
      "learning_rate": 9.343571213489913e-06,
      "loss": 0.8302,
      "step": 4109
    },
    {
      "epoch": 1.5911730545876888,
      "grad_norm": 24.646343231201172,
      "learning_rate": 9.343141050458125e-06,
      "loss": 2.0979,
      "step": 4110
    },
    {
      "epoch": 1.5915602013162988,
      "grad_norm": 16.470911026000977,
      "learning_rate": 9.342710887426334e-06,
      "loss": 0.8659,
      "step": 4111
    },
    {
      "epoch": 1.5919473480449091,
      "grad_norm": 19.824012756347656,
      "learning_rate": 9.342280724394547e-06,
      "loss": 0.9924,
      "step": 4112
    },
    {
      "epoch": 1.5923344947735192,
      "grad_norm": 30.213884353637695,
      "learning_rate": 9.341850561362757e-06,
      "loss": 2.0693,
      "step": 4113
    },
    {
      "epoch": 1.5927216415021292,
      "grad_norm": 45.39655303955078,
      "learning_rate": 9.341420398330969e-06,
      "loss": 1.9287,
      "step": 4114
    },
    {
      "epoch": 1.5931087882307393,
      "grad_norm": 31.958852767944336,
      "learning_rate": 9.340990235299178e-06,
      "loss": 1.4746,
      "step": 4115
    },
    {
      "epoch": 1.5934959349593496,
      "grad_norm": 6.947291374206543,
      "learning_rate": 9.340560072267391e-06,
      "loss": 0.4581,
      "step": 4116
    },
    {
      "epoch": 1.59388308168796,
      "grad_norm": 21.103078842163086,
      "learning_rate": 9.340129909235601e-06,
      "loss": 1.9159,
      "step": 4117
    },
    {
      "epoch": 1.59427022841657,
      "grad_norm": 18.70794677734375,
      "learning_rate": 9.339699746203812e-06,
      "loss": 1.2624,
      "step": 4118
    },
    {
      "epoch": 1.59465737514518,
      "grad_norm": 13.5807466506958,
      "learning_rate": 9.339269583172022e-06,
      "loss": 0.8633,
      "step": 4119
    },
    {
      "epoch": 1.59504452187379,
      "grad_norm": 22.455869674682617,
      "learning_rate": 9.338839420140234e-06,
      "loss": 2.0457,
      "step": 4120
    },
    {
      "epoch": 1.5954316686024002,
      "grad_norm": 16.224042892456055,
      "learning_rate": 9.338409257108445e-06,
      "loss": 1.3245,
      "step": 4121
    },
    {
      "epoch": 1.5958188153310104,
      "grad_norm": 31.20893669128418,
      "learning_rate": 9.337979094076656e-06,
      "loss": 1.655,
      "step": 4122
    },
    {
      "epoch": 1.5962059620596207,
      "grad_norm": 19.57966423034668,
      "learning_rate": 9.337548931044866e-06,
      "loss": 1.9226,
      "step": 4123
    },
    {
      "epoch": 1.5965931087882308,
      "grad_norm": 22.400249481201172,
      "learning_rate": 9.337118768013078e-06,
      "loss": 1.3418,
      "step": 4124
    },
    {
      "epoch": 1.5969802555168409,
      "grad_norm": 12.911104202270508,
      "learning_rate": 9.336688604981289e-06,
      "loss": 0.7448,
      "step": 4125
    },
    {
      "epoch": 1.597367402245451,
      "grad_norm": 28.309226989746094,
      "learning_rate": 9.336258441949499e-06,
      "loss": 1.4788,
      "step": 4126
    },
    {
      "epoch": 1.5977545489740612,
      "grad_norm": 16.60659408569336,
      "learning_rate": 9.33582827891771e-06,
      "loss": 1.2268,
      "step": 4127
    },
    {
      "epoch": 1.5981416957026713,
      "grad_norm": 22.43778419494629,
      "learning_rate": 9.335398115885922e-06,
      "loss": 1.2828,
      "step": 4128
    },
    {
      "epoch": 1.5985288424312816,
      "grad_norm": 17.824411392211914,
      "learning_rate": 9.334967952854133e-06,
      "loss": 1.2235,
      "step": 4129
    },
    {
      "epoch": 1.5989159891598916,
      "grad_norm": 23.8215274810791,
      "learning_rate": 9.334537789822343e-06,
      "loss": 1.3491,
      "step": 4130
    },
    {
      "epoch": 1.5993031358885017,
      "grad_norm": 11.722540855407715,
      "learning_rate": 9.334107626790554e-06,
      "loss": 0.6537,
      "step": 4131
    },
    {
      "epoch": 1.5996902826171118,
      "grad_norm": 24.932300567626953,
      "learning_rate": 9.333677463758766e-06,
      "loss": 1.3896,
      "step": 4132
    },
    {
      "epoch": 1.600077429345722,
      "grad_norm": 15.977005004882812,
      "learning_rate": 9.333247300726977e-06,
      "loss": 1.1009,
      "step": 4133
    },
    {
      "epoch": 1.6004645760743321,
      "grad_norm": 32.38204574584961,
      "learning_rate": 9.332817137695187e-06,
      "loss": 2.547,
      "step": 4134
    },
    {
      "epoch": 1.6008517228029424,
      "grad_norm": 22.532379150390625,
      "learning_rate": 9.332386974663398e-06,
      "loss": 1.7098,
      "step": 4135
    },
    {
      "epoch": 1.6012388695315525,
      "grad_norm": 23.96293830871582,
      "learning_rate": 9.33195681163161e-06,
      "loss": 2.0297,
      "step": 4136
    },
    {
      "epoch": 1.6016260162601625,
      "grad_norm": 28.883560180664062,
      "learning_rate": 9.331526648599821e-06,
      "loss": 1.0756,
      "step": 4137
    },
    {
      "epoch": 1.6020131629887726,
      "grad_norm": 28.403955459594727,
      "learning_rate": 9.33109648556803e-06,
      "loss": 2.1112,
      "step": 4138
    },
    {
      "epoch": 1.602400309717383,
      "grad_norm": 26.25531768798828,
      "learning_rate": 9.330666322536242e-06,
      "loss": 1.7434,
      "step": 4139
    },
    {
      "epoch": 1.6027874564459932,
      "grad_norm": 23.409690856933594,
      "learning_rate": 9.330236159504453e-06,
      "loss": 1.592,
      "step": 4140
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 37.810890197753906,
      "learning_rate": 9.329805996472663e-06,
      "loss": 1.0607,
      "step": 4141
    },
    {
      "epoch": 1.6035617499032133,
      "grad_norm": 13.868578910827637,
      "learning_rate": 9.329375833440875e-06,
      "loss": 1.3903,
      "step": 4142
    },
    {
      "epoch": 1.6039488966318234,
      "grad_norm": 26.629364013671875,
      "learning_rate": 9.328945670409086e-06,
      "loss": 2.5481,
      "step": 4143
    },
    {
      "epoch": 1.6043360433604335,
      "grad_norm": 16.671491622924805,
      "learning_rate": 9.328515507377297e-06,
      "loss": 1.5453,
      "step": 4144
    },
    {
      "epoch": 1.6047231900890437,
      "grad_norm": 54.28145217895508,
      "learning_rate": 9.328085344345507e-06,
      "loss": 2.6773,
      "step": 4145
    },
    {
      "epoch": 1.605110336817654,
      "grad_norm": 18.708803176879883,
      "learning_rate": 9.327655181313719e-06,
      "loss": 1.0173,
      "step": 4146
    },
    {
      "epoch": 1.605497483546264,
      "grad_norm": 19.974239349365234,
      "learning_rate": 9.32722501828193e-06,
      "loss": 1.6406,
      "step": 4147
    },
    {
      "epoch": 1.6058846302748742,
      "grad_norm": 18.127216339111328,
      "learning_rate": 9.326794855250141e-06,
      "loss": 1.4369,
      "step": 4148
    },
    {
      "epoch": 1.6062717770034842,
      "grad_norm": 33.32419204711914,
      "learning_rate": 9.326364692218351e-06,
      "loss": 2.1967,
      "step": 4149
    },
    {
      "epoch": 1.6066589237320945,
      "grad_norm": 16.124752044677734,
      "learning_rate": 9.325934529186563e-06,
      "loss": 1.654,
      "step": 4150
    },
    {
      "epoch": 1.6070460704607046,
      "grad_norm": 8.692889213562012,
      "learning_rate": 9.325504366154774e-06,
      "loss": 0.3177,
      "step": 4151
    },
    {
      "epoch": 1.6074332171893149,
      "grad_norm": 16.90325355529785,
      "learning_rate": 9.325074203122985e-06,
      "loss": 1.2529,
      "step": 4152
    },
    {
      "epoch": 1.607820363917925,
      "grad_norm": 30.52513885498047,
      "learning_rate": 9.324644040091195e-06,
      "loss": 2.2081,
      "step": 4153
    },
    {
      "epoch": 1.608207510646535,
      "grad_norm": 15.308099746704102,
      "learning_rate": 9.324213877059407e-06,
      "loss": 1.3101,
      "step": 4154
    },
    {
      "epoch": 1.608594657375145,
      "grad_norm": 21.37240219116211,
      "learning_rate": 9.323783714027618e-06,
      "loss": 1.5438,
      "step": 4155
    },
    {
      "epoch": 1.6089818041037554,
      "grad_norm": 17.684429168701172,
      "learning_rate": 9.323353550995828e-06,
      "loss": 1.4766,
      "step": 4156
    },
    {
      "epoch": 1.6093689508323654,
      "grad_norm": 34.76742935180664,
      "learning_rate": 9.322923387964039e-06,
      "loss": 2.5863,
      "step": 4157
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 14.996479988098145,
      "learning_rate": 9.32249322493225e-06,
      "loss": 1.3988,
      "step": 4158
    },
    {
      "epoch": 1.6101432442895858,
      "grad_norm": 13.54751205444336,
      "learning_rate": 9.322063061900462e-06,
      "loss": 1.3273,
      "step": 4159
    },
    {
      "epoch": 1.6105303910181958,
      "grad_norm": 24.23432731628418,
      "learning_rate": 9.321632898868672e-06,
      "loss": 2.0658,
      "step": 4160
    },
    {
      "epoch": 1.610917537746806,
      "grad_norm": 13.856364250183105,
      "learning_rate": 9.321202735836883e-06,
      "loss": 1.2677,
      "step": 4161
    },
    {
      "epoch": 1.6113046844754162,
      "grad_norm": 16.99272346496582,
      "learning_rate": 9.320772572805093e-06,
      "loss": 1.1977,
      "step": 4162
    },
    {
      "epoch": 1.6116918312040265,
      "grad_norm": 15.436083793640137,
      "learning_rate": 9.320342409773306e-06,
      "loss": 1.2923,
      "step": 4163
    },
    {
      "epoch": 1.6120789779326365,
      "grad_norm": 17.554893493652344,
      "learning_rate": 9.319912246741516e-06,
      "loss": 1.8017,
      "step": 4164
    },
    {
      "epoch": 1.6124661246612466,
      "grad_norm": 14.737011909484863,
      "learning_rate": 9.319482083709727e-06,
      "loss": 1.489,
      "step": 4165
    },
    {
      "epoch": 1.6128532713898567,
      "grad_norm": 25.779897689819336,
      "learning_rate": 9.319051920677937e-06,
      "loss": 1.5019,
      "step": 4166
    },
    {
      "epoch": 1.6132404181184667,
      "grad_norm": 15.315616607666016,
      "learning_rate": 9.31862175764615e-06,
      "loss": 1.5956,
      "step": 4167
    },
    {
      "epoch": 1.613627564847077,
      "grad_norm": 21.50710105895996,
      "learning_rate": 9.31819159461436e-06,
      "loss": 1.4919,
      "step": 4168
    },
    {
      "epoch": 1.6140147115756873,
      "grad_norm": 19.70393943786621,
      "learning_rate": 9.317761431582571e-06,
      "loss": 1.7546,
      "step": 4169
    },
    {
      "epoch": 1.6144018583042974,
      "grad_norm": 14.826733589172363,
      "learning_rate": 9.31733126855078e-06,
      "loss": 1.4941,
      "step": 4170
    },
    {
      "epoch": 1.6147890050329075,
      "grad_norm": 23.495529174804688,
      "learning_rate": 9.316901105518992e-06,
      "loss": 1.5315,
      "step": 4171
    },
    {
      "epoch": 1.6151761517615175,
      "grad_norm": 15.056807518005371,
      "learning_rate": 9.316470942487204e-06,
      "loss": 1.3017,
      "step": 4172
    },
    {
      "epoch": 1.6155632984901278,
      "grad_norm": 13.43264102935791,
      "learning_rate": 9.316040779455415e-06,
      "loss": 1.1169,
      "step": 4173
    },
    {
      "epoch": 1.6159504452187379,
      "grad_norm": 25.123432159423828,
      "learning_rate": 9.315610616423625e-06,
      "loss": 2.1752,
      "step": 4174
    },
    {
      "epoch": 1.6163375919473482,
      "grad_norm": 17.843761444091797,
      "learning_rate": 9.315180453391836e-06,
      "loss": 1.7174,
      "step": 4175
    },
    {
      "epoch": 1.6167247386759582,
      "grad_norm": 24.96756362915039,
      "learning_rate": 9.314750290360047e-06,
      "loss": 1.1467,
      "step": 4176
    },
    {
      "epoch": 1.6171118854045683,
      "grad_norm": 13.098641395568848,
      "learning_rate": 9.314320127328257e-06,
      "loss": 0.8414,
      "step": 4177
    },
    {
      "epoch": 1.6174990321331784,
      "grad_norm": 11.352010726928711,
      "learning_rate": 9.313889964296469e-06,
      "loss": 1.2574,
      "step": 4178
    },
    {
      "epoch": 1.6178861788617886,
      "grad_norm": 27.216270446777344,
      "learning_rate": 9.31345980126468e-06,
      "loss": 2.0836,
      "step": 4179
    },
    {
      "epoch": 1.6182733255903987,
      "grad_norm": 25.475690841674805,
      "learning_rate": 9.313029638232891e-06,
      "loss": 1.4347,
      "step": 4180
    },
    {
      "epoch": 1.618660472319009,
      "grad_norm": 12.843114852905273,
      "learning_rate": 9.312599475201101e-06,
      "loss": 1.5469,
      "step": 4181
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 14.590983390808105,
      "learning_rate": 9.312169312169313e-06,
      "loss": 0.8702,
      "step": 4182
    },
    {
      "epoch": 1.6194347657762291,
      "grad_norm": 20.735177993774414,
      "learning_rate": 9.311739149137524e-06,
      "loss": 3.3929,
      "step": 4183
    },
    {
      "epoch": 1.6198219125048392,
      "grad_norm": 15.68274974822998,
      "learning_rate": 9.311308986105735e-06,
      "loss": 1.4237,
      "step": 4184
    },
    {
      "epoch": 1.6202090592334495,
      "grad_norm": 24.041292190551758,
      "learning_rate": 9.310878823073945e-06,
      "loss": 1.3113,
      "step": 4185
    },
    {
      "epoch": 1.6205962059620598,
      "grad_norm": 13.684220314025879,
      "learning_rate": 9.310448660042157e-06,
      "loss": 1.3894,
      "step": 4186
    },
    {
      "epoch": 1.6209833526906698,
      "grad_norm": 20.61174774169922,
      "learning_rate": 9.310018497010368e-06,
      "loss": 3.274,
      "step": 4187
    },
    {
      "epoch": 1.62137049941928,
      "grad_norm": 22.825021743774414,
      "learning_rate": 9.30958833397858e-06,
      "loss": 2.1251,
      "step": 4188
    },
    {
      "epoch": 1.62175764614789,
      "grad_norm": 9.812503814697266,
      "learning_rate": 9.309158170946789e-06,
      "loss": 1.1048,
      "step": 4189
    },
    {
      "epoch": 1.6221447928765,
      "grad_norm": 26.1920223236084,
      "learning_rate": 9.308728007915e-06,
      "loss": 1.5608,
      "step": 4190
    },
    {
      "epoch": 1.6225319396051103,
      "grad_norm": 31.18404197692871,
      "learning_rate": 9.308297844883212e-06,
      "loss": 2.1664,
      "step": 4191
    },
    {
      "epoch": 1.6229190863337206,
      "grad_norm": 14.65050220489502,
      "learning_rate": 9.307867681851422e-06,
      "loss": 0.996,
      "step": 4192
    },
    {
      "epoch": 1.6233062330623307,
      "grad_norm": 21.569944381713867,
      "learning_rate": 9.307437518819633e-06,
      "loss": 0.8163,
      "step": 4193
    },
    {
      "epoch": 1.6236933797909407,
      "grad_norm": 17.927507400512695,
      "learning_rate": 9.307007355787845e-06,
      "loss": 1.0823,
      "step": 4194
    },
    {
      "epoch": 1.6240805265195508,
      "grad_norm": 15.77232837677002,
      "learning_rate": 9.306577192756056e-06,
      "loss": 1.3474,
      "step": 4195
    },
    {
      "epoch": 1.624467673248161,
      "grad_norm": 23.1616268157959,
      "learning_rate": 9.306147029724266e-06,
      "loss": 1.7759,
      "step": 4196
    },
    {
      "epoch": 1.6248548199767712,
      "grad_norm": 14.678008079528809,
      "learning_rate": 9.305716866692477e-06,
      "loss": 1.4977,
      "step": 4197
    },
    {
      "epoch": 1.6252419667053815,
      "grad_norm": 19.876976013183594,
      "learning_rate": 9.305286703660688e-06,
      "loss": 1.3032,
      "step": 4198
    },
    {
      "epoch": 1.6256291134339915,
      "grad_norm": 13.204312324523926,
      "learning_rate": 9.3048565406289e-06,
      "loss": 1.5177,
      "step": 4199
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 12.811399459838867,
      "learning_rate": 9.30442637759711e-06,
      "loss": 0.9981,
      "step": 4200
    },
    {
      "epoch": 1.6264034068912117,
      "grad_norm": 17.186368942260742,
      "learning_rate": 9.303996214565321e-06,
      "loss": 1.5928,
      "step": 4201
    },
    {
      "epoch": 1.626790553619822,
      "grad_norm": 23.72660255432129,
      "learning_rate": 9.303566051533532e-06,
      "loss": 1.422,
      "step": 4202
    },
    {
      "epoch": 1.627177700348432,
      "grad_norm": 25.532670974731445,
      "learning_rate": 9.303135888501744e-06,
      "loss": 1.614,
      "step": 4203
    },
    {
      "epoch": 1.6275648470770423,
      "grad_norm": 19.226802825927734,
      "learning_rate": 9.302705725469954e-06,
      "loss": 1.4732,
      "step": 4204
    },
    {
      "epoch": 1.6279519938056524,
      "grad_norm": 30.27008819580078,
      "learning_rate": 9.302275562438165e-06,
      "loss": 3.1302,
      "step": 4205
    },
    {
      "epoch": 1.6283391405342624,
      "grad_norm": 27.660324096679688,
      "learning_rate": 9.301845399406376e-06,
      "loss": 1.8533,
      "step": 4206
    },
    {
      "epoch": 1.6287262872628725,
      "grad_norm": 9.290343284606934,
      "learning_rate": 9.301415236374586e-06,
      "loss": 0.9351,
      "step": 4207
    },
    {
      "epoch": 1.6291134339914828,
      "grad_norm": 14.27415657043457,
      "learning_rate": 9.300985073342798e-06,
      "loss": 1.0368,
      "step": 4208
    },
    {
      "epoch": 1.629500580720093,
      "grad_norm": 26.43857192993164,
      "learning_rate": 9.300554910311009e-06,
      "loss": 1.8459,
      "step": 4209
    },
    {
      "epoch": 1.6298877274487031,
      "grad_norm": 7.016826152801514,
      "learning_rate": 9.30012474727922e-06,
      "loss": 0.4597,
      "step": 4210
    },
    {
      "epoch": 1.6302748741773132,
      "grad_norm": 12.5070161819458,
      "learning_rate": 9.29969458424743e-06,
      "loss": 0.8543,
      "step": 4211
    },
    {
      "epoch": 1.6306620209059233,
      "grad_norm": 10.646514892578125,
      "learning_rate": 9.299264421215642e-06,
      "loss": 1.369,
      "step": 4212
    },
    {
      "epoch": 1.6310491676345333,
      "grad_norm": 42.99148178100586,
      "learning_rate": 9.298834258183851e-06,
      "loss": 2.0581,
      "step": 4213
    },
    {
      "epoch": 1.6314363143631436,
      "grad_norm": 19.09624671936035,
      "learning_rate": 9.298404095152064e-06,
      "loss": 1.6861,
      "step": 4214
    },
    {
      "epoch": 1.631823461091754,
      "grad_norm": 10.415050506591797,
      "learning_rate": 9.297973932120274e-06,
      "loss": 0.655,
      "step": 4215
    },
    {
      "epoch": 1.632210607820364,
      "grad_norm": 35.44053268432617,
      "learning_rate": 9.297543769088485e-06,
      "loss": 2.2995,
      "step": 4216
    },
    {
      "epoch": 1.632597754548974,
      "grad_norm": 16.418033599853516,
      "learning_rate": 9.297113606056695e-06,
      "loss": 1.4826,
      "step": 4217
    },
    {
      "epoch": 1.6329849012775841,
      "grad_norm": 22.52870750427246,
      "learning_rate": 9.296683443024908e-06,
      "loss": 1.4953,
      "step": 4218
    },
    {
      "epoch": 1.6333720480061944,
      "grad_norm": 14.037686347961426,
      "learning_rate": 9.296253279993118e-06,
      "loss": 1.4054,
      "step": 4219
    },
    {
      "epoch": 1.6337591947348045,
      "grad_norm": 24.507959365844727,
      "learning_rate": 9.29582311696133e-06,
      "loss": 1.8903,
      "step": 4220
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 17.28823471069336,
      "learning_rate": 9.29539295392954e-06,
      "loss": 1.5439,
      "step": 4221
    },
    {
      "epoch": 1.6345334881920248,
      "grad_norm": 14.46258544921875,
      "learning_rate": 9.29496279089775e-06,
      "loss": 1.507,
      "step": 4222
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 13.656926155090332,
      "learning_rate": 9.294532627865962e-06,
      "loss": 1.3435,
      "step": 4223
    },
    {
      "epoch": 1.635307781649245,
      "grad_norm": 13.297588348388672,
      "learning_rate": 9.294102464834173e-06,
      "loss": 1.0581,
      "step": 4224
    },
    {
      "epoch": 1.6356949283778552,
      "grad_norm": 20.246889114379883,
      "learning_rate": 9.293672301802383e-06,
      "loss": 1.5925,
      "step": 4225
    },
    {
      "epoch": 1.6360820751064653,
      "grad_norm": 10.820215225219727,
      "learning_rate": 9.293242138770595e-06,
      "loss": 0.6689,
      "step": 4226
    },
    {
      "epoch": 1.6364692218350756,
      "grad_norm": 22.302051544189453,
      "learning_rate": 9.292811975738806e-06,
      "loss": 1.9669,
      "step": 4227
    },
    {
      "epoch": 1.6368563685636857,
      "grad_norm": 27.759904861450195,
      "learning_rate": 9.292381812707016e-06,
      "loss": 1.5314,
      "step": 4228
    },
    {
      "epoch": 1.6372435152922957,
      "grad_norm": 12.62214469909668,
      "learning_rate": 9.291951649675229e-06,
      "loss": 1.1228,
      "step": 4229
    },
    {
      "epoch": 1.6376306620209058,
      "grad_norm": 13.731943130493164,
      "learning_rate": 9.291521486643439e-06,
      "loss": 0.8792,
      "step": 4230
    },
    {
      "epoch": 1.638017808749516,
      "grad_norm": 16.60959243774414,
      "learning_rate": 9.29109132361165e-06,
      "loss": 1.7162,
      "step": 4231
    },
    {
      "epoch": 1.6384049554781264,
      "grad_norm": 14.50012493133545,
      "learning_rate": 9.29066116057986e-06,
      "loss": 1.1921,
      "step": 4232
    },
    {
      "epoch": 1.6387921022067364,
      "grad_norm": 16.69760513305664,
      "learning_rate": 9.290230997548073e-06,
      "loss": 1.6685,
      "step": 4233
    },
    {
      "epoch": 1.6391792489353465,
      "grad_norm": 22.938236236572266,
      "learning_rate": 9.289800834516283e-06,
      "loss": 1.5133,
      "step": 4234
    },
    {
      "epoch": 1.6395663956639566,
      "grad_norm": 24.44687271118164,
      "learning_rate": 9.289370671484494e-06,
      "loss": 1.0277,
      "step": 4235
    },
    {
      "epoch": 1.6399535423925666,
      "grad_norm": 14.63229751586914,
      "learning_rate": 9.288940508452704e-06,
      "loss": 1.0233,
      "step": 4236
    },
    {
      "epoch": 1.640340689121177,
      "grad_norm": 16.095504760742188,
      "learning_rate": 9.288510345420915e-06,
      "loss": 1.7066,
      "step": 4237
    },
    {
      "epoch": 1.6407278358497872,
      "grad_norm": 15.68777847290039,
      "learning_rate": 9.288080182389126e-06,
      "loss": 1.568,
      "step": 4238
    },
    {
      "epoch": 1.6411149825783973,
      "grad_norm": 22.765361785888672,
      "learning_rate": 9.287650019357338e-06,
      "loss": 1.3658,
      "step": 4239
    },
    {
      "epoch": 1.6415021293070073,
      "grad_norm": 11.377098083496094,
      "learning_rate": 9.287219856325548e-06,
      "loss": 0.8741,
      "step": 4240
    },
    {
      "epoch": 1.6418892760356174,
      "grad_norm": 20.876005172729492,
      "learning_rate": 9.286789693293759e-06,
      "loss": 1.3681,
      "step": 4241
    },
    {
      "epoch": 1.6422764227642277,
      "grad_norm": 17.71363639831543,
      "learning_rate": 9.28635953026197e-06,
      "loss": 1.8792,
      "step": 4242
    },
    {
      "epoch": 1.6426635694928378,
      "grad_norm": 23.507291793823242,
      "learning_rate": 9.28592936723018e-06,
      "loss": 1.5045,
      "step": 4243
    },
    {
      "epoch": 1.643050716221448,
      "grad_norm": 15.871716499328613,
      "learning_rate": 9.285499204198392e-06,
      "loss": 1.4958,
      "step": 4244
    },
    {
      "epoch": 1.6434378629500581,
      "grad_norm": 18.63460922241211,
      "learning_rate": 9.285069041166603e-06,
      "loss": 2.016,
      "step": 4245
    },
    {
      "epoch": 1.6438250096786682,
      "grad_norm": 16.705488204956055,
      "learning_rate": 9.284638878134814e-06,
      "loss": 1.6284,
      "step": 4246
    },
    {
      "epoch": 1.6442121564072782,
      "grad_norm": 18.565675735473633,
      "learning_rate": 9.284208715103024e-06,
      "loss": 1.2204,
      "step": 4247
    },
    {
      "epoch": 1.6445993031358885,
      "grad_norm": 24.591215133666992,
      "learning_rate": 9.283778552071236e-06,
      "loss": 1.6367,
      "step": 4248
    },
    {
      "epoch": 1.6449864498644986,
      "grad_norm": 14.470715522766113,
      "learning_rate": 9.283348389039447e-06,
      "loss": 1.267,
      "step": 4249
    },
    {
      "epoch": 1.645373596593109,
      "grad_norm": 15.463166236877441,
      "learning_rate": 9.282918226007658e-06,
      "loss": 1.5126,
      "step": 4250
    },
    {
      "epoch": 1.645760743321719,
      "grad_norm": 15.275872230529785,
      "learning_rate": 9.282488062975868e-06,
      "loss": 1.6545,
      "step": 4251
    },
    {
      "epoch": 1.646147890050329,
      "grad_norm": 13.41774845123291,
      "learning_rate": 9.28205789994408e-06,
      "loss": 1.4796,
      "step": 4252
    },
    {
      "epoch": 1.646535036778939,
      "grad_norm": 33.589622497558594,
      "learning_rate": 9.281627736912291e-06,
      "loss": 2.7132,
      "step": 4253
    },
    {
      "epoch": 1.6469221835075494,
      "grad_norm": 17.634363174438477,
      "learning_rate": 9.281197573880502e-06,
      "loss": 1.6941,
      "step": 4254
    },
    {
      "epoch": 1.6473093302361597,
      "grad_norm": 32.556678771972656,
      "learning_rate": 9.280767410848712e-06,
      "loss": 1.1174,
      "step": 4255
    },
    {
      "epoch": 1.6476964769647697,
      "grad_norm": 17.827165603637695,
      "learning_rate": 9.280337247816923e-06,
      "loss": 0.3475,
      "step": 4256
    },
    {
      "epoch": 1.6480836236933798,
      "grad_norm": 26.73976707458496,
      "learning_rate": 9.279907084785135e-06,
      "loss": 1.3809,
      "step": 4257
    },
    {
      "epoch": 1.6484707704219899,
      "grad_norm": 26.423274993896484,
      "learning_rate": 9.279476921753345e-06,
      "loss": 1.3127,
      "step": 4258
    },
    {
      "epoch": 1.6488579171506,
      "grad_norm": 20.93657875061035,
      "learning_rate": 9.279046758721556e-06,
      "loss": 1.7048,
      "step": 4259
    },
    {
      "epoch": 1.6492450638792102,
      "grad_norm": 17.898195266723633,
      "learning_rate": 9.278616595689767e-06,
      "loss": 1.2714,
      "step": 4260
    },
    {
      "epoch": 1.6496322106078205,
      "grad_norm": 18.79642677307129,
      "learning_rate": 9.278186432657979e-06,
      "loss": 1.6333,
      "step": 4261
    },
    {
      "epoch": 1.6500193573364306,
      "grad_norm": 18.392478942871094,
      "learning_rate": 9.277756269626189e-06,
      "loss": 1.3411,
      "step": 4262
    },
    {
      "epoch": 1.6504065040650406,
      "grad_norm": 15.21732234954834,
      "learning_rate": 9.2773261065944e-06,
      "loss": 1.0661,
      "step": 4263
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 17.833663940429688,
      "learning_rate": 9.27689594356261e-06,
      "loss": 1.5846,
      "step": 4264
    },
    {
      "epoch": 1.651180797522261,
      "grad_norm": 11.36895751953125,
      "learning_rate": 9.276465780530823e-06,
      "loss": 0.6142,
      "step": 4265
    },
    {
      "epoch": 1.651567944250871,
      "grad_norm": 24.013355255126953,
      "learning_rate": 9.276035617499033e-06,
      "loss": 1.7517,
      "step": 4266
    },
    {
      "epoch": 1.6519550909794813,
      "grad_norm": 14.679115295410156,
      "learning_rate": 9.275605454467244e-06,
      "loss": 1.1594,
      "step": 4267
    },
    {
      "epoch": 1.6523422377080914,
      "grad_norm": 26.492172241210938,
      "learning_rate": 9.275175291435454e-06,
      "loss": 1.1012,
      "step": 4268
    },
    {
      "epoch": 1.6527293844367015,
      "grad_norm": 28.05732536315918,
      "learning_rate": 9.274745128403667e-06,
      "loss": 1.7222,
      "step": 4269
    },
    {
      "epoch": 1.6531165311653115,
      "grad_norm": 12.832670211791992,
      "learning_rate": 9.274314965371877e-06,
      "loss": 0.7658,
      "step": 4270
    },
    {
      "epoch": 1.6535036778939218,
      "grad_norm": 31.60272979736328,
      "learning_rate": 9.273884802340088e-06,
      "loss": 2.6542,
      "step": 4271
    },
    {
      "epoch": 1.653890824622532,
      "grad_norm": 29.59255599975586,
      "learning_rate": 9.2734546393083e-06,
      "loss": 1.4137,
      "step": 4272
    },
    {
      "epoch": 1.6542779713511422,
      "grad_norm": 25.302927017211914,
      "learning_rate": 9.273024476276509e-06,
      "loss": 2.4924,
      "step": 4273
    },
    {
      "epoch": 1.6546651180797523,
      "grad_norm": 15.19959831237793,
      "learning_rate": 9.27259431324472e-06,
      "loss": 1.2752,
      "step": 4274
    },
    {
      "epoch": 1.6550522648083623,
      "grad_norm": 25.1585750579834,
      "learning_rate": 9.272164150212932e-06,
      "loss": 3.1266,
      "step": 4275
    },
    {
      "epoch": 1.6554394115369724,
      "grad_norm": 31.60601043701172,
      "learning_rate": 9.271733987181143e-06,
      "loss": 2.4035,
      "step": 4276
    },
    {
      "epoch": 1.6558265582655827,
      "grad_norm": 13.022612571716309,
      "learning_rate": 9.271303824149353e-06,
      "loss": 0.9478,
      "step": 4277
    },
    {
      "epoch": 1.656213704994193,
      "grad_norm": 14.370631217956543,
      "learning_rate": 9.270873661117564e-06,
      "loss": 0.7121,
      "step": 4278
    },
    {
      "epoch": 1.656600851722803,
      "grad_norm": 33.447235107421875,
      "learning_rate": 9.270443498085774e-06,
      "loss": 1.609,
      "step": 4279
    },
    {
      "epoch": 1.656987998451413,
      "grad_norm": 18.562095642089844,
      "learning_rate": 9.270013335053987e-06,
      "loss": 1.874,
      "step": 4280
    },
    {
      "epoch": 1.6573751451800232,
      "grad_norm": 16.100461959838867,
      "learning_rate": 9.269583172022197e-06,
      "loss": 0.8413,
      "step": 4281
    },
    {
      "epoch": 1.6577622919086332,
      "grad_norm": 17.66569709777832,
      "learning_rate": 9.269153008990408e-06,
      "loss": 1.983,
      "step": 4282
    },
    {
      "epoch": 1.6581494386372435,
      "grad_norm": 27.425697326660156,
      "learning_rate": 9.268722845958618e-06,
      "loss": 1.4489,
      "step": 4283
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 22.65583038330078,
      "learning_rate": 9.268292682926831e-06,
      "loss": 1.4231,
      "step": 4284
    },
    {
      "epoch": 1.6589237320944639,
      "grad_norm": 20.165077209472656,
      "learning_rate": 9.267862519895041e-06,
      "loss": 1.0635,
      "step": 4285
    },
    {
      "epoch": 1.659310878823074,
      "grad_norm": 14.02633285522461,
      "learning_rate": 9.267432356863252e-06,
      "loss": 1.8176,
      "step": 4286
    },
    {
      "epoch": 1.659698025551684,
      "grad_norm": 14.355517387390137,
      "learning_rate": 9.267002193831462e-06,
      "loss": 1.4333,
      "step": 4287
    },
    {
      "epoch": 1.6600851722802943,
      "grad_norm": 36.96015548706055,
      "learning_rate": 9.266572030799674e-06,
      "loss": 1.6492,
      "step": 4288
    },
    {
      "epoch": 1.6604723190089044,
      "grad_norm": 25.71731948852539,
      "learning_rate": 9.266141867767885e-06,
      "loss": 1.6743,
      "step": 4289
    },
    {
      "epoch": 1.6608594657375146,
      "grad_norm": 30.2637882232666,
      "learning_rate": 9.265711704736096e-06,
      "loss": 2.5192,
      "step": 4290
    },
    {
      "epoch": 1.6612466124661247,
      "grad_norm": 40.778900146484375,
      "learning_rate": 9.265281541704306e-06,
      "loss": 2.7306,
      "step": 4291
    },
    {
      "epoch": 1.6616337591947348,
      "grad_norm": 15.002294540405273,
      "learning_rate": 9.264851378672518e-06,
      "loss": 1.8274,
      "step": 4292
    },
    {
      "epoch": 1.6620209059233448,
      "grad_norm": 44.533206939697266,
      "learning_rate": 9.264421215640729e-06,
      "loss": 1.7553,
      "step": 4293
    },
    {
      "epoch": 1.6624080526519551,
      "grad_norm": 5.980274200439453,
      "learning_rate": 9.263991052608939e-06,
      "loss": 0.1978,
      "step": 4294
    },
    {
      "epoch": 1.6627951993805652,
      "grad_norm": 22.488052368164062,
      "learning_rate": 9.26356088957715e-06,
      "loss": 1.5714,
      "step": 4295
    },
    {
      "epoch": 1.6631823461091755,
      "grad_norm": 35.71044158935547,
      "learning_rate": 9.263130726545361e-06,
      "loss": 1.4679,
      "step": 4296
    },
    {
      "epoch": 1.6635694928377855,
      "grad_norm": 16.018213272094727,
      "learning_rate": 9.262700563513573e-06,
      "loss": 1.6096,
      "step": 4297
    },
    {
      "epoch": 1.6639566395663956,
      "grad_norm": 32.71603775024414,
      "learning_rate": 9.262270400481783e-06,
      "loss": 1.745,
      "step": 4298
    },
    {
      "epoch": 1.6643437862950057,
      "grad_norm": 36.99655532836914,
      "learning_rate": 9.261840237449994e-06,
      "loss": 2.0758,
      "step": 4299
    },
    {
      "epoch": 1.664730933023616,
      "grad_norm": 33.20533752441406,
      "learning_rate": 9.261410074418205e-06,
      "loss": 2.9374,
      "step": 4300
    },
    {
      "epoch": 1.6651180797522263,
      "grad_norm": 30.99820899963379,
      "learning_rate": 9.260979911386417e-06,
      "loss": 2.0709,
      "step": 4301
    },
    {
      "epoch": 1.6655052264808363,
      "grad_norm": 29.09837532043457,
      "learning_rate": 9.260549748354627e-06,
      "loss": 0.6687,
      "step": 4302
    },
    {
      "epoch": 1.6658923732094464,
      "grad_norm": 13.331209182739258,
      "learning_rate": 9.260119585322838e-06,
      "loss": 0.9565,
      "step": 4303
    },
    {
      "epoch": 1.6662795199380565,
      "grad_norm": 26.241634368896484,
      "learning_rate": 9.25968942229105e-06,
      "loss": 2.8861,
      "step": 4304
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 35.369903564453125,
      "learning_rate": 9.25925925925926e-06,
      "loss": 2.2062,
      "step": 4305
    },
    {
      "epoch": 1.6670538133952768,
      "grad_norm": 21.87674903869629,
      "learning_rate": 9.25882909622747e-06,
      "loss": 1.0504,
      "step": 4306
    },
    {
      "epoch": 1.667440960123887,
      "grad_norm": 15.63996696472168,
      "learning_rate": 9.258398933195682e-06,
      "loss": 0.9692,
      "step": 4307
    },
    {
      "epoch": 1.6678281068524972,
      "grad_norm": 14.117318153381348,
      "learning_rate": 9.257968770163893e-06,
      "loss": 0.8856,
      "step": 4308
    },
    {
      "epoch": 1.6682152535811072,
      "grad_norm": 22.95919418334961,
      "learning_rate": 9.257538607132103e-06,
      "loss": 1.7984,
      "step": 4309
    },
    {
      "epoch": 1.6686024003097173,
      "grad_norm": 23.24422264099121,
      "learning_rate": 9.257108444100315e-06,
      "loss": 2.2067,
      "step": 4310
    },
    {
      "epoch": 1.6689895470383276,
      "grad_norm": 15.208248138427734,
      "learning_rate": 9.256678281068526e-06,
      "loss": 0.9882,
      "step": 4311
    },
    {
      "epoch": 1.6693766937669376,
      "grad_norm": 13.944034576416016,
      "learning_rate": 9.256248118036737e-06,
      "loss": 1.1578,
      "step": 4312
    },
    {
      "epoch": 1.669763840495548,
      "grad_norm": 10.636375427246094,
      "learning_rate": 9.255817955004947e-06,
      "loss": 0.6338,
      "step": 4313
    },
    {
      "epoch": 1.670150987224158,
      "grad_norm": 22.313589096069336,
      "learning_rate": 9.255387791973158e-06,
      "loss": 1.598,
      "step": 4314
    },
    {
      "epoch": 1.670538133952768,
      "grad_norm": 16.271928787231445,
      "learning_rate": 9.25495762894137e-06,
      "loss": 1.2873,
      "step": 4315
    },
    {
      "epoch": 1.6709252806813781,
      "grad_norm": 8.821406364440918,
      "learning_rate": 9.254527465909581e-06,
      "loss": 0.5773,
      "step": 4316
    },
    {
      "epoch": 1.6713124274099884,
      "grad_norm": 24.63607406616211,
      "learning_rate": 9.254097302877791e-06,
      "loss": 1.6379,
      "step": 4317
    },
    {
      "epoch": 1.6716995741385985,
      "grad_norm": 21.456645965576172,
      "learning_rate": 9.253667139846002e-06,
      "loss": 2.003,
      "step": 4318
    },
    {
      "epoch": 1.6720867208672088,
      "grad_norm": 25.302913665771484,
      "learning_rate": 9.253236976814214e-06,
      "loss": 1.9521,
      "step": 4319
    },
    {
      "epoch": 1.6724738675958188,
      "grad_norm": 18.597942352294922,
      "learning_rate": 9.252806813782425e-06,
      "loss": 1.6401,
      "step": 4320
    },
    {
      "epoch": 1.672861014324429,
      "grad_norm": 22.29169273376465,
      "learning_rate": 9.252376650750635e-06,
      "loss": 2.1138,
      "step": 4321
    },
    {
      "epoch": 1.673248161053039,
      "grad_norm": 19.93821907043457,
      "learning_rate": 9.251946487718846e-06,
      "loss": 1.3708,
      "step": 4322
    },
    {
      "epoch": 1.6736353077816493,
      "grad_norm": 11.243326187133789,
      "learning_rate": 9.251516324687058e-06,
      "loss": 0.9328,
      "step": 4323
    },
    {
      "epoch": 1.6740224545102595,
      "grad_norm": 17.13962745666504,
      "learning_rate": 9.251086161655268e-06,
      "loss": 1.6368,
      "step": 4324
    },
    {
      "epoch": 1.6744096012388696,
      "grad_norm": 11.87857723236084,
      "learning_rate": 9.250655998623479e-06,
      "loss": 1.0555,
      "step": 4325
    },
    {
      "epoch": 1.6747967479674797,
      "grad_norm": 20.99456214904785,
      "learning_rate": 9.25022583559169e-06,
      "loss": 1.7774,
      "step": 4326
    },
    {
      "epoch": 1.6751838946960897,
      "grad_norm": 15.499956130981445,
      "learning_rate": 9.249795672559902e-06,
      "loss": 1.2343,
      "step": 4327
    },
    {
      "epoch": 1.6755710414246998,
      "grad_norm": 18.337026596069336,
      "learning_rate": 9.249365509528112e-06,
      "loss": 1.3866,
      "step": 4328
    },
    {
      "epoch": 1.67595818815331,
      "grad_norm": 13.371996879577637,
      "learning_rate": 9.248935346496323e-06,
      "loss": 0.9432,
      "step": 4329
    },
    {
      "epoch": 1.6763453348819204,
      "grad_norm": 13.966998100280762,
      "learning_rate": 9.248505183464533e-06,
      "loss": 0.8803,
      "step": 4330
    },
    {
      "epoch": 1.6767324816105305,
      "grad_norm": 16.525474548339844,
      "learning_rate": 9.248075020432746e-06,
      "loss": 1.5461,
      "step": 4331
    },
    {
      "epoch": 1.6771196283391405,
      "grad_norm": 26.456859588623047,
      "learning_rate": 9.247644857400956e-06,
      "loss": 1.5127,
      "step": 4332
    },
    {
      "epoch": 1.6775067750677506,
      "grad_norm": 21.527803421020508,
      "learning_rate": 9.247214694369167e-06,
      "loss": 1.6696,
      "step": 4333
    },
    {
      "epoch": 1.6778939217963607,
      "grad_norm": 17.677209854125977,
      "learning_rate": 9.246784531337377e-06,
      "loss": 1.2548,
      "step": 4334
    },
    {
      "epoch": 1.678281068524971,
      "grad_norm": 22.845773696899414,
      "learning_rate": 9.24635436830559e-06,
      "loss": 1.5632,
      "step": 4335
    },
    {
      "epoch": 1.6786682152535812,
      "grad_norm": 25.04315757751465,
      "learning_rate": 9.2459242052738e-06,
      "loss": 1.5574,
      "step": 4336
    },
    {
      "epoch": 1.6790553619821913,
      "grad_norm": 19.744699478149414,
      "learning_rate": 9.245494042242011e-06,
      "loss": 1.7126,
      "step": 4337
    },
    {
      "epoch": 1.6794425087108014,
      "grad_norm": 17.08208656311035,
      "learning_rate": 9.24506387921022e-06,
      "loss": 1.65,
      "step": 4338
    },
    {
      "epoch": 1.6798296554394114,
      "grad_norm": 28.14984703063965,
      "learning_rate": 9.244633716178432e-06,
      "loss": 1.8355,
      "step": 4339
    },
    {
      "epoch": 1.6802168021680217,
      "grad_norm": 13.34072208404541,
      "learning_rate": 9.244203553146643e-06,
      "loss": 1.3777,
      "step": 4340
    },
    {
      "epoch": 1.6806039488966318,
      "grad_norm": 18.479145050048828,
      "learning_rate": 9.243773390114855e-06,
      "loss": 1.3024,
      "step": 4341
    },
    {
      "epoch": 1.680991095625242,
      "grad_norm": 19.02326202392578,
      "learning_rate": 9.243343227083065e-06,
      "loss": 1.6691,
      "step": 4342
    },
    {
      "epoch": 1.6813782423538521,
      "grad_norm": 25.373098373413086,
      "learning_rate": 9.242913064051276e-06,
      "loss": 1.8547,
      "step": 4343
    },
    {
      "epoch": 1.6817653890824622,
      "grad_norm": 15.198352813720703,
      "learning_rate": 9.242482901019487e-06,
      "loss": 1.4839,
      "step": 4344
    },
    {
      "epoch": 1.6821525358110723,
      "grad_norm": 24.25505256652832,
      "learning_rate": 9.242052737987697e-06,
      "loss": 1.0619,
      "step": 4345
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 14.855430603027344,
      "learning_rate": 9.241622574955909e-06,
      "loss": 0.7748,
      "step": 4346
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 20.986637115478516,
      "learning_rate": 9.24119241192412e-06,
      "loss": 1.7112,
      "step": 4347
    },
    {
      "epoch": 1.683313975996903,
      "grad_norm": 19.854455947875977,
      "learning_rate": 9.240762248892331e-06,
      "loss": 1.3627,
      "step": 4348
    },
    {
      "epoch": 1.683701122725513,
      "grad_norm": 25.990665435791016,
      "learning_rate": 9.240332085860541e-06,
      "loss": 1.7093,
      "step": 4349
    },
    {
      "epoch": 1.684088269454123,
      "grad_norm": 20.63603973388672,
      "learning_rate": 9.239901922828753e-06,
      "loss": 1.28,
      "step": 4350
    },
    {
      "epoch": 1.684475416182733,
      "grad_norm": 26.045167922973633,
      "learning_rate": 9.239471759796964e-06,
      "loss": 1.408,
      "step": 4351
    },
    {
      "epoch": 1.6848625629113434,
      "grad_norm": 17.984678268432617,
      "learning_rate": 9.239041596765175e-06,
      "loss": 1.4778,
      "step": 4352
    },
    {
      "epoch": 1.6852497096399537,
      "grad_norm": 16.21076774597168,
      "learning_rate": 9.238611433733385e-06,
      "loss": 1.5372,
      "step": 4353
    },
    {
      "epoch": 1.6856368563685638,
      "grad_norm": 8.907779693603516,
      "learning_rate": 9.238181270701596e-06,
      "loss": 1.1863,
      "step": 4354
    },
    {
      "epoch": 1.6860240030971738,
      "grad_norm": 25.727447509765625,
      "learning_rate": 9.237751107669808e-06,
      "loss": 1.6503,
      "step": 4355
    },
    {
      "epoch": 1.6864111498257839,
      "grad_norm": 21.82853889465332,
      "learning_rate": 9.23732094463802e-06,
      "loss": 1.2793,
      "step": 4356
    },
    {
      "epoch": 1.686798296554394,
      "grad_norm": 21.169342041015625,
      "learning_rate": 9.236890781606229e-06,
      "loss": 0.9887,
      "step": 4357
    },
    {
      "epoch": 1.6871854432830042,
      "grad_norm": 27.636934280395508,
      "learning_rate": 9.23646061857444e-06,
      "loss": 1.6634,
      "step": 4358
    },
    {
      "epoch": 1.6875725900116145,
      "grad_norm": 17.6590576171875,
      "learning_rate": 9.236030455542652e-06,
      "loss": 1.8629,
      "step": 4359
    },
    {
      "epoch": 1.6879597367402246,
      "grad_norm": 15.729812622070312,
      "learning_rate": 9.235600292510862e-06,
      "loss": 1.4779,
      "step": 4360
    },
    {
      "epoch": 1.6883468834688347,
      "grad_norm": 18.04213523864746,
      "learning_rate": 9.235170129479073e-06,
      "loss": 1.2494,
      "step": 4361
    },
    {
      "epoch": 1.6887340301974447,
      "grad_norm": 20.036334991455078,
      "learning_rate": 9.234739966447284e-06,
      "loss": 0.9702,
      "step": 4362
    },
    {
      "epoch": 1.689121176926055,
      "grad_norm": 17.286176681518555,
      "learning_rate": 9.234309803415496e-06,
      "loss": 1.5683,
      "step": 4363
    },
    {
      "epoch": 1.689508323654665,
      "grad_norm": 15.921704292297363,
      "learning_rate": 9.233879640383706e-06,
      "loss": 1.3013,
      "step": 4364
    },
    {
      "epoch": 1.6898954703832754,
      "grad_norm": 21.105113983154297,
      "learning_rate": 9.233449477351917e-06,
      "loss": 2.3469,
      "step": 4365
    },
    {
      "epoch": 1.6902826171118854,
      "grad_norm": 9.725896835327148,
      "learning_rate": 9.233019314320128e-06,
      "loss": 0.6969,
      "step": 4366
    },
    {
      "epoch": 1.6906697638404955,
      "grad_norm": 22.006921768188477,
      "learning_rate": 9.23258915128834e-06,
      "loss": 1.5145,
      "step": 4367
    },
    {
      "epoch": 1.6910569105691056,
      "grad_norm": 27.907228469848633,
      "learning_rate": 9.23215898825655e-06,
      "loss": 2.157,
      "step": 4368
    },
    {
      "epoch": 1.6914440572977159,
      "grad_norm": 14.99786376953125,
      "learning_rate": 9.231728825224761e-06,
      "loss": 1.5696,
      "step": 4369
    },
    {
      "epoch": 1.6918312040263261,
      "grad_norm": 24.94982147216797,
      "learning_rate": 9.231298662192972e-06,
      "loss": 1.8884,
      "step": 4370
    },
    {
      "epoch": 1.6922183507549362,
      "grad_norm": 17.390066146850586,
      "learning_rate": 9.230868499161184e-06,
      "loss": 1.5323,
      "step": 4371
    },
    {
      "epoch": 1.6926054974835463,
      "grad_norm": 11.922277450561523,
      "learning_rate": 9.230438336129394e-06,
      "loss": 1.4112,
      "step": 4372
    },
    {
      "epoch": 1.6929926442121563,
      "grad_norm": 34.425254821777344,
      "learning_rate": 9.230008173097605e-06,
      "loss": 1.8581,
      "step": 4373
    },
    {
      "epoch": 1.6933797909407664,
      "grad_norm": 28.555896759033203,
      "learning_rate": 9.229578010065816e-06,
      "loss": 0.847,
      "step": 4374
    },
    {
      "epoch": 1.6937669376693767,
      "grad_norm": 40.06651306152344,
      "learning_rate": 9.229147847034026e-06,
      "loss": 1.6084,
      "step": 4375
    },
    {
      "epoch": 1.694154084397987,
      "grad_norm": 21.804182052612305,
      "learning_rate": 9.228717684002237e-06,
      "loss": 1.7286,
      "step": 4376
    },
    {
      "epoch": 1.694541231126597,
      "grad_norm": 10.066481590270996,
      "learning_rate": 9.228287520970449e-06,
      "loss": 0.6464,
      "step": 4377
    },
    {
      "epoch": 1.6949283778552071,
      "grad_norm": 29.237218856811523,
      "learning_rate": 9.22785735793866e-06,
      "loss": 2.1054,
      "step": 4378
    },
    {
      "epoch": 1.6953155245838172,
      "grad_norm": 17.246931076049805,
      "learning_rate": 9.22742719490687e-06,
      "loss": 1.5911,
      "step": 4379
    },
    {
      "epoch": 1.6957026713124272,
      "grad_norm": 13.901362419128418,
      "learning_rate": 9.226997031875081e-06,
      "loss": 1.3007,
      "step": 4380
    },
    {
      "epoch": 1.6960898180410375,
      "grad_norm": 27.20305633544922,
      "learning_rate": 9.226566868843291e-06,
      "loss": 1.9295,
      "step": 4381
    },
    {
      "epoch": 1.6964769647696478,
      "grad_norm": 13.889681816101074,
      "learning_rate": 9.226136705811504e-06,
      "loss": 1.4145,
      "step": 4382
    },
    {
      "epoch": 1.6968641114982579,
      "grad_norm": 20.579198837280273,
      "learning_rate": 9.225706542779714e-06,
      "loss": 1.6057,
      "step": 4383
    },
    {
      "epoch": 1.697251258226868,
      "grad_norm": 23.714479446411133,
      "learning_rate": 9.225276379747925e-06,
      "loss": 1.3813,
      "step": 4384
    },
    {
      "epoch": 1.697638404955478,
      "grad_norm": 14.421454429626465,
      "learning_rate": 9.224846216716135e-06,
      "loss": 1.4211,
      "step": 4385
    },
    {
      "epoch": 1.6980255516840883,
      "grad_norm": 21.036401748657227,
      "learning_rate": 9.224416053684348e-06,
      "loss": 1.7569,
      "step": 4386
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 25.38158416748047,
      "learning_rate": 9.223985890652558e-06,
      "loss": 2.5442,
      "step": 4387
    },
    {
      "epoch": 1.6987998451413087,
      "grad_norm": 12.267827987670898,
      "learning_rate": 9.22355572762077e-06,
      "loss": 0.8069,
      "step": 4388
    },
    {
      "epoch": 1.6991869918699187,
      "grad_norm": 29.39716911315918,
      "learning_rate": 9.223125564588979e-06,
      "loss": 2.5682,
      "step": 4389
    },
    {
      "epoch": 1.6995741385985288,
      "grad_norm": 18.356081008911133,
      "learning_rate": 9.22269540155719e-06,
      "loss": 1.2736,
      "step": 4390
    },
    {
      "epoch": 1.6999612853271389,
      "grad_norm": 14.93539047241211,
      "learning_rate": 9.222265238525402e-06,
      "loss": 1.7794,
      "step": 4391
    },
    {
      "epoch": 1.7003484320557491,
      "grad_norm": 24.712745666503906,
      "learning_rate": 9.221835075493613e-06,
      "loss": 2.1763,
      "step": 4392
    },
    {
      "epoch": 1.7007355787843594,
      "grad_norm": 17.965181350708008,
      "learning_rate": 9.221404912461823e-06,
      "loss": 1.3611,
      "step": 4393
    },
    {
      "epoch": 1.7011227255129695,
      "grad_norm": 27.850086212158203,
      "learning_rate": 9.220974749430034e-06,
      "loss": 1.4486,
      "step": 4394
    },
    {
      "epoch": 1.7015098722415796,
      "grad_norm": 16.120466232299805,
      "learning_rate": 9.220544586398246e-06,
      "loss": 1.6277,
      "step": 4395
    },
    {
      "epoch": 1.7018970189701896,
      "grad_norm": 10.582768440246582,
      "learning_rate": 9.220114423366456e-06,
      "loss": 1.2914,
      "step": 4396
    },
    {
      "epoch": 1.7022841656987997,
      "grad_norm": 31.721302032470703,
      "learning_rate": 9.219684260334669e-06,
      "loss": 1.7744,
      "step": 4397
    },
    {
      "epoch": 1.70267131242741,
      "grad_norm": 23.28312110900879,
      "learning_rate": 9.219254097302878e-06,
      "loss": 2.0182,
      "step": 4398
    },
    {
      "epoch": 1.7030584591560203,
      "grad_norm": 27.66520118713379,
      "learning_rate": 9.21882393427109e-06,
      "loss": 1.8441,
      "step": 4399
    },
    {
      "epoch": 1.7034456058846303,
      "grad_norm": 29.0831298828125,
      "learning_rate": 9.2183937712393e-06,
      "loss": 1.4614,
      "step": 4400
    },
    {
      "epoch": 1.7038327526132404,
      "grad_norm": 22.761245727539062,
      "learning_rate": 9.217963608207513e-06,
      "loss": 0.9167,
      "step": 4401
    },
    {
      "epoch": 1.7042198993418505,
      "grad_norm": 23.648914337158203,
      "learning_rate": 9.217533445175722e-06,
      "loss": 1.5211,
      "step": 4402
    },
    {
      "epoch": 1.7046070460704605,
      "grad_norm": 40.65217971801758,
      "learning_rate": 9.217103282143934e-06,
      "loss": 0.5159,
      "step": 4403
    },
    {
      "epoch": 1.7049941927990708,
      "grad_norm": 18.970834732055664,
      "learning_rate": 9.216673119112144e-06,
      "loss": 1.3421,
      "step": 4404
    },
    {
      "epoch": 1.7053813395276811,
      "grad_norm": 16.80160903930664,
      "learning_rate": 9.216242956080355e-06,
      "loss": 0.9544,
      "step": 4405
    },
    {
      "epoch": 1.7057684862562912,
      "grad_norm": 17.951404571533203,
      "learning_rate": 9.215812793048566e-06,
      "loss": 1.4003,
      "step": 4406
    },
    {
      "epoch": 1.7061556329849012,
      "grad_norm": 27.85550308227539,
      "learning_rate": 9.215382630016778e-06,
      "loss": 1.585,
      "step": 4407
    },
    {
      "epoch": 1.7065427797135113,
      "grad_norm": 13.530662536621094,
      "learning_rate": 9.214952466984988e-06,
      "loss": 1.7994,
      "step": 4408
    },
    {
      "epoch": 1.7069299264421216,
      "grad_norm": 16.028663635253906,
      "learning_rate": 9.214522303953199e-06,
      "loss": 0.9455,
      "step": 4409
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 18.46578025817871,
      "learning_rate": 9.21409214092141e-06,
      "loss": 1.6046,
      "step": 4410
    },
    {
      "epoch": 1.707704219899342,
      "grad_norm": 21.805103302001953,
      "learning_rate": 9.21366197788962e-06,
      "loss": 1.0571,
      "step": 4411
    },
    {
      "epoch": 1.708091366627952,
      "grad_norm": 14.879240989685059,
      "learning_rate": 9.213231814857831e-06,
      "loss": 1.5315,
      "step": 4412
    },
    {
      "epoch": 1.708478513356562,
      "grad_norm": 15.903364181518555,
      "learning_rate": 9.212801651826043e-06,
      "loss": 1.5105,
      "step": 4413
    },
    {
      "epoch": 1.7088656600851722,
      "grad_norm": 20.67316436767578,
      "learning_rate": 9.212371488794254e-06,
      "loss": 1.7013,
      "step": 4414
    },
    {
      "epoch": 1.7092528068137824,
      "grad_norm": 30.050981521606445,
      "learning_rate": 9.211941325762464e-06,
      "loss": 1.9887,
      "step": 4415
    },
    {
      "epoch": 1.7096399535423927,
      "grad_norm": 18.067928314208984,
      "learning_rate": 9.211511162730675e-06,
      "loss": 1.7537,
      "step": 4416
    },
    {
      "epoch": 1.7100271002710028,
      "grad_norm": 24.930084228515625,
      "learning_rate": 9.211080999698887e-06,
      "loss": 1.8374,
      "step": 4417
    },
    {
      "epoch": 1.7104142469996129,
      "grad_norm": 22.5344181060791,
      "learning_rate": 9.210650836667098e-06,
      "loss": 1.6306,
      "step": 4418
    },
    {
      "epoch": 1.710801393728223,
      "grad_norm": 21.78977394104004,
      "learning_rate": 9.210220673635308e-06,
      "loss": 1.3708,
      "step": 4419
    },
    {
      "epoch": 1.711188540456833,
      "grad_norm": 24.17893409729004,
      "learning_rate": 9.20979051060352e-06,
      "loss": 1.0595,
      "step": 4420
    },
    {
      "epoch": 1.7115756871854433,
      "grad_norm": 31.7674560546875,
      "learning_rate": 9.209360347571731e-06,
      "loss": 1.3054,
      "step": 4421
    },
    {
      "epoch": 1.7119628339140536,
      "grad_norm": 24.29538345336914,
      "learning_rate": 9.208930184539942e-06,
      "loss": 1.8919,
      "step": 4422
    },
    {
      "epoch": 1.7123499806426636,
      "grad_norm": 10.830338478088379,
      "learning_rate": 9.208500021508152e-06,
      "loss": 1.1054,
      "step": 4423
    },
    {
      "epoch": 1.7127371273712737,
      "grad_norm": 27.32147789001465,
      "learning_rate": 9.208069858476363e-06,
      "loss": 1.2957,
      "step": 4424
    },
    {
      "epoch": 1.7131242740998838,
      "grad_norm": 7.731200218200684,
      "learning_rate": 9.207639695444575e-06,
      "loss": 0.5072,
      "step": 4425
    },
    {
      "epoch": 1.7135114208284938,
      "grad_norm": 22.019245147705078,
      "learning_rate": 9.207209532412785e-06,
      "loss": 1.4947,
      "step": 4426
    },
    {
      "epoch": 1.7138985675571041,
      "grad_norm": 12.086294174194336,
      "learning_rate": 9.206779369380996e-06,
      "loss": 0.9641,
      "step": 4427
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 20.408462524414062,
      "learning_rate": 9.206349206349207e-06,
      "loss": 1.8556,
      "step": 4428
    },
    {
      "epoch": 1.7146728610143245,
      "grad_norm": 17.958539962768555,
      "learning_rate": 9.205919043317419e-06,
      "loss": 0.9744,
      "step": 4429
    },
    {
      "epoch": 1.7150600077429345,
      "grad_norm": 23.242630004882812,
      "learning_rate": 9.205488880285629e-06,
      "loss": 1.4977,
      "step": 4430
    },
    {
      "epoch": 1.7154471544715446,
      "grad_norm": 16.577308654785156,
      "learning_rate": 9.20505871725384e-06,
      "loss": 1.6963,
      "step": 4431
    },
    {
      "epoch": 1.715834301200155,
      "grad_norm": 10.314142227172852,
      "learning_rate": 9.20462855422205e-06,
      "loss": 1.081,
      "step": 4432
    },
    {
      "epoch": 1.716221447928765,
      "grad_norm": 21.2368106842041,
      "learning_rate": 9.204198391190263e-06,
      "loss": 1.5796,
      "step": 4433
    },
    {
      "epoch": 1.7166085946573753,
      "grad_norm": 22.866003036499023,
      "learning_rate": 9.203768228158472e-06,
      "loss": 1.2523,
      "step": 4434
    },
    {
      "epoch": 1.7169957413859853,
      "grad_norm": 17.150279998779297,
      "learning_rate": 9.203338065126684e-06,
      "loss": 2.4068,
      "step": 4435
    },
    {
      "epoch": 1.7173828881145954,
      "grad_norm": 7.560318946838379,
      "learning_rate": 9.202907902094895e-06,
      "loss": 0.4673,
      "step": 4436
    },
    {
      "epoch": 1.7177700348432055,
      "grad_norm": 15.207324028015137,
      "learning_rate": 9.202477739063107e-06,
      "loss": 1.5275,
      "step": 4437
    },
    {
      "epoch": 1.7181571815718157,
      "grad_norm": 29.267520904541016,
      "learning_rate": 9.202047576031316e-06,
      "loss": 1.9138,
      "step": 4438
    },
    {
      "epoch": 1.718544328300426,
      "grad_norm": 24.074359893798828,
      "learning_rate": 9.201617412999528e-06,
      "loss": 0.8051,
      "step": 4439
    },
    {
      "epoch": 1.718931475029036,
      "grad_norm": 24.182941436767578,
      "learning_rate": 9.20118724996774e-06,
      "loss": 1.1398,
      "step": 4440
    },
    {
      "epoch": 1.7193186217576462,
      "grad_norm": 21.37498664855957,
      "learning_rate": 9.200757086935949e-06,
      "loss": 0.9702,
      "step": 4441
    },
    {
      "epoch": 1.7197057684862562,
      "grad_norm": 35.72555923461914,
      "learning_rate": 9.20032692390416e-06,
      "loss": 1.9244,
      "step": 4442
    },
    {
      "epoch": 1.7200929152148663,
      "grad_norm": 18.983535766601562,
      "learning_rate": 9.199896760872372e-06,
      "loss": 1.7488,
      "step": 4443
    },
    {
      "epoch": 1.7204800619434766,
      "grad_norm": 28.08824348449707,
      "learning_rate": 9.199466597840583e-06,
      "loss": 1.0929,
      "step": 4444
    },
    {
      "epoch": 1.7208672086720869,
      "grad_norm": 33.339229583740234,
      "learning_rate": 9.199036434808793e-06,
      "loss": 1.3901,
      "step": 4445
    },
    {
      "epoch": 1.721254355400697,
      "grad_norm": 26.58721351623535,
      "learning_rate": 9.198606271777004e-06,
      "loss": 2.0234,
      "step": 4446
    },
    {
      "epoch": 1.721641502129307,
      "grad_norm": 18.25115394592285,
      "learning_rate": 9.198176108745214e-06,
      "loss": 0.8991,
      "step": 4447
    },
    {
      "epoch": 1.722028648857917,
      "grad_norm": 16.675817489624023,
      "learning_rate": 9.197745945713427e-06,
      "loss": 1.2924,
      "step": 4448
    },
    {
      "epoch": 1.7224157955865271,
      "grad_norm": 7.746568202972412,
      "learning_rate": 9.197315782681637e-06,
      "loss": 0.4053,
      "step": 4449
    },
    {
      "epoch": 1.7228029423151374,
      "grad_norm": 18.114225387573242,
      "learning_rate": 9.196885619649848e-06,
      "loss": 1.0872,
      "step": 4450
    },
    {
      "epoch": 1.7231900890437477,
      "grad_norm": 24.844017028808594,
      "learning_rate": 9.196455456618058e-06,
      "loss": 0.5864,
      "step": 4451
    },
    {
      "epoch": 1.7235772357723578,
      "grad_norm": 10.88758373260498,
      "learning_rate": 9.196025293586271e-06,
      "loss": 0.6769,
      "step": 4452
    },
    {
      "epoch": 1.7239643825009678,
      "grad_norm": 15.334258079528809,
      "learning_rate": 9.195595130554481e-06,
      "loss": 0.6711,
      "step": 4453
    },
    {
      "epoch": 1.724351529229578,
      "grad_norm": 13.855875015258789,
      "learning_rate": 9.195164967522692e-06,
      "loss": 0.7326,
      "step": 4454
    },
    {
      "epoch": 1.7247386759581882,
      "grad_norm": 17.104171752929688,
      "learning_rate": 9.194734804490902e-06,
      "loss": 1.6569,
      "step": 4455
    },
    {
      "epoch": 1.7251258226867983,
      "grad_norm": 12.459426879882812,
      "learning_rate": 9.194304641459113e-06,
      "loss": 0.8141,
      "step": 4456
    },
    {
      "epoch": 1.7255129694154085,
      "grad_norm": 5.991300582885742,
      "learning_rate": 9.193874478427325e-06,
      "loss": 0.3507,
      "step": 4457
    },
    {
      "epoch": 1.7259001161440186,
      "grad_norm": 20.055688858032227,
      "learning_rate": 9.193444315395536e-06,
      "loss": 1.7215,
      "step": 4458
    },
    {
      "epoch": 1.7262872628726287,
      "grad_norm": 16.294553756713867,
      "learning_rate": 9.193014152363746e-06,
      "loss": 0.9782,
      "step": 4459
    },
    {
      "epoch": 1.7266744096012387,
      "grad_norm": 35.79800796508789,
      "learning_rate": 9.192583989331957e-06,
      "loss": 1.8681,
      "step": 4460
    },
    {
      "epoch": 1.727061556329849,
      "grad_norm": 24.18794059753418,
      "learning_rate": 9.192153826300169e-06,
      "loss": 1.8541,
      "step": 4461
    },
    {
      "epoch": 1.727448703058459,
      "grad_norm": 16.499357223510742,
      "learning_rate": 9.191723663268379e-06,
      "loss": 0.9207,
      "step": 4462
    },
    {
      "epoch": 1.7278358497870694,
      "grad_norm": 19.669864654541016,
      "learning_rate": 9.19129350023659e-06,
      "loss": 1.9462,
      "step": 4463
    },
    {
      "epoch": 1.7282229965156795,
      "grad_norm": 24.523653030395508,
      "learning_rate": 9.190863337204801e-06,
      "loss": 1.5549,
      "step": 4464
    },
    {
      "epoch": 1.7286101432442895,
      "grad_norm": 17.59524917602539,
      "learning_rate": 9.190433174173013e-06,
      "loss": 1.5567,
      "step": 4465
    },
    {
      "epoch": 1.7289972899728996,
      "grad_norm": 14.219677925109863,
      "learning_rate": 9.190003011141223e-06,
      "loss": 1.4983,
      "step": 4466
    },
    {
      "epoch": 1.7293844367015099,
      "grad_norm": 13.626916885375977,
      "learning_rate": 9.189572848109434e-06,
      "loss": 0.7972,
      "step": 4467
    },
    {
      "epoch": 1.7297715834301202,
      "grad_norm": 21.099573135375977,
      "learning_rate": 9.189142685077645e-06,
      "loss": 2.0181,
      "step": 4468
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 13.053382873535156,
      "learning_rate": 9.188712522045857e-06,
      "loss": 0.897,
      "step": 4469
    },
    {
      "epoch": 1.7305458768873403,
      "grad_norm": 19.516963958740234,
      "learning_rate": 9.188282359014067e-06,
      "loss": 1.812,
      "step": 4470
    },
    {
      "epoch": 1.7309330236159504,
      "grad_norm": 22.92887306213379,
      "learning_rate": 9.187852195982278e-06,
      "loss": 1.5623,
      "step": 4471
    },
    {
      "epoch": 1.7313201703445604,
      "grad_norm": 36.629295349121094,
      "learning_rate": 9.18742203295049e-06,
      "loss": 2.3212,
      "step": 4472
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 12.380600929260254,
      "learning_rate": 9.1869918699187e-06,
      "loss": 0.7925,
      "step": 4473
    },
    {
      "epoch": 1.732094463801781,
      "grad_norm": 19.85953712463379,
      "learning_rate": 9.18656170688691e-06,
      "loss": 1.2847,
      "step": 4474
    },
    {
      "epoch": 1.732481610530391,
      "grad_norm": 19.276193618774414,
      "learning_rate": 9.186131543855122e-06,
      "loss": 1.2809,
      "step": 4475
    },
    {
      "epoch": 1.7328687572590011,
      "grad_norm": 14.649266242980957,
      "learning_rate": 9.185701380823333e-06,
      "loss": 1.0727,
      "step": 4476
    },
    {
      "epoch": 1.7332559039876112,
      "grad_norm": 18.073593139648438,
      "learning_rate": 9.185271217791543e-06,
      "loss": 1.9435,
      "step": 4477
    },
    {
      "epoch": 1.7336430507162215,
      "grad_norm": 28.675411224365234,
      "learning_rate": 9.184841054759754e-06,
      "loss": 2.5937,
      "step": 4478
    },
    {
      "epoch": 1.7340301974448316,
      "grad_norm": 14.290305137634277,
      "learning_rate": 9.184410891727966e-06,
      "loss": 1.0701,
      "step": 4479
    },
    {
      "epoch": 1.7344173441734418,
      "grad_norm": 10.853109359741211,
      "learning_rate": 9.183980728696177e-06,
      "loss": 1.1927,
      "step": 4480
    },
    {
      "epoch": 1.734804490902052,
      "grad_norm": 38.29320526123047,
      "learning_rate": 9.183550565664387e-06,
      "loss": 2.1443,
      "step": 4481
    },
    {
      "epoch": 1.735191637630662,
      "grad_norm": 14.139076232910156,
      "learning_rate": 9.183120402632598e-06,
      "loss": 1.3476,
      "step": 4482
    },
    {
      "epoch": 1.735578784359272,
      "grad_norm": 15.987998962402344,
      "learning_rate": 9.18269023960081e-06,
      "loss": 1.5097,
      "step": 4483
    },
    {
      "epoch": 1.7359659310878823,
      "grad_norm": 26.095218658447266,
      "learning_rate": 9.182260076569021e-06,
      "loss": 1.5168,
      "step": 4484
    },
    {
      "epoch": 1.7363530778164924,
      "grad_norm": 16.624298095703125,
      "learning_rate": 9.181829913537231e-06,
      "loss": 1.5727,
      "step": 4485
    },
    {
      "epoch": 1.7367402245451027,
      "grad_norm": 10.554410934448242,
      "learning_rate": 9.181399750505442e-06,
      "loss": 0.7677,
      "step": 4486
    },
    {
      "epoch": 1.7371273712737128,
      "grad_norm": 23.77164077758789,
      "learning_rate": 9.180969587473654e-06,
      "loss": 1.1236,
      "step": 4487
    },
    {
      "epoch": 1.7375145180023228,
      "grad_norm": 26.081701278686523,
      "learning_rate": 9.180539424441865e-06,
      "loss": 2.4391,
      "step": 4488
    },
    {
      "epoch": 1.7379016647309329,
      "grad_norm": 22.718042373657227,
      "learning_rate": 9.180109261410075e-06,
      "loss": 1.2705,
      "step": 4489
    },
    {
      "epoch": 1.7382888114595432,
      "grad_norm": 17.286602020263672,
      "learning_rate": 9.179679098378286e-06,
      "loss": 1.6031,
      "step": 4490
    },
    {
      "epoch": 1.7386759581881535,
      "grad_norm": 24.36741828918457,
      "learning_rate": 9.179248935346498e-06,
      "loss": 1.9885,
      "step": 4491
    },
    {
      "epoch": 1.7390631049167635,
      "grad_norm": 16.65433692932129,
      "learning_rate": 9.178818772314707e-06,
      "loss": 1.3173,
      "step": 4492
    },
    {
      "epoch": 1.7394502516453736,
      "grad_norm": 27.231891632080078,
      "learning_rate": 9.178388609282919e-06,
      "loss": 1.1933,
      "step": 4493
    },
    {
      "epoch": 1.7398373983739837,
      "grad_norm": 14.514915466308594,
      "learning_rate": 9.17795844625113e-06,
      "loss": 1.5053,
      "step": 4494
    },
    {
      "epoch": 1.7402245451025937,
      "grad_norm": 16.62972068786621,
      "learning_rate": 9.177528283219342e-06,
      "loss": 1.6251,
      "step": 4495
    },
    {
      "epoch": 1.740611691831204,
      "grad_norm": 23.241897583007812,
      "learning_rate": 9.177098120187551e-06,
      "loss": 1.8409,
      "step": 4496
    },
    {
      "epoch": 1.7409988385598143,
      "grad_norm": 16.291322708129883,
      "learning_rate": 9.176667957155763e-06,
      "loss": 1.5371,
      "step": 4497
    },
    {
      "epoch": 1.7413859852884244,
      "grad_norm": 18.720542907714844,
      "learning_rate": 9.176237794123973e-06,
      "loss": 1.7311,
      "step": 4498
    },
    {
      "epoch": 1.7417731320170344,
      "grad_norm": 26.483949661254883,
      "learning_rate": 9.175807631092186e-06,
      "loss": 1.8123,
      "step": 4499
    },
    {
      "epoch": 1.7421602787456445,
      "grad_norm": 18.75701904296875,
      "learning_rate": 9.175377468060395e-06,
      "loss": 1.5508,
      "step": 4500
    },
    {
      "epoch": 1.7425474254742548,
      "grad_norm": 18.85723304748535,
      "learning_rate": 9.174947305028607e-06,
      "loss": 1.4105,
      "step": 4501
    },
    {
      "epoch": 1.7429345722028649,
      "grad_norm": 24.17290687561035,
      "learning_rate": 9.174517141996817e-06,
      "loss": 1.1334,
      "step": 4502
    },
    {
      "epoch": 1.7433217189314751,
      "grad_norm": 15.149174690246582,
      "learning_rate": 9.17408697896503e-06,
      "loss": 1.5256,
      "step": 4503
    },
    {
      "epoch": 1.7437088656600852,
      "grad_norm": 16.89396095275879,
      "learning_rate": 9.17365681593324e-06,
      "loss": 1.597,
      "step": 4504
    },
    {
      "epoch": 1.7440960123886953,
      "grad_norm": 23.736783981323242,
      "learning_rate": 9.17322665290145e-06,
      "loss": 1.627,
      "step": 4505
    },
    {
      "epoch": 1.7444831591173053,
      "grad_norm": 20.196788787841797,
      "learning_rate": 9.17279648986966e-06,
      "loss": 1.1293,
      "step": 4506
    },
    {
      "epoch": 1.7448703058459156,
      "grad_norm": 21.0212345123291,
      "learning_rate": 9.172366326837872e-06,
      "loss": 1.736,
      "step": 4507
    },
    {
      "epoch": 1.7452574525745257,
      "grad_norm": 20.108478546142578,
      "learning_rate": 9.171936163806083e-06,
      "loss": 1.2976,
      "step": 4508
    },
    {
      "epoch": 1.745644599303136,
      "grad_norm": 18.758146286010742,
      "learning_rate": 9.171506000774295e-06,
      "loss": 1.8374,
      "step": 4509
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 19.61980628967285,
      "learning_rate": 9.171075837742504e-06,
      "loss": 1.7882,
      "step": 4510
    },
    {
      "epoch": 1.7464188927603561,
      "grad_norm": 17.120094299316406,
      "learning_rate": 9.170645674710716e-06,
      "loss": 1.5471,
      "step": 4511
    },
    {
      "epoch": 1.7468060394889662,
      "grad_norm": 9.545855522155762,
      "learning_rate": 9.170215511678927e-06,
      "loss": 1.2965,
      "step": 4512
    },
    {
      "epoch": 1.7471931862175765,
      "grad_norm": 19.835731506347656,
      "learning_rate": 9.169785348647137e-06,
      "loss": 1.5122,
      "step": 4513
    },
    {
      "epoch": 1.7475803329461868,
      "grad_norm": 23.32377052307129,
      "learning_rate": 9.169355185615348e-06,
      "loss": 1.5253,
      "step": 4514
    },
    {
      "epoch": 1.7479674796747968,
      "grad_norm": 23.381855010986328,
      "learning_rate": 9.16892502258356e-06,
      "loss": 1.4891,
      "step": 4515
    },
    {
      "epoch": 1.7483546264034069,
      "grad_norm": 23.537670135498047,
      "learning_rate": 9.168494859551771e-06,
      "loss": 1.1614,
      "step": 4516
    },
    {
      "epoch": 1.748741773132017,
      "grad_norm": 19.368249893188477,
      "learning_rate": 9.168064696519981e-06,
      "loss": 1.1947,
      "step": 4517
    },
    {
      "epoch": 1.749128919860627,
      "grad_norm": 27.85027313232422,
      "learning_rate": 9.167634533488194e-06,
      "loss": 2.1036,
      "step": 4518
    },
    {
      "epoch": 1.7495160665892373,
      "grad_norm": 21.802690505981445,
      "learning_rate": 9.167204370456404e-06,
      "loss": 1.5972,
      "step": 4519
    },
    {
      "epoch": 1.7499032133178476,
      "grad_norm": 14.391538619995117,
      "learning_rate": 9.166774207424615e-06,
      "loss": 1.465,
      "step": 4520
    },
    {
      "epoch": 1.7502903600464577,
      "grad_norm": 29.930076599121094,
      "learning_rate": 9.166344044392825e-06,
      "loss": 1.6491,
      "step": 4521
    },
    {
      "epoch": 1.7506775067750677,
      "grad_norm": 24.224016189575195,
      "learning_rate": 9.165913881361036e-06,
      "loss": 1.2869,
      "step": 4522
    },
    {
      "epoch": 1.7510646535036778,
      "grad_norm": 16.734277725219727,
      "learning_rate": 9.165483718329248e-06,
      "loss": 1.0763,
      "step": 4523
    },
    {
      "epoch": 1.751451800232288,
      "grad_norm": 15.640958786010742,
      "learning_rate": 9.16505355529746e-06,
      "loss": 1.2961,
      "step": 4524
    },
    {
      "epoch": 1.7518389469608981,
      "grad_norm": 16.36305046081543,
      "learning_rate": 9.164623392265669e-06,
      "loss": 1.4876,
      "step": 4525
    },
    {
      "epoch": 1.7522260936895084,
      "grad_norm": 15.450028419494629,
      "learning_rate": 9.16419322923388e-06,
      "loss": 1.5788,
      "step": 4526
    },
    {
      "epoch": 1.7526132404181185,
      "grad_norm": 24.73768424987793,
      "learning_rate": 9.163763066202092e-06,
      "loss": 1.526,
      "step": 4527
    },
    {
      "epoch": 1.7530003871467286,
      "grad_norm": 19.16252326965332,
      "learning_rate": 9.163332903170302e-06,
      "loss": 2.082,
      "step": 4528
    },
    {
      "epoch": 1.7533875338753386,
      "grad_norm": 27.403255462646484,
      "learning_rate": 9.162902740138513e-06,
      "loss": 1.1262,
      "step": 4529
    },
    {
      "epoch": 1.753774680603949,
      "grad_norm": 14.941604614257812,
      "learning_rate": 9.162472577106724e-06,
      "loss": 0.3092,
      "step": 4530
    },
    {
      "epoch": 1.754161827332559,
      "grad_norm": 10.688907623291016,
      "learning_rate": 9.162042414074936e-06,
      "loss": 0.4922,
      "step": 4531
    },
    {
      "epoch": 1.7545489740611693,
      "grad_norm": 14.584424018859863,
      "learning_rate": 9.161612251043145e-06,
      "loss": 1.4353,
      "step": 4532
    },
    {
      "epoch": 1.7549361207897793,
      "grad_norm": 14.548165321350098,
      "learning_rate": 9.161182088011357e-06,
      "loss": 1.404,
      "step": 4533
    },
    {
      "epoch": 1.7553232675183894,
      "grad_norm": 14.812557220458984,
      "learning_rate": 9.160751924979568e-06,
      "loss": 1.4873,
      "step": 4534
    },
    {
      "epoch": 1.7557104142469995,
      "grad_norm": 26.2131404876709,
      "learning_rate": 9.16032176194778e-06,
      "loss": 3.0054,
      "step": 4535
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 12.603919982910156,
      "learning_rate": 9.15989159891599e-06,
      "loss": 0.851,
      "step": 4536
    },
    {
      "epoch": 1.75648470770422,
      "grad_norm": 21.5288028717041,
      "learning_rate": 9.159461435884201e-06,
      "loss": 1.4929,
      "step": 4537
    },
    {
      "epoch": 1.7568718544328301,
      "grad_norm": 12.659451484680176,
      "learning_rate": 9.159031272852412e-06,
      "loss": 0.7728,
      "step": 4538
    },
    {
      "epoch": 1.7572590011614402,
      "grad_norm": 25.973934173583984,
      "learning_rate": 9.158601109820624e-06,
      "loss": 2.0874,
      "step": 4539
    },
    {
      "epoch": 1.7576461478900502,
      "grad_norm": 20.82561683654785,
      "learning_rate": 9.158170946788833e-06,
      "loss": 2.8401,
      "step": 4540
    },
    {
      "epoch": 1.7580332946186603,
      "grad_norm": 19.50092315673828,
      "learning_rate": 9.157740783757045e-06,
      "loss": 1.767,
      "step": 4541
    },
    {
      "epoch": 1.7584204413472706,
      "grad_norm": 33.2757682800293,
      "learning_rate": 9.157310620725256e-06,
      "loss": 1.3564,
      "step": 4542
    },
    {
      "epoch": 1.758807588075881,
      "grad_norm": 16.137157440185547,
      "learning_rate": 9.156880457693466e-06,
      "loss": 1.4755,
      "step": 4543
    },
    {
      "epoch": 1.759194734804491,
      "grad_norm": 7.719583034515381,
      "learning_rate": 9.156450294661677e-06,
      "loss": 0.2483,
      "step": 4544
    },
    {
      "epoch": 1.759581881533101,
      "grad_norm": 29.75459098815918,
      "learning_rate": 9.156020131629889e-06,
      "loss": 1.4113,
      "step": 4545
    },
    {
      "epoch": 1.759969028261711,
      "grad_norm": 22.617103576660156,
      "learning_rate": 9.1555899685981e-06,
      "loss": 1.4787,
      "step": 4546
    },
    {
      "epoch": 1.7603561749903214,
      "grad_norm": 14.117708206176758,
      "learning_rate": 9.15515980556631e-06,
      "loss": 0.8108,
      "step": 4547
    },
    {
      "epoch": 1.7607433217189314,
      "grad_norm": 24.438486099243164,
      "learning_rate": 9.154729642534521e-06,
      "loss": 0.8271,
      "step": 4548
    },
    {
      "epoch": 1.7611304684475417,
      "grad_norm": 28.06772232055664,
      "learning_rate": 9.154299479502731e-06,
      "loss": 1.6372,
      "step": 4549
    },
    {
      "epoch": 1.7615176151761518,
      "grad_norm": 14.844985961914062,
      "learning_rate": 9.153869316470944e-06,
      "loss": 0.9577,
      "step": 4550
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 22.551952362060547,
      "learning_rate": 9.153439153439154e-06,
      "loss": 2.1998,
      "step": 4551
    },
    {
      "epoch": 1.762291908633372,
      "grad_norm": 15.018440246582031,
      "learning_rate": 9.153008990407365e-06,
      "loss": 0.5866,
      "step": 4552
    },
    {
      "epoch": 1.7626790553619822,
      "grad_norm": 29.674833297729492,
      "learning_rate": 9.152578827375575e-06,
      "loss": 1.359,
      "step": 4553
    },
    {
      "epoch": 1.7630662020905923,
      "grad_norm": 22.86784553527832,
      "learning_rate": 9.152148664343788e-06,
      "loss": 1.8715,
      "step": 4554
    },
    {
      "epoch": 1.7634533488192026,
      "grad_norm": 14.819486618041992,
      "learning_rate": 9.151718501311998e-06,
      "loss": 0.8739,
      "step": 4555
    },
    {
      "epoch": 1.7638404955478126,
      "grad_norm": 17.8555850982666,
      "learning_rate": 9.15128833828021e-06,
      "loss": 1.6129,
      "step": 4556
    },
    {
      "epoch": 1.7642276422764227,
      "grad_norm": 25.12284278869629,
      "learning_rate": 9.150858175248419e-06,
      "loss": 0.8943,
      "step": 4557
    },
    {
      "epoch": 1.7646147890050328,
      "grad_norm": 24.32223892211914,
      "learning_rate": 9.15042801221663e-06,
      "loss": 2.0972,
      "step": 4558
    },
    {
      "epoch": 1.765001935733643,
      "grad_norm": 15.381014823913574,
      "learning_rate": 9.149997849184842e-06,
      "loss": 1.2772,
      "step": 4559
    },
    {
      "epoch": 1.7653890824622533,
      "grad_norm": 14.32927417755127,
      "learning_rate": 9.149567686153053e-06,
      "loss": 1.4038,
      "step": 4560
    },
    {
      "epoch": 1.7657762291908634,
      "grad_norm": 20.363313674926758,
      "learning_rate": 9.149137523121265e-06,
      "loss": 1.8724,
      "step": 4561
    },
    {
      "epoch": 1.7661633759194735,
      "grad_norm": 67.725830078125,
      "learning_rate": 9.148707360089474e-06,
      "loss": 2.3878,
      "step": 4562
    },
    {
      "epoch": 1.7665505226480835,
      "grad_norm": 12.874127388000488,
      "learning_rate": 9.148277197057686e-06,
      "loss": 1.011,
      "step": 4563
    },
    {
      "epoch": 1.7669376693766936,
      "grad_norm": 17.408344268798828,
      "learning_rate": 9.147847034025896e-06,
      "loss": 1.3006,
      "step": 4564
    },
    {
      "epoch": 1.767324816105304,
      "grad_norm": 19.56664276123047,
      "learning_rate": 9.147416870994109e-06,
      "loss": 1.95,
      "step": 4565
    },
    {
      "epoch": 1.7677119628339142,
      "grad_norm": 15.216754913330078,
      "learning_rate": 9.146986707962318e-06,
      "loss": 1.232,
      "step": 4566
    },
    {
      "epoch": 1.7680991095625243,
      "grad_norm": 22.809499740600586,
      "learning_rate": 9.14655654493053e-06,
      "loss": 1.0398,
      "step": 4567
    },
    {
      "epoch": 1.7684862562911343,
      "grad_norm": 27.263811111450195,
      "learning_rate": 9.14612638189874e-06,
      "loss": 1.2856,
      "step": 4568
    },
    {
      "epoch": 1.7688734030197444,
      "grad_norm": 19.445053100585938,
      "learning_rate": 9.145696218866953e-06,
      "loss": 1.6648,
      "step": 4569
    },
    {
      "epoch": 1.7692605497483547,
      "grad_norm": 20.91402816772461,
      "learning_rate": 9.145266055835162e-06,
      "loss": 1.3524,
      "step": 4570
    },
    {
      "epoch": 1.7696476964769647,
      "grad_norm": 14.350419998168945,
      "learning_rate": 9.144835892803374e-06,
      "loss": 1.6282,
      "step": 4571
    },
    {
      "epoch": 1.770034843205575,
      "grad_norm": 23.761516571044922,
      "learning_rate": 9.144405729771583e-06,
      "loss": 2.592,
      "step": 4572
    },
    {
      "epoch": 1.770421989934185,
      "grad_norm": 14.84110164642334,
      "learning_rate": 9.143975566739795e-06,
      "loss": 1.6213,
      "step": 4573
    },
    {
      "epoch": 1.7708091366627952,
      "grad_norm": 25.282779693603516,
      "learning_rate": 9.143545403708006e-06,
      "loss": 2.494,
      "step": 4574
    },
    {
      "epoch": 1.7711962833914052,
      "grad_norm": 23.86191177368164,
      "learning_rate": 9.143115240676218e-06,
      "loss": 1.4155,
      "step": 4575
    },
    {
      "epoch": 1.7715834301200155,
      "grad_norm": 10.375974655151367,
      "learning_rate": 9.142685077644427e-06,
      "loss": 1.3078,
      "step": 4576
    },
    {
      "epoch": 1.7719705768486256,
      "grad_norm": 11.747454643249512,
      "learning_rate": 9.142254914612639e-06,
      "loss": 1.0467,
      "step": 4577
    },
    {
      "epoch": 1.7723577235772359,
      "grad_norm": 20.231735229492188,
      "learning_rate": 9.14182475158085e-06,
      "loss": 1.8555,
      "step": 4578
    },
    {
      "epoch": 1.772744870305846,
      "grad_norm": 16.557392120361328,
      "learning_rate": 9.14139458854906e-06,
      "loss": 1.0535,
      "step": 4579
    },
    {
      "epoch": 1.773132017034456,
      "grad_norm": 20.999420166015625,
      "learning_rate": 9.140964425517271e-06,
      "loss": 0.9774,
      "step": 4580
    },
    {
      "epoch": 1.773519163763066,
      "grad_norm": 14.986236572265625,
      "learning_rate": 9.140534262485483e-06,
      "loss": 0.9069,
      "step": 4581
    },
    {
      "epoch": 1.7739063104916764,
      "grad_norm": 17.554094314575195,
      "learning_rate": 9.140104099453694e-06,
      "loss": 1.2965,
      "step": 4582
    },
    {
      "epoch": 1.7742934572202866,
      "grad_norm": 50.11005401611328,
      "learning_rate": 9.139673936421904e-06,
      "loss": 1.885,
      "step": 4583
    },
    {
      "epoch": 1.7746806039488967,
      "grad_norm": 16.936494827270508,
      "learning_rate": 9.139243773390115e-06,
      "loss": 1.18,
      "step": 4584
    },
    {
      "epoch": 1.7750677506775068,
      "grad_norm": 15.116061210632324,
      "learning_rate": 9.138813610358327e-06,
      "loss": 1.0944,
      "step": 4585
    },
    {
      "epoch": 1.7754548974061168,
      "grad_norm": 22.059614181518555,
      "learning_rate": 9.138383447326538e-06,
      "loss": 0.9324,
      "step": 4586
    },
    {
      "epoch": 1.775842044134727,
      "grad_norm": 24.247215270996094,
      "learning_rate": 9.137953284294748e-06,
      "loss": 1.4891,
      "step": 4587
    },
    {
      "epoch": 1.7762291908633372,
      "grad_norm": 26.66714859008789,
      "learning_rate": 9.13752312126296e-06,
      "loss": 1.9139,
      "step": 4588
    },
    {
      "epoch": 1.7766163375919475,
      "grad_norm": 26.74095916748047,
      "learning_rate": 9.13709295823117e-06,
      "loss": 1.5049,
      "step": 4589
    },
    {
      "epoch": 1.7770034843205575,
      "grad_norm": 17.864242553710938,
      "learning_rate": 9.136662795199382e-06,
      "loss": 1.2338,
      "step": 4590
    },
    {
      "epoch": 1.7773906310491676,
      "grad_norm": 17.266746520996094,
      "learning_rate": 9.136232632167592e-06,
      "loss": 1.0891,
      "step": 4591
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 24.731613159179688,
      "learning_rate": 9.135802469135803e-06,
      "loss": 1.4694,
      "step": 4592
    },
    {
      "epoch": 1.778164924506388,
      "grad_norm": 14.820184707641602,
      "learning_rate": 9.135372306104015e-06,
      "loss": 1.4947,
      "step": 4593
    },
    {
      "epoch": 1.778552071234998,
      "grad_norm": 17.972148895263672,
      "learning_rate": 9.134942143072224e-06,
      "loss": 2.9015,
      "step": 4594
    },
    {
      "epoch": 1.7789392179636083,
      "grad_norm": 25.1130313873291,
      "learning_rate": 9.134511980040436e-06,
      "loss": 1.8965,
      "step": 4595
    },
    {
      "epoch": 1.7793263646922184,
      "grad_norm": 17.45899200439453,
      "learning_rate": 9.134081817008647e-06,
      "loss": 1.1631,
      "step": 4596
    },
    {
      "epoch": 1.7797135114208285,
      "grad_norm": 13.505768775939941,
      "learning_rate": 9.133651653976859e-06,
      "loss": 2.2468,
      "step": 4597
    },
    {
      "epoch": 1.7801006581494385,
      "grad_norm": 21.386165618896484,
      "learning_rate": 9.133221490945068e-06,
      "loss": 1.9022,
      "step": 4598
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 13.605504989624023,
      "learning_rate": 9.13279132791328e-06,
      "loss": 0.9549,
      "step": 4599
    },
    {
      "epoch": 1.7808749516066589,
      "grad_norm": 10.877789497375488,
      "learning_rate": 9.132361164881491e-06,
      "loss": 0.4905,
      "step": 4600
    },
    {
      "epoch": 1.7812620983352692,
      "grad_norm": 16.274320602416992,
      "learning_rate": 9.131931001849703e-06,
      "loss": 1.4912,
      "step": 4601
    },
    {
      "epoch": 1.7816492450638792,
      "grad_norm": 27.972822189331055,
      "learning_rate": 9.131500838817912e-06,
      "loss": 1.5586,
      "step": 4602
    },
    {
      "epoch": 1.7820363917924893,
      "grad_norm": 18.038673400878906,
      "learning_rate": 9.131070675786124e-06,
      "loss": 1.5495,
      "step": 4603
    },
    {
      "epoch": 1.7824235385210994,
      "grad_norm": 23.25387191772461,
      "learning_rate": 9.130640512754335e-06,
      "loss": 1.5715,
      "step": 4604
    },
    {
      "epoch": 1.7828106852497096,
      "grad_norm": 14.387360572814941,
      "learning_rate": 9.130210349722547e-06,
      "loss": 0.9389,
      "step": 4605
    },
    {
      "epoch": 1.78319783197832,
      "grad_norm": 25.21636199951172,
      "learning_rate": 9.129780186690756e-06,
      "loss": 2.0225,
      "step": 4606
    },
    {
      "epoch": 1.78358497870693,
      "grad_norm": 33.97183609008789,
      "learning_rate": 9.129350023658968e-06,
      "loss": 1.9928,
      "step": 4607
    },
    {
      "epoch": 1.78397212543554,
      "grad_norm": 19.0562801361084,
      "learning_rate": 9.12891986062718e-06,
      "loss": 2.1351,
      "step": 4608
    },
    {
      "epoch": 1.7843592721641501,
      "grad_norm": 13.281758308410645,
      "learning_rate": 9.128489697595389e-06,
      "loss": 0.7078,
      "step": 4609
    },
    {
      "epoch": 1.7847464188927602,
      "grad_norm": 19.826133728027344,
      "learning_rate": 9.1280595345636e-06,
      "loss": 0.8356,
      "step": 4610
    },
    {
      "epoch": 1.7851335656213705,
      "grad_norm": 13.684333801269531,
      "learning_rate": 9.127629371531812e-06,
      "loss": 1.3464,
      "step": 4611
    },
    {
      "epoch": 1.7855207123499808,
      "grad_norm": 17.480772018432617,
      "learning_rate": 9.127199208500023e-06,
      "loss": 1.4344,
      "step": 4612
    },
    {
      "epoch": 1.7859078590785908,
      "grad_norm": 17.252161026000977,
      "learning_rate": 9.126769045468233e-06,
      "loss": 1.6929,
      "step": 4613
    },
    {
      "epoch": 1.786295005807201,
      "grad_norm": 13.966492652893066,
      "learning_rate": 9.126338882436444e-06,
      "loss": 1.1063,
      "step": 4614
    },
    {
      "epoch": 1.786682152535811,
      "grad_norm": 15.08607006072998,
      "learning_rate": 9.125908719404654e-06,
      "loss": 1.3971,
      "step": 4615
    },
    {
      "epoch": 1.7870692992644213,
      "grad_norm": 24.393068313598633,
      "learning_rate": 9.125478556372867e-06,
      "loss": 1.9061,
      "step": 4616
    },
    {
      "epoch": 1.7874564459930313,
      "grad_norm": 22.888050079345703,
      "learning_rate": 9.125048393341077e-06,
      "loss": 1.5517,
      "step": 4617
    },
    {
      "epoch": 1.7878435927216416,
      "grad_norm": 12.790576934814453,
      "learning_rate": 9.124618230309288e-06,
      "loss": 1.2876,
      "step": 4618
    },
    {
      "epoch": 1.7882307394502517,
      "grad_norm": 14.523802757263184,
      "learning_rate": 9.124188067277498e-06,
      "loss": 0.9253,
      "step": 4619
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 15.605945587158203,
      "learning_rate": 9.123757904245711e-06,
      "loss": 1.1902,
      "step": 4620
    },
    {
      "epoch": 1.7890050329074718,
      "grad_norm": 18.38454246520996,
      "learning_rate": 9.12332774121392e-06,
      "loss": 1.7644,
      "step": 4621
    },
    {
      "epoch": 1.789392179636082,
      "grad_norm": 12.19017219543457,
      "learning_rate": 9.122897578182132e-06,
      "loss": 1.492,
      "step": 4622
    },
    {
      "epoch": 1.7897793263646922,
      "grad_norm": 20.162109375,
      "learning_rate": 9.122467415150342e-06,
      "loss": 1.2208,
      "step": 4623
    },
    {
      "epoch": 1.7901664730933025,
      "grad_norm": 26.259050369262695,
      "learning_rate": 9.122037252118553e-06,
      "loss": 1.9364,
      "step": 4624
    },
    {
      "epoch": 1.7905536198219125,
      "grad_norm": 22.133100509643555,
      "learning_rate": 9.121607089086765e-06,
      "loss": 1.0572,
      "step": 4625
    },
    {
      "epoch": 1.7909407665505226,
      "grad_norm": 25.303892135620117,
      "learning_rate": 9.121176926054976e-06,
      "loss": 1.266,
      "step": 4626
    },
    {
      "epoch": 1.7913279132791327,
      "grad_norm": 15.066391944885254,
      "learning_rate": 9.120746763023186e-06,
      "loss": 1.415,
      "step": 4627
    },
    {
      "epoch": 1.791715060007743,
      "grad_norm": 32.96600341796875,
      "learning_rate": 9.120316599991397e-06,
      "loss": 2.675,
      "step": 4628
    },
    {
      "epoch": 1.7921022067363532,
      "grad_norm": 26.473388671875,
      "learning_rate": 9.119886436959609e-06,
      "loss": 1.9169,
      "step": 4629
    },
    {
      "epoch": 1.7924893534649633,
      "grad_norm": 16.76715660095215,
      "learning_rate": 9.119456273927818e-06,
      "loss": 1.0854,
      "step": 4630
    },
    {
      "epoch": 1.7928765001935734,
      "grad_norm": 20.812271118164062,
      "learning_rate": 9.11902611089603e-06,
      "loss": 0.7364,
      "step": 4631
    },
    {
      "epoch": 1.7932636469221834,
      "grad_norm": 19.766273498535156,
      "learning_rate": 9.118595947864241e-06,
      "loss": 1.2462,
      "step": 4632
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 13.518383979797363,
      "learning_rate": 9.118165784832453e-06,
      "loss": 0.575,
      "step": 4633
    },
    {
      "epoch": 1.7940379403794038,
      "grad_norm": 13.823429107666016,
      "learning_rate": 9.117735621800662e-06,
      "loss": 0.9903,
      "step": 4634
    },
    {
      "epoch": 1.794425087108014,
      "grad_norm": 19.44105339050293,
      "learning_rate": 9.117305458768874e-06,
      "loss": 1.4057,
      "step": 4635
    },
    {
      "epoch": 1.7948122338366241,
      "grad_norm": 33.58798599243164,
      "learning_rate": 9.116875295737085e-06,
      "loss": 1.8894,
      "step": 4636
    },
    {
      "epoch": 1.7951993805652342,
      "grad_norm": 18.675382614135742,
      "learning_rate": 9.116445132705297e-06,
      "loss": 1.0055,
      "step": 4637
    },
    {
      "epoch": 1.7955865272938443,
      "grad_norm": 16.190196990966797,
      "learning_rate": 9.116014969673506e-06,
      "loss": 1.2409,
      "step": 4638
    },
    {
      "epoch": 1.7959736740224546,
      "grad_norm": 17.17102813720703,
      "learning_rate": 9.115584806641718e-06,
      "loss": 1.2354,
      "step": 4639
    },
    {
      "epoch": 1.7963608207510646,
      "grad_norm": 23.497678756713867,
      "learning_rate": 9.11515464360993e-06,
      "loss": 1.3753,
      "step": 4640
    },
    {
      "epoch": 1.796747967479675,
      "grad_norm": 19.736345291137695,
      "learning_rate": 9.11472448057814e-06,
      "loss": 1.6109,
      "step": 4641
    },
    {
      "epoch": 1.797135114208285,
      "grad_norm": 10.819583892822266,
      "learning_rate": 9.11429431754635e-06,
      "loss": 0.6906,
      "step": 4642
    },
    {
      "epoch": 1.797522260936895,
      "grad_norm": 26.23871421813965,
      "learning_rate": 9.113864154514562e-06,
      "loss": 1.5394,
      "step": 4643
    },
    {
      "epoch": 1.797909407665505,
      "grad_norm": 24.167470932006836,
      "learning_rate": 9.113433991482773e-06,
      "loss": 2.5027,
      "step": 4644
    },
    {
      "epoch": 1.7982965543941154,
      "grad_norm": 30.81083106994629,
      "learning_rate": 9.113003828450983e-06,
      "loss": 3.0649,
      "step": 4645
    },
    {
      "epoch": 1.7986837011227255,
      "grad_norm": 16.394790649414062,
      "learning_rate": 9.112573665419194e-06,
      "loss": 1.0691,
      "step": 4646
    },
    {
      "epoch": 1.7990708478513358,
      "grad_norm": 23.087684631347656,
      "learning_rate": 9.112143502387406e-06,
      "loss": 1.6729,
      "step": 4647
    },
    {
      "epoch": 1.7994579945799458,
      "grad_norm": 23.86063575744629,
      "learning_rate": 9.111713339355617e-06,
      "loss": 1.4825,
      "step": 4648
    },
    {
      "epoch": 1.7998451413085559,
      "grad_norm": 24.01270294189453,
      "learning_rate": 9.111283176323827e-06,
      "loss": 1.2829,
      "step": 4649
    },
    {
      "epoch": 1.800232288037166,
      "grad_norm": 46.29359817504883,
      "learning_rate": 9.110853013292038e-06,
      "loss": 2.5051,
      "step": 4650
    },
    {
      "epoch": 1.8006194347657762,
      "grad_norm": 7.216480731964111,
      "learning_rate": 9.11042285026025e-06,
      "loss": 0.2282,
      "step": 4651
    },
    {
      "epoch": 1.8010065814943865,
      "grad_norm": 14.147415161132812,
      "learning_rate": 9.109992687228461e-06,
      "loss": 0.883,
      "step": 4652
    },
    {
      "epoch": 1.8013937282229966,
      "grad_norm": 14.966934204101562,
      "learning_rate": 9.109562524196671e-06,
      "loss": 0.8051,
      "step": 4653
    },
    {
      "epoch": 1.8017808749516067,
      "grad_norm": 21.678268432617188,
      "learning_rate": 9.109132361164882e-06,
      "loss": 1.6858,
      "step": 4654
    },
    {
      "epoch": 1.8021680216802167,
      "grad_norm": 14.016328811645508,
      "learning_rate": 9.108702198133094e-06,
      "loss": 1.3387,
      "step": 4655
    },
    {
      "epoch": 1.8025551684088268,
      "grad_norm": 16.02761459350586,
      "learning_rate": 9.108272035101305e-06,
      "loss": 1.3923,
      "step": 4656
    },
    {
      "epoch": 1.802942315137437,
      "grad_norm": 28.98533058166504,
      "learning_rate": 9.107841872069515e-06,
      "loss": 1.9284,
      "step": 4657
    },
    {
      "epoch": 1.8033294618660474,
      "grad_norm": 18.691036224365234,
      "learning_rate": 9.107411709037726e-06,
      "loss": 1.5941,
      "step": 4658
    },
    {
      "epoch": 1.8037166085946574,
      "grad_norm": 22.56295394897461,
      "learning_rate": 9.106981546005938e-06,
      "loss": 1.7079,
      "step": 4659
    },
    {
      "epoch": 1.8041037553232675,
      "grad_norm": 13.460077285766602,
      "learning_rate": 9.106551382974147e-06,
      "loss": 1.0661,
      "step": 4660
    },
    {
      "epoch": 1.8044909020518776,
      "grad_norm": 13.567485809326172,
      "learning_rate": 9.106121219942359e-06,
      "loss": 0.8054,
      "step": 4661
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 13.677257537841797,
      "learning_rate": 9.10569105691057e-06,
      "loss": 0.8951,
      "step": 4662
    },
    {
      "epoch": 1.805265195509098,
      "grad_norm": 14.394182205200195,
      "learning_rate": 9.105260893878782e-06,
      "loss": 0.987,
      "step": 4663
    },
    {
      "epoch": 1.8056523422377082,
      "grad_norm": 30.73436737060547,
      "learning_rate": 9.104830730846991e-06,
      "loss": 1.0659,
      "step": 4664
    },
    {
      "epoch": 1.8060394889663183,
      "grad_norm": 25.233705520629883,
      "learning_rate": 9.104400567815203e-06,
      "loss": 1.4349,
      "step": 4665
    },
    {
      "epoch": 1.8064266356949283,
      "grad_norm": 34.19230270385742,
      "learning_rate": 9.103970404783413e-06,
      "loss": 2.4309,
      "step": 4666
    },
    {
      "epoch": 1.8068137824235384,
      "grad_norm": 28.033540725708008,
      "learning_rate": 9.103540241751626e-06,
      "loss": 1.1249,
      "step": 4667
    },
    {
      "epoch": 1.8072009291521487,
      "grad_norm": 17.124540328979492,
      "learning_rate": 9.103110078719835e-06,
      "loss": 1.2176,
      "step": 4668
    },
    {
      "epoch": 1.8075880758807588,
      "grad_norm": 20.082792282104492,
      "learning_rate": 9.102679915688047e-06,
      "loss": 0.8024,
      "step": 4669
    },
    {
      "epoch": 1.807975222609369,
      "grad_norm": 16.428234100341797,
      "learning_rate": 9.102249752656256e-06,
      "loss": 1.0127,
      "step": 4670
    },
    {
      "epoch": 1.8083623693379791,
      "grad_norm": 14.07869815826416,
      "learning_rate": 9.10181958962447e-06,
      "loss": 0.977,
      "step": 4671
    },
    {
      "epoch": 1.8087495160665892,
      "grad_norm": 18.936771392822266,
      "learning_rate": 9.10138942659268e-06,
      "loss": 1.6585,
      "step": 4672
    },
    {
      "epoch": 1.8091366627951992,
      "grad_norm": 16.570817947387695,
      "learning_rate": 9.10095926356089e-06,
      "loss": 1.4638,
      "step": 4673
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 14.783575057983398,
      "learning_rate": 9.1005291005291e-06,
      "loss": 1.6066,
      "step": 4674
    },
    {
      "epoch": 1.8099109562524198,
      "grad_norm": 19.027999877929688,
      "learning_rate": 9.100098937497312e-06,
      "loss": 0.741,
      "step": 4675
    },
    {
      "epoch": 1.8102981029810299,
      "grad_norm": 13.174925804138184,
      "learning_rate": 9.099668774465523e-06,
      "loss": 0.7509,
      "step": 4676
    },
    {
      "epoch": 1.81068524970964,
      "grad_norm": 20.378664016723633,
      "learning_rate": 9.099238611433735e-06,
      "loss": 1.8055,
      "step": 4677
    },
    {
      "epoch": 1.81107239643825,
      "grad_norm": 18.584985733032227,
      "learning_rate": 9.098808448401944e-06,
      "loss": 1.238,
      "step": 4678
    },
    {
      "epoch": 1.81145954316686,
      "grad_norm": 17.527578353881836,
      "learning_rate": 9.098378285370156e-06,
      "loss": 1.5626,
      "step": 4679
    },
    {
      "epoch": 1.8118466898954704,
      "grad_norm": 24.397504806518555,
      "learning_rate": 9.097948122338367e-06,
      "loss": 2.124,
      "step": 4680
    },
    {
      "epoch": 1.8122338366240807,
      "grad_norm": 20.804956436157227,
      "learning_rate": 9.097517959306577e-06,
      "loss": 1.5479,
      "step": 4681
    },
    {
      "epoch": 1.8126209833526907,
      "grad_norm": 15.115038871765137,
      "learning_rate": 9.09708779627479e-06,
      "loss": 1.4153,
      "step": 4682
    },
    {
      "epoch": 1.8130081300813008,
      "grad_norm": 21.61750602722168,
      "learning_rate": 9.096657633243e-06,
      "loss": 1.7184,
      "step": 4683
    },
    {
      "epoch": 1.8133952768099109,
      "grad_norm": 21.904491424560547,
      "learning_rate": 9.096227470211211e-06,
      "loss": 1.6684,
      "step": 4684
    },
    {
      "epoch": 1.8137824235385211,
      "grad_norm": 28.801706314086914,
      "learning_rate": 9.095797307179421e-06,
      "loss": 2.2498,
      "step": 4685
    },
    {
      "epoch": 1.8141695702671312,
      "grad_norm": 25.440006256103516,
      "learning_rate": 9.095367144147634e-06,
      "loss": 1.305,
      "step": 4686
    },
    {
      "epoch": 1.8145567169957415,
      "grad_norm": 8.483223915100098,
      "learning_rate": 9.094936981115844e-06,
      "loss": 0.3485,
      "step": 4687
    },
    {
      "epoch": 1.8149438637243516,
      "grad_norm": 39.17142105102539,
      "learning_rate": 9.094506818084055e-06,
      "loss": 2.3069,
      "step": 4688
    },
    {
      "epoch": 1.8153310104529616,
      "grad_norm": 20.911758422851562,
      "learning_rate": 9.094076655052265e-06,
      "loss": 1.7043,
      "step": 4689
    },
    {
      "epoch": 1.8157181571815717,
      "grad_norm": 19.37892723083496,
      "learning_rate": 9.093646492020476e-06,
      "loss": 1.6873,
      "step": 4690
    },
    {
      "epoch": 1.816105303910182,
      "grad_norm": 26.744487762451172,
      "learning_rate": 9.093216328988688e-06,
      "loss": 3.8207,
      "step": 4691
    },
    {
      "epoch": 1.816492450638792,
      "grad_norm": 19.85626220703125,
      "learning_rate": 9.0927861659569e-06,
      "loss": 2.3377,
      "step": 4692
    },
    {
      "epoch": 1.8168795973674023,
      "grad_norm": 28.156740188598633,
      "learning_rate": 9.092356002925109e-06,
      "loss": 2.5679,
      "step": 4693
    },
    {
      "epoch": 1.8172667440960124,
      "grad_norm": 18.01519203186035,
      "learning_rate": 9.09192583989332e-06,
      "loss": 1.5446,
      "step": 4694
    },
    {
      "epoch": 1.8176538908246225,
      "grad_norm": 12.032381057739258,
      "learning_rate": 9.091495676861532e-06,
      "loss": 0.5468,
      "step": 4695
    },
    {
      "epoch": 1.8180410375532325,
      "grad_norm": 18.528240203857422,
      "learning_rate": 9.091065513829741e-06,
      "loss": 1.3596,
      "step": 4696
    },
    {
      "epoch": 1.8184281842818428,
      "grad_norm": 15.683764457702637,
      "learning_rate": 9.090635350797953e-06,
      "loss": 1.0041,
      "step": 4697
    },
    {
      "epoch": 1.8188153310104531,
      "grad_norm": 32.717288970947266,
      "learning_rate": 9.090205187766164e-06,
      "loss": 1.1922,
      "step": 4698
    },
    {
      "epoch": 1.8192024777390632,
      "grad_norm": 14.342623710632324,
      "learning_rate": 9.089775024734376e-06,
      "loss": 1.4727,
      "step": 4699
    },
    {
      "epoch": 1.8195896244676733,
      "grad_norm": 20.59796905517578,
      "learning_rate": 9.089344861702585e-06,
      "loss": 0.9084,
      "step": 4700
    },
    {
      "epoch": 1.8199767711962833,
      "grad_norm": 25.421886444091797,
      "learning_rate": 9.088914698670797e-06,
      "loss": 1.5512,
      "step": 4701
    },
    {
      "epoch": 1.8203639179248934,
      "grad_norm": 30.12930679321289,
      "learning_rate": 9.088484535639008e-06,
      "loss": 2.489,
      "step": 4702
    },
    {
      "epoch": 1.8207510646535037,
      "grad_norm": 13.32548999786377,
      "learning_rate": 9.08805437260722e-06,
      "loss": 0.8011,
      "step": 4703
    },
    {
      "epoch": 1.821138211382114,
      "grad_norm": 40.96614074707031,
      "learning_rate": 9.08762420957543e-06,
      "loss": 1.9293,
      "step": 4704
    },
    {
      "epoch": 1.821525358110724,
      "grad_norm": 17.10525131225586,
      "learning_rate": 9.08719404654364e-06,
      "loss": 1.622,
      "step": 4705
    },
    {
      "epoch": 1.821912504839334,
      "grad_norm": 13.107076644897461,
      "learning_rate": 9.086763883511852e-06,
      "loss": 0.8672,
      "step": 4706
    },
    {
      "epoch": 1.8222996515679442,
      "grad_norm": 18.5621280670166,
      "learning_rate": 9.086333720480064e-06,
      "loss": 1.1973,
      "step": 4707
    },
    {
      "epoch": 1.8226867982965544,
      "grad_norm": 16.534631729125977,
      "learning_rate": 9.085903557448273e-06,
      "loss": 0.9444,
      "step": 4708
    },
    {
      "epoch": 1.8230739450251645,
      "grad_norm": 16.50160789489746,
      "learning_rate": 9.085473394416485e-06,
      "loss": 0.6599,
      "step": 4709
    },
    {
      "epoch": 1.8234610917537748,
      "grad_norm": 15.50728702545166,
      "learning_rate": 9.085043231384696e-06,
      "loss": 0.7606,
      "step": 4710
    },
    {
      "epoch": 1.8238482384823849,
      "grad_norm": 21.26910972595215,
      "learning_rate": 9.084613068352906e-06,
      "loss": 1.7214,
      "step": 4711
    },
    {
      "epoch": 1.824235385210995,
      "grad_norm": 10.359936714172363,
      "learning_rate": 9.084182905321117e-06,
      "loss": 1.1222,
      "step": 4712
    },
    {
      "epoch": 1.824622531939605,
      "grad_norm": 54.040748596191406,
      "learning_rate": 9.083752742289329e-06,
      "loss": 1.7876,
      "step": 4713
    },
    {
      "epoch": 1.8250096786682153,
      "grad_norm": 16.002439498901367,
      "learning_rate": 9.08332257925754e-06,
      "loss": 0.9005,
      "step": 4714
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 16.573368072509766,
      "learning_rate": 9.08289241622575e-06,
      "loss": 1.0502,
      "step": 4715
    },
    {
      "epoch": 1.8257839721254356,
      "grad_norm": 12.731379508972168,
      "learning_rate": 9.082462253193961e-06,
      "loss": 0.7754,
      "step": 4716
    },
    {
      "epoch": 1.8261711188540457,
      "grad_norm": 16.716434478759766,
      "learning_rate": 9.082032090162171e-06,
      "loss": 1.509,
      "step": 4717
    },
    {
      "epoch": 1.8265582655826558,
      "grad_norm": 17.859046936035156,
      "learning_rate": 9.081601927130384e-06,
      "loss": 1.2797,
      "step": 4718
    },
    {
      "epoch": 1.8269454123112658,
      "grad_norm": 13.62917423248291,
      "learning_rate": 9.081171764098594e-06,
      "loss": 0.6727,
      "step": 4719
    },
    {
      "epoch": 1.8273325590398761,
      "grad_norm": 21.967517852783203,
      "learning_rate": 9.080741601066805e-06,
      "loss": 1.102,
      "step": 4720
    },
    {
      "epoch": 1.8277197057684864,
      "grad_norm": 17.707082748413086,
      "learning_rate": 9.080311438035015e-06,
      "loss": 1.3329,
      "step": 4721
    },
    {
      "epoch": 1.8281068524970965,
      "grad_norm": 59.55329895019531,
      "learning_rate": 9.079881275003228e-06,
      "loss": 3.4179,
      "step": 4722
    },
    {
      "epoch": 1.8284939992257065,
      "grad_norm": 57.547576904296875,
      "learning_rate": 9.079451111971438e-06,
      "loss": 3.6522,
      "step": 4723
    },
    {
      "epoch": 1.8288811459543166,
      "grad_norm": 16.085952758789062,
      "learning_rate": 9.07902094893965e-06,
      "loss": 1.0819,
      "step": 4724
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 12.748781204223633,
      "learning_rate": 9.07859078590786e-06,
      "loss": 0.9518,
      "step": 4725
    },
    {
      "epoch": 1.829655439411537,
      "grad_norm": 21.87891960144043,
      "learning_rate": 9.07816062287607e-06,
      "loss": 2.1397,
      "step": 4726
    },
    {
      "epoch": 1.8300425861401473,
      "grad_norm": 12.068488121032715,
      "learning_rate": 9.077730459844282e-06,
      "loss": 1.0799,
      "step": 4727
    },
    {
      "epoch": 1.8304297328687573,
      "grad_norm": 19.47056770324707,
      "learning_rate": 9.077300296812493e-06,
      "loss": 1.988,
      "step": 4728
    },
    {
      "epoch": 1.8308168795973674,
      "grad_norm": 21.73441505432129,
      "learning_rate": 9.076870133780705e-06,
      "loss": 2.6527,
      "step": 4729
    },
    {
      "epoch": 1.8312040263259775,
      "grad_norm": 26.396364212036133,
      "learning_rate": 9.076439970748914e-06,
      "loss": 2.7494,
      "step": 4730
    },
    {
      "epoch": 1.8315911730545877,
      "grad_norm": 18.084714889526367,
      "learning_rate": 9.076009807717126e-06,
      "loss": 1.7473,
      "step": 4731
    },
    {
      "epoch": 1.8319783197831978,
      "grad_norm": 9.633057594299316,
      "learning_rate": 9.075579644685335e-06,
      "loss": 0.6152,
      "step": 4732
    },
    {
      "epoch": 1.832365466511808,
      "grad_norm": 15.700702667236328,
      "learning_rate": 9.075149481653549e-06,
      "loss": 1.0031,
      "step": 4733
    },
    {
      "epoch": 1.8327526132404182,
      "grad_norm": 19.861698150634766,
      "learning_rate": 9.074719318621758e-06,
      "loss": 1.3151,
      "step": 4734
    },
    {
      "epoch": 1.8331397599690282,
      "grad_norm": 25.351274490356445,
      "learning_rate": 9.07428915558997e-06,
      "loss": 1.7763,
      "step": 4735
    },
    {
      "epoch": 1.8335269066976383,
      "grad_norm": 45.254878997802734,
      "learning_rate": 9.07385899255818e-06,
      "loss": 1.2898,
      "step": 4736
    },
    {
      "epoch": 1.8339140534262486,
      "grad_norm": 15.296209335327148,
      "learning_rate": 9.073428829526393e-06,
      "loss": 1.0225,
      "step": 4737
    },
    {
      "epoch": 1.8343012001548586,
      "grad_norm": 23.304718017578125,
      "learning_rate": 9.072998666494602e-06,
      "loss": 1.4655,
      "step": 4738
    },
    {
      "epoch": 1.834688346883469,
      "grad_norm": 10.28002643585205,
      "learning_rate": 9.072568503462814e-06,
      "loss": 1.2175,
      "step": 4739
    },
    {
      "epoch": 1.835075493612079,
      "grad_norm": 11.539523124694824,
      "learning_rate": 9.072138340431023e-06,
      "loss": 1.1621,
      "step": 4740
    },
    {
      "epoch": 1.835462640340689,
      "grad_norm": 20.335124969482422,
      "learning_rate": 9.071708177399235e-06,
      "loss": 1.3476,
      "step": 4741
    },
    {
      "epoch": 1.8358497870692991,
      "grad_norm": 45.76581573486328,
      "learning_rate": 9.071278014367446e-06,
      "loss": 1.6607,
      "step": 4742
    },
    {
      "epoch": 1.8362369337979094,
      "grad_norm": 23.19999885559082,
      "learning_rate": 9.070847851335658e-06,
      "loss": 0.8655,
      "step": 4743
    },
    {
      "epoch": 1.8366240805265197,
      "grad_norm": 13.703385353088379,
      "learning_rate": 9.070417688303867e-06,
      "loss": 0.826,
      "step": 4744
    },
    {
      "epoch": 1.8370112272551298,
      "grad_norm": 21.16792869567871,
      "learning_rate": 9.069987525272079e-06,
      "loss": 2.0475,
      "step": 4745
    },
    {
      "epoch": 1.8373983739837398,
      "grad_norm": 9.595331192016602,
      "learning_rate": 9.06955736224029e-06,
      "loss": 0.6149,
      "step": 4746
    },
    {
      "epoch": 1.83778552071235,
      "grad_norm": 22.84583854675293,
      "learning_rate": 9.0691271992085e-06,
      "loss": 1.6638,
      "step": 4747
    },
    {
      "epoch": 1.83817266744096,
      "grad_norm": 23.357730865478516,
      "learning_rate": 9.068697036176711e-06,
      "loss": 2.4136,
      "step": 4748
    },
    {
      "epoch": 1.8385598141695703,
      "grad_norm": 18.276216506958008,
      "learning_rate": 9.068266873144923e-06,
      "loss": 1.3755,
      "step": 4749
    },
    {
      "epoch": 1.8389469608981805,
      "grad_norm": 23.345197677612305,
      "learning_rate": 9.067836710113134e-06,
      "loss": 2.4195,
      "step": 4750
    },
    {
      "epoch": 1.8393341076267906,
      "grad_norm": 14.439151763916016,
      "learning_rate": 9.067406547081344e-06,
      "loss": 0.7689,
      "step": 4751
    },
    {
      "epoch": 1.8397212543554007,
      "grad_norm": 23.39655303955078,
      "learning_rate": 9.066976384049555e-06,
      "loss": 1.6226,
      "step": 4752
    },
    {
      "epoch": 1.8401084010840107,
      "grad_norm": 15.727763175964355,
      "learning_rate": 9.066546221017767e-06,
      "loss": 0.9439,
      "step": 4753
    },
    {
      "epoch": 1.840495547812621,
      "grad_norm": 16.315696716308594,
      "learning_rate": 9.066116057985978e-06,
      "loss": 1.2588,
      "step": 4754
    },
    {
      "epoch": 1.840882694541231,
      "grad_norm": 13.965880393981934,
      "learning_rate": 9.065685894954188e-06,
      "loss": 0.86,
      "step": 4755
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 13.084175109863281,
      "learning_rate": 9.0652557319224e-06,
      "loss": 0.8393,
      "step": 4756
    },
    {
      "epoch": 1.8416569879984515,
      "grad_norm": 34.75734329223633,
      "learning_rate": 9.06482556889061e-06,
      "loss": 2.5995,
      "step": 4757
    },
    {
      "epoch": 1.8420441347270615,
      "grad_norm": 29.392772674560547,
      "learning_rate": 9.064395405858822e-06,
      "loss": 1.3374,
      "step": 4758
    },
    {
      "epoch": 1.8424312814556716,
      "grad_norm": 19.343809127807617,
      "learning_rate": 9.063965242827032e-06,
      "loss": 1.1225,
      "step": 4759
    },
    {
      "epoch": 1.8428184281842819,
      "grad_norm": 20.661174774169922,
      "learning_rate": 9.063535079795243e-06,
      "loss": 1.2181,
      "step": 4760
    },
    {
      "epoch": 1.843205574912892,
      "grad_norm": 28.99399185180664,
      "learning_rate": 9.063104916763455e-06,
      "loss": 1.5197,
      "step": 4761
    },
    {
      "epoch": 1.8435927216415022,
      "grad_norm": 15.362297058105469,
      "learning_rate": 9.062674753731664e-06,
      "loss": 1.3532,
      "step": 4762
    },
    {
      "epoch": 1.8439798683701123,
      "grad_norm": 29.799381256103516,
      "learning_rate": 9.062244590699876e-06,
      "loss": 1.6654,
      "step": 4763
    },
    {
      "epoch": 1.8443670150987224,
      "grad_norm": 19.358633041381836,
      "learning_rate": 9.061814427668087e-06,
      "loss": 1.745,
      "step": 4764
    },
    {
      "epoch": 1.8447541618273324,
      "grad_norm": 29.13005256652832,
      "learning_rate": 9.061384264636299e-06,
      "loss": 1.4424,
      "step": 4765
    },
    {
      "epoch": 1.8451413085559427,
      "grad_norm": 16.269596099853516,
      "learning_rate": 9.060954101604508e-06,
      "loss": 0.9686,
      "step": 4766
    },
    {
      "epoch": 1.845528455284553,
      "grad_norm": 17.469667434692383,
      "learning_rate": 9.06052393857272e-06,
      "loss": 1.0925,
      "step": 4767
    },
    {
      "epoch": 1.845915602013163,
      "grad_norm": 30.074661254882812,
      "learning_rate": 9.060093775540931e-06,
      "loss": 1.0673,
      "step": 4768
    },
    {
      "epoch": 1.8463027487417731,
      "grad_norm": 16.127859115600586,
      "learning_rate": 9.059663612509143e-06,
      "loss": 1.2864,
      "step": 4769
    },
    {
      "epoch": 1.8466898954703832,
      "grad_norm": 23.439823150634766,
      "learning_rate": 9.059233449477352e-06,
      "loss": 2.0354,
      "step": 4770
    },
    {
      "epoch": 1.8470770421989933,
      "grad_norm": 18.812597274780273,
      "learning_rate": 9.058803286445564e-06,
      "loss": 1.957,
      "step": 4771
    },
    {
      "epoch": 1.8474641889276036,
      "grad_norm": 26.657691955566406,
      "learning_rate": 9.058373123413775e-06,
      "loss": 1.4634,
      "step": 4772
    },
    {
      "epoch": 1.8478513356562138,
      "grad_norm": 14.918055534362793,
      "learning_rate": 9.057942960381987e-06,
      "loss": 1.0426,
      "step": 4773
    },
    {
      "epoch": 1.848238482384824,
      "grad_norm": 14.6629056930542,
      "learning_rate": 9.057512797350196e-06,
      "loss": 1.4661,
      "step": 4774
    },
    {
      "epoch": 1.848625629113434,
      "grad_norm": 27.451555252075195,
      "learning_rate": 9.057082634318408e-06,
      "loss": 1.7831,
      "step": 4775
    },
    {
      "epoch": 1.849012775842044,
      "grad_norm": 13.879433631896973,
      "learning_rate": 9.056652471286619e-06,
      "loss": 1.0763,
      "step": 4776
    },
    {
      "epoch": 1.8493999225706543,
      "grad_norm": 25.5767765045166,
      "learning_rate": 9.056222308254829e-06,
      "loss": 1.4026,
      "step": 4777
    },
    {
      "epoch": 1.8497870692992644,
      "grad_norm": 15.759086608886719,
      "learning_rate": 9.05579214522304e-06,
      "loss": 1.5206,
      "step": 4778
    },
    {
      "epoch": 1.8501742160278747,
      "grad_norm": 28.017648696899414,
      "learning_rate": 9.055361982191252e-06,
      "loss": 1.4793,
      "step": 4779
    },
    {
      "epoch": 1.8505613627564848,
      "grad_norm": 14.912091255187988,
      "learning_rate": 9.054931819159463e-06,
      "loss": 1.4594,
      "step": 4780
    },
    {
      "epoch": 1.8509485094850948,
      "grad_norm": 29.08673858642578,
      "learning_rate": 9.054501656127673e-06,
      "loss": 3.808,
      "step": 4781
    },
    {
      "epoch": 1.8513356562137049,
      "grad_norm": 15.783758163452148,
      "learning_rate": 9.054071493095884e-06,
      "loss": 1.5341,
      "step": 4782
    },
    {
      "epoch": 1.8517228029423152,
      "grad_norm": 19.635221481323242,
      "learning_rate": 9.053641330064094e-06,
      "loss": 3.5291,
      "step": 4783
    },
    {
      "epoch": 1.8521099496709252,
      "grad_norm": 16.68414878845215,
      "learning_rate": 9.053211167032307e-06,
      "loss": 1.2438,
      "step": 4784
    },
    {
      "epoch": 1.8524970963995355,
      "grad_norm": 13.648909568786621,
      "learning_rate": 9.052781004000517e-06,
      "loss": 1.0456,
      "step": 4785
    },
    {
      "epoch": 1.8528842431281456,
      "grad_norm": 26.786710739135742,
      "learning_rate": 9.052350840968728e-06,
      "loss": 0.9611,
      "step": 4786
    },
    {
      "epoch": 1.8532713898567557,
      "grad_norm": 16.374773025512695,
      "learning_rate": 9.051920677936938e-06,
      "loss": 1.4488,
      "step": 4787
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 18.653093338012695,
      "learning_rate": 9.051490514905151e-06,
      "loss": 1.4016,
      "step": 4788
    },
    {
      "epoch": 1.854045683313976,
      "grad_norm": 24.571386337280273,
      "learning_rate": 9.05106035187336e-06,
      "loss": 2.7764,
      "step": 4789
    },
    {
      "epoch": 1.8544328300425863,
      "grad_norm": 15.28890323638916,
      "learning_rate": 9.050630188841572e-06,
      "loss": 1.1779,
      "step": 4790
    },
    {
      "epoch": 1.8548199767711964,
      "grad_norm": 15.232799530029297,
      "learning_rate": 9.050200025809782e-06,
      "loss": 1.43,
      "step": 4791
    },
    {
      "epoch": 1.8552071234998064,
      "grad_norm": 14.443206787109375,
      "learning_rate": 9.049769862777993e-06,
      "loss": 1.3688,
      "step": 4792
    },
    {
      "epoch": 1.8555942702284165,
      "grad_norm": 19.68345069885254,
      "learning_rate": 9.049339699746205e-06,
      "loss": 1.0042,
      "step": 4793
    },
    {
      "epoch": 1.8559814169570266,
      "grad_norm": 17.868690490722656,
      "learning_rate": 9.048909536714416e-06,
      "loss": 1.3129,
      "step": 4794
    },
    {
      "epoch": 1.8563685636856369,
      "grad_norm": 30.245113372802734,
      "learning_rate": 9.048479373682626e-06,
      "loss": 1.7796,
      "step": 4795
    },
    {
      "epoch": 1.8567557104142471,
      "grad_norm": 28.681182861328125,
      "learning_rate": 9.048049210650837e-06,
      "loss": 2.0294,
      "step": 4796
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 22.450902938842773,
      "learning_rate": 9.047619047619049e-06,
      "loss": 1.5833,
      "step": 4797
    },
    {
      "epoch": 1.8575300038714673,
      "grad_norm": 14.8160982131958,
      "learning_rate": 9.047188884587258e-06,
      "loss": 1.421,
      "step": 4798
    },
    {
      "epoch": 1.8579171506000773,
      "grad_norm": 25.503637313842773,
      "learning_rate": 9.04675872155547e-06,
      "loss": 1.7958,
      "step": 4799
    },
    {
      "epoch": 1.8583042973286876,
      "grad_norm": 42.9366340637207,
      "learning_rate": 9.046328558523681e-06,
      "loss": 2.3292,
      "step": 4800
    },
    {
      "epoch": 1.8586914440572977,
      "grad_norm": 20.017213821411133,
      "learning_rate": 9.045898395491893e-06,
      "loss": 1.6969,
      "step": 4801
    },
    {
      "epoch": 1.859078590785908,
      "grad_norm": 20.79746437072754,
      "learning_rate": 9.045468232460102e-06,
      "loss": 0.9016,
      "step": 4802
    },
    {
      "epoch": 1.859465737514518,
      "grad_norm": 14.973353385925293,
      "learning_rate": 9.045038069428314e-06,
      "loss": 1.4254,
      "step": 4803
    },
    {
      "epoch": 1.8598528842431281,
      "grad_norm": 25.202787399291992,
      "learning_rate": 9.044607906396525e-06,
      "loss": 3.1384,
      "step": 4804
    },
    {
      "epoch": 1.8602400309717382,
      "grad_norm": 28.751760482788086,
      "learning_rate": 9.044177743364737e-06,
      "loss": 2.2753,
      "step": 4805
    },
    {
      "epoch": 1.8606271777003485,
      "grad_norm": 15.216693878173828,
      "learning_rate": 9.043747580332946e-06,
      "loss": 1.5844,
      "step": 4806
    },
    {
      "epoch": 1.8610143244289585,
      "grad_norm": 20.611061096191406,
      "learning_rate": 9.043317417301158e-06,
      "loss": 1.1166,
      "step": 4807
    },
    {
      "epoch": 1.8614014711575688,
      "grad_norm": 12.601858139038086,
      "learning_rate": 9.04288725426937e-06,
      "loss": 0.8852,
      "step": 4808
    },
    {
      "epoch": 1.8617886178861789,
      "grad_norm": 22.922582626342773,
      "learning_rate": 9.04245709123758e-06,
      "loss": 1.0052,
      "step": 4809
    },
    {
      "epoch": 1.862175764614789,
      "grad_norm": 37.34313201904297,
      "learning_rate": 9.04202692820579e-06,
      "loss": 1.4511,
      "step": 4810
    },
    {
      "epoch": 1.862562911343399,
      "grad_norm": 28.52491569519043,
      "learning_rate": 9.041596765174002e-06,
      "loss": 1.8636,
      "step": 4811
    },
    {
      "epoch": 1.8629500580720093,
      "grad_norm": 27.182514190673828,
      "learning_rate": 9.041166602142213e-06,
      "loss": 1.4325,
      "step": 4812
    },
    {
      "epoch": 1.8633372048006196,
      "grad_norm": 20.66800308227539,
      "learning_rate": 9.040736439110423e-06,
      "loss": 1.4575,
      "step": 4813
    },
    {
      "epoch": 1.8637243515292297,
      "grad_norm": 30.405241012573242,
      "learning_rate": 9.040306276078634e-06,
      "loss": 1.8148,
      "step": 4814
    },
    {
      "epoch": 1.8641114982578397,
      "grad_norm": 17.600982666015625,
      "learning_rate": 9.039876113046846e-06,
      "loss": 1.7735,
      "step": 4815
    },
    {
      "epoch": 1.8644986449864498,
      "grad_norm": 11.933355331420898,
      "learning_rate": 9.039445950015057e-06,
      "loss": 0.7676,
      "step": 4816
    },
    {
      "epoch": 1.8648857917150599,
      "grad_norm": 17.385284423828125,
      "learning_rate": 9.039015786983267e-06,
      "loss": 1.5362,
      "step": 4817
    },
    {
      "epoch": 1.8652729384436701,
      "grad_norm": 18.979774475097656,
      "learning_rate": 9.038585623951478e-06,
      "loss": 1.9994,
      "step": 4818
    },
    {
      "epoch": 1.8656600851722804,
      "grad_norm": 20.254302978515625,
      "learning_rate": 9.03815546091969e-06,
      "loss": 1.748,
      "step": 4819
    },
    {
      "epoch": 1.8660472319008905,
      "grad_norm": 13.354011535644531,
      "learning_rate": 9.037725297887901e-06,
      "loss": 0.5541,
      "step": 4820
    },
    {
      "epoch": 1.8664343786295006,
      "grad_norm": 18.440994262695312,
      "learning_rate": 9.03729513485611e-06,
      "loss": 1.4478,
      "step": 4821
    },
    {
      "epoch": 1.8668215253581106,
      "grad_norm": 13.879532814025879,
      "learning_rate": 9.036864971824322e-06,
      "loss": 0.8082,
      "step": 4822
    },
    {
      "epoch": 1.867208672086721,
      "grad_norm": 26.9272403717041,
      "learning_rate": 9.036434808792534e-06,
      "loss": 0.7744,
      "step": 4823
    },
    {
      "epoch": 1.867595818815331,
      "grad_norm": 36.60193634033203,
      "learning_rate": 9.036004645760745e-06,
      "loss": 2.0296,
      "step": 4824
    },
    {
      "epoch": 1.8679829655439413,
      "grad_norm": 30.688915252685547,
      "learning_rate": 9.035574482728955e-06,
      "loss": 1.2468,
      "step": 4825
    },
    {
      "epoch": 1.8683701122725513,
      "grad_norm": 24.419816970825195,
      "learning_rate": 9.035144319697166e-06,
      "loss": 1.6243,
      "step": 4826
    },
    {
      "epoch": 1.8687572590011614,
      "grad_norm": 28.06755828857422,
      "learning_rate": 9.034714156665378e-06,
      "loss": 1.3556,
      "step": 4827
    },
    {
      "epoch": 1.8691444057297715,
      "grad_norm": 22.00957489013672,
      "learning_rate": 9.034283993633587e-06,
      "loss": 1.81,
      "step": 4828
    },
    {
      "epoch": 1.8695315524583818,
      "grad_norm": 21.631187438964844,
      "learning_rate": 9.033853830601799e-06,
      "loss": 2.4254,
      "step": 4829
    },
    {
      "epoch": 1.8699186991869918,
      "grad_norm": 17.648027420043945,
      "learning_rate": 9.03342366757001e-06,
      "loss": 1.5423,
      "step": 4830
    },
    {
      "epoch": 1.8703058459156021,
      "grad_norm": 15.049750328063965,
      "learning_rate": 9.032993504538222e-06,
      "loss": 0.9388,
      "step": 4831
    },
    {
      "epoch": 1.8706929926442122,
      "grad_norm": 10.187567710876465,
      "learning_rate": 9.032563341506431e-06,
      "loss": 1.1964,
      "step": 4832
    },
    {
      "epoch": 1.8710801393728222,
      "grad_norm": 24.172895431518555,
      "learning_rate": 9.032133178474643e-06,
      "loss": 2.0852,
      "step": 4833
    },
    {
      "epoch": 1.8714672861014323,
      "grad_norm": 19.82834815979004,
      "learning_rate": 9.031703015442852e-06,
      "loss": 2.305,
      "step": 4834
    },
    {
      "epoch": 1.8718544328300426,
      "grad_norm": 18.459020614624023,
      "learning_rate": 9.031272852411066e-06,
      "loss": 1.5589,
      "step": 4835
    },
    {
      "epoch": 1.872241579558653,
      "grad_norm": 25.391210556030273,
      "learning_rate": 9.030842689379275e-06,
      "loss": 2.1315,
      "step": 4836
    },
    {
      "epoch": 1.872628726287263,
      "grad_norm": 32.55442428588867,
      "learning_rate": 9.030412526347487e-06,
      "loss": 1.6763,
      "step": 4837
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 12.192415237426758,
      "learning_rate": 9.029982363315696e-06,
      "loss": 0.6951,
      "step": 4838
    },
    {
      "epoch": 1.873403019744483,
      "grad_norm": 27.946279525756836,
      "learning_rate": 9.02955220028391e-06,
      "loss": 1.0936,
      "step": 4839
    },
    {
      "epoch": 1.8737901664730932,
      "grad_norm": 26.880645751953125,
      "learning_rate": 9.02912203725212e-06,
      "loss": 0.5588,
      "step": 4840
    },
    {
      "epoch": 1.8741773132017034,
      "grad_norm": 14.52866268157959,
      "learning_rate": 9.02869187422033e-06,
      "loss": 1.5185,
      "step": 4841
    },
    {
      "epoch": 1.8745644599303137,
      "grad_norm": 14.86014175415039,
      "learning_rate": 9.02826171118854e-06,
      "loss": 0.9726,
      "step": 4842
    },
    {
      "epoch": 1.8749516066589238,
      "grad_norm": 17.194509506225586,
      "learning_rate": 9.027831548156752e-06,
      "loss": 1.0442,
      "step": 4843
    },
    {
      "epoch": 1.8753387533875339,
      "grad_norm": 13.533390045166016,
      "learning_rate": 9.027401385124963e-06,
      "loss": 1.2204,
      "step": 4844
    },
    {
      "epoch": 1.875725900116144,
      "grad_norm": 17.0097599029541,
      "learning_rate": 9.026971222093175e-06,
      "loss": 1.5597,
      "step": 4845
    },
    {
      "epoch": 1.876113046844754,
      "grad_norm": 16.152118682861328,
      "learning_rate": 9.026541059061386e-06,
      "loss": 1.5325,
      "step": 4846
    },
    {
      "epoch": 1.8765001935733643,
      "grad_norm": 16.320127487182617,
      "learning_rate": 9.026110896029596e-06,
      "loss": 0.9221,
      "step": 4847
    },
    {
      "epoch": 1.8768873403019746,
      "grad_norm": 19.385662078857422,
      "learning_rate": 9.025680732997807e-06,
      "loss": 1.2747,
      "step": 4848
    },
    {
      "epoch": 1.8772744870305846,
      "grad_norm": 15.370759010314941,
      "learning_rate": 9.025250569966017e-06,
      "loss": 0.8985,
      "step": 4849
    },
    {
      "epoch": 1.8776616337591947,
      "grad_norm": 12.312460899353027,
      "learning_rate": 9.02482040693423e-06,
      "loss": 1.3343,
      "step": 4850
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 22.102134704589844,
      "learning_rate": 9.02439024390244e-06,
      "loss": 1.4608,
      "step": 4851
    },
    {
      "epoch": 1.878435927216415,
      "grad_norm": 21.798173904418945,
      "learning_rate": 9.023960080870651e-06,
      "loss": 1.8657,
      "step": 4852
    },
    {
      "epoch": 1.8788230739450251,
      "grad_norm": 20.847139358520508,
      "learning_rate": 9.023529917838861e-06,
      "loss": 0.985,
      "step": 4853
    },
    {
      "epoch": 1.8792102206736354,
      "grad_norm": 17.23723030090332,
      "learning_rate": 9.023099754807072e-06,
      "loss": 1.1416,
      "step": 4854
    },
    {
      "epoch": 1.8795973674022455,
      "grad_norm": 24.79956817626953,
      "learning_rate": 9.022669591775284e-06,
      "loss": 1.544,
      "step": 4855
    },
    {
      "epoch": 1.8799845141308555,
      "grad_norm": 22.3746337890625,
      "learning_rate": 9.022239428743495e-06,
      "loss": 1.7372,
      "step": 4856
    },
    {
      "epoch": 1.8803716608594656,
      "grad_norm": 14.972613334655762,
      "learning_rate": 9.021809265711705e-06,
      "loss": 1.3692,
      "step": 4857
    },
    {
      "epoch": 1.880758807588076,
      "grad_norm": 18.260515213012695,
      "learning_rate": 9.021379102679916e-06,
      "loss": 1.3446,
      "step": 4858
    },
    {
      "epoch": 1.8811459543166862,
      "grad_norm": 20.09796714782715,
      "learning_rate": 9.020948939648128e-06,
      "loss": 1.3448,
      "step": 4859
    },
    {
      "epoch": 1.8815331010452963,
      "grad_norm": 25.705270767211914,
      "learning_rate": 9.020518776616339e-06,
      "loss": 1.3962,
      "step": 4860
    },
    {
      "epoch": 1.8819202477739063,
      "grad_norm": 20.59067153930664,
      "learning_rate": 9.020088613584549e-06,
      "loss": 1.3067,
      "step": 4861
    },
    {
      "epoch": 1.8823073945025164,
      "grad_norm": 31.531557083129883,
      "learning_rate": 9.01965845055276e-06,
      "loss": 1.6964,
      "step": 4862
    },
    {
      "epoch": 1.8826945412311265,
      "grad_norm": 30.551698684692383,
      "learning_rate": 9.019228287520972e-06,
      "loss": 1.5251,
      "step": 4863
    },
    {
      "epoch": 1.8830816879597367,
      "grad_norm": 34.48265075683594,
      "learning_rate": 9.018798124489181e-06,
      "loss": 3.0648,
      "step": 4864
    },
    {
      "epoch": 1.883468834688347,
      "grad_norm": 20.82428741455078,
      "learning_rate": 9.018367961457393e-06,
      "loss": 1.81,
      "step": 4865
    },
    {
      "epoch": 1.883855981416957,
      "grad_norm": 36.51818084716797,
      "learning_rate": 9.017937798425604e-06,
      "loss": 2.2805,
      "step": 4866
    },
    {
      "epoch": 1.8842431281455672,
      "grad_norm": 22.640655517578125,
      "learning_rate": 9.017507635393816e-06,
      "loss": 1.2713,
      "step": 4867
    },
    {
      "epoch": 1.8846302748741772,
      "grad_norm": 6.875522136688232,
      "learning_rate": 9.017077472362025e-06,
      "loss": 0.2192,
      "step": 4868
    },
    {
      "epoch": 1.8850174216027873,
      "grad_norm": 9.207972526550293,
      "learning_rate": 9.016647309330237e-06,
      "loss": 0.3702,
      "step": 4869
    },
    {
      "epoch": 1.8854045683313976,
      "grad_norm": 27.489046096801758,
      "learning_rate": 9.016217146298448e-06,
      "loss": 1.3441,
      "step": 4870
    },
    {
      "epoch": 1.8857917150600079,
      "grad_norm": 23.544109344482422,
      "learning_rate": 9.01578698326666e-06,
      "loss": 1.6055,
      "step": 4871
    },
    {
      "epoch": 1.886178861788618,
      "grad_norm": 8.501269340515137,
      "learning_rate": 9.01535682023487e-06,
      "loss": 0.281,
      "step": 4872
    },
    {
      "epoch": 1.886566008517228,
      "grad_norm": 24.326276779174805,
      "learning_rate": 9.01492665720308e-06,
      "loss": 2.0524,
      "step": 4873
    },
    {
      "epoch": 1.886953155245838,
      "grad_norm": 45.6463737487793,
      "learning_rate": 9.014496494171292e-06,
      "loss": 2.2004,
      "step": 4874
    },
    {
      "epoch": 1.8873403019744484,
      "grad_norm": 12.111969947814941,
      "learning_rate": 9.014066331139504e-06,
      "loss": 1.1056,
      "step": 4875
    },
    {
      "epoch": 1.8877274487030584,
      "grad_norm": 15.28996467590332,
      "learning_rate": 9.013636168107713e-06,
      "loss": 1.395,
      "step": 4876
    },
    {
      "epoch": 1.8881145954316687,
      "grad_norm": 31.05906105041504,
      "learning_rate": 9.013206005075925e-06,
      "loss": 1.6388,
      "step": 4877
    },
    {
      "epoch": 1.8885017421602788,
      "grad_norm": 21.70121192932129,
      "learning_rate": 9.012775842044136e-06,
      "loss": 1.9754,
      "step": 4878
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 18.480268478393555,
      "learning_rate": 9.012345679012346e-06,
      "loss": 1.5846,
      "step": 4879
    },
    {
      "epoch": 1.889276035617499,
      "grad_norm": 23.643251419067383,
      "learning_rate": 9.011915515980557e-06,
      "loss": 2.0424,
      "step": 4880
    },
    {
      "epoch": 1.8896631823461092,
      "grad_norm": 15.464435577392578,
      "learning_rate": 9.011485352948769e-06,
      "loss": 1.2022,
      "step": 4881
    },
    {
      "epoch": 1.8900503290747195,
      "grad_norm": 13.01025390625,
      "learning_rate": 9.01105518991698e-06,
      "loss": 1.395,
      "step": 4882
    },
    {
      "epoch": 1.8904374758033295,
      "grad_norm": 16.55594253540039,
      "learning_rate": 9.01062502688519e-06,
      "loss": 1.2876,
      "step": 4883
    },
    {
      "epoch": 1.8908246225319396,
      "grad_norm": 22.38551139831543,
      "learning_rate": 9.010194863853401e-06,
      "loss": 2.256,
      "step": 4884
    },
    {
      "epoch": 1.8912117692605497,
      "grad_norm": 26.88243293762207,
      "learning_rate": 9.009764700821611e-06,
      "loss": 1.7546,
      "step": 4885
    },
    {
      "epoch": 1.8915989159891597,
      "grad_norm": 18.60291290283203,
      "learning_rate": 9.009334537789824e-06,
      "loss": 1.403,
      "step": 4886
    },
    {
      "epoch": 1.89198606271777,
      "grad_norm": 19.555078506469727,
      "learning_rate": 9.008904374758034e-06,
      "loss": 1.8596,
      "step": 4887
    },
    {
      "epoch": 1.8923732094463803,
      "grad_norm": 12.822184562683105,
      "learning_rate": 9.008474211726245e-06,
      "loss": 0.9218,
      "step": 4888
    },
    {
      "epoch": 1.8927603561749904,
      "grad_norm": 18.987356185913086,
      "learning_rate": 9.008044048694457e-06,
      "loss": 1.9458,
      "step": 4889
    },
    {
      "epoch": 1.8931475029036005,
      "grad_norm": 20.090641021728516,
      "learning_rate": 9.007613885662666e-06,
      "loss": 1.5383,
      "step": 4890
    },
    {
      "epoch": 1.8935346496322105,
      "grad_norm": 21.058300018310547,
      "learning_rate": 9.007183722630878e-06,
      "loss": 2.4167,
      "step": 4891
    },
    {
      "epoch": 1.8939217963608206,
      "grad_norm": 9.63124942779541,
      "learning_rate": 9.006753559599089e-06,
      "loss": 1.288,
      "step": 4892
    },
    {
      "epoch": 1.8943089430894309,
      "grad_norm": 13.493571281433105,
      "learning_rate": 9.0063233965673e-06,
      "loss": 0.8912,
      "step": 4893
    },
    {
      "epoch": 1.8946960898180412,
      "grad_norm": 17.705041885375977,
      "learning_rate": 9.00589323353551e-06,
      "loss": 0.9878,
      "step": 4894
    },
    {
      "epoch": 1.8950832365466512,
      "grad_norm": 33.49905776977539,
      "learning_rate": 9.005463070503722e-06,
      "loss": 1.6185,
      "step": 4895
    },
    {
      "epoch": 1.8954703832752613,
      "grad_norm": 15.033263206481934,
      "learning_rate": 9.005032907471933e-06,
      "loss": 0.9303,
      "step": 4896
    },
    {
      "epoch": 1.8958575300038714,
      "grad_norm": 29.672603607177734,
      "learning_rate": 9.004602744440145e-06,
      "loss": 1.5941,
      "step": 4897
    },
    {
      "epoch": 1.8962446767324816,
      "grad_norm": 39.99323654174805,
      "learning_rate": 9.004172581408354e-06,
      "loss": 2.2276,
      "step": 4898
    },
    {
      "epoch": 1.8966318234610917,
      "grad_norm": 19.313974380493164,
      "learning_rate": 9.003742418376566e-06,
      "loss": 1.535,
      "step": 4899
    },
    {
      "epoch": 1.897018970189702,
      "grad_norm": 29.926616668701172,
      "learning_rate": 9.003312255344775e-06,
      "loss": 1.3861,
      "step": 4900
    },
    {
      "epoch": 1.897406116918312,
      "grad_norm": 17.096752166748047,
      "learning_rate": 9.002882092312988e-06,
      "loss": 0.3354,
      "step": 4901
    },
    {
      "epoch": 1.8977932636469221,
      "grad_norm": 16.81753921508789,
      "learning_rate": 9.002451929281198e-06,
      "loss": 1.0258,
      "step": 4902
    },
    {
      "epoch": 1.8981804103755322,
      "grad_norm": 22.825397491455078,
      "learning_rate": 9.00202176624941e-06,
      "loss": 1.9347,
      "step": 4903
    },
    {
      "epoch": 1.8985675571041425,
      "grad_norm": 9.373381614685059,
      "learning_rate": 9.00159160321762e-06,
      "loss": 1.2859,
      "step": 4904
    },
    {
      "epoch": 1.8989547038327528,
      "grad_norm": 22.79969596862793,
      "learning_rate": 9.00116144018583e-06,
      "loss": 1.552,
      "step": 4905
    },
    {
      "epoch": 1.8993418505613628,
      "grad_norm": 26.879638671875,
      "learning_rate": 9.000731277154042e-06,
      "loss": 1.0207,
      "step": 4906
    },
    {
      "epoch": 1.899728997289973,
      "grad_norm": 25.43794822692871,
      "learning_rate": 9.000301114122254e-06,
      "loss": 1.1575,
      "step": 4907
    },
    {
      "epoch": 1.900116144018583,
      "grad_norm": 25.885757446289062,
      "learning_rate": 8.999870951090463e-06,
      "loss": 1.4017,
      "step": 4908
    },
    {
      "epoch": 1.900503290747193,
      "grad_norm": 35.24931716918945,
      "learning_rate": 8.999440788058675e-06,
      "loss": 1.0081,
      "step": 4909
    },
    {
      "epoch": 1.9008904374758033,
      "grad_norm": 29.761072158813477,
      "learning_rate": 8.999010625026886e-06,
      "loss": 1.2149,
      "step": 4910
    },
    {
      "epoch": 1.9012775842044136,
      "grad_norm": 10.461771011352539,
      "learning_rate": 8.998580461995098e-06,
      "loss": 1.3643,
      "step": 4911
    },
    {
      "epoch": 1.9016647309330237,
      "grad_norm": 24.999794006347656,
      "learning_rate": 8.998150298963307e-06,
      "loss": 2.8978,
      "step": 4912
    },
    {
      "epoch": 1.9020518776616337,
      "grad_norm": 27.203588485717773,
      "learning_rate": 8.997720135931519e-06,
      "loss": 1.577,
      "step": 4913
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 24.79545783996582,
      "learning_rate": 8.99728997289973e-06,
      "loss": 1.2629,
      "step": 4914
    },
    {
      "epoch": 1.9028261711188539,
      "grad_norm": 20.289138793945312,
      "learning_rate": 8.99685980986794e-06,
      "loss": 0.9668,
      "step": 4915
    },
    {
      "epoch": 1.9032133178474642,
      "grad_norm": 25.089807510375977,
      "learning_rate": 8.996429646836151e-06,
      "loss": 1.4409,
      "step": 4916
    },
    {
      "epoch": 1.9036004645760745,
      "grad_norm": 51.35795593261719,
      "learning_rate": 8.995999483804363e-06,
      "loss": 2.2146,
      "step": 4917
    },
    {
      "epoch": 1.9039876113046845,
      "grad_norm": 17.856700897216797,
      "learning_rate": 8.995569320772574e-06,
      "loss": 1.3768,
      "step": 4918
    },
    {
      "epoch": 1.9043747580332946,
      "grad_norm": 12.666621208190918,
      "learning_rate": 8.995139157740784e-06,
      "loss": 1.1887,
      "step": 4919
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 18.414051055908203,
      "learning_rate": 8.994708994708995e-06,
      "loss": 1.4409,
      "step": 4920
    },
    {
      "epoch": 1.905149051490515,
      "grad_norm": 21.924274444580078,
      "learning_rate": 8.994278831677207e-06,
      "loss": 1.6524,
      "step": 4921
    },
    {
      "epoch": 1.905536198219125,
      "grad_norm": 20.565805435180664,
      "learning_rate": 8.993848668645418e-06,
      "loss": 1.9846,
      "step": 4922
    },
    {
      "epoch": 1.9059233449477353,
      "grad_norm": 18.570789337158203,
      "learning_rate": 8.993418505613628e-06,
      "loss": 1.539,
      "step": 4923
    },
    {
      "epoch": 1.9063104916763454,
      "grad_norm": 16.439891815185547,
      "learning_rate": 8.99298834258184e-06,
      "loss": 1.1402,
      "step": 4924
    },
    {
      "epoch": 1.9066976384049554,
      "grad_norm": 21.317459106445312,
      "learning_rate": 8.99255817955005e-06,
      "loss": 1.8219,
      "step": 4925
    },
    {
      "epoch": 1.9070847851335655,
      "grad_norm": 24.63262176513672,
      "learning_rate": 8.99212801651826e-06,
      "loss": 1.0184,
      "step": 4926
    },
    {
      "epoch": 1.9074719318621758,
      "grad_norm": 16.02054214477539,
      "learning_rate": 8.991697853486472e-06,
      "loss": 1.1517,
      "step": 4927
    },
    {
      "epoch": 1.907859078590786,
      "grad_norm": 18.70572280883789,
      "learning_rate": 8.991267690454683e-06,
      "loss": 1.1682,
      "step": 4928
    },
    {
      "epoch": 1.9082462253193961,
      "grad_norm": 25.162269592285156,
      "learning_rate": 8.990837527422895e-06,
      "loss": 1.5876,
      "step": 4929
    },
    {
      "epoch": 1.9086333720480062,
      "grad_norm": 16.48763656616211,
      "learning_rate": 8.990407364391104e-06,
      "loss": 1.9778,
      "step": 4930
    },
    {
      "epoch": 1.9090205187766163,
      "grad_norm": 18.969533920288086,
      "learning_rate": 8.989977201359316e-06,
      "loss": 1.2704,
      "step": 4931
    },
    {
      "epoch": 1.9094076655052263,
      "grad_norm": 41.66298294067383,
      "learning_rate": 8.989547038327527e-06,
      "loss": 1.0822,
      "step": 4932
    },
    {
      "epoch": 1.9097948122338366,
      "grad_norm": 26.027294158935547,
      "learning_rate": 8.989116875295739e-06,
      "loss": 1.5257,
      "step": 4933
    },
    {
      "epoch": 1.910181958962447,
      "grad_norm": 24.02448844909668,
      "learning_rate": 8.988686712263948e-06,
      "loss": 1.8519,
      "step": 4934
    },
    {
      "epoch": 1.910569105691057,
      "grad_norm": 21.22686004638672,
      "learning_rate": 8.98825654923216e-06,
      "loss": 2.1691,
      "step": 4935
    },
    {
      "epoch": 1.910956252419667,
      "grad_norm": 22.871536254882812,
      "learning_rate": 8.987826386200371e-06,
      "loss": 1.3318,
      "step": 4936
    },
    {
      "epoch": 1.9113433991482771,
      "grad_norm": 13.411397933959961,
      "learning_rate": 8.987396223168583e-06,
      "loss": 0.827,
      "step": 4937
    },
    {
      "epoch": 1.9117305458768872,
      "grad_norm": 19.786052703857422,
      "learning_rate": 8.986966060136792e-06,
      "loss": 1.2776,
      "step": 4938
    },
    {
      "epoch": 1.9121176926054975,
      "grad_norm": 14.237548828125,
      "learning_rate": 8.986535897105004e-06,
      "loss": 0.8481,
      "step": 4939
    },
    {
      "epoch": 1.9125048393341078,
      "grad_norm": 25.06519889831543,
      "learning_rate": 8.986105734073215e-06,
      "loss": 1.6934,
      "step": 4940
    },
    {
      "epoch": 1.9128919860627178,
      "grad_norm": 21.115345001220703,
      "learning_rate": 8.985675571041425e-06,
      "loss": 1.3234,
      "step": 4941
    },
    {
      "epoch": 1.9132791327913279,
      "grad_norm": 20.585487365722656,
      "learning_rate": 8.985245408009636e-06,
      "loss": 1.6723,
      "step": 4942
    },
    {
      "epoch": 1.913666279519938,
      "grad_norm": 13.805294036865234,
      "learning_rate": 8.984815244977848e-06,
      "loss": 1.414,
      "step": 4943
    },
    {
      "epoch": 1.9140534262485482,
      "grad_norm": 26.34968376159668,
      "learning_rate": 8.984385081946059e-06,
      "loss": 1.4427,
      "step": 4944
    },
    {
      "epoch": 1.9144405729771583,
      "grad_norm": 27.496482849121094,
      "learning_rate": 8.983954918914269e-06,
      "loss": 1.2428,
      "step": 4945
    },
    {
      "epoch": 1.9148277197057686,
      "grad_norm": 27.63329315185547,
      "learning_rate": 8.98352475588248e-06,
      "loss": 1.4792,
      "step": 4946
    },
    {
      "epoch": 1.9152148664343787,
      "grad_norm": 17.35972785949707,
      "learning_rate": 8.983094592850692e-06,
      "loss": 0.82,
      "step": 4947
    },
    {
      "epoch": 1.9156020131629887,
      "grad_norm": 23.872060775756836,
      "learning_rate": 8.982664429818903e-06,
      "loss": 1.7518,
      "step": 4948
    },
    {
      "epoch": 1.9159891598915988,
      "grad_norm": 32.231502532958984,
      "learning_rate": 8.982234266787113e-06,
      "loss": 2.7638,
      "step": 4949
    },
    {
      "epoch": 1.916376306620209,
      "grad_norm": 14.199078559875488,
      "learning_rate": 8.981804103755324e-06,
      "loss": 0.8954,
      "step": 4950
    },
    {
      "epoch": 1.9167634533488194,
      "grad_norm": 24.528013229370117,
      "learning_rate": 8.981373940723534e-06,
      "loss": 1.3217,
      "step": 4951
    },
    {
      "epoch": 1.9171506000774294,
      "grad_norm": 26.732643127441406,
      "learning_rate": 8.980943777691747e-06,
      "loss": 1.9691,
      "step": 4952
    },
    {
      "epoch": 1.9175377468060395,
      "grad_norm": 15.953629493713379,
      "learning_rate": 8.980513614659957e-06,
      "loss": 1.3936,
      "step": 4953
    },
    {
      "epoch": 1.9179248935346496,
      "grad_norm": 8.628576278686523,
      "learning_rate": 8.980083451628168e-06,
      "loss": 0.4089,
      "step": 4954
    },
    {
      "epoch": 1.9183120402632596,
      "grad_norm": 26.416601181030273,
      "learning_rate": 8.979653288596378e-06,
      "loss": 1.6096,
      "step": 4955
    },
    {
      "epoch": 1.91869918699187,
      "grad_norm": 21.26247787475586,
      "learning_rate": 8.97922312556459e-06,
      "loss": 2.5864,
      "step": 4956
    },
    {
      "epoch": 1.9190863337204802,
      "grad_norm": 23.64114761352539,
      "learning_rate": 8.9787929625328e-06,
      "loss": 1.4178,
      "step": 4957
    },
    {
      "epoch": 1.9194734804490903,
      "grad_norm": 16.936702728271484,
      "learning_rate": 8.978362799501012e-06,
      "loss": 1.7217,
      "step": 4958
    },
    {
      "epoch": 1.9198606271777003,
      "grad_norm": 25.5001163482666,
      "learning_rate": 8.977932636469222e-06,
      "loss": 1.9446,
      "step": 4959
    },
    {
      "epoch": 1.9202477739063104,
      "grad_norm": 29.822956085205078,
      "learning_rate": 8.977502473437433e-06,
      "loss": 1.6047,
      "step": 4960
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 34.72706985473633,
      "learning_rate": 8.977072310405645e-06,
      "loss": 1.7574,
      "step": 4961
    },
    {
      "epoch": 1.9210220673635308,
      "grad_norm": 8.17906379699707,
      "learning_rate": 8.976642147373854e-06,
      "loss": 0.5461,
      "step": 4962
    },
    {
      "epoch": 1.921409214092141,
      "grad_norm": 11.645334243774414,
      "learning_rate": 8.976211984342066e-06,
      "loss": 0.5216,
      "step": 4963
    },
    {
      "epoch": 1.9217963608207511,
      "grad_norm": 41.04625701904297,
      "learning_rate": 8.975781821310277e-06,
      "loss": 2.5799,
      "step": 4964
    },
    {
      "epoch": 1.9221835075493612,
      "grad_norm": 18.667362213134766,
      "learning_rate": 8.975351658278489e-06,
      "loss": 1.3428,
      "step": 4965
    },
    {
      "epoch": 1.9225706542779712,
      "grad_norm": 14.303143501281738,
      "learning_rate": 8.974921495246698e-06,
      "loss": 0.9794,
      "step": 4966
    },
    {
      "epoch": 1.9229578010065815,
      "grad_norm": 12.849405288696289,
      "learning_rate": 8.97449133221491e-06,
      "loss": 0.8764,
      "step": 4967
    },
    {
      "epoch": 1.9233449477351916,
      "grad_norm": 22.610260009765625,
      "learning_rate": 8.974061169183121e-06,
      "loss": 1.1984,
      "step": 4968
    },
    {
      "epoch": 1.923732094463802,
      "grad_norm": 23.663394927978516,
      "learning_rate": 8.973631006151333e-06,
      "loss": 2.002,
      "step": 4969
    },
    {
      "epoch": 1.924119241192412,
      "grad_norm": 15.697454452514648,
      "learning_rate": 8.973200843119542e-06,
      "loss": 0.896,
      "step": 4970
    },
    {
      "epoch": 1.924506387921022,
      "grad_norm": 14.352622032165527,
      "learning_rate": 8.972770680087754e-06,
      "loss": 1.3712,
      "step": 4971
    },
    {
      "epoch": 1.924893534649632,
      "grad_norm": 20.42909812927246,
      "learning_rate": 8.972340517055965e-06,
      "loss": 1.3822,
      "step": 4972
    },
    {
      "epoch": 1.9252806813782424,
      "grad_norm": 17.873180389404297,
      "learning_rate": 8.971910354024177e-06,
      "loss": 1.7086,
      "step": 4973
    },
    {
      "epoch": 1.9256678281068524,
      "grad_norm": 23.521024703979492,
      "learning_rate": 8.971480190992386e-06,
      "loss": 1.163,
      "step": 4974
    },
    {
      "epoch": 1.9260549748354627,
      "grad_norm": 17.26421356201172,
      "learning_rate": 8.971050027960598e-06,
      "loss": 1.5236,
      "step": 4975
    },
    {
      "epoch": 1.9264421215640728,
      "grad_norm": 14.830324172973633,
      "learning_rate": 8.970619864928809e-06,
      "loss": 1.1913,
      "step": 4976
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 21.70372200012207,
      "learning_rate": 8.970189701897019e-06,
      "loss": 1.5759,
      "step": 4977
    },
    {
      "epoch": 1.927216415021293,
      "grad_norm": 22.018672943115234,
      "learning_rate": 8.96975953886523e-06,
      "loss": 1.7433,
      "step": 4978
    },
    {
      "epoch": 1.9276035617499032,
      "grad_norm": 42.068206787109375,
      "learning_rate": 8.969329375833442e-06,
      "loss": 1.4337,
      "step": 4979
    },
    {
      "epoch": 1.9279907084785135,
      "grad_norm": 19.72142791748047,
      "learning_rate": 8.968899212801653e-06,
      "loss": 1.624,
      "step": 4980
    },
    {
      "epoch": 1.9283778552071236,
      "grad_norm": 25.839208602905273,
      "learning_rate": 8.968469049769863e-06,
      "loss": 1.4811,
      "step": 4981
    },
    {
      "epoch": 1.9287650019357336,
      "grad_norm": 14.108383178710938,
      "learning_rate": 8.968038886738074e-06,
      "loss": 1.0157,
      "step": 4982
    },
    {
      "epoch": 1.9291521486643437,
      "grad_norm": 15.001276969909668,
      "learning_rate": 8.967608723706286e-06,
      "loss": 0.5326,
      "step": 4983
    },
    {
      "epoch": 1.9295392953929538,
      "grad_norm": 30.3634033203125,
      "learning_rate": 8.967178560674497e-06,
      "loss": 2.4995,
      "step": 4984
    },
    {
      "epoch": 1.929926442121564,
      "grad_norm": 20.692209243774414,
      "learning_rate": 8.966748397642707e-06,
      "loss": 1.7759,
      "step": 4985
    },
    {
      "epoch": 1.9303135888501743,
      "grad_norm": 16.235151290893555,
      "learning_rate": 8.966318234610918e-06,
      "loss": 1.3394,
      "step": 4986
    },
    {
      "epoch": 1.9307007355787844,
      "grad_norm": 20.598243713378906,
      "learning_rate": 8.96588807157913e-06,
      "loss": 1.7926,
      "step": 4987
    },
    {
      "epoch": 1.9310878823073945,
      "grad_norm": 23.412961959838867,
      "learning_rate": 8.965457908547341e-06,
      "loss": 1.8497,
      "step": 4988
    },
    {
      "epoch": 1.9314750290360045,
      "grad_norm": 29.564186096191406,
      "learning_rate": 8.96502774551555e-06,
      "loss": 1.5428,
      "step": 4989
    },
    {
      "epoch": 1.9318621757646148,
      "grad_norm": 21.28982925415039,
      "learning_rate": 8.964597582483762e-06,
      "loss": 1.3082,
      "step": 4990
    },
    {
      "epoch": 1.932249322493225,
      "grad_norm": 24.595672607421875,
      "learning_rate": 8.964167419451974e-06,
      "loss": 1.7253,
      "step": 4991
    },
    {
      "epoch": 1.9326364692218352,
      "grad_norm": 14.839834213256836,
      "learning_rate": 8.963737256420183e-06,
      "loss": 0.9543,
      "step": 4992
    },
    {
      "epoch": 1.9330236159504453,
      "grad_norm": 22.162818908691406,
      "learning_rate": 8.963307093388395e-06,
      "loss": 1.7232,
      "step": 4993
    },
    {
      "epoch": 1.9334107626790553,
      "grad_norm": 19.117101669311523,
      "learning_rate": 8.962876930356606e-06,
      "loss": 1.8885,
      "step": 4994
    },
    {
      "epoch": 1.9337979094076654,
      "grad_norm": 19.931550979614258,
      "learning_rate": 8.962446767324818e-06,
      "loss": 1.6844,
      "step": 4995
    },
    {
      "epoch": 1.9341850561362757,
      "grad_norm": 29.34485626220703,
      "learning_rate": 8.962016604293027e-06,
      "loss": 1.6281,
      "step": 4996
    },
    {
      "epoch": 1.9345722028648857,
      "grad_norm": 18.886865615844727,
      "learning_rate": 8.961586441261239e-06,
      "loss": 1.3513,
      "step": 4997
    },
    {
      "epoch": 1.934959349593496,
      "grad_norm": 16.671855926513672,
      "learning_rate": 8.961156278229448e-06,
      "loss": 1.3599,
      "step": 4998
    },
    {
      "epoch": 1.935346496322106,
      "grad_norm": 13.790915489196777,
      "learning_rate": 8.960726115197661e-06,
      "loss": 0.7882,
      "step": 4999
    },
    {
      "epoch": 1.9357336430507162,
      "grad_norm": 18.460559844970703,
      "learning_rate": 8.960295952165871e-06,
      "loss": 1.6672,
      "step": 5000
    },
    {
      "epoch": 1.9361207897793262,
      "grad_norm": 23.79583168029785,
      "learning_rate": 8.959865789134083e-06,
      "loss": 1.7994,
      "step": 5001
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 18.61696434020996,
      "learning_rate": 8.959435626102292e-06,
      "loss": 0.9771,
      "step": 5002
    },
    {
      "epoch": 1.9368950832365468,
      "grad_norm": 13.473126411437988,
      "learning_rate": 8.959005463070505e-06,
      "loss": 0.5516,
      "step": 5003
    },
    {
      "epoch": 1.9372822299651569,
      "grad_norm": 20.256925582885742,
      "learning_rate": 8.958575300038715e-06,
      "loss": 1.5957,
      "step": 5004
    },
    {
      "epoch": 1.937669376693767,
      "grad_norm": 18.566518783569336,
      "learning_rate": 8.958145137006927e-06,
      "loss": 1.5907,
      "step": 5005
    },
    {
      "epoch": 1.938056523422377,
      "grad_norm": 22.403587341308594,
      "learning_rate": 8.957714973975136e-06,
      "loss": 1.6719,
      "step": 5006
    },
    {
      "epoch": 1.938443670150987,
      "grad_norm": 18.056713104248047,
      "learning_rate": 8.957284810943348e-06,
      "loss": 1.3833,
      "step": 5007
    },
    {
      "epoch": 1.9388308168795974,
      "grad_norm": 28.40604019165039,
      "learning_rate": 8.956854647911559e-06,
      "loss": 1.6835,
      "step": 5008
    },
    {
      "epoch": 1.9392179636082076,
      "grad_norm": 17.384031295776367,
      "learning_rate": 8.95642448487977e-06,
      "loss": 1.355,
      "step": 5009
    },
    {
      "epoch": 1.9396051103368177,
      "grad_norm": 25.678709030151367,
      "learning_rate": 8.95599432184798e-06,
      "loss": 0.9626,
      "step": 5010
    },
    {
      "epoch": 1.9399922570654278,
      "grad_norm": 28.737470626831055,
      "learning_rate": 8.955564158816192e-06,
      "loss": 1.5817,
      "step": 5011
    },
    {
      "epoch": 1.9403794037940378,
      "grad_norm": 13.74497127532959,
      "learning_rate": 8.955133995784403e-06,
      "loss": 1.0506,
      "step": 5012
    },
    {
      "epoch": 1.9407665505226481,
      "grad_norm": 15.772296905517578,
      "learning_rate": 8.954703832752613e-06,
      "loss": 1.0893,
      "step": 5013
    },
    {
      "epoch": 1.9411536972512582,
      "grad_norm": 27.078197479248047,
      "learning_rate": 8.954273669720826e-06,
      "loss": 2.8366,
      "step": 5014
    },
    {
      "epoch": 1.9415408439798685,
      "grad_norm": 36.06413650512695,
      "learning_rate": 8.953843506689036e-06,
      "loss": 2.0108,
      "step": 5015
    },
    {
      "epoch": 1.9419279907084785,
      "grad_norm": 23.48630714416504,
      "learning_rate": 8.953413343657247e-06,
      "loss": 2.476,
      "step": 5016
    },
    {
      "epoch": 1.9423151374370886,
      "grad_norm": 18.036592483520508,
      "learning_rate": 8.952983180625457e-06,
      "loss": 1.3987,
      "step": 5017
    },
    {
      "epoch": 1.9427022841656987,
      "grad_norm": 34.800724029541016,
      "learning_rate": 8.95255301759367e-06,
      "loss": 1.3232,
      "step": 5018
    },
    {
      "epoch": 1.943089430894309,
      "grad_norm": 13.481511116027832,
      "learning_rate": 8.95212285456188e-06,
      "loss": 0.8536,
      "step": 5019
    },
    {
      "epoch": 1.943476577622919,
      "grad_norm": 16.697940826416016,
      "learning_rate": 8.951692691530091e-06,
      "loss": 1.3831,
      "step": 5020
    },
    {
      "epoch": 1.9438637243515293,
      "grad_norm": 12.888999938964844,
      "learning_rate": 8.9512625284983e-06,
      "loss": 1.0941,
      "step": 5021
    },
    {
      "epoch": 1.9442508710801394,
      "grad_norm": 16.493701934814453,
      "learning_rate": 8.950832365466512e-06,
      "loss": 1.6612,
      "step": 5022
    },
    {
      "epoch": 1.9446380178087495,
      "grad_norm": 25.98165512084961,
      "learning_rate": 8.950402202434724e-06,
      "loss": 2.028,
      "step": 5023
    },
    {
      "epoch": 1.9450251645373595,
      "grad_norm": 19.496999740600586,
      "learning_rate": 8.949972039402935e-06,
      "loss": 0.3908,
      "step": 5024
    },
    {
      "epoch": 1.9454123112659698,
      "grad_norm": 14.68396282196045,
      "learning_rate": 8.949541876371145e-06,
      "loss": 0.9793,
      "step": 5025
    },
    {
      "epoch": 1.94579945799458,
      "grad_norm": 39.87046813964844,
      "learning_rate": 8.949111713339356e-06,
      "loss": 2.8255,
      "step": 5026
    },
    {
      "epoch": 1.9461866047231902,
      "grad_norm": 30.765535354614258,
      "learning_rate": 8.948681550307568e-06,
      "loss": 1.0577,
      "step": 5027
    },
    {
      "epoch": 1.9465737514518002,
      "grad_norm": 39.40552520751953,
      "learning_rate": 8.948251387275777e-06,
      "loss": 1.4707,
      "step": 5028
    },
    {
      "epoch": 1.9469608981804103,
      "grad_norm": 15.692784309387207,
      "learning_rate": 8.947821224243989e-06,
      "loss": 1.4032,
      "step": 5029
    },
    {
      "epoch": 1.9473480449090204,
      "grad_norm": 47.64657974243164,
      "learning_rate": 8.9473910612122e-06,
      "loss": 1.3878,
      "step": 5030
    },
    {
      "epoch": 1.9477351916376306,
      "grad_norm": 14.196982383728027,
      "learning_rate": 8.946960898180412e-06,
      "loss": 1.3315,
      "step": 5031
    },
    {
      "epoch": 1.948122338366241,
      "grad_norm": 12.911508560180664,
      "learning_rate": 8.946530735148621e-06,
      "loss": 0.8248,
      "step": 5032
    },
    {
      "epoch": 1.948509485094851,
      "grad_norm": 9.948476791381836,
      "learning_rate": 8.946100572116833e-06,
      "loss": 0.4315,
      "step": 5033
    },
    {
      "epoch": 1.948896631823461,
      "grad_norm": 30.982824325561523,
      "learning_rate": 8.945670409085044e-06,
      "loss": 2.4275,
      "step": 5034
    },
    {
      "epoch": 1.9492837785520711,
      "grad_norm": 13.27056884765625,
      "learning_rate": 8.945240246053256e-06,
      "loss": 1.3572,
      "step": 5035
    },
    {
      "epoch": 1.9496709252806814,
      "grad_norm": 20.373085021972656,
      "learning_rate": 8.944810083021465e-06,
      "loss": 3.6855,
      "step": 5036
    },
    {
      "epoch": 1.9500580720092915,
      "grad_norm": 18.303537368774414,
      "learning_rate": 8.944379919989677e-06,
      "loss": 1.5146,
      "step": 5037
    },
    {
      "epoch": 1.9504452187379018,
      "grad_norm": 12.003211975097656,
      "learning_rate": 8.943949756957888e-06,
      "loss": 1.088,
      "step": 5038
    },
    {
      "epoch": 1.9508323654665118,
      "grad_norm": 28.192487716674805,
      "learning_rate": 8.9435195939261e-06,
      "loss": 2.0845,
      "step": 5039
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 30.38310432434082,
      "learning_rate": 8.94308943089431e-06,
      "loss": 1.9182,
      "step": 5040
    },
    {
      "epoch": 1.951606658923732,
      "grad_norm": 22.06005096435547,
      "learning_rate": 8.94265926786252e-06,
      "loss": 2.6774,
      "step": 5041
    },
    {
      "epoch": 1.9519938056523423,
      "grad_norm": 20.07892608642578,
      "learning_rate": 8.942229104830732e-06,
      "loss": 2.0473,
      "step": 5042
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 12.130998611450195,
      "learning_rate": 8.941798941798942e-06,
      "loss": 1.14,
      "step": 5043
    },
    {
      "epoch": 1.9527680991095626,
      "grad_norm": 34.77061462402344,
      "learning_rate": 8.941368778767153e-06,
      "loss": 1.8194,
      "step": 5044
    },
    {
      "epoch": 1.9531552458381727,
      "grad_norm": 43.321372985839844,
      "learning_rate": 8.940938615735365e-06,
      "loss": 1.2439,
      "step": 5045
    },
    {
      "epoch": 1.9535423925667827,
      "grad_norm": 30.865442276000977,
      "learning_rate": 8.940508452703576e-06,
      "loss": 1.3808,
      "step": 5046
    },
    {
      "epoch": 1.9539295392953928,
      "grad_norm": 40.06325149536133,
      "learning_rate": 8.940078289671786e-06,
      "loss": 2.1459,
      "step": 5047
    },
    {
      "epoch": 1.954316686024003,
      "grad_norm": 22.136259078979492,
      "learning_rate": 8.939648126639997e-06,
      "loss": 1.33,
      "step": 5048
    },
    {
      "epoch": 1.9547038327526134,
      "grad_norm": 16.23042869567871,
      "learning_rate": 8.939217963608207e-06,
      "loss": 1.6541,
      "step": 5049
    },
    {
      "epoch": 1.9550909794812235,
      "grad_norm": 16.286090850830078,
      "learning_rate": 8.93878780057642e-06,
      "loss": 1.5186,
      "step": 5050
    },
    {
      "epoch": 1.9554781262098335,
      "grad_norm": 36.57447052001953,
      "learning_rate": 8.93835763754463e-06,
      "loss": 1.5664,
      "step": 5051
    },
    {
      "epoch": 1.9558652729384436,
      "grad_norm": 32.15333557128906,
      "learning_rate": 8.937927474512841e-06,
      "loss": 4.6609,
      "step": 5052
    },
    {
      "epoch": 1.9562524196670537,
      "grad_norm": 16.023290634155273,
      "learning_rate": 8.937497311481053e-06,
      "loss": 1.5514,
      "step": 5053
    },
    {
      "epoch": 1.956639566395664,
      "grad_norm": 13.492148399353027,
      "learning_rate": 8.937067148449264e-06,
      "loss": 0.7571,
      "step": 5054
    },
    {
      "epoch": 1.9570267131242742,
      "grad_norm": 19.243133544921875,
      "learning_rate": 8.936636985417474e-06,
      "loss": 1.6833,
      "step": 5055
    },
    {
      "epoch": 1.9574138598528843,
      "grad_norm": 15.968663215637207,
      "learning_rate": 8.936206822385685e-06,
      "loss": 1.066,
      "step": 5056
    },
    {
      "epoch": 1.9578010065814944,
      "grad_norm": 11.537787437438965,
      "learning_rate": 8.935776659353896e-06,
      "loss": 0.6234,
      "step": 5057
    },
    {
      "epoch": 1.9581881533101044,
      "grad_norm": 58.885562896728516,
      "learning_rate": 8.935346496322106e-06,
      "loss": 1.5358,
      "step": 5058
    },
    {
      "epoch": 1.9585753000387147,
      "grad_norm": 19.7783145904541,
      "learning_rate": 8.934916333290318e-06,
      "loss": 2.4503,
      "step": 5059
    },
    {
      "epoch": 1.9589624467673248,
      "grad_norm": 16.37778091430664,
      "learning_rate": 8.934486170258529e-06,
      "loss": 1.5457,
      "step": 5060
    },
    {
      "epoch": 1.959349593495935,
      "grad_norm": 26.414962768554688,
      "learning_rate": 8.93405600722674e-06,
      "loss": 1.5584,
      "step": 5061
    },
    {
      "epoch": 1.9597367402245451,
      "grad_norm": 22.50078010559082,
      "learning_rate": 8.93362584419495e-06,
      "loss": 2.0213,
      "step": 5062
    },
    {
      "epoch": 1.9601238869531552,
      "grad_norm": 42.97148513793945,
      "learning_rate": 8.933195681163162e-06,
      "loss": 1.4458,
      "step": 5063
    },
    {
      "epoch": 1.9605110336817653,
      "grad_norm": 28.216140747070312,
      "learning_rate": 8.932765518131371e-06,
      "loss": 1.4513,
      "step": 5064
    },
    {
      "epoch": 1.9608981804103756,
      "grad_norm": 15.856345176696777,
      "learning_rate": 8.932335355099584e-06,
      "loss": 1.1619,
      "step": 5065
    },
    {
      "epoch": 1.9612853271389856,
      "grad_norm": 20.520109176635742,
      "learning_rate": 8.931905192067794e-06,
      "loss": 1.4508,
      "step": 5066
    },
    {
      "epoch": 1.961672473867596,
      "grad_norm": 10.177332878112793,
      "learning_rate": 8.931475029036006e-06,
      "loss": 1.1992,
      "step": 5067
    },
    {
      "epoch": 1.962059620596206,
      "grad_norm": 28.254623413085938,
      "learning_rate": 8.931044866004215e-06,
      "loss": 1.6819,
      "step": 5068
    },
    {
      "epoch": 1.962446767324816,
      "grad_norm": 30.59616470336914,
      "learning_rate": 8.930614702972428e-06,
      "loss": 1.7784,
      "step": 5069
    },
    {
      "epoch": 1.962833914053426,
      "grad_norm": 14.747424125671387,
      "learning_rate": 8.930184539940638e-06,
      "loss": 0.8766,
      "step": 5070
    },
    {
      "epoch": 1.9632210607820364,
      "grad_norm": 19.92981719970703,
      "learning_rate": 8.92975437690885e-06,
      "loss": 1.2762,
      "step": 5071
    },
    {
      "epoch": 1.9636082075106467,
      "grad_norm": 16.607160568237305,
      "learning_rate": 8.92932421387706e-06,
      "loss": 1.3156,
      "step": 5072
    },
    {
      "epoch": 1.9639953542392568,
      "grad_norm": 25.851394653320312,
      "learning_rate": 8.92889405084527e-06,
      "loss": 1.3573,
      "step": 5073
    },
    {
      "epoch": 1.9643825009678668,
      "grad_norm": 21.419614791870117,
      "learning_rate": 8.928463887813482e-06,
      "loss": 1.7765,
      "step": 5074
    },
    {
      "epoch": 1.9647696476964769,
      "grad_norm": 18.649211883544922,
      "learning_rate": 8.928033724781694e-06,
      "loss": 1.3704,
      "step": 5075
    },
    {
      "epoch": 1.965156794425087,
      "grad_norm": 26.410093307495117,
      "learning_rate": 8.927603561749903e-06,
      "loss": 1.4014,
      "step": 5076
    },
    {
      "epoch": 1.9655439411536972,
      "grad_norm": 66.94514465332031,
      "learning_rate": 8.927173398718115e-06,
      "loss": 2.7198,
      "step": 5077
    },
    {
      "epoch": 1.9659310878823075,
      "grad_norm": 16.62771224975586,
      "learning_rate": 8.926743235686326e-06,
      "loss": 1.4551,
      "step": 5078
    },
    {
      "epoch": 1.9663182346109176,
      "grad_norm": 33.97562789916992,
      "learning_rate": 8.926313072654536e-06,
      "loss": 1.5145,
      "step": 5079
    },
    {
      "epoch": 1.9667053813395277,
      "grad_norm": 20.423072814941406,
      "learning_rate": 8.925882909622747e-06,
      "loss": 1.6092,
      "step": 5080
    },
    {
      "epoch": 1.9670925280681377,
      "grad_norm": 16.590843200683594,
      "learning_rate": 8.925452746590959e-06,
      "loss": 1.6157,
      "step": 5081
    },
    {
      "epoch": 1.967479674796748,
      "grad_norm": 13.103160858154297,
      "learning_rate": 8.92502258355917e-06,
      "loss": 0.8734,
      "step": 5082
    },
    {
      "epoch": 1.967866821525358,
      "grad_norm": 30.16637420654297,
      "learning_rate": 8.92459242052738e-06,
      "loss": 2.2792,
      "step": 5083
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 29.49611473083496,
      "learning_rate": 8.924162257495591e-06,
      "loss": 1.5791,
      "step": 5084
    },
    {
      "epoch": 1.9686411149825784,
      "grad_norm": 28.307647705078125,
      "learning_rate": 8.923732094463803e-06,
      "loss": 2.0614,
      "step": 5085
    },
    {
      "epoch": 1.9690282617111885,
      "grad_norm": 19.164813995361328,
      "learning_rate": 8.923301931432014e-06,
      "loss": 1.3219,
      "step": 5086
    },
    {
      "epoch": 1.9694154084397986,
      "grad_norm": 12.012247085571289,
      "learning_rate": 8.922871768400224e-06,
      "loss": 0.6395,
      "step": 5087
    },
    {
      "epoch": 1.9698025551684089,
      "grad_norm": 21.6662540435791,
      "learning_rate": 8.922441605368435e-06,
      "loss": 1.4938,
      "step": 5088
    },
    {
      "epoch": 1.970189701897019,
      "grad_norm": 14.841596603393555,
      "learning_rate": 8.922011442336647e-06,
      "loss": 0.8594,
      "step": 5089
    },
    {
      "epoch": 1.9705768486256292,
      "grad_norm": 24.2488956451416,
      "learning_rate": 8.921581279304858e-06,
      "loss": 1.5893,
      "step": 5090
    },
    {
      "epoch": 1.9709639953542393,
      "grad_norm": 24.07193946838379,
      "learning_rate": 8.921151116273068e-06,
      "loss": 1.7391,
      "step": 5091
    },
    {
      "epoch": 1.9713511420828493,
      "grad_norm": 14.020094871520996,
      "learning_rate": 8.920720953241279e-06,
      "loss": 1.2958,
      "step": 5092
    },
    {
      "epoch": 1.9717382888114594,
      "grad_norm": 21.800079345703125,
      "learning_rate": 8.92029079020949e-06,
      "loss": 1.5865,
      "step": 5093
    },
    {
      "epoch": 1.9721254355400697,
      "grad_norm": 17.113872528076172,
      "learning_rate": 8.9198606271777e-06,
      "loss": 1.5815,
      "step": 5094
    },
    {
      "epoch": 1.97251258226868,
      "grad_norm": 32.22504806518555,
      "learning_rate": 8.919430464145912e-06,
      "loss": 1.7309,
      "step": 5095
    },
    {
      "epoch": 1.97289972899729,
      "grad_norm": 22.034461975097656,
      "learning_rate": 8.919000301114123e-06,
      "loss": 2.0597,
      "step": 5096
    },
    {
      "epoch": 1.9732868757259001,
      "grad_norm": 15.282878875732422,
      "learning_rate": 8.918570138082334e-06,
      "loss": 1.0442,
      "step": 5097
    },
    {
      "epoch": 1.9736740224545102,
      "grad_norm": 24.93488311767578,
      "learning_rate": 8.918139975050544e-06,
      "loss": 1.3731,
      "step": 5098
    },
    {
      "epoch": 1.9740611691831202,
      "grad_norm": 23.699443817138672,
      "learning_rate": 8.917709812018756e-06,
      "loss": 1.5669,
      "step": 5099
    },
    {
      "epoch": 1.9744483159117305,
      "grad_norm": 16.005857467651367,
      "learning_rate": 8.917279648986967e-06,
      "loss": 1.1406,
      "step": 5100
    },
    {
      "epoch": 1.9748354626403408,
      "grad_norm": 17.878875732421875,
      "learning_rate": 8.916849485955178e-06,
      "loss": 1.6122,
      "step": 5101
    },
    {
      "epoch": 1.9752226093689509,
      "grad_norm": 16.089630126953125,
      "learning_rate": 8.916419322923388e-06,
      "loss": 1.7255,
      "step": 5102
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 20.468366622924805,
      "learning_rate": 8.9159891598916e-06,
      "loss": 1.4316,
      "step": 5103
    },
    {
      "epoch": 1.975996902826171,
      "grad_norm": 17.24050521850586,
      "learning_rate": 8.915558996859811e-06,
      "loss": 1.0907,
      "step": 5104
    },
    {
      "epoch": 1.9763840495547813,
      "grad_norm": 25.300413131713867,
      "learning_rate": 8.915128833828022e-06,
      "loss": 1.7965,
      "step": 5105
    },
    {
      "epoch": 1.9767711962833914,
      "grad_norm": 20.702165603637695,
      "learning_rate": 8.914698670796232e-06,
      "loss": 0.8733,
      "step": 5106
    },
    {
      "epoch": 1.9771583430120017,
      "grad_norm": 20.634004592895508,
      "learning_rate": 8.914268507764444e-06,
      "loss": 1.8152,
      "step": 5107
    },
    {
      "epoch": 1.9775454897406117,
      "grad_norm": 22.72633934020996,
      "learning_rate": 8.913838344732655e-06,
      "loss": 1.6052,
      "step": 5108
    },
    {
      "epoch": 1.9779326364692218,
      "grad_norm": 26.17214584350586,
      "learning_rate": 8.913408181700865e-06,
      "loss": 1.4701,
      "step": 5109
    },
    {
      "epoch": 1.9783197831978319,
      "grad_norm": 20.926300048828125,
      "learning_rate": 8.912978018669076e-06,
      "loss": 1.139,
      "step": 5110
    },
    {
      "epoch": 1.9787069299264421,
      "grad_norm": 22.57765769958496,
      "learning_rate": 8.912547855637288e-06,
      "loss": 1.8059,
      "step": 5111
    },
    {
      "epoch": 1.9790940766550522,
      "grad_norm": 26.34497833251953,
      "learning_rate": 8.912117692605499e-06,
      "loss": 1.1736,
      "step": 5112
    },
    {
      "epoch": 1.9794812233836625,
      "grad_norm": 25.313993453979492,
      "learning_rate": 8.911687529573709e-06,
      "loss": 1.5926,
      "step": 5113
    },
    {
      "epoch": 1.9798683701122726,
      "grad_norm": 28.109262466430664,
      "learning_rate": 8.91125736654192e-06,
      "loss": 2.5271,
      "step": 5114
    },
    {
      "epoch": 1.9802555168408826,
      "grad_norm": 22.546096801757812,
      "learning_rate": 8.91082720351013e-06,
      "loss": 1.0622,
      "step": 5115
    },
    {
      "epoch": 1.9806426635694927,
      "grad_norm": 14.79718017578125,
      "learning_rate": 8.910397040478343e-06,
      "loss": 1.2346,
      "step": 5116
    },
    {
      "epoch": 1.981029810298103,
      "grad_norm": 16.067081451416016,
      "learning_rate": 8.909966877446553e-06,
      "loss": 1.3985,
      "step": 5117
    },
    {
      "epoch": 1.9814169570267133,
      "grad_norm": 23.512046813964844,
      "learning_rate": 8.909536714414764e-06,
      "loss": 1.4701,
      "step": 5118
    },
    {
      "epoch": 1.9818041037553233,
      "grad_norm": 24.276819229125977,
      "learning_rate": 8.909106551382974e-06,
      "loss": 1.7406,
      "step": 5119
    },
    {
      "epoch": 1.9821912504839334,
      "grad_norm": 20.780975341796875,
      "learning_rate": 8.908676388351187e-06,
      "loss": 1.8246,
      "step": 5120
    },
    {
      "epoch": 1.9825783972125435,
      "grad_norm": 15.267280578613281,
      "learning_rate": 8.908246225319397e-06,
      "loss": 0.9166,
      "step": 5121
    },
    {
      "epoch": 1.9829655439411535,
      "grad_norm": 11.298053741455078,
      "learning_rate": 8.907816062287608e-06,
      "loss": 0.9897,
      "step": 5122
    },
    {
      "epoch": 1.9833526906697638,
      "grad_norm": 16.452783584594727,
      "learning_rate": 8.907385899255818e-06,
      "loss": 1.2999,
      "step": 5123
    },
    {
      "epoch": 1.9837398373983741,
      "grad_norm": 13.438739776611328,
      "learning_rate": 8.90695573622403e-06,
      "loss": 0.7316,
      "step": 5124
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 8.873785972595215,
      "learning_rate": 8.90652557319224e-06,
      "loss": 1.2314,
      "step": 5125
    },
    {
      "epoch": 1.9845141308555942,
      "grad_norm": 23.566085815429688,
      "learning_rate": 8.906095410160452e-06,
      "loss": 1.5802,
      "step": 5126
    },
    {
      "epoch": 1.9849012775842043,
      "grad_norm": 18.50239372253418,
      "learning_rate": 8.905665247128662e-06,
      "loss": 1.4084,
      "step": 5127
    },
    {
      "epoch": 1.9852884243128146,
      "grad_norm": 26.04203987121582,
      "learning_rate": 8.905235084096873e-06,
      "loss": 1.5448,
      "step": 5128
    },
    {
      "epoch": 1.9856755710414247,
      "grad_norm": 17.22743034362793,
      "learning_rate": 8.904804921065085e-06,
      "loss": 3.1625,
      "step": 5129
    },
    {
      "epoch": 1.986062717770035,
      "grad_norm": 15.887223243713379,
      "learning_rate": 8.904374758033294e-06,
      "loss": 1.3405,
      "step": 5130
    },
    {
      "epoch": 1.986449864498645,
      "grad_norm": 13.759778022766113,
      "learning_rate": 8.903944595001506e-06,
      "loss": 0.8945,
      "step": 5131
    },
    {
      "epoch": 1.986837011227255,
      "grad_norm": 11.975772857666016,
      "learning_rate": 8.903514431969717e-06,
      "loss": 2.1105,
      "step": 5132
    },
    {
      "epoch": 1.9872241579558652,
      "grad_norm": 12.834270477294922,
      "learning_rate": 8.903084268937929e-06,
      "loss": 0.7792,
      "step": 5133
    },
    {
      "epoch": 1.9876113046844754,
      "grad_norm": 26.31777000427246,
      "learning_rate": 8.902654105906138e-06,
      "loss": 0.896,
      "step": 5134
    },
    {
      "epoch": 1.9879984514130855,
      "grad_norm": 17.266155242919922,
      "learning_rate": 8.902223942874351e-06,
      "loss": 1.2382,
      "step": 5135
    },
    {
      "epoch": 1.9883855981416958,
      "grad_norm": 25.256160736083984,
      "learning_rate": 8.901793779842561e-06,
      "loss": 1.0057,
      "step": 5136
    },
    {
      "epoch": 1.9887727448703059,
      "grad_norm": 16.617509841918945,
      "learning_rate": 8.901363616810772e-06,
      "loss": 1.5818,
      "step": 5137
    },
    {
      "epoch": 1.989159891598916,
      "grad_norm": 25.78107452392578,
      "learning_rate": 8.900933453778982e-06,
      "loss": 1.4601,
      "step": 5138
    },
    {
      "epoch": 1.989547038327526,
      "grad_norm": 16.603744506835938,
      "learning_rate": 8.900503290747194e-06,
      "loss": 1.503,
      "step": 5139
    },
    {
      "epoch": 1.9899341850561363,
      "grad_norm": 20.366905212402344,
      "learning_rate": 8.900073127715405e-06,
      "loss": 1.5788,
      "step": 5140
    },
    {
      "epoch": 1.9903213317847466,
      "grad_norm": 37.17045593261719,
      "learning_rate": 8.899642964683616e-06,
      "loss": 2.9594,
      "step": 5141
    },
    {
      "epoch": 1.9907084785133566,
      "grad_norm": 35.04768371582031,
      "learning_rate": 8.899212801651826e-06,
      "loss": 1.9838,
      "step": 5142
    },
    {
      "epoch": 1.9910956252419667,
      "grad_norm": 22.63867950439453,
      "learning_rate": 8.898782638620038e-06,
      "loss": 1.6409,
      "step": 5143
    },
    {
      "epoch": 1.9914827719705768,
      "grad_norm": 26.467220306396484,
      "learning_rate": 8.898352475588249e-06,
      "loss": 2.0948,
      "step": 5144
    },
    {
      "epoch": 1.9918699186991868,
      "grad_norm": 15.964762687683105,
      "learning_rate": 8.897922312556459e-06,
      "loss": 1.5777,
      "step": 5145
    },
    {
      "epoch": 1.9922570654277971,
      "grad_norm": 6.411641597747803,
      "learning_rate": 8.89749214952467e-06,
      "loss": 0.4026,
      "step": 5146
    },
    {
      "epoch": 1.9926442121564074,
      "grad_norm": 11.948575973510742,
      "learning_rate": 8.897061986492882e-06,
      "loss": 1.0605,
      "step": 5147
    },
    {
      "epoch": 1.9930313588850175,
      "grad_norm": 16.324018478393555,
      "learning_rate": 8.896631823461093e-06,
      "loss": 1.6197,
      "step": 5148
    },
    {
      "epoch": 1.9934185056136275,
      "grad_norm": 34.89543533325195,
      "learning_rate": 8.896201660429303e-06,
      "loss": 1.1257,
      "step": 5149
    },
    {
      "epoch": 1.9938056523422376,
      "grad_norm": 13.395251274108887,
      "learning_rate": 8.895771497397514e-06,
      "loss": 1.5348,
      "step": 5150
    },
    {
      "epoch": 1.994192799070848,
      "grad_norm": 23.23766326904297,
      "learning_rate": 8.895341334365726e-06,
      "loss": 1.3587,
      "step": 5151
    },
    {
      "epoch": 1.994579945799458,
      "grad_norm": 22.807100296020508,
      "learning_rate": 8.894911171333937e-06,
      "loss": 1.5504,
      "step": 5152
    },
    {
      "epoch": 1.9949670925280683,
      "grad_norm": 18.683162689208984,
      "learning_rate": 8.894481008302147e-06,
      "loss": 1.7337,
      "step": 5153
    },
    {
      "epoch": 1.9953542392566783,
      "grad_norm": 15.663130760192871,
      "learning_rate": 8.894050845270358e-06,
      "loss": 1.2712,
      "step": 5154
    },
    {
      "epoch": 1.9957413859852884,
      "grad_norm": 12.65172004699707,
      "learning_rate": 8.89362068223857e-06,
      "loss": 0.8572,
      "step": 5155
    },
    {
      "epoch": 1.9961285327138985,
      "grad_norm": 22.445858001708984,
      "learning_rate": 8.893190519206781e-06,
      "loss": 1.5504,
      "step": 5156
    },
    {
      "epoch": 1.9965156794425087,
      "grad_norm": 18.06094741821289,
      "learning_rate": 8.89276035617499e-06,
      "loss": 1.2677,
      "step": 5157
    },
    {
      "epoch": 1.9969028261711188,
      "grad_norm": 12.256243705749512,
      "learning_rate": 8.892330193143202e-06,
      "loss": 0.6064,
      "step": 5158
    },
    {
      "epoch": 1.997289972899729,
      "grad_norm": 16.136953353881836,
      "learning_rate": 8.891900030111413e-06,
      "loss": 0.8408,
      "step": 5159
    },
    {
      "epoch": 1.9976771196283392,
      "grad_norm": 34.73657989501953,
      "learning_rate": 8.891469867079623e-06,
      "loss": 1.4119,
      "step": 5160
    },
    {
      "epoch": 1.9980642663569492,
      "grad_norm": 25.083860397338867,
      "learning_rate": 8.891039704047835e-06,
      "loss": 3.11,
      "step": 5161
    },
    {
      "epoch": 1.9984514130855593,
      "grad_norm": 12.184844017028809,
      "learning_rate": 8.890609541016046e-06,
      "loss": 0.7453,
      "step": 5162
    },
    {
      "epoch": 1.9988385598141696,
      "grad_norm": 38.69596481323242,
      "learning_rate": 8.890179377984257e-06,
      "loss": 1.9925,
      "step": 5163
    },
    {
      "epoch": 1.9992257065427799,
      "grad_norm": 7.004556179046631,
      "learning_rate": 8.889749214952467e-06,
      "loss": 0.4341,
      "step": 5164
    },
    {
      "epoch": 1.99961285327139,
      "grad_norm": 18.260684967041016,
      "learning_rate": 8.889319051920679e-06,
      "loss": 1.5575,
      "step": 5165
    },
    {
      "epoch": 2.0,
      "grad_norm": 37.05977249145508,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.3387,
      "step": 5166
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.4179245283018868,
      "eval_f1": 0.3349792039497048,
      "eval_loss": 1.5170574188232422,
      "eval_runtime": 384.1361,
      "eval_samples_per_second": 2.759,
      "eval_steps_per_second": 1.38,
      "step": 5166
    },
    {
      "epoch": 2.00038714672861,
      "grad_norm": 12.469918251037598,
      "learning_rate": 8.888458725857101e-06,
      "loss": 0.8011,
      "step": 5167
    },
    {
      "epoch": 2.00077429345722,
      "grad_norm": 15.270923614501953,
      "learning_rate": 8.888028562825311e-06,
      "loss": 1.6286,
      "step": 5168
    },
    {
      "epoch": 2.0011614401858306,
      "grad_norm": 12.142326354980469,
      "learning_rate": 8.887598399793523e-06,
      "loss": 0.9793,
      "step": 5169
    },
    {
      "epoch": 2.0015485869144407,
      "grad_norm": 23.50999641418457,
      "learning_rate": 8.887168236761732e-06,
      "loss": 1.6186,
      "step": 5170
    },
    {
      "epoch": 2.0019357336430508,
      "grad_norm": 40.09588623046875,
      "learning_rate": 8.886738073729945e-06,
      "loss": 1.1431,
      "step": 5171
    },
    {
      "epoch": 2.002322880371661,
      "grad_norm": 16.982465744018555,
      "learning_rate": 8.886307910698155e-06,
      "loss": 0.9652,
      "step": 5172
    },
    {
      "epoch": 2.002710027100271,
      "grad_norm": 23.12416648864746,
      "learning_rate": 8.885877747666367e-06,
      "loss": 1.3357,
      "step": 5173
    },
    {
      "epoch": 2.003097173828881,
      "grad_norm": 11.033178329467773,
      "learning_rate": 8.885447584634576e-06,
      "loss": 0.571,
      "step": 5174
    },
    {
      "epoch": 2.0034843205574915,
      "grad_norm": 25.256996154785156,
      "learning_rate": 8.885017421602788e-06,
      "loss": 1.1852,
      "step": 5175
    },
    {
      "epoch": 2.0038714672861015,
      "grad_norm": 15.734246253967285,
      "learning_rate": 8.884587258570999e-06,
      "loss": 0.5739,
      "step": 5176
    },
    {
      "epoch": 2.0042586140147116,
      "grad_norm": 20.253589630126953,
      "learning_rate": 8.88415709553921e-06,
      "loss": 1.7766,
      "step": 5177
    },
    {
      "epoch": 2.0046457607433217,
      "grad_norm": 13.455538749694824,
      "learning_rate": 8.883726932507422e-06,
      "loss": 0.8131,
      "step": 5178
    },
    {
      "epoch": 2.0050329074719317,
      "grad_norm": 13.075383186340332,
      "learning_rate": 8.883296769475632e-06,
      "loss": 0.8323,
      "step": 5179
    },
    {
      "epoch": 2.005420054200542,
      "grad_norm": 28.98955535888672,
      "learning_rate": 8.882866606443843e-06,
      "loss": 1.9334,
      "step": 5180
    },
    {
      "epoch": 2.0058072009291523,
      "grad_norm": 21.13695526123047,
      "learning_rate": 8.882436443412053e-06,
      "loss": 1.7823,
      "step": 5181
    },
    {
      "epoch": 2.0061943476577624,
      "grad_norm": 13.51959228515625,
      "learning_rate": 8.882006280380266e-06,
      "loss": 0.7897,
      "step": 5182
    },
    {
      "epoch": 2.0065814943863725,
      "grad_norm": 18.832542419433594,
      "learning_rate": 8.881576117348476e-06,
      "loss": 1.8319,
      "step": 5183
    },
    {
      "epoch": 2.0069686411149825,
      "grad_norm": 6.642711162567139,
      "learning_rate": 8.881145954316687e-06,
      "loss": 0.4273,
      "step": 5184
    },
    {
      "epoch": 2.0073557878435926,
      "grad_norm": 42.16477966308594,
      "learning_rate": 8.880715791284897e-06,
      "loss": 2.1973,
      "step": 5185
    },
    {
      "epoch": 2.0077429345722027,
      "grad_norm": 31.73929214477539,
      "learning_rate": 8.88028562825311e-06,
      "loss": 2.7788,
      "step": 5186
    },
    {
      "epoch": 2.008130081300813,
      "grad_norm": 16.267333984375,
      "learning_rate": 8.87985546522132e-06,
      "loss": 1.5236,
      "step": 5187
    },
    {
      "epoch": 2.0085172280294232,
      "grad_norm": 11.661026954650879,
      "learning_rate": 8.879425302189531e-06,
      "loss": 0.5422,
      "step": 5188
    },
    {
      "epoch": 2.0089043747580333,
      "grad_norm": 19.39784812927246,
      "learning_rate": 8.87899513915774e-06,
      "loss": 1.4066,
      "step": 5189
    },
    {
      "epoch": 2.0092915214866434,
      "grad_norm": 15.351884841918945,
      "learning_rate": 8.878564976125952e-06,
      "loss": 1.2843,
      "step": 5190
    },
    {
      "epoch": 2.0096786682152534,
      "grad_norm": 23.648836135864258,
      "learning_rate": 8.878134813094164e-06,
      "loss": 1.9406,
      "step": 5191
    },
    {
      "epoch": 2.010065814943864,
      "grad_norm": 29.18762969970703,
      "learning_rate": 8.877704650062375e-06,
      "loss": 1.7413,
      "step": 5192
    },
    {
      "epoch": 2.010452961672474,
      "grad_norm": 15.705753326416016,
      "learning_rate": 8.877274487030585e-06,
      "loss": 1.2069,
      "step": 5193
    },
    {
      "epoch": 2.010840108401084,
      "grad_norm": 23.700441360473633,
      "learning_rate": 8.876844323998796e-06,
      "loss": 1.7228,
      "step": 5194
    },
    {
      "epoch": 2.011227255129694,
      "grad_norm": 22.387317657470703,
      "learning_rate": 8.876414160967007e-06,
      "loss": 1.5486,
      "step": 5195
    },
    {
      "epoch": 2.011614401858304,
      "grad_norm": 28.559335708618164,
      "learning_rate": 8.875983997935217e-06,
      "loss": 1.1091,
      "step": 5196
    },
    {
      "epoch": 2.0120015485869143,
      "grad_norm": 34.89916229248047,
      "learning_rate": 8.875553834903429e-06,
      "loss": 2.1615,
      "step": 5197
    },
    {
      "epoch": 2.012388695315525,
      "grad_norm": 14.466558456420898,
      "learning_rate": 8.87512367187164e-06,
      "loss": 1.073,
      "step": 5198
    },
    {
      "epoch": 2.012775842044135,
      "grad_norm": 32.409915924072266,
      "learning_rate": 8.874693508839851e-06,
      "loss": 1.9611,
      "step": 5199
    },
    {
      "epoch": 2.013162988772745,
      "grad_norm": 17.05964469909668,
      "learning_rate": 8.874263345808061e-06,
      "loss": 1.5679,
      "step": 5200
    },
    {
      "epoch": 2.013550135501355,
      "grad_norm": 12.140432357788086,
      "learning_rate": 8.873833182776273e-06,
      "loss": 0.8733,
      "step": 5201
    },
    {
      "epoch": 2.013937282229965,
      "grad_norm": 7.121780872344971,
      "learning_rate": 8.873403019744484e-06,
      "loss": 0.3687,
      "step": 5202
    },
    {
      "epoch": 2.014324428958575,
      "grad_norm": 21.0531005859375,
      "learning_rate": 8.872972856712695e-06,
      "loss": 1.6618,
      "step": 5203
    },
    {
      "epoch": 2.0147115756871856,
      "grad_norm": 25.946449279785156,
      "learning_rate": 8.872542693680905e-06,
      "loss": 1.3983,
      "step": 5204
    },
    {
      "epoch": 2.0150987224157957,
      "grad_norm": 14.27526569366455,
      "learning_rate": 8.872112530649117e-06,
      "loss": 1.4902,
      "step": 5205
    },
    {
      "epoch": 2.0154858691444058,
      "grad_norm": 15.478309631347656,
      "learning_rate": 8.871682367617328e-06,
      "loss": 0.8597,
      "step": 5206
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 21.079429626464844,
      "learning_rate": 8.87125220458554e-06,
      "loss": 0.9209,
      "step": 5207
    },
    {
      "epoch": 2.016260162601626,
      "grad_norm": 40.21015167236328,
      "learning_rate": 8.870822041553749e-06,
      "loss": 0.9496,
      "step": 5208
    },
    {
      "epoch": 2.016647309330236,
      "grad_norm": 24.85200309753418,
      "learning_rate": 8.87039187852196e-06,
      "loss": 1.5203,
      "step": 5209
    },
    {
      "epoch": 2.0170344560588465,
      "grad_norm": 16.508127212524414,
      "learning_rate": 8.869961715490172e-06,
      "loss": 0.9379,
      "step": 5210
    },
    {
      "epoch": 2.0174216027874565,
      "grad_norm": 20.369144439697266,
      "learning_rate": 8.869531552458382e-06,
      "loss": 3.2077,
      "step": 5211
    },
    {
      "epoch": 2.0178087495160666,
      "grad_norm": 30.41213607788086,
      "learning_rate": 8.869101389426593e-06,
      "loss": 1.3691,
      "step": 5212
    },
    {
      "epoch": 2.0181958962446767,
      "grad_norm": 11.665181159973145,
      "learning_rate": 8.868671226394805e-06,
      "loss": 0.6226,
      "step": 5213
    },
    {
      "epoch": 2.0185830429732867,
      "grad_norm": 37.52671432495117,
      "learning_rate": 8.868241063363016e-06,
      "loss": 2.7684,
      "step": 5214
    },
    {
      "epoch": 2.0189701897018972,
      "grad_norm": 31.01368522644043,
      "learning_rate": 8.867810900331226e-06,
      "loss": 1.307,
      "step": 5215
    },
    {
      "epoch": 2.0193573364305073,
      "grad_norm": 27.4733943939209,
      "learning_rate": 8.867380737299437e-06,
      "loss": 1.6884,
      "step": 5216
    },
    {
      "epoch": 2.0197444831591174,
      "grad_norm": 16.41004753112793,
      "learning_rate": 8.866950574267648e-06,
      "loss": 1.7154,
      "step": 5217
    },
    {
      "epoch": 2.0201316298877274,
      "grad_norm": 28.92599105834961,
      "learning_rate": 8.86652041123586e-06,
      "loss": 1.398,
      "step": 5218
    },
    {
      "epoch": 2.0205187766163375,
      "grad_norm": 13.447602272033691,
      "learning_rate": 8.86609024820407e-06,
      "loss": 0.932,
      "step": 5219
    },
    {
      "epoch": 2.0209059233449476,
      "grad_norm": 23.14986801147461,
      "learning_rate": 8.865660085172281e-06,
      "loss": 2.5602,
      "step": 5220
    },
    {
      "epoch": 2.021293070073558,
      "grad_norm": 25.622880935668945,
      "learning_rate": 8.865229922140492e-06,
      "loss": 1.7698,
      "step": 5221
    },
    {
      "epoch": 2.021680216802168,
      "grad_norm": 9.672566413879395,
      "learning_rate": 8.864799759108704e-06,
      "loss": 0.5463,
      "step": 5222
    },
    {
      "epoch": 2.022067363530778,
      "grad_norm": 21.62165069580078,
      "learning_rate": 8.864369596076914e-06,
      "loss": 1.8564,
      "step": 5223
    },
    {
      "epoch": 2.0224545102593883,
      "grad_norm": 20.5743350982666,
      "learning_rate": 8.863939433045125e-06,
      "loss": 1.9382,
      "step": 5224
    },
    {
      "epoch": 2.0228416569879983,
      "grad_norm": 27.3149471282959,
      "learning_rate": 8.863509270013336e-06,
      "loss": 3.4029,
      "step": 5225
    },
    {
      "epoch": 2.0232288037166084,
      "grad_norm": 28.99734878540039,
      "learning_rate": 8.863079106981546e-06,
      "loss": 3.1968,
      "step": 5226
    },
    {
      "epoch": 2.023615950445219,
      "grad_norm": 19.040557861328125,
      "learning_rate": 8.862648943949758e-06,
      "loss": 1.1729,
      "step": 5227
    },
    {
      "epoch": 2.024003097173829,
      "grad_norm": 27.671955108642578,
      "learning_rate": 8.862218780917969e-06,
      "loss": 2.7004,
      "step": 5228
    },
    {
      "epoch": 2.024390243902439,
      "grad_norm": 28.52119255065918,
      "learning_rate": 8.86178861788618e-06,
      "loss": 1.0695,
      "step": 5229
    },
    {
      "epoch": 2.024777390631049,
      "grad_norm": 29.859792709350586,
      "learning_rate": 8.86135845485439e-06,
      "loss": 2.1697,
      "step": 5230
    },
    {
      "epoch": 2.025164537359659,
      "grad_norm": 25.07796859741211,
      "learning_rate": 8.860928291822602e-06,
      "loss": 1.0318,
      "step": 5231
    },
    {
      "epoch": 2.0255516840882692,
      "grad_norm": 28.785364151000977,
      "learning_rate": 8.860498128790811e-06,
      "loss": 2.7063,
      "step": 5232
    },
    {
      "epoch": 2.0259388308168798,
      "grad_norm": 18.434234619140625,
      "learning_rate": 8.860067965759024e-06,
      "loss": 1.5911,
      "step": 5233
    },
    {
      "epoch": 2.02632597754549,
      "grad_norm": 24.460533142089844,
      "learning_rate": 8.859637802727234e-06,
      "loss": 0.9435,
      "step": 5234
    },
    {
      "epoch": 2.0267131242741,
      "grad_norm": 15.837151527404785,
      "learning_rate": 8.859207639695445e-06,
      "loss": 1.1822,
      "step": 5235
    },
    {
      "epoch": 2.02710027100271,
      "grad_norm": 18.78538703918457,
      "learning_rate": 8.858777476663655e-06,
      "loss": 0.8724,
      "step": 5236
    },
    {
      "epoch": 2.02748741773132,
      "grad_norm": 16.222400665283203,
      "learning_rate": 8.858347313631868e-06,
      "loss": 1.4566,
      "step": 5237
    },
    {
      "epoch": 2.0278745644599305,
      "grad_norm": 17.231895446777344,
      "learning_rate": 8.857917150600078e-06,
      "loss": 1.4938,
      "step": 5238
    },
    {
      "epoch": 2.0282617111885406,
      "grad_norm": 16.9628849029541,
      "learning_rate": 8.85748698756829e-06,
      "loss": 1.593,
      "step": 5239
    },
    {
      "epoch": 2.0286488579171507,
      "grad_norm": 14.213948249816895,
      "learning_rate": 8.8570568245365e-06,
      "loss": 0.8176,
      "step": 5240
    },
    {
      "epoch": 2.0290360046457607,
      "grad_norm": 29.42449951171875,
      "learning_rate": 8.85662666150471e-06,
      "loss": 1.3464,
      "step": 5241
    },
    {
      "epoch": 2.029423151374371,
      "grad_norm": 28.838367462158203,
      "learning_rate": 8.856196498472922e-06,
      "loss": 1.5782,
      "step": 5242
    },
    {
      "epoch": 2.029810298102981,
      "grad_norm": 22.399494171142578,
      "learning_rate": 8.855766335441133e-06,
      "loss": 1.6843,
      "step": 5243
    },
    {
      "epoch": 2.0301974448315914,
      "grad_norm": 43.45531463623047,
      "learning_rate": 8.855336172409343e-06,
      "loss": 1.2747,
      "step": 5244
    },
    {
      "epoch": 2.0305845915602014,
      "grad_norm": 17.570837020874023,
      "learning_rate": 8.854906009377555e-06,
      "loss": 1.5309,
      "step": 5245
    },
    {
      "epoch": 2.0309717382888115,
      "grad_norm": 23.029436111450195,
      "learning_rate": 8.854475846345766e-06,
      "loss": 2.0388,
      "step": 5246
    },
    {
      "epoch": 2.0313588850174216,
      "grad_norm": 15.179142951965332,
      "learning_rate": 8.854045683313976e-06,
      "loss": 1.8044,
      "step": 5247
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 26.90972137451172,
      "learning_rate": 8.853615520282187e-06,
      "loss": 1.2685,
      "step": 5248
    },
    {
      "epoch": 2.0321331784746417,
      "grad_norm": 26.506975173950195,
      "learning_rate": 8.853185357250399e-06,
      "loss": 1.3366,
      "step": 5249
    },
    {
      "epoch": 2.032520325203252,
      "grad_norm": 15.991565704345703,
      "learning_rate": 8.85275519421861e-06,
      "loss": 0.9906,
      "step": 5250
    },
    {
      "epoch": 2.0329074719318623,
      "grad_norm": 24.687143325805664,
      "learning_rate": 8.85232503118682e-06,
      "loss": 1.6006,
      "step": 5251
    },
    {
      "epoch": 2.0332946186604723,
      "grad_norm": 15.640265464782715,
      "learning_rate": 8.851894868155031e-06,
      "loss": 1.0542,
      "step": 5252
    },
    {
      "epoch": 2.0336817653890824,
      "grad_norm": 25.983394622802734,
      "learning_rate": 8.851464705123243e-06,
      "loss": 1.4572,
      "step": 5253
    },
    {
      "epoch": 2.0340689121176925,
      "grad_norm": 13.480133056640625,
      "learning_rate": 8.851034542091454e-06,
      "loss": 0.4417,
      "step": 5254
    },
    {
      "epoch": 2.0344560588463025,
      "grad_norm": 69.73641967773438,
      "learning_rate": 8.850604379059664e-06,
      "loss": 2.349,
      "step": 5255
    },
    {
      "epoch": 2.034843205574913,
      "grad_norm": 31.301340103149414,
      "learning_rate": 8.850174216027875e-06,
      "loss": 1.8991,
      "step": 5256
    },
    {
      "epoch": 2.035230352303523,
      "grad_norm": 35.097774505615234,
      "learning_rate": 8.849744052996086e-06,
      "loss": 1.7403,
      "step": 5257
    },
    {
      "epoch": 2.035617499032133,
      "grad_norm": 20.318090438842773,
      "learning_rate": 8.849313889964298e-06,
      "loss": 1.791,
      "step": 5258
    },
    {
      "epoch": 2.0360046457607432,
      "grad_norm": 23.41549301147461,
      "learning_rate": 8.848883726932508e-06,
      "loss": 1.1889,
      "step": 5259
    },
    {
      "epoch": 2.0363917924893533,
      "grad_norm": 17.34352684020996,
      "learning_rate": 8.848453563900719e-06,
      "loss": 1.4619,
      "step": 5260
    },
    {
      "epoch": 2.036778939217964,
      "grad_norm": 15.91549301147461,
      "learning_rate": 8.84802340086893e-06,
      "loss": 0.9968,
      "step": 5261
    },
    {
      "epoch": 2.037166085946574,
      "grad_norm": 9.992873191833496,
      "learning_rate": 8.84759323783714e-06,
      "loss": 1.3519,
      "step": 5262
    },
    {
      "epoch": 2.037553232675184,
      "grad_norm": 24.179229736328125,
      "learning_rate": 8.847163074805352e-06,
      "loss": 1.6138,
      "step": 5263
    },
    {
      "epoch": 2.037940379403794,
      "grad_norm": 16.556427001953125,
      "learning_rate": 8.846732911773563e-06,
      "loss": 1.6065,
      "step": 5264
    },
    {
      "epoch": 2.038327526132404,
      "grad_norm": 33.76530838012695,
      "learning_rate": 8.846302748741774e-06,
      "loss": 3.2262,
      "step": 5265
    },
    {
      "epoch": 2.038714672861014,
      "grad_norm": 24.635574340820312,
      "learning_rate": 8.845872585709984e-06,
      "loss": 1.7165,
      "step": 5266
    },
    {
      "epoch": 2.0391018195896247,
      "grad_norm": 21.842308044433594,
      "learning_rate": 8.845442422678196e-06,
      "loss": 2.0045,
      "step": 5267
    },
    {
      "epoch": 2.0394889663182347,
      "grad_norm": 14.315924644470215,
      "learning_rate": 8.845012259646407e-06,
      "loss": 1.1797,
      "step": 5268
    },
    {
      "epoch": 2.039876113046845,
      "grad_norm": 16.7281436920166,
      "learning_rate": 8.844582096614618e-06,
      "loss": 1.6775,
      "step": 5269
    },
    {
      "epoch": 2.040263259775455,
      "grad_norm": 24.599185943603516,
      "learning_rate": 8.844151933582828e-06,
      "loss": 2.3567,
      "step": 5270
    },
    {
      "epoch": 2.040650406504065,
      "grad_norm": 16.640377044677734,
      "learning_rate": 8.84372177055104e-06,
      "loss": 1.1211,
      "step": 5271
    },
    {
      "epoch": 2.041037553232675,
      "grad_norm": 15.511281967163086,
      "learning_rate": 8.843291607519251e-06,
      "loss": 0.6919,
      "step": 5272
    },
    {
      "epoch": 2.0414246999612855,
      "grad_norm": 13.122479438781738,
      "learning_rate": 8.842861444487462e-06,
      "loss": 1.0425,
      "step": 5273
    },
    {
      "epoch": 2.0418118466898956,
      "grad_norm": 15.480423927307129,
      "learning_rate": 8.842431281455672e-06,
      "loss": 1.1877,
      "step": 5274
    },
    {
      "epoch": 2.0421989934185056,
      "grad_norm": 12.7503080368042,
      "learning_rate": 8.842001118423883e-06,
      "loss": 1.6218,
      "step": 5275
    },
    {
      "epoch": 2.0425861401471157,
      "grad_norm": 15.252778053283691,
      "learning_rate": 8.841570955392095e-06,
      "loss": 1.5122,
      "step": 5276
    },
    {
      "epoch": 2.0429732868757258,
      "grad_norm": 19.316408157348633,
      "learning_rate": 8.841140792360305e-06,
      "loss": 0.8272,
      "step": 5277
    },
    {
      "epoch": 2.043360433604336,
      "grad_norm": 14.719864845275879,
      "learning_rate": 8.840710629328516e-06,
      "loss": 1.0139,
      "step": 5278
    },
    {
      "epoch": 2.0437475803329463,
      "grad_norm": 21.59795379638672,
      "learning_rate": 8.840280466296727e-06,
      "loss": 0.9344,
      "step": 5279
    },
    {
      "epoch": 2.0441347270615564,
      "grad_norm": 16.91336441040039,
      "learning_rate": 8.839850303264939e-06,
      "loss": 1.4633,
      "step": 5280
    },
    {
      "epoch": 2.0445218737901665,
      "grad_norm": 15.276266098022461,
      "learning_rate": 8.839420140233149e-06,
      "loss": 1.0767,
      "step": 5281
    },
    {
      "epoch": 2.0449090205187765,
      "grad_norm": 16.984580993652344,
      "learning_rate": 8.83898997720136e-06,
      "loss": 1.4646,
      "step": 5282
    },
    {
      "epoch": 2.0452961672473866,
      "grad_norm": 15.137110710144043,
      "learning_rate": 8.83855981416957e-06,
      "loss": 1.005,
      "step": 5283
    },
    {
      "epoch": 2.045683313975997,
      "grad_norm": 19.60561180114746,
      "learning_rate": 8.838129651137783e-06,
      "loss": 1.8432,
      "step": 5284
    },
    {
      "epoch": 2.046070460704607,
      "grad_norm": 12.362703323364258,
      "learning_rate": 8.837699488105993e-06,
      "loss": 0.6065,
      "step": 5285
    },
    {
      "epoch": 2.0464576074332173,
      "grad_norm": 15.948601722717285,
      "learning_rate": 8.837269325074204e-06,
      "loss": 0.9309,
      "step": 5286
    },
    {
      "epoch": 2.0468447541618273,
      "grad_norm": 13.10516357421875,
      "learning_rate": 8.836839162042414e-06,
      "loss": 1.2524,
      "step": 5287
    },
    {
      "epoch": 2.0472319008904374,
      "grad_norm": 27.152185440063477,
      "learning_rate": 8.836408999010627e-06,
      "loss": 1.706,
      "step": 5288
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 19.120315551757812,
      "learning_rate": 8.835978835978837e-06,
      "loss": 1.7283,
      "step": 5289
    },
    {
      "epoch": 2.048006194347658,
      "grad_norm": 7.99722957611084,
      "learning_rate": 8.835548672947048e-06,
      "loss": 0.5244,
      "step": 5290
    },
    {
      "epoch": 2.048393341076268,
      "grad_norm": 28.609943389892578,
      "learning_rate": 8.835118509915258e-06,
      "loss": 1.5281,
      "step": 5291
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 16.968786239624023,
      "learning_rate": 8.834688346883469e-06,
      "loss": 1.6958,
      "step": 5292
    },
    {
      "epoch": 2.049167634533488,
      "grad_norm": 35.781761169433594,
      "learning_rate": 8.83425818385168e-06,
      "loss": 1.5281,
      "step": 5293
    },
    {
      "epoch": 2.0495547812620982,
      "grad_norm": 12.7510347366333,
      "learning_rate": 8.833828020819892e-06,
      "loss": 0.8221,
      "step": 5294
    },
    {
      "epoch": 2.0499419279907083,
      "grad_norm": 17.562185287475586,
      "learning_rate": 8.833397857788102e-06,
      "loss": 1.1014,
      "step": 5295
    },
    {
      "epoch": 2.050329074719319,
      "grad_norm": 42.74247741699219,
      "learning_rate": 8.832967694756313e-06,
      "loss": 1.4601,
      "step": 5296
    },
    {
      "epoch": 2.050716221447929,
      "grad_norm": 13.476424217224121,
      "learning_rate": 8.832537531724524e-06,
      "loss": 0.8067,
      "step": 5297
    },
    {
      "epoch": 2.051103368176539,
      "grad_norm": 23.96453857421875,
      "learning_rate": 8.832107368692734e-06,
      "loss": 1.4537,
      "step": 5298
    },
    {
      "epoch": 2.051490514905149,
      "grad_norm": 11.443099975585938,
      "learning_rate": 8.831677205660947e-06,
      "loss": 0.3224,
      "step": 5299
    },
    {
      "epoch": 2.051877661633759,
      "grad_norm": 13.938823699951172,
      "learning_rate": 8.831247042629157e-06,
      "loss": 0.9775,
      "step": 5300
    },
    {
      "epoch": 2.052264808362369,
      "grad_norm": 15.540149688720703,
      "learning_rate": 8.830816879597368e-06,
      "loss": 1.7582,
      "step": 5301
    },
    {
      "epoch": 2.0526519550909796,
      "grad_norm": 16.50342559814453,
      "learning_rate": 8.830386716565578e-06,
      "loss": 1.508,
      "step": 5302
    },
    {
      "epoch": 2.0530391018195897,
      "grad_norm": 19.267459869384766,
      "learning_rate": 8.829956553533791e-06,
      "loss": 0.9366,
      "step": 5303
    },
    {
      "epoch": 2.0534262485481998,
      "grad_norm": 13.17273235321045,
      "learning_rate": 8.829526390502001e-06,
      "loss": 1.1847,
      "step": 5304
    },
    {
      "epoch": 2.05381339527681,
      "grad_norm": 12.085047721862793,
      "learning_rate": 8.829096227470212e-06,
      "loss": 0.9541,
      "step": 5305
    },
    {
      "epoch": 2.05420054200542,
      "grad_norm": 46.55402374267578,
      "learning_rate": 8.828666064438422e-06,
      "loss": 1.6834,
      "step": 5306
    },
    {
      "epoch": 2.0545876887340304,
      "grad_norm": 7.320636749267578,
      "learning_rate": 8.828235901406634e-06,
      "loss": 0.4044,
      "step": 5307
    },
    {
      "epoch": 2.0549748354626405,
      "grad_norm": 15.732946395874023,
      "learning_rate": 8.827805738374845e-06,
      "loss": 1.1791,
      "step": 5308
    },
    {
      "epoch": 2.0553619821912505,
      "grad_norm": 88.78375244140625,
      "learning_rate": 8.827375575343056e-06,
      "loss": 2.0438,
      "step": 5309
    },
    {
      "epoch": 2.0557491289198606,
      "grad_norm": 22.289180755615234,
      "learning_rate": 8.826945412311266e-06,
      "loss": 1.5233,
      "step": 5310
    },
    {
      "epoch": 2.0561362756484707,
      "grad_norm": 14.42411994934082,
      "learning_rate": 8.826515249279478e-06,
      "loss": 0.9013,
      "step": 5311
    },
    {
      "epoch": 2.0565234223770807,
      "grad_norm": 33.35084915161133,
      "learning_rate": 8.826085086247689e-06,
      "loss": 2.1586,
      "step": 5312
    },
    {
      "epoch": 2.0569105691056913,
      "grad_norm": 14.871529579162598,
      "learning_rate": 8.825654923215899e-06,
      "loss": 1.1713,
      "step": 5313
    },
    {
      "epoch": 2.0572977158343013,
      "grad_norm": 20.63605499267578,
      "learning_rate": 8.82522476018411e-06,
      "loss": 1.7607,
      "step": 5314
    },
    {
      "epoch": 2.0576848625629114,
      "grad_norm": 17.583789825439453,
      "learning_rate": 8.824794597152321e-06,
      "loss": 1.3471,
      "step": 5315
    },
    {
      "epoch": 2.0580720092915215,
      "grad_norm": 17.984874725341797,
      "learning_rate": 8.824364434120533e-06,
      "loss": 1.7815,
      "step": 5316
    },
    {
      "epoch": 2.0584591560201315,
      "grad_norm": 12.2960844039917,
      "learning_rate": 8.823934271088743e-06,
      "loss": 0.7254,
      "step": 5317
    },
    {
      "epoch": 2.0588463027487416,
      "grad_norm": 25.66693115234375,
      "learning_rate": 8.823504108056954e-06,
      "loss": 1.6096,
      "step": 5318
    },
    {
      "epoch": 2.059233449477352,
      "grad_norm": 24.645099639892578,
      "learning_rate": 8.823073945025165e-06,
      "loss": 1.5546,
      "step": 5319
    },
    {
      "epoch": 2.059620596205962,
      "grad_norm": 8.836212158203125,
      "learning_rate": 8.822643781993377e-06,
      "loss": 1.2162,
      "step": 5320
    },
    {
      "epoch": 2.0600077429345722,
      "grad_norm": 18.655427932739258,
      "learning_rate": 8.822213618961587e-06,
      "loss": 1.6357,
      "step": 5321
    },
    {
      "epoch": 2.0603948896631823,
      "grad_norm": 25.012483596801758,
      "learning_rate": 8.821783455929798e-06,
      "loss": 1.3131,
      "step": 5322
    },
    {
      "epoch": 2.0607820363917924,
      "grad_norm": 33.42828369140625,
      "learning_rate": 8.82135329289801e-06,
      "loss": 1.8525,
      "step": 5323
    },
    {
      "epoch": 2.0611691831204024,
      "grad_norm": 17.906614303588867,
      "learning_rate": 8.82092312986622e-06,
      "loss": 1.8514,
      "step": 5324
    },
    {
      "epoch": 2.061556329849013,
      "grad_norm": 27.293960571289062,
      "learning_rate": 8.82049296683443e-06,
      "loss": 1.1947,
      "step": 5325
    },
    {
      "epoch": 2.061943476577623,
      "grad_norm": 29.357967376708984,
      "learning_rate": 8.820062803802642e-06,
      "loss": 0.6829,
      "step": 5326
    },
    {
      "epoch": 2.062330623306233,
      "grad_norm": 28.224281311035156,
      "learning_rate": 8.819632640770853e-06,
      "loss": 2.4621,
      "step": 5327
    },
    {
      "epoch": 2.062717770034843,
      "grad_norm": 30.114131927490234,
      "learning_rate": 8.819202477739063e-06,
      "loss": 1.732,
      "step": 5328
    },
    {
      "epoch": 2.063104916763453,
      "grad_norm": 27.570446014404297,
      "learning_rate": 8.818772314707275e-06,
      "loss": 2.5895,
      "step": 5329
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 19.58695411682129,
      "learning_rate": 8.818342151675486e-06,
      "loss": 0.9856,
      "step": 5330
    },
    {
      "epoch": 2.0638792102206738,
      "grad_norm": 15.941386222839355,
      "learning_rate": 8.817911988643697e-06,
      "loss": 1.078,
      "step": 5331
    },
    {
      "epoch": 2.064266356949284,
      "grad_norm": 18.70539093017578,
      "learning_rate": 8.817481825611907e-06,
      "loss": 1.6823,
      "step": 5332
    },
    {
      "epoch": 2.064653503677894,
      "grad_norm": 21.345144271850586,
      "learning_rate": 8.817051662580118e-06,
      "loss": 1.6455,
      "step": 5333
    },
    {
      "epoch": 2.065040650406504,
      "grad_norm": 10.794562339782715,
      "learning_rate": 8.816621499548328e-06,
      "loss": 0.6206,
      "step": 5334
    },
    {
      "epoch": 2.065427797135114,
      "grad_norm": 22.119142532348633,
      "learning_rate": 8.816191336516541e-06,
      "loss": 2.1073,
      "step": 5335
    },
    {
      "epoch": 2.0658149438637246,
      "grad_norm": 15.799921989440918,
      "learning_rate": 8.815761173484751e-06,
      "loss": 0.8401,
      "step": 5336
    },
    {
      "epoch": 2.0662020905923346,
      "grad_norm": 22.102846145629883,
      "learning_rate": 8.815331010452962e-06,
      "loss": 1.935,
      "step": 5337
    },
    {
      "epoch": 2.0665892373209447,
      "grad_norm": 66.20753479003906,
      "learning_rate": 8.814900847421172e-06,
      "loss": 1.8996,
      "step": 5338
    },
    {
      "epoch": 2.0669763840495547,
      "grad_norm": 29.53036880493164,
      "learning_rate": 8.814470684389385e-06,
      "loss": 1.8235,
      "step": 5339
    },
    {
      "epoch": 2.067363530778165,
      "grad_norm": 12.932026863098145,
      "learning_rate": 8.814040521357595e-06,
      "loss": 0.7502,
      "step": 5340
    },
    {
      "epoch": 2.067750677506775,
      "grad_norm": 24.596826553344727,
      "learning_rate": 8.813610358325806e-06,
      "loss": 2.6912,
      "step": 5341
    },
    {
      "epoch": 2.0681378242353854,
      "grad_norm": 27.047353744506836,
      "learning_rate": 8.813180195294018e-06,
      "loss": 1.4698,
      "step": 5342
    },
    {
      "epoch": 2.0685249709639955,
      "grad_norm": 4.454951286315918,
      "learning_rate": 8.812750032262228e-06,
      "loss": 0.1496,
      "step": 5343
    },
    {
      "epoch": 2.0689121176926055,
      "grad_norm": 18.89039421081543,
      "learning_rate": 8.812319869230439e-06,
      "loss": 1.8978,
      "step": 5344
    },
    {
      "epoch": 2.0692992644212156,
      "grad_norm": 21.84629249572754,
      "learning_rate": 8.81188970619865e-06,
      "loss": 2.0591,
      "step": 5345
    },
    {
      "epoch": 2.0696864111498257,
      "grad_norm": 14.338268280029297,
      "learning_rate": 8.811459543166862e-06,
      "loss": 0.862,
      "step": 5346
    },
    {
      "epoch": 2.0700735578784357,
      "grad_norm": 15.987637519836426,
      "learning_rate": 8.811029380135072e-06,
      "loss": 1.4864,
      "step": 5347
    },
    {
      "epoch": 2.0704607046070462,
      "grad_norm": 27.519901275634766,
      "learning_rate": 8.810599217103283e-06,
      "loss": 1.6569,
      "step": 5348
    },
    {
      "epoch": 2.0708478513356563,
      "grad_norm": 14.722236633300781,
      "learning_rate": 8.810169054071493e-06,
      "loss": 1.2199,
      "step": 5349
    },
    {
      "epoch": 2.0712349980642664,
      "grad_norm": 8.375170707702637,
      "learning_rate": 8.809738891039706e-06,
      "loss": 0.4259,
      "step": 5350
    },
    {
      "epoch": 2.0716221447928764,
      "grad_norm": 20.073951721191406,
      "learning_rate": 8.809308728007916e-06,
      "loss": 2.1498,
      "step": 5351
    },
    {
      "epoch": 2.0720092915214865,
      "grad_norm": 14.446979522705078,
      "learning_rate": 8.808878564976127e-06,
      "loss": 0.8457,
      "step": 5352
    },
    {
      "epoch": 2.072396438250097,
      "grad_norm": 16.400062561035156,
      "learning_rate": 8.808448401944337e-06,
      "loss": 1.5696,
      "step": 5353
    },
    {
      "epoch": 2.072783584978707,
      "grad_norm": 15.826266288757324,
      "learning_rate": 8.80801823891255e-06,
      "loss": 1.661,
      "step": 5354
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 54.5934944152832,
      "learning_rate": 8.80758807588076e-06,
      "loss": 1.6087,
      "step": 5355
    },
    {
      "epoch": 2.073557878435927,
      "grad_norm": 20.22399139404297,
      "learning_rate": 8.807157912848971e-06,
      "loss": 2.4684,
      "step": 5356
    },
    {
      "epoch": 2.0739450251645373,
      "grad_norm": 22.395002365112305,
      "learning_rate": 8.80672774981718e-06,
      "loss": 2.3117,
      "step": 5357
    },
    {
      "epoch": 2.0743321718931473,
      "grad_norm": 13.073339462280273,
      "learning_rate": 8.806297586785392e-06,
      "loss": 0.7765,
      "step": 5358
    },
    {
      "epoch": 2.074719318621758,
      "grad_norm": 25.7995662689209,
      "learning_rate": 8.805867423753603e-06,
      "loss": 1.419,
      "step": 5359
    },
    {
      "epoch": 2.075106465350368,
      "grad_norm": 15.38979721069336,
      "learning_rate": 8.805437260721815e-06,
      "loss": 1.45,
      "step": 5360
    },
    {
      "epoch": 2.075493612078978,
      "grad_norm": 14.633212089538574,
      "learning_rate": 8.805007097690025e-06,
      "loss": 1.4658,
      "step": 5361
    },
    {
      "epoch": 2.075880758807588,
      "grad_norm": 14.661187171936035,
      "learning_rate": 8.804576934658236e-06,
      "loss": 2.95,
      "step": 5362
    },
    {
      "epoch": 2.076267905536198,
      "grad_norm": 15.903286933898926,
      "learning_rate": 8.804146771626447e-06,
      "loss": 0.9998,
      "step": 5363
    },
    {
      "epoch": 2.076655052264808,
      "grad_norm": 25.328216552734375,
      "learning_rate": 8.803716608594657e-06,
      "loss": 1.0178,
      "step": 5364
    },
    {
      "epoch": 2.0770421989934187,
      "grad_norm": 35.3859977722168,
      "learning_rate": 8.803286445562869e-06,
      "loss": 1.9338,
      "step": 5365
    },
    {
      "epoch": 2.0774293457220288,
      "grad_norm": 44.874420166015625,
      "learning_rate": 8.80285628253108e-06,
      "loss": 2.9736,
      "step": 5366
    },
    {
      "epoch": 2.077816492450639,
      "grad_norm": 18.643808364868164,
      "learning_rate": 8.802426119499291e-06,
      "loss": 1.3888,
      "step": 5367
    },
    {
      "epoch": 2.078203639179249,
      "grad_norm": 5.710572719573975,
      "learning_rate": 8.801995956467501e-06,
      "loss": 0.3334,
      "step": 5368
    },
    {
      "epoch": 2.078590785907859,
      "grad_norm": 16.524402618408203,
      "learning_rate": 8.801565793435713e-06,
      "loss": 0.6874,
      "step": 5369
    },
    {
      "epoch": 2.078977932636469,
      "grad_norm": 11.629517555236816,
      "learning_rate": 8.801135630403924e-06,
      "loss": 0.703,
      "step": 5370
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 15.618741989135742,
      "learning_rate": 8.800705467372135e-06,
      "loss": 1.3525,
      "step": 5371
    },
    {
      "epoch": 2.0797522260936896,
      "grad_norm": 34.45285415649414,
      "learning_rate": 8.800275304340345e-06,
      "loss": 1.8998,
      "step": 5372
    },
    {
      "epoch": 2.0801393728222997,
      "grad_norm": 14.727685928344727,
      "learning_rate": 8.799845141308556e-06,
      "loss": 0.8537,
      "step": 5373
    },
    {
      "epoch": 2.0805265195509097,
      "grad_norm": 44.79212188720703,
      "learning_rate": 8.799414978276768e-06,
      "loss": 0.7053,
      "step": 5374
    },
    {
      "epoch": 2.08091366627952,
      "grad_norm": 12.322593688964844,
      "learning_rate": 8.79898481524498e-06,
      "loss": 0.8106,
      "step": 5375
    },
    {
      "epoch": 2.08130081300813,
      "grad_norm": 18.62436294555664,
      "learning_rate": 8.798554652213189e-06,
      "loss": 2.6262,
      "step": 5376
    },
    {
      "epoch": 2.0816879597367404,
      "grad_norm": 17.907407760620117,
      "learning_rate": 8.7981244891814e-06,
      "loss": 1.2551,
      "step": 5377
    },
    {
      "epoch": 2.0820751064653504,
      "grad_norm": 24.051355361938477,
      "learning_rate": 8.797694326149612e-06,
      "loss": 3.3635,
      "step": 5378
    },
    {
      "epoch": 2.0824622531939605,
      "grad_norm": 21.283578872680664,
      "learning_rate": 8.797264163117822e-06,
      "loss": 2.3844,
      "step": 5379
    },
    {
      "epoch": 2.0828493999225706,
      "grad_norm": 18.678300857543945,
      "learning_rate": 8.796834000086033e-06,
      "loss": 2.202,
      "step": 5380
    },
    {
      "epoch": 2.0832365466511806,
      "grad_norm": 20.5657958984375,
      "learning_rate": 8.796403837054244e-06,
      "loss": 1.6187,
      "step": 5381
    },
    {
      "epoch": 2.083623693379791,
      "grad_norm": 73.9774169921875,
      "learning_rate": 8.795973674022456e-06,
      "loss": 3.226,
      "step": 5382
    },
    {
      "epoch": 2.084010840108401,
      "grad_norm": 14.40211296081543,
      "learning_rate": 8.795543510990666e-06,
      "loss": 0.8074,
      "step": 5383
    },
    {
      "epoch": 2.0843979868370113,
      "grad_norm": 15.099727630615234,
      "learning_rate": 8.795113347958877e-06,
      "loss": 0.6134,
      "step": 5384
    },
    {
      "epoch": 2.0847851335656213,
      "grad_norm": 16.526214599609375,
      "learning_rate": 8.794683184927088e-06,
      "loss": 0.8672,
      "step": 5385
    },
    {
      "epoch": 2.0851722802942314,
      "grad_norm": 13.781281471252441,
      "learning_rate": 8.7942530218953e-06,
      "loss": 0.6713,
      "step": 5386
    },
    {
      "epoch": 2.0855594270228415,
      "grad_norm": 19.45653533935547,
      "learning_rate": 8.79382285886351e-06,
      "loss": 0.8441,
      "step": 5387
    },
    {
      "epoch": 2.085946573751452,
      "grad_norm": 14.004570960998535,
      "learning_rate": 8.793392695831721e-06,
      "loss": 1.4227,
      "step": 5388
    },
    {
      "epoch": 2.086333720480062,
      "grad_norm": 21.63005828857422,
      "learning_rate": 8.792962532799932e-06,
      "loss": 1.9346,
      "step": 5389
    },
    {
      "epoch": 2.086720867208672,
      "grad_norm": 54.84817123413086,
      "learning_rate": 8.792532369768144e-06,
      "loss": 0.8205,
      "step": 5390
    },
    {
      "epoch": 2.087108013937282,
      "grad_norm": 17.265274047851562,
      "learning_rate": 8.792102206736354e-06,
      "loss": 1.5755,
      "step": 5391
    },
    {
      "epoch": 2.0874951606658922,
      "grad_norm": 13.342823028564453,
      "learning_rate": 8.791672043704565e-06,
      "loss": 1.0689,
      "step": 5392
    },
    {
      "epoch": 2.0878823073945023,
      "grad_norm": 15.423653602600098,
      "learning_rate": 8.791241880672776e-06,
      "loss": 1.3051,
      "step": 5393
    },
    {
      "epoch": 2.088269454123113,
      "grad_norm": 18.495588302612305,
      "learning_rate": 8.790811717640986e-06,
      "loss": 1.5523,
      "step": 5394
    },
    {
      "epoch": 2.088656600851723,
      "grad_norm": 21.20388412475586,
      "learning_rate": 8.790381554609197e-06,
      "loss": 1.4601,
      "step": 5395
    },
    {
      "epoch": 2.089043747580333,
      "grad_norm": 22.553314208984375,
      "learning_rate": 8.789951391577409e-06,
      "loss": 1.2859,
      "step": 5396
    },
    {
      "epoch": 2.089430894308943,
      "grad_norm": 25.889089584350586,
      "learning_rate": 8.78952122854562e-06,
      "loss": 2.007,
      "step": 5397
    },
    {
      "epoch": 2.089818041037553,
      "grad_norm": 33.90541458129883,
      "learning_rate": 8.78909106551383e-06,
      "loss": 2.275,
      "step": 5398
    },
    {
      "epoch": 2.0902051877661636,
      "grad_norm": 27.966970443725586,
      "learning_rate": 8.788660902482041e-06,
      "loss": 1.8818,
      "step": 5399
    },
    {
      "epoch": 2.0905923344947737,
      "grad_norm": 11.04706859588623,
      "learning_rate": 8.788230739450251e-06,
      "loss": 0.7209,
      "step": 5400
    },
    {
      "epoch": 2.0909794812233837,
      "grad_norm": 8.585047721862793,
      "learning_rate": 8.787800576418464e-06,
      "loss": 0.2584,
      "step": 5401
    },
    {
      "epoch": 2.091366627951994,
      "grad_norm": 19.9748592376709,
      "learning_rate": 8.787370413386674e-06,
      "loss": 1.6743,
      "step": 5402
    },
    {
      "epoch": 2.091753774680604,
      "grad_norm": 16.9166316986084,
      "learning_rate": 8.786940250354885e-06,
      "loss": 1.2344,
      "step": 5403
    },
    {
      "epoch": 2.092140921409214,
      "grad_norm": 20.011934280395508,
      "learning_rate": 8.786510087323095e-06,
      "loss": 0.678,
      "step": 5404
    },
    {
      "epoch": 2.0925280681378244,
      "grad_norm": 17.061737060546875,
      "learning_rate": 8.786079924291308e-06,
      "loss": 1.5077,
      "step": 5405
    },
    {
      "epoch": 2.0929152148664345,
      "grad_norm": 16.80703353881836,
      "learning_rate": 8.785649761259518e-06,
      "loss": 1.0763,
      "step": 5406
    },
    {
      "epoch": 2.0933023615950446,
      "grad_norm": 11.483606338500977,
      "learning_rate": 8.78521959822773e-06,
      "loss": 0.7078,
      "step": 5407
    },
    {
      "epoch": 2.0936895083236546,
      "grad_norm": 19.668214797973633,
      "learning_rate": 8.784789435195939e-06,
      "loss": 1.6308,
      "step": 5408
    },
    {
      "epoch": 2.0940766550522647,
      "grad_norm": 5.619824409484863,
      "learning_rate": 8.78435927216415e-06,
      "loss": 0.3868,
      "step": 5409
    },
    {
      "epoch": 2.0944638017808748,
      "grad_norm": 13.54746150970459,
      "learning_rate": 8.783929109132362e-06,
      "loss": 0.9687,
      "step": 5410
    },
    {
      "epoch": 2.0948509485094853,
      "grad_norm": 12.335101127624512,
      "learning_rate": 8.783498946100573e-06,
      "loss": 0.8519,
      "step": 5411
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 42.042633056640625,
      "learning_rate": 8.783068783068783e-06,
      "loss": 1.1356,
      "step": 5412
    },
    {
      "epoch": 2.0956252419667054,
      "grad_norm": 41.393798828125,
      "learning_rate": 8.782638620036994e-06,
      "loss": 1.7339,
      "step": 5413
    },
    {
      "epoch": 2.0960123886953155,
      "grad_norm": 51.41362380981445,
      "learning_rate": 8.782208457005206e-06,
      "loss": 3.375,
      "step": 5414
    },
    {
      "epoch": 2.0963995354239255,
      "grad_norm": 35.3565788269043,
      "learning_rate": 8.781778293973416e-06,
      "loss": 1.6827,
      "step": 5415
    },
    {
      "epoch": 2.0967866821525356,
      "grad_norm": 23.36135482788086,
      "learning_rate": 8.781348130941627e-06,
      "loss": 1.138,
      "step": 5416
    },
    {
      "epoch": 2.097173828881146,
      "grad_norm": 19.360137939453125,
      "learning_rate": 8.780917967909838e-06,
      "loss": 2.074,
      "step": 5417
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 16.060028076171875,
      "learning_rate": 8.78048780487805e-06,
      "loss": 1.0176,
      "step": 5418
    },
    {
      "epoch": 2.0979481223383663,
      "grad_norm": 15.319169044494629,
      "learning_rate": 8.78005764184626e-06,
      "loss": 1.3613,
      "step": 5419
    },
    {
      "epoch": 2.0983352690669763,
      "grad_norm": 13.840097427368164,
      "learning_rate": 8.779627478814471e-06,
      "loss": 1.0997,
      "step": 5420
    },
    {
      "epoch": 2.0987224157955864,
      "grad_norm": 8.894158363342285,
      "learning_rate": 8.779197315782682e-06,
      "loss": 0.4358,
      "step": 5421
    },
    {
      "epoch": 2.0991095625241964,
      "grad_norm": 17.758323669433594,
      "learning_rate": 8.778767152750894e-06,
      "loss": 0.8594,
      "step": 5422
    },
    {
      "epoch": 2.099496709252807,
      "grad_norm": 22.658916473388672,
      "learning_rate": 8.778336989719104e-06,
      "loss": 1.9844,
      "step": 5423
    },
    {
      "epoch": 2.099883855981417,
      "grad_norm": 17.941083908081055,
      "learning_rate": 8.777906826687315e-06,
      "loss": 2.1002,
      "step": 5424
    },
    {
      "epoch": 2.100271002710027,
      "grad_norm": 13.935158729553223,
      "learning_rate": 8.777476663655526e-06,
      "loss": 1.3904,
      "step": 5425
    },
    {
      "epoch": 2.100658149438637,
      "grad_norm": 48.92192459106445,
      "learning_rate": 8.777046500623738e-06,
      "loss": 1.8698,
      "step": 5426
    },
    {
      "epoch": 2.1010452961672472,
      "grad_norm": 22.83053207397461,
      "learning_rate": 8.776616337591948e-06,
      "loss": 1.4431,
      "step": 5427
    },
    {
      "epoch": 2.1014324428958577,
      "grad_norm": 13.815629959106445,
      "learning_rate": 8.776186174560159e-06,
      "loss": 0.8598,
      "step": 5428
    },
    {
      "epoch": 2.101819589624468,
      "grad_norm": 27.469825744628906,
      "learning_rate": 8.77575601152837e-06,
      "loss": 2.9488,
      "step": 5429
    },
    {
      "epoch": 2.102206736353078,
      "grad_norm": 17.938079833984375,
      "learning_rate": 8.77532584849658e-06,
      "loss": 1.0738,
      "step": 5430
    },
    {
      "epoch": 2.102593883081688,
      "grad_norm": 47.58565139770508,
      "learning_rate": 8.774895685464791e-06,
      "loss": 1.5236,
      "step": 5431
    },
    {
      "epoch": 2.102981029810298,
      "grad_norm": 59.06420135498047,
      "learning_rate": 8.774465522433003e-06,
      "loss": 1.8426,
      "step": 5432
    },
    {
      "epoch": 2.103368176538908,
      "grad_norm": 11.765705108642578,
      "learning_rate": 8.774035359401214e-06,
      "loss": 1.0819,
      "step": 5433
    },
    {
      "epoch": 2.1037553232675186,
      "grad_norm": 64.98958587646484,
      "learning_rate": 8.773605196369424e-06,
      "loss": 3.0766,
      "step": 5434
    },
    {
      "epoch": 2.1041424699961286,
      "grad_norm": 37.77132797241211,
      "learning_rate": 8.773175033337635e-06,
      "loss": 1.565,
      "step": 5435
    },
    {
      "epoch": 2.1045296167247387,
      "grad_norm": 22.61358070373535,
      "learning_rate": 8.772744870305847e-06,
      "loss": 1.4287,
      "step": 5436
    },
    {
      "epoch": 2.1049167634533488,
      "grad_norm": 29.527027130126953,
      "learning_rate": 8.772314707274058e-06,
      "loss": 3.3448,
      "step": 5437
    },
    {
      "epoch": 2.105303910181959,
      "grad_norm": 26.23744010925293,
      "learning_rate": 8.771884544242268e-06,
      "loss": 1.7201,
      "step": 5438
    },
    {
      "epoch": 2.105691056910569,
      "grad_norm": 13.761069297790527,
      "learning_rate": 8.77145438121048e-06,
      "loss": 0.4405,
      "step": 5439
    },
    {
      "epoch": 2.1060782036391794,
      "grad_norm": 14.765442848205566,
      "learning_rate": 8.771024218178691e-06,
      "loss": 0.8456,
      "step": 5440
    },
    {
      "epoch": 2.1064653503677895,
      "grad_norm": 36.09375,
      "learning_rate": 8.770594055146902e-06,
      "loss": 2.3753,
      "step": 5441
    },
    {
      "epoch": 2.1068524970963995,
      "grad_norm": 15.58292293548584,
      "learning_rate": 8.770163892115112e-06,
      "loss": 1.0133,
      "step": 5442
    },
    {
      "epoch": 2.1072396438250096,
      "grad_norm": 15.964179039001465,
      "learning_rate": 8.769733729083323e-06,
      "loss": 0.9041,
      "step": 5443
    },
    {
      "epoch": 2.1076267905536197,
      "grad_norm": 16.190465927124023,
      "learning_rate": 8.769303566051535e-06,
      "loss": 0.6601,
      "step": 5444
    },
    {
      "epoch": 2.10801393728223,
      "grad_norm": 24.068330764770508,
      "learning_rate": 8.768873403019745e-06,
      "loss": 1.7745,
      "step": 5445
    },
    {
      "epoch": 2.1084010840108403,
      "grad_norm": 21.25668716430664,
      "learning_rate": 8.768443239987956e-06,
      "loss": 2.5358,
      "step": 5446
    },
    {
      "epoch": 2.1087882307394503,
      "grad_norm": 15.601598739624023,
      "learning_rate": 8.768013076956167e-06,
      "loss": 0.9148,
      "step": 5447
    },
    {
      "epoch": 2.1091753774680604,
      "grad_norm": 20.018814086914062,
      "learning_rate": 8.767582913924379e-06,
      "loss": 1.128,
      "step": 5448
    },
    {
      "epoch": 2.1095625241966705,
      "grad_norm": 20.49239158630371,
      "learning_rate": 8.767152750892589e-06,
      "loss": 1.3102,
      "step": 5449
    },
    {
      "epoch": 2.1099496709252805,
      "grad_norm": 22.393020629882812,
      "learning_rate": 8.7667225878608e-06,
      "loss": 1.7171,
      "step": 5450
    },
    {
      "epoch": 2.110336817653891,
      "grad_norm": 11.805240631103516,
      "learning_rate": 8.76629242482901e-06,
      "loss": 0.6894,
      "step": 5451
    },
    {
      "epoch": 2.110723964382501,
      "grad_norm": 13.046709060668945,
      "learning_rate": 8.765862261797223e-06,
      "loss": 1.29,
      "step": 5452
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 28.36259651184082,
      "learning_rate": 8.765432098765432e-06,
      "loss": 1.8487,
      "step": 5453
    },
    {
      "epoch": 2.1114982578397212,
      "grad_norm": 24.32274627685547,
      "learning_rate": 8.765001935733644e-06,
      "loss": 1.5336,
      "step": 5454
    },
    {
      "epoch": 2.1118854045683313,
      "grad_norm": 19.071569442749023,
      "learning_rate": 8.764571772701854e-06,
      "loss": 2.4499,
      "step": 5455
    },
    {
      "epoch": 2.1122725512969414,
      "grad_norm": 31.19804573059082,
      "learning_rate": 8.764141609670067e-06,
      "loss": 1.5142,
      "step": 5456
    },
    {
      "epoch": 2.112659698025552,
      "grad_norm": 35.0242919921875,
      "learning_rate": 8.763711446638276e-06,
      "loss": 1.9195,
      "step": 5457
    },
    {
      "epoch": 2.113046844754162,
      "grad_norm": 12.120471000671387,
      "learning_rate": 8.763281283606488e-06,
      "loss": 0.9384,
      "step": 5458
    },
    {
      "epoch": 2.113433991482772,
      "grad_norm": 22.270343780517578,
      "learning_rate": 8.762851120574698e-06,
      "loss": 1.9563,
      "step": 5459
    },
    {
      "epoch": 2.113821138211382,
      "grad_norm": 14.313211441040039,
      "learning_rate": 8.762420957542909e-06,
      "loss": 1.1841,
      "step": 5460
    },
    {
      "epoch": 2.114208284939992,
      "grad_norm": 24.11211585998535,
      "learning_rate": 8.76199079451112e-06,
      "loss": 1.6221,
      "step": 5461
    },
    {
      "epoch": 2.114595431668602,
      "grad_norm": 50.964412689208984,
      "learning_rate": 8.761560631479332e-06,
      "loss": 1.7632,
      "step": 5462
    },
    {
      "epoch": 2.1149825783972127,
      "grad_norm": 13.555194854736328,
      "learning_rate": 8.761130468447542e-06,
      "loss": 1.4486,
      "step": 5463
    },
    {
      "epoch": 2.1153697251258228,
      "grad_norm": 15.29783821105957,
      "learning_rate": 8.760700305415753e-06,
      "loss": 1.459,
      "step": 5464
    },
    {
      "epoch": 2.115756871854433,
      "grad_norm": 13.238360404968262,
      "learning_rate": 8.760270142383964e-06,
      "loss": 0.7941,
      "step": 5465
    },
    {
      "epoch": 2.116144018583043,
      "grad_norm": 12.717109680175781,
      "learning_rate": 8.759839979352174e-06,
      "loss": 1.4103,
      "step": 5466
    },
    {
      "epoch": 2.116531165311653,
      "grad_norm": 22.96436309814453,
      "learning_rate": 8.759409816320387e-06,
      "loss": 2.3989,
      "step": 5467
    },
    {
      "epoch": 2.116918312040263,
      "grad_norm": 27.585935592651367,
      "learning_rate": 8.758979653288597e-06,
      "loss": 1.1663,
      "step": 5468
    },
    {
      "epoch": 2.1173054587688735,
      "grad_norm": 25.236948013305664,
      "learning_rate": 8.758549490256808e-06,
      "loss": 2.1201,
      "step": 5469
    },
    {
      "epoch": 2.1176926054974836,
      "grad_norm": 14.442621231079102,
      "learning_rate": 8.758119327225018e-06,
      "loss": 0.6983,
      "step": 5470
    },
    {
      "epoch": 2.1180797522260937,
      "grad_norm": 13.609702110290527,
      "learning_rate": 8.757689164193231e-06,
      "loss": 1.2189,
      "step": 5471
    },
    {
      "epoch": 2.1184668989547037,
      "grad_norm": 25.2722110748291,
      "learning_rate": 8.757259001161441e-06,
      "loss": 2.0583,
      "step": 5472
    },
    {
      "epoch": 2.118854045683314,
      "grad_norm": 18.350017547607422,
      "learning_rate": 8.756828838129652e-06,
      "loss": 0.8601,
      "step": 5473
    },
    {
      "epoch": 2.1192411924119243,
      "grad_norm": 41.23414611816406,
      "learning_rate": 8.756398675097862e-06,
      "loss": 1.1201,
      "step": 5474
    },
    {
      "epoch": 2.1196283391405344,
      "grad_norm": 14.632118225097656,
      "learning_rate": 8.755968512066073e-06,
      "loss": 1.2879,
      "step": 5475
    },
    {
      "epoch": 2.1200154858691445,
      "grad_norm": 29.743772506713867,
      "learning_rate": 8.755538349034285e-06,
      "loss": 2.0598,
      "step": 5476
    },
    {
      "epoch": 2.1204026325977545,
      "grad_norm": 30.44284439086914,
      "learning_rate": 8.755108186002496e-06,
      "loss": 1.682,
      "step": 5477
    },
    {
      "epoch": 2.1207897793263646,
      "grad_norm": 14.831629753112793,
      "learning_rate": 8.754678022970706e-06,
      "loss": 0.9774,
      "step": 5478
    },
    {
      "epoch": 2.1211769260549747,
      "grad_norm": 21.74472999572754,
      "learning_rate": 8.754247859938917e-06,
      "loss": 1.5338,
      "step": 5479
    },
    {
      "epoch": 2.121564072783585,
      "grad_norm": 20.183528900146484,
      "learning_rate": 8.753817696907129e-06,
      "loss": 1.7652,
      "step": 5480
    },
    {
      "epoch": 2.1219512195121952,
      "grad_norm": 25.81972312927246,
      "learning_rate": 8.753387533875339e-06,
      "loss": 2.6645,
      "step": 5481
    },
    {
      "epoch": 2.1223383662408053,
      "grad_norm": 24.860170364379883,
      "learning_rate": 8.75295737084355e-06,
      "loss": 2.381,
      "step": 5482
    },
    {
      "epoch": 2.1227255129694154,
      "grad_norm": 13.859825134277344,
      "learning_rate": 8.752527207811761e-06,
      "loss": 1.2708,
      "step": 5483
    },
    {
      "epoch": 2.1231126596980254,
      "grad_norm": 37.47441864013672,
      "learning_rate": 8.752097044779973e-06,
      "loss": 1.6675,
      "step": 5484
    },
    {
      "epoch": 2.1234998064266355,
      "grad_norm": 28.195186614990234,
      "learning_rate": 8.751666881748183e-06,
      "loss": 1.5722,
      "step": 5485
    },
    {
      "epoch": 2.123886953155246,
      "grad_norm": 23.275482177734375,
      "learning_rate": 8.751236718716394e-06,
      "loss": 1.2593,
      "step": 5486
    },
    {
      "epoch": 2.124274099883856,
      "grad_norm": 23.3780460357666,
      "learning_rate": 8.750806555684605e-06,
      "loss": 1.4952,
      "step": 5487
    },
    {
      "epoch": 2.124661246612466,
      "grad_norm": 20.013765335083008,
      "learning_rate": 8.750376392652817e-06,
      "loss": 1.6609,
      "step": 5488
    },
    {
      "epoch": 2.125048393341076,
      "grad_norm": 43.94383239746094,
      "learning_rate": 8.749946229621027e-06,
      "loss": 1.3972,
      "step": 5489
    },
    {
      "epoch": 2.1254355400696863,
      "grad_norm": 41.841487884521484,
      "learning_rate": 8.749516066589238e-06,
      "loss": 2.6057,
      "step": 5490
    },
    {
      "epoch": 2.125822686798297,
      "grad_norm": 36.100860595703125,
      "learning_rate": 8.74908590355745e-06,
      "loss": 2.0067,
      "step": 5491
    },
    {
      "epoch": 2.126209833526907,
      "grad_norm": 42.83831787109375,
      "learning_rate": 8.74865574052566e-06,
      "loss": 1.7716,
      "step": 5492
    },
    {
      "epoch": 2.126596980255517,
      "grad_norm": 12.032014846801758,
      "learning_rate": 8.74822557749387e-06,
      "loss": 0.8978,
      "step": 5493
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 13.873848915100098,
      "learning_rate": 8.747795414462082e-06,
      "loss": 1.3049,
      "step": 5494
    },
    {
      "epoch": 2.127371273712737,
      "grad_norm": 20.320743560791016,
      "learning_rate": 8.747365251430293e-06,
      "loss": 1.4803,
      "step": 5495
    },
    {
      "epoch": 2.127758420441347,
      "grad_norm": 15.638463020324707,
      "learning_rate": 8.746935088398503e-06,
      "loss": 1.3427,
      "step": 5496
    },
    {
      "epoch": 2.1281455671699576,
      "grad_norm": 30.578554153442383,
      "learning_rate": 8.746504925366714e-06,
      "loss": 1.9758,
      "step": 5497
    },
    {
      "epoch": 2.1285327138985677,
      "grad_norm": 20.487905502319336,
      "learning_rate": 8.746074762334926e-06,
      "loss": 1.7454,
      "step": 5498
    },
    {
      "epoch": 2.1289198606271778,
      "grad_norm": 13.150039672851562,
      "learning_rate": 8.745644599303137e-06,
      "loss": 1.0678,
      "step": 5499
    },
    {
      "epoch": 2.129307007355788,
      "grad_norm": 13.339587211608887,
      "learning_rate": 8.745214436271347e-06,
      "loss": 0.8516,
      "step": 5500
    },
    {
      "epoch": 2.129694154084398,
      "grad_norm": 13.905889511108398,
      "learning_rate": 8.744784273239558e-06,
      "loss": 1.3508,
      "step": 5501
    },
    {
      "epoch": 2.130081300813008,
      "grad_norm": 23.817304611206055,
      "learning_rate": 8.744354110207768e-06,
      "loss": 1.4052,
      "step": 5502
    },
    {
      "epoch": 2.1304684475416185,
      "grad_norm": 14.730607032775879,
      "learning_rate": 8.743923947175981e-06,
      "loss": 0.5375,
      "step": 5503
    },
    {
      "epoch": 2.1308555942702285,
      "grad_norm": 12.886999130249023,
      "learning_rate": 8.743493784144191e-06,
      "loss": 0.577,
      "step": 5504
    },
    {
      "epoch": 2.1312427409988386,
      "grad_norm": 12.96403694152832,
      "learning_rate": 8.743063621112402e-06,
      "loss": 0.9199,
      "step": 5505
    },
    {
      "epoch": 2.1316298877274487,
      "grad_norm": 29.194421768188477,
      "learning_rate": 8.742633458080614e-06,
      "loss": 2.0108,
      "step": 5506
    },
    {
      "epoch": 2.1320170344560587,
      "grad_norm": 13.44262409210205,
      "learning_rate": 8.742203295048825e-06,
      "loss": 1.4728,
      "step": 5507
    },
    {
      "epoch": 2.132404181184669,
      "grad_norm": 15.553369522094727,
      "learning_rate": 8.741773132017035e-06,
      "loss": 0.9904,
      "step": 5508
    },
    {
      "epoch": 2.1327913279132793,
      "grad_norm": 35.55663299560547,
      "learning_rate": 8.741342968985246e-06,
      "loss": 1.5654,
      "step": 5509
    },
    {
      "epoch": 2.1331784746418894,
      "grad_norm": 21.96167755126953,
      "learning_rate": 8.740912805953458e-06,
      "loss": 1.602,
      "step": 5510
    },
    {
      "epoch": 2.1335656213704994,
      "grad_norm": 26.394311904907227,
      "learning_rate": 8.740482642921667e-06,
      "loss": 1.5324,
      "step": 5511
    },
    {
      "epoch": 2.1339527680991095,
      "grad_norm": 27.62633514404297,
      "learning_rate": 8.740052479889879e-06,
      "loss": 1.1431,
      "step": 5512
    },
    {
      "epoch": 2.1343399148277196,
      "grad_norm": 13.809944152832031,
      "learning_rate": 8.73962231685809e-06,
      "loss": 0.9114,
      "step": 5513
    },
    {
      "epoch": 2.1347270615563296,
      "grad_norm": 17.73818588256836,
      "learning_rate": 8.739192153826302e-06,
      "loss": 1.5264,
      "step": 5514
    },
    {
      "epoch": 2.13511420828494,
      "grad_norm": 24.234107971191406,
      "learning_rate": 8.738761990794511e-06,
      "loss": 1.1306,
      "step": 5515
    },
    {
      "epoch": 2.13550135501355,
      "grad_norm": 19.172435760498047,
      "learning_rate": 8.738331827762723e-06,
      "loss": 1.1303,
      "step": 5516
    },
    {
      "epoch": 2.1358885017421603,
      "grad_norm": 16.59514617919922,
      "learning_rate": 8.737901664730933e-06,
      "loss": 1.476,
      "step": 5517
    },
    {
      "epoch": 2.1362756484707703,
      "grad_norm": 23.145198822021484,
      "learning_rate": 8.737471501699146e-06,
      "loss": 1.2811,
      "step": 5518
    },
    {
      "epoch": 2.1366627951993804,
      "grad_norm": 52.81734848022461,
      "learning_rate": 8.737041338667355e-06,
      "loss": 2.0819,
      "step": 5519
    },
    {
      "epoch": 2.137049941927991,
      "grad_norm": 10.682951927185059,
      "learning_rate": 8.736611175635567e-06,
      "loss": 1.0315,
      "step": 5520
    },
    {
      "epoch": 2.137437088656601,
      "grad_norm": 17.709924697875977,
      "learning_rate": 8.736181012603777e-06,
      "loss": 1.0862,
      "step": 5521
    },
    {
      "epoch": 2.137824235385211,
      "grad_norm": 19.34964942932129,
      "learning_rate": 8.73575084957199e-06,
      "loss": 1.8373,
      "step": 5522
    },
    {
      "epoch": 2.138211382113821,
      "grad_norm": 47.531272888183594,
      "learning_rate": 8.7353206865402e-06,
      "loss": 2.2186,
      "step": 5523
    },
    {
      "epoch": 2.138598528842431,
      "grad_norm": 17.673538208007812,
      "learning_rate": 8.73489052350841e-06,
      "loss": 1.0564,
      "step": 5524
    },
    {
      "epoch": 2.1389856755710412,
      "grad_norm": 12.939813613891602,
      "learning_rate": 8.73446036047662e-06,
      "loss": 0.909,
      "step": 5525
    },
    {
      "epoch": 2.1393728222996518,
      "grad_norm": 18.389057159423828,
      "learning_rate": 8.734030197444832e-06,
      "loss": 1.4286,
      "step": 5526
    },
    {
      "epoch": 2.139759969028262,
      "grad_norm": 6.385679244995117,
      "learning_rate": 8.733600034413043e-06,
      "loss": 0.3555,
      "step": 5527
    },
    {
      "epoch": 2.140147115756872,
      "grad_norm": 16.221738815307617,
      "learning_rate": 8.733169871381255e-06,
      "loss": 0.8675,
      "step": 5528
    },
    {
      "epoch": 2.140534262485482,
      "grad_norm": 20.534563064575195,
      "learning_rate": 8.732739708349465e-06,
      "loss": 1.4087,
      "step": 5529
    },
    {
      "epoch": 2.140921409214092,
      "grad_norm": 16.991741180419922,
      "learning_rate": 8.732309545317676e-06,
      "loss": 1.5793,
      "step": 5530
    },
    {
      "epoch": 2.141308555942702,
      "grad_norm": 29.558692932128906,
      "learning_rate": 8.731879382285887e-06,
      "loss": 2.1722,
      "step": 5531
    },
    {
      "epoch": 2.1416957026713126,
      "grad_norm": 20.335899353027344,
      "learning_rate": 8.731449219254097e-06,
      "loss": 1.6276,
      "step": 5532
    },
    {
      "epoch": 2.1420828493999227,
      "grad_norm": 19.3277530670166,
      "learning_rate": 8.731019056222308e-06,
      "loss": 2.1412,
      "step": 5533
    },
    {
      "epoch": 2.1424699961285327,
      "grad_norm": 19.572973251342773,
      "learning_rate": 8.73058889319052e-06,
      "loss": 1.5771,
      "step": 5534
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 18.54538345336914,
      "learning_rate": 8.730158730158731e-06,
      "loss": 1.095,
      "step": 5535
    },
    {
      "epoch": 2.143244289585753,
      "grad_norm": 37.16446304321289,
      "learning_rate": 8.729728567126941e-06,
      "loss": 1.6997,
      "step": 5536
    },
    {
      "epoch": 2.1436314363143634,
      "grad_norm": 14.078298568725586,
      "learning_rate": 8.729298404095152e-06,
      "loss": 1.3007,
      "step": 5537
    },
    {
      "epoch": 2.1440185830429734,
      "grad_norm": 15.868370056152344,
      "learning_rate": 8.728868241063364e-06,
      "loss": 1.6534,
      "step": 5538
    },
    {
      "epoch": 2.1444057297715835,
      "grad_norm": 10.215500831604004,
      "learning_rate": 8.728438078031575e-06,
      "loss": 0.9436,
      "step": 5539
    },
    {
      "epoch": 2.1447928765001936,
      "grad_norm": 25.79248046875,
      "learning_rate": 8.728007914999785e-06,
      "loss": 1.8894,
      "step": 5540
    },
    {
      "epoch": 2.1451800232288036,
      "grad_norm": 12.571565628051758,
      "learning_rate": 8.727577751967996e-06,
      "loss": 0.9179,
      "step": 5541
    },
    {
      "epoch": 2.1455671699574137,
      "grad_norm": 15.511063575744629,
      "learning_rate": 8.727147588936208e-06,
      "loss": 1.5313,
      "step": 5542
    },
    {
      "epoch": 2.145954316686024,
      "grad_norm": 13.670872688293457,
      "learning_rate": 8.72671742590442e-06,
      "loss": 0.9248,
      "step": 5543
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 19.23642921447754,
      "learning_rate": 8.726287262872629e-06,
      "loss": 1.6542,
      "step": 5544
    },
    {
      "epoch": 2.1467286101432443,
      "grad_norm": 13.490459442138672,
      "learning_rate": 8.72585709984084e-06,
      "loss": 0.6661,
      "step": 5545
    },
    {
      "epoch": 2.1471157568718544,
      "grad_norm": 23.750993728637695,
      "learning_rate": 8.725426936809052e-06,
      "loss": 1.3655,
      "step": 5546
    },
    {
      "epoch": 2.1475029036004645,
      "grad_norm": 16.953718185424805,
      "learning_rate": 8.724996773777262e-06,
      "loss": 1.5309,
      "step": 5547
    },
    {
      "epoch": 2.1478900503290745,
      "grad_norm": 13.342262268066406,
      "learning_rate": 8.724566610745473e-06,
      "loss": 0.9818,
      "step": 5548
    },
    {
      "epoch": 2.148277197057685,
      "grad_norm": 17.175289154052734,
      "learning_rate": 8.724136447713684e-06,
      "loss": 1.4248,
      "step": 5549
    },
    {
      "epoch": 2.148664343786295,
      "grad_norm": 12.379402160644531,
      "learning_rate": 8.723706284681896e-06,
      "loss": 0.9004,
      "step": 5550
    },
    {
      "epoch": 2.149051490514905,
      "grad_norm": 16.12058448791504,
      "learning_rate": 8.723276121650105e-06,
      "loss": 0.9872,
      "step": 5551
    },
    {
      "epoch": 2.1494386372435152,
      "grad_norm": 24.238033294677734,
      "learning_rate": 8.722845958618317e-06,
      "loss": 2.56,
      "step": 5552
    },
    {
      "epoch": 2.1498257839721253,
      "grad_norm": 15.820018768310547,
      "learning_rate": 8.722415795586528e-06,
      "loss": 1.4206,
      "step": 5553
    },
    {
      "epoch": 2.1502129307007354,
      "grad_norm": 20.745725631713867,
      "learning_rate": 8.72198563255474e-06,
      "loss": 1.6712,
      "step": 5554
    },
    {
      "epoch": 2.150600077429346,
      "grad_norm": 15.648917198181152,
      "learning_rate": 8.72155546952295e-06,
      "loss": 1.2458,
      "step": 5555
    },
    {
      "epoch": 2.150987224157956,
      "grad_norm": 30.16804313659668,
      "learning_rate": 8.721125306491161e-06,
      "loss": 1.5599,
      "step": 5556
    },
    {
      "epoch": 2.151374370886566,
      "grad_norm": 20.939022064208984,
      "learning_rate": 8.720695143459372e-06,
      "loss": 1.8141,
      "step": 5557
    },
    {
      "epoch": 2.151761517615176,
      "grad_norm": 29.85820198059082,
      "learning_rate": 8.720264980427584e-06,
      "loss": 1.7023,
      "step": 5558
    },
    {
      "epoch": 2.152148664343786,
      "grad_norm": 14.379895210266113,
      "learning_rate": 8.719834817395793e-06,
      "loss": 0.9776,
      "step": 5559
    },
    {
      "epoch": 2.152535811072396,
      "grad_norm": 20.128273010253906,
      "learning_rate": 8.719404654364005e-06,
      "loss": 1.0758,
      "step": 5560
    },
    {
      "epoch": 2.1529229578010067,
      "grad_norm": 15.519817352294922,
      "learning_rate": 8.718974491332216e-06,
      "loss": 0.8798,
      "step": 5561
    },
    {
      "epoch": 2.153310104529617,
      "grad_norm": 11.657852172851562,
      "learning_rate": 8.718544328300426e-06,
      "loss": 0.4112,
      "step": 5562
    },
    {
      "epoch": 2.153697251258227,
      "grad_norm": 73.40205383300781,
      "learning_rate": 8.718114165268637e-06,
      "loss": 1.7303,
      "step": 5563
    },
    {
      "epoch": 2.154084397986837,
      "grad_norm": 14.569931030273438,
      "learning_rate": 8.717684002236849e-06,
      "loss": 1.4153,
      "step": 5564
    },
    {
      "epoch": 2.154471544715447,
      "grad_norm": 27.54849624633789,
      "learning_rate": 8.71725383920506e-06,
      "loss": 3.6219,
      "step": 5565
    },
    {
      "epoch": 2.1548586914440575,
      "grad_norm": 12.5906982421875,
      "learning_rate": 8.71682367617327e-06,
      "loss": 0.7357,
      "step": 5566
    },
    {
      "epoch": 2.1552458381726676,
      "grad_norm": 24.073684692382812,
      "learning_rate": 8.716393513141481e-06,
      "loss": 2.0947,
      "step": 5567
    },
    {
      "epoch": 2.1556329849012776,
      "grad_norm": 67.25160217285156,
      "learning_rate": 8.715963350109691e-06,
      "loss": 1.0477,
      "step": 5568
    },
    {
      "epoch": 2.1560201316298877,
      "grad_norm": 27.85824966430664,
      "learning_rate": 8.715533187077904e-06,
      "loss": 1.8308,
      "step": 5569
    },
    {
      "epoch": 2.1564072783584978,
      "grad_norm": 18.09551239013672,
      "learning_rate": 8.715103024046114e-06,
      "loss": 1.7496,
      "step": 5570
    },
    {
      "epoch": 2.156794425087108,
      "grad_norm": 15.444225311279297,
      "learning_rate": 8.714672861014325e-06,
      "loss": 1.4756,
      "step": 5571
    },
    {
      "epoch": 2.1571815718157183,
      "grad_norm": 66.61254119873047,
      "learning_rate": 8.714242697982535e-06,
      "loss": 1.1629,
      "step": 5572
    },
    {
      "epoch": 2.1575687185443284,
      "grad_norm": 33.25580596923828,
      "learning_rate": 8.713812534950748e-06,
      "loss": 1.5504,
      "step": 5573
    },
    {
      "epoch": 2.1579558652729385,
      "grad_norm": 14.610329627990723,
      "learning_rate": 8.713382371918958e-06,
      "loss": 0.949,
      "step": 5574
    },
    {
      "epoch": 2.1583430120015485,
      "grad_norm": 33.33892822265625,
      "learning_rate": 8.71295220888717e-06,
      "loss": 1.1816,
      "step": 5575
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 15.902046203613281,
      "learning_rate": 8.712522045855379e-06,
      "loss": 1.3204,
      "step": 5576
    },
    {
      "epoch": 2.1591173054587687,
      "grad_norm": 30.252845764160156,
      "learning_rate": 8.71209188282359e-06,
      "loss": 1.7609,
      "step": 5577
    },
    {
      "epoch": 2.159504452187379,
      "grad_norm": 15.364675521850586,
      "learning_rate": 8.711661719791802e-06,
      "loss": 1.2388,
      "step": 5578
    },
    {
      "epoch": 2.1598915989159893,
      "grad_norm": 17.906232833862305,
      "learning_rate": 8.711231556760013e-06,
      "loss": 1.5829,
      "step": 5579
    },
    {
      "epoch": 2.1602787456445993,
      "grad_norm": 14.334857940673828,
      "learning_rate": 8.710801393728223e-06,
      "loss": 1.3896,
      "step": 5580
    },
    {
      "epoch": 2.1606658923732094,
      "grad_norm": 71.65946960449219,
      "learning_rate": 8.710371230696434e-06,
      "loss": 2.047,
      "step": 5581
    },
    {
      "epoch": 2.1610530391018195,
      "grad_norm": 13.809273719787598,
      "learning_rate": 8.709941067664646e-06,
      "loss": 0.423,
      "step": 5582
    },
    {
      "epoch": 2.16144018583043,
      "grad_norm": 39.18682861328125,
      "learning_rate": 8.709510904632856e-06,
      "loss": 2.2343,
      "step": 5583
    },
    {
      "epoch": 2.16182733255904,
      "grad_norm": 21.373743057250977,
      "learning_rate": 8.709080741601067e-06,
      "loss": 1.5061,
      "step": 5584
    },
    {
      "epoch": 2.16221447928765,
      "grad_norm": 24.526430130004883,
      "learning_rate": 8.708650578569278e-06,
      "loss": 1.4169,
      "step": 5585
    },
    {
      "epoch": 2.16260162601626,
      "grad_norm": 24.785858154296875,
      "learning_rate": 8.70822041553749e-06,
      "loss": 1.2561,
      "step": 5586
    },
    {
      "epoch": 2.1629887727448702,
      "grad_norm": 34.1836051940918,
      "learning_rate": 8.7077902525057e-06,
      "loss": 3.0306,
      "step": 5587
    },
    {
      "epoch": 2.1633759194734803,
      "grad_norm": 17.974227905273438,
      "learning_rate": 8.707360089473913e-06,
      "loss": 1.6093,
      "step": 5588
    },
    {
      "epoch": 2.1637630662020904,
      "grad_norm": 15.811424255371094,
      "learning_rate": 8.706929926442122e-06,
      "loss": 0.4131,
      "step": 5589
    },
    {
      "epoch": 2.164150212930701,
      "grad_norm": 39.51288986206055,
      "learning_rate": 8.706499763410334e-06,
      "loss": 1.5449,
      "step": 5590
    },
    {
      "epoch": 2.164537359659311,
      "grad_norm": 16.884105682373047,
      "learning_rate": 8.706069600378543e-06,
      "loss": 1.4809,
      "step": 5591
    },
    {
      "epoch": 2.164924506387921,
      "grad_norm": 6.713564395904541,
      "learning_rate": 8.705639437346755e-06,
      "loss": 0.4254,
      "step": 5592
    },
    {
      "epoch": 2.165311653116531,
      "grad_norm": 48.218536376953125,
      "learning_rate": 8.705209274314966e-06,
      "loss": 1.3379,
      "step": 5593
    },
    {
      "epoch": 2.165698799845141,
      "grad_norm": 17.687524795532227,
      "learning_rate": 8.704779111283178e-06,
      "loss": 1.3326,
      "step": 5594
    },
    {
      "epoch": 2.1660859465737516,
      "grad_norm": 16.637033462524414,
      "learning_rate": 8.704348948251387e-06,
      "loss": 0.9712,
      "step": 5595
    },
    {
      "epoch": 2.1664730933023617,
      "grad_norm": 56.31962966918945,
      "learning_rate": 8.703918785219599e-06,
      "loss": 0.9894,
      "step": 5596
    },
    {
      "epoch": 2.1668602400309718,
      "grad_norm": 15.924088478088379,
      "learning_rate": 8.70348862218781e-06,
      "loss": 1.1624,
      "step": 5597
    },
    {
      "epoch": 2.167247386759582,
      "grad_norm": 13.462034225463867,
      "learning_rate": 8.70305845915602e-06,
      "loss": 1.2339,
      "step": 5598
    },
    {
      "epoch": 2.167634533488192,
      "grad_norm": 54.61967849731445,
      "learning_rate": 8.702628296124231e-06,
      "loss": 2.8346,
      "step": 5599
    },
    {
      "epoch": 2.168021680216802,
      "grad_norm": 26.92546272277832,
      "learning_rate": 8.702198133092443e-06,
      "loss": 1.502,
      "step": 5600
    },
    {
      "epoch": 2.1684088269454125,
      "grad_norm": 11.193740844726562,
      "learning_rate": 8.701767970060654e-06,
      "loss": 0.6483,
      "step": 5601
    },
    {
      "epoch": 2.1687959736740225,
      "grad_norm": 21.617008209228516,
      "learning_rate": 8.701337807028864e-06,
      "loss": 1.8115,
      "step": 5602
    },
    {
      "epoch": 2.1691831204026326,
      "grad_norm": 16.287683486938477,
      "learning_rate": 8.700907643997075e-06,
      "loss": 1.4742,
      "step": 5603
    },
    {
      "epoch": 2.1695702671312427,
      "grad_norm": 24.744524002075195,
      "learning_rate": 8.700477480965287e-06,
      "loss": 0.602,
      "step": 5604
    },
    {
      "epoch": 2.1699574138598527,
      "grad_norm": 28.67487907409668,
      "learning_rate": 8.700047317933498e-06,
      "loss": 1.6241,
      "step": 5605
    },
    {
      "epoch": 2.170344560588463,
      "grad_norm": 32.49822235107422,
      "learning_rate": 8.699617154901708e-06,
      "loss": 1.7303,
      "step": 5606
    },
    {
      "epoch": 2.1707317073170733,
      "grad_norm": 19.537626266479492,
      "learning_rate": 8.69918699186992e-06,
      "loss": 1.6886,
      "step": 5607
    },
    {
      "epoch": 2.1711188540456834,
      "grad_norm": 28.749908447265625,
      "learning_rate": 8.69875682883813e-06,
      "loss": 1.1563,
      "step": 5608
    },
    {
      "epoch": 2.1715060007742935,
      "grad_norm": 10.682608604431152,
      "learning_rate": 8.698326665806342e-06,
      "loss": 0.7777,
      "step": 5609
    },
    {
      "epoch": 2.1718931475029035,
      "grad_norm": 50.361900329589844,
      "learning_rate": 8.697896502774552e-06,
      "loss": 1.3772,
      "step": 5610
    },
    {
      "epoch": 2.1722802942315136,
      "grad_norm": 19.933732986450195,
      "learning_rate": 8.697466339742763e-06,
      "loss": 0.9924,
      "step": 5611
    },
    {
      "epoch": 2.172667440960124,
      "grad_norm": 27.145950317382812,
      "learning_rate": 8.697036176710975e-06,
      "loss": 1.8916,
      "step": 5612
    },
    {
      "epoch": 2.173054587688734,
      "grad_norm": 22.96731185913086,
      "learning_rate": 8.696606013679184e-06,
      "loss": 1.3846,
      "step": 5613
    },
    {
      "epoch": 2.1734417344173442,
      "grad_norm": 25.20335578918457,
      "learning_rate": 8.696175850647396e-06,
      "loss": 2.5426,
      "step": 5614
    },
    {
      "epoch": 2.1738288811459543,
      "grad_norm": 23.64990234375,
      "learning_rate": 8.695745687615607e-06,
      "loss": 1.0473,
      "step": 5615
    },
    {
      "epoch": 2.1742160278745644,
      "grad_norm": 21.065013885498047,
      "learning_rate": 8.695315524583819e-06,
      "loss": 1.5633,
      "step": 5616
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 37.93104934692383,
      "learning_rate": 8.694885361552028e-06,
      "loss": 2.854,
      "step": 5617
    },
    {
      "epoch": 2.174990321331785,
      "grad_norm": 10.136884689331055,
      "learning_rate": 8.69445519852024e-06,
      "loss": 0.3699,
      "step": 5618
    },
    {
      "epoch": 2.175377468060395,
      "grad_norm": 16.31790542602539,
      "learning_rate": 8.69402503548845e-06,
      "loss": 1.6626,
      "step": 5619
    },
    {
      "epoch": 2.175764614789005,
      "grad_norm": 25.839956283569336,
      "learning_rate": 8.693594872456663e-06,
      "loss": 1.9587,
      "step": 5620
    },
    {
      "epoch": 2.176151761517615,
      "grad_norm": 32.861690521240234,
      "learning_rate": 8.693164709424872e-06,
      "loss": 2.0731,
      "step": 5621
    },
    {
      "epoch": 2.176538908246225,
      "grad_norm": 13.506763458251953,
      "learning_rate": 8.692734546393084e-06,
      "loss": 0.8152,
      "step": 5622
    },
    {
      "epoch": 2.1769260549748353,
      "grad_norm": 26.92336654663086,
      "learning_rate": 8.692304383361294e-06,
      "loss": 3.6823,
      "step": 5623
    },
    {
      "epoch": 2.1773132017034458,
      "grad_norm": 18.75119972229004,
      "learning_rate": 8.691874220329507e-06,
      "loss": 0.9899,
      "step": 5624
    },
    {
      "epoch": 2.177700348432056,
      "grad_norm": 27.13536834716797,
      "learning_rate": 8.691444057297716e-06,
      "loss": 0.9819,
      "step": 5625
    },
    {
      "epoch": 2.178087495160666,
      "grad_norm": 22.54461669921875,
      "learning_rate": 8.691013894265928e-06,
      "loss": 1.1496,
      "step": 5626
    },
    {
      "epoch": 2.178474641889276,
      "grad_norm": 12.797321319580078,
      "learning_rate": 8.690583731234138e-06,
      "loss": 1.3425,
      "step": 5627
    },
    {
      "epoch": 2.178861788617886,
      "grad_norm": 28.64940071105957,
      "learning_rate": 8.690153568202349e-06,
      "loss": 1.8189,
      "step": 5628
    },
    {
      "epoch": 2.1792489353464966,
      "grad_norm": 32.1628303527832,
      "learning_rate": 8.68972340517056e-06,
      "loss": 1.2129,
      "step": 5629
    },
    {
      "epoch": 2.1796360820751066,
      "grad_norm": 13.91480827331543,
      "learning_rate": 8.689293242138772e-06,
      "loss": 0.932,
      "step": 5630
    },
    {
      "epoch": 2.1800232288037167,
      "grad_norm": 25.756132125854492,
      "learning_rate": 8.688863079106983e-06,
      "loss": 3.0579,
      "step": 5631
    },
    {
      "epoch": 2.1804103755323267,
      "grad_norm": 22.27522850036621,
      "learning_rate": 8.688432916075193e-06,
      "loss": 1.8336,
      "step": 5632
    },
    {
      "epoch": 2.180797522260937,
      "grad_norm": 14.170891761779785,
      "learning_rate": 8.688002753043404e-06,
      "loss": 1.3633,
      "step": 5633
    },
    {
      "epoch": 2.181184668989547,
      "grad_norm": 5.16738224029541,
      "learning_rate": 8.687572590011614e-06,
      "loss": 0.3049,
      "step": 5634
    },
    {
      "epoch": 2.181571815718157,
      "grad_norm": 17.48908805847168,
      "learning_rate": 8.687142426979827e-06,
      "loss": 1.445,
      "step": 5635
    },
    {
      "epoch": 2.1819589624467675,
      "grad_norm": 11.5468111038208,
      "learning_rate": 8.686712263948037e-06,
      "loss": 1.2148,
      "step": 5636
    },
    {
      "epoch": 2.1823461091753775,
      "grad_norm": 23.58120346069336,
      "learning_rate": 8.686282100916248e-06,
      "loss": 1.3988,
      "step": 5637
    },
    {
      "epoch": 2.1827332559039876,
      "grad_norm": 11.596490859985352,
      "learning_rate": 8.685851937884458e-06,
      "loss": 0.7007,
      "step": 5638
    },
    {
      "epoch": 2.1831204026325977,
      "grad_norm": 30.496051788330078,
      "learning_rate": 8.685421774852671e-06,
      "loss": 2.2946,
      "step": 5639
    },
    {
      "epoch": 2.1835075493612077,
      "grad_norm": 13.67969036102295,
      "learning_rate": 8.68499161182088e-06,
      "loss": 0.7548,
      "step": 5640
    },
    {
      "epoch": 2.1838946960898182,
      "grad_norm": 45.68698501586914,
      "learning_rate": 8.684561448789092e-06,
      "loss": 1.2063,
      "step": 5641
    },
    {
      "epoch": 2.1842818428184283,
      "grad_norm": 29.896760940551758,
      "learning_rate": 8.684131285757302e-06,
      "loss": 1.9256,
      "step": 5642
    },
    {
      "epoch": 2.1846689895470384,
      "grad_norm": 12.412979125976562,
      "learning_rate": 8.683701122725513e-06,
      "loss": 0.5971,
      "step": 5643
    },
    {
      "epoch": 2.1850561362756484,
      "grad_norm": 11.985725402832031,
      "learning_rate": 8.683270959693725e-06,
      "loss": 0.7547,
      "step": 5644
    },
    {
      "epoch": 2.1854432830042585,
      "grad_norm": 16.291568756103516,
      "learning_rate": 8.682840796661936e-06,
      "loss": 1.4305,
      "step": 5645
    },
    {
      "epoch": 2.1858304297328686,
      "grad_norm": 25.673742294311523,
      "learning_rate": 8.682410633630146e-06,
      "loss": 2.6777,
      "step": 5646
    },
    {
      "epoch": 2.186217576461479,
      "grad_norm": 21.943466186523438,
      "learning_rate": 8.681980470598357e-06,
      "loss": 1.3586,
      "step": 5647
    },
    {
      "epoch": 2.186604723190089,
      "grad_norm": 9.901951789855957,
      "learning_rate": 8.681550307566569e-06,
      "loss": 0.5079,
      "step": 5648
    },
    {
      "epoch": 2.186991869918699,
      "grad_norm": 16.38618278503418,
      "learning_rate": 8.681120144534778e-06,
      "loss": 1.7735,
      "step": 5649
    },
    {
      "epoch": 2.1873790166473093,
      "grad_norm": 30.50965690612793,
      "learning_rate": 8.68068998150299e-06,
      "loss": 2.2423,
      "step": 5650
    },
    {
      "epoch": 2.1877661633759193,
      "grad_norm": 33.979129791259766,
      "learning_rate": 8.680259818471201e-06,
      "loss": 0.6657,
      "step": 5651
    },
    {
      "epoch": 2.1881533101045294,
      "grad_norm": 17.17148208618164,
      "learning_rate": 8.679829655439413e-06,
      "loss": 1.4833,
      "step": 5652
    },
    {
      "epoch": 2.18854045683314,
      "grad_norm": 43.750972747802734,
      "learning_rate": 8.679399492407622e-06,
      "loss": 1.5628,
      "step": 5653
    },
    {
      "epoch": 2.18892760356175,
      "grad_norm": 17.35064125061035,
      "learning_rate": 8.678969329375834e-06,
      "loss": 1.527,
      "step": 5654
    },
    {
      "epoch": 2.18931475029036,
      "grad_norm": 14.566039085388184,
      "learning_rate": 8.678539166344045e-06,
      "loss": 1.4068,
      "step": 5655
    },
    {
      "epoch": 2.18970189701897,
      "grad_norm": 31.55963897705078,
      "learning_rate": 8.678109003312257e-06,
      "loss": 1.9275,
      "step": 5656
    },
    {
      "epoch": 2.19008904374758,
      "grad_norm": 17.734642028808594,
      "learning_rate": 8.677678840280466e-06,
      "loss": 1.0969,
      "step": 5657
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 13.627419471740723,
      "learning_rate": 8.677248677248678e-06,
      "loss": 0.8354,
      "step": 5658
    },
    {
      "epoch": 2.1908633372048008,
      "grad_norm": 19.251916885375977,
      "learning_rate": 8.67681851421689e-06,
      "loss": 1.1582,
      "step": 5659
    },
    {
      "epoch": 2.191250483933411,
      "grad_norm": 27.29096221923828,
      "learning_rate": 8.6763883511851e-06,
      "loss": 1.5297,
      "step": 5660
    },
    {
      "epoch": 2.191637630662021,
      "grad_norm": 38.77918243408203,
      "learning_rate": 8.67595818815331e-06,
      "loss": 1.4638,
      "step": 5661
    },
    {
      "epoch": 2.192024777390631,
      "grad_norm": 17.602296829223633,
      "learning_rate": 8.675528025121522e-06,
      "loss": 0.9641,
      "step": 5662
    },
    {
      "epoch": 2.192411924119241,
      "grad_norm": 18.6450138092041,
      "learning_rate": 8.675097862089733e-06,
      "loss": 0.8111,
      "step": 5663
    },
    {
      "epoch": 2.1927990708478515,
      "grad_norm": 23.98210334777832,
      "learning_rate": 8.674667699057943e-06,
      "loss": 1.3939,
      "step": 5664
    },
    {
      "epoch": 2.1931862175764616,
      "grad_norm": 12.271434783935547,
      "learning_rate": 8.674237536026154e-06,
      "loss": 0.747,
      "step": 5665
    },
    {
      "epoch": 2.1935733643050717,
      "grad_norm": 18.278566360473633,
      "learning_rate": 8.673807372994366e-06,
      "loss": 1.1519,
      "step": 5666
    },
    {
      "epoch": 2.1939605110336817,
      "grad_norm": 33.8089485168457,
      "learning_rate": 8.673377209962577e-06,
      "loss": 1.8597,
      "step": 5667
    },
    {
      "epoch": 2.194347657762292,
      "grad_norm": 18.04059410095215,
      "learning_rate": 8.672947046930787e-06,
      "loss": 1.4154,
      "step": 5668
    },
    {
      "epoch": 2.194734804490902,
      "grad_norm": 33.06069564819336,
      "learning_rate": 8.672516883898998e-06,
      "loss": 1.0316,
      "step": 5669
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 13.443243980407715,
      "learning_rate": 8.67208672086721e-06,
      "loss": 0.7606,
      "step": 5670
    },
    {
      "epoch": 2.1955090979481224,
      "grad_norm": 15.432515144348145,
      "learning_rate": 8.671656557835421e-06,
      "loss": 1.1271,
      "step": 5671
    },
    {
      "epoch": 2.1958962446767325,
      "grad_norm": 26.598005294799805,
      "learning_rate": 8.671226394803631e-06,
      "loss": 2.7933,
      "step": 5672
    },
    {
      "epoch": 2.1962833914053426,
      "grad_norm": 17.571386337280273,
      "learning_rate": 8.670796231771842e-06,
      "loss": 1.4701,
      "step": 5673
    },
    {
      "epoch": 2.1966705381339526,
      "grad_norm": 16.04978370666504,
      "learning_rate": 8.670366068740054e-06,
      "loss": 1.9055,
      "step": 5674
    },
    {
      "epoch": 2.197057684862563,
      "grad_norm": 13.524290084838867,
      "learning_rate": 8.669935905708265e-06,
      "loss": 0.7808,
      "step": 5675
    },
    {
      "epoch": 2.197444831591173,
      "grad_norm": 4.841613292694092,
      "learning_rate": 8.669505742676475e-06,
      "loss": 0.2883,
      "step": 5676
    },
    {
      "epoch": 2.1978319783197833,
      "grad_norm": 26.897075653076172,
      "learning_rate": 8.669075579644686e-06,
      "loss": 1.4645,
      "step": 5677
    },
    {
      "epoch": 2.1982191250483933,
      "grad_norm": 15.567298889160156,
      "learning_rate": 8.668645416612898e-06,
      "loss": 1.1339,
      "step": 5678
    },
    {
      "epoch": 2.1986062717770034,
      "grad_norm": 12.071548461914062,
      "learning_rate": 8.668215253581107e-06,
      "loss": 0.763,
      "step": 5679
    },
    {
      "epoch": 2.1989934185056135,
      "grad_norm": 18.61468505859375,
      "learning_rate": 8.667785090549319e-06,
      "loss": 1.6675,
      "step": 5680
    },
    {
      "epoch": 2.1993805652342235,
      "grad_norm": 18.082298278808594,
      "learning_rate": 8.66735492751753e-06,
      "loss": 1.7247,
      "step": 5681
    },
    {
      "epoch": 2.199767711962834,
      "grad_norm": 31.678302764892578,
      "learning_rate": 8.666924764485742e-06,
      "loss": 1.9421,
      "step": 5682
    },
    {
      "epoch": 2.200154858691444,
      "grad_norm": 37.652366638183594,
      "learning_rate": 8.666494601453951e-06,
      "loss": 3.9971,
      "step": 5683
    },
    {
      "epoch": 2.200542005420054,
      "grad_norm": 28.612701416015625,
      "learning_rate": 8.666064438422163e-06,
      "loss": 0.7292,
      "step": 5684
    },
    {
      "epoch": 2.2009291521486642,
      "grad_norm": 11.818868637084961,
      "learning_rate": 8.665634275390373e-06,
      "loss": 0.772,
      "step": 5685
    },
    {
      "epoch": 2.2013162988772743,
      "grad_norm": 12.406148910522461,
      "learning_rate": 8.665204112358586e-06,
      "loss": 0.5393,
      "step": 5686
    },
    {
      "epoch": 2.201703445605885,
      "grad_norm": 19.71336555480957,
      "learning_rate": 8.664773949326795e-06,
      "loss": 1.4786,
      "step": 5687
    },
    {
      "epoch": 2.202090592334495,
      "grad_norm": 32.739566802978516,
      "learning_rate": 8.664343786295007e-06,
      "loss": 1.4328,
      "step": 5688
    },
    {
      "epoch": 2.202477739063105,
      "grad_norm": 9.46727466583252,
      "learning_rate": 8.663913623263216e-06,
      "loss": 0.45,
      "step": 5689
    },
    {
      "epoch": 2.202864885791715,
      "grad_norm": 3.808995246887207,
      "learning_rate": 8.66348346023143e-06,
      "loss": 0.1162,
      "step": 5690
    },
    {
      "epoch": 2.203252032520325,
      "grad_norm": 22.472515106201172,
      "learning_rate": 8.66305329719964e-06,
      "loss": 1.7183,
      "step": 5691
    },
    {
      "epoch": 2.203639179248935,
      "grad_norm": 19.749523162841797,
      "learning_rate": 8.66262313416785e-06,
      "loss": 0.6862,
      "step": 5692
    },
    {
      "epoch": 2.2040263259775457,
      "grad_norm": 23.209897994995117,
      "learning_rate": 8.66219297113606e-06,
      "loss": 1.6337,
      "step": 5693
    },
    {
      "epoch": 2.2044134727061557,
      "grad_norm": 24.343341827392578,
      "learning_rate": 8.661762808104272e-06,
      "loss": 2.9926,
      "step": 5694
    },
    {
      "epoch": 2.204800619434766,
      "grad_norm": 10.961103439331055,
      "learning_rate": 8.661332645072483e-06,
      "loss": 0.6693,
      "step": 5695
    },
    {
      "epoch": 2.205187766163376,
      "grad_norm": 25.91400909423828,
      "learning_rate": 8.660902482040695e-06,
      "loss": 1.357,
      "step": 5696
    },
    {
      "epoch": 2.205574912891986,
      "grad_norm": 32.4426383972168,
      "learning_rate": 8.660472319008904e-06,
      "loss": 0.8652,
      "step": 5697
    },
    {
      "epoch": 2.205962059620596,
      "grad_norm": 22.473031997680664,
      "learning_rate": 8.660042155977116e-06,
      "loss": 1.9666,
      "step": 5698
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 21.656789779663086,
      "learning_rate": 8.659611992945327e-06,
      "loss": 0.5056,
      "step": 5699
    },
    {
      "epoch": 2.2067363530778166,
      "grad_norm": 13.069352149963379,
      "learning_rate": 8.659181829913537e-06,
      "loss": 0.8431,
      "step": 5700
    },
    {
      "epoch": 2.2071234998064266,
      "grad_norm": 21.653369903564453,
      "learning_rate": 8.658751666881748e-06,
      "loss": 1.6978,
      "step": 5701
    },
    {
      "epoch": 2.2075106465350367,
      "grad_norm": 87.03356170654297,
      "learning_rate": 8.65832150384996e-06,
      "loss": 2.0948,
      "step": 5702
    },
    {
      "epoch": 2.2078977932636468,
      "grad_norm": 24.622404098510742,
      "learning_rate": 8.657891340818171e-06,
      "loss": 1.63,
      "step": 5703
    },
    {
      "epoch": 2.2082849399922573,
      "grad_norm": 29.88750648498535,
      "learning_rate": 8.657461177786381e-06,
      "loss": 1.9582,
      "step": 5704
    },
    {
      "epoch": 2.2086720867208673,
      "grad_norm": 36.71696472167969,
      "learning_rate": 8.657031014754592e-06,
      "loss": 1.3772,
      "step": 5705
    },
    {
      "epoch": 2.2090592334494774,
      "grad_norm": 41.802764892578125,
      "learning_rate": 8.656600851722804e-06,
      "loss": 2.7193,
      "step": 5706
    },
    {
      "epoch": 2.2094463801780875,
      "grad_norm": 19.114620208740234,
      "learning_rate": 8.656170688691015e-06,
      "loss": 2.1807,
      "step": 5707
    },
    {
      "epoch": 2.2098335269066975,
      "grad_norm": 18.313440322875977,
      "learning_rate": 8.655740525659225e-06,
      "loss": 0.9251,
      "step": 5708
    },
    {
      "epoch": 2.2102206736353076,
      "grad_norm": 19.105207443237305,
      "learning_rate": 8.655310362627436e-06,
      "loss": 0.9223,
      "step": 5709
    },
    {
      "epoch": 2.210607820363918,
      "grad_norm": 33.758575439453125,
      "learning_rate": 8.654880199595648e-06,
      "loss": 4.5376,
      "step": 5710
    },
    {
      "epoch": 2.210994967092528,
      "grad_norm": 24.329814910888672,
      "learning_rate": 8.65445003656386e-06,
      "loss": 1.5976,
      "step": 5711
    },
    {
      "epoch": 2.2113821138211383,
      "grad_norm": 14.477089881896973,
      "learning_rate": 8.654019873532069e-06,
      "loss": 0.9947,
      "step": 5712
    },
    {
      "epoch": 2.2117692605497483,
      "grad_norm": 25.732961654663086,
      "learning_rate": 8.65358971050028e-06,
      "loss": 2.5286,
      "step": 5713
    },
    {
      "epoch": 2.2121564072783584,
      "grad_norm": 24.749818801879883,
      "learning_rate": 8.653159547468492e-06,
      "loss": 2.2921,
      "step": 5714
    },
    {
      "epoch": 2.2125435540069684,
      "grad_norm": 18.872201919555664,
      "learning_rate": 8.652729384436701e-06,
      "loss": 1.6774,
      "step": 5715
    },
    {
      "epoch": 2.212930700735579,
      "grad_norm": 25.347068786621094,
      "learning_rate": 8.652299221404913e-06,
      "loss": 1.5672,
      "step": 5716
    },
    {
      "epoch": 2.213317847464189,
      "grad_norm": 19.266460418701172,
      "learning_rate": 8.651869058373124e-06,
      "loss": 0.8372,
      "step": 5717
    },
    {
      "epoch": 2.213704994192799,
      "grad_norm": 20.276844024658203,
      "learning_rate": 8.651438895341336e-06,
      "loss": 1.7973,
      "step": 5718
    },
    {
      "epoch": 2.214092140921409,
      "grad_norm": 13.142781257629395,
      "learning_rate": 8.651008732309545e-06,
      "loss": 1.1716,
      "step": 5719
    },
    {
      "epoch": 2.2144792876500192,
      "grad_norm": 59.192108154296875,
      "learning_rate": 8.650578569277757e-06,
      "loss": 1.5227,
      "step": 5720
    },
    {
      "epoch": 2.2148664343786297,
      "grad_norm": 13.16676139831543,
      "learning_rate": 8.650148406245968e-06,
      "loss": 0.8417,
      "step": 5721
    },
    {
      "epoch": 2.21525358110724,
      "grad_norm": 33.27337646484375,
      "learning_rate": 8.64971824321418e-06,
      "loss": 1.6483,
      "step": 5722
    },
    {
      "epoch": 2.21564072783585,
      "grad_norm": 20.159299850463867,
      "learning_rate": 8.64928808018239e-06,
      "loss": 1.5855,
      "step": 5723
    },
    {
      "epoch": 2.21602787456446,
      "grad_norm": 34.85837936401367,
      "learning_rate": 8.6488579171506e-06,
      "loss": 1.3898,
      "step": 5724
    },
    {
      "epoch": 2.21641502129307,
      "grad_norm": 20.54491424560547,
      "learning_rate": 8.648427754118812e-06,
      "loss": 1.5787,
      "step": 5725
    },
    {
      "epoch": 2.21680216802168,
      "grad_norm": 30.673852920532227,
      "learning_rate": 8.647997591087024e-06,
      "loss": 2.2045,
      "step": 5726
    },
    {
      "epoch": 2.21718931475029,
      "grad_norm": 9.560256958007812,
      "learning_rate": 8.647567428055233e-06,
      "loss": 1.283,
      "step": 5727
    },
    {
      "epoch": 2.2175764614789006,
      "grad_norm": 31.019668579101562,
      "learning_rate": 8.647137265023445e-06,
      "loss": 2.0186,
      "step": 5728
    },
    {
      "epoch": 2.2179636082075107,
      "grad_norm": 13.538093566894531,
      "learning_rate": 8.646707101991656e-06,
      "loss": 1.4496,
      "step": 5729
    },
    {
      "epoch": 2.2183507549361208,
      "grad_norm": 31.030742645263672,
      "learning_rate": 8.646276938959866e-06,
      "loss": 1.6326,
      "step": 5730
    },
    {
      "epoch": 2.218737901664731,
      "grad_norm": 52.9681396484375,
      "learning_rate": 8.645846775928077e-06,
      "loss": 3.1336,
      "step": 5731
    },
    {
      "epoch": 2.219125048393341,
      "grad_norm": 12.63992977142334,
      "learning_rate": 8.645416612896289e-06,
      "loss": 0.9014,
      "step": 5732
    },
    {
      "epoch": 2.2195121951219514,
      "grad_norm": 14.793625831604004,
      "learning_rate": 8.6449864498645e-06,
      "loss": 1.3943,
      "step": 5733
    },
    {
      "epoch": 2.2198993418505615,
      "grad_norm": 36.969085693359375,
      "learning_rate": 8.64455628683271e-06,
      "loss": 1.2107,
      "step": 5734
    },
    {
      "epoch": 2.2202864885791715,
      "grad_norm": 12.268299102783203,
      "learning_rate": 8.644126123800921e-06,
      "loss": 0.9307,
      "step": 5735
    },
    {
      "epoch": 2.2206736353077816,
      "grad_norm": 16.424142837524414,
      "learning_rate": 8.643695960769131e-06,
      "loss": 1.4059,
      "step": 5736
    },
    {
      "epoch": 2.2210607820363917,
      "grad_norm": 14.399559020996094,
      "learning_rate": 8.643265797737344e-06,
      "loss": 0.7709,
      "step": 5737
    },
    {
      "epoch": 2.2214479287650017,
      "grad_norm": 26.307626724243164,
      "learning_rate": 8.642835634705554e-06,
      "loss": 1.3982,
      "step": 5738
    },
    {
      "epoch": 2.2218350754936123,
      "grad_norm": 16.040672302246094,
      "learning_rate": 8.642405471673765e-06,
      "loss": 1.2607,
      "step": 5739
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 19.167251586914062,
      "learning_rate": 8.641975308641975e-06,
      "loss": 1.775,
      "step": 5740
    },
    {
      "epoch": 2.2226093689508324,
      "grad_norm": 14.994040489196777,
      "learning_rate": 8.641545145610188e-06,
      "loss": 0.93,
      "step": 5741
    },
    {
      "epoch": 2.2229965156794425,
      "grad_norm": 25.378543853759766,
      "learning_rate": 8.641114982578398e-06,
      "loss": 1.3416,
      "step": 5742
    },
    {
      "epoch": 2.2233836624080525,
      "grad_norm": 19.573631286621094,
      "learning_rate": 8.64068481954661e-06,
      "loss": 1.5759,
      "step": 5743
    },
    {
      "epoch": 2.2237708091366626,
      "grad_norm": 16.89473533630371,
      "learning_rate": 8.640254656514819e-06,
      "loss": 1.2447,
      "step": 5744
    },
    {
      "epoch": 2.224157955865273,
      "grad_norm": 17.17972755432129,
      "learning_rate": 8.63982449348303e-06,
      "loss": 1.7423,
      "step": 5745
    },
    {
      "epoch": 2.224545102593883,
      "grad_norm": 15.37057113647461,
      "learning_rate": 8.639394330451242e-06,
      "loss": 1.4475,
      "step": 5746
    },
    {
      "epoch": 2.2249322493224932,
      "grad_norm": 8.533224105834961,
      "learning_rate": 8.638964167419453e-06,
      "loss": 1.2683,
      "step": 5747
    },
    {
      "epoch": 2.2253193960511033,
      "grad_norm": 17.847293853759766,
      "learning_rate": 8.638534004387663e-06,
      "loss": 1.5367,
      "step": 5748
    },
    {
      "epoch": 2.2257065427797134,
      "grad_norm": 25.17490005493164,
      "learning_rate": 8.638103841355874e-06,
      "loss": 2.3021,
      "step": 5749
    },
    {
      "epoch": 2.226093689508324,
      "grad_norm": 13.160557746887207,
      "learning_rate": 8.637673678324086e-06,
      "loss": 0.8879,
      "step": 5750
    },
    {
      "epoch": 2.226480836236934,
      "grad_norm": 27.700340270996094,
      "learning_rate": 8.637243515292295e-06,
      "loss": 1.3721,
      "step": 5751
    },
    {
      "epoch": 2.226867982965544,
      "grad_norm": 19.532947540283203,
      "learning_rate": 8.636813352260509e-06,
      "loss": 1.5582,
      "step": 5752
    },
    {
      "epoch": 2.227255129694154,
      "grad_norm": 46.181339263916016,
      "learning_rate": 8.636383189228718e-06,
      "loss": 2.3688,
      "step": 5753
    },
    {
      "epoch": 2.227642276422764,
      "grad_norm": 22.301044464111328,
      "learning_rate": 8.63595302619693e-06,
      "loss": 1.9655,
      "step": 5754
    },
    {
      "epoch": 2.228029423151374,
      "grad_norm": 25.20313262939453,
      "learning_rate": 8.63552286316514e-06,
      "loss": 1.384,
      "step": 5755
    },
    {
      "epoch": 2.2284165698799847,
      "grad_norm": 22.572614669799805,
      "learning_rate": 8.635092700133353e-06,
      "loss": 1.5164,
      "step": 5756
    },
    {
      "epoch": 2.2288037166085948,
      "grad_norm": 34.91500473022461,
      "learning_rate": 8.634662537101562e-06,
      "loss": 1.3274,
      "step": 5757
    },
    {
      "epoch": 2.229190863337205,
      "grad_norm": 47.200199127197266,
      "learning_rate": 8.634232374069774e-06,
      "loss": 1.3352,
      "step": 5758
    },
    {
      "epoch": 2.229578010065815,
      "grad_norm": 26.68342399597168,
      "learning_rate": 8.633802211037983e-06,
      "loss": 1.6246,
      "step": 5759
    },
    {
      "epoch": 2.229965156794425,
      "grad_norm": 7.146963119506836,
      "learning_rate": 8.633372048006195e-06,
      "loss": 0.354,
      "step": 5760
    },
    {
      "epoch": 2.230352303523035,
      "grad_norm": 14.549541473388672,
      "learning_rate": 8.632941884974406e-06,
      "loss": 1.3679,
      "step": 5761
    },
    {
      "epoch": 2.2307394502516456,
      "grad_norm": 44.43584442138672,
      "learning_rate": 8.632511721942618e-06,
      "loss": 1.6165,
      "step": 5762
    },
    {
      "epoch": 2.2311265969802556,
      "grad_norm": 13.390754699707031,
      "learning_rate": 8.632081558910827e-06,
      "loss": 0.3281,
      "step": 5763
    },
    {
      "epoch": 2.2315137437088657,
      "grad_norm": 22.669113159179688,
      "learning_rate": 8.631651395879039e-06,
      "loss": 0.9952,
      "step": 5764
    },
    {
      "epoch": 2.2319008904374757,
      "grad_norm": 20.483312606811523,
      "learning_rate": 8.63122123284725e-06,
      "loss": 1.6449,
      "step": 5765
    },
    {
      "epoch": 2.232288037166086,
      "grad_norm": 25.19856834411621,
      "learning_rate": 8.63079106981546e-06,
      "loss": 1.2542,
      "step": 5766
    },
    {
      "epoch": 2.2326751838946963,
      "grad_norm": 14.552696228027344,
      "learning_rate": 8.630360906783671e-06,
      "loss": 1.1668,
      "step": 5767
    },
    {
      "epoch": 2.2330623306233064,
      "grad_norm": 15.986563682556152,
      "learning_rate": 8.629930743751883e-06,
      "loss": 1.379,
      "step": 5768
    },
    {
      "epoch": 2.2334494773519165,
      "grad_norm": 14.544148445129395,
      "learning_rate": 8.629500580720094e-06,
      "loss": 0.6936,
      "step": 5769
    },
    {
      "epoch": 2.2338366240805265,
      "grad_norm": 27.574018478393555,
      "learning_rate": 8.629070417688304e-06,
      "loss": 1.4759,
      "step": 5770
    },
    {
      "epoch": 2.2342237708091366,
      "grad_norm": 14.483154296875,
      "learning_rate": 8.628640254656515e-06,
      "loss": 1.1451,
      "step": 5771
    },
    {
      "epoch": 2.2346109175377467,
      "grad_norm": 15.774869918823242,
      "learning_rate": 8.628210091624727e-06,
      "loss": 0.8971,
      "step": 5772
    },
    {
      "epoch": 2.2349980642663567,
      "grad_norm": 27.20709800720215,
      "learning_rate": 8.627779928592938e-06,
      "loss": 1.5739,
      "step": 5773
    },
    {
      "epoch": 2.2353852109949672,
      "grad_norm": 12.519439697265625,
      "learning_rate": 8.627349765561148e-06,
      "loss": 0.9026,
      "step": 5774
    },
    {
      "epoch": 2.2357723577235773,
      "grad_norm": 13.538370132446289,
      "learning_rate": 8.62691960252936e-06,
      "loss": 1.3588,
      "step": 5775
    },
    {
      "epoch": 2.2361595044521874,
      "grad_norm": 19.77648162841797,
      "learning_rate": 8.62648943949757e-06,
      "loss": 1.2101,
      "step": 5776
    },
    {
      "epoch": 2.2365466511807974,
      "grad_norm": 32.62482452392578,
      "learning_rate": 8.626059276465782e-06,
      "loss": 1.1775,
      "step": 5777
    },
    {
      "epoch": 2.2369337979094075,
      "grad_norm": 17.96583366394043,
      "learning_rate": 8.625629113433992e-06,
      "loss": 0.8201,
      "step": 5778
    },
    {
      "epoch": 2.237320944638018,
      "grad_norm": 25.0937442779541,
      "learning_rate": 8.625198950402203e-06,
      "loss": 1.3689,
      "step": 5779
    },
    {
      "epoch": 2.237708091366628,
      "grad_norm": 26.901123046875,
      "learning_rate": 8.624768787370415e-06,
      "loss": 1.3778,
      "step": 5780
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 34.301361083984375,
      "learning_rate": 8.624338624338624e-06,
      "loss": 1.949,
      "step": 5781
    },
    {
      "epoch": 2.238482384823848,
      "grad_norm": 18.918291091918945,
      "learning_rate": 8.623908461306836e-06,
      "loss": 1.5365,
      "step": 5782
    },
    {
      "epoch": 2.2388695315524583,
      "grad_norm": 15.797140121459961,
      "learning_rate": 8.623478298275047e-06,
      "loss": 1.4596,
      "step": 5783
    },
    {
      "epoch": 2.2392566782810683,
      "grad_norm": 15.13597583770752,
      "learning_rate": 8.623048135243259e-06,
      "loss": 0.9174,
      "step": 5784
    },
    {
      "epoch": 2.239643825009679,
      "grad_norm": 22.818689346313477,
      "learning_rate": 8.622617972211468e-06,
      "loss": 2.9998,
      "step": 5785
    },
    {
      "epoch": 2.240030971738289,
      "grad_norm": 33.063865661621094,
      "learning_rate": 8.62218780917968e-06,
      "loss": 2.0253,
      "step": 5786
    },
    {
      "epoch": 2.240418118466899,
      "grad_norm": 24.026220321655273,
      "learning_rate": 8.62175764614789e-06,
      "loss": 1.0644,
      "step": 5787
    },
    {
      "epoch": 2.240805265195509,
      "grad_norm": 18.59556007385254,
      "learning_rate": 8.621327483116103e-06,
      "loss": 0.8578,
      "step": 5788
    },
    {
      "epoch": 2.241192411924119,
      "grad_norm": 10.73974323272705,
      "learning_rate": 8.620897320084312e-06,
      "loss": 0.5646,
      "step": 5789
    },
    {
      "epoch": 2.241579558652729,
      "grad_norm": 20.8892765045166,
      "learning_rate": 8.620467157052524e-06,
      "loss": 1.6831,
      "step": 5790
    },
    {
      "epoch": 2.2419667053813397,
      "grad_norm": 28.32622528076172,
      "learning_rate": 8.620036994020733e-06,
      "loss": 1.6389,
      "step": 5791
    },
    {
      "epoch": 2.2423538521099498,
      "grad_norm": 5.027558326721191,
      "learning_rate": 8.619606830988947e-06,
      "loss": 0.2931,
      "step": 5792
    },
    {
      "epoch": 2.24274099883856,
      "grad_norm": 20.259267807006836,
      "learning_rate": 8.619176667957156e-06,
      "loss": 1.2709,
      "step": 5793
    },
    {
      "epoch": 2.24312814556717,
      "grad_norm": 14.222272872924805,
      "learning_rate": 8.618746504925368e-06,
      "loss": 1.1416,
      "step": 5794
    },
    {
      "epoch": 2.24351529229578,
      "grad_norm": 18.70956802368164,
      "learning_rate": 8.618316341893579e-06,
      "loss": 1.6727,
      "step": 5795
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 23.130718231201172,
      "learning_rate": 8.617886178861789e-06,
      "loss": 1.8411,
      "step": 5796
    },
    {
      "epoch": 2.2442895857530005,
      "grad_norm": 40.3341178894043,
      "learning_rate": 8.61745601583e-06,
      "loss": 1.9894,
      "step": 5797
    },
    {
      "epoch": 2.2446767324816106,
      "grad_norm": 21.988264083862305,
      "learning_rate": 8.617025852798212e-06,
      "loss": 1.7375,
      "step": 5798
    },
    {
      "epoch": 2.2450638792102207,
      "grad_norm": 41.746559143066406,
      "learning_rate": 8.616595689766423e-06,
      "loss": 0.5981,
      "step": 5799
    },
    {
      "epoch": 2.2454510259388307,
      "grad_norm": 31.200807571411133,
      "learning_rate": 8.616165526734633e-06,
      "loss": 1.2145,
      "step": 5800
    },
    {
      "epoch": 2.245838172667441,
      "grad_norm": 27.233867645263672,
      "learning_rate": 8.615735363702844e-06,
      "loss": 3.7272,
      "step": 5801
    },
    {
      "epoch": 2.2462253193960513,
      "grad_norm": 16.573619842529297,
      "learning_rate": 8.615305200671054e-06,
      "loss": 1.5187,
      "step": 5802
    },
    {
      "epoch": 2.2466124661246614,
      "grad_norm": 14.343463897705078,
      "learning_rate": 8.614875037639267e-06,
      "loss": 0.9052,
      "step": 5803
    },
    {
      "epoch": 2.2469996128532714,
      "grad_norm": 22.169340133666992,
      "learning_rate": 8.614444874607477e-06,
      "loss": 1.8583,
      "step": 5804
    },
    {
      "epoch": 2.2473867595818815,
      "grad_norm": 16.833972930908203,
      "learning_rate": 8.614014711575688e-06,
      "loss": 1.0662,
      "step": 5805
    },
    {
      "epoch": 2.2477739063104916,
      "grad_norm": 17.556900024414062,
      "learning_rate": 8.613584548543898e-06,
      "loss": 1.7206,
      "step": 5806
    },
    {
      "epoch": 2.2481610530391016,
      "grad_norm": 15.90139389038086,
      "learning_rate": 8.613154385512111e-06,
      "loss": 1.0386,
      "step": 5807
    },
    {
      "epoch": 2.248548199767712,
      "grad_norm": 26.998811721801758,
      "learning_rate": 8.61272422248032e-06,
      "loss": 1.4785,
      "step": 5808
    },
    {
      "epoch": 2.248935346496322,
      "grad_norm": 30.742525100708008,
      "learning_rate": 8.612294059448532e-06,
      "loss": 1.3066,
      "step": 5809
    },
    {
      "epoch": 2.2493224932249323,
      "grad_norm": 22.543323516845703,
      "learning_rate": 8.611863896416742e-06,
      "loss": 1.9727,
      "step": 5810
    },
    {
      "epoch": 2.2497096399535423,
      "grad_norm": 21.355884552001953,
      "learning_rate": 8.611433733384953e-06,
      "loss": 1.4427,
      "step": 5811
    },
    {
      "epoch": 2.2500967866821524,
      "grad_norm": 24.761812210083008,
      "learning_rate": 8.611003570353165e-06,
      "loss": 1.3406,
      "step": 5812
    },
    {
      "epoch": 2.250483933410763,
      "grad_norm": 13.608951568603516,
      "learning_rate": 8.610573407321376e-06,
      "loss": 1.3094,
      "step": 5813
    },
    {
      "epoch": 2.250871080139373,
      "grad_norm": 28.321794509887695,
      "learning_rate": 8.610143244289586e-06,
      "loss": 1.443,
      "step": 5814
    },
    {
      "epoch": 2.251258226867983,
      "grad_norm": 18.438037872314453,
      "learning_rate": 8.609713081257797e-06,
      "loss": 0.9389,
      "step": 5815
    },
    {
      "epoch": 2.251645373596593,
      "grad_norm": 15.45367431640625,
      "learning_rate": 8.609282918226009e-06,
      "loss": 1.4604,
      "step": 5816
    },
    {
      "epoch": 2.252032520325203,
      "grad_norm": 29.697111129760742,
      "learning_rate": 8.608852755194218e-06,
      "loss": 1.6964,
      "step": 5817
    },
    {
      "epoch": 2.2524196670538132,
      "grad_norm": 22.036439895629883,
      "learning_rate": 8.60842259216243e-06,
      "loss": 2.3696,
      "step": 5818
    },
    {
      "epoch": 2.2528068137824233,
      "grad_norm": 14.359818458557129,
      "learning_rate": 8.607992429130641e-06,
      "loss": 1.3535,
      "step": 5819
    },
    {
      "epoch": 2.253193960511034,
      "grad_norm": 19.69683837890625,
      "learning_rate": 8.607562266098853e-06,
      "loss": 1.6818,
      "step": 5820
    },
    {
      "epoch": 2.253581107239644,
      "grad_norm": 16.906143188476562,
      "learning_rate": 8.607132103067062e-06,
      "loss": 1.5011,
      "step": 5821
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 15.37514591217041,
      "learning_rate": 8.606701940035274e-06,
      "loss": 1.0552,
      "step": 5822
    },
    {
      "epoch": 2.254355400696864,
      "grad_norm": 21.444181442260742,
      "learning_rate": 8.606271777003485e-06,
      "loss": 1.9527,
      "step": 5823
    },
    {
      "epoch": 2.254742547425474,
      "grad_norm": 15.414813995361328,
      "learning_rate": 8.605841613971697e-06,
      "loss": 1.2435,
      "step": 5824
    },
    {
      "epoch": 2.2551296941540846,
      "grad_norm": 17.8336238861084,
      "learning_rate": 8.605411450939906e-06,
      "loss": 1.2223,
      "step": 5825
    },
    {
      "epoch": 2.2555168408826947,
      "grad_norm": 24.911949157714844,
      "learning_rate": 8.604981287908118e-06,
      "loss": 1.5399,
      "step": 5826
    },
    {
      "epoch": 2.2559039876113047,
      "grad_norm": 21.87189483642578,
      "learning_rate": 8.60455112487633e-06,
      "loss": 1.9043,
      "step": 5827
    },
    {
      "epoch": 2.256291134339915,
      "grad_norm": 21.627487182617188,
      "learning_rate": 8.60412096184454e-06,
      "loss": 1.2587,
      "step": 5828
    },
    {
      "epoch": 2.256678281068525,
      "grad_norm": 25.9980411529541,
      "learning_rate": 8.60369079881275e-06,
      "loss": 0.8685,
      "step": 5829
    },
    {
      "epoch": 2.257065427797135,
      "grad_norm": 31.177148818969727,
      "learning_rate": 8.603260635780962e-06,
      "loss": 2.062,
      "step": 5830
    },
    {
      "epoch": 2.2574525745257454,
      "grad_norm": 21.72126579284668,
      "learning_rate": 8.602830472749173e-06,
      "loss": 2.0085,
      "step": 5831
    },
    {
      "epoch": 2.2578397212543555,
      "grad_norm": 14.251598358154297,
      "learning_rate": 8.602400309717383e-06,
      "loss": 1.165,
      "step": 5832
    },
    {
      "epoch": 2.2582268679829656,
      "grad_norm": 19.698396682739258,
      "learning_rate": 8.601970146685594e-06,
      "loss": 1.6336,
      "step": 5833
    },
    {
      "epoch": 2.2586140147115756,
      "grad_norm": 18.10692596435547,
      "learning_rate": 8.601539983653806e-06,
      "loss": 1.6023,
      "step": 5834
    },
    {
      "epoch": 2.2590011614401857,
      "grad_norm": 20.29986572265625,
      "learning_rate": 8.601109820622017e-06,
      "loss": 1.9524,
      "step": 5835
    },
    {
      "epoch": 2.2593883081687958,
      "grad_norm": 39.18864440917969,
      "learning_rate": 8.600679657590227e-06,
      "loss": 2.9172,
      "step": 5836
    },
    {
      "epoch": 2.2597754548974063,
      "grad_norm": 25.739938735961914,
      "learning_rate": 8.600249494558438e-06,
      "loss": 1.9673,
      "step": 5837
    },
    {
      "epoch": 2.2601626016260163,
      "grad_norm": 10.219795227050781,
      "learning_rate": 8.59981933152665e-06,
      "loss": 1.1709,
      "step": 5838
    },
    {
      "epoch": 2.2605497483546264,
      "grad_norm": 12.849098205566406,
      "learning_rate": 8.599389168494861e-06,
      "loss": 0.8068,
      "step": 5839
    },
    {
      "epoch": 2.2609368950832365,
      "grad_norm": 30.379554748535156,
      "learning_rate": 8.59895900546307e-06,
      "loss": 0.7817,
      "step": 5840
    },
    {
      "epoch": 2.2613240418118465,
      "grad_norm": 22.571916580200195,
      "learning_rate": 8.598528842431282e-06,
      "loss": 1.3225,
      "step": 5841
    },
    {
      "epoch": 2.261711188540457,
      "grad_norm": 85.0419692993164,
      "learning_rate": 8.598098679399494e-06,
      "loss": 0.7242,
      "step": 5842
    },
    {
      "epoch": 2.262098335269067,
      "grad_norm": 15.76247787475586,
      "learning_rate": 8.597668516367705e-06,
      "loss": 1.1727,
      "step": 5843
    },
    {
      "epoch": 2.262485481997677,
      "grad_norm": 45.483760833740234,
      "learning_rate": 8.597238353335915e-06,
      "loss": 2.7498,
      "step": 5844
    },
    {
      "epoch": 2.2628726287262872,
      "grad_norm": 21.30520248413086,
      "learning_rate": 8.596808190304126e-06,
      "loss": 1.9976,
      "step": 5845
    },
    {
      "epoch": 2.2632597754548973,
      "grad_norm": 22.859098434448242,
      "learning_rate": 8.596378027272338e-06,
      "loss": 0.877,
      "step": 5846
    },
    {
      "epoch": 2.2636469221835074,
      "grad_norm": 22.26677894592285,
      "learning_rate": 8.595947864240547e-06,
      "loss": 1.4041,
      "step": 5847
    },
    {
      "epoch": 2.2640340689121174,
      "grad_norm": 15.843491554260254,
      "learning_rate": 8.595517701208759e-06,
      "loss": 1.1794,
      "step": 5848
    },
    {
      "epoch": 2.264421215640728,
      "grad_norm": 36.819969177246094,
      "learning_rate": 8.59508753817697e-06,
      "loss": 1.5475,
      "step": 5849
    },
    {
      "epoch": 2.264808362369338,
      "grad_norm": 22.88143539428711,
      "learning_rate": 8.594657375145182e-06,
      "loss": 1.8102,
      "step": 5850
    },
    {
      "epoch": 2.265195509097948,
      "grad_norm": 31.58234405517578,
      "learning_rate": 8.594227212113391e-06,
      "loss": 2.2531,
      "step": 5851
    },
    {
      "epoch": 2.265582655826558,
      "grad_norm": 29.171945571899414,
      "learning_rate": 8.593797049081603e-06,
      "loss": 2.4836,
      "step": 5852
    },
    {
      "epoch": 2.2659698025551682,
      "grad_norm": 52.59333038330078,
      "learning_rate": 8.593366886049812e-06,
      "loss": 1.0588,
      "step": 5853
    },
    {
      "epoch": 2.2663569492837787,
      "grad_norm": 3.6480112075805664,
      "learning_rate": 8.592936723018026e-06,
      "loss": 0.1181,
      "step": 5854
    },
    {
      "epoch": 2.266744096012389,
      "grad_norm": 14.072564125061035,
      "learning_rate": 8.592506559986235e-06,
      "loss": 1.228,
      "step": 5855
    },
    {
      "epoch": 2.267131242740999,
      "grad_norm": 20.77191925048828,
      "learning_rate": 8.592076396954447e-06,
      "loss": 1.4808,
      "step": 5856
    },
    {
      "epoch": 2.267518389469609,
      "grad_norm": 25.5130672454834,
      "learning_rate": 8.591646233922656e-06,
      "loss": 1.6461,
      "step": 5857
    },
    {
      "epoch": 2.267905536198219,
      "grad_norm": 15.93921184539795,
      "learning_rate": 8.59121607089087e-06,
      "loss": 0.6608,
      "step": 5858
    },
    {
      "epoch": 2.2682926829268295,
      "grad_norm": 23.193702697753906,
      "learning_rate": 8.59078590785908e-06,
      "loss": 2.0176,
      "step": 5859
    },
    {
      "epoch": 2.2686798296554396,
      "grad_norm": 32.6268310546875,
      "learning_rate": 8.59035574482729e-06,
      "loss": 1.8986,
      "step": 5860
    },
    {
      "epoch": 2.2690669763840496,
      "grad_norm": 18.901098251342773,
      "learning_rate": 8.5899255817955e-06,
      "loss": 1.3976,
      "step": 5861
    },
    {
      "epoch": 2.2694541231126597,
      "grad_norm": 15.795952796936035,
      "learning_rate": 8.589495418763712e-06,
      "loss": 0.8434,
      "step": 5862
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 27.013643264770508,
      "learning_rate": 8.589065255731923e-06,
      "loss": 1.9279,
      "step": 5863
    },
    {
      "epoch": 2.27022841656988,
      "grad_norm": 35.958316802978516,
      "learning_rate": 8.588635092700135e-06,
      "loss": 2.0167,
      "step": 5864
    },
    {
      "epoch": 2.27061556329849,
      "grad_norm": 17.57177734375,
      "learning_rate": 8.588204929668344e-06,
      "loss": 1.6602,
      "step": 5865
    },
    {
      "epoch": 2.2710027100271004,
      "grad_norm": 21.85748863220215,
      "learning_rate": 8.587774766636556e-06,
      "loss": 2.6685,
      "step": 5866
    },
    {
      "epoch": 2.2713898567557105,
      "grad_norm": 14.71509075164795,
      "learning_rate": 8.587344603604767e-06,
      "loss": 0.4961,
      "step": 5867
    },
    {
      "epoch": 2.2717770034843205,
      "grad_norm": 39.267601013183594,
      "learning_rate": 8.586914440572977e-06,
      "loss": 2.3818,
      "step": 5868
    },
    {
      "epoch": 2.2721641502129306,
      "grad_norm": 13.936033248901367,
      "learning_rate": 8.586484277541188e-06,
      "loss": 1.288,
      "step": 5869
    },
    {
      "epoch": 2.2725512969415407,
      "grad_norm": 29.54446792602539,
      "learning_rate": 8.5860541145094e-06,
      "loss": 1.3296,
      "step": 5870
    },
    {
      "epoch": 2.272938443670151,
      "grad_norm": 12.819576263427734,
      "learning_rate": 8.585623951477611e-06,
      "loss": 1.4783,
      "step": 5871
    },
    {
      "epoch": 2.2733255903987613,
      "grad_norm": 32.493370056152344,
      "learning_rate": 8.585193788445821e-06,
      "loss": 2.2686,
      "step": 5872
    },
    {
      "epoch": 2.2737127371273713,
      "grad_norm": 25.947229385375977,
      "learning_rate": 8.584763625414032e-06,
      "loss": 1.5394,
      "step": 5873
    },
    {
      "epoch": 2.2740998838559814,
      "grad_norm": 14.014128684997559,
      "learning_rate": 8.584333462382244e-06,
      "loss": 1.0157,
      "step": 5874
    },
    {
      "epoch": 2.2744870305845915,
      "grad_norm": 16.816097259521484,
      "learning_rate": 8.583903299350455e-06,
      "loss": 1.6174,
      "step": 5875
    },
    {
      "epoch": 2.2748741773132015,
      "grad_norm": 23.577024459838867,
      "learning_rate": 8.583473136318665e-06,
      "loss": 1.4619,
      "step": 5876
    },
    {
      "epoch": 2.275261324041812,
      "grad_norm": 29.178550720214844,
      "learning_rate": 8.583042973286876e-06,
      "loss": 0.9721,
      "step": 5877
    },
    {
      "epoch": 2.275648470770422,
      "grad_norm": 4.658807277679443,
      "learning_rate": 8.582612810255088e-06,
      "loss": 0.14,
      "step": 5878
    },
    {
      "epoch": 2.276035617499032,
      "grad_norm": 34.85592269897461,
      "learning_rate": 8.582182647223299e-06,
      "loss": 1.5508,
      "step": 5879
    },
    {
      "epoch": 2.2764227642276422,
      "grad_norm": 17.244356155395508,
      "learning_rate": 8.581752484191509e-06,
      "loss": 1.1349,
      "step": 5880
    },
    {
      "epoch": 2.2768099109562523,
      "grad_norm": 17.90730094909668,
      "learning_rate": 8.58132232115972e-06,
      "loss": 1.0498,
      "step": 5881
    },
    {
      "epoch": 2.2771970576848624,
      "grad_norm": 7.900460243225098,
      "learning_rate": 8.580892158127932e-06,
      "loss": 0.4143,
      "step": 5882
    },
    {
      "epoch": 2.277584204413473,
      "grad_norm": 25.346221923828125,
      "learning_rate": 8.580461995096141e-06,
      "loss": 1.5393,
      "step": 5883
    },
    {
      "epoch": 2.277971351142083,
      "grad_norm": 24.37812042236328,
      "learning_rate": 8.580031832064353e-06,
      "loss": 0.7257,
      "step": 5884
    },
    {
      "epoch": 2.278358497870693,
      "grad_norm": 17.1657772064209,
      "learning_rate": 8.579601669032564e-06,
      "loss": 1.3481,
      "step": 5885
    },
    {
      "epoch": 2.278745644599303,
      "grad_norm": 30.94816780090332,
      "learning_rate": 8.579171506000776e-06,
      "loss": 1.4961,
      "step": 5886
    },
    {
      "epoch": 2.279132791327913,
      "grad_norm": 43.44512176513672,
      "learning_rate": 8.578741342968985e-06,
      "loss": 1.5301,
      "step": 5887
    },
    {
      "epoch": 2.2795199380565236,
      "grad_norm": 21.499988555908203,
      "learning_rate": 8.578311179937197e-06,
      "loss": 1.5503,
      "step": 5888
    },
    {
      "epoch": 2.2799070847851337,
      "grad_norm": 29.01264190673828,
      "learning_rate": 8.577881016905408e-06,
      "loss": 1.8049,
      "step": 5889
    },
    {
      "epoch": 2.2802942315137438,
      "grad_norm": 20.69333267211914,
      "learning_rate": 8.57745085387362e-06,
      "loss": 1.3214,
      "step": 5890
    },
    {
      "epoch": 2.280681378242354,
      "grad_norm": 29.592679977416992,
      "learning_rate": 8.57702069084183e-06,
      "loss": 1.682,
      "step": 5891
    },
    {
      "epoch": 2.281068524970964,
      "grad_norm": 17.429492950439453,
      "learning_rate": 8.57659052781004e-06,
      "loss": 1.0066,
      "step": 5892
    },
    {
      "epoch": 2.281455671699574,
      "grad_norm": 15.0574951171875,
      "learning_rate": 8.576160364778252e-06,
      "loss": 0.8904,
      "step": 5893
    },
    {
      "epoch": 2.281842818428184,
      "grad_norm": 10.715339660644531,
      "learning_rate": 8.575730201746464e-06,
      "loss": 0.5159,
      "step": 5894
    },
    {
      "epoch": 2.2822299651567945,
      "grad_norm": 26.99582862854004,
      "learning_rate": 8.575300038714673e-06,
      "loss": 1.5895,
      "step": 5895
    },
    {
      "epoch": 2.2826171118854046,
      "grad_norm": 29.703136444091797,
      "learning_rate": 8.574869875682885e-06,
      "loss": 1.409,
      "step": 5896
    },
    {
      "epoch": 2.2830042586140147,
      "grad_norm": 6.702640056610107,
      "learning_rate": 8.574439712651096e-06,
      "loss": 0.1911,
      "step": 5897
    },
    {
      "epoch": 2.2833914053426247,
      "grad_norm": 18.711711883544922,
      "learning_rate": 8.574009549619306e-06,
      "loss": 1.6347,
      "step": 5898
    },
    {
      "epoch": 2.283778552071235,
      "grad_norm": 43.24456787109375,
      "learning_rate": 8.573579386587517e-06,
      "loss": 1.4636,
      "step": 5899
    },
    {
      "epoch": 2.2841656987998453,
      "grad_norm": 28.43694496154785,
      "learning_rate": 8.573149223555729e-06,
      "loss": 1.2414,
      "step": 5900
    },
    {
      "epoch": 2.2845528455284554,
      "grad_norm": 17.347755432128906,
      "learning_rate": 8.57271906052394e-06,
      "loss": 1.4425,
      "step": 5901
    },
    {
      "epoch": 2.2849399922570655,
      "grad_norm": 17.794940948486328,
      "learning_rate": 8.57228889749215e-06,
      "loss": 1.1395,
      "step": 5902
    },
    {
      "epoch": 2.2853271389856755,
      "grad_norm": 24.806188583374023,
      "learning_rate": 8.571858734460361e-06,
      "loss": 2.0695,
      "step": 5903
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 14.560234069824219,
      "learning_rate": 8.571428571428571e-06,
      "loss": 0.789,
      "step": 5904
    },
    {
      "epoch": 2.286101432442896,
      "grad_norm": 25.6607666015625,
      "learning_rate": 8.570998408396784e-06,
      "loss": 2.6928,
      "step": 5905
    },
    {
      "epoch": 2.286488579171506,
      "grad_norm": 23.97498893737793,
      "learning_rate": 8.570568245364994e-06,
      "loss": 0.9866,
      "step": 5906
    },
    {
      "epoch": 2.2868757259001162,
      "grad_norm": 18.379764556884766,
      "learning_rate": 8.570138082333205e-06,
      "loss": 1.0514,
      "step": 5907
    },
    {
      "epoch": 2.2872628726287263,
      "grad_norm": 35.645015716552734,
      "learning_rate": 8.569707919301415e-06,
      "loss": 1.6429,
      "step": 5908
    },
    {
      "epoch": 2.2876500193573364,
      "grad_norm": 22.296401977539062,
      "learning_rate": 8.569277756269628e-06,
      "loss": 1.8444,
      "step": 5909
    },
    {
      "epoch": 2.2880371660859464,
      "grad_norm": 24.638612747192383,
      "learning_rate": 8.568847593237838e-06,
      "loss": 1.6795,
      "step": 5910
    },
    {
      "epoch": 2.2884243128145565,
      "grad_norm": 27.832910537719727,
      "learning_rate": 8.568417430206049e-06,
      "loss": 1.754,
      "step": 5911
    },
    {
      "epoch": 2.288811459543167,
      "grad_norm": 40.753639221191406,
      "learning_rate": 8.567987267174259e-06,
      "loss": 1.0499,
      "step": 5912
    },
    {
      "epoch": 2.289198606271777,
      "grad_norm": 18.270151138305664,
      "learning_rate": 8.56755710414247e-06,
      "loss": 1.3979,
      "step": 5913
    },
    {
      "epoch": 2.289585753000387,
      "grad_norm": 25.64008331298828,
      "learning_rate": 8.567126941110682e-06,
      "loss": 1.4793,
      "step": 5914
    },
    {
      "epoch": 2.289972899728997,
      "grad_norm": 22.057111740112305,
      "learning_rate": 8.566696778078893e-06,
      "loss": 1.7741,
      "step": 5915
    },
    {
      "epoch": 2.2903600464576073,
      "grad_norm": 27.668399810791016,
      "learning_rate": 8.566266615047105e-06,
      "loss": 1.4326,
      "step": 5916
    },
    {
      "epoch": 2.290747193186218,
      "grad_norm": 14.242630958557129,
      "learning_rate": 8.565836452015314e-06,
      "loss": 1.4488,
      "step": 5917
    },
    {
      "epoch": 2.291134339914828,
      "grad_norm": 19.178951263427734,
      "learning_rate": 8.565406288983526e-06,
      "loss": 0.6663,
      "step": 5918
    },
    {
      "epoch": 2.291521486643438,
      "grad_norm": 14.272316932678223,
      "learning_rate": 8.564976125951735e-06,
      "loss": 0.9713,
      "step": 5919
    },
    {
      "epoch": 2.291908633372048,
      "grad_norm": 18.88576889038086,
      "learning_rate": 8.564545962919948e-06,
      "loss": 1.4723,
      "step": 5920
    },
    {
      "epoch": 2.292295780100658,
      "grad_norm": 19.24614715576172,
      "learning_rate": 8.564115799888158e-06,
      "loss": 1.5263,
      "step": 5921
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 19.383962631225586,
      "learning_rate": 8.56368563685637e-06,
      "loss": 1.3854,
      "step": 5922
    },
    {
      "epoch": 2.2930700735578786,
      "grad_norm": 15.138973236083984,
      "learning_rate": 8.56325547382458e-06,
      "loss": 1.1169,
      "step": 5923
    },
    {
      "epoch": 2.2934572202864887,
      "grad_norm": 12.800238609313965,
      "learning_rate": 8.562825310792792e-06,
      "loss": 1.1677,
      "step": 5924
    },
    {
      "epoch": 2.2938443670150988,
      "grad_norm": 25.966190338134766,
      "learning_rate": 8.562395147761002e-06,
      "loss": 2.343,
      "step": 5925
    },
    {
      "epoch": 2.294231513743709,
      "grad_norm": 40.625755310058594,
      "learning_rate": 8.561964984729214e-06,
      "loss": 1.4016,
      "step": 5926
    },
    {
      "epoch": 2.294618660472319,
      "grad_norm": 15.58193302154541,
      "learning_rate": 8.561534821697423e-06,
      "loss": 1.0292,
      "step": 5927
    },
    {
      "epoch": 2.295005807200929,
      "grad_norm": 11.691841125488281,
      "learning_rate": 8.561104658665635e-06,
      "loss": 0.7751,
      "step": 5928
    },
    {
      "epoch": 2.2953929539295395,
      "grad_norm": 13.226194381713867,
      "learning_rate": 8.560674495633846e-06,
      "loss": 1.0705,
      "step": 5929
    },
    {
      "epoch": 2.2957801006581495,
      "grad_norm": 9.521918296813965,
      "learning_rate": 8.560244332602058e-06,
      "loss": 0.4957,
      "step": 5930
    },
    {
      "epoch": 2.2961672473867596,
      "grad_norm": 13.955345153808594,
      "learning_rate": 8.559814169570267e-06,
      "loss": 1.151,
      "step": 5931
    },
    {
      "epoch": 2.2965543941153697,
      "grad_norm": 20.08509635925293,
      "learning_rate": 8.559384006538479e-06,
      "loss": 1.8241,
      "step": 5932
    },
    {
      "epoch": 2.2969415408439797,
      "grad_norm": 22.820274353027344,
      "learning_rate": 8.55895384350669e-06,
      "loss": 1.3219,
      "step": 5933
    },
    {
      "epoch": 2.2973286875725902,
      "grad_norm": 15.127276420593262,
      "learning_rate": 8.5585236804749e-06,
      "loss": 1.392,
      "step": 5934
    },
    {
      "epoch": 2.2977158343012003,
      "grad_norm": 6.989736080169678,
      "learning_rate": 8.558093517443111e-06,
      "loss": 0.3507,
      "step": 5935
    },
    {
      "epoch": 2.2981029810298104,
      "grad_norm": 23.379444122314453,
      "learning_rate": 8.557663354411323e-06,
      "loss": 1.9281,
      "step": 5936
    },
    {
      "epoch": 2.2984901277584204,
      "grad_norm": 22.79583168029785,
      "learning_rate": 8.557233191379534e-06,
      "loss": 1.776,
      "step": 5937
    },
    {
      "epoch": 2.2988772744870305,
      "grad_norm": 27.01609992980957,
      "learning_rate": 8.556803028347744e-06,
      "loss": 1.4755,
      "step": 5938
    },
    {
      "epoch": 2.2992644212156406,
      "grad_norm": 12.313289642333984,
      "learning_rate": 8.556372865315955e-06,
      "loss": 0.4925,
      "step": 5939
    },
    {
      "epoch": 2.2996515679442506,
      "grad_norm": 18.008575439453125,
      "learning_rate": 8.555942702284167e-06,
      "loss": 1.149,
      "step": 5940
    },
    {
      "epoch": 2.300038714672861,
      "grad_norm": 13.5087308883667,
      "learning_rate": 8.555512539252378e-06,
      "loss": 1.3881,
      "step": 5941
    },
    {
      "epoch": 2.300425861401471,
      "grad_norm": 18.88750457763672,
      "learning_rate": 8.555082376220588e-06,
      "loss": 1.1879,
      "step": 5942
    },
    {
      "epoch": 2.3008130081300813,
      "grad_norm": 30.213258743286133,
      "learning_rate": 8.5546522131888e-06,
      "loss": 1.9437,
      "step": 5943
    },
    {
      "epoch": 2.3012001548586913,
      "grad_norm": 35.8561897277832,
      "learning_rate": 8.55422205015701e-06,
      "loss": 1.7054,
      "step": 5944
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 15.559099197387695,
      "learning_rate": 8.553791887125222e-06,
      "loss": 1.376,
      "step": 5945
    },
    {
      "epoch": 2.301974448315912,
      "grad_norm": 22.6722469329834,
      "learning_rate": 8.553361724093432e-06,
      "loss": 1.9733,
      "step": 5946
    },
    {
      "epoch": 2.302361595044522,
      "grad_norm": 37.25730895996094,
      "learning_rate": 8.552931561061643e-06,
      "loss": 2.6373,
      "step": 5947
    },
    {
      "epoch": 2.302748741773132,
      "grad_norm": 13.856497764587402,
      "learning_rate": 8.552501398029855e-06,
      "loss": 0.9558,
      "step": 5948
    },
    {
      "epoch": 2.303135888501742,
      "grad_norm": 10.715883255004883,
      "learning_rate": 8.552071234998064e-06,
      "loss": 1.3297,
      "step": 5949
    },
    {
      "epoch": 2.303523035230352,
      "grad_norm": 30.69430160522461,
      "learning_rate": 8.551641071966276e-06,
      "loss": 1.844,
      "step": 5950
    },
    {
      "epoch": 2.3039101819589627,
      "grad_norm": 27.17557716369629,
      "learning_rate": 8.551210908934487e-06,
      "loss": 1.6999,
      "step": 5951
    },
    {
      "epoch": 2.3042973286875728,
      "grad_norm": 42.0362663269043,
      "learning_rate": 8.550780745902699e-06,
      "loss": 2.4551,
      "step": 5952
    },
    {
      "epoch": 2.304684475416183,
      "grad_norm": 15.861359596252441,
      "learning_rate": 8.550350582870908e-06,
      "loss": 0.9026,
      "step": 5953
    },
    {
      "epoch": 2.305071622144793,
      "grad_norm": 26.903596878051758,
      "learning_rate": 8.54992041983912e-06,
      "loss": 2.3438,
      "step": 5954
    },
    {
      "epoch": 2.305458768873403,
      "grad_norm": 31.09304428100586,
      "learning_rate": 8.54949025680733e-06,
      "loss": 1.1481,
      "step": 5955
    },
    {
      "epoch": 2.305845915602013,
      "grad_norm": 21.779882431030273,
      "learning_rate": 8.549060093775543e-06,
      "loss": 2.2754,
      "step": 5956
    },
    {
      "epoch": 2.306233062330623,
      "grad_norm": 16.760425567626953,
      "learning_rate": 8.548629930743752e-06,
      "loss": 0.7791,
      "step": 5957
    },
    {
      "epoch": 2.3066202090592336,
      "grad_norm": 33.45551681518555,
      "learning_rate": 8.548199767711964e-06,
      "loss": 1.3851,
      "step": 5958
    },
    {
      "epoch": 2.3070073557878437,
      "grad_norm": 24.65604019165039,
      "learning_rate": 8.547769604680175e-06,
      "loss": 1.2353,
      "step": 5959
    },
    {
      "epoch": 2.3073945025164537,
      "grad_norm": 30.857446670532227,
      "learning_rate": 8.547339441648386e-06,
      "loss": 1.9504,
      "step": 5960
    },
    {
      "epoch": 2.307781649245064,
      "grad_norm": 44.32413864135742,
      "learning_rate": 8.546909278616596e-06,
      "loss": 1.6748,
      "step": 5961
    },
    {
      "epoch": 2.308168795973674,
      "grad_norm": 12.473164558410645,
      "learning_rate": 8.546479115584808e-06,
      "loss": 0.7328,
      "step": 5962
    },
    {
      "epoch": 2.3085559427022844,
      "grad_norm": 33.396087646484375,
      "learning_rate": 8.546048952553019e-06,
      "loss": 0.7923,
      "step": 5963
    },
    {
      "epoch": 2.3089430894308944,
      "grad_norm": 11.87527847290039,
      "learning_rate": 8.545618789521229e-06,
      "loss": 1.0179,
      "step": 5964
    },
    {
      "epoch": 2.3093302361595045,
      "grad_norm": 14.695366859436035,
      "learning_rate": 8.54518862648944e-06,
      "loss": 0.8934,
      "step": 5965
    },
    {
      "epoch": 2.3097173828881146,
      "grad_norm": 39.04562759399414,
      "learning_rate": 8.544758463457652e-06,
      "loss": 1.513,
      "step": 5966
    },
    {
      "epoch": 2.3101045296167246,
      "grad_norm": 9.210719108581543,
      "learning_rate": 8.544328300425863e-06,
      "loss": 0.4254,
      "step": 5967
    },
    {
      "epoch": 2.3104916763453347,
      "grad_norm": 15.635775566101074,
      "learning_rate": 8.543898137394073e-06,
      "loss": 0.8854,
      "step": 5968
    },
    {
      "epoch": 2.310878823073945,
      "grad_norm": 22.521528244018555,
      "learning_rate": 8.543467974362284e-06,
      "loss": 1.4518,
      "step": 5969
    },
    {
      "epoch": 2.3112659698025553,
      "grad_norm": 14.389398574829102,
      "learning_rate": 8.543037811330494e-06,
      "loss": 1.0929,
      "step": 5970
    },
    {
      "epoch": 2.3116531165311653,
      "grad_norm": 15.698540687561035,
      "learning_rate": 8.542607648298707e-06,
      "loss": 0.9974,
      "step": 5971
    },
    {
      "epoch": 2.3120402632597754,
      "grad_norm": 23.57647705078125,
      "learning_rate": 8.542177485266917e-06,
      "loss": 1.7007,
      "step": 5972
    },
    {
      "epoch": 2.3124274099883855,
      "grad_norm": 43.24846267700195,
      "learning_rate": 8.541747322235128e-06,
      "loss": 2.3337,
      "step": 5973
    },
    {
      "epoch": 2.3128145567169955,
      "grad_norm": 23.26882553100586,
      "learning_rate": 8.541317159203338e-06,
      "loss": 1.4089,
      "step": 5974
    },
    {
      "epoch": 2.313201703445606,
      "grad_norm": 24.226516723632812,
      "learning_rate": 8.540886996171551e-06,
      "loss": 2.7477,
      "step": 5975
    },
    {
      "epoch": 2.313588850174216,
      "grad_norm": 28.15823745727539,
      "learning_rate": 8.54045683313976e-06,
      "loss": 1.3248,
      "step": 5976
    },
    {
      "epoch": 2.313975996902826,
      "grad_norm": 28.797555923461914,
      "learning_rate": 8.540026670107972e-06,
      "loss": 2.2061,
      "step": 5977
    },
    {
      "epoch": 2.3143631436314362,
      "grad_norm": 18.74573516845703,
      "learning_rate": 8.539596507076182e-06,
      "loss": 1.2345,
      "step": 5978
    },
    {
      "epoch": 2.3147502903600463,
      "grad_norm": 28.362958908081055,
      "learning_rate": 8.539166344044393e-06,
      "loss": 1.0318,
      "step": 5979
    },
    {
      "epoch": 2.315137437088657,
      "grad_norm": 17.23097038269043,
      "learning_rate": 8.538736181012605e-06,
      "loss": 1.655,
      "step": 5980
    },
    {
      "epoch": 2.315524583817267,
      "grad_norm": 17.251867294311523,
      "learning_rate": 8.538306017980816e-06,
      "loss": 1.5163,
      "step": 5981
    },
    {
      "epoch": 2.315911730545877,
      "grad_norm": 28.28480339050293,
      "learning_rate": 8.537875854949026e-06,
      "loss": 2.4346,
      "step": 5982
    },
    {
      "epoch": 2.316298877274487,
      "grad_norm": 21.688993453979492,
      "learning_rate": 8.537445691917237e-06,
      "loss": 1.3536,
      "step": 5983
    },
    {
      "epoch": 2.316686024003097,
      "grad_norm": 43.519012451171875,
      "learning_rate": 8.537015528885449e-06,
      "loss": 2.5382,
      "step": 5984
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 27.816585540771484,
      "learning_rate": 8.536585365853658e-06,
      "loss": 1.8367,
      "step": 5985
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 35.30522918701172,
      "learning_rate": 8.53615520282187e-06,
      "loss": 1.5206,
      "step": 5986
    },
    {
      "epoch": 2.3178474641889277,
      "grad_norm": 13.695777893066406,
      "learning_rate": 8.535725039790081e-06,
      "loss": 0.8895,
      "step": 5987
    },
    {
      "epoch": 2.318234610917538,
      "grad_norm": 27.723520278930664,
      "learning_rate": 8.535294876758293e-06,
      "loss": 1.2489,
      "step": 5988
    },
    {
      "epoch": 2.318621757646148,
      "grad_norm": 19.440977096557617,
      "learning_rate": 8.534864713726502e-06,
      "loss": 0.8122,
      "step": 5989
    },
    {
      "epoch": 2.319008904374758,
      "grad_norm": 14.704282760620117,
      "learning_rate": 8.534434550694714e-06,
      "loss": 1.5016,
      "step": 5990
    },
    {
      "epoch": 2.319396051103368,
      "grad_norm": 22.71788215637207,
      "learning_rate": 8.534004387662925e-06,
      "loss": 1.4607,
      "step": 5991
    },
    {
      "epoch": 2.3197831978319785,
      "grad_norm": 52.53942108154297,
      "learning_rate": 8.533574224631137e-06,
      "loss": 3.3961,
      "step": 5992
    },
    {
      "epoch": 2.3201703445605886,
      "grad_norm": 14.533709526062012,
      "learning_rate": 8.533144061599346e-06,
      "loss": 2.1342,
      "step": 5993
    },
    {
      "epoch": 2.3205574912891986,
      "grad_norm": 15.700026512145996,
      "learning_rate": 8.532713898567558e-06,
      "loss": 1.2357,
      "step": 5994
    },
    {
      "epoch": 2.3209446380178087,
      "grad_norm": 23.105125427246094,
      "learning_rate": 8.532283735535769e-06,
      "loss": 1.0016,
      "step": 5995
    },
    {
      "epoch": 2.3213317847464188,
      "grad_norm": 23.308025360107422,
      "learning_rate": 8.53185357250398e-06,
      "loss": 1.1822,
      "step": 5996
    },
    {
      "epoch": 2.3217189314750293,
      "grad_norm": 23.91897201538086,
      "learning_rate": 8.53142340947219e-06,
      "loss": 1.9838,
      "step": 5997
    },
    {
      "epoch": 2.3221060782036393,
      "grad_norm": 17.86998176574707,
      "learning_rate": 8.530993246440402e-06,
      "loss": 1.1892,
      "step": 5998
    },
    {
      "epoch": 2.3224932249322494,
      "grad_norm": 13.668244361877441,
      "learning_rate": 8.530563083408613e-06,
      "loss": 0.861,
      "step": 5999
    },
    {
      "epoch": 2.3228803716608595,
      "grad_norm": 23.004535675048828,
      "learning_rate": 8.530132920376823e-06,
      "loss": 1.398,
      "step": 6000
    },
    {
      "epoch": 2.3232675183894695,
      "grad_norm": 27.174230575561523,
      "learning_rate": 8.529702757345034e-06,
      "loss": 1.5479,
      "step": 6001
    },
    {
      "epoch": 2.3236546651180796,
      "grad_norm": 26.426645278930664,
      "learning_rate": 8.529272594313246e-06,
      "loss": 2.3814,
      "step": 6002
    },
    {
      "epoch": 2.3240418118466897,
      "grad_norm": 16.85911750793457,
      "learning_rate": 8.528842431281457e-06,
      "loss": 1.4816,
      "step": 6003
    },
    {
      "epoch": 2.3244289585753,
      "grad_norm": 28.22122573852539,
      "learning_rate": 8.528412268249667e-06,
      "loss": 1.8296,
      "step": 6004
    },
    {
      "epoch": 2.3248161053039103,
      "grad_norm": 23.26055908203125,
      "learning_rate": 8.527982105217878e-06,
      "loss": 1.4539,
      "step": 6005
    },
    {
      "epoch": 2.3252032520325203,
      "grad_norm": 47.227012634277344,
      "learning_rate": 8.52755194218609e-06,
      "loss": 2.0327,
      "step": 6006
    },
    {
      "epoch": 2.3255903987611304,
      "grad_norm": 16.82936668395996,
      "learning_rate": 8.527121779154301e-06,
      "loss": 0.8086,
      "step": 6007
    },
    {
      "epoch": 2.3259775454897405,
      "grad_norm": 28.137964248657227,
      "learning_rate": 8.52669161612251e-06,
      "loss": 2.0425,
      "step": 6008
    },
    {
      "epoch": 2.326364692218351,
      "grad_norm": 21.705463409423828,
      "learning_rate": 8.526261453090722e-06,
      "loss": 1.1915,
      "step": 6009
    },
    {
      "epoch": 2.326751838946961,
      "grad_norm": 24.850717544555664,
      "learning_rate": 8.525831290058934e-06,
      "loss": 1.417,
      "step": 6010
    },
    {
      "epoch": 2.327138985675571,
      "grad_norm": 26.41318130493164,
      "learning_rate": 8.525401127027145e-06,
      "loss": 1.6294,
      "step": 6011
    },
    {
      "epoch": 2.327526132404181,
      "grad_norm": 23.339990615844727,
      "learning_rate": 8.524970963995355e-06,
      "loss": 1.8494,
      "step": 6012
    },
    {
      "epoch": 2.3279132791327912,
      "grad_norm": 26.83950424194336,
      "learning_rate": 8.524540800963566e-06,
      "loss": 1.1214,
      "step": 6013
    },
    {
      "epoch": 2.3283004258614013,
      "grad_norm": 10.084704399108887,
      "learning_rate": 8.524110637931778e-06,
      "loss": 1.006,
      "step": 6014
    },
    {
      "epoch": 2.328687572590012,
      "grad_norm": 11.148438453674316,
      "learning_rate": 8.523680474899987e-06,
      "loss": 0.6643,
      "step": 6015
    },
    {
      "epoch": 2.329074719318622,
      "grad_norm": 22.79640769958496,
      "learning_rate": 8.523250311868199e-06,
      "loss": 1.269,
      "step": 6016
    },
    {
      "epoch": 2.329461866047232,
      "grad_norm": 23.92296028137207,
      "learning_rate": 8.52282014883641e-06,
      "loss": 1.7822,
      "step": 6017
    },
    {
      "epoch": 2.329849012775842,
      "grad_norm": 15.65219497680664,
      "learning_rate": 8.522389985804621e-06,
      "loss": 1.2299,
      "step": 6018
    },
    {
      "epoch": 2.330236159504452,
      "grad_norm": 15.058509826660156,
      "learning_rate": 8.521959822772831e-06,
      "loss": 1.1115,
      "step": 6019
    },
    {
      "epoch": 2.330623306233062,
      "grad_norm": 18.064149856567383,
      "learning_rate": 8.521529659741043e-06,
      "loss": 0.5786,
      "step": 6020
    },
    {
      "epoch": 2.3310104529616726,
      "grad_norm": 27.42302131652832,
      "learning_rate": 8.521099496709252e-06,
      "loss": 1.3913,
      "step": 6021
    },
    {
      "epoch": 2.3313975996902827,
      "grad_norm": 15.220134735107422,
      "learning_rate": 8.520669333677465e-06,
      "loss": 1.4896,
      "step": 6022
    },
    {
      "epoch": 2.3317847464188928,
      "grad_norm": 31.88533592224121,
      "learning_rate": 8.520239170645675e-06,
      "loss": 1.229,
      "step": 6023
    },
    {
      "epoch": 2.332171893147503,
      "grad_norm": 14.431973457336426,
      "learning_rate": 8.519809007613887e-06,
      "loss": 1.1737,
      "step": 6024
    },
    {
      "epoch": 2.332559039876113,
      "grad_norm": 14.14016342163086,
      "learning_rate": 8.519378844582096e-06,
      "loss": 1.1932,
      "step": 6025
    },
    {
      "epoch": 2.3329461866047234,
      "grad_norm": 33.09461975097656,
      "learning_rate": 8.51894868155031e-06,
      "loss": 1.1033,
      "step": 6026
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 14.268733978271484,
      "learning_rate": 8.518518518518519e-06,
      "loss": 1.3387,
      "step": 6027
    },
    {
      "epoch": 2.3337204800619435,
      "grad_norm": 15.690348625183105,
      "learning_rate": 8.51808835548673e-06,
      "loss": 0.8477,
      "step": 6028
    },
    {
      "epoch": 2.3341076267905536,
      "grad_norm": 12.931178092956543,
      "learning_rate": 8.51765819245494e-06,
      "loss": 0.4314,
      "step": 6029
    },
    {
      "epoch": 2.3344947735191637,
      "grad_norm": 18.43888282775879,
      "learning_rate": 8.517228029423152e-06,
      "loss": 1.7637,
      "step": 6030
    },
    {
      "epoch": 2.3348819202477737,
      "grad_norm": 12.144197463989258,
      "learning_rate": 8.516797866391363e-06,
      "loss": 0.9531,
      "step": 6031
    },
    {
      "epoch": 2.335269066976384,
      "grad_norm": 46.465736389160156,
      "learning_rate": 8.516367703359575e-06,
      "loss": 1.7667,
      "step": 6032
    },
    {
      "epoch": 2.3356562137049943,
      "grad_norm": 25.471223831176758,
      "learning_rate": 8.515937540327784e-06,
      "loss": 1.386,
      "step": 6033
    },
    {
      "epoch": 2.3360433604336044,
      "grad_norm": 40.71974563598633,
      "learning_rate": 8.515507377295996e-06,
      "loss": 2.8473,
      "step": 6034
    },
    {
      "epoch": 2.3364305071622145,
      "grad_norm": 15.707852363586426,
      "learning_rate": 8.515077214264207e-06,
      "loss": 1.0611,
      "step": 6035
    },
    {
      "epoch": 2.3368176538908245,
      "grad_norm": 25.430994033813477,
      "learning_rate": 8.514647051232417e-06,
      "loss": 1.479,
      "step": 6036
    },
    {
      "epoch": 2.3372048006194346,
      "grad_norm": 30.84294891357422,
      "learning_rate": 8.514216888200628e-06,
      "loss": 1.6998,
      "step": 6037
    },
    {
      "epoch": 2.337591947348045,
      "grad_norm": 22.88399314880371,
      "learning_rate": 8.51378672516884e-06,
      "loss": 1.885,
      "step": 6038
    },
    {
      "epoch": 2.337979094076655,
      "grad_norm": 15.586702346801758,
      "learning_rate": 8.513356562137051e-06,
      "loss": 0.9641,
      "step": 6039
    },
    {
      "epoch": 2.3383662408052652,
      "grad_norm": 27.131685256958008,
      "learning_rate": 8.51292639910526e-06,
      "loss": 2.1385,
      "step": 6040
    },
    {
      "epoch": 2.3387533875338753,
      "grad_norm": 27.42022705078125,
      "learning_rate": 8.512496236073474e-06,
      "loss": 2.7376,
      "step": 6041
    },
    {
      "epoch": 2.3391405342624854,
      "grad_norm": 19.00783920288086,
      "learning_rate": 8.512066073041684e-06,
      "loss": 1.1372,
      "step": 6042
    },
    {
      "epoch": 2.339527680991096,
      "grad_norm": 25.46596908569336,
      "learning_rate": 8.511635910009895e-06,
      "loss": 0.6437,
      "step": 6043
    },
    {
      "epoch": 2.339914827719706,
      "grad_norm": 22.971677780151367,
      "learning_rate": 8.511205746978105e-06,
      "loss": 1.9523,
      "step": 6044
    },
    {
      "epoch": 2.340301974448316,
      "grad_norm": 25.419803619384766,
      "learning_rate": 8.510775583946316e-06,
      "loss": 1.6105,
      "step": 6045
    },
    {
      "epoch": 2.340689121176926,
      "grad_norm": 19.76164436340332,
      "learning_rate": 8.510345420914528e-06,
      "loss": 1.3991,
      "step": 6046
    },
    {
      "epoch": 2.341076267905536,
      "grad_norm": 24.852937698364258,
      "learning_rate": 8.509915257882739e-06,
      "loss": 1.6922,
      "step": 6047
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 15.184470176696777,
      "learning_rate": 8.509485094850949e-06,
      "loss": 0.9656,
      "step": 6048
    },
    {
      "epoch": 2.3418505613627563,
      "grad_norm": 25.268535614013672,
      "learning_rate": 8.50905493181916e-06,
      "loss": 1.4864,
      "step": 6049
    },
    {
      "epoch": 2.3422377080913668,
      "grad_norm": 10.595870018005371,
      "learning_rate": 8.508624768787372e-06,
      "loss": 0.6277,
      "step": 6050
    },
    {
      "epoch": 2.342624854819977,
      "grad_norm": 15.22640609741211,
      "learning_rate": 8.508194605755581e-06,
      "loss": 1.1454,
      "step": 6051
    },
    {
      "epoch": 2.343012001548587,
      "grad_norm": 22.945003509521484,
      "learning_rate": 8.507764442723793e-06,
      "loss": 1.5842,
      "step": 6052
    },
    {
      "epoch": 2.343399148277197,
      "grad_norm": 47.86716079711914,
      "learning_rate": 8.507334279692004e-06,
      "loss": 2.0097,
      "step": 6053
    },
    {
      "epoch": 2.343786295005807,
      "grad_norm": 23.228858947753906,
      "learning_rate": 8.506904116660216e-06,
      "loss": 1.87,
      "step": 6054
    },
    {
      "epoch": 2.3441734417344176,
      "grad_norm": 29.96240997314453,
      "learning_rate": 8.506473953628425e-06,
      "loss": 1.5753,
      "step": 6055
    },
    {
      "epoch": 2.3445605884630276,
      "grad_norm": 17.012042999267578,
      "learning_rate": 8.506043790596637e-06,
      "loss": 1.0612,
      "step": 6056
    },
    {
      "epoch": 2.3449477351916377,
      "grad_norm": 22.388755798339844,
      "learning_rate": 8.505613627564848e-06,
      "loss": 2.2929,
      "step": 6057
    },
    {
      "epoch": 2.3453348819202477,
      "grad_norm": 17.4726505279541,
      "learning_rate": 8.50518346453306e-06,
      "loss": 1.1956,
      "step": 6058
    },
    {
      "epoch": 2.345722028648858,
      "grad_norm": 17.633182525634766,
      "learning_rate": 8.50475330150127e-06,
      "loss": 2.0481,
      "step": 6059
    },
    {
      "epoch": 2.346109175377468,
      "grad_norm": 9.100809097290039,
      "learning_rate": 8.50432313846948e-06,
      "loss": 0.58,
      "step": 6060
    },
    {
      "epoch": 2.3464963221060784,
      "grad_norm": 29.160661697387695,
      "learning_rate": 8.503892975437692e-06,
      "loss": 2.1093,
      "step": 6061
    },
    {
      "epoch": 2.3468834688346885,
      "grad_norm": 27.791297912597656,
      "learning_rate": 8.503462812405903e-06,
      "loss": 1.7204,
      "step": 6062
    },
    {
      "epoch": 2.3472706155632985,
      "grad_norm": 46.624237060546875,
      "learning_rate": 8.503032649374113e-06,
      "loss": 2.1875,
      "step": 6063
    },
    {
      "epoch": 2.3476577622919086,
      "grad_norm": 21.44854736328125,
      "learning_rate": 8.502602486342325e-06,
      "loss": 0.8581,
      "step": 6064
    },
    {
      "epoch": 2.3480449090205187,
      "grad_norm": 23.176271438598633,
      "learning_rate": 8.502172323310536e-06,
      "loss": 1.1746,
      "step": 6065
    },
    {
      "epoch": 2.3484320557491287,
      "grad_norm": 32.72173309326172,
      "learning_rate": 8.501742160278746e-06,
      "loss": 2.0371,
      "step": 6066
    },
    {
      "epoch": 2.3488192024777392,
      "grad_norm": 30.156156539916992,
      "learning_rate": 8.501311997246957e-06,
      "loss": 1.927,
      "step": 6067
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 47.236881256103516,
      "learning_rate": 8.500881834215169e-06,
      "loss": 1.9221,
      "step": 6068
    },
    {
      "epoch": 2.3495934959349594,
      "grad_norm": 24.219877243041992,
      "learning_rate": 8.50045167118338e-06,
      "loss": 1.8527,
      "step": 6069
    },
    {
      "epoch": 2.3499806426635694,
      "grad_norm": 19.510251998901367,
      "learning_rate": 8.50002150815159e-06,
      "loss": 1.235,
      "step": 6070
    },
    {
      "epoch": 2.3503677893921795,
      "grad_norm": 19.755388259887695,
      "learning_rate": 8.499591345119801e-06,
      "loss": 1.2081,
      "step": 6071
    },
    {
      "epoch": 2.35075493612079,
      "grad_norm": 13.861082077026367,
      "learning_rate": 8.49916118208801e-06,
      "loss": 1.414,
      "step": 6072
    },
    {
      "epoch": 2.3511420828494,
      "grad_norm": 28.605873107910156,
      "learning_rate": 8.498731019056224e-06,
      "loss": 1.274,
      "step": 6073
    },
    {
      "epoch": 2.35152922957801,
      "grad_norm": 20.43401336669922,
      "learning_rate": 8.498300856024434e-06,
      "loss": 0.9724,
      "step": 6074
    },
    {
      "epoch": 2.35191637630662,
      "grad_norm": 20.86387825012207,
      "learning_rate": 8.497870692992645e-06,
      "loss": 1.4295,
      "step": 6075
    },
    {
      "epoch": 2.3523035230352303,
      "grad_norm": 28.791316986083984,
      "learning_rate": 8.497440529960855e-06,
      "loss": 1.3332,
      "step": 6076
    },
    {
      "epoch": 2.3526906697638403,
      "grad_norm": 22.333049774169922,
      "learning_rate": 8.497010366929068e-06,
      "loss": 1.0831,
      "step": 6077
    },
    {
      "epoch": 2.3530778164924504,
      "grad_norm": 13.751280784606934,
      "learning_rate": 8.496580203897278e-06,
      "loss": 1.416,
      "step": 6078
    },
    {
      "epoch": 2.353464963221061,
      "grad_norm": 19.074098587036133,
      "learning_rate": 8.496150040865489e-06,
      "loss": 1.2319,
      "step": 6079
    },
    {
      "epoch": 2.353852109949671,
      "grad_norm": 27.493257522583008,
      "learning_rate": 8.495719877833699e-06,
      "loss": 1.6841,
      "step": 6080
    },
    {
      "epoch": 2.354239256678281,
      "grad_norm": 18.577939987182617,
      "learning_rate": 8.49528971480191e-06,
      "loss": 1.3466,
      "step": 6081
    },
    {
      "epoch": 2.354626403406891,
      "grad_norm": 51.275001525878906,
      "learning_rate": 8.494859551770122e-06,
      "loss": 2.2817,
      "step": 6082
    },
    {
      "epoch": 2.355013550135501,
      "grad_norm": 30.395559310913086,
      "learning_rate": 8.494429388738333e-06,
      "loss": 1.8746,
      "step": 6083
    },
    {
      "epoch": 2.3554006968641117,
      "grad_norm": 29.6375732421875,
      "learning_rate": 8.493999225706544e-06,
      "loss": 1.5475,
      "step": 6084
    },
    {
      "epoch": 2.3557878435927218,
      "grad_norm": 13.170792579650879,
      "learning_rate": 8.493569062674754e-06,
      "loss": 0.9018,
      "step": 6085
    },
    {
      "epoch": 2.356174990321332,
      "grad_norm": 13.7869291305542,
      "learning_rate": 8.493138899642966e-06,
      "loss": 0.8453,
      "step": 6086
    },
    {
      "epoch": 2.356562137049942,
      "grad_norm": 16.83365821838379,
      "learning_rate": 8.492708736611175e-06,
      "loss": 1.1709,
      "step": 6087
    },
    {
      "epoch": 2.356949283778552,
      "grad_norm": 20.208683013916016,
      "learning_rate": 8.492278573579388e-06,
      "loss": 1.1886,
      "step": 6088
    },
    {
      "epoch": 2.3573364305071625,
      "grad_norm": 21.342334747314453,
      "learning_rate": 8.491848410547598e-06,
      "loss": 1.7302,
      "step": 6089
    },
    {
      "epoch": 2.3577235772357725,
      "grad_norm": 26.334938049316406,
      "learning_rate": 8.49141824751581e-06,
      "loss": 2.0816,
      "step": 6090
    },
    {
      "epoch": 2.3581107239643826,
      "grad_norm": 33.85717010498047,
      "learning_rate": 8.49098808448402e-06,
      "loss": 1.4588,
      "step": 6091
    },
    {
      "epoch": 2.3584978706929927,
      "grad_norm": 44.98942947387695,
      "learning_rate": 8.490557921452232e-06,
      "loss": 2.38,
      "step": 6092
    },
    {
      "epoch": 2.3588850174216027,
      "grad_norm": 10.720662117004395,
      "learning_rate": 8.490127758420442e-06,
      "loss": 0.6302,
      "step": 6093
    },
    {
      "epoch": 2.359272164150213,
      "grad_norm": 21.3302059173584,
      "learning_rate": 8.489697595388654e-06,
      "loss": 1.7719,
      "step": 6094
    },
    {
      "epoch": 2.359659310878823,
      "grad_norm": 17.52560806274414,
      "learning_rate": 8.489267432356863e-06,
      "loss": 1.4083,
      "step": 6095
    },
    {
      "epoch": 2.3600464576074334,
      "grad_norm": 23.760282516479492,
      "learning_rate": 8.488837269325075e-06,
      "loss": 1.6304,
      "step": 6096
    },
    {
      "epoch": 2.3604336043360434,
      "grad_norm": 27.8863468170166,
      "learning_rate": 8.488407106293286e-06,
      "loss": 1.5484,
      "step": 6097
    },
    {
      "epoch": 2.3608207510646535,
      "grad_norm": 31.611967086791992,
      "learning_rate": 8.487976943261497e-06,
      "loss": 1.8256,
      "step": 6098
    },
    {
      "epoch": 2.3612078977932636,
      "grad_norm": 12.421359062194824,
      "learning_rate": 8.487546780229707e-06,
      "loss": 0.7086,
      "step": 6099
    },
    {
      "epoch": 2.3615950445218736,
      "grad_norm": 27.077621459960938,
      "learning_rate": 8.487116617197919e-06,
      "loss": 1.326,
      "step": 6100
    },
    {
      "epoch": 2.361982191250484,
      "grad_norm": 22.83156394958496,
      "learning_rate": 8.48668645416613e-06,
      "loss": 1.6352,
      "step": 6101
    },
    {
      "epoch": 2.362369337979094,
      "grad_norm": 20.232633590698242,
      "learning_rate": 8.48625629113434e-06,
      "loss": 1.802,
      "step": 6102
    },
    {
      "epoch": 2.3627564847077043,
      "grad_norm": 16.33730125427246,
      "learning_rate": 8.485826128102551e-06,
      "loss": 1.5851,
      "step": 6103
    },
    {
      "epoch": 2.3631436314363143,
      "grad_norm": 46.755184173583984,
      "learning_rate": 8.485395965070763e-06,
      "loss": 2.3087,
      "step": 6104
    },
    {
      "epoch": 2.3635307781649244,
      "grad_norm": 14.439321517944336,
      "learning_rate": 8.484965802038974e-06,
      "loss": 1.0292,
      "step": 6105
    },
    {
      "epoch": 2.3639179248935345,
      "grad_norm": 29.716867446899414,
      "learning_rate": 8.484535639007184e-06,
      "loss": 2.0975,
      "step": 6106
    },
    {
      "epoch": 2.364305071622145,
      "grad_norm": 8.63700008392334,
      "learning_rate": 8.484105475975395e-06,
      "loss": 0.5378,
      "step": 6107
    },
    {
      "epoch": 2.364692218350755,
      "grad_norm": 26.96453094482422,
      "learning_rate": 8.483675312943607e-06,
      "loss": 1.6303,
      "step": 6108
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 25.89319610595703,
      "learning_rate": 8.483245149911818e-06,
      "loss": 1.7185,
      "step": 6109
    },
    {
      "epoch": 2.365466511807975,
      "grad_norm": 22.92190170288086,
      "learning_rate": 8.482814986880028e-06,
      "loss": 1.6592,
      "step": 6110
    },
    {
      "epoch": 2.3658536585365852,
      "grad_norm": 12.91634464263916,
      "learning_rate": 8.482384823848239e-06,
      "loss": 0.971,
      "step": 6111
    },
    {
      "epoch": 2.3662408052651953,
      "grad_norm": 21.975934982299805,
      "learning_rate": 8.48195466081645e-06,
      "loss": 1.9798,
      "step": 6112
    },
    {
      "epoch": 2.366627951993806,
      "grad_norm": 16.361608505249023,
      "learning_rate": 8.481524497784662e-06,
      "loss": 1.0327,
      "step": 6113
    },
    {
      "epoch": 2.367015098722416,
      "grad_norm": 27.19974136352539,
      "learning_rate": 8.481094334752872e-06,
      "loss": 1.7242,
      "step": 6114
    },
    {
      "epoch": 2.367402245451026,
      "grad_norm": 18.014638900756836,
      "learning_rate": 8.480664171721083e-06,
      "loss": 1.2694,
      "step": 6115
    },
    {
      "epoch": 2.367789392179636,
      "grad_norm": 31.203819274902344,
      "learning_rate": 8.480234008689294e-06,
      "loss": 1.7165,
      "step": 6116
    },
    {
      "epoch": 2.368176538908246,
      "grad_norm": 23.3695068359375,
      "learning_rate": 8.479803845657504e-06,
      "loss": 1.1797,
      "step": 6117
    },
    {
      "epoch": 2.3685636856368566,
      "grad_norm": 16.258609771728516,
      "learning_rate": 8.479373682625716e-06,
      "loss": 1.2415,
      "step": 6118
    },
    {
      "epoch": 2.3689508323654667,
      "grad_norm": 20.679019927978516,
      "learning_rate": 8.478943519593927e-06,
      "loss": 3.4297,
      "step": 6119
    },
    {
      "epoch": 2.3693379790940767,
      "grad_norm": 16.84478187561035,
      "learning_rate": 8.478513356562138e-06,
      "loss": 0.9796,
      "step": 6120
    },
    {
      "epoch": 2.369725125822687,
      "grad_norm": 15.321216583251953,
      "learning_rate": 8.478083193530348e-06,
      "loss": 0.8348,
      "step": 6121
    },
    {
      "epoch": 2.370112272551297,
      "grad_norm": 13.33958625793457,
      "learning_rate": 8.47765303049856e-06,
      "loss": 0.8527,
      "step": 6122
    },
    {
      "epoch": 2.370499419279907,
      "grad_norm": 14.990398406982422,
      "learning_rate": 8.477222867466771e-06,
      "loss": 1.1552,
      "step": 6123
    },
    {
      "epoch": 2.370886566008517,
      "grad_norm": 14.010930061340332,
      "learning_rate": 8.476792704434982e-06,
      "loss": 1.0301,
      "step": 6124
    },
    {
      "epoch": 2.3712737127371275,
      "grad_norm": 17.205331802368164,
      "learning_rate": 8.476362541403192e-06,
      "loss": 1.1093,
      "step": 6125
    },
    {
      "epoch": 2.3716608594657376,
      "grad_norm": 15.638218879699707,
      "learning_rate": 8.475932378371404e-06,
      "loss": 1.0271,
      "step": 6126
    },
    {
      "epoch": 2.3720480061943476,
      "grad_norm": 18.689308166503906,
      "learning_rate": 8.475502215339615e-06,
      "loss": 1.6234,
      "step": 6127
    },
    {
      "epoch": 2.3724351529229577,
      "grad_norm": 17.18704605102539,
      "learning_rate": 8.475072052307826e-06,
      "loss": 1.3337,
      "step": 6128
    },
    {
      "epoch": 2.3728222996515678,
      "grad_norm": 17.278926849365234,
      "learning_rate": 8.474641889276036e-06,
      "loss": 1.0539,
      "step": 6129
    },
    {
      "epoch": 2.3732094463801783,
      "grad_norm": 13.267315864562988,
      "learning_rate": 8.474211726244248e-06,
      "loss": 1.4297,
      "step": 6130
    },
    {
      "epoch": 2.3735965931087883,
      "grad_norm": 13.272462844848633,
      "learning_rate": 8.473781563212459e-06,
      "loss": 0.7923,
      "step": 6131
    },
    {
      "epoch": 2.3739837398373984,
      "grad_norm": 41.11898422241211,
      "learning_rate": 8.473351400180669e-06,
      "loss": 1.8332,
      "step": 6132
    },
    {
      "epoch": 2.3743708865660085,
      "grad_norm": 20.689790725708008,
      "learning_rate": 8.47292123714888e-06,
      "loss": 1.7742,
      "step": 6133
    },
    {
      "epoch": 2.3747580332946185,
      "grad_norm": 32.169715881347656,
      "learning_rate": 8.472491074117092e-06,
      "loss": 1.2065,
      "step": 6134
    },
    {
      "epoch": 2.375145180023229,
      "grad_norm": 32.09081268310547,
      "learning_rate": 8.472060911085303e-06,
      "loss": 0.7741,
      "step": 6135
    },
    {
      "epoch": 2.375532326751839,
      "grad_norm": 19.06886100769043,
      "learning_rate": 8.471630748053513e-06,
      "loss": 1.9915,
      "step": 6136
    },
    {
      "epoch": 2.375919473480449,
      "grad_norm": 16.05754280090332,
      "learning_rate": 8.471200585021724e-06,
      "loss": 1.4816,
      "step": 6137
    },
    {
      "epoch": 2.3763066202090593,
      "grad_norm": 20.451671600341797,
      "learning_rate": 8.470770421989934e-06,
      "loss": 1.4275,
      "step": 6138
    },
    {
      "epoch": 2.3766937669376693,
      "grad_norm": 12.860904693603516,
      "learning_rate": 8.470340258958147e-06,
      "loss": 0.8048,
      "step": 6139
    },
    {
      "epoch": 2.3770809136662794,
      "grad_norm": 28.373546600341797,
      "learning_rate": 8.469910095926357e-06,
      "loss": 1.7396,
      "step": 6140
    },
    {
      "epoch": 2.3774680603948894,
      "grad_norm": 21.92537498474121,
      "learning_rate": 8.469479932894568e-06,
      "loss": 1.0258,
      "step": 6141
    },
    {
      "epoch": 2.3778552071235,
      "grad_norm": 13.645694732666016,
      "learning_rate": 8.469049769862778e-06,
      "loss": 1.2459,
      "step": 6142
    },
    {
      "epoch": 2.37824235385211,
      "grad_norm": 14.863818168640137,
      "learning_rate": 8.468619606830991e-06,
      "loss": 1.1859,
      "step": 6143
    },
    {
      "epoch": 2.37862950058072,
      "grad_norm": 31.637245178222656,
      "learning_rate": 8.4681894437992e-06,
      "loss": 1.8177,
      "step": 6144
    },
    {
      "epoch": 2.37901664730933,
      "grad_norm": 10.758285522460938,
      "learning_rate": 8.467759280767412e-06,
      "loss": 1.2157,
      "step": 6145
    },
    {
      "epoch": 2.3794037940379402,
      "grad_norm": 21.893104553222656,
      "learning_rate": 8.467329117735622e-06,
      "loss": 1.7846,
      "step": 6146
    },
    {
      "epoch": 2.3797909407665507,
      "grad_norm": 20.615049362182617,
      "learning_rate": 8.466898954703833e-06,
      "loss": 1.2225,
      "step": 6147
    },
    {
      "epoch": 2.380178087495161,
      "grad_norm": 13.625354766845703,
      "learning_rate": 8.466468791672045e-06,
      "loss": 0.7828,
      "step": 6148
    },
    {
      "epoch": 2.380565234223771,
      "grad_norm": 30.742494583129883,
      "learning_rate": 8.466038628640256e-06,
      "loss": 1.4898,
      "step": 6149
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 29.587425231933594,
      "learning_rate": 8.465608465608466e-06,
      "loss": 1.4183,
      "step": 6150
    },
    {
      "epoch": 2.381339527680991,
      "grad_norm": 23.32748794555664,
      "learning_rate": 8.465178302576677e-06,
      "loss": 0.5691,
      "step": 6151
    },
    {
      "epoch": 2.381726674409601,
      "grad_norm": 13.65196418762207,
      "learning_rate": 8.464748139544889e-06,
      "loss": 0.565,
      "step": 6152
    },
    {
      "epoch": 2.3821138211382116,
      "grad_norm": 23.106916427612305,
      "learning_rate": 8.464317976513098e-06,
      "loss": 1.8138,
      "step": 6153
    },
    {
      "epoch": 2.3825009678668216,
      "grad_norm": 18.947338104248047,
      "learning_rate": 8.46388781348131e-06,
      "loss": 1.7676,
      "step": 6154
    },
    {
      "epoch": 2.3828881145954317,
      "grad_norm": 14.468155860900879,
      "learning_rate": 8.463457650449521e-06,
      "loss": 0.9875,
      "step": 6155
    },
    {
      "epoch": 2.3832752613240418,
      "grad_norm": 15.500000953674316,
      "learning_rate": 8.463027487417732e-06,
      "loss": 1.1251,
      "step": 6156
    },
    {
      "epoch": 2.383662408052652,
      "grad_norm": 22.20153045654297,
      "learning_rate": 8.462597324385942e-06,
      "loss": 1.1533,
      "step": 6157
    },
    {
      "epoch": 2.384049554781262,
      "grad_norm": 27.002073287963867,
      "learning_rate": 8.462167161354154e-06,
      "loss": 1.2561,
      "step": 6158
    },
    {
      "epoch": 2.3844367015098724,
      "grad_norm": 27.043781280517578,
      "learning_rate": 8.461736998322365e-06,
      "loss": 1.6631,
      "step": 6159
    },
    {
      "epoch": 2.3848238482384825,
      "grad_norm": 6.751690864562988,
      "learning_rate": 8.461306835290576e-06,
      "loss": 0.3417,
      "step": 6160
    },
    {
      "epoch": 2.3852109949670925,
      "grad_norm": 25.416337966918945,
      "learning_rate": 8.460876672258786e-06,
      "loss": 1.8296,
      "step": 6161
    },
    {
      "epoch": 2.3855981416957026,
      "grad_norm": 26.449615478515625,
      "learning_rate": 8.460446509226998e-06,
      "loss": 1.8682,
      "step": 6162
    },
    {
      "epoch": 2.3859852884243127,
      "grad_norm": 12.947225570678711,
      "learning_rate": 8.460016346195209e-06,
      "loss": 1.3646,
      "step": 6163
    },
    {
      "epoch": 2.386372435152923,
      "grad_norm": 13.648802757263184,
      "learning_rate": 8.45958618316342e-06,
      "loss": 1.3365,
      "step": 6164
    },
    {
      "epoch": 2.3867595818815333,
      "grad_norm": 16.826839447021484,
      "learning_rate": 8.45915602013163e-06,
      "loss": 1.5698,
      "step": 6165
    },
    {
      "epoch": 2.3871467286101433,
      "grad_norm": 14.67233657836914,
      "learning_rate": 8.458725857099842e-06,
      "loss": 1.2084,
      "step": 6166
    },
    {
      "epoch": 2.3875338753387534,
      "grad_norm": 25.69815444946289,
      "learning_rate": 8.458295694068053e-06,
      "loss": 3.3997,
      "step": 6167
    },
    {
      "epoch": 2.3879210220673635,
      "grad_norm": 13.230786323547363,
      "learning_rate": 8.457865531036263e-06,
      "loss": 0.7642,
      "step": 6168
    },
    {
      "epoch": 2.3883081687959735,
      "grad_norm": 27.106529235839844,
      "learning_rate": 8.457435368004474e-06,
      "loss": 1.3783,
      "step": 6169
    },
    {
      "epoch": 2.3886953155245836,
      "grad_norm": 16.979764938354492,
      "learning_rate": 8.457005204972686e-06,
      "loss": 3.2798,
      "step": 6170
    },
    {
      "epoch": 2.389082462253194,
      "grad_norm": 13.429558753967285,
      "learning_rate": 8.456575041940897e-06,
      "loss": 0.8175,
      "step": 6171
    },
    {
      "epoch": 2.389469608981804,
      "grad_norm": 13.622315406799316,
      "learning_rate": 8.456144878909107e-06,
      "loss": 1.0925,
      "step": 6172
    },
    {
      "epoch": 2.3898567557104142,
      "grad_norm": 15.997929573059082,
      "learning_rate": 8.455714715877318e-06,
      "loss": 0.7987,
      "step": 6173
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 22.174062728881836,
      "learning_rate": 8.45528455284553e-06,
      "loss": 1.3563,
      "step": 6174
    },
    {
      "epoch": 2.3906310491676344,
      "grad_norm": 52.18630599975586,
      "learning_rate": 8.454854389813741e-06,
      "loss": 2.8837,
      "step": 6175
    },
    {
      "epoch": 2.391018195896245,
      "grad_norm": 17.37371063232422,
      "learning_rate": 8.45442422678195e-06,
      "loss": 2.2351,
      "step": 6176
    },
    {
      "epoch": 2.391405342624855,
      "grad_norm": 31.46874237060547,
      "learning_rate": 8.453994063750162e-06,
      "loss": 2.4034,
      "step": 6177
    },
    {
      "epoch": 2.391792489353465,
      "grad_norm": 10.050087928771973,
      "learning_rate": 8.453563900718373e-06,
      "loss": 1.2147,
      "step": 6178
    },
    {
      "epoch": 2.392179636082075,
      "grad_norm": 14.958104133605957,
      "learning_rate": 8.453133737686585e-06,
      "loss": 1.5153,
      "step": 6179
    },
    {
      "epoch": 2.392566782810685,
      "grad_norm": 33.69038391113281,
      "learning_rate": 8.452703574654795e-06,
      "loss": 1.937,
      "step": 6180
    },
    {
      "epoch": 2.392953929539295,
      "grad_norm": 28.364397048950195,
      "learning_rate": 8.452273411623006e-06,
      "loss": 1.3856,
      "step": 6181
    },
    {
      "epoch": 2.3933410762679057,
      "grad_norm": 17.69939422607422,
      "learning_rate": 8.451843248591217e-06,
      "loss": 1.6063,
      "step": 6182
    },
    {
      "epoch": 2.3937282229965158,
      "grad_norm": 47.944053649902344,
      "learning_rate": 8.451413085559427e-06,
      "loss": 2.0517,
      "step": 6183
    },
    {
      "epoch": 2.394115369725126,
      "grad_norm": 18.30397605895996,
      "learning_rate": 8.450982922527639e-06,
      "loss": 1.1341,
      "step": 6184
    },
    {
      "epoch": 2.394502516453736,
      "grad_norm": 22.43625831604004,
      "learning_rate": 8.45055275949585e-06,
      "loss": 2.9175,
      "step": 6185
    },
    {
      "epoch": 2.394889663182346,
      "grad_norm": 33.548057556152344,
      "learning_rate": 8.450122596464061e-06,
      "loss": 2.3909,
      "step": 6186
    },
    {
      "epoch": 2.395276809910956,
      "grad_norm": 24.49785804748535,
      "learning_rate": 8.449692433432271e-06,
      "loss": 1.2637,
      "step": 6187
    },
    {
      "epoch": 2.3956639566395665,
      "grad_norm": 25.441099166870117,
      "learning_rate": 8.449262270400483e-06,
      "loss": 1.5446,
      "step": 6188
    },
    {
      "epoch": 2.3960511033681766,
      "grad_norm": 20.550729751586914,
      "learning_rate": 8.448832107368692e-06,
      "loss": 1.9529,
      "step": 6189
    },
    {
      "epoch": 2.3964382500967867,
      "grad_norm": 16.824447631835938,
      "learning_rate": 8.448401944336905e-06,
      "loss": 0.6866,
      "step": 6190
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 36.36467742919922,
      "learning_rate": 8.447971781305115e-06,
      "loss": 1.0464,
      "step": 6191
    },
    {
      "epoch": 2.397212543554007,
      "grad_norm": 25.0352725982666,
      "learning_rate": 8.447541618273327e-06,
      "loss": 1.9778,
      "step": 6192
    },
    {
      "epoch": 2.3975996902826173,
      "grad_norm": 13.430075645446777,
      "learning_rate": 8.447111455241536e-06,
      "loss": 1.3274,
      "step": 6193
    },
    {
      "epoch": 2.3979868370112274,
      "grad_norm": 12.178522109985352,
      "learning_rate": 8.44668129220975e-06,
      "loss": 0.7229,
      "step": 6194
    },
    {
      "epoch": 2.3983739837398375,
      "grad_norm": 13.322046279907227,
      "learning_rate": 8.446251129177959e-06,
      "loss": 0.9781,
      "step": 6195
    },
    {
      "epoch": 2.3987611304684475,
      "grad_norm": 26.51669692993164,
      "learning_rate": 8.44582096614617e-06,
      "loss": 2.0042,
      "step": 6196
    },
    {
      "epoch": 2.3991482771970576,
      "grad_norm": 33.902828216552734,
      "learning_rate": 8.44539080311438e-06,
      "loss": 1.6249,
      "step": 6197
    },
    {
      "epoch": 2.3995354239256677,
      "grad_norm": 24.165435791015625,
      "learning_rate": 8.444960640082592e-06,
      "loss": 1.2928,
      "step": 6198
    },
    {
      "epoch": 2.399922570654278,
      "grad_norm": 20.970224380493164,
      "learning_rate": 8.444530477050803e-06,
      "loss": 1.0963,
      "step": 6199
    },
    {
      "epoch": 2.4003097173828882,
      "grad_norm": 50.71638107299805,
      "learning_rate": 8.444100314019014e-06,
      "loss": 1.3768,
      "step": 6200
    },
    {
      "epoch": 2.4006968641114983,
      "grad_norm": 30.728515625,
      "learning_rate": 8.443670150987224e-06,
      "loss": 1.731,
      "step": 6201
    },
    {
      "epoch": 2.4010840108401084,
      "grad_norm": 12.475939750671387,
      "learning_rate": 8.443239987955436e-06,
      "loss": 0.7981,
      "step": 6202
    },
    {
      "epoch": 2.4014711575687184,
      "grad_norm": 17.596189498901367,
      "learning_rate": 8.442809824923647e-06,
      "loss": 1.4477,
      "step": 6203
    },
    {
      "epoch": 2.4018583042973285,
      "grad_norm": 13.58961296081543,
      "learning_rate": 8.442379661891857e-06,
      "loss": 1.0604,
      "step": 6204
    },
    {
      "epoch": 2.402245451025939,
      "grad_norm": 49.33101272583008,
      "learning_rate": 8.44194949886007e-06,
      "loss": 2.6679,
      "step": 6205
    },
    {
      "epoch": 2.402632597754549,
      "grad_norm": 15.072225570678711,
      "learning_rate": 8.44151933582828e-06,
      "loss": 1.2259,
      "step": 6206
    },
    {
      "epoch": 2.403019744483159,
      "grad_norm": 25.48409652709961,
      "learning_rate": 8.441089172796491e-06,
      "loss": 1.4076,
      "step": 6207
    },
    {
      "epoch": 2.403406891211769,
      "grad_norm": 19.68397331237793,
      "learning_rate": 8.4406590097647e-06,
      "loss": 1.6226,
      "step": 6208
    },
    {
      "epoch": 2.4037940379403793,
      "grad_norm": 19.79789161682129,
      "learning_rate": 8.440228846732914e-06,
      "loss": 2.1312,
      "step": 6209
    },
    {
      "epoch": 2.40418118466899,
      "grad_norm": 19.362154006958008,
      "learning_rate": 8.439798683701124e-06,
      "loss": 1.5154,
      "step": 6210
    },
    {
      "epoch": 2.4045683313976,
      "grad_norm": 18.985248565673828,
      "learning_rate": 8.439368520669335e-06,
      "loss": 1.5978,
      "step": 6211
    },
    {
      "epoch": 2.40495547812621,
      "grad_norm": 29.675020217895508,
      "learning_rate": 8.438938357637545e-06,
      "loss": 1.0472,
      "step": 6212
    },
    {
      "epoch": 2.40534262485482,
      "grad_norm": 16.620840072631836,
      "learning_rate": 8.438508194605756e-06,
      "loss": 1.3791,
      "step": 6213
    },
    {
      "epoch": 2.40572977158343,
      "grad_norm": 14.677718162536621,
      "learning_rate": 8.438078031573968e-06,
      "loss": 1.1026,
      "step": 6214
    },
    {
      "epoch": 2.40611691831204,
      "grad_norm": 20.88092803955078,
      "learning_rate": 8.437647868542179e-06,
      "loss": 1.7607,
      "step": 6215
    },
    {
      "epoch": 2.40650406504065,
      "grad_norm": 25.328107833862305,
      "learning_rate": 8.437217705510389e-06,
      "loss": 1.5865,
      "step": 6216
    },
    {
      "epoch": 2.4068912117692607,
      "grad_norm": 23.251537322998047,
      "learning_rate": 8.4367875424786e-06,
      "loss": 1.6613,
      "step": 6217
    },
    {
      "epoch": 2.4072783584978708,
      "grad_norm": 23.738025665283203,
      "learning_rate": 8.436357379446811e-06,
      "loss": 1.549,
      "step": 6218
    },
    {
      "epoch": 2.407665505226481,
      "grad_norm": 13.182076454162598,
      "learning_rate": 8.435927216415021e-06,
      "loss": 1.3755,
      "step": 6219
    },
    {
      "epoch": 2.408052651955091,
      "grad_norm": 14.490857124328613,
      "learning_rate": 8.435497053383233e-06,
      "loss": 1.4084,
      "step": 6220
    },
    {
      "epoch": 2.408439798683701,
      "grad_norm": 17.54180145263672,
      "learning_rate": 8.435066890351444e-06,
      "loss": 1.447,
      "step": 6221
    },
    {
      "epoch": 2.4088269454123115,
      "grad_norm": 14.478226661682129,
      "learning_rate": 8.434636727319655e-06,
      "loss": 1.4666,
      "step": 6222
    },
    {
      "epoch": 2.4092140921409215,
      "grad_norm": 17.122493743896484,
      "learning_rate": 8.434206564287865e-06,
      "loss": 0.7495,
      "step": 6223
    },
    {
      "epoch": 2.4096012388695316,
      "grad_norm": 36.415401458740234,
      "learning_rate": 8.433776401256077e-06,
      "loss": 1.711,
      "step": 6224
    },
    {
      "epoch": 2.4099883855981417,
      "grad_norm": 21.006866455078125,
      "learning_rate": 8.433346238224288e-06,
      "loss": 1.5105,
      "step": 6225
    },
    {
      "epoch": 2.4103755323267517,
      "grad_norm": 20.45578956604004,
      "learning_rate": 8.4329160751925e-06,
      "loss": 1.7227,
      "step": 6226
    },
    {
      "epoch": 2.410762679055362,
      "grad_norm": 30.741228103637695,
      "learning_rate": 8.432485912160709e-06,
      "loss": 1.6261,
      "step": 6227
    },
    {
      "epoch": 2.4111498257839723,
      "grad_norm": 23.10300064086914,
      "learning_rate": 8.43205574912892e-06,
      "loss": 1.7521,
      "step": 6228
    },
    {
      "epoch": 2.4115369725125824,
      "grad_norm": 17.516172409057617,
      "learning_rate": 8.431625586097132e-06,
      "loss": 1.0332,
      "step": 6229
    },
    {
      "epoch": 2.4119241192411924,
      "grad_norm": 22.151060104370117,
      "learning_rate": 8.431195423065343e-06,
      "loss": 1.4941,
      "step": 6230
    },
    {
      "epoch": 2.4123112659698025,
      "grad_norm": 25.709291458129883,
      "learning_rate": 8.430765260033553e-06,
      "loss": 1.4034,
      "step": 6231
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 23.865503311157227,
      "learning_rate": 8.430335097001765e-06,
      "loss": 1.1569,
      "step": 6232
    },
    {
      "epoch": 2.4130855594270226,
      "grad_norm": 24.322237014770508,
      "learning_rate": 8.429904933969976e-06,
      "loss": 2.8859,
      "step": 6233
    },
    {
      "epoch": 2.413472706155633,
      "grad_norm": 23.0448055267334,
      "learning_rate": 8.429474770938186e-06,
      "loss": 2.715,
      "step": 6234
    },
    {
      "epoch": 2.413859852884243,
      "grad_norm": 40.101768493652344,
      "learning_rate": 8.429044607906397e-06,
      "loss": 2.287,
      "step": 6235
    },
    {
      "epoch": 2.4142469996128533,
      "grad_norm": 23.89995765686035,
      "learning_rate": 8.428614444874608e-06,
      "loss": 2.1142,
      "step": 6236
    },
    {
      "epoch": 2.4146341463414633,
      "grad_norm": 39.29611587524414,
      "learning_rate": 8.42818428184282e-06,
      "loss": 0.867,
      "step": 6237
    },
    {
      "epoch": 2.4150212930700734,
      "grad_norm": 13.253388404846191,
      "learning_rate": 8.42775411881103e-06,
      "loss": 1.3103,
      "step": 6238
    },
    {
      "epoch": 2.415408439798684,
      "grad_norm": 27.254573822021484,
      "learning_rate": 8.427323955779241e-06,
      "loss": 1.5247,
      "step": 6239
    },
    {
      "epoch": 2.415795586527294,
      "grad_norm": 42.580440521240234,
      "learning_rate": 8.42689379274745e-06,
      "loss": 1.9903,
      "step": 6240
    },
    {
      "epoch": 2.416182733255904,
      "grad_norm": 14.56130599975586,
      "learning_rate": 8.426463629715664e-06,
      "loss": 0.8502,
      "step": 6241
    },
    {
      "epoch": 2.416569879984514,
      "grad_norm": 21.609086990356445,
      "learning_rate": 8.426033466683874e-06,
      "loss": 0.6911,
      "step": 6242
    },
    {
      "epoch": 2.416957026713124,
      "grad_norm": 23.44732666015625,
      "learning_rate": 8.425603303652085e-06,
      "loss": 2.2463,
      "step": 6243
    },
    {
      "epoch": 2.4173441734417342,
      "grad_norm": 25.21940040588379,
      "learning_rate": 8.425173140620295e-06,
      "loss": 2.4894,
      "step": 6244
    },
    {
      "epoch": 2.4177313201703443,
      "grad_norm": 22.477615356445312,
      "learning_rate": 8.424742977588508e-06,
      "loss": 1.4275,
      "step": 6245
    },
    {
      "epoch": 2.418118466898955,
      "grad_norm": 17.075342178344727,
      "learning_rate": 8.424312814556718e-06,
      "loss": 1.3552,
      "step": 6246
    },
    {
      "epoch": 2.418505613627565,
      "grad_norm": 27.75156021118164,
      "learning_rate": 8.423882651524929e-06,
      "loss": 1.195,
      "step": 6247
    },
    {
      "epoch": 2.418892760356175,
      "grad_norm": 19.09430503845215,
      "learning_rate": 8.42345248849314e-06,
      "loss": 1.2296,
      "step": 6248
    },
    {
      "epoch": 2.419279907084785,
      "grad_norm": 26.630151748657227,
      "learning_rate": 8.42302232546135e-06,
      "loss": 2.324,
      "step": 6249
    },
    {
      "epoch": 2.419667053813395,
      "grad_norm": 16.66210174560547,
      "learning_rate": 8.422592162429562e-06,
      "loss": 1.2151,
      "step": 6250
    },
    {
      "epoch": 2.4200542005420056,
      "grad_norm": 17.32227325439453,
      "learning_rate": 8.422161999397773e-06,
      "loss": 1.7544,
      "step": 6251
    },
    {
      "epoch": 2.4204413472706157,
      "grad_norm": 27.18492317199707,
      "learning_rate": 8.421731836365984e-06,
      "loss": 1.6082,
      "step": 6252
    },
    {
      "epoch": 2.4208284939992257,
      "grad_norm": 14.895359992980957,
      "learning_rate": 8.421301673334194e-06,
      "loss": 1.4036,
      "step": 6253
    },
    {
      "epoch": 2.421215640727836,
      "grad_norm": 24.46790313720703,
      "learning_rate": 8.420871510302405e-06,
      "loss": 2.4383,
      "step": 6254
    },
    {
      "epoch": 2.421602787456446,
      "grad_norm": 14.973274230957031,
      "learning_rate": 8.420441347270615e-06,
      "loss": 0.842,
      "step": 6255
    },
    {
      "epoch": 2.4219899341850564,
      "grad_norm": 23.393075942993164,
      "learning_rate": 8.420011184238828e-06,
      "loss": 1.3373,
      "step": 6256
    },
    {
      "epoch": 2.4223770809136664,
      "grad_norm": 20.537246704101562,
      "learning_rate": 8.419581021207038e-06,
      "loss": 1.2956,
      "step": 6257
    },
    {
      "epoch": 2.4227642276422765,
      "grad_norm": 25.455684661865234,
      "learning_rate": 8.41915085817525e-06,
      "loss": 1.4185,
      "step": 6258
    },
    {
      "epoch": 2.4231513743708866,
      "grad_norm": 21.06663703918457,
      "learning_rate": 8.41872069514346e-06,
      "loss": 0.6718,
      "step": 6259
    },
    {
      "epoch": 2.4235385210994966,
      "grad_norm": 13.819876670837402,
      "learning_rate": 8.418290532111672e-06,
      "loss": 0.5479,
      "step": 6260
    },
    {
      "epoch": 2.4239256678281067,
      "grad_norm": 14.822278022766113,
      "learning_rate": 8.417860369079882e-06,
      "loss": 0.8755,
      "step": 6261
    },
    {
      "epoch": 2.4243128145567168,
      "grad_norm": 14.556396484375,
      "learning_rate": 8.417430206048093e-06,
      "loss": 0.9213,
      "step": 6262
    },
    {
      "epoch": 2.4246999612853273,
      "grad_norm": 14.175599098205566,
      "learning_rate": 8.417000043016303e-06,
      "loss": 1.0313,
      "step": 6263
    },
    {
      "epoch": 2.4250871080139373,
      "grad_norm": 37.236690521240234,
      "learning_rate": 8.416569879984515e-06,
      "loss": 0.95,
      "step": 6264
    },
    {
      "epoch": 2.4254742547425474,
      "grad_norm": 19.559814453125,
      "learning_rate": 8.416139716952726e-06,
      "loss": 1.2442,
      "step": 6265
    },
    {
      "epoch": 2.4258614014711575,
      "grad_norm": 16.57164192199707,
      "learning_rate": 8.415709553920937e-06,
      "loss": 0.6427,
      "step": 6266
    },
    {
      "epoch": 2.4262485481997675,
      "grad_norm": 22.21108055114746,
      "learning_rate": 8.415279390889147e-06,
      "loss": 1.3235,
      "step": 6267
    },
    {
      "epoch": 2.426635694928378,
      "grad_norm": 12.977739334106445,
      "learning_rate": 8.414849227857359e-06,
      "loss": 1.362,
      "step": 6268
    },
    {
      "epoch": 2.427022841656988,
      "grad_norm": 22.848722457885742,
      "learning_rate": 8.41441906482557e-06,
      "loss": 0.9623,
      "step": 6269
    },
    {
      "epoch": 2.427409988385598,
      "grad_norm": 12.501619338989258,
      "learning_rate": 8.41398890179378e-06,
      "loss": 0.5279,
      "step": 6270
    },
    {
      "epoch": 2.4277971351142082,
      "grad_norm": 14.756318092346191,
      "learning_rate": 8.413558738761991e-06,
      "loss": 0.7671,
      "step": 6271
    },
    {
      "epoch": 2.4281842818428183,
      "grad_norm": 16.94761085510254,
      "learning_rate": 8.413128575730203e-06,
      "loss": 1.2813,
      "step": 6272
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 23.33285140991211,
      "learning_rate": 8.412698412698414e-06,
      "loss": 1.7662,
      "step": 6273
    },
    {
      "epoch": 2.428958575300039,
      "grad_norm": 68.5396499633789,
      "learning_rate": 8.412268249666624e-06,
      "loss": 1.4679,
      "step": 6274
    },
    {
      "epoch": 2.429345722028649,
      "grad_norm": 21.875844955444336,
      "learning_rate": 8.411838086634835e-06,
      "loss": 2.8793,
      "step": 6275
    },
    {
      "epoch": 2.429732868757259,
      "grad_norm": 39.89899826049805,
      "learning_rate": 8.411407923603046e-06,
      "loss": 1.6539,
      "step": 6276
    },
    {
      "epoch": 2.430120015485869,
      "grad_norm": 22.30874252319336,
      "learning_rate": 8.410977760571258e-06,
      "loss": 1.6633,
      "step": 6277
    },
    {
      "epoch": 2.430507162214479,
      "grad_norm": 19.621410369873047,
      "learning_rate": 8.410547597539468e-06,
      "loss": 1.7478,
      "step": 6278
    },
    {
      "epoch": 2.430894308943089,
      "grad_norm": 49.17440414428711,
      "learning_rate": 8.410117434507679e-06,
      "loss": 3.0444,
      "step": 6279
    },
    {
      "epoch": 2.4312814556716997,
      "grad_norm": 26.821142196655273,
      "learning_rate": 8.40968727147589e-06,
      "loss": 1.108,
      "step": 6280
    },
    {
      "epoch": 2.43166860240031,
      "grad_norm": 18.438276290893555,
      "learning_rate": 8.409257108444102e-06,
      "loss": 0.9674,
      "step": 6281
    },
    {
      "epoch": 2.43205574912892,
      "grad_norm": 25.706966400146484,
      "learning_rate": 8.408826945412312e-06,
      "loss": 1.5115,
      "step": 6282
    },
    {
      "epoch": 2.43244289585753,
      "grad_norm": 12.55277156829834,
      "learning_rate": 8.408396782380523e-06,
      "loss": 0.858,
      "step": 6283
    },
    {
      "epoch": 2.43283004258614,
      "grad_norm": 16.736812591552734,
      "learning_rate": 8.407966619348734e-06,
      "loss": 0.9655,
      "step": 6284
    },
    {
      "epoch": 2.4332171893147505,
      "grad_norm": 22.058584213256836,
      "learning_rate": 8.407536456316944e-06,
      "loss": 1.9966,
      "step": 6285
    },
    {
      "epoch": 2.4336043360433606,
      "grad_norm": 25.74349021911621,
      "learning_rate": 8.407106293285156e-06,
      "loss": 0.9378,
      "step": 6286
    },
    {
      "epoch": 2.4339914827719706,
      "grad_norm": 18.05889892578125,
      "learning_rate": 8.406676130253367e-06,
      "loss": 1.485,
      "step": 6287
    },
    {
      "epoch": 2.4343786295005807,
      "grad_norm": 25.272245407104492,
      "learning_rate": 8.406245967221578e-06,
      "loss": 1.3058,
      "step": 6288
    },
    {
      "epoch": 2.4347657762291908,
      "grad_norm": 10.123165130615234,
      "learning_rate": 8.405815804189788e-06,
      "loss": 1.2517,
      "step": 6289
    },
    {
      "epoch": 2.435152922957801,
      "grad_norm": 20.4592227935791,
      "learning_rate": 8.405385641158e-06,
      "loss": 1.2465,
      "step": 6290
    },
    {
      "epoch": 2.435540069686411,
      "grad_norm": 23.55632209777832,
      "learning_rate": 8.404955478126211e-06,
      "loss": 1.3937,
      "step": 6291
    },
    {
      "epoch": 2.4359272164150214,
      "grad_norm": 53.29998779296875,
      "learning_rate": 8.404525315094422e-06,
      "loss": 1.0956,
      "step": 6292
    },
    {
      "epoch": 2.4363143631436315,
      "grad_norm": 33.13041305541992,
      "learning_rate": 8.404095152062632e-06,
      "loss": 1.3955,
      "step": 6293
    },
    {
      "epoch": 2.4367015098722415,
      "grad_norm": 39.90620040893555,
      "learning_rate": 8.403664989030843e-06,
      "loss": 2.2177,
      "step": 6294
    },
    {
      "epoch": 2.4370886566008516,
      "grad_norm": 41.448570251464844,
      "learning_rate": 8.403234825999055e-06,
      "loss": 1.4731,
      "step": 6295
    },
    {
      "epoch": 2.4374758033294617,
      "grad_norm": 15.625779151916504,
      "learning_rate": 8.402804662967266e-06,
      "loss": 0.9031,
      "step": 6296
    },
    {
      "epoch": 2.437862950058072,
      "grad_norm": 24.292388916015625,
      "learning_rate": 8.402374499935476e-06,
      "loss": 1.4137,
      "step": 6297
    },
    {
      "epoch": 2.4382500967866823,
      "grad_norm": 29.515207290649414,
      "learning_rate": 8.401944336903687e-06,
      "loss": 1.831,
      "step": 6298
    },
    {
      "epoch": 2.4386372435152923,
      "grad_norm": 26.404340744018555,
      "learning_rate": 8.401514173871899e-06,
      "loss": 2.5469,
      "step": 6299
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 26.713592529296875,
      "learning_rate": 8.401084010840109e-06,
      "loss": 1.7978,
      "step": 6300
    },
    {
      "epoch": 2.4394115369725125,
      "grad_norm": 33.061912536621094,
      "learning_rate": 8.40065384780832e-06,
      "loss": 2.9946,
      "step": 6301
    },
    {
      "epoch": 2.439798683701123,
      "grad_norm": 18.038585662841797,
      "learning_rate": 8.400223684776531e-06,
      "loss": 1.3512,
      "step": 6302
    },
    {
      "epoch": 2.440185830429733,
      "grad_norm": 16.372522354125977,
      "learning_rate": 8.399793521744743e-06,
      "loss": 1.4022,
      "step": 6303
    },
    {
      "epoch": 2.440572977158343,
      "grad_norm": 24.492897033691406,
      "learning_rate": 8.399363358712953e-06,
      "loss": 1.3218,
      "step": 6304
    },
    {
      "epoch": 2.440960123886953,
      "grad_norm": 24.998523712158203,
      "learning_rate": 8.398933195681164e-06,
      "loss": 2.0003,
      "step": 6305
    },
    {
      "epoch": 2.4413472706155632,
      "grad_norm": 19.79254722595215,
      "learning_rate": 8.398503032649374e-06,
      "loss": 1.7504,
      "step": 6306
    },
    {
      "epoch": 2.4417344173441733,
      "grad_norm": 18.14030647277832,
      "learning_rate": 8.398072869617587e-06,
      "loss": 1.0493,
      "step": 6307
    },
    {
      "epoch": 2.4421215640727834,
      "grad_norm": 16.4326114654541,
      "learning_rate": 8.397642706585797e-06,
      "loss": 1.4219,
      "step": 6308
    },
    {
      "epoch": 2.442508710801394,
      "grad_norm": 42.04150390625,
      "learning_rate": 8.397212543554008e-06,
      "loss": 1.2428,
      "step": 6309
    },
    {
      "epoch": 2.442895857530004,
      "grad_norm": 16.29375648498535,
      "learning_rate": 8.396782380522218e-06,
      "loss": 1.4741,
      "step": 6310
    },
    {
      "epoch": 2.443283004258614,
      "grad_norm": 23.86681365966797,
      "learning_rate": 8.39635221749043e-06,
      "loss": 1.7387,
      "step": 6311
    },
    {
      "epoch": 2.443670150987224,
      "grad_norm": 29.657541275024414,
      "learning_rate": 8.39592205445864e-06,
      "loss": 1.5742,
      "step": 6312
    },
    {
      "epoch": 2.444057297715834,
      "grad_norm": 23.127756118774414,
      "learning_rate": 8.395491891426852e-06,
      "loss": 1.8045,
      "step": 6313
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 18.95789337158203,
      "learning_rate": 8.395061728395062e-06,
      "loss": 1.8676,
      "step": 6314
    },
    {
      "epoch": 2.4448315911730547,
      "grad_norm": 18.45952796936035,
      "learning_rate": 8.394631565363273e-06,
      "loss": 1.0997,
      "step": 6315
    },
    {
      "epoch": 2.4452187379016648,
      "grad_norm": 27.50907325744629,
      "learning_rate": 8.394201402331484e-06,
      "loss": 1.28,
      "step": 6316
    },
    {
      "epoch": 2.445605884630275,
      "grad_norm": 45.35536193847656,
      "learning_rate": 8.393771239299696e-06,
      "loss": 1.3175,
      "step": 6317
    },
    {
      "epoch": 2.445993031358885,
      "grad_norm": 19.08108139038086,
      "learning_rate": 8.393341076267906e-06,
      "loss": 1.4162,
      "step": 6318
    },
    {
      "epoch": 2.446380178087495,
      "grad_norm": 18.915834426879883,
      "learning_rate": 8.392910913236117e-06,
      "loss": 1.3588,
      "step": 6319
    },
    {
      "epoch": 2.4467673248161055,
      "grad_norm": 19.92026710510254,
      "learning_rate": 8.392480750204328e-06,
      "loss": 1.4655,
      "step": 6320
    },
    {
      "epoch": 2.4471544715447155,
      "grad_norm": 16.403217315673828,
      "learning_rate": 8.392050587172538e-06,
      "loss": 1.5695,
      "step": 6321
    },
    {
      "epoch": 2.4475416182733256,
      "grad_norm": 15.074677467346191,
      "learning_rate": 8.39162042414075e-06,
      "loss": 1.5251,
      "step": 6322
    },
    {
      "epoch": 2.4479287650019357,
      "grad_norm": 19.825637817382812,
      "learning_rate": 8.391190261108961e-06,
      "loss": 1.1483,
      "step": 6323
    },
    {
      "epoch": 2.4483159117305457,
      "grad_norm": 31.022438049316406,
      "learning_rate": 8.390760098077172e-06,
      "loss": 1.2674,
      "step": 6324
    },
    {
      "epoch": 2.448703058459156,
      "grad_norm": 20.800016403198242,
      "learning_rate": 8.390329935045382e-06,
      "loss": 1.9972,
      "step": 6325
    },
    {
      "epoch": 2.4490902051877663,
      "grad_norm": 12.31654167175293,
      "learning_rate": 8.389899772013594e-06,
      "loss": 0.7053,
      "step": 6326
    },
    {
      "epoch": 2.4494773519163764,
      "grad_norm": 12.888931274414062,
      "learning_rate": 8.389469608981805e-06,
      "loss": 0.4339,
      "step": 6327
    },
    {
      "epoch": 2.4498644986449865,
      "grad_norm": 15.141844749450684,
      "learning_rate": 8.389039445950016e-06,
      "loss": 1.4021,
      "step": 6328
    },
    {
      "epoch": 2.4502516453735965,
      "grad_norm": 14.302535057067871,
      "learning_rate": 8.388609282918226e-06,
      "loss": 1.3901,
      "step": 6329
    },
    {
      "epoch": 2.4506387921022066,
      "grad_norm": 11.26046085357666,
      "learning_rate": 8.388179119886438e-06,
      "loss": 0.4115,
      "step": 6330
    },
    {
      "epoch": 2.451025938830817,
      "grad_norm": 94.68094635009766,
      "learning_rate": 8.387748956854649e-06,
      "loss": 0.5937,
      "step": 6331
    },
    {
      "epoch": 2.451413085559427,
      "grad_norm": 25.672924041748047,
      "learning_rate": 8.38731879382286e-06,
      "loss": 1.1068,
      "step": 6332
    },
    {
      "epoch": 2.4518002322880372,
      "grad_norm": 21.65131187438965,
      "learning_rate": 8.38688863079107e-06,
      "loss": 1.1915,
      "step": 6333
    },
    {
      "epoch": 2.4521873790166473,
      "grad_norm": 16.419160842895508,
      "learning_rate": 8.386458467759281e-06,
      "loss": 1.2489,
      "step": 6334
    },
    {
      "epoch": 2.4525745257452574,
      "grad_norm": 7.331625461578369,
      "learning_rate": 8.386028304727493e-06,
      "loss": 0.2425,
      "step": 6335
    },
    {
      "epoch": 2.4529616724738674,
      "grad_norm": 22.443077087402344,
      "learning_rate": 8.385598141695703e-06,
      "loss": 2.0199,
      "step": 6336
    },
    {
      "epoch": 2.4533488192024775,
      "grad_norm": 15.088494300842285,
      "learning_rate": 8.385167978663914e-06,
      "loss": 0.9519,
      "step": 6337
    },
    {
      "epoch": 2.453735965931088,
      "grad_norm": 19.03240203857422,
      "learning_rate": 8.384737815632125e-06,
      "loss": 1.7798,
      "step": 6338
    },
    {
      "epoch": 2.454123112659698,
      "grad_norm": 51.68601989746094,
      "learning_rate": 8.384307652600337e-06,
      "loss": 2.0561,
      "step": 6339
    },
    {
      "epoch": 2.454510259388308,
      "grad_norm": 22.642478942871094,
      "learning_rate": 8.383877489568547e-06,
      "loss": 1.4016,
      "step": 6340
    },
    {
      "epoch": 2.454897406116918,
      "grad_norm": 21.68618392944336,
      "learning_rate": 8.383447326536758e-06,
      "loss": 1.4674,
      "step": 6341
    },
    {
      "epoch": 2.4552845528455283,
      "grad_norm": 13.92357349395752,
      "learning_rate": 8.38301716350497e-06,
      "loss": 1.4743,
      "step": 6342
    },
    {
      "epoch": 2.4556716995741388,
      "grad_norm": 15.177433967590332,
      "learning_rate": 8.38258700047318e-06,
      "loss": 0.9476,
      "step": 6343
    },
    {
      "epoch": 2.456058846302749,
      "grad_norm": 13.003645896911621,
      "learning_rate": 8.38215683744139e-06,
      "loss": 0.4819,
      "step": 6344
    },
    {
      "epoch": 2.456445993031359,
      "grad_norm": 38.822750091552734,
      "learning_rate": 8.381726674409602e-06,
      "loss": 1.6674,
      "step": 6345
    },
    {
      "epoch": 2.456833139759969,
      "grad_norm": 30.124799728393555,
      "learning_rate": 8.381296511377813e-06,
      "loss": 1.5944,
      "step": 6346
    },
    {
      "epoch": 2.457220286488579,
      "grad_norm": 39.977962493896484,
      "learning_rate": 8.380866348346025e-06,
      "loss": 1.8369,
      "step": 6347
    },
    {
      "epoch": 2.4576074332171896,
      "grad_norm": 28.07257843017578,
      "learning_rate": 8.380436185314235e-06,
      "loss": 2.0751,
      "step": 6348
    },
    {
      "epoch": 2.4579945799457996,
      "grad_norm": 50.843929290771484,
      "learning_rate": 8.380006022282446e-06,
      "loss": 1.5064,
      "step": 6349
    },
    {
      "epoch": 2.4583817266744097,
      "grad_norm": 15.909407615661621,
      "learning_rate": 8.379575859250657e-06,
      "loss": 0.7494,
      "step": 6350
    },
    {
      "epoch": 2.4587688734030198,
      "grad_norm": 16.481687545776367,
      "learning_rate": 8.379145696218867e-06,
      "loss": 0.9217,
      "step": 6351
    },
    {
      "epoch": 2.45915602013163,
      "grad_norm": 27.587350845336914,
      "learning_rate": 8.378715533187078e-06,
      "loss": 1.9889,
      "step": 6352
    },
    {
      "epoch": 2.45954316686024,
      "grad_norm": 15.80490493774414,
      "learning_rate": 8.37828537015529e-06,
      "loss": 1.0739,
      "step": 6353
    },
    {
      "epoch": 2.45993031358885,
      "grad_norm": 20.261703491210938,
      "learning_rate": 8.377855207123501e-06,
      "loss": 1.1433,
      "step": 6354
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 17.32928466796875,
      "learning_rate": 8.377425044091711e-06,
      "loss": 1.5428,
      "step": 6355
    },
    {
      "epoch": 2.4607046070460705,
      "grad_norm": 42.903724670410156,
      "learning_rate": 8.376994881059922e-06,
      "loss": 2.3693,
      "step": 6356
    },
    {
      "epoch": 2.4610917537746806,
      "grad_norm": 41.31612014770508,
      "learning_rate": 8.376564718028132e-06,
      "loss": 2.6168,
      "step": 6357
    },
    {
      "epoch": 2.4614789005032907,
      "grad_norm": 38.12701416015625,
      "learning_rate": 8.376134554996345e-06,
      "loss": 2.636,
      "step": 6358
    },
    {
      "epoch": 2.4618660472319007,
      "grad_norm": 23.95599365234375,
      "learning_rate": 8.375704391964555e-06,
      "loss": 1.508,
      "step": 6359
    },
    {
      "epoch": 2.4622531939605112,
      "grad_norm": 36.11648941040039,
      "learning_rate": 8.375274228932766e-06,
      "loss": 1.8334,
      "step": 6360
    },
    {
      "epoch": 2.4626403406891213,
      "grad_norm": 18.93311309814453,
      "learning_rate": 8.374844065900976e-06,
      "loss": 0.592,
      "step": 6361
    },
    {
      "epoch": 2.4630274874177314,
      "grad_norm": 13.422120094299316,
      "learning_rate": 8.37441390286919e-06,
      "loss": 0.4775,
      "step": 6362
    },
    {
      "epoch": 2.4634146341463414,
      "grad_norm": 39.44029235839844,
      "learning_rate": 8.373983739837399e-06,
      "loss": 2.1909,
      "step": 6363
    },
    {
      "epoch": 2.4638017808749515,
      "grad_norm": 22.295650482177734,
      "learning_rate": 8.37355357680561e-06,
      "loss": 1.1986,
      "step": 6364
    },
    {
      "epoch": 2.4641889276035616,
      "grad_norm": 20.437030792236328,
      "learning_rate": 8.37312341377382e-06,
      "loss": 1.7676,
      "step": 6365
    },
    {
      "epoch": 2.464576074332172,
      "grad_norm": 31.021055221557617,
      "learning_rate": 8.372693250742032e-06,
      "loss": 1.6402,
      "step": 6366
    },
    {
      "epoch": 2.464963221060782,
      "grad_norm": 34.13609313964844,
      "learning_rate": 8.372263087710243e-06,
      "loss": 1.9091,
      "step": 6367
    },
    {
      "epoch": 2.465350367789392,
      "grad_norm": 10.763948440551758,
      "learning_rate": 8.371832924678454e-06,
      "loss": 0.5666,
      "step": 6368
    },
    {
      "epoch": 2.4657375145180023,
      "grad_norm": 19.3569278717041,
      "learning_rate": 8.371402761646666e-06,
      "loss": 0.9033,
      "step": 6369
    },
    {
      "epoch": 2.4661246612466123,
      "grad_norm": 19.78533363342285,
      "learning_rate": 8.370972598614876e-06,
      "loss": 1.5448,
      "step": 6370
    },
    {
      "epoch": 2.4665118079752224,
      "grad_norm": 30.69906234741211,
      "learning_rate": 8.370542435583087e-06,
      "loss": 1.6682,
      "step": 6371
    },
    {
      "epoch": 2.466898954703833,
      "grad_norm": 33.40437316894531,
      "learning_rate": 8.370112272551297e-06,
      "loss": 1.4918,
      "step": 6372
    },
    {
      "epoch": 2.467286101432443,
      "grad_norm": 27.83169174194336,
      "learning_rate": 8.36968210951951e-06,
      "loss": 2.2167,
      "step": 6373
    },
    {
      "epoch": 2.467673248161053,
      "grad_norm": 24.697975158691406,
      "learning_rate": 8.36925194648772e-06,
      "loss": 1.2551,
      "step": 6374
    },
    {
      "epoch": 2.468060394889663,
      "grad_norm": 28.488380432128906,
      "learning_rate": 8.368821783455931e-06,
      "loss": 1.7834,
      "step": 6375
    },
    {
      "epoch": 2.468447541618273,
      "grad_norm": 13.82490348815918,
      "learning_rate": 8.36839162042414e-06,
      "loss": 1.0499,
      "step": 6376
    },
    {
      "epoch": 2.4688346883468837,
      "grad_norm": 22.374711990356445,
      "learning_rate": 8.367961457392354e-06,
      "loss": 2.0092,
      "step": 6377
    },
    {
      "epoch": 2.4692218350754938,
      "grad_norm": 20.638517379760742,
      "learning_rate": 8.367531294360563e-06,
      "loss": 0.3966,
      "step": 6378
    },
    {
      "epoch": 2.469608981804104,
      "grad_norm": 26.712934494018555,
      "learning_rate": 8.367101131328775e-06,
      "loss": 1.1923,
      "step": 6379
    },
    {
      "epoch": 2.469996128532714,
      "grad_norm": 23.937971115112305,
      "learning_rate": 8.366670968296985e-06,
      "loss": 3.1783,
      "step": 6380
    },
    {
      "epoch": 2.470383275261324,
      "grad_norm": 10.513495445251465,
      "learning_rate": 8.366240805265196e-06,
      "loss": 0.5665,
      "step": 6381
    },
    {
      "epoch": 2.470770421989934,
      "grad_norm": 22.92256736755371,
      "learning_rate": 8.365810642233407e-06,
      "loss": 1.5818,
      "step": 6382
    },
    {
      "epoch": 2.471157568718544,
      "grad_norm": 18.993579864501953,
      "learning_rate": 8.365380479201619e-06,
      "loss": 0.6446,
      "step": 6383
    },
    {
      "epoch": 2.4715447154471546,
      "grad_norm": 22.955923080444336,
      "learning_rate": 8.364950316169829e-06,
      "loss": 2.1175,
      "step": 6384
    },
    {
      "epoch": 2.4719318621757647,
      "grad_norm": 31.878978729248047,
      "learning_rate": 8.36452015313804e-06,
      "loss": 1.6742,
      "step": 6385
    },
    {
      "epoch": 2.4723190089043747,
      "grad_norm": 26.647581100463867,
      "learning_rate": 8.364089990106251e-06,
      "loss": 1.7651,
      "step": 6386
    },
    {
      "epoch": 2.472706155632985,
      "grad_norm": 13.991878509521484,
      "learning_rate": 8.363659827074461e-06,
      "loss": 1.1961,
      "step": 6387
    },
    {
      "epoch": 2.473093302361595,
      "grad_norm": 28.434051513671875,
      "learning_rate": 8.363229664042673e-06,
      "loss": 2.0341,
      "step": 6388
    },
    {
      "epoch": 2.4734804490902054,
      "grad_norm": 21.190807342529297,
      "learning_rate": 8.362799501010884e-06,
      "loss": 1.8229,
      "step": 6389
    },
    {
      "epoch": 2.4738675958188154,
      "grad_norm": 14.663166046142578,
      "learning_rate": 8.362369337979095e-06,
      "loss": 0.9699,
      "step": 6390
    },
    {
      "epoch": 2.4742547425474255,
      "grad_norm": 16.903968811035156,
      "learning_rate": 8.361939174947305e-06,
      "loss": 1.2111,
      "step": 6391
    },
    {
      "epoch": 2.4746418892760356,
      "grad_norm": 12.333232879638672,
      "learning_rate": 8.361509011915516e-06,
      "loss": 0.7889,
      "step": 6392
    },
    {
      "epoch": 2.4750290360046456,
      "grad_norm": 32.46806335449219,
      "learning_rate": 8.361078848883728e-06,
      "loss": 1.1843,
      "step": 6393
    },
    {
      "epoch": 2.475416182733256,
      "grad_norm": 12.413578987121582,
      "learning_rate": 8.36064868585194e-06,
      "loss": 0.596,
      "step": 6394
    },
    {
      "epoch": 2.475803329461866,
      "grad_norm": 34.290435791015625,
      "learning_rate": 8.360218522820149e-06,
      "loss": 1.8103,
      "step": 6395
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 33.80609893798828,
      "learning_rate": 8.35978835978836e-06,
      "loss": 1.8133,
      "step": 6396
    },
    {
      "epoch": 2.4765776229190863,
      "grad_norm": 20.611927032470703,
      "learning_rate": 8.359358196756572e-06,
      "loss": 1.4829,
      "step": 6397
    },
    {
      "epoch": 2.4769647696476964,
      "grad_norm": 20.88433265686035,
      "learning_rate": 8.358928033724783e-06,
      "loss": 1.4924,
      "step": 6398
    },
    {
      "epoch": 2.4773519163763065,
      "grad_norm": 29.755470275878906,
      "learning_rate": 8.358497870692993e-06,
      "loss": 2.4644,
      "step": 6399
    },
    {
      "epoch": 2.4777390631049165,
      "grad_norm": 20.209327697753906,
      "learning_rate": 8.358067707661204e-06,
      "loss": 1.0658,
      "step": 6400
    },
    {
      "epoch": 2.478126209833527,
      "grad_norm": 17.276151657104492,
      "learning_rate": 8.357637544629416e-06,
      "loss": 1.0288,
      "step": 6401
    },
    {
      "epoch": 2.478513356562137,
      "grad_norm": 21.16608238220215,
      "learning_rate": 8.357207381597626e-06,
      "loss": 1.3382,
      "step": 6402
    },
    {
      "epoch": 2.478900503290747,
      "grad_norm": 25.167957305908203,
      "learning_rate": 8.356777218565837e-06,
      "loss": 1.6119,
      "step": 6403
    },
    {
      "epoch": 2.4792876500193572,
      "grad_norm": 11.307354927062988,
      "learning_rate": 8.356347055534048e-06,
      "loss": 0.8336,
      "step": 6404
    },
    {
      "epoch": 2.4796747967479673,
      "grad_norm": 29.29197883605957,
      "learning_rate": 8.35591689250226e-06,
      "loss": 1.3927,
      "step": 6405
    },
    {
      "epoch": 2.480061943476578,
      "grad_norm": 27.577444076538086,
      "learning_rate": 8.35548672947047e-06,
      "loss": 1.7345,
      "step": 6406
    },
    {
      "epoch": 2.480449090205188,
      "grad_norm": 20.10213279724121,
      "learning_rate": 8.355056566438681e-06,
      "loss": 1.3878,
      "step": 6407
    },
    {
      "epoch": 2.480836236933798,
      "grad_norm": 23.97292709350586,
      "learning_rate": 8.35462640340689e-06,
      "loss": 0.9595,
      "step": 6408
    },
    {
      "epoch": 2.481223383662408,
      "grad_norm": 10.995614051818848,
      "learning_rate": 8.354196240375104e-06,
      "loss": 0.6218,
      "step": 6409
    },
    {
      "epoch": 2.481610530391018,
      "grad_norm": 23.653371810913086,
      "learning_rate": 8.353766077343314e-06,
      "loss": 0.9506,
      "step": 6410
    },
    {
      "epoch": 2.481997677119628,
      "grad_norm": 46.29795455932617,
      "learning_rate": 8.353335914311525e-06,
      "loss": 2.3458,
      "step": 6411
    },
    {
      "epoch": 2.4823848238482387,
      "grad_norm": 27.498046875,
      "learning_rate": 8.352905751279736e-06,
      "loss": 1.7224,
      "step": 6412
    },
    {
      "epoch": 2.4827719705768487,
      "grad_norm": 41.8160514831543,
      "learning_rate": 8.352475588247948e-06,
      "loss": 1.5843,
      "step": 6413
    },
    {
      "epoch": 2.483159117305459,
      "grad_norm": 9.707711219787598,
      "learning_rate": 8.352045425216157e-06,
      "loss": 0.5373,
      "step": 6414
    },
    {
      "epoch": 2.483546264034069,
      "grad_norm": 18.43319320678711,
      "learning_rate": 8.351615262184369e-06,
      "loss": 1.3349,
      "step": 6415
    },
    {
      "epoch": 2.483933410762679,
      "grad_norm": 29.621658325195312,
      "learning_rate": 8.35118509915258e-06,
      "loss": 0.6948,
      "step": 6416
    },
    {
      "epoch": 2.484320557491289,
      "grad_norm": 20.605670928955078,
      "learning_rate": 8.35075493612079e-06,
      "loss": 3.5896,
      "step": 6417
    },
    {
      "epoch": 2.4847077042198995,
      "grad_norm": 10.676511764526367,
      "learning_rate": 8.350324773089001e-06,
      "loss": 0.6222,
      "step": 6418
    },
    {
      "epoch": 2.4850948509485096,
      "grad_norm": 18.480836868286133,
      "learning_rate": 8.349894610057213e-06,
      "loss": 1.0457,
      "step": 6419
    },
    {
      "epoch": 2.4854819976771196,
      "grad_norm": 14.305941581726074,
      "learning_rate": 8.349464447025424e-06,
      "loss": 1.4758,
      "step": 6420
    },
    {
      "epoch": 2.4858691444057297,
      "grad_norm": 41.0087776184082,
      "learning_rate": 8.349034283993634e-06,
      "loss": 2.4319,
      "step": 6421
    },
    {
      "epoch": 2.4862562911343398,
      "grad_norm": 23.945878982543945,
      "learning_rate": 8.348604120961845e-06,
      "loss": 1.865,
      "step": 6422
    },
    {
      "epoch": 2.4866434378629503,
      "grad_norm": 14.438483238220215,
      "learning_rate": 8.348173957930055e-06,
      "loss": 3.0088,
      "step": 6423
    },
    {
      "epoch": 2.4870305845915603,
      "grad_norm": 23.855649948120117,
      "learning_rate": 8.347743794898268e-06,
      "loss": 1.6305,
      "step": 6424
    },
    {
      "epoch": 2.4874177313201704,
      "grad_norm": 33.56563186645508,
      "learning_rate": 8.347313631866478e-06,
      "loss": 1.9426,
      "step": 6425
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 16.804811477661133,
      "learning_rate": 8.34688346883469e-06,
      "loss": 1.6176,
      "step": 6426
    },
    {
      "epoch": 2.4881920247773905,
      "grad_norm": 9.574609756469727,
      "learning_rate": 8.346453305802899e-06,
      "loss": 1.1984,
      "step": 6427
    },
    {
      "epoch": 2.4885791715060006,
      "grad_norm": 20.52532958984375,
      "learning_rate": 8.34602314277111e-06,
      "loss": 1.2537,
      "step": 6428
    },
    {
      "epoch": 2.4889663182346107,
      "grad_norm": 11.642061233520508,
      "learning_rate": 8.345592979739322e-06,
      "loss": 0.7643,
      "step": 6429
    },
    {
      "epoch": 2.489353464963221,
      "grad_norm": 13.016176223754883,
      "learning_rate": 8.345162816707533e-06,
      "loss": 1.3334,
      "step": 6430
    },
    {
      "epoch": 2.4897406116918313,
      "grad_norm": 20.34708023071289,
      "learning_rate": 8.344732653675743e-06,
      "loss": 1.488,
      "step": 6431
    },
    {
      "epoch": 2.4901277584204413,
      "grad_norm": 27.672040939331055,
      "learning_rate": 8.344302490643954e-06,
      "loss": 1.4394,
      "step": 6432
    },
    {
      "epoch": 2.4905149051490514,
      "grad_norm": 10.745226860046387,
      "learning_rate": 8.343872327612166e-06,
      "loss": 0.5169,
      "step": 6433
    },
    {
      "epoch": 2.4909020518776614,
      "grad_norm": 24.006359100341797,
      "learning_rate": 8.343442164580377e-06,
      "loss": 1.6964,
      "step": 6434
    },
    {
      "epoch": 2.491289198606272,
      "grad_norm": 23.519914627075195,
      "learning_rate": 8.343012001548587e-06,
      "loss": 0.9926,
      "step": 6435
    },
    {
      "epoch": 2.491676345334882,
      "grad_norm": 20.41346549987793,
      "learning_rate": 8.342581838516798e-06,
      "loss": 0.6993,
      "step": 6436
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 28.504972457885742,
      "learning_rate": 8.34215167548501e-06,
      "loss": 1.4923,
      "step": 6437
    },
    {
      "epoch": 2.492450638792102,
      "grad_norm": 27.80693244934082,
      "learning_rate": 8.34172151245322e-06,
      "loss": 1.6295,
      "step": 6438
    },
    {
      "epoch": 2.4928377855207122,
      "grad_norm": 19.527021408081055,
      "learning_rate": 8.341291349421431e-06,
      "loss": 2.0295,
      "step": 6439
    },
    {
      "epoch": 2.4932249322493227,
      "grad_norm": 14.342367172241211,
      "learning_rate": 8.340861186389642e-06,
      "loss": 1.1332,
      "step": 6440
    },
    {
      "epoch": 2.493612078977933,
      "grad_norm": 13.387900352478027,
      "learning_rate": 8.340431023357854e-06,
      "loss": 0.966,
      "step": 6441
    },
    {
      "epoch": 2.493999225706543,
      "grad_norm": 21.203462600708008,
      "learning_rate": 8.340000860326064e-06,
      "loss": 1.6638,
      "step": 6442
    },
    {
      "epoch": 2.494386372435153,
      "grad_norm": 23.74569320678711,
      "learning_rate": 8.339570697294275e-06,
      "loss": 1.4196,
      "step": 6443
    },
    {
      "epoch": 2.494773519163763,
      "grad_norm": 14.787854194641113,
      "learning_rate": 8.339140534262486e-06,
      "loss": 1.5132,
      "step": 6444
    },
    {
      "epoch": 2.495160665892373,
      "grad_norm": 22.600061416625977,
      "learning_rate": 8.338710371230698e-06,
      "loss": 1.3245,
      "step": 6445
    },
    {
      "epoch": 2.495547812620983,
      "grad_norm": 15.041440963745117,
      "learning_rate": 8.338280208198908e-06,
      "loss": 1.0762,
      "step": 6446
    },
    {
      "epoch": 2.4959349593495936,
      "grad_norm": 14.043095588684082,
      "learning_rate": 8.337850045167119e-06,
      "loss": 1.2987,
      "step": 6447
    },
    {
      "epoch": 2.4963221060782037,
      "grad_norm": 19.8508358001709,
      "learning_rate": 8.33741988213533e-06,
      "loss": 1.8151,
      "step": 6448
    },
    {
      "epoch": 2.4967092528068138,
      "grad_norm": 15.666501998901367,
      "learning_rate": 8.336989719103542e-06,
      "loss": 1.53,
      "step": 6449
    },
    {
      "epoch": 2.497096399535424,
      "grad_norm": 25.57322883605957,
      "learning_rate": 8.336559556071752e-06,
      "loss": 0.8348,
      "step": 6450
    },
    {
      "epoch": 2.497483546264034,
      "grad_norm": 39.89375686645508,
      "learning_rate": 8.336129393039963e-06,
      "loss": 1.6078,
      "step": 6451
    },
    {
      "epoch": 2.4978706929926444,
      "grad_norm": 12.980964660644531,
      "learning_rate": 8.335699230008174e-06,
      "loss": 0.811,
      "step": 6452
    },
    {
      "epoch": 2.4982578397212545,
      "grad_norm": 12.289501190185547,
      "learning_rate": 8.335269066976384e-06,
      "loss": 1.1072,
      "step": 6453
    },
    {
      "epoch": 2.4986449864498645,
      "grad_norm": 14.83763313293457,
      "learning_rate": 8.334838903944595e-06,
      "loss": 1.353,
      "step": 6454
    },
    {
      "epoch": 2.4990321331784746,
      "grad_norm": 19.241870880126953,
      "learning_rate": 8.334408740912807e-06,
      "loss": 1.1406,
      "step": 6455
    },
    {
      "epoch": 2.4994192799070847,
      "grad_norm": 20.09542465209961,
      "learning_rate": 8.333978577881018e-06,
      "loss": 1.4104,
      "step": 6456
    },
    {
      "epoch": 2.4998064266356947,
      "grad_norm": 15.59621524810791,
      "learning_rate": 8.333548414849228e-06,
      "loss": 1.47,
      "step": 6457
    },
    {
      "epoch": 2.500193573364305,
      "grad_norm": 13.046630859375,
      "learning_rate": 8.33311825181744e-06,
      "loss": 1.3439,
      "step": 6458
    },
    {
      "epoch": 2.5005807200929153,
      "grad_norm": 21.735898971557617,
      "learning_rate": 8.332688088785651e-06,
      "loss": 1.7002,
      "step": 6459
    },
    {
      "epoch": 2.5009678668215254,
      "grad_norm": 7.594080448150635,
      "learning_rate": 8.332257925753862e-06,
      "loss": 0.1666,
      "step": 6460
    },
    {
      "epoch": 2.5013550135501355,
      "grad_norm": 18.987966537475586,
      "learning_rate": 8.331827762722072e-06,
      "loss": 0.6736,
      "step": 6461
    },
    {
      "epoch": 2.5017421602787455,
      "grad_norm": 21.67728042602539,
      "learning_rate": 8.331397599690283e-06,
      "loss": 1.735,
      "step": 6462
    },
    {
      "epoch": 2.5021293070073556,
      "grad_norm": 13.738360404968262,
      "learning_rate": 8.330967436658495e-06,
      "loss": 1.24,
      "step": 6463
    },
    {
      "epoch": 2.502516453735966,
      "grad_norm": 36.09787368774414,
      "learning_rate": 8.330537273626705e-06,
      "loss": 1.9508,
      "step": 6464
    },
    {
      "epoch": 2.502903600464576,
      "grad_norm": 19.52826690673828,
      "learning_rate": 8.330107110594916e-06,
      "loss": 1.8614,
      "step": 6465
    },
    {
      "epoch": 2.5032907471931862,
      "grad_norm": 19.25617218017578,
      "learning_rate": 8.329676947563127e-06,
      "loss": 1.6853,
      "step": 6466
    },
    {
      "epoch": 2.5036778939217963,
      "grad_norm": 28.533321380615234,
      "learning_rate": 8.329246784531339e-06,
      "loss": 1.7192,
      "step": 6467
    },
    {
      "epoch": 2.5040650406504064,
      "grad_norm": 17.58362579345703,
      "learning_rate": 8.328816621499549e-06,
      "loss": 0.9344,
      "step": 6468
    },
    {
      "epoch": 2.504452187379017,
      "grad_norm": 33.76087188720703,
      "learning_rate": 8.32838645846776e-06,
      "loss": 1.9695,
      "step": 6469
    },
    {
      "epoch": 2.504839334107627,
      "grad_norm": 28.106014251708984,
      "learning_rate": 8.327956295435971e-06,
      "loss": 1.1405,
      "step": 6470
    },
    {
      "epoch": 2.505226480836237,
      "grad_norm": 15.277581214904785,
      "learning_rate": 8.327526132404183e-06,
      "loss": 1.0601,
      "step": 6471
    },
    {
      "epoch": 2.505613627564847,
      "grad_norm": 12.834311485290527,
      "learning_rate": 8.327095969372392e-06,
      "loss": 1.3614,
      "step": 6472
    },
    {
      "epoch": 2.506000774293457,
      "grad_norm": 18.883420944213867,
      "learning_rate": 8.326665806340604e-06,
      "loss": 1.5467,
      "step": 6473
    },
    {
      "epoch": 2.506387921022067,
      "grad_norm": 63.41770553588867,
      "learning_rate": 8.326235643308814e-06,
      "loss": 1.7111,
      "step": 6474
    },
    {
      "epoch": 2.5067750677506773,
      "grad_norm": 23.538307189941406,
      "learning_rate": 8.325805480277027e-06,
      "loss": 1.5271,
      "step": 6475
    },
    {
      "epoch": 2.5071622144792878,
      "grad_norm": 22.240676879882812,
      "learning_rate": 8.325375317245236e-06,
      "loss": 1.4742,
      "step": 6476
    },
    {
      "epoch": 2.507549361207898,
      "grad_norm": 28.552532196044922,
      "learning_rate": 8.324945154213448e-06,
      "loss": 2.0131,
      "step": 6477
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 21.698610305786133,
      "learning_rate": 8.324514991181658e-06,
      "loss": 1.474,
      "step": 6478
    },
    {
      "epoch": 2.508323654665118,
      "grad_norm": 28.08293342590332,
      "learning_rate": 8.324084828149869e-06,
      "loss": 1.5724,
      "step": 6479
    },
    {
      "epoch": 2.508710801393728,
      "grad_norm": 23.935626983642578,
      "learning_rate": 8.32365466511808e-06,
      "loss": 1.5438,
      "step": 6480
    },
    {
      "epoch": 2.5090979481223386,
      "grad_norm": 25.333839416503906,
      "learning_rate": 8.323224502086292e-06,
      "loss": 1.3869,
      "step": 6481
    },
    {
      "epoch": 2.5094850948509486,
      "grad_norm": 25.06500244140625,
      "learning_rate": 8.322794339054502e-06,
      "loss": 1.5841,
      "step": 6482
    },
    {
      "epoch": 2.5098722415795587,
      "grad_norm": 23.306232452392578,
      "learning_rate": 8.322364176022713e-06,
      "loss": 1.8951,
      "step": 6483
    },
    {
      "epoch": 2.5102593883081687,
      "grad_norm": 18.03692054748535,
      "learning_rate": 8.321934012990924e-06,
      "loss": 1.6451,
      "step": 6484
    },
    {
      "epoch": 2.510646535036779,
      "grad_norm": 14.486477851867676,
      "learning_rate": 8.321503849959136e-06,
      "loss": 1.1125,
      "step": 6485
    },
    {
      "epoch": 2.5110336817653893,
      "grad_norm": 16.916582107543945,
      "learning_rate": 8.321073686927346e-06,
      "loss": 1.3136,
      "step": 6486
    },
    {
      "epoch": 2.5114208284939994,
      "grad_norm": 20.608919143676758,
      "learning_rate": 8.320643523895557e-06,
      "loss": 1.2984,
      "step": 6487
    },
    {
      "epoch": 2.5118079752226095,
      "grad_norm": 20.64354705810547,
      "learning_rate": 8.320213360863768e-06,
      "loss": 1.0927,
      "step": 6488
    },
    {
      "epoch": 2.5121951219512195,
      "grad_norm": 15.281045913696289,
      "learning_rate": 8.319783197831978e-06,
      "loss": 1.678,
      "step": 6489
    },
    {
      "epoch": 2.5125822686798296,
      "grad_norm": 19.368629455566406,
      "learning_rate": 8.31935303480019e-06,
      "loss": 1.0193,
      "step": 6490
    },
    {
      "epoch": 2.5129694154084397,
      "grad_norm": 14.889111518859863,
      "learning_rate": 8.318922871768401e-06,
      "loss": 1.0936,
      "step": 6491
    },
    {
      "epoch": 2.5133565621370497,
      "grad_norm": 14.584795951843262,
      "learning_rate": 8.318492708736612e-06,
      "loss": 1.1287,
      "step": 6492
    },
    {
      "epoch": 2.5137437088656602,
      "grad_norm": 14.170954704284668,
      "learning_rate": 8.318062545704822e-06,
      "loss": 1.3151,
      "step": 6493
    },
    {
      "epoch": 2.5141308555942703,
      "grad_norm": 19.815357208251953,
      "learning_rate": 8.317632382673033e-06,
      "loss": 1.735,
      "step": 6494
    },
    {
      "epoch": 2.5145180023228804,
      "grad_norm": 28.827762603759766,
      "learning_rate": 8.317202219641245e-06,
      "loss": 1.9013,
      "step": 6495
    },
    {
      "epoch": 2.5149051490514904,
      "grad_norm": 20.33135414123535,
      "learning_rate": 8.316772056609456e-06,
      "loss": 1.3839,
      "step": 6496
    },
    {
      "epoch": 2.5152922957801005,
      "grad_norm": 31.223426818847656,
      "learning_rate": 8.316341893577666e-06,
      "loss": 2.6853,
      "step": 6497
    },
    {
      "epoch": 2.515679442508711,
      "grad_norm": 14.141624450683594,
      "learning_rate": 8.315911730545877e-06,
      "loss": 1.1311,
      "step": 6498
    },
    {
      "epoch": 2.516066589237321,
      "grad_norm": 17.355968475341797,
      "learning_rate": 8.315481567514089e-06,
      "loss": 1.199,
      "step": 6499
    },
    {
      "epoch": 2.516453735965931,
      "grad_norm": 29.37929916381836,
      "learning_rate": 8.315051404482299e-06,
      "loss": 1.2436,
      "step": 6500
    },
    {
      "epoch": 2.516840882694541,
      "grad_norm": 8.809768676757812,
      "learning_rate": 8.31462124145051e-06,
      "loss": 0.3423,
      "step": 6501
    },
    {
      "epoch": 2.5172280294231513,
      "grad_norm": 19.977331161499023,
      "learning_rate": 8.314191078418721e-06,
      "loss": 1.2763,
      "step": 6502
    },
    {
      "epoch": 2.517615176151762,
      "grad_norm": 25.794357299804688,
      "learning_rate": 8.313760915386933e-06,
      "loss": 1.3346,
      "step": 6503
    },
    {
      "epoch": 2.5180023228803714,
      "grad_norm": 35.61018753051758,
      "learning_rate": 8.313330752355143e-06,
      "loss": 2.4292,
      "step": 6504
    },
    {
      "epoch": 2.518389469608982,
      "grad_norm": 15.333882331848145,
      "learning_rate": 8.312900589323354e-06,
      "loss": 0.7514,
      "step": 6505
    },
    {
      "epoch": 2.518776616337592,
      "grad_norm": 36.842864990234375,
      "learning_rate": 8.312470426291565e-06,
      "loss": 1.3513,
      "step": 6506
    },
    {
      "epoch": 2.519163763066202,
      "grad_norm": 15.649866104125977,
      "learning_rate": 8.312040263259777e-06,
      "loss": 0.6189,
      "step": 6507
    },
    {
      "epoch": 2.519550909794812,
      "grad_norm": 17.140108108520508,
      "learning_rate": 8.311610100227987e-06,
      "loss": 0.5212,
      "step": 6508
    },
    {
      "epoch": 2.519938056523422,
      "grad_norm": 50.83640670776367,
      "learning_rate": 8.311179937196198e-06,
      "loss": 1.9118,
      "step": 6509
    },
    {
      "epoch": 2.5203252032520327,
      "grad_norm": 16.325197219848633,
      "learning_rate": 8.31074977416441e-06,
      "loss": 1.4408,
      "step": 6510
    },
    {
      "epoch": 2.5207123499806428,
      "grad_norm": 16.226789474487305,
      "learning_rate": 8.31031961113262e-06,
      "loss": 1.715,
      "step": 6511
    },
    {
      "epoch": 2.521099496709253,
      "grad_norm": 33.354801177978516,
      "learning_rate": 8.30988944810083e-06,
      "loss": 1.2734,
      "step": 6512
    },
    {
      "epoch": 2.521486643437863,
      "grad_norm": 13.489748001098633,
      "learning_rate": 8.309459285069042e-06,
      "loss": 0.8103,
      "step": 6513
    },
    {
      "epoch": 2.521873790166473,
      "grad_norm": 39.03090286254883,
      "learning_rate": 8.309029122037253e-06,
      "loss": 0.8007,
      "step": 6514
    },
    {
      "epoch": 2.5222609368950835,
      "grad_norm": 15.683109283447266,
      "learning_rate": 8.308598959005463e-06,
      "loss": 0.9459,
      "step": 6515
    },
    {
      "epoch": 2.5226480836236935,
      "grad_norm": 26.64459800720215,
      "learning_rate": 8.308168795973674e-06,
      "loss": 1.5149,
      "step": 6516
    },
    {
      "epoch": 2.5230352303523036,
      "grad_norm": 34.58442306518555,
      "learning_rate": 8.307738632941886e-06,
      "loss": 2.2941,
      "step": 6517
    },
    {
      "epoch": 2.5234223770809137,
      "grad_norm": 25.853322982788086,
      "learning_rate": 8.307308469910097e-06,
      "loss": 1.331,
      "step": 6518
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 24.89281463623047,
      "learning_rate": 8.306878306878307e-06,
      "loss": 1.6568,
      "step": 6519
    },
    {
      "epoch": 2.524196670538134,
      "grad_norm": 12.714157104492188,
      "learning_rate": 8.306448143846518e-06,
      "loss": 1.5689,
      "step": 6520
    },
    {
      "epoch": 2.524583817266744,
      "grad_norm": 14.161920547485352,
      "learning_rate": 8.30601798081473e-06,
      "loss": 1.2942,
      "step": 6521
    },
    {
      "epoch": 2.5249709639953544,
      "grad_norm": 24.90749740600586,
      "learning_rate": 8.305587817782941e-06,
      "loss": 1.3608,
      "step": 6522
    },
    {
      "epoch": 2.5253581107239644,
      "grad_norm": 14.179140090942383,
      "learning_rate": 8.305157654751151e-06,
      "loss": 1.3587,
      "step": 6523
    },
    {
      "epoch": 2.5257452574525745,
      "grad_norm": 29.312105178833008,
      "learning_rate": 8.304727491719362e-06,
      "loss": 1.0185,
      "step": 6524
    },
    {
      "epoch": 2.5261324041811846,
      "grad_norm": 22.672313690185547,
      "learning_rate": 8.304297328687572e-06,
      "loss": 1.7353,
      "step": 6525
    },
    {
      "epoch": 2.5265195509097946,
      "grad_norm": 26.254566192626953,
      "learning_rate": 8.303867165655785e-06,
      "loss": 1.6653,
      "step": 6526
    },
    {
      "epoch": 2.526906697638405,
      "grad_norm": 23.89554214477539,
      "learning_rate": 8.303437002623995e-06,
      "loss": 1.2728,
      "step": 6527
    },
    {
      "epoch": 2.527293844367015,
      "grad_norm": 27.262222290039062,
      "learning_rate": 8.303006839592206e-06,
      "loss": 1.7846,
      "step": 6528
    },
    {
      "epoch": 2.5276809910956253,
      "grad_norm": 21.05702781677246,
      "learning_rate": 8.302576676560416e-06,
      "loss": 0.9575,
      "step": 6529
    },
    {
      "epoch": 2.5280681378242353,
      "grad_norm": 15.961860656738281,
      "learning_rate": 8.302146513528627e-06,
      "loss": 1.0459,
      "step": 6530
    },
    {
      "epoch": 2.5284552845528454,
      "grad_norm": 17.563133239746094,
      "learning_rate": 8.301716350496839e-06,
      "loss": 0.5317,
      "step": 6531
    },
    {
      "epoch": 2.528842431281456,
      "grad_norm": 13.472872734069824,
      "learning_rate": 8.30128618746505e-06,
      "loss": 0.8168,
      "step": 6532
    },
    {
      "epoch": 2.5292295780100655,
      "grad_norm": 17.71633529663086,
      "learning_rate": 8.300856024433262e-06,
      "loss": 0.9624,
      "step": 6533
    },
    {
      "epoch": 2.529616724738676,
      "grad_norm": 13.85167407989502,
      "learning_rate": 8.300425861401471e-06,
      "loss": 1.3042,
      "step": 6534
    },
    {
      "epoch": 2.530003871467286,
      "grad_norm": 14.888191223144531,
      "learning_rate": 8.299995698369683e-06,
      "loss": 1.0568,
      "step": 6535
    },
    {
      "epoch": 2.530391018195896,
      "grad_norm": 23.399309158325195,
      "learning_rate": 8.299565535337893e-06,
      "loss": 1.5571,
      "step": 6536
    },
    {
      "epoch": 2.5307781649245062,
      "grad_norm": 22.988798141479492,
      "learning_rate": 8.299135372306106e-06,
      "loss": 1.8694,
      "step": 6537
    },
    {
      "epoch": 2.5311653116531163,
      "grad_norm": 33.81867980957031,
      "learning_rate": 8.298705209274315e-06,
      "loss": 1.3096,
      "step": 6538
    },
    {
      "epoch": 2.531552458381727,
      "grad_norm": 37.10335159301758,
      "learning_rate": 8.298275046242527e-06,
      "loss": 0.6355,
      "step": 6539
    },
    {
      "epoch": 2.531939605110337,
      "grad_norm": 21.470989227294922,
      "learning_rate": 8.297844883210737e-06,
      "loss": 2.1576,
      "step": 6540
    },
    {
      "epoch": 2.532326751838947,
      "grad_norm": 16.406248092651367,
      "learning_rate": 8.29741472017895e-06,
      "loss": 1.0712,
      "step": 6541
    },
    {
      "epoch": 2.532713898567557,
      "grad_norm": 17.541515350341797,
      "learning_rate": 8.29698455714716e-06,
      "loss": 1.7659,
      "step": 6542
    },
    {
      "epoch": 2.533101045296167,
      "grad_norm": 15.30136489868164,
      "learning_rate": 8.29655439411537e-06,
      "loss": 0.8662,
      "step": 6543
    },
    {
      "epoch": 2.5334881920247776,
      "grad_norm": 24.657596588134766,
      "learning_rate": 8.29612423108358e-06,
      "loss": 2.1528,
      "step": 6544
    },
    {
      "epoch": 2.5338753387533877,
      "grad_norm": 12.448860168457031,
      "learning_rate": 8.295694068051792e-06,
      "loss": 0.4971,
      "step": 6545
    },
    {
      "epoch": 2.5342624854819977,
      "grad_norm": 26.62841033935547,
      "learning_rate": 8.295263905020003e-06,
      "loss": 1.9401,
      "step": 6546
    },
    {
      "epoch": 2.534649632210608,
      "grad_norm": 37.81526565551758,
      "learning_rate": 8.294833741988215e-06,
      "loss": 0.9558,
      "step": 6547
    },
    {
      "epoch": 2.535036778939218,
      "grad_norm": 22.582250595092773,
      "learning_rate": 8.294403578956425e-06,
      "loss": 1.9868,
      "step": 6548
    },
    {
      "epoch": 2.5354239256678284,
      "grad_norm": 37.88092041015625,
      "learning_rate": 8.293973415924636e-06,
      "loss": 1.3662,
      "step": 6549
    },
    {
      "epoch": 2.535811072396438,
      "grad_norm": 20.81193733215332,
      "learning_rate": 8.293543252892847e-06,
      "loss": 1.2515,
      "step": 6550
    },
    {
      "epoch": 2.5361982191250485,
      "grad_norm": 22.651832580566406,
      "learning_rate": 8.293113089861057e-06,
      "loss": 2.7326,
      "step": 6551
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 21.105417251586914,
      "learning_rate": 8.292682926829268e-06,
      "loss": 2.2014,
      "step": 6552
    },
    {
      "epoch": 2.5369725125822686,
      "grad_norm": 20.03730583190918,
      "learning_rate": 8.29225276379748e-06,
      "loss": 2.3818,
      "step": 6553
    },
    {
      "epoch": 2.5373596593108787,
      "grad_norm": 15.309534072875977,
      "learning_rate": 8.291822600765691e-06,
      "loss": 1.0403,
      "step": 6554
    },
    {
      "epoch": 2.5377468060394888,
      "grad_norm": 20.777042388916016,
      "learning_rate": 8.291392437733901e-06,
      "loss": 1.1601,
      "step": 6555
    },
    {
      "epoch": 2.5381339527680993,
      "grad_norm": 13.041324615478516,
      "learning_rate": 8.290962274702112e-06,
      "loss": 0.8506,
      "step": 6556
    },
    {
      "epoch": 2.5385210994967093,
      "grad_norm": 13.339285850524902,
      "learning_rate": 8.290532111670324e-06,
      "loss": 0.9665,
      "step": 6557
    },
    {
      "epoch": 2.5389082462253194,
      "grad_norm": 20.779035568237305,
      "learning_rate": 8.290101948638535e-06,
      "loss": 1.1393,
      "step": 6558
    },
    {
      "epoch": 2.5392953929539295,
      "grad_norm": 16.650367736816406,
      "learning_rate": 8.289671785606745e-06,
      "loss": 1.509,
      "step": 6559
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 18.446025848388672,
      "learning_rate": 8.289241622574956e-06,
      "loss": 1.0955,
      "step": 6560
    },
    {
      "epoch": 2.54006968641115,
      "grad_norm": 18.988876342773438,
      "learning_rate": 8.288811459543168e-06,
      "loss": 1.6318,
      "step": 6561
    },
    {
      "epoch": 2.54045683313976,
      "grad_norm": 27.805871963500977,
      "learning_rate": 8.28838129651138e-06,
      "loss": 1.4704,
      "step": 6562
    },
    {
      "epoch": 2.54084397986837,
      "grad_norm": 17.83941650390625,
      "learning_rate": 8.287951133479589e-06,
      "loss": 1.0169,
      "step": 6563
    },
    {
      "epoch": 2.5412311265969802,
      "grad_norm": 18.225038528442383,
      "learning_rate": 8.2875209704478e-06,
      "loss": 1.1306,
      "step": 6564
    },
    {
      "epoch": 2.5416182733255903,
      "grad_norm": 31.15797233581543,
      "learning_rate": 8.287090807416012e-06,
      "loss": 1.0881,
      "step": 6565
    },
    {
      "epoch": 2.5420054200542004,
      "grad_norm": 13.38355541229248,
      "learning_rate": 8.286660644384222e-06,
      "loss": 1.1921,
      "step": 6566
    },
    {
      "epoch": 2.5423925667828104,
      "grad_norm": 12.401418685913086,
      "learning_rate": 8.286230481352433e-06,
      "loss": 0.8079,
      "step": 6567
    },
    {
      "epoch": 2.542779713511421,
      "grad_norm": 20.618940353393555,
      "learning_rate": 8.285800318320644e-06,
      "loss": 1.4352,
      "step": 6568
    },
    {
      "epoch": 2.543166860240031,
      "grad_norm": 19.85950469970703,
      "learning_rate": 8.285370155288856e-06,
      "loss": 0.8567,
      "step": 6569
    },
    {
      "epoch": 2.543554006968641,
      "grad_norm": 13.236684799194336,
      "learning_rate": 8.284939992257065e-06,
      "loss": 1.3265,
      "step": 6570
    },
    {
      "epoch": 2.543941153697251,
      "grad_norm": 14.778871536254883,
      "learning_rate": 8.284509829225277e-06,
      "loss": 0.9277,
      "step": 6571
    },
    {
      "epoch": 2.5443283004258612,
      "grad_norm": 21.356082916259766,
      "learning_rate": 8.284079666193487e-06,
      "loss": 0.8995,
      "step": 6572
    },
    {
      "epoch": 2.5447154471544717,
      "grad_norm": 14.519862174987793,
      "learning_rate": 8.2836495031617e-06,
      "loss": 1.131,
      "step": 6573
    },
    {
      "epoch": 2.545102593883082,
      "grad_norm": 56.01575469970703,
      "learning_rate": 8.28321934012991e-06,
      "loss": 1.1907,
      "step": 6574
    },
    {
      "epoch": 2.545489740611692,
      "grad_norm": 25.45133399963379,
      "learning_rate": 8.282789177098121e-06,
      "loss": 1.6357,
      "step": 6575
    },
    {
      "epoch": 2.545876887340302,
      "grad_norm": 11.217961311340332,
      "learning_rate": 8.282359014066332e-06,
      "loss": 1.5074,
      "step": 6576
    },
    {
      "epoch": 2.546264034068912,
      "grad_norm": 15.927596092224121,
      "learning_rate": 8.281928851034544e-06,
      "loss": 0.9475,
      "step": 6577
    },
    {
      "epoch": 2.5466511807975225,
      "grad_norm": 20.719797134399414,
      "learning_rate": 8.281498688002753e-06,
      "loss": 1.0782,
      "step": 6578
    },
    {
      "epoch": 2.547038327526132,
      "grad_norm": 27.61850929260254,
      "learning_rate": 8.281068524970965e-06,
      "loss": 1.7027,
      "step": 6579
    },
    {
      "epoch": 2.5474254742547426,
      "grad_norm": 24.408353805541992,
      "learning_rate": 8.280638361939176e-06,
      "loss": 1.1343,
      "step": 6580
    },
    {
      "epoch": 2.5478126209833527,
      "grad_norm": 25.307329177856445,
      "learning_rate": 8.280208198907386e-06,
      "loss": 1.3239,
      "step": 6581
    },
    {
      "epoch": 2.5481997677119628,
      "grad_norm": 17.948484420776367,
      "learning_rate": 8.279778035875597e-06,
      "loss": 1.555,
      "step": 6582
    },
    {
      "epoch": 2.548586914440573,
      "grad_norm": 27.30409812927246,
      "learning_rate": 8.279347872843809e-06,
      "loss": 2.5787,
      "step": 6583
    },
    {
      "epoch": 2.548974061169183,
      "grad_norm": 25.592233657836914,
      "learning_rate": 8.27891770981202e-06,
      "loss": 2.9019,
      "step": 6584
    },
    {
      "epoch": 2.5493612078977934,
      "grad_norm": 14.838194847106934,
      "learning_rate": 8.27848754678023e-06,
      "loss": 0.7765,
      "step": 6585
    },
    {
      "epoch": 2.5497483546264035,
      "grad_norm": 22.911224365234375,
      "learning_rate": 8.278057383748441e-06,
      "loss": 2.0426,
      "step": 6586
    },
    {
      "epoch": 2.5501355013550135,
      "grad_norm": 16.12553596496582,
      "learning_rate": 8.277627220716651e-06,
      "loss": 1.3695,
      "step": 6587
    },
    {
      "epoch": 2.5505226480836236,
      "grad_norm": 33.09804153442383,
      "learning_rate": 8.277197057684864e-06,
      "loss": 2.5141,
      "step": 6588
    },
    {
      "epoch": 2.5509097948122337,
      "grad_norm": 28.810632705688477,
      "learning_rate": 8.276766894653074e-06,
      "loss": 2.9433,
      "step": 6589
    },
    {
      "epoch": 2.551296941540844,
      "grad_norm": 28.94549560546875,
      "learning_rate": 8.276336731621285e-06,
      "loss": 1.9487,
      "step": 6590
    },
    {
      "epoch": 2.5516840882694543,
      "grad_norm": 22.084768295288086,
      "learning_rate": 8.275906568589495e-06,
      "loss": 0.9135,
      "step": 6591
    },
    {
      "epoch": 2.5520712349980643,
      "grad_norm": 18.49317169189453,
      "learning_rate": 8.275476405557708e-06,
      "loss": 1.2264,
      "step": 6592
    },
    {
      "epoch": 2.5524583817266744,
      "grad_norm": 24.061599731445312,
      "learning_rate": 8.275046242525918e-06,
      "loss": 2.2454,
      "step": 6593
    },
    {
      "epoch": 2.5528455284552845,
      "grad_norm": 15.64351749420166,
      "learning_rate": 8.27461607949413e-06,
      "loss": 0.9802,
      "step": 6594
    },
    {
      "epoch": 2.553232675183895,
      "grad_norm": 14.469648361206055,
      "learning_rate": 8.274185916462339e-06,
      "loss": 0.6363,
      "step": 6595
    },
    {
      "epoch": 2.5536198219125046,
      "grad_norm": 12.576984405517578,
      "learning_rate": 8.27375575343055e-06,
      "loss": 0.586,
      "step": 6596
    },
    {
      "epoch": 2.554006968641115,
      "grad_norm": 21.39670753479004,
      "learning_rate": 8.273325590398762e-06,
      "loss": 1.1181,
      "step": 6597
    },
    {
      "epoch": 2.554394115369725,
      "grad_norm": 11.058210372924805,
      "learning_rate": 8.272895427366973e-06,
      "loss": 1.1898,
      "step": 6598
    },
    {
      "epoch": 2.5547812620983352,
      "grad_norm": 29.94928741455078,
      "learning_rate": 8.272465264335183e-06,
      "loss": 1.5197,
      "step": 6599
    },
    {
      "epoch": 2.5551684088269453,
      "grad_norm": 14.808874130249023,
      "learning_rate": 8.272035101303394e-06,
      "loss": 1.3728,
      "step": 6600
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 30.00786018371582,
      "learning_rate": 8.271604938271606e-06,
      "loss": 1.7327,
      "step": 6601
    },
    {
      "epoch": 2.555942702284166,
      "grad_norm": 14.83565902709961,
      "learning_rate": 8.271174775239816e-06,
      "loss": 1.1027,
      "step": 6602
    },
    {
      "epoch": 2.556329849012776,
      "grad_norm": 17.9450626373291,
      "learning_rate": 8.270744612208027e-06,
      "loss": 0.8226,
      "step": 6603
    },
    {
      "epoch": 2.556716995741386,
      "grad_norm": 28.154172897338867,
      "learning_rate": 8.270314449176238e-06,
      "loss": 1.5411,
      "step": 6604
    },
    {
      "epoch": 2.557104142469996,
      "grad_norm": 24.93691062927246,
      "learning_rate": 8.26988428614445e-06,
      "loss": 1.2118,
      "step": 6605
    },
    {
      "epoch": 2.557491289198606,
      "grad_norm": 24.76024627685547,
      "learning_rate": 8.26945412311266e-06,
      "loss": 1.4407,
      "step": 6606
    },
    {
      "epoch": 2.5578784359272166,
      "grad_norm": 15.929967880249023,
      "learning_rate": 8.269023960080871e-06,
      "loss": 1.3612,
      "step": 6607
    },
    {
      "epoch": 2.5582655826558267,
      "grad_norm": 39.07257080078125,
      "learning_rate": 8.268593797049082e-06,
      "loss": 1.7237,
      "step": 6608
    },
    {
      "epoch": 2.5586527293844368,
      "grad_norm": 21.56123924255371,
      "learning_rate": 8.268163634017294e-06,
      "loss": 2.1688,
      "step": 6609
    },
    {
      "epoch": 2.559039876113047,
      "grad_norm": 40.65340805053711,
      "learning_rate": 8.267733470985503e-06,
      "loss": 2.4897,
      "step": 6610
    },
    {
      "epoch": 2.559427022841657,
      "grad_norm": 16.493982315063477,
      "learning_rate": 8.267303307953715e-06,
      "loss": 1.0713,
      "step": 6611
    },
    {
      "epoch": 2.559814169570267,
      "grad_norm": 27.161054611206055,
      "learning_rate": 8.266873144921926e-06,
      "loss": 1.4839,
      "step": 6612
    },
    {
      "epoch": 2.560201316298877,
      "grad_norm": 35.59758758544922,
      "learning_rate": 8.266442981890138e-06,
      "loss": 1.4989,
      "step": 6613
    },
    {
      "epoch": 2.5605884630274875,
      "grad_norm": 15.312708854675293,
      "learning_rate": 8.266012818858347e-06,
      "loss": 1.1165,
      "step": 6614
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 18.324682235717773,
      "learning_rate": 8.265582655826559e-06,
      "loss": 1.6297,
      "step": 6615
    },
    {
      "epoch": 2.5613627564847077,
      "grad_norm": 19.052082061767578,
      "learning_rate": 8.26515249279477e-06,
      "loss": 1.6705,
      "step": 6616
    },
    {
      "epoch": 2.5617499032133177,
      "grad_norm": 19.095172882080078,
      "learning_rate": 8.26472232976298e-06,
      "loss": 1.8329,
      "step": 6617
    },
    {
      "epoch": 2.562137049941928,
      "grad_norm": 18.2680721282959,
      "learning_rate": 8.264292166731191e-06,
      "loss": 1.755,
      "step": 6618
    },
    {
      "epoch": 2.5625241966705383,
      "grad_norm": 21.651376724243164,
      "learning_rate": 8.263862003699403e-06,
      "loss": 2.5246,
      "step": 6619
    },
    {
      "epoch": 2.5629113433991484,
      "grad_norm": 15.219318389892578,
      "learning_rate": 8.263431840667614e-06,
      "loss": 1.3221,
      "step": 6620
    },
    {
      "epoch": 2.5632984901277585,
      "grad_norm": 19.764413833618164,
      "learning_rate": 8.263001677635824e-06,
      "loss": 1.6083,
      "step": 6621
    },
    {
      "epoch": 2.5636856368563685,
      "grad_norm": 45.56130599975586,
      "learning_rate": 8.262571514604035e-06,
      "loss": 2.3542,
      "step": 6622
    },
    {
      "epoch": 2.5640727835849786,
      "grad_norm": 17.24356460571289,
      "learning_rate": 8.262141351572247e-06,
      "loss": 1.5973,
      "step": 6623
    },
    {
      "epoch": 2.564459930313589,
      "grad_norm": 24.91828727722168,
      "learning_rate": 8.261711188540458e-06,
      "loss": 1.5784,
      "step": 6624
    },
    {
      "epoch": 2.5648470770421987,
      "grad_norm": 22.893909454345703,
      "learning_rate": 8.261281025508668e-06,
      "loss": 1.818,
      "step": 6625
    },
    {
      "epoch": 2.5652342237708092,
      "grad_norm": 21.413818359375,
      "learning_rate": 8.26085086247688e-06,
      "loss": 1.7441,
      "step": 6626
    },
    {
      "epoch": 2.5656213704994193,
      "grad_norm": 35.25287628173828,
      "learning_rate": 8.26042069944509e-06,
      "loss": 1.8809,
      "step": 6627
    },
    {
      "epoch": 2.5660085172280294,
      "grad_norm": 25.19228172302246,
      "learning_rate": 8.259990536413302e-06,
      "loss": 1.2152,
      "step": 6628
    },
    {
      "epoch": 2.5663956639566394,
      "grad_norm": 47.46689224243164,
      "learning_rate": 8.259560373381512e-06,
      "loss": 2.3069,
      "step": 6629
    },
    {
      "epoch": 2.5667828106852495,
      "grad_norm": 23.812740325927734,
      "learning_rate": 8.259130210349723e-06,
      "loss": 1.2442,
      "step": 6630
    },
    {
      "epoch": 2.56716995741386,
      "grad_norm": 21.82427215576172,
      "learning_rate": 8.258700047317935e-06,
      "loss": 0.9511,
      "step": 6631
    },
    {
      "epoch": 2.56755710414247,
      "grad_norm": 24.050264358520508,
      "learning_rate": 8.258269884286144e-06,
      "loss": 1.6458,
      "step": 6632
    },
    {
      "epoch": 2.56794425087108,
      "grad_norm": 19.121042251586914,
      "learning_rate": 8.257839721254356e-06,
      "loss": 1.9195,
      "step": 6633
    },
    {
      "epoch": 2.56833139759969,
      "grad_norm": 15.840896606445312,
      "learning_rate": 8.257409558222567e-06,
      "loss": 1.4247,
      "step": 6634
    },
    {
      "epoch": 2.5687185443283003,
      "grad_norm": 11.309069633483887,
      "learning_rate": 8.256979395190779e-06,
      "loss": 1.0257,
      "step": 6635
    },
    {
      "epoch": 2.569105691056911,
      "grad_norm": 27.5294246673584,
      "learning_rate": 8.256549232158988e-06,
      "loss": 1.0417,
      "step": 6636
    },
    {
      "epoch": 2.569492837785521,
      "grad_norm": 12.484399795532227,
      "learning_rate": 8.2561190691272e-06,
      "loss": 0.52,
      "step": 6637
    },
    {
      "epoch": 2.569879984514131,
      "grad_norm": 16.576677322387695,
      "learning_rate": 8.25568890609541e-06,
      "loss": 1.2114,
      "step": 6638
    },
    {
      "epoch": 2.570267131242741,
      "grad_norm": 33.57497787475586,
      "learning_rate": 8.255258743063623e-06,
      "loss": 1.3264,
      "step": 6639
    },
    {
      "epoch": 2.570654277971351,
      "grad_norm": 24.345352172851562,
      "learning_rate": 8.254828580031832e-06,
      "loss": 1.5692,
      "step": 6640
    },
    {
      "epoch": 2.5710414246999616,
      "grad_norm": 13.552007675170898,
      "learning_rate": 8.254398417000044e-06,
      "loss": 1.3887,
      "step": 6641
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 13.998462677001953,
      "learning_rate": 8.253968253968254e-06,
      "loss": 1.0315,
      "step": 6642
    },
    {
      "epoch": 2.5718157181571817,
      "grad_norm": 36.938514709472656,
      "learning_rate": 8.253538090936467e-06,
      "loss": 1.2733,
      "step": 6643
    },
    {
      "epoch": 2.5722028648857918,
      "grad_norm": 34.79233169555664,
      "learning_rate": 8.253107927904676e-06,
      "loss": 0.9813,
      "step": 6644
    },
    {
      "epoch": 2.572590011614402,
      "grad_norm": 14.253339767456055,
      "learning_rate": 8.252677764872888e-06,
      "loss": 1.0408,
      "step": 6645
    },
    {
      "epoch": 2.572977158343012,
      "grad_norm": 13.641956329345703,
      "learning_rate": 8.252247601841098e-06,
      "loss": 1.0656,
      "step": 6646
    },
    {
      "epoch": 2.573364305071622,
      "grad_norm": 54.87945556640625,
      "learning_rate": 8.251817438809309e-06,
      "loss": 2.3274,
      "step": 6647
    },
    {
      "epoch": 2.5737514518002325,
      "grad_norm": 25.931884765625,
      "learning_rate": 8.25138727577752e-06,
      "loss": 0.9169,
      "step": 6648
    },
    {
      "epoch": 2.5741385985288425,
      "grad_norm": 25.884532928466797,
      "learning_rate": 8.250957112745732e-06,
      "loss": 2.1432,
      "step": 6649
    },
    {
      "epoch": 2.5745257452574526,
      "grad_norm": 34.68015670776367,
      "learning_rate": 8.250526949713941e-06,
      "loss": 1.482,
      "step": 6650
    },
    {
      "epoch": 2.5749128919860627,
      "grad_norm": 43.105045318603516,
      "learning_rate": 8.250096786682153e-06,
      "loss": 2.1268,
      "step": 6651
    },
    {
      "epoch": 2.5753000387146727,
      "grad_norm": 24.116527557373047,
      "learning_rate": 8.249666623650364e-06,
      "loss": 1.2031,
      "step": 6652
    },
    {
      "epoch": 2.5756871854432832,
      "grad_norm": 12.900115013122559,
      "learning_rate": 8.249236460618574e-06,
      "loss": 1.3536,
      "step": 6653
    },
    {
      "epoch": 2.5760743321718933,
      "grad_norm": 12.836307525634766,
      "learning_rate": 8.248806297586785e-06,
      "loss": 0.97,
      "step": 6654
    },
    {
      "epoch": 2.5764614789005034,
      "grad_norm": 25.235376358032227,
      "learning_rate": 8.248376134554997e-06,
      "loss": 1.3382,
      "step": 6655
    },
    {
      "epoch": 2.5768486256291134,
      "grad_norm": 35.96395492553711,
      "learning_rate": 8.247945971523208e-06,
      "loss": 1.822,
      "step": 6656
    },
    {
      "epoch": 2.5772357723577235,
      "grad_norm": 19.99747085571289,
      "learning_rate": 8.247515808491418e-06,
      "loss": 2.7686,
      "step": 6657
    },
    {
      "epoch": 2.5776229190863336,
      "grad_norm": 17.794403076171875,
      "learning_rate": 8.247085645459631e-06,
      "loss": 1.4594,
      "step": 6658
    },
    {
      "epoch": 2.5780100658149436,
      "grad_norm": 15.80921745300293,
      "learning_rate": 8.24665548242784e-06,
      "loss": 1.6486,
      "step": 6659
    },
    {
      "epoch": 2.578397212543554,
      "grad_norm": 17.07121467590332,
      "learning_rate": 8.246225319396052e-06,
      "loss": 1.4251,
      "step": 6660
    },
    {
      "epoch": 2.578784359272164,
      "grad_norm": 35.73323440551758,
      "learning_rate": 8.245795156364262e-06,
      "loss": 1.7617,
      "step": 6661
    },
    {
      "epoch": 2.5791715060007743,
      "grad_norm": 33.160980224609375,
      "learning_rate": 8.245364993332473e-06,
      "loss": 1.6204,
      "step": 6662
    },
    {
      "epoch": 2.5795586527293843,
      "grad_norm": 14.230320930480957,
      "learning_rate": 8.244934830300685e-06,
      "loss": 1.1391,
      "step": 6663
    },
    {
      "epoch": 2.5799457994579944,
      "grad_norm": 11.780088424682617,
      "learning_rate": 8.244504667268896e-06,
      "loss": 0.5051,
      "step": 6664
    },
    {
      "epoch": 2.580332946186605,
      "grad_norm": 12.839888572692871,
      "learning_rate": 8.244074504237106e-06,
      "loss": 0.8102,
      "step": 6665
    },
    {
      "epoch": 2.580720092915215,
      "grad_norm": 20.04764747619629,
      "learning_rate": 8.243644341205317e-06,
      "loss": 1.6449,
      "step": 6666
    },
    {
      "epoch": 2.581107239643825,
      "grad_norm": 15.532073020935059,
      "learning_rate": 8.243214178173529e-06,
      "loss": 0.8572,
      "step": 6667
    },
    {
      "epoch": 2.581494386372435,
      "grad_norm": 12.088016510009766,
      "learning_rate": 8.242784015141738e-06,
      "loss": 0.7544,
      "step": 6668
    },
    {
      "epoch": 2.581881533101045,
      "grad_norm": 15.428149223327637,
      "learning_rate": 8.24235385210995e-06,
      "loss": 0.9772,
      "step": 6669
    },
    {
      "epoch": 2.5822686798296557,
      "grad_norm": 23.381261825561523,
      "learning_rate": 8.241923689078161e-06,
      "loss": 1.4907,
      "step": 6670
    },
    {
      "epoch": 2.5826558265582653,
      "grad_norm": 24.0771484375,
      "learning_rate": 8.241493526046373e-06,
      "loss": 1.3928,
      "step": 6671
    },
    {
      "epoch": 2.583042973286876,
      "grad_norm": 11.126252174377441,
      "learning_rate": 8.241063363014582e-06,
      "loss": 0.7459,
      "step": 6672
    },
    {
      "epoch": 2.583430120015486,
      "grad_norm": 14.005887031555176,
      "learning_rate": 8.240633199982794e-06,
      "loss": 1.0787,
      "step": 6673
    },
    {
      "epoch": 2.583817266744096,
      "grad_norm": 7.303370952606201,
      "learning_rate": 8.240203036951005e-06,
      "loss": 0.1921,
      "step": 6674
    },
    {
      "epoch": 2.584204413472706,
      "grad_norm": 16.63590431213379,
      "learning_rate": 8.239772873919217e-06,
      "loss": 1.4519,
      "step": 6675
    },
    {
      "epoch": 2.584591560201316,
      "grad_norm": 22.3074893951416,
      "learning_rate": 8.239342710887426e-06,
      "loss": 1.5886,
      "step": 6676
    },
    {
      "epoch": 2.5849787069299266,
      "grad_norm": 15.053366661071777,
      "learning_rate": 8.238912547855638e-06,
      "loss": 0.8149,
      "step": 6677
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 24.70357894897461,
      "learning_rate": 8.23848238482385e-06,
      "loss": 1.5997,
      "step": 6678
    },
    {
      "epoch": 2.5857530003871467,
      "grad_norm": 23.569862365722656,
      "learning_rate": 8.23805222179206e-06,
      "loss": 1.3978,
      "step": 6679
    },
    {
      "epoch": 2.586140147115757,
      "grad_norm": 13.371706008911133,
      "learning_rate": 8.23762205876027e-06,
      "loss": 1.3023,
      "step": 6680
    },
    {
      "epoch": 2.586527293844367,
      "grad_norm": 12.310827255249023,
      "learning_rate": 8.237191895728482e-06,
      "loss": 0.7329,
      "step": 6681
    },
    {
      "epoch": 2.5869144405729774,
      "grad_norm": 28.878637313842773,
      "learning_rate": 8.236761732696693e-06,
      "loss": 1.7473,
      "step": 6682
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 15.83698844909668,
      "learning_rate": 8.236331569664903e-06,
      "loss": 1.4928,
      "step": 6683
    },
    {
      "epoch": 2.5876887340301975,
      "grad_norm": 14.096611976623535,
      "learning_rate": 8.235901406633114e-06,
      "loss": 1.4046,
      "step": 6684
    },
    {
      "epoch": 2.5880758807588076,
      "grad_norm": 21.72480010986328,
      "learning_rate": 8.235471243601326e-06,
      "loss": 1.2702,
      "step": 6685
    },
    {
      "epoch": 2.5884630274874176,
      "grad_norm": 43.18500900268555,
      "learning_rate": 8.235041080569537e-06,
      "loss": 1.0228,
      "step": 6686
    },
    {
      "epoch": 2.588850174216028,
      "grad_norm": 27.827970504760742,
      "learning_rate": 8.234610917537747e-06,
      "loss": 1.429,
      "step": 6687
    },
    {
      "epoch": 2.5892373209446378,
      "grad_norm": 74.8778305053711,
      "learning_rate": 8.234180754505958e-06,
      "loss": 0.979,
      "step": 6688
    },
    {
      "epoch": 2.5896244676732483,
      "grad_norm": 22.101787567138672,
      "learning_rate": 8.233750591474168e-06,
      "loss": 1.4822,
      "step": 6689
    },
    {
      "epoch": 2.5900116144018583,
      "grad_norm": 25.46770477294922,
      "learning_rate": 8.233320428442381e-06,
      "loss": 1.6081,
      "step": 6690
    },
    {
      "epoch": 2.5903987611304684,
      "grad_norm": 20.439462661743164,
      "learning_rate": 8.232890265410591e-06,
      "loss": 3.525,
      "step": 6691
    },
    {
      "epoch": 2.5907859078590785,
      "grad_norm": 14.230329513549805,
      "learning_rate": 8.232460102378802e-06,
      "loss": 0.9834,
      "step": 6692
    },
    {
      "epoch": 2.5911730545876885,
      "grad_norm": 25.02967071533203,
      "learning_rate": 8.232029939347012e-06,
      "loss": 1.8624,
      "step": 6693
    },
    {
      "epoch": 2.591560201316299,
      "grad_norm": 21.635534286499023,
      "learning_rate": 8.231599776315225e-06,
      "loss": 0.6095,
      "step": 6694
    },
    {
      "epoch": 2.591947348044909,
      "grad_norm": 22.86844825744629,
      "learning_rate": 8.231169613283435e-06,
      "loss": 1.8083,
      "step": 6695
    },
    {
      "epoch": 2.592334494773519,
      "grad_norm": 34.126895904541016,
      "learning_rate": 8.230739450251646e-06,
      "loss": 1.9565,
      "step": 6696
    },
    {
      "epoch": 2.5927216415021292,
      "grad_norm": 29.96186065673828,
      "learning_rate": 8.230309287219856e-06,
      "loss": 2.7418,
      "step": 6697
    },
    {
      "epoch": 2.5931087882307393,
      "grad_norm": 25.066612243652344,
      "learning_rate": 8.229879124188067e-06,
      "loss": 1.3536,
      "step": 6698
    },
    {
      "epoch": 2.59349593495935,
      "grad_norm": 21.28375244140625,
      "learning_rate": 8.229448961156279e-06,
      "loss": 3.8366,
      "step": 6699
    },
    {
      "epoch": 2.59388308168796,
      "grad_norm": 12.04384708404541,
      "learning_rate": 8.22901879812449e-06,
      "loss": 1.3027,
      "step": 6700
    },
    {
      "epoch": 2.59427022841657,
      "grad_norm": 23.570035934448242,
      "learning_rate": 8.228588635092702e-06,
      "loss": 1.3276,
      "step": 6701
    },
    {
      "epoch": 2.59465737514518,
      "grad_norm": 55.755592346191406,
      "learning_rate": 8.228158472060911e-06,
      "loss": 1.9606,
      "step": 6702
    },
    {
      "epoch": 2.59504452187379,
      "grad_norm": 40.123924255371094,
      "learning_rate": 8.227728309029123e-06,
      "loss": 1.1107,
      "step": 6703
    },
    {
      "epoch": 2.5954316686024,
      "grad_norm": 11.444648742675781,
      "learning_rate": 8.227298145997333e-06,
      "loss": 1.0916,
      "step": 6704
    },
    {
      "epoch": 2.59581881533101,
      "grad_norm": 23.783538818359375,
      "learning_rate": 8.226867982965546e-06,
      "loss": 1.6467,
      "step": 6705
    },
    {
      "epoch": 2.5962059620596207,
      "grad_norm": 13.466407775878906,
      "learning_rate": 8.226437819933755e-06,
      "loss": 1.3777,
      "step": 6706
    },
    {
      "epoch": 2.596593108788231,
      "grad_norm": 5.34808349609375,
      "learning_rate": 8.226007656901967e-06,
      "loss": 0.298,
      "step": 6707
    },
    {
      "epoch": 2.596980255516841,
      "grad_norm": 28.449743270874023,
      "learning_rate": 8.225577493870176e-06,
      "loss": 1.2746,
      "step": 6708
    },
    {
      "epoch": 2.597367402245451,
      "grad_norm": 43.86897277832031,
      "learning_rate": 8.22514733083839e-06,
      "loss": 1.3724,
      "step": 6709
    },
    {
      "epoch": 2.597754548974061,
      "grad_norm": 22.454666137695312,
      "learning_rate": 8.2247171678066e-06,
      "loss": 2.2473,
      "step": 6710
    },
    {
      "epoch": 2.5981416957026715,
      "grad_norm": 22.488393783569336,
      "learning_rate": 8.22428700477481e-06,
      "loss": 1.8882,
      "step": 6711
    },
    {
      "epoch": 2.5985288424312816,
      "grad_norm": 12.292214393615723,
      "learning_rate": 8.22385684174302e-06,
      "loss": 0.7053,
      "step": 6712
    },
    {
      "epoch": 2.5989159891598916,
      "grad_norm": 11.58292293548584,
      "learning_rate": 8.223426678711232e-06,
      "loss": 0.9687,
      "step": 6713
    },
    {
      "epoch": 2.5993031358885017,
      "grad_norm": 17.94167709350586,
      "learning_rate": 8.222996515679443e-06,
      "loss": 2.7174,
      "step": 6714
    },
    {
      "epoch": 2.5996902826171118,
      "grad_norm": 63.20341873168945,
      "learning_rate": 8.222566352647655e-06,
      "loss": 2.4527,
      "step": 6715
    },
    {
      "epoch": 2.6000774293457223,
      "grad_norm": 48.883602142333984,
      "learning_rate": 8.222136189615864e-06,
      "loss": 1.6893,
      "step": 6716
    },
    {
      "epoch": 2.600464576074332,
      "grad_norm": 91.7459487915039,
      "learning_rate": 8.221706026584076e-06,
      "loss": 0.7148,
      "step": 6717
    },
    {
      "epoch": 2.6008517228029424,
      "grad_norm": 25.709245681762695,
      "learning_rate": 8.221275863552287e-06,
      "loss": 1.3524,
      "step": 6718
    },
    {
      "epoch": 2.6012388695315525,
      "grad_norm": 16.083005905151367,
      "learning_rate": 8.220845700520497e-06,
      "loss": 1.7166,
      "step": 6719
    },
    {
      "epoch": 2.6016260162601625,
      "grad_norm": 37.258392333984375,
      "learning_rate": 8.220415537488708e-06,
      "loss": 2.5066,
      "step": 6720
    },
    {
      "epoch": 2.6020131629887726,
      "grad_norm": 27.439565658569336,
      "learning_rate": 8.21998537445692e-06,
      "loss": 0.6865,
      "step": 6721
    },
    {
      "epoch": 2.6024003097173827,
      "grad_norm": 24.61700439453125,
      "learning_rate": 8.219555211425131e-06,
      "loss": 1.2553,
      "step": 6722
    },
    {
      "epoch": 2.602787456445993,
      "grad_norm": 58.37273406982422,
      "learning_rate": 8.219125048393341e-06,
      "loss": 2.1957,
      "step": 6723
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 14.360011100769043,
      "learning_rate": 8.218694885361552e-06,
      "loss": 0.9198,
      "step": 6724
    },
    {
      "epoch": 2.6035617499032133,
      "grad_norm": 28.011436462402344,
      "learning_rate": 8.218264722329764e-06,
      "loss": 1.6365,
      "step": 6725
    },
    {
      "epoch": 2.6039488966318234,
      "grad_norm": 14.291027069091797,
      "learning_rate": 8.217834559297975e-06,
      "loss": 0.9373,
      "step": 6726
    },
    {
      "epoch": 2.6043360433604335,
      "grad_norm": 16.05330467224121,
      "learning_rate": 8.217404396266185e-06,
      "loss": 0.9205,
      "step": 6727
    },
    {
      "epoch": 2.604723190089044,
      "grad_norm": 14.775154113769531,
      "learning_rate": 8.216974233234396e-06,
      "loss": 1.3886,
      "step": 6728
    },
    {
      "epoch": 2.605110336817654,
      "grad_norm": 8.388107299804688,
      "learning_rate": 8.216544070202608e-06,
      "loss": 0.3997,
      "step": 6729
    },
    {
      "epoch": 2.605497483546264,
      "grad_norm": 10.436429977416992,
      "learning_rate": 8.21611390717082e-06,
      "loss": 1.2887,
      "step": 6730
    },
    {
      "epoch": 2.605884630274874,
      "grad_norm": 12.22391128540039,
      "learning_rate": 8.215683744139029e-06,
      "loss": 0.7365,
      "step": 6731
    },
    {
      "epoch": 2.6062717770034842,
      "grad_norm": 45.749568939208984,
      "learning_rate": 8.21525358110724e-06,
      "loss": 2.0681,
      "step": 6732
    },
    {
      "epoch": 2.6066589237320947,
      "grad_norm": 26.13079261779785,
      "learning_rate": 8.214823418075452e-06,
      "loss": 1.1861,
      "step": 6733
    },
    {
      "epoch": 2.6070460704607044,
      "grad_norm": 14.267942428588867,
      "learning_rate": 8.214393255043661e-06,
      "loss": 1.269,
      "step": 6734
    },
    {
      "epoch": 2.607433217189315,
      "grad_norm": 12.925281524658203,
      "learning_rate": 8.213963092011873e-06,
      "loss": 1.3556,
      "step": 6735
    },
    {
      "epoch": 2.607820363917925,
      "grad_norm": 11.719027519226074,
      "learning_rate": 8.213532928980084e-06,
      "loss": 0.5716,
      "step": 6736
    },
    {
      "epoch": 2.608207510646535,
      "grad_norm": 24.158313751220703,
      "learning_rate": 8.213102765948296e-06,
      "loss": 1.424,
      "step": 6737
    },
    {
      "epoch": 2.608594657375145,
      "grad_norm": 28.980613708496094,
      "learning_rate": 8.212672602916505e-06,
      "loss": 1.4352,
      "step": 6738
    },
    {
      "epoch": 2.608981804103755,
      "grad_norm": 29.59172821044922,
      "learning_rate": 8.212242439884717e-06,
      "loss": 1.0382,
      "step": 6739
    },
    {
      "epoch": 2.6093689508323656,
      "grad_norm": 16.144550323486328,
      "learning_rate": 8.211812276852928e-06,
      "loss": 1.2038,
      "step": 6740
    },
    {
      "epoch": 2.6097560975609757,
      "grad_norm": 21.780866622924805,
      "learning_rate": 8.21138211382114e-06,
      "loss": 1.0079,
      "step": 6741
    },
    {
      "epoch": 2.6101432442895858,
      "grad_norm": 15.077619552612305,
      "learning_rate": 8.21095195078935e-06,
      "loss": 1.3998,
      "step": 6742
    },
    {
      "epoch": 2.610530391018196,
      "grad_norm": 26.3847599029541,
      "learning_rate": 8.21052178775756e-06,
      "loss": 1.9007,
      "step": 6743
    },
    {
      "epoch": 2.610917537746806,
      "grad_norm": 14.014765739440918,
      "learning_rate": 8.210091624725772e-06,
      "loss": 1.1837,
      "step": 6744
    },
    {
      "epoch": 2.6113046844754164,
      "grad_norm": 23.973491668701172,
      "learning_rate": 8.209661461693984e-06,
      "loss": 1.2668,
      "step": 6745
    },
    {
      "epoch": 2.6116918312040265,
      "grad_norm": 17.041614532470703,
      "learning_rate": 8.209231298662193e-06,
      "loss": 1.0502,
      "step": 6746
    },
    {
      "epoch": 2.6120789779326365,
      "grad_norm": 15.05147647857666,
      "learning_rate": 8.208801135630405e-06,
      "loss": 0.8167,
      "step": 6747
    },
    {
      "epoch": 2.6124661246612466,
      "grad_norm": 33.58732604980469,
      "learning_rate": 8.208370972598616e-06,
      "loss": 0.6934,
      "step": 6748
    },
    {
      "epoch": 2.6128532713898567,
      "grad_norm": 51.4859504699707,
      "learning_rate": 8.207940809566826e-06,
      "loss": 1.6162,
      "step": 6749
    },
    {
      "epoch": 2.6132404181184667,
      "grad_norm": 18.139850616455078,
      "learning_rate": 8.207510646535037e-06,
      "loss": 1.4468,
      "step": 6750
    },
    {
      "epoch": 2.613627564847077,
      "grad_norm": 42.75245666503906,
      "learning_rate": 8.207080483503249e-06,
      "loss": 0.9941,
      "step": 6751
    },
    {
      "epoch": 2.6140147115756873,
      "grad_norm": 25.072532653808594,
      "learning_rate": 8.20665032047146e-06,
      "loss": 1.3403,
      "step": 6752
    },
    {
      "epoch": 2.6144018583042974,
      "grad_norm": 24.755939483642578,
      "learning_rate": 8.20622015743967e-06,
      "loss": 1.4395,
      "step": 6753
    },
    {
      "epoch": 2.6147890050329075,
      "grad_norm": 25.58188819885254,
      "learning_rate": 8.205789994407881e-06,
      "loss": 2.1013,
      "step": 6754
    },
    {
      "epoch": 2.6151761517615175,
      "grad_norm": 25.143844604492188,
      "learning_rate": 8.205359831376091e-06,
      "loss": 2.2646,
      "step": 6755
    },
    {
      "epoch": 2.6155632984901276,
      "grad_norm": 12.5060453414917,
      "learning_rate": 8.204929668344304e-06,
      "loss": 1.3205,
      "step": 6756
    },
    {
      "epoch": 2.615950445218738,
      "grad_norm": 21.612586975097656,
      "learning_rate": 8.204499505312514e-06,
      "loss": 1.465,
      "step": 6757
    },
    {
      "epoch": 2.616337591947348,
      "grad_norm": 16.688875198364258,
      "learning_rate": 8.204069342280725e-06,
      "loss": 1.3299,
      "step": 6758
    },
    {
      "epoch": 2.6167247386759582,
      "grad_norm": 29.44492530822754,
      "learning_rate": 8.203639179248935e-06,
      "loss": 1.372,
      "step": 6759
    },
    {
      "epoch": 2.6171118854045683,
      "grad_norm": 35.771575927734375,
      "learning_rate": 8.203209016217148e-06,
      "loss": 1.2435,
      "step": 6760
    },
    {
      "epoch": 2.6174990321331784,
      "grad_norm": 38.248382568359375,
      "learning_rate": 8.202778853185358e-06,
      "loss": 1.4723,
      "step": 6761
    },
    {
      "epoch": 2.617886178861789,
      "grad_norm": 32.690528869628906,
      "learning_rate": 8.20234869015357e-06,
      "loss": 3.7085,
      "step": 6762
    },
    {
      "epoch": 2.6182733255903985,
      "grad_norm": 21.3464412689209,
      "learning_rate": 8.201918527121779e-06,
      "loss": 0.9204,
      "step": 6763
    },
    {
      "epoch": 2.618660472319009,
      "grad_norm": 28.959510803222656,
      "learning_rate": 8.20148836408999e-06,
      "loss": 1.8898,
      "step": 6764
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 23.22504234313965,
      "learning_rate": 8.201058201058202e-06,
      "loss": 1.5908,
      "step": 6765
    },
    {
      "epoch": 2.619434765776229,
      "grad_norm": 25.01123046875,
      "learning_rate": 8.200628038026413e-06,
      "loss": 2.3087,
      "step": 6766
    },
    {
      "epoch": 2.619821912504839,
      "grad_norm": 32.77255630493164,
      "learning_rate": 8.200197874994623e-06,
      "loss": 2.1669,
      "step": 6767
    },
    {
      "epoch": 2.6202090592334493,
      "grad_norm": 45.42306137084961,
      "learning_rate": 8.199767711962834e-06,
      "loss": 0.9106,
      "step": 6768
    },
    {
      "epoch": 2.6205962059620598,
      "grad_norm": 6.818270206451416,
      "learning_rate": 8.199337548931046e-06,
      "loss": 0.3635,
      "step": 6769
    },
    {
      "epoch": 2.62098335269067,
      "grad_norm": 17.08702278137207,
      "learning_rate": 8.198907385899255e-06,
      "loss": 1.3182,
      "step": 6770
    },
    {
      "epoch": 2.62137049941928,
      "grad_norm": 15.267102241516113,
      "learning_rate": 8.198477222867467e-06,
      "loss": 0.3628,
      "step": 6771
    },
    {
      "epoch": 2.62175764614789,
      "grad_norm": 13.584625244140625,
      "learning_rate": 8.198047059835678e-06,
      "loss": 0.896,
      "step": 6772
    },
    {
      "epoch": 2.6221447928765,
      "grad_norm": 28.148693084716797,
      "learning_rate": 8.19761689680389e-06,
      "loss": 1.5382,
      "step": 6773
    },
    {
      "epoch": 2.6225319396051106,
      "grad_norm": 18.597227096557617,
      "learning_rate": 8.1971867337721e-06,
      "loss": 0.6212,
      "step": 6774
    },
    {
      "epoch": 2.6229190863337206,
      "grad_norm": 19.353792190551758,
      "learning_rate": 8.196756570740311e-06,
      "loss": 1.8649,
      "step": 6775
    },
    {
      "epoch": 2.6233062330623307,
      "grad_norm": 28.676856994628906,
      "learning_rate": 8.196326407708522e-06,
      "loss": 1.6222,
      "step": 6776
    },
    {
      "epoch": 2.6236933797909407,
      "grad_norm": 14.287002563476562,
      "learning_rate": 8.195896244676734e-06,
      "loss": 1.03,
      "step": 6777
    },
    {
      "epoch": 2.624080526519551,
      "grad_norm": 17.211612701416016,
      "learning_rate": 8.195466081644943e-06,
      "loss": 1.2141,
      "step": 6778
    },
    {
      "epoch": 2.6244676732481613,
      "grad_norm": 23.607275009155273,
      "learning_rate": 8.195035918613155e-06,
      "loss": 1.7107,
      "step": 6779
    },
    {
      "epoch": 2.624854819976771,
      "grad_norm": 10.326000213623047,
      "learning_rate": 8.194605755581366e-06,
      "loss": 1.334,
      "step": 6780
    },
    {
      "epoch": 2.6252419667053815,
      "grad_norm": 31.889787673950195,
      "learning_rate": 8.194175592549578e-06,
      "loss": 1.0187,
      "step": 6781
    },
    {
      "epoch": 2.6256291134339915,
      "grad_norm": 34.21887969970703,
      "learning_rate": 8.193745429517787e-06,
      "loss": 1.4267,
      "step": 6782
    },
    {
      "epoch": 2.6260162601626016,
      "grad_norm": 42.11840057373047,
      "learning_rate": 8.193315266485999e-06,
      "loss": 1.7238,
      "step": 6783
    },
    {
      "epoch": 2.6264034068912117,
      "grad_norm": 14.327359199523926,
      "learning_rate": 8.19288510345421e-06,
      "loss": 0.9127,
      "step": 6784
    },
    {
      "epoch": 2.6267905536198217,
      "grad_norm": 4.328949928283691,
      "learning_rate": 8.19245494042242e-06,
      "loss": 0.1398,
      "step": 6785
    },
    {
      "epoch": 2.6271777003484322,
      "grad_norm": 25.115259170532227,
      "learning_rate": 8.192024777390631e-06,
      "loss": 1.6619,
      "step": 6786
    },
    {
      "epoch": 2.6275648470770423,
      "grad_norm": 16.844348907470703,
      "learning_rate": 8.191594614358843e-06,
      "loss": 1.5239,
      "step": 6787
    },
    {
      "epoch": 2.6279519938056524,
      "grad_norm": 20.329212188720703,
      "learning_rate": 8.191164451327054e-06,
      "loss": 1.7941,
      "step": 6788
    },
    {
      "epoch": 2.6283391405342624,
      "grad_norm": 4.23396635055542,
      "learning_rate": 8.190734288295264e-06,
      "loss": 0.1396,
      "step": 6789
    },
    {
      "epoch": 2.6287262872628725,
      "grad_norm": 4.516357898712158,
      "learning_rate": 8.190304125263475e-06,
      "loss": 0.2607,
      "step": 6790
    },
    {
      "epoch": 2.629113433991483,
      "grad_norm": 24.293888092041016,
      "learning_rate": 8.189873962231687e-06,
      "loss": 2.5151,
      "step": 6791
    },
    {
      "epoch": 2.629500580720093,
      "grad_norm": 21.401498794555664,
      "learning_rate": 8.189443799199898e-06,
      "loss": 1.9127,
      "step": 6792
    },
    {
      "epoch": 2.629887727448703,
      "grad_norm": 23.699113845825195,
      "learning_rate": 8.189013636168108e-06,
      "loss": 1.6373,
      "step": 6793
    },
    {
      "epoch": 2.630274874177313,
      "grad_norm": 14.917882919311523,
      "learning_rate": 8.18858347313632e-06,
      "loss": 1.1449,
      "step": 6794
    },
    {
      "epoch": 2.6306620209059233,
      "grad_norm": 15.138022422790527,
      "learning_rate": 8.18815331010453e-06,
      "loss": 0.5192,
      "step": 6795
    },
    {
      "epoch": 2.6310491676345333,
      "grad_norm": 33.855873107910156,
      "learning_rate": 8.187723147072742e-06,
      "loss": 2.1341,
      "step": 6796
    },
    {
      "epoch": 2.6314363143631434,
      "grad_norm": 26.152545928955078,
      "learning_rate": 8.187292984040952e-06,
      "loss": 2.7438,
      "step": 6797
    },
    {
      "epoch": 2.631823461091754,
      "grad_norm": 70.29946899414062,
      "learning_rate": 8.186862821009163e-06,
      "loss": 2.414,
      "step": 6798
    },
    {
      "epoch": 2.632210607820364,
      "grad_norm": 20.2047119140625,
      "learning_rate": 8.186432657977375e-06,
      "loss": 1.5399,
      "step": 6799
    },
    {
      "epoch": 2.632597754548974,
      "grad_norm": 23.607492446899414,
      "learning_rate": 8.186002494945584e-06,
      "loss": 1.5795,
      "step": 6800
    },
    {
      "epoch": 2.632984901277584,
      "grad_norm": 4.55867862701416,
      "learning_rate": 8.185572331913796e-06,
      "loss": 0.1507,
      "step": 6801
    },
    {
      "epoch": 2.633372048006194,
      "grad_norm": 12.966931343078613,
      "learning_rate": 8.185142168882007e-06,
      "loss": 0.7151,
      "step": 6802
    },
    {
      "epoch": 2.6337591947348047,
      "grad_norm": 12.58231258392334,
      "learning_rate": 8.184712005850219e-06,
      "loss": 0.8694,
      "step": 6803
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 35.72650909423828,
      "learning_rate": 8.184281842818428e-06,
      "loss": 1.7943,
      "step": 6804
    },
    {
      "epoch": 2.634533488192025,
      "grad_norm": 22.86780548095703,
      "learning_rate": 8.18385167978664e-06,
      "loss": 1.9576,
      "step": 6805
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 23.918926239013672,
      "learning_rate": 8.18342151675485e-06,
      "loss": 2.1379,
      "step": 6806
    },
    {
      "epoch": 2.635307781649245,
      "grad_norm": 34.962066650390625,
      "learning_rate": 8.182991353723063e-06,
      "loss": 1.5648,
      "step": 6807
    },
    {
      "epoch": 2.6356949283778555,
      "grad_norm": 13.979260444641113,
      "learning_rate": 8.182561190691272e-06,
      "loss": 1.0496,
      "step": 6808
    },
    {
      "epoch": 2.636082075106465,
      "grad_norm": 42.99247741699219,
      "learning_rate": 8.182131027659484e-06,
      "loss": 1.7907,
      "step": 6809
    },
    {
      "epoch": 2.6364692218350756,
      "grad_norm": 17.598604202270508,
      "learning_rate": 8.181700864627693e-06,
      "loss": 0.9736,
      "step": 6810
    },
    {
      "epoch": 2.6368563685636857,
      "grad_norm": 34.99984359741211,
      "learning_rate": 8.181270701595907e-06,
      "loss": 3.1024,
      "step": 6811
    },
    {
      "epoch": 2.6372435152922957,
      "grad_norm": 22.52560806274414,
      "learning_rate": 8.180840538564116e-06,
      "loss": 1.2197,
      "step": 6812
    },
    {
      "epoch": 2.637630662020906,
      "grad_norm": 21.42768669128418,
      "learning_rate": 8.180410375532328e-06,
      "loss": 2.6411,
      "step": 6813
    },
    {
      "epoch": 2.638017808749516,
      "grad_norm": 22.535953521728516,
      "learning_rate": 8.179980212500537e-06,
      "loss": 1.672,
      "step": 6814
    },
    {
      "epoch": 2.6384049554781264,
      "grad_norm": 43.11405944824219,
      "learning_rate": 8.179550049468749e-06,
      "loss": 1.4385,
      "step": 6815
    },
    {
      "epoch": 2.6387921022067364,
      "grad_norm": 20.74634552001953,
      "learning_rate": 8.17911988643696e-06,
      "loss": 1.8094,
      "step": 6816
    },
    {
      "epoch": 2.6391792489353465,
      "grad_norm": 34.913509368896484,
      "learning_rate": 8.178689723405172e-06,
      "loss": 1.6755,
      "step": 6817
    },
    {
      "epoch": 2.6395663956639566,
      "grad_norm": 18.65781593322754,
      "learning_rate": 8.178259560373381e-06,
      "loss": 1.8384,
      "step": 6818
    },
    {
      "epoch": 2.6399535423925666,
      "grad_norm": 19.49274253845215,
      "learning_rate": 8.177829397341593e-06,
      "loss": 1.1593,
      "step": 6819
    },
    {
      "epoch": 2.640340689121177,
      "grad_norm": 26.351980209350586,
      "learning_rate": 8.177399234309804e-06,
      "loss": 1.4605,
      "step": 6820
    },
    {
      "epoch": 2.640727835849787,
      "grad_norm": 9.116948127746582,
      "learning_rate": 8.176969071278014e-06,
      "loss": 1.3028,
      "step": 6821
    },
    {
      "epoch": 2.6411149825783973,
      "grad_norm": 12.702764511108398,
      "learning_rate": 8.176538908246227e-06,
      "loss": 0.9591,
      "step": 6822
    },
    {
      "epoch": 2.6415021293070073,
      "grad_norm": 17.619953155517578,
      "learning_rate": 8.176108745214437e-06,
      "loss": 1.4811,
      "step": 6823
    },
    {
      "epoch": 2.6418892760356174,
      "grad_norm": 42.35275650024414,
      "learning_rate": 8.175678582182648e-06,
      "loss": 3.0044,
      "step": 6824
    },
    {
      "epoch": 2.642276422764228,
      "grad_norm": 35.443580627441406,
      "learning_rate": 8.175248419150858e-06,
      "loss": 1.7628,
      "step": 6825
    },
    {
      "epoch": 2.6426635694928375,
      "grad_norm": 13.061493873596191,
      "learning_rate": 8.174818256119071e-06,
      "loss": 0.8077,
      "step": 6826
    },
    {
      "epoch": 2.643050716221448,
      "grad_norm": 38.69757080078125,
      "learning_rate": 8.17438809308728e-06,
      "loss": 1.0347,
      "step": 6827
    },
    {
      "epoch": 2.643437862950058,
      "grad_norm": 23.38625144958496,
      "learning_rate": 8.173957930055492e-06,
      "loss": 1.9193,
      "step": 6828
    },
    {
      "epoch": 2.643825009678668,
      "grad_norm": 34.39397048950195,
      "learning_rate": 8.173527767023702e-06,
      "loss": 1.2205,
      "step": 6829
    },
    {
      "epoch": 2.6442121564072782,
      "grad_norm": 5.19598913192749,
      "learning_rate": 8.173097603991913e-06,
      "loss": 0.1717,
      "step": 6830
    },
    {
      "epoch": 2.6445993031358883,
      "grad_norm": 16.458358764648438,
      "learning_rate": 8.172667440960125e-06,
      "loss": 1.0132,
      "step": 6831
    },
    {
      "epoch": 2.644986449864499,
      "grad_norm": 20.629411697387695,
      "learning_rate": 8.172237277928336e-06,
      "loss": 1.4804,
      "step": 6832
    },
    {
      "epoch": 2.645373596593109,
      "grad_norm": 27.80439567565918,
      "learning_rate": 8.171807114896546e-06,
      "loss": 1.5259,
      "step": 6833
    },
    {
      "epoch": 2.645760743321719,
      "grad_norm": 17.359024047851562,
      "learning_rate": 8.171376951864757e-06,
      "loss": 1.377,
      "step": 6834
    },
    {
      "epoch": 2.646147890050329,
      "grad_norm": 22.10283088684082,
      "learning_rate": 8.170946788832969e-06,
      "loss": 1.4688,
      "step": 6835
    },
    {
      "epoch": 2.646535036778939,
      "grad_norm": 14.450483322143555,
      "learning_rate": 8.170516625801178e-06,
      "loss": 0.8348,
      "step": 6836
    },
    {
      "epoch": 2.6469221835075496,
      "grad_norm": 11.347043991088867,
      "learning_rate": 8.17008646276939e-06,
      "loss": 1.0931,
      "step": 6837
    },
    {
      "epoch": 2.6473093302361597,
      "grad_norm": 13.298074722290039,
      "learning_rate": 8.169656299737601e-06,
      "loss": 1.3011,
      "step": 6838
    },
    {
      "epoch": 2.6476964769647697,
      "grad_norm": 36.73592758178711,
      "learning_rate": 8.169226136705813e-06,
      "loss": 2.4097,
      "step": 6839
    },
    {
      "epoch": 2.64808362369338,
      "grad_norm": 12.693828582763672,
      "learning_rate": 8.168795973674022e-06,
      "loss": 0.8613,
      "step": 6840
    },
    {
      "epoch": 2.64847077042199,
      "grad_norm": 38.02173614501953,
      "learning_rate": 8.168365810642234e-06,
      "loss": 1.6257,
      "step": 6841
    },
    {
      "epoch": 2.6488579171506,
      "grad_norm": 13.683550834655762,
      "learning_rate": 8.167935647610445e-06,
      "loss": 1.107,
      "step": 6842
    },
    {
      "epoch": 2.64924506387921,
      "grad_norm": 31.00716781616211,
      "learning_rate": 8.167505484578657e-06,
      "loss": 1.6647,
      "step": 6843
    },
    {
      "epoch": 2.6496322106078205,
      "grad_norm": 25.85207176208496,
      "learning_rate": 8.167075321546866e-06,
      "loss": 0.9698,
      "step": 6844
    },
    {
      "epoch": 2.6500193573364306,
      "grad_norm": 34.33409118652344,
      "learning_rate": 8.166645158515078e-06,
      "loss": 1.9656,
      "step": 6845
    },
    {
      "epoch": 2.6504065040650406,
      "grad_norm": 14.55190372467041,
      "learning_rate": 8.16621499548329e-06,
      "loss": 1.4178,
      "step": 6846
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 20.65404510498047,
      "learning_rate": 8.1657848324515e-06,
      "loss": 1.578,
      "step": 6847
    },
    {
      "epoch": 2.6511807975222608,
      "grad_norm": 17.034399032592773,
      "learning_rate": 8.16535466941971e-06,
      "loss": 1.4611,
      "step": 6848
    },
    {
      "epoch": 2.6515679442508713,
      "grad_norm": 42.88684844970703,
      "learning_rate": 8.164924506387922e-06,
      "loss": 1.8885,
      "step": 6849
    },
    {
      "epoch": 2.6519550909794813,
      "grad_norm": 26.775917053222656,
      "learning_rate": 8.164494343356133e-06,
      "loss": 1.5193,
      "step": 6850
    },
    {
      "epoch": 2.6523422377080914,
      "grad_norm": 6.3697404861450195,
      "learning_rate": 8.164064180324343e-06,
      "loss": 0.3484,
      "step": 6851
    },
    {
      "epoch": 2.6527293844367015,
      "grad_norm": 12.143036842346191,
      "learning_rate": 8.163634017292554e-06,
      "loss": 0.7453,
      "step": 6852
    },
    {
      "epoch": 2.6531165311653115,
      "grad_norm": 17.643939971923828,
      "learning_rate": 8.163203854260766e-06,
      "loss": 1.1829,
      "step": 6853
    },
    {
      "epoch": 2.653503677893922,
      "grad_norm": 16.148378372192383,
      "learning_rate": 8.162773691228977e-06,
      "loss": 1.1359,
      "step": 6854
    },
    {
      "epoch": 2.6538908246225317,
      "grad_norm": 15.776527404785156,
      "learning_rate": 8.162343528197187e-06,
      "loss": 1.3572,
      "step": 6855
    },
    {
      "epoch": 2.654277971351142,
      "grad_norm": 24.14165496826172,
      "learning_rate": 8.161913365165398e-06,
      "loss": 1.7366,
      "step": 6856
    },
    {
      "epoch": 2.6546651180797523,
      "grad_norm": 46.310142517089844,
      "learning_rate": 8.161483202133608e-06,
      "loss": 1.9539,
      "step": 6857
    },
    {
      "epoch": 2.6550522648083623,
      "grad_norm": 18.078153610229492,
      "learning_rate": 8.161053039101821e-06,
      "loss": 0.9244,
      "step": 6858
    },
    {
      "epoch": 2.6554394115369724,
      "grad_norm": 13.782843589782715,
      "learning_rate": 8.16062287607003e-06,
      "loss": 0.851,
      "step": 6859
    },
    {
      "epoch": 2.6558265582655824,
      "grad_norm": 25.177974700927734,
      "learning_rate": 8.160192713038242e-06,
      "loss": 0.6868,
      "step": 6860
    },
    {
      "epoch": 2.656213704994193,
      "grad_norm": 68.21583557128906,
      "learning_rate": 8.159762550006452e-06,
      "loss": 2.6572,
      "step": 6861
    },
    {
      "epoch": 2.656600851722803,
      "grad_norm": 18.408918380737305,
      "learning_rate": 8.159332386974665e-06,
      "loss": 1.181,
      "step": 6862
    },
    {
      "epoch": 2.656987998451413,
      "grad_norm": 59.991058349609375,
      "learning_rate": 8.158902223942875e-06,
      "loss": 0.9375,
      "step": 6863
    },
    {
      "epoch": 2.657375145180023,
      "grad_norm": 20.742544174194336,
      "learning_rate": 8.158472060911086e-06,
      "loss": 1.5921,
      "step": 6864
    },
    {
      "epoch": 2.6577622919086332,
      "grad_norm": 9.835295677185059,
      "learning_rate": 8.158041897879298e-06,
      "loss": 1.2734,
      "step": 6865
    },
    {
      "epoch": 2.6581494386372437,
      "grad_norm": 15.690967559814453,
      "learning_rate": 8.157611734847507e-06,
      "loss": 0.9704,
      "step": 6866
    },
    {
      "epoch": 2.658536585365854,
      "grad_norm": 62.20330047607422,
      "learning_rate": 8.157181571815719e-06,
      "loss": 2.2876,
      "step": 6867
    },
    {
      "epoch": 2.658923732094464,
      "grad_norm": 21.094884872436523,
      "learning_rate": 8.15675140878393e-06,
      "loss": 3.1784,
      "step": 6868
    },
    {
      "epoch": 2.659310878823074,
      "grad_norm": 26.899858474731445,
      "learning_rate": 8.156321245752142e-06,
      "loss": 1.4875,
      "step": 6869
    },
    {
      "epoch": 2.659698025551684,
      "grad_norm": 17.32151222229004,
      "learning_rate": 8.155891082720351e-06,
      "loss": 1.7969,
      "step": 6870
    },
    {
      "epoch": 2.6600851722802945,
      "grad_norm": 31.508182525634766,
      "learning_rate": 8.155460919688563e-06,
      "loss": 2.8475,
      "step": 6871
    },
    {
      "epoch": 2.660472319008904,
      "grad_norm": 23.738811492919922,
      "learning_rate": 8.155030756656772e-06,
      "loss": 1.32,
      "step": 6872
    },
    {
      "epoch": 2.6608594657375146,
      "grad_norm": 21.34337043762207,
      "learning_rate": 8.154600593624986e-06,
      "loss": 3.3149,
      "step": 6873
    },
    {
      "epoch": 2.6612466124661247,
      "grad_norm": 15.578225135803223,
      "learning_rate": 8.154170430593195e-06,
      "loss": 1.3133,
      "step": 6874
    },
    {
      "epoch": 2.6616337591947348,
      "grad_norm": 13.601556777954102,
      "learning_rate": 8.153740267561407e-06,
      "loss": 0.7949,
      "step": 6875
    },
    {
      "epoch": 2.662020905923345,
      "grad_norm": 39.00292205810547,
      "learning_rate": 8.153310104529616e-06,
      "loss": 2.1708,
      "step": 6876
    },
    {
      "epoch": 2.662408052651955,
      "grad_norm": 22.417593002319336,
      "learning_rate": 8.15287994149783e-06,
      "loss": 2.2689,
      "step": 6877
    },
    {
      "epoch": 2.6627951993805654,
      "grad_norm": 17.277069091796875,
      "learning_rate": 8.15244977846604e-06,
      "loss": 1.1222,
      "step": 6878
    },
    {
      "epoch": 2.6631823461091755,
      "grad_norm": 16.578256607055664,
      "learning_rate": 8.15201961543425e-06,
      "loss": 1.067,
      "step": 6879
    },
    {
      "epoch": 2.6635694928377855,
      "grad_norm": 17.356595993041992,
      "learning_rate": 8.15158945240246e-06,
      "loss": 0.7,
      "step": 6880
    },
    {
      "epoch": 2.6639566395663956,
      "grad_norm": 29.139114379882812,
      "learning_rate": 8.151159289370672e-06,
      "loss": 1.578,
      "step": 6881
    },
    {
      "epoch": 2.6643437862950057,
      "grad_norm": 22.611135482788086,
      "learning_rate": 8.150729126338883e-06,
      "loss": 1.6843,
      "step": 6882
    },
    {
      "epoch": 2.664730933023616,
      "grad_norm": 12.277639389038086,
      "learning_rate": 8.150298963307095e-06,
      "loss": 0.8456,
      "step": 6883
    },
    {
      "epoch": 2.6651180797522263,
      "grad_norm": 20.367599487304688,
      "learning_rate": 8.149868800275304e-06,
      "loss": 0.8388,
      "step": 6884
    },
    {
      "epoch": 2.6655052264808363,
      "grad_norm": 15.244606018066406,
      "learning_rate": 8.149438637243516e-06,
      "loss": 1.4295,
      "step": 6885
    },
    {
      "epoch": 2.6658923732094464,
      "grad_norm": 30.012054443359375,
      "learning_rate": 8.149008474211727e-06,
      "loss": 2.2438,
      "step": 6886
    },
    {
      "epoch": 2.6662795199380565,
      "grad_norm": 15.00890827178955,
      "learning_rate": 8.148578311179937e-06,
      "loss": 1.3062,
      "step": 6887
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 6.317322254180908,
      "learning_rate": 8.148148148148148e-06,
      "loss": 0.3025,
      "step": 6888
    },
    {
      "epoch": 2.6670538133952766,
      "grad_norm": 26.591049194335938,
      "learning_rate": 8.14771798511636e-06,
      "loss": 0.669,
      "step": 6889
    },
    {
      "epoch": 2.667440960123887,
      "grad_norm": 14.759337425231934,
      "learning_rate": 8.147287822084571e-06,
      "loss": 1.3228,
      "step": 6890
    },
    {
      "epoch": 2.667828106852497,
      "grad_norm": 15.049444198608398,
      "learning_rate": 8.146857659052781e-06,
      "loss": 1.4739,
      "step": 6891
    },
    {
      "epoch": 2.6682152535811072,
      "grad_norm": 22.575977325439453,
      "learning_rate": 8.146427496020992e-06,
      "loss": 0.9305,
      "step": 6892
    },
    {
      "epoch": 2.6686024003097173,
      "grad_norm": 25.128395080566406,
      "learning_rate": 8.145997332989204e-06,
      "loss": 1.7911,
      "step": 6893
    },
    {
      "epoch": 2.6689895470383274,
      "grad_norm": 21.06012535095215,
      "learning_rate": 8.145567169957415e-06,
      "loss": 1.5744,
      "step": 6894
    },
    {
      "epoch": 2.669376693766938,
      "grad_norm": 19.792896270751953,
      "learning_rate": 8.145137006925625e-06,
      "loss": 1.2452,
      "step": 6895
    },
    {
      "epoch": 2.669763840495548,
      "grad_norm": 20.91855239868164,
      "learning_rate": 8.144706843893836e-06,
      "loss": 1.8876,
      "step": 6896
    },
    {
      "epoch": 2.670150987224158,
      "grad_norm": 58.633541107177734,
      "learning_rate": 8.144276680862048e-06,
      "loss": 1.4324,
      "step": 6897
    },
    {
      "epoch": 2.670538133952768,
      "grad_norm": 13.016094207763672,
      "learning_rate": 8.143846517830259e-06,
      "loss": 1.37,
      "step": 6898
    },
    {
      "epoch": 2.670925280681378,
      "grad_norm": 18.221662521362305,
      "learning_rate": 8.143416354798469e-06,
      "loss": 1.7443,
      "step": 6899
    },
    {
      "epoch": 2.6713124274099886,
      "grad_norm": 27.506872177124023,
      "learning_rate": 8.14298619176668e-06,
      "loss": 1.7021,
      "step": 6900
    },
    {
      "epoch": 2.6716995741385983,
      "grad_norm": 13.61814022064209,
      "learning_rate": 8.142556028734892e-06,
      "loss": 0.9904,
      "step": 6901
    },
    {
      "epoch": 2.6720867208672088,
      "grad_norm": 12.41499137878418,
      "learning_rate": 8.142125865703101e-06,
      "loss": 0.7208,
      "step": 6902
    },
    {
      "epoch": 2.672473867595819,
      "grad_norm": 19.15549659729004,
      "learning_rate": 8.141695702671313e-06,
      "loss": 1.0295,
      "step": 6903
    },
    {
      "epoch": 2.672861014324429,
      "grad_norm": 24.522232055664062,
      "learning_rate": 8.141265539639524e-06,
      "loss": 2.0013,
      "step": 6904
    },
    {
      "epoch": 2.673248161053039,
      "grad_norm": 15.922727584838867,
      "learning_rate": 8.140835376607736e-06,
      "loss": 1.3183,
      "step": 6905
    },
    {
      "epoch": 2.673635307781649,
      "grad_norm": 58.172096252441406,
      "learning_rate": 8.140405213575945e-06,
      "loss": 1.9962,
      "step": 6906
    },
    {
      "epoch": 2.6740224545102595,
      "grad_norm": 22.36731719970703,
      "learning_rate": 8.139975050544157e-06,
      "loss": 1.668,
      "step": 6907
    },
    {
      "epoch": 2.6744096012388696,
      "grad_norm": 15.982538223266602,
      "learning_rate": 8.139544887512368e-06,
      "loss": 1.3345,
      "step": 6908
    },
    {
      "epoch": 2.6747967479674797,
      "grad_norm": 24.61739158630371,
      "learning_rate": 8.13911472448058e-06,
      "loss": 1.4104,
      "step": 6909
    },
    {
      "epoch": 2.6751838946960897,
      "grad_norm": 14.568934440612793,
      "learning_rate": 8.13868456144879e-06,
      "loss": 1.3233,
      "step": 6910
    },
    {
      "epoch": 2.6755710414247,
      "grad_norm": 13.278650283813477,
      "learning_rate": 8.138254398417e-06,
      "loss": 0.8657,
      "step": 6911
    },
    {
      "epoch": 2.6759581881533103,
      "grad_norm": 31.710447311401367,
      "learning_rate": 8.137824235385212e-06,
      "loss": 0.8848,
      "step": 6912
    },
    {
      "epoch": 2.6763453348819204,
      "grad_norm": 32.453983306884766,
      "learning_rate": 8.137394072353424e-06,
      "loss": 1.3827,
      "step": 6913
    },
    {
      "epoch": 2.6767324816105305,
      "grad_norm": 26.001544952392578,
      "learning_rate": 8.136963909321633e-06,
      "loss": 1.4959,
      "step": 6914
    },
    {
      "epoch": 2.6771196283391405,
      "grad_norm": 14.383692741394043,
      "learning_rate": 8.136533746289845e-06,
      "loss": 1.3299,
      "step": 6915
    },
    {
      "epoch": 2.6775067750677506,
      "grad_norm": 17.752735137939453,
      "learning_rate": 8.136103583258056e-06,
      "loss": 1.3318,
      "step": 6916
    },
    {
      "epoch": 2.6778939217963607,
      "grad_norm": 24.682825088500977,
      "learning_rate": 8.135673420226266e-06,
      "loss": 1.2567,
      "step": 6917
    },
    {
      "epoch": 2.6782810685249707,
      "grad_norm": 16.129119873046875,
      "learning_rate": 8.135243257194477e-06,
      "loss": 1.3508,
      "step": 6918
    },
    {
      "epoch": 2.6786682152535812,
      "grad_norm": 18.40675926208496,
      "learning_rate": 8.134813094162689e-06,
      "loss": 1.6052,
      "step": 6919
    },
    {
      "epoch": 2.6790553619821913,
      "grad_norm": 12.099274635314941,
      "learning_rate": 8.1343829311309e-06,
      "loss": 0.7806,
      "step": 6920
    },
    {
      "epoch": 2.6794425087108014,
      "grad_norm": 23.01643180847168,
      "learning_rate": 8.13395276809911e-06,
      "loss": 1.1806,
      "step": 6921
    },
    {
      "epoch": 2.6798296554394114,
      "grad_norm": 20.722675323486328,
      "learning_rate": 8.133522605067321e-06,
      "loss": 1.926,
      "step": 6922
    },
    {
      "epoch": 2.6802168021680215,
      "grad_norm": 15.011143684387207,
      "learning_rate": 8.133092442035531e-06,
      "loss": 1.4306,
      "step": 6923
    },
    {
      "epoch": 2.680603948896632,
      "grad_norm": 26.7178897857666,
      "learning_rate": 8.132662279003744e-06,
      "loss": 1.1535,
      "step": 6924
    },
    {
      "epoch": 2.680991095625242,
      "grad_norm": 41.94824981689453,
      "learning_rate": 8.132232115971954e-06,
      "loss": 1.0941,
      "step": 6925
    },
    {
      "epoch": 2.681378242353852,
      "grad_norm": 5.577817440032959,
      "learning_rate": 8.131801952940165e-06,
      "loss": 0.1779,
      "step": 6926
    },
    {
      "epoch": 2.681765389082462,
      "grad_norm": 23.203231811523438,
      "learning_rate": 8.131371789908375e-06,
      "loss": 1.537,
      "step": 6927
    },
    {
      "epoch": 2.6821525358110723,
      "grad_norm": 9.610389709472656,
      "learning_rate": 8.130941626876588e-06,
      "loss": 0.5149,
      "step": 6928
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 23.800106048583984,
      "learning_rate": 8.130511463844798e-06,
      "loss": 2.0112,
      "step": 6929
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 24.39564323425293,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.1564,
      "step": 6930
    },
    {
      "epoch": 2.683313975996903,
      "grad_norm": 32.54556655883789,
      "learning_rate": 8.129651137781219e-06,
      "loss": 1.8207,
      "step": 6931
    },
    {
      "epoch": 2.683701122725513,
      "grad_norm": 34.4888916015625,
      "learning_rate": 8.12922097474943e-06,
      "loss": 0.942,
      "step": 6932
    },
    {
      "epoch": 2.684088269454123,
      "grad_norm": 14.427404403686523,
      "learning_rate": 8.128790811717642e-06,
      "loss": 0.9904,
      "step": 6933
    },
    {
      "epoch": 2.684475416182733,
      "grad_norm": 24.897756576538086,
      "learning_rate": 8.128360648685853e-06,
      "loss": 1.2172,
      "step": 6934
    },
    {
      "epoch": 2.684862562911343,
      "grad_norm": 23.55988311767578,
      "learning_rate": 8.127930485654063e-06,
      "loss": 2.7467,
      "step": 6935
    },
    {
      "epoch": 2.6852497096399537,
      "grad_norm": 12.097504615783691,
      "learning_rate": 8.127500322622274e-06,
      "loss": 1.3307,
      "step": 6936
    },
    {
      "epoch": 2.6856368563685638,
      "grad_norm": 12.343101501464844,
      "learning_rate": 8.127070159590486e-06,
      "loss": 1.174,
      "step": 6937
    },
    {
      "epoch": 2.686024003097174,
      "grad_norm": 59.28813934326172,
      "learning_rate": 8.126639996558695e-06,
      "loss": 1.6725,
      "step": 6938
    },
    {
      "epoch": 2.686411149825784,
      "grad_norm": 47.93258285522461,
      "learning_rate": 8.126209833526907e-06,
      "loss": 2.8693,
      "step": 6939
    },
    {
      "epoch": 2.686798296554394,
      "grad_norm": 18.121198654174805,
      "learning_rate": 8.125779670495118e-06,
      "loss": 1.2278,
      "step": 6940
    },
    {
      "epoch": 2.6871854432830045,
      "grad_norm": 14.434223175048828,
      "learning_rate": 8.12534950746333e-06,
      "loss": 1.0343,
      "step": 6941
    },
    {
      "epoch": 2.6875725900116145,
      "grad_norm": 15.920064926147461,
      "learning_rate": 8.12491934443154e-06,
      "loss": 0.9254,
      "step": 6942
    },
    {
      "epoch": 2.6879597367402246,
      "grad_norm": 22.641130447387695,
      "learning_rate": 8.12448918139975e-06,
      "loss": 1.819,
      "step": 6943
    },
    {
      "epoch": 2.6883468834688347,
      "grad_norm": 33.96339797973633,
      "learning_rate": 8.124059018367962e-06,
      "loss": 1.443,
      "step": 6944
    },
    {
      "epoch": 2.6887340301974447,
      "grad_norm": 23.854766845703125,
      "learning_rate": 8.123628855336174e-06,
      "loss": 0.8638,
      "step": 6945
    },
    {
      "epoch": 2.6891211769260552,
      "grad_norm": 20.297819137573242,
      "learning_rate": 8.123198692304383e-06,
      "loss": 1.7825,
      "step": 6946
    },
    {
      "epoch": 2.689508323654665,
      "grad_norm": 16.012596130371094,
      "learning_rate": 8.122768529272595e-06,
      "loss": 1.4179,
      "step": 6947
    },
    {
      "epoch": 2.6898954703832754,
      "grad_norm": 24.108814239501953,
      "learning_rate": 8.122338366240806e-06,
      "loss": 1.9823,
      "step": 6948
    },
    {
      "epoch": 2.6902826171118854,
      "grad_norm": 11.294517517089844,
      "learning_rate": 8.121908203209018e-06,
      "loss": 0.6776,
      "step": 6949
    },
    {
      "epoch": 2.6906697638404955,
      "grad_norm": 9.98677921295166,
      "learning_rate": 8.121478040177227e-06,
      "loss": 1.3975,
      "step": 6950
    },
    {
      "epoch": 2.6910569105691056,
      "grad_norm": 27.344152450561523,
      "learning_rate": 8.121047877145439e-06,
      "loss": 1.6124,
      "step": 6951
    },
    {
      "epoch": 2.6914440572977156,
      "grad_norm": 27.98661994934082,
      "learning_rate": 8.12061771411365e-06,
      "loss": 1.6204,
      "step": 6952
    },
    {
      "epoch": 2.691831204026326,
      "grad_norm": 22.096054077148438,
      "learning_rate": 8.12018755108186e-06,
      "loss": 2.9565,
      "step": 6953
    },
    {
      "epoch": 2.692218350754936,
      "grad_norm": 26.15512466430664,
      "learning_rate": 8.119757388050071e-06,
      "loss": 1.8015,
      "step": 6954
    },
    {
      "epoch": 2.6926054974835463,
      "grad_norm": 89.75724792480469,
      "learning_rate": 8.119327225018283e-06,
      "loss": 0.935,
      "step": 6955
    },
    {
      "epoch": 2.6929926442121563,
      "grad_norm": 25.75307273864746,
      "learning_rate": 8.118897061986494e-06,
      "loss": 2.1894,
      "step": 6956
    },
    {
      "epoch": 2.6933797909407664,
      "grad_norm": 37.13037872314453,
      "learning_rate": 8.118466898954704e-06,
      "loss": 1.4161,
      "step": 6957
    },
    {
      "epoch": 2.693766937669377,
      "grad_norm": 21.43087387084961,
      "learning_rate": 8.118036735922915e-06,
      "loss": 1.4888,
      "step": 6958
    },
    {
      "epoch": 2.694154084397987,
      "grad_norm": 12.629855155944824,
      "learning_rate": 8.117606572891127e-06,
      "loss": 0.598,
      "step": 6959
    },
    {
      "epoch": 2.694541231126597,
      "grad_norm": 14.333861351013184,
      "learning_rate": 8.117176409859338e-06,
      "loss": 1.4319,
      "step": 6960
    },
    {
      "epoch": 2.694928377855207,
      "grad_norm": 14.215580940246582,
      "learning_rate": 8.116746246827548e-06,
      "loss": 1.4095,
      "step": 6961
    },
    {
      "epoch": 2.695315524583817,
      "grad_norm": 21.171207427978516,
      "learning_rate": 8.11631608379576e-06,
      "loss": 1.4529,
      "step": 6962
    },
    {
      "epoch": 2.6957026713124272,
      "grad_norm": 33.36982345581055,
      "learning_rate": 8.11588592076397e-06,
      "loss": 1.7492,
      "step": 6963
    },
    {
      "epoch": 2.6960898180410373,
      "grad_norm": 12.37049388885498,
      "learning_rate": 8.115455757732182e-06,
      "loss": 0.9272,
      "step": 6964
    },
    {
      "epoch": 2.696476964769648,
      "grad_norm": 28.69647979736328,
      "learning_rate": 8.115025594700392e-06,
      "loss": 1.5271,
      "step": 6965
    },
    {
      "epoch": 2.696864111498258,
      "grad_norm": 4.860927581787109,
      "learning_rate": 8.114595431668603e-06,
      "loss": 0.2423,
      "step": 6966
    },
    {
      "epoch": 2.697251258226868,
      "grad_norm": 11.489604949951172,
      "learning_rate": 8.114165268636815e-06,
      "loss": 1.1217,
      "step": 6967
    },
    {
      "epoch": 2.697638404955478,
      "grad_norm": 20.816389083862305,
      "learning_rate": 8.113735105605024e-06,
      "loss": 0.9073,
      "step": 6968
    },
    {
      "epoch": 2.698025551684088,
      "grad_norm": 17.855342864990234,
      "learning_rate": 8.113304942573236e-06,
      "loss": 1.5571,
      "step": 6969
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 25.371082305908203,
      "learning_rate": 8.112874779541447e-06,
      "loss": 0.9996,
      "step": 6970
    },
    {
      "epoch": 2.6987998451413087,
      "grad_norm": 20.547344207763672,
      "learning_rate": 8.112444616509659e-06,
      "loss": 1.6268,
      "step": 6971
    },
    {
      "epoch": 2.6991869918699187,
      "grad_norm": 48.07506561279297,
      "learning_rate": 8.112014453477868e-06,
      "loss": 1.5094,
      "step": 6972
    },
    {
      "epoch": 2.699574138598529,
      "grad_norm": 20.532865524291992,
      "learning_rate": 8.11158429044608e-06,
      "loss": 1.6714,
      "step": 6973
    },
    {
      "epoch": 2.699961285327139,
      "grad_norm": 14.21374797821045,
      "learning_rate": 8.11115412741429e-06,
      "loss": 0.9414,
      "step": 6974
    },
    {
      "epoch": 2.7003484320557494,
      "grad_norm": 16.28261375427246,
      "learning_rate": 8.110723964382503e-06,
      "loss": 1.5685,
      "step": 6975
    },
    {
      "epoch": 2.7007355787843594,
      "grad_norm": 12.837137222290039,
      "learning_rate": 8.110293801350712e-06,
      "loss": 0.7039,
      "step": 6976
    },
    {
      "epoch": 2.7011227255129695,
      "grad_norm": 15.382270812988281,
      "learning_rate": 8.109863638318924e-06,
      "loss": 1.7159,
      "step": 6977
    },
    {
      "epoch": 2.7015098722415796,
      "grad_norm": 20.767440795898438,
      "learning_rate": 8.109433475287133e-06,
      "loss": 1.6216,
      "step": 6978
    },
    {
      "epoch": 2.7018970189701896,
      "grad_norm": 20.54705238342285,
      "learning_rate": 8.109003312255346e-06,
      "loss": 2.0129,
      "step": 6979
    },
    {
      "epoch": 2.7022841656987997,
      "grad_norm": 24.147335052490234,
      "learning_rate": 8.108573149223556e-06,
      "loss": 1.5915,
      "step": 6980
    },
    {
      "epoch": 2.7026713124274098,
      "grad_norm": 15.81137466430664,
      "learning_rate": 8.108142986191768e-06,
      "loss": 0.8536,
      "step": 6981
    },
    {
      "epoch": 2.7030584591560203,
      "grad_norm": 17.387022018432617,
      "learning_rate": 8.107712823159977e-06,
      "loss": 1.5908,
      "step": 6982
    },
    {
      "epoch": 2.7034456058846303,
      "grad_norm": 33.798912048339844,
      "learning_rate": 8.107282660128189e-06,
      "loss": 2.2044,
      "step": 6983
    },
    {
      "epoch": 2.7038327526132404,
      "grad_norm": 17.290990829467773,
      "learning_rate": 8.1068524970964e-06,
      "loss": 1.5533,
      "step": 6984
    },
    {
      "epoch": 2.7042198993418505,
      "grad_norm": 31.911649703979492,
      "learning_rate": 8.106422334064612e-06,
      "loss": 1.264,
      "step": 6985
    },
    {
      "epoch": 2.7046070460704605,
      "grad_norm": 15.855948448181152,
      "learning_rate": 8.105992171032823e-06,
      "loss": 1.6158,
      "step": 6986
    },
    {
      "epoch": 2.704994192799071,
      "grad_norm": 26.374900817871094,
      "learning_rate": 8.105562008001033e-06,
      "loss": 1.7096,
      "step": 6987
    },
    {
      "epoch": 2.705381339527681,
      "grad_norm": 20.237546920776367,
      "learning_rate": 8.105131844969244e-06,
      "loss": 1.1341,
      "step": 6988
    },
    {
      "epoch": 2.705768486256291,
      "grad_norm": 26.54198455810547,
      "learning_rate": 8.104701681937454e-06,
      "loss": 1.7487,
      "step": 6989
    },
    {
      "epoch": 2.7061556329849012,
      "grad_norm": 26.40629768371582,
      "learning_rate": 8.104271518905667e-06,
      "loss": 0.8285,
      "step": 6990
    },
    {
      "epoch": 2.7065427797135113,
      "grad_norm": 21.979511260986328,
      "learning_rate": 8.103841355873877e-06,
      "loss": 2.7603,
      "step": 6991
    },
    {
      "epoch": 2.706929926442122,
      "grad_norm": 19.314130783081055,
      "learning_rate": 8.103411192842088e-06,
      "loss": 1.3698,
      "step": 6992
    },
    {
      "epoch": 2.7073170731707314,
      "grad_norm": 34.470252990722656,
      "learning_rate": 8.102981029810298e-06,
      "loss": 2.0861,
      "step": 6993
    },
    {
      "epoch": 2.707704219899342,
      "grad_norm": 34.04669189453125,
      "learning_rate": 8.102550866778511e-06,
      "loss": 0.9778,
      "step": 6994
    },
    {
      "epoch": 2.708091366627952,
      "grad_norm": 17.781618118286133,
      "learning_rate": 8.10212070374672e-06,
      "loss": 1.569,
      "step": 6995
    },
    {
      "epoch": 2.708478513356562,
      "grad_norm": 37.5426025390625,
      "learning_rate": 8.101690540714932e-06,
      "loss": 1.7771,
      "step": 6996
    },
    {
      "epoch": 2.708865660085172,
      "grad_norm": 10.223275184631348,
      "learning_rate": 8.101260377683142e-06,
      "loss": 0.5162,
      "step": 6997
    },
    {
      "epoch": 2.709252806813782,
      "grad_norm": 51.00102996826172,
      "learning_rate": 8.100830214651353e-06,
      "loss": 2.5827,
      "step": 6998
    },
    {
      "epoch": 2.7096399535423927,
      "grad_norm": 23.393230438232422,
      "learning_rate": 8.100400051619565e-06,
      "loss": 1.1148,
      "step": 6999
    },
    {
      "epoch": 2.710027100271003,
      "grad_norm": 13.598454475402832,
      "learning_rate": 8.099969888587776e-06,
      "loss": 0.862,
      "step": 7000
    },
    {
      "epoch": 2.710414246999613,
      "grad_norm": 34.58905029296875,
      "learning_rate": 8.099539725555986e-06,
      "loss": 1.9727,
      "step": 7001
    },
    {
      "epoch": 2.710801393728223,
      "grad_norm": 13.812541961669922,
      "learning_rate": 8.099109562524197e-06,
      "loss": 0.7898,
      "step": 7002
    },
    {
      "epoch": 2.711188540456833,
      "grad_norm": 15.493898391723633,
      "learning_rate": 8.098679399492409e-06,
      "loss": 1.2865,
      "step": 7003
    },
    {
      "epoch": 2.7115756871854435,
      "grad_norm": 20.855762481689453,
      "learning_rate": 8.098249236460618e-06,
      "loss": 1.57,
      "step": 7004
    },
    {
      "epoch": 2.7119628339140536,
      "grad_norm": 19.143583297729492,
      "learning_rate": 8.09781907342883e-06,
      "loss": 0.5565,
      "step": 7005
    },
    {
      "epoch": 2.7123499806426636,
      "grad_norm": 15.171135902404785,
      "learning_rate": 8.097388910397041e-06,
      "loss": 1.4513,
      "step": 7006
    },
    {
      "epoch": 2.7127371273712737,
      "grad_norm": 30.848833084106445,
      "learning_rate": 8.096958747365253e-06,
      "loss": 1.6439,
      "step": 7007
    },
    {
      "epoch": 2.7131242740998838,
      "grad_norm": 27.461389541625977,
      "learning_rate": 8.096528584333462e-06,
      "loss": 0.4956,
      "step": 7008
    },
    {
      "epoch": 2.713511420828494,
      "grad_norm": 15.725780487060547,
      "learning_rate": 8.096098421301674e-06,
      "loss": 1.4533,
      "step": 7009
    },
    {
      "epoch": 2.713898567557104,
      "grad_norm": 16.5645751953125,
      "learning_rate": 8.095668258269885e-06,
      "loss": 1.1524,
      "step": 7010
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 19.79360008239746,
      "learning_rate": 8.095238095238097e-06,
      "loss": 2.0193,
      "step": 7011
    },
    {
      "epoch": 2.7146728610143245,
      "grad_norm": 11.74072265625,
      "learning_rate": 8.094807932206306e-06,
      "loss": 1.463,
      "step": 7012
    },
    {
      "epoch": 2.7150600077429345,
      "grad_norm": 47.31709671020508,
      "learning_rate": 8.094377769174518e-06,
      "loss": 1.526,
      "step": 7013
    },
    {
      "epoch": 2.7154471544715446,
      "grad_norm": 18.03563690185547,
      "learning_rate": 8.093947606142729e-06,
      "loss": 1.507,
      "step": 7014
    },
    {
      "epoch": 2.7158343012001547,
      "grad_norm": 18.414831161499023,
      "learning_rate": 8.09351744311094e-06,
      "loss": 1.0083,
      "step": 7015
    },
    {
      "epoch": 2.716221447928765,
      "grad_norm": 16.092111587524414,
      "learning_rate": 8.09308728007915e-06,
      "loss": 1.4158,
      "step": 7016
    },
    {
      "epoch": 2.7166085946573753,
      "grad_norm": 17.223430633544922,
      "learning_rate": 8.092657117047362e-06,
      "loss": 1.5137,
      "step": 7017
    },
    {
      "epoch": 2.7169957413859853,
      "grad_norm": 14.20837688446045,
      "learning_rate": 8.092226954015573e-06,
      "loss": 1.0928,
      "step": 7018
    },
    {
      "epoch": 2.7173828881145954,
      "grad_norm": 44.71728515625,
      "learning_rate": 8.091796790983783e-06,
      "loss": 2.3202,
      "step": 7019
    },
    {
      "epoch": 2.7177700348432055,
      "grad_norm": 51.03029251098633,
      "learning_rate": 8.091366627951994e-06,
      "loss": 1.984,
      "step": 7020
    },
    {
      "epoch": 2.718157181571816,
      "grad_norm": 13.59841251373291,
      "learning_rate": 8.090936464920206e-06,
      "loss": 0.7859,
      "step": 7021
    },
    {
      "epoch": 2.718544328300426,
      "grad_norm": 36.581764221191406,
      "learning_rate": 8.090506301888417e-06,
      "loss": 1.2111,
      "step": 7022
    },
    {
      "epoch": 2.718931475029036,
      "grad_norm": 27.275564193725586,
      "learning_rate": 8.090076138856627e-06,
      "loss": 1.8271,
      "step": 7023
    },
    {
      "epoch": 2.719318621757646,
      "grad_norm": 20.876951217651367,
      "learning_rate": 8.089645975824838e-06,
      "loss": 0.9426,
      "step": 7024
    },
    {
      "epoch": 2.7197057684862562,
      "grad_norm": 39.83933639526367,
      "learning_rate": 8.089215812793048e-06,
      "loss": 1.1679,
      "step": 7025
    },
    {
      "epoch": 2.7200929152148663,
      "grad_norm": 21.043827056884766,
      "learning_rate": 8.088785649761261e-06,
      "loss": 1.7135,
      "step": 7026
    },
    {
      "epoch": 2.7204800619434764,
      "grad_norm": 18.112430572509766,
      "learning_rate": 8.08835548672947e-06,
      "loss": 1.0347,
      "step": 7027
    },
    {
      "epoch": 2.720867208672087,
      "grad_norm": 21.740612030029297,
      "learning_rate": 8.087925323697682e-06,
      "loss": 3.2433,
      "step": 7028
    },
    {
      "epoch": 2.721254355400697,
      "grad_norm": 26.09868812561035,
      "learning_rate": 8.087495160665894e-06,
      "loss": 1.5711,
      "step": 7029
    },
    {
      "epoch": 2.721641502129307,
      "grad_norm": 17.404617309570312,
      "learning_rate": 8.087064997634105e-06,
      "loss": 1.2152,
      "step": 7030
    },
    {
      "epoch": 2.722028648857917,
      "grad_norm": 19.696462631225586,
      "learning_rate": 8.086634834602315e-06,
      "loss": 1.0269,
      "step": 7031
    },
    {
      "epoch": 2.722415795586527,
      "grad_norm": 14.317967414855957,
      "learning_rate": 8.086204671570526e-06,
      "loss": 1.0923,
      "step": 7032
    },
    {
      "epoch": 2.7228029423151376,
      "grad_norm": 38.5226936340332,
      "learning_rate": 8.085774508538738e-06,
      "loss": 1.4144,
      "step": 7033
    },
    {
      "epoch": 2.7231900890437477,
      "grad_norm": 21.871938705444336,
      "learning_rate": 8.085344345506947e-06,
      "loss": 1.0238,
      "step": 7034
    },
    {
      "epoch": 2.7235772357723578,
      "grad_norm": 13.487154006958008,
      "learning_rate": 8.084914182475159e-06,
      "loss": 0.7277,
      "step": 7035
    },
    {
      "epoch": 2.723964382500968,
      "grad_norm": 20.224977493286133,
      "learning_rate": 8.08448401944337e-06,
      "loss": 1.5712,
      "step": 7036
    },
    {
      "epoch": 2.724351529229578,
      "grad_norm": 29.110342025756836,
      "learning_rate": 8.084053856411581e-06,
      "loss": 0.6008,
      "step": 7037
    },
    {
      "epoch": 2.7247386759581884,
      "grad_norm": 14.053235054016113,
      "learning_rate": 8.083623693379791e-06,
      "loss": 0.7156,
      "step": 7038
    },
    {
      "epoch": 2.725125822686798,
      "grad_norm": 12.81679630279541,
      "learning_rate": 8.083193530348003e-06,
      "loss": 0.7405,
      "step": 7039
    },
    {
      "epoch": 2.7255129694154085,
      "grad_norm": 35.30990982055664,
      "learning_rate": 8.082763367316212e-06,
      "loss": 2.0735,
      "step": 7040
    },
    {
      "epoch": 2.7259001161440186,
      "grad_norm": 22.00933265686035,
      "learning_rate": 8.082333204284425e-06,
      "loss": 0.6907,
      "step": 7041
    },
    {
      "epoch": 2.7262872628726287,
      "grad_norm": 21.890897750854492,
      "learning_rate": 8.081903041252635e-06,
      "loss": 1.0723,
      "step": 7042
    },
    {
      "epoch": 2.7266744096012387,
      "grad_norm": 37.540184020996094,
      "learning_rate": 8.081472878220847e-06,
      "loss": 1.486,
      "step": 7043
    },
    {
      "epoch": 2.727061556329849,
      "grad_norm": 26.49849510192871,
      "learning_rate": 8.081042715189056e-06,
      "loss": 1.67,
      "step": 7044
    },
    {
      "epoch": 2.7274487030584593,
      "grad_norm": 23.866226196289062,
      "learning_rate": 8.08061255215727e-06,
      "loss": 2.5621,
      "step": 7045
    },
    {
      "epoch": 2.7278358497870694,
      "grad_norm": 28.182321548461914,
      "learning_rate": 8.08018238912548e-06,
      "loss": 1.427,
      "step": 7046
    },
    {
      "epoch": 2.7282229965156795,
      "grad_norm": 17.635828018188477,
      "learning_rate": 8.07975222609369e-06,
      "loss": 1.7119,
      "step": 7047
    },
    {
      "epoch": 2.7286101432442895,
      "grad_norm": 16.83719253540039,
      "learning_rate": 8.0793220630619e-06,
      "loss": 1.5289,
      "step": 7048
    },
    {
      "epoch": 2.7289972899728996,
      "grad_norm": 17.31296157836914,
      "learning_rate": 8.078891900030112e-06,
      "loss": 1.5062,
      "step": 7049
    },
    {
      "epoch": 2.72938443670151,
      "grad_norm": 18.88442039489746,
      "learning_rate": 8.078461736998323e-06,
      "loss": 1.5459,
      "step": 7050
    },
    {
      "epoch": 2.72977158343012,
      "grad_norm": 19.38784408569336,
      "learning_rate": 8.078031573966535e-06,
      "loss": 0.6251,
      "step": 7051
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 12.625946998596191,
      "learning_rate": 8.077601410934744e-06,
      "loss": 0.8099,
      "step": 7052
    },
    {
      "epoch": 2.7305458768873403,
      "grad_norm": 23.7990665435791,
      "learning_rate": 8.077171247902956e-06,
      "loss": 2.4635,
      "step": 7053
    },
    {
      "epoch": 2.7309330236159504,
      "grad_norm": 16.5762996673584,
      "learning_rate": 8.076741084871167e-06,
      "loss": 1.1481,
      "step": 7054
    },
    {
      "epoch": 2.7313201703445604,
      "grad_norm": 13.760950088500977,
      "learning_rate": 8.076310921839377e-06,
      "loss": 0.8119,
      "step": 7055
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 34.06745529174805,
      "learning_rate": 8.075880758807588e-06,
      "loss": 1.6232,
      "step": 7056
    },
    {
      "epoch": 2.732094463801781,
      "grad_norm": 22.69525718688965,
      "learning_rate": 8.0754505957758e-06,
      "loss": 1.9882,
      "step": 7057
    },
    {
      "epoch": 2.732481610530391,
      "grad_norm": 22.570791244506836,
      "learning_rate": 8.075020432744011e-06,
      "loss": 1.5838,
      "step": 7058
    },
    {
      "epoch": 2.732868757259001,
      "grad_norm": 14.956437110900879,
      "learning_rate": 8.07459026971222e-06,
      "loss": 1.0884,
      "step": 7059
    },
    {
      "epoch": 2.733255903987611,
      "grad_norm": 10.855314254760742,
      "learning_rate": 8.074160106680432e-06,
      "loss": 1.0693,
      "step": 7060
    },
    {
      "epoch": 2.7336430507162213,
      "grad_norm": 33.23270797729492,
      "learning_rate": 8.073729943648644e-06,
      "loss": 0.9933,
      "step": 7061
    },
    {
      "epoch": 2.7340301974448318,
      "grad_norm": 12.767133712768555,
      "learning_rate": 8.073299780616855e-06,
      "loss": 0.7321,
      "step": 7062
    },
    {
      "epoch": 2.734417344173442,
      "grad_norm": 16.639972686767578,
      "learning_rate": 8.072869617585065e-06,
      "loss": 1.0405,
      "step": 7063
    },
    {
      "epoch": 2.734804490902052,
      "grad_norm": 21.27292251586914,
      "learning_rate": 8.072439454553276e-06,
      "loss": 0.7078,
      "step": 7064
    },
    {
      "epoch": 2.735191637630662,
      "grad_norm": 15.400503158569336,
      "learning_rate": 8.072009291521488e-06,
      "loss": 0.551,
      "step": 7065
    },
    {
      "epoch": 2.735578784359272,
      "grad_norm": 16.40639877319336,
      "learning_rate": 8.071579128489699e-06,
      "loss": 1.1349,
      "step": 7066
    },
    {
      "epoch": 2.7359659310878826,
      "grad_norm": 30.84383773803711,
      "learning_rate": 8.071148965457909e-06,
      "loss": 1.1122,
      "step": 7067
    },
    {
      "epoch": 2.736353077816492,
      "grad_norm": 12.992862701416016,
      "learning_rate": 8.07071880242612e-06,
      "loss": 1.0267,
      "step": 7068
    },
    {
      "epoch": 2.7367402245451027,
      "grad_norm": 48.6314582824707,
      "learning_rate": 8.070288639394332e-06,
      "loss": 1.7679,
      "step": 7069
    },
    {
      "epoch": 2.7371273712737128,
      "grad_norm": 12.006497383117676,
      "learning_rate": 8.069858476362541e-06,
      "loss": 0.9274,
      "step": 7070
    },
    {
      "epoch": 2.737514518002323,
      "grad_norm": 14.069965362548828,
      "learning_rate": 8.069428313330753e-06,
      "loss": 1.0089,
      "step": 7071
    },
    {
      "epoch": 2.737901664730933,
      "grad_norm": 24.451231002807617,
      "learning_rate": 8.068998150298964e-06,
      "loss": 1.4643,
      "step": 7072
    },
    {
      "epoch": 2.738288811459543,
      "grad_norm": 26.04349136352539,
      "learning_rate": 8.068567987267176e-06,
      "loss": 1.6018,
      "step": 7073
    },
    {
      "epoch": 2.7386759581881535,
      "grad_norm": 28.694772720336914,
      "learning_rate": 8.068137824235385e-06,
      "loss": 2.2164,
      "step": 7074
    },
    {
      "epoch": 2.7390631049167635,
      "grad_norm": 12.493948936462402,
      "learning_rate": 8.067707661203597e-06,
      "loss": 0.7883,
      "step": 7075
    },
    {
      "epoch": 2.7394502516453736,
      "grad_norm": 21.89902114868164,
      "learning_rate": 8.067277498171808e-06,
      "loss": 1.5411,
      "step": 7076
    },
    {
      "epoch": 2.7398373983739837,
      "grad_norm": 26.68691635131836,
      "learning_rate": 8.06684733514002e-06,
      "loss": 2.237,
      "step": 7077
    },
    {
      "epoch": 2.7402245451025937,
      "grad_norm": 16.910701751708984,
      "learning_rate": 8.06641717210823e-06,
      "loss": 0.9293,
      "step": 7078
    },
    {
      "epoch": 2.7406116918312042,
      "grad_norm": 12.327709197998047,
      "learning_rate": 8.06598700907644e-06,
      "loss": 0.9557,
      "step": 7079
    },
    {
      "epoch": 2.7409988385598143,
      "grad_norm": 50.73070526123047,
      "learning_rate": 8.065556846044652e-06,
      "loss": 2.6174,
      "step": 7080
    },
    {
      "epoch": 2.7413859852884244,
      "grad_norm": 21.639705657958984,
      "learning_rate": 8.065126683012863e-06,
      "loss": 1.9422,
      "step": 7081
    },
    {
      "epoch": 2.7417731320170344,
      "grad_norm": 21.901521682739258,
      "learning_rate": 8.064696519981073e-06,
      "loss": 1.9327,
      "step": 7082
    },
    {
      "epoch": 2.7421602787456445,
      "grad_norm": 32.9222412109375,
      "learning_rate": 8.064266356949285e-06,
      "loss": 1.4713,
      "step": 7083
    },
    {
      "epoch": 2.742547425474255,
      "grad_norm": 16.314945220947266,
      "learning_rate": 8.063836193917496e-06,
      "loss": 1.3837,
      "step": 7084
    },
    {
      "epoch": 2.7429345722028646,
      "grad_norm": 18.285863876342773,
      "learning_rate": 8.063406030885706e-06,
      "loss": 1.1522,
      "step": 7085
    },
    {
      "epoch": 2.743321718931475,
      "grad_norm": 10.812846183776855,
      "learning_rate": 8.062975867853917e-06,
      "loss": 0.5116,
      "step": 7086
    },
    {
      "epoch": 2.743708865660085,
      "grad_norm": 12.659780502319336,
      "learning_rate": 8.062545704822129e-06,
      "loss": 1.335,
      "step": 7087
    },
    {
      "epoch": 2.7440960123886953,
      "grad_norm": 15.0938720703125,
      "learning_rate": 8.06211554179034e-06,
      "loss": 1.2911,
      "step": 7088
    },
    {
      "epoch": 2.7444831591173053,
      "grad_norm": 25.296175003051758,
      "learning_rate": 8.06168537875855e-06,
      "loss": 1.3714,
      "step": 7089
    },
    {
      "epoch": 2.7448703058459154,
      "grad_norm": 14.813282012939453,
      "learning_rate": 8.061255215726761e-06,
      "loss": 1.1937,
      "step": 7090
    },
    {
      "epoch": 2.745257452574526,
      "grad_norm": 14.404796600341797,
      "learning_rate": 8.06082505269497e-06,
      "loss": 0.9406,
      "step": 7091
    },
    {
      "epoch": 2.745644599303136,
      "grad_norm": 14.258151054382324,
      "learning_rate": 8.060394889663184e-06,
      "loss": 1.0262,
      "step": 7092
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 47.13587951660156,
      "learning_rate": 8.059964726631394e-06,
      "loss": 3.6544,
      "step": 7093
    },
    {
      "epoch": 2.746418892760356,
      "grad_norm": 48.927207946777344,
      "learning_rate": 8.059534563599605e-06,
      "loss": 0.711,
      "step": 7094
    },
    {
      "epoch": 2.746806039488966,
      "grad_norm": 16.06431770324707,
      "learning_rate": 8.059104400567815e-06,
      "loss": 0.6095,
      "step": 7095
    },
    {
      "epoch": 2.7471931862175767,
      "grad_norm": 24.337495803833008,
      "learning_rate": 8.058674237536028e-06,
      "loss": 0.8022,
      "step": 7096
    },
    {
      "epoch": 2.7475803329461868,
      "grad_norm": 21.448123931884766,
      "learning_rate": 8.058244074504238e-06,
      "loss": 0.8707,
      "step": 7097
    },
    {
      "epoch": 2.747967479674797,
      "grad_norm": 38.28279495239258,
      "learning_rate": 8.057813911472449e-06,
      "loss": 2.1931,
      "step": 7098
    },
    {
      "epoch": 2.748354626403407,
      "grad_norm": 14.036734580993652,
      "learning_rate": 8.057383748440659e-06,
      "loss": 0.9747,
      "step": 7099
    },
    {
      "epoch": 2.748741773132017,
      "grad_norm": 5.117849349975586,
      "learning_rate": 8.05695358540887e-06,
      "loss": 0.1603,
      "step": 7100
    },
    {
      "epoch": 2.749128919860627,
      "grad_norm": 16.053030014038086,
      "learning_rate": 8.056523422377082e-06,
      "loss": 1.0706,
      "step": 7101
    },
    {
      "epoch": 2.749516066589237,
      "grad_norm": 30.612056732177734,
      "learning_rate": 8.056093259345293e-06,
      "loss": 2.1657,
      "step": 7102
    },
    {
      "epoch": 2.7499032133178476,
      "grad_norm": 22.44410514831543,
      "learning_rate": 8.055663096313503e-06,
      "loss": 1.2387,
      "step": 7103
    },
    {
      "epoch": 2.7502903600464577,
      "grad_norm": 12.544965744018555,
      "learning_rate": 8.055232933281714e-06,
      "loss": 0.7121,
      "step": 7104
    },
    {
      "epoch": 2.7506775067750677,
      "grad_norm": 36.130104064941406,
      "learning_rate": 8.054802770249926e-06,
      "loss": 2.7807,
      "step": 7105
    },
    {
      "epoch": 2.751064653503678,
      "grad_norm": 26.609365463256836,
      "learning_rate": 8.054372607218135e-06,
      "loss": 2.7592,
      "step": 7106
    },
    {
      "epoch": 2.751451800232288,
      "grad_norm": 13.738825798034668,
      "learning_rate": 8.053942444186347e-06,
      "loss": 0.9317,
      "step": 7107
    },
    {
      "epoch": 2.7518389469608984,
      "grad_norm": 16.581331253051758,
      "learning_rate": 8.053512281154558e-06,
      "loss": 1.5146,
      "step": 7108
    },
    {
      "epoch": 2.7522260936895084,
      "grad_norm": 58.10212326049805,
      "learning_rate": 8.05308211812277e-06,
      "loss": 1.881,
      "step": 7109
    },
    {
      "epoch": 2.7526132404181185,
      "grad_norm": 29.435550689697266,
      "learning_rate": 8.05265195509098e-06,
      "loss": 2.2473,
      "step": 7110
    },
    {
      "epoch": 2.7530003871467286,
      "grad_norm": 9.462213516235352,
      "learning_rate": 8.052221792059192e-06,
      "loss": 0.476,
      "step": 7111
    },
    {
      "epoch": 2.7533875338753386,
      "grad_norm": 12.546217918395996,
      "learning_rate": 8.051791629027402e-06,
      "loss": 1.3498,
      "step": 7112
    },
    {
      "epoch": 2.753774680603949,
      "grad_norm": 13.134109497070312,
      "learning_rate": 8.051361465995614e-06,
      "loss": 1.0146,
      "step": 7113
    },
    {
      "epoch": 2.7541618273325588,
      "grad_norm": 25.903669357299805,
      "learning_rate": 8.050931302963823e-06,
      "loss": 1.1616,
      "step": 7114
    },
    {
      "epoch": 2.7545489740611693,
      "grad_norm": 11.690214157104492,
      "learning_rate": 8.050501139932035e-06,
      "loss": 1.4387,
      "step": 7115
    },
    {
      "epoch": 2.7549361207897793,
      "grad_norm": 42.81486511230469,
      "learning_rate": 8.050070976900246e-06,
      "loss": 1.6976,
      "step": 7116
    },
    {
      "epoch": 2.7553232675183894,
      "grad_norm": 17.701738357543945,
      "learning_rate": 8.049640813868457e-06,
      "loss": 0.7821,
      "step": 7117
    },
    {
      "epoch": 2.7557104142469995,
      "grad_norm": 13.259284973144531,
      "learning_rate": 8.049210650836667e-06,
      "loss": 1.4018,
      "step": 7118
    },
    {
      "epoch": 2.7560975609756095,
      "grad_norm": 26.890241622924805,
      "learning_rate": 8.048780487804879e-06,
      "loss": 2.8417,
      "step": 7119
    },
    {
      "epoch": 2.75648470770422,
      "grad_norm": 34.374603271484375,
      "learning_rate": 8.04835032477309e-06,
      "loss": 1.3754,
      "step": 7120
    },
    {
      "epoch": 2.75687185443283,
      "grad_norm": 9.199748992919922,
      "learning_rate": 8.0479201617413e-06,
      "loss": 0.4841,
      "step": 7121
    },
    {
      "epoch": 2.75725900116144,
      "grad_norm": 26.843158721923828,
      "learning_rate": 8.047489998709511e-06,
      "loss": 1.6736,
      "step": 7122
    },
    {
      "epoch": 2.7576461478900502,
      "grad_norm": 14.326163291931152,
      "learning_rate": 8.047059835677723e-06,
      "loss": 0.8508,
      "step": 7123
    },
    {
      "epoch": 2.7580332946186603,
      "grad_norm": 34.606101989746094,
      "learning_rate": 8.046629672645934e-06,
      "loss": 1.7504,
      "step": 7124
    },
    {
      "epoch": 2.758420441347271,
      "grad_norm": 16.73243522644043,
      "learning_rate": 8.046199509614144e-06,
      "loss": 1.2913,
      "step": 7125
    },
    {
      "epoch": 2.758807588075881,
      "grad_norm": 28.691635131835938,
      "learning_rate": 8.045769346582355e-06,
      "loss": 1.5438,
      "step": 7126
    },
    {
      "epoch": 2.759194734804491,
      "grad_norm": 21.34273910522461,
      "learning_rate": 8.045339183550567e-06,
      "loss": 1.2247,
      "step": 7127
    },
    {
      "epoch": 2.759581881533101,
      "grad_norm": 56.78982162475586,
      "learning_rate": 8.044909020518778e-06,
      "loss": 2.1773,
      "step": 7128
    },
    {
      "epoch": 2.759969028261711,
      "grad_norm": 18.980478286743164,
      "learning_rate": 8.044478857486988e-06,
      "loss": 1.0483,
      "step": 7129
    },
    {
      "epoch": 2.7603561749903216,
      "grad_norm": 20.368921279907227,
      "learning_rate": 8.044048694455199e-06,
      "loss": 1.4411,
      "step": 7130
    },
    {
      "epoch": 2.760743321718931,
      "grad_norm": 23.70928192138672,
      "learning_rate": 8.04361853142341e-06,
      "loss": 1.1116,
      "step": 7131
    },
    {
      "epoch": 2.7611304684475417,
      "grad_norm": 12.605256080627441,
      "learning_rate": 8.043188368391622e-06,
      "loss": 1.0048,
      "step": 7132
    },
    {
      "epoch": 2.761517615176152,
      "grad_norm": 11.36899185180664,
      "learning_rate": 8.042758205359832e-06,
      "loss": 0.7137,
      "step": 7133
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 36.6869010925293,
      "learning_rate": 8.042328042328043e-06,
      "loss": 1.4236,
      "step": 7134
    },
    {
      "epoch": 2.762291908633372,
      "grad_norm": 25.339696884155273,
      "learning_rate": 8.041897879296255e-06,
      "loss": 1.5018,
      "step": 7135
    },
    {
      "epoch": 2.762679055361982,
      "grad_norm": 23.239112854003906,
      "learning_rate": 8.041467716264464e-06,
      "loss": 1.5348,
      "step": 7136
    },
    {
      "epoch": 2.7630662020905925,
      "grad_norm": 6.089605808258057,
      "learning_rate": 8.041037553232676e-06,
      "loss": 0.174,
      "step": 7137
    },
    {
      "epoch": 2.7634533488192026,
      "grad_norm": 10.33807373046875,
      "learning_rate": 8.040607390200887e-06,
      "loss": 0.4913,
      "step": 7138
    },
    {
      "epoch": 2.7638404955478126,
      "grad_norm": 15.254080772399902,
      "learning_rate": 8.040177227169098e-06,
      "loss": 1.3126,
      "step": 7139
    },
    {
      "epoch": 2.7642276422764227,
      "grad_norm": 17.12805938720703,
      "learning_rate": 8.039747064137308e-06,
      "loss": 1.4796,
      "step": 7140
    },
    {
      "epoch": 2.7646147890050328,
      "grad_norm": 15.019600868225098,
      "learning_rate": 8.03931690110552e-06,
      "loss": 0.8264,
      "step": 7141
    },
    {
      "epoch": 2.7650019357336433,
      "grad_norm": 14.92647933959961,
      "learning_rate": 8.03888673807373e-06,
      "loss": 0.8558,
      "step": 7142
    },
    {
      "epoch": 2.7653890824622533,
      "grad_norm": 12.205437660217285,
      "learning_rate": 8.038456575041942e-06,
      "loss": 0.5404,
      "step": 7143
    },
    {
      "epoch": 2.7657762291908634,
      "grad_norm": 24.69274139404297,
      "learning_rate": 8.038026412010152e-06,
      "loss": 1.9923,
      "step": 7144
    },
    {
      "epoch": 2.7661633759194735,
      "grad_norm": 20.33807945251465,
      "learning_rate": 8.037596248978364e-06,
      "loss": 1.1836,
      "step": 7145
    },
    {
      "epoch": 2.7665505226480835,
      "grad_norm": 20.222909927368164,
      "learning_rate": 8.037166085946573e-06,
      "loss": 1.8777,
      "step": 7146
    },
    {
      "epoch": 2.7669376693766936,
      "grad_norm": 16.306547164916992,
      "learning_rate": 8.036735922914786e-06,
      "loss": 1.2231,
      "step": 7147
    },
    {
      "epoch": 2.7673248161053037,
      "grad_norm": 35.098697662353516,
      "learning_rate": 8.036305759882996e-06,
      "loss": 1.6388,
      "step": 7148
    },
    {
      "epoch": 2.767711962833914,
      "grad_norm": 63.56222915649414,
      "learning_rate": 8.035875596851208e-06,
      "loss": 2.3004,
      "step": 7149
    },
    {
      "epoch": 2.7680991095625243,
      "grad_norm": 13.597322463989258,
      "learning_rate": 8.035445433819417e-06,
      "loss": 0.7704,
      "step": 7150
    },
    {
      "epoch": 2.7684862562911343,
      "grad_norm": 12.321859359741211,
      "learning_rate": 8.035015270787629e-06,
      "loss": 1.3157,
      "step": 7151
    },
    {
      "epoch": 2.7688734030197444,
      "grad_norm": 16.353179931640625,
      "learning_rate": 8.03458510775584e-06,
      "loss": 0.9214,
      "step": 7152
    },
    {
      "epoch": 2.7692605497483544,
      "grad_norm": 21.717071533203125,
      "learning_rate": 8.034154944724052e-06,
      "loss": 1.6251,
      "step": 7153
    },
    {
      "epoch": 2.769647696476965,
      "grad_norm": 22.45531463623047,
      "learning_rate": 8.033724781692263e-06,
      "loss": 1.5271,
      "step": 7154
    },
    {
      "epoch": 2.770034843205575,
      "grad_norm": 33.10977554321289,
      "learning_rate": 8.033294618660473e-06,
      "loss": 1.8679,
      "step": 7155
    },
    {
      "epoch": 2.770421989934185,
      "grad_norm": 13.36409854888916,
      "learning_rate": 8.032864455628684e-06,
      "loss": 0.9863,
      "step": 7156
    },
    {
      "epoch": 2.770809136662795,
      "grad_norm": 18.305419921875,
      "learning_rate": 8.032434292596894e-06,
      "loss": 0.6173,
      "step": 7157
    },
    {
      "epoch": 2.7711962833914052,
      "grad_norm": 19.277990341186523,
      "learning_rate": 8.032004129565107e-06,
      "loss": 1.7989,
      "step": 7158
    },
    {
      "epoch": 2.7715834301200157,
      "grad_norm": 17.688213348388672,
      "learning_rate": 8.031573966533317e-06,
      "loss": 1.6851,
      "step": 7159
    },
    {
      "epoch": 2.7719705768486254,
      "grad_norm": 26.46255874633789,
      "learning_rate": 8.031143803501528e-06,
      "loss": 1.2884,
      "step": 7160
    },
    {
      "epoch": 2.772357723577236,
      "grad_norm": 16.44368553161621,
      "learning_rate": 8.030713640469738e-06,
      "loss": 1.3951,
      "step": 7161
    },
    {
      "epoch": 2.772744870305846,
      "grad_norm": 15.590792655944824,
      "learning_rate": 8.030283477437951e-06,
      "loss": 1.4605,
      "step": 7162
    },
    {
      "epoch": 2.773132017034456,
      "grad_norm": 19.449783325195312,
      "learning_rate": 8.02985331440616e-06,
      "loss": 0.8191,
      "step": 7163
    },
    {
      "epoch": 2.773519163763066,
      "grad_norm": 17.262001037597656,
      "learning_rate": 8.029423151374372e-06,
      "loss": 0.8499,
      "step": 7164
    },
    {
      "epoch": 2.773906310491676,
      "grad_norm": 14.768202781677246,
      "learning_rate": 8.028992988342582e-06,
      "loss": 0.8758,
      "step": 7165
    },
    {
      "epoch": 2.7742934572202866,
      "grad_norm": 13.061513900756836,
      "learning_rate": 8.028562825310793e-06,
      "loss": 0.7279,
      "step": 7166
    },
    {
      "epoch": 2.7746806039488967,
      "grad_norm": 18.90823745727539,
      "learning_rate": 8.028132662279005e-06,
      "loss": 0.9072,
      "step": 7167
    },
    {
      "epoch": 2.7750677506775068,
      "grad_norm": 25.349576950073242,
      "learning_rate": 8.027702499247216e-06,
      "loss": 1.2478,
      "step": 7168
    },
    {
      "epoch": 2.775454897406117,
      "grad_norm": 33.47298812866211,
      "learning_rate": 8.027272336215426e-06,
      "loss": 1.8293,
      "step": 7169
    },
    {
      "epoch": 2.775842044134727,
      "grad_norm": 25.9862060546875,
      "learning_rate": 8.026842173183637e-06,
      "loss": 1.9124,
      "step": 7170
    },
    {
      "epoch": 2.7762291908633374,
      "grad_norm": 24.49104881286621,
      "learning_rate": 8.026412010151849e-06,
      "loss": 1.3394,
      "step": 7171
    },
    {
      "epoch": 2.7766163375919475,
      "grad_norm": 20.582948684692383,
      "learning_rate": 8.025981847120058e-06,
      "loss": 1.4466,
      "step": 7172
    },
    {
      "epoch": 2.7770034843205575,
      "grad_norm": 29.285728454589844,
      "learning_rate": 8.02555168408827e-06,
      "loss": 1.806,
      "step": 7173
    },
    {
      "epoch": 2.7773906310491676,
      "grad_norm": 15.187931060791016,
      "learning_rate": 8.025121521056481e-06,
      "loss": 1.4365,
      "step": 7174
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 11.081944465637207,
      "learning_rate": 8.024691358024692e-06,
      "loss": 0.9475,
      "step": 7175
    },
    {
      "epoch": 2.778164924506388,
      "grad_norm": 12.630402565002441,
      "learning_rate": 8.024261194992902e-06,
      "loss": 1.2564,
      "step": 7176
    },
    {
      "epoch": 2.778552071234998,
      "grad_norm": 28.152185440063477,
      "learning_rate": 8.023831031961114e-06,
      "loss": 1.4673,
      "step": 7177
    },
    {
      "epoch": 2.7789392179636083,
      "grad_norm": 33.72768783569336,
      "learning_rate": 8.023400868929325e-06,
      "loss": 1.5926,
      "step": 7178
    },
    {
      "epoch": 2.7793263646922184,
      "grad_norm": 14.592655181884766,
      "learning_rate": 8.022970705897536e-06,
      "loss": 1.5317,
      "step": 7179
    },
    {
      "epoch": 2.7797135114208285,
      "grad_norm": 31.27548599243164,
      "learning_rate": 8.022540542865746e-06,
      "loss": 1.9829,
      "step": 7180
    },
    {
      "epoch": 2.7801006581494385,
      "grad_norm": 14.678866386413574,
      "learning_rate": 8.022110379833958e-06,
      "loss": 0.5816,
      "step": 7181
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 38.03860092163086,
      "learning_rate": 8.021680216802169e-06,
      "loss": 1.4832,
      "step": 7182
    },
    {
      "epoch": 2.780874951606659,
      "grad_norm": 49.43368148803711,
      "learning_rate": 8.02125005377038e-06,
      "loss": 3.0106,
      "step": 7183
    },
    {
      "epoch": 2.781262098335269,
      "grad_norm": 23.218029022216797,
      "learning_rate": 8.02081989073859e-06,
      "loss": 2.082,
      "step": 7184
    },
    {
      "epoch": 2.7816492450638792,
      "grad_norm": 17.508729934692383,
      "learning_rate": 8.020389727706802e-06,
      "loss": 0.9358,
      "step": 7185
    },
    {
      "epoch": 2.7820363917924893,
      "grad_norm": 25.326658248901367,
      "learning_rate": 8.019959564675013e-06,
      "loss": 2.1277,
      "step": 7186
    },
    {
      "epoch": 2.7824235385210994,
      "grad_norm": 63.521324157714844,
      "learning_rate": 8.019529401643223e-06,
      "loss": 1.442,
      "step": 7187
    },
    {
      "epoch": 2.78281068524971,
      "grad_norm": 12.094059944152832,
      "learning_rate": 8.019099238611434e-06,
      "loss": 0.7327,
      "step": 7188
    },
    {
      "epoch": 2.78319783197832,
      "grad_norm": 15.002274513244629,
      "learning_rate": 8.018669075579646e-06,
      "loss": 0.856,
      "step": 7189
    },
    {
      "epoch": 2.78358497870693,
      "grad_norm": 17.28476333618164,
      "learning_rate": 8.018238912547857e-06,
      "loss": 1.0244,
      "step": 7190
    },
    {
      "epoch": 2.78397212543554,
      "grad_norm": 42.87890625,
      "learning_rate": 8.017808749516067e-06,
      "loss": 1.1388,
      "step": 7191
    },
    {
      "epoch": 2.78435927216415,
      "grad_norm": 21.22772979736328,
      "learning_rate": 8.017378586484278e-06,
      "loss": 1.5031,
      "step": 7192
    },
    {
      "epoch": 2.78474641889276,
      "grad_norm": 27.626585006713867,
      "learning_rate": 8.01694842345249e-06,
      "loss": 1.3419,
      "step": 7193
    },
    {
      "epoch": 2.7851335656213703,
      "grad_norm": 19.79791831970215,
      "learning_rate": 8.016518260420701e-06,
      "loss": 1.8624,
      "step": 7194
    },
    {
      "epoch": 2.7855207123499808,
      "grad_norm": 30.731721878051758,
      "learning_rate": 8.01608809738891e-06,
      "loss": 1.91,
      "step": 7195
    },
    {
      "epoch": 2.785907859078591,
      "grad_norm": 15.063294410705566,
      "learning_rate": 8.015657934357122e-06,
      "loss": 1.3759,
      "step": 7196
    },
    {
      "epoch": 2.786295005807201,
      "grad_norm": 11.706151008605957,
      "learning_rate": 8.015227771325333e-06,
      "loss": 0.7343,
      "step": 7197
    },
    {
      "epoch": 2.786682152535811,
      "grad_norm": 65.98474884033203,
      "learning_rate": 8.014797608293545e-06,
      "loss": 2.6619,
      "step": 7198
    },
    {
      "epoch": 2.787069299264421,
      "grad_norm": 23.248340606689453,
      "learning_rate": 8.014367445261755e-06,
      "loss": 1.0453,
      "step": 7199
    },
    {
      "epoch": 2.7874564459930316,
      "grad_norm": 18.438508987426758,
      "learning_rate": 8.013937282229966e-06,
      "loss": 1.1633,
      "step": 7200
    },
    {
      "epoch": 2.7878435927216416,
      "grad_norm": 14.132196426391602,
      "learning_rate": 8.013507119198177e-06,
      "loss": 0.8454,
      "step": 7201
    },
    {
      "epoch": 2.7882307394502517,
      "grad_norm": 32.141761779785156,
      "learning_rate": 8.013076956166387e-06,
      "loss": 1.6137,
      "step": 7202
    },
    {
      "epoch": 2.7886178861788617,
      "grad_norm": 8.927716255187988,
      "learning_rate": 8.012646793134599e-06,
      "loss": 1.159,
      "step": 7203
    },
    {
      "epoch": 2.789005032907472,
      "grad_norm": 11.59013843536377,
      "learning_rate": 8.01221663010281e-06,
      "loss": 0.7692,
      "step": 7204
    },
    {
      "epoch": 2.7893921796360823,
      "grad_norm": 38.82855224609375,
      "learning_rate": 8.011786467071021e-06,
      "loss": 1.0909,
      "step": 7205
    },
    {
      "epoch": 2.789779326364692,
      "grad_norm": 19.517528533935547,
      "learning_rate": 8.011356304039231e-06,
      "loss": 0.9452,
      "step": 7206
    },
    {
      "epoch": 2.7901664730933025,
      "grad_norm": 22.725194931030273,
      "learning_rate": 8.010926141007443e-06,
      "loss": 0.6341,
      "step": 7207
    },
    {
      "epoch": 2.7905536198219125,
      "grad_norm": 29.739097595214844,
      "learning_rate": 8.010495977975652e-06,
      "loss": 1.3639,
      "step": 7208
    },
    {
      "epoch": 2.7909407665505226,
      "grad_norm": 17.68157386779785,
      "learning_rate": 8.010065814943865e-06,
      "loss": 1.1348,
      "step": 7209
    },
    {
      "epoch": 2.7913279132791327,
      "grad_norm": 33.010108947753906,
      "learning_rate": 8.009635651912075e-06,
      "loss": 1.3217,
      "step": 7210
    },
    {
      "epoch": 2.7917150600077427,
      "grad_norm": 11.510595321655273,
      "learning_rate": 8.009205488880287e-06,
      "loss": 0.4863,
      "step": 7211
    },
    {
      "epoch": 2.7921022067363532,
      "grad_norm": 15.064359664916992,
      "learning_rate": 8.008775325848496e-06,
      "loss": 0.7013,
      "step": 7212
    },
    {
      "epoch": 2.7924893534649633,
      "grad_norm": 26.140178680419922,
      "learning_rate": 8.00834516281671e-06,
      "loss": 1.4659,
      "step": 7213
    },
    {
      "epoch": 2.7928765001935734,
      "grad_norm": 18.414260864257812,
      "learning_rate": 8.007914999784919e-06,
      "loss": 2.8706,
      "step": 7214
    },
    {
      "epoch": 2.7932636469221834,
      "grad_norm": 22.81218719482422,
      "learning_rate": 8.00748483675313e-06,
      "loss": 1.6616,
      "step": 7215
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 15.013833045959473,
      "learning_rate": 8.00705467372134e-06,
      "loss": 1.3505,
      "step": 7216
    },
    {
      "epoch": 2.794037940379404,
      "grad_norm": 33.568241119384766,
      "learning_rate": 8.006624510689552e-06,
      "loss": 2.4556,
      "step": 7217
    },
    {
      "epoch": 2.794425087108014,
      "grad_norm": 15.504402160644531,
      "learning_rate": 8.006194347657763e-06,
      "loss": 1.1132,
      "step": 7218
    },
    {
      "epoch": 2.794812233836624,
      "grad_norm": 23.488828659057617,
      "learning_rate": 8.005764184625974e-06,
      "loss": 1.8503,
      "step": 7219
    },
    {
      "epoch": 2.795199380565234,
      "grad_norm": 26.648950576782227,
      "learning_rate": 8.005334021594184e-06,
      "loss": 2.0754,
      "step": 7220
    },
    {
      "epoch": 2.7955865272938443,
      "grad_norm": 14.075849533081055,
      "learning_rate": 8.004903858562396e-06,
      "loss": 0.822,
      "step": 7221
    },
    {
      "epoch": 2.795973674022455,
      "grad_norm": 12.697378158569336,
      "learning_rate": 8.004473695530607e-06,
      "loss": 0.9958,
      "step": 7222
    },
    {
      "epoch": 2.7963608207510644,
      "grad_norm": 14.436689376831055,
      "learning_rate": 8.004043532498817e-06,
      "loss": 1.288,
      "step": 7223
    },
    {
      "epoch": 2.796747967479675,
      "grad_norm": 21.41312599182129,
      "learning_rate": 8.003613369467028e-06,
      "loss": 1.5431,
      "step": 7224
    },
    {
      "epoch": 2.797135114208285,
      "grad_norm": 11.80262565612793,
      "learning_rate": 8.00318320643524e-06,
      "loss": 0.9826,
      "step": 7225
    },
    {
      "epoch": 2.797522260936895,
      "grad_norm": 13.525383949279785,
      "learning_rate": 8.002753043403451e-06,
      "loss": 0.8087,
      "step": 7226
    },
    {
      "epoch": 2.797909407665505,
      "grad_norm": 14.81161117553711,
      "learning_rate": 8.00232288037166e-06,
      "loss": 0.2155,
      "step": 7227
    },
    {
      "epoch": 2.798296554394115,
      "grad_norm": 19.789997100830078,
      "learning_rate": 8.001892717339872e-06,
      "loss": 1.0939,
      "step": 7228
    },
    {
      "epoch": 2.7986837011227257,
      "grad_norm": 40.92793273925781,
      "learning_rate": 8.001462554308084e-06,
      "loss": 1.0615,
      "step": 7229
    },
    {
      "epoch": 2.7990708478513358,
      "grad_norm": 7.3455376625061035,
      "learning_rate": 8.001032391276295e-06,
      "loss": 1.203,
      "step": 7230
    },
    {
      "epoch": 2.799457994579946,
      "grad_norm": 11.871594429016113,
      "learning_rate": 8.000602228244505e-06,
      "loss": 0.7215,
      "step": 7231
    },
    {
      "epoch": 2.799845141308556,
      "grad_norm": 13.90105152130127,
      "learning_rate": 8.000172065212716e-06,
      "loss": 0.7431,
      "step": 7232
    },
    {
      "epoch": 2.800232288037166,
      "grad_norm": 18.27199363708496,
      "learning_rate": 7.999741902180928e-06,
      "loss": 0.7634,
      "step": 7233
    },
    {
      "epoch": 2.8006194347657765,
      "grad_norm": 23.26997184753418,
      "learning_rate": 7.999311739149139e-06,
      "loss": 0.9187,
      "step": 7234
    },
    {
      "epoch": 2.8010065814943865,
      "grad_norm": 27.62308692932129,
      "learning_rate": 7.998881576117349e-06,
      "loss": 1.4652,
      "step": 7235
    },
    {
      "epoch": 2.8013937282229966,
      "grad_norm": 35.949867248535156,
      "learning_rate": 7.99845141308556e-06,
      "loss": 3.2867,
      "step": 7236
    },
    {
      "epoch": 2.8017808749516067,
      "grad_norm": 54.09250259399414,
      "learning_rate": 7.998021250053771e-06,
      "loss": 2.0937,
      "step": 7237
    },
    {
      "epoch": 2.8021680216802167,
      "grad_norm": 13.488533973693848,
      "learning_rate": 7.997591087021981e-06,
      "loss": 0.7748,
      "step": 7238
    },
    {
      "epoch": 2.802555168408827,
      "grad_norm": 30.609251022338867,
      "learning_rate": 7.997160923990193e-06,
      "loss": 2.0426,
      "step": 7239
    },
    {
      "epoch": 2.802942315137437,
      "grad_norm": 25.43604278564453,
      "learning_rate": 7.996730760958404e-06,
      "loss": 2.0942,
      "step": 7240
    },
    {
      "epoch": 2.8033294618660474,
      "grad_norm": 12.456792831420898,
      "learning_rate": 7.996300597926615e-06,
      "loss": 0.7317,
      "step": 7241
    },
    {
      "epoch": 2.8037166085946574,
      "grad_norm": 4.809971809387207,
      "learning_rate": 7.995870434894825e-06,
      "loss": 0.1457,
      "step": 7242
    },
    {
      "epoch": 2.8041037553232675,
      "grad_norm": 19.653186798095703,
      "learning_rate": 7.995440271863037e-06,
      "loss": 1.1161,
      "step": 7243
    },
    {
      "epoch": 2.8044909020518776,
      "grad_norm": 45.79845428466797,
      "learning_rate": 7.995010108831248e-06,
      "loss": 0.545,
      "step": 7244
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 18.34200668334961,
      "learning_rate": 7.99457994579946e-06,
      "loss": 1.2,
      "step": 7245
    },
    {
      "epoch": 2.805265195509098,
      "grad_norm": 16.640336990356445,
      "learning_rate": 7.994149782767669e-06,
      "loss": 1.0005,
      "step": 7246
    },
    {
      "epoch": 2.805652342237708,
      "grad_norm": 72.86869812011719,
      "learning_rate": 7.99371961973588e-06,
      "loss": 1.9046,
      "step": 7247
    },
    {
      "epoch": 2.8060394889663183,
      "grad_norm": 52.07019805908203,
      "learning_rate": 7.993289456704092e-06,
      "loss": 2.6172,
      "step": 7248
    },
    {
      "epoch": 2.8064266356949283,
      "grad_norm": 33.383399963378906,
      "learning_rate": 7.992859293672303e-06,
      "loss": 1.6598,
      "step": 7249
    },
    {
      "epoch": 2.8068137824235384,
      "grad_norm": 12.770208358764648,
      "learning_rate": 7.992429130640513e-06,
      "loss": 0.5425,
      "step": 7250
    },
    {
      "epoch": 2.807200929152149,
      "grad_norm": 23.538270950317383,
      "learning_rate": 7.991998967608725e-06,
      "loss": 0.5046,
      "step": 7251
    },
    {
      "epoch": 2.8075880758807585,
      "grad_norm": 13.078351020812988,
      "learning_rate": 7.991568804576936e-06,
      "loss": 0.8908,
      "step": 7252
    },
    {
      "epoch": 2.807975222609369,
      "grad_norm": 26.69074249267578,
      "learning_rate": 7.991138641545146e-06,
      "loss": 1.8102,
      "step": 7253
    },
    {
      "epoch": 2.808362369337979,
      "grad_norm": 28.34469985961914,
      "learning_rate": 7.990708478513357e-06,
      "loss": 2.8553,
      "step": 7254
    },
    {
      "epoch": 2.808749516066589,
      "grad_norm": 33.43528366088867,
      "learning_rate": 7.990278315481568e-06,
      "loss": 1.2697,
      "step": 7255
    },
    {
      "epoch": 2.8091366627951992,
      "grad_norm": 18.134544372558594,
      "learning_rate": 7.98984815244978e-06,
      "loss": 1.1915,
      "step": 7256
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 58.62785720825195,
      "learning_rate": 7.98941798941799e-06,
      "loss": 2.6464,
      "step": 7257
    },
    {
      "epoch": 2.80991095625242,
      "grad_norm": 42.17241668701172,
      "learning_rate": 7.988987826386201e-06,
      "loss": 2.3999,
      "step": 7258
    },
    {
      "epoch": 2.81029810298103,
      "grad_norm": 4.987955570220947,
      "learning_rate": 7.98855766335441e-06,
      "loss": 0.165,
      "step": 7259
    },
    {
      "epoch": 2.81068524970964,
      "grad_norm": 8.574647903442383,
      "learning_rate": 7.988127500322624e-06,
      "loss": 0.2426,
      "step": 7260
    },
    {
      "epoch": 2.81107239643825,
      "grad_norm": 29.910539627075195,
      "learning_rate": 7.987697337290834e-06,
      "loss": 0.8961,
      "step": 7261
    },
    {
      "epoch": 2.81145954316686,
      "grad_norm": 5.495007514953613,
      "learning_rate": 7.987267174259045e-06,
      "loss": 0.326,
      "step": 7262
    },
    {
      "epoch": 2.8118466898954706,
      "grad_norm": 22.82708168029785,
      "learning_rate": 7.986837011227255e-06,
      "loss": 0.8027,
      "step": 7263
    },
    {
      "epoch": 2.8122338366240807,
      "grad_norm": 15.276567459106445,
      "learning_rate": 7.986406848195468e-06,
      "loss": 1.0887,
      "step": 7264
    },
    {
      "epoch": 2.8126209833526907,
      "grad_norm": 31.29368782043457,
      "learning_rate": 7.985976685163678e-06,
      "loss": 1.7122,
      "step": 7265
    },
    {
      "epoch": 2.813008130081301,
      "grad_norm": 23.942689895629883,
      "learning_rate": 7.985546522131889e-06,
      "loss": 1.7479,
      "step": 7266
    },
    {
      "epoch": 2.813395276809911,
      "grad_norm": 30.341882705688477,
      "learning_rate": 7.985116359100099e-06,
      "loss": 0.9557,
      "step": 7267
    },
    {
      "epoch": 2.8137824235385214,
      "grad_norm": 23.846067428588867,
      "learning_rate": 7.98468619606831e-06,
      "loss": 1.0913,
      "step": 7268
    },
    {
      "epoch": 2.814169570267131,
      "grad_norm": 18.31607437133789,
      "learning_rate": 7.984256033036522e-06,
      "loss": 1.0976,
      "step": 7269
    },
    {
      "epoch": 2.8145567169957415,
      "grad_norm": 25.8137149810791,
      "learning_rate": 7.983825870004733e-06,
      "loss": 1.3217,
      "step": 7270
    },
    {
      "epoch": 2.8149438637243516,
      "grad_norm": 40.195613861083984,
      "learning_rate": 7.983395706972943e-06,
      "loss": 1.3901,
      "step": 7271
    },
    {
      "epoch": 2.8153310104529616,
      "grad_norm": 41.45471954345703,
      "learning_rate": 7.982965543941154e-06,
      "loss": 2.3439,
      "step": 7272
    },
    {
      "epoch": 2.8157181571815717,
      "grad_norm": 58.269775390625,
      "learning_rate": 7.982535380909365e-06,
      "loss": 2.0952,
      "step": 7273
    },
    {
      "epoch": 2.8161053039101818,
      "grad_norm": 20.16917610168457,
      "learning_rate": 7.982105217877575e-06,
      "loss": 1.0476,
      "step": 7274
    },
    {
      "epoch": 2.8164924506387923,
      "grad_norm": 21.882661819458008,
      "learning_rate": 7.981675054845788e-06,
      "loss": 1.6284,
      "step": 7275
    },
    {
      "epoch": 2.8168795973674023,
      "grad_norm": 13.810257911682129,
      "learning_rate": 7.981244891813998e-06,
      "loss": 0.8112,
      "step": 7276
    },
    {
      "epoch": 2.8172667440960124,
      "grad_norm": 26.084716796875,
      "learning_rate": 7.98081472878221e-06,
      "loss": 1.7496,
      "step": 7277
    },
    {
      "epoch": 2.8176538908246225,
      "grad_norm": 30.62770652770996,
      "learning_rate": 7.98038456575042e-06,
      "loss": 1.9541,
      "step": 7278
    },
    {
      "epoch": 2.8180410375532325,
      "grad_norm": 38.710350036621094,
      "learning_rate": 7.979954402718632e-06,
      "loss": 2.1443,
      "step": 7279
    },
    {
      "epoch": 2.818428184281843,
      "grad_norm": 22.315025329589844,
      "learning_rate": 7.979524239686842e-06,
      "loss": 2.6736,
      "step": 7280
    },
    {
      "epoch": 2.818815331010453,
      "grad_norm": 19.920869827270508,
      "learning_rate": 7.979094076655053e-06,
      "loss": 1.7932,
      "step": 7281
    },
    {
      "epoch": 2.819202477739063,
      "grad_norm": 20.036724090576172,
      "learning_rate": 7.978663913623263e-06,
      "loss": 1.1852,
      "step": 7282
    },
    {
      "epoch": 2.8195896244676733,
      "grad_norm": 10.070487976074219,
      "learning_rate": 7.978233750591475e-06,
      "loss": 0.4707,
      "step": 7283
    },
    {
      "epoch": 2.8199767711962833,
      "grad_norm": 11.844327926635742,
      "learning_rate": 7.977803587559686e-06,
      "loss": 0.6673,
      "step": 7284
    },
    {
      "epoch": 2.8203639179248934,
      "grad_norm": 17.553237915039062,
      "learning_rate": 7.977373424527897e-06,
      "loss": 1.73,
      "step": 7285
    },
    {
      "epoch": 2.8207510646535034,
      "grad_norm": 14.360601425170898,
      "learning_rate": 7.976943261496107e-06,
      "loss": 0.9183,
      "step": 7286
    },
    {
      "epoch": 2.821138211382114,
      "grad_norm": 23.00815200805664,
      "learning_rate": 7.976513098464319e-06,
      "loss": 1.5004,
      "step": 7287
    },
    {
      "epoch": 2.821525358110724,
      "grad_norm": 26.29194450378418,
      "learning_rate": 7.97608293543253e-06,
      "loss": 2.1203,
      "step": 7288
    },
    {
      "epoch": 2.821912504839334,
      "grad_norm": 32.35588836669922,
      "learning_rate": 7.97565277240074e-06,
      "loss": 1.3585,
      "step": 7289
    },
    {
      "epoch": 2.822299651567944,
      "grad_norm": 73.02574920654297,
      "learning_rate": 7.975222609368951e-06,
      "loss": 2.8841,
      "step": 7290
    },
    {
      "epoch": 2.8226867982965542,
      "grad_norm": 21.333065032958984,
      "learning_rate": 7.974792446337163e-06,
      "loss": 2.5263,
      "step": 7291
    },
    {
      "epoch": 2.8230739450251647,
      "grad_norm": 23.467432022094727,
      "learning_rate": 7.974362283305374e-06,
      "loss": 1.143,
      "step": 7292
    },
    {
      "epoch": 2.823461091753775,
      "grad_norm": 26.84126853942871,
      "learning_rate": 7.973932120273584e-06,
      "loss": 1.4877,
      "step": 7293
    },
    {
      "epoch": 2.823848238482385,
      "grad_norm": 15.027612686157227,
      "learning_rate": 7.973501957241795e-06,
      "loss": 0.7315,
      "step": 7294
    },
    {
      "epoch": 2.824235385210995,
      "grad_norm": 20.567485809326172,
      "learning_rate": 7.973071794210006e-06,
      "loss": 0.7983,
      "step": 7295
    },
    {
      "epoch": 2.824622531939605,
      "grad_norm": 28.12624168395996,
      "learning_rate": 7.972641631178218e-06,
      "loss": 1.6336,
      "step": 7296
    },
    {
      "epoch": 2.8250096786682155,
      "grad_norm": 27.059345245361328,
      "learning_rate": 7.972211468146428e-06,
      "loss": 1.0937,
      "step": 7297
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 34.68313980102539,
      "learning_rate": 7.971781305114639e-06,
      "loss": 1.4825,
      "step": 7298
    },
    {
      "epoch": 2.8257839721254356,
      "grad_norm": 11.600642204284668,
      "learning_rate": 7.97135114208285e-06,
      "loss": 0.6906,
      "step": 7299
    },
    {
      "epoch": 2.8261711188540457,
      "grad_norm": 8.12117862701416,
      "learning_rate": 7.970920979051062e-06,
      "loss": 0.2503,
      "step": 7300
    },
    {
      "epoch": 2.8265582655826558,
      "grad_norm": 16.63116455078125,
      "learning_rate": 7.970490816019272e-06,
      "loss": 1.6161,
      "step": 7301
    },
    {
      "epoch": 2.826945412311266,
      "grad_norm": 24.003128051757812,
      "learning_rate": 7.970060652987483e-06,
      "loss": 2.2227,
      "step": 7302
    },
    {
      "epoch": 2.827332559039876,
      "grad_norm": 15.413222312927246,
      "learning_rate": 7.969630489955694e-06,
      "loss": 1.3622,
      "step": 7303
    },
    {
      "epoch": 2.8277197057684864,
      "grad_norm": 24.63412094116211,
      "learning_rate": 7.969200326923904e-06,
      "loss": 1.68,
      "step": 7304
    },
    {
      "epoch": 2.8281068524970965,
      "grad_norm": 46.68855667114258,
      "learning_rate": 7.968770163892116e-06,
      "loss": 1.3491,
      "step": 7305
    },
    {
      "epoch": 2.8284939992257065,
      "grad_norm": 9.615629196166992,
      "learning_rate": 7.968340000860327e-06,
      "loss": 0.736,
      "step": 7306
    },
    {
      "epoch": 2.8288811459543166,
      "grad_norm": 36.49643325805664,
      "learning_rate": 7.967909837828538e-06,
      "loss": 1.7084,
      "step": 7307
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 42.00132369995117,
      "learning_rate": 7.967479674796748e-06,
      "loss": 1.4709,
      "step": 7308
    },
    {
      "epoch": 2.829655439411537,
      "grad_norm": 16.106550216674805,
      "learning_rate": 7.96704951176496e-06,
      "loss": 1.3671,
      "step": 7309
    },
    {
      "epoch": 2.8300425861401473,
      "grad_norm": 22.943349838256836,
      "learning_rate": 7.96661934873317e-06,
      "loss": 1.2843,
      "step": 7310
    },
    {
      "epoch": 2.8304297328687573,
      "grad_norm": 19.337045669555664,
      "learning_rate": 7.966189185701382e-06,
      "loss": 2.2028,
      "step": 7311
    },
    {
      "epoch": 2.8308168795973674,
      "grad_norm": 23.67597198486328,
      "learning_rate": 7.965759022669592e-06,
      "loss": 0.6833,
      "step": 7312
    },
    {
      "epoch": 2.8312040263259775,
      "grad_norm": 17.888206481933594,
      "learning_rate": 7.965328859637803e-06,
      "loss": 0.9719,
      "step": 7313
    },
    {
      "epoch": 2.831591173054588,
      "grad_norm": 13.110998153686523,
      "learning_rate": 7.964898696606013e-06,
      "loss": 1.2226,
      "step": 7314
    },
    {
      "epoch": 2.8319783197831976,
      "grad_norm": 20.212627410888672,
      "learning_rate": 7.964468533574226e-06,
      "loss": 0.8146,
      "step": 7315
    },
    {
      "epoch": 2.832365466511808,
      "grad_norm": 17.160293579101562,
      "learning_rate": 7.964038370542436e-06,
      "loss": 1.4415,
      "step": 7316
    },
    {
      "epoch": 2.832752613240418,
      "grad_norm": 47.53095626831055,
      "learning_rate": 7.963608207510647e-06,
      "loss": 0.8704,
      "step": 7317
    },
    {
      "epoch": 2.8331397599690282,
      "grad_norm": 32.77962875366211,
      "learning_rate": 7.963178044478859e-06,
      "loss": 0.9018,
      "step": 7318
    },
    {
      "epoch": 2.8335269066976383,
      "grad_norm": 6.311821460723877,
      "learning_rate": 7.962747881447069e-06,
      "loss": 0.3148,
      "step": 7319
    },
    {
      "epoch": 2.8339140534262484,
      "grad_norm": 21.609590530395508,
      "learning_rate": 7.96231771841528e-06,
      "loss": 1.1765,
      "step": 7320
    },
    {
      "epoch": 2.834301200154859,
      "grad_norm": 37.167972564697266,
      "learning_rate": 7.961887555383491e-06,
      "loss": 3.9607,
      "step": 7321
    },
    {
      "epoch": 2.834688346883469,
      "grad_norm": 43.89338302612305,
      "learning_rate": 7.961457392351703e-06,
      "loss": 1.6862,
      "step": 7322
    },
    {
      "epoch": 2.835075493612079,
      "grad_norm": 11.455789566040039,
      "learning_rate": 7.961027229319913e-06,
      "loss": 1.2789,
      "step": 7323
    },
    {
      "epoch": 2.835462640340689,
      "grad_norm": 21.04143714904785,
      "learning_rate": 7.960597066288124e-06,
      "loss": 1.7572,
      "step": 7324
    },
    {
      "epoch": 2.835849787069299,
      "grad_norm": 39.654296875,
      "learning_rate": 7.960166903256334e-06,
      "loss": 1.5451,
      "step": 7325
    },
    {
      "epoch": 2.8362369337979096,
      "grad_norm": 11.877128601074219,
      "learning_rate": 7.959736740224547e-06,
      "loss": 0.657,
      "step": 7326
    },
    {
      "epoch": 2.8366240805265197,
      "grad_norm": 68.37645721435547,
      "learning_rate": 7.959306577192757e-06,
      "loss": 1.7501,
      "step": 7327
    },
    {
      "epoch": 2.8370112272551298,
      "grad_norm": 22.803016662597656,
      "learning_rate": 7.958876414160968e-06,
      "loss": 1.2708,
      "step": 7328
    },
    {
      "epoch": 2.83739837398374,
      "grad_norm": 13.10368824005127,
      "learning_rate": 7.958446251129178e-06,
      "loss": 0.8295,
      "step": 7329
    },
    {
      "epoch": 2.83778552071235,
      "grad_norm": 18.930757522583008,
      "learning_rate": 7.95801608809739e-06,
      "loss": 1.1212,
      "step": 7330
    },
    {
      "epoch": 2.83817266744096,
      "grad_norm": 27.44232177734375,
      "learning_rate": 7.9575859250656e-06,
      "loss": 1.8003,
      "step": 7331
    },
    {
      "epoch": 2.83855981416957,
      "grad_norm": 36.77300262451172,
      "learning_rate": 7.957155762033812e-06,
      "loss": 1.4271,
      "step": 7332
    },
    {
      "epoch": 2.8389469608981805,
      "grad_norm": 19.549407958984375,
      "learning_rate": 7.956725599002022e-06,
      "loss": 1.5669,
      "step": 7333
    },
    {
      "epoch": 2.8393341076267906,
      "grad_norm": 16.098052978515625,
      "learning_rate": 7.956295435970233e-06,
      "loss": 0.6847,
      "step": 7334
    },
    {
      "epoch": 2.8397212543554007,
      "grad_norm": 76.73174285888672,
      "learning_rate": 7.955865272938444e-06,
      "loss": 1.9789,
      "step": 7335
    },
    {
      "epoch": 2.8401084010840107,
      "grad_norm": 13.514798164367676,
      "learning_rate": 7.955435109906656e-06,
      "loss": 0.917,
      "step": 7336
    },
    {
      "epoch": 2.840495547812621,
      "grad_norm": 14.754523277282715,
      "learning_rate": 7.955004946874866e-06,
      "loss": 0.6428,
      "step": 7337
    },
    {
      "epoch": 2.8408826945412313,
      "grad_norm": 51.75570297241211,
      "learning_rate": 7.954574783843077e-06,
      "loss": 2.0135,
      "step": 7338
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 21.160247802734375,
      "learning_rate": 7.954144620811288e-06,
      "loss": 1.1227,
      "step": 7339
    },
    {
      "epoch": 2.8416569879984515,
      "grad_norm": 15.362080574035645,
      "learning_rate": 7.953714457779498e-06,
      "loss": 1.4341,
      "step": 7340
    },
    {
      "epoch": 2.8420441347270615,
      "grad_norm": 32.92377471923828,
      "learning_rate": 7.95328429474771e-06,
      "loss": 1.2125,
      "step": 7341
    },
    {
      "epoch": 2.8424312814556716,
      "grad_norm": 20.850942611694336,
      "learning_rate": 7.952854131715921e-06,
      "loss": 1.2879,
      "step": 7342
    },
    {
      "epoch": 2.842818428184282,
      "grad_norm": 8.743510246276855,
      "learning_rate": 7.952423968684132e-06,
      "loss": 0.639,
      "step": 7343
    },
    {
      "epoch": 2.8432055749128917,
      "grad_norm": 20.826658248901367,
      "learning_rate": 7.951993805652342e-06,
      "loss": 1.3635,
      "step": 7344
    },
    {
      "epoch": 2.8435927216415022,
      "grad_norm": 43.740291595458984,
      "learning_rate": 7.951563642620554e-06,
      "loss": 0.8835,
      "step": 7345
    },
    {
      "epoch": 2.8439798683701123,
      "grad_norm": 47.284217834472656,
      "learning_rate": 7.951133479588765e-06,
      "loss": 0.5163,
      "step": 7346
    },
    {
      "epoch": 2.8443670150987224,
      "grad_norm": 11.178693771362305,
      "learning_rate": 7.950703316556976e-06,
      "loss": 0.8099,
      "step": 7347
    },
    {
      "epoch": 2.8447541618273324,
      "grad_norm": 53.25722885131836,
      "learning_rate": 7.950273153525186e-06,
      "loss": 1.4067,
      "step": 7348
    },
    {
      "epoch": 2.8451413085559425,
      "grad_norm": 16.187124252319336,
      "learning_rate": 7.949842990493398e-06,
      "loss": 1.0214,
      "step": 7349
    },
    {
      "epoch": 2.845528455284553,
      "grad_norm": 28.30167579650879,
      "learning_rate": 7.949412827461609e-06,
      "loss": 1.7177,
      "step": 7350
    },
    {
      "epoch": 2.845915602013163,
      "grad_norm": 16.70755958557129,
      "learning_rate": 7.94898266442982e-06,
      "loss": 1.8511,
      "step": 7351
    },
    {
      "epoch": 2.846302748741773,
      "grad_norm": 25.79487419128418,
      "learning_rate": 7.94855250139803e-06,
      "loss": 1.5835,
      "step": 7352
    },
    {
      "epoch": 2.846689895470383,
      "grad_norm": 34.44961929321289,
      "learning_rate": 7.948122338366241e-06,
      "loss": 1.3629,
      "step": 7353
    },
    {
      "epoch": 2.8470770421989933,
      "grad_norm": 11.93077564239502,
      "learning_rate": 7.947692175334453e-06,
      "loss": 1.5904,
      "step": 7354
    },
    {
      "epoch": 2.847464188927604,
      "grad_norm": 26.051830291748047,
      "learning_rate": 7.947262012302663e-06,
      "loss": 1.8483,
      "step": 7355
    },
    {
      "epoch": 2.847851335656214,
      "grad_norm": 32.99980926513672,
      "learning_rate": 7.946831849270874e-06,
      "loss": 2.1225,
      "step": 7356
    },
    {
      "epoch": 2.848238482384824,
      "grad_norm": 16.938135147094727,
      "learning_rate": 7.946401686239085e-06,
      "loss": 1.2421,
      "step": 7357
    },
    {
      "epoch": 2.848625629113434,
      "grad_norm": 12.457690238952637,
      "learning_rate": 7.945971523207297e-06,
      "loss": 1.1269,
      "step": 7358
    },
    {
      "epoch": 2.849012775842044,
      "grad_norm": 44.33602523803711,
      "learning_rate": 7.945541360175507e-06,
      "loss": 1.9206,
      "step": 7359
    },
    {
      "epoch": 2.8493999225706546,
      "grad_norm": 16.42288589477539,
      "learning_rate": 7.945111197143718e-06,
      "loss": 0.8041,
      "step": 7360
    },
    {
      "epoch": 2.849787069299264,
      "grad_norm": 22.975231170654297,
      "learning_rate": 7.94468103411193e-06,
      "loss": 1.2554,
      "step": 7361
    },
    {
      "epoch": 2.8501742160278747,
      "grad_norm": 27.70033836364746,
      "learning_rate": 7.94425087108014e-06,
      "loss": 1.3021,
      "step": 7362
    },
    {
      "epoch": 2.8505613627564848,
      "grad_norm": 15.168691635131836,
      "learning_rate": 7.94382070804835e-06,
      "loss": 1.2798,
      "step": 7363
    },
    {
      "epoch": 2.850948509485095,
      "grad_norm": 60.63220977783203,
      "learning_rate": 7.943390545016562e-06,
      "loss": 1.396,
      "step": 7364
    },
    {
      "epoch": 2.851335656213705,
      "grad_norm": 22.904258728027344,
      "learning_rate": 7.942960381984773e-06,
      "loss": 0.9398,
      "step": 7365
    },
    {
      "epoch": 2.851722802942315,
      "grad_norm": 15.262042999267578,
      "learning_rate": 7.942530218952985e-06,
      "loss": 1.0912,
      "step": 7366
    },
    {
      "epoch": 2.8521099496709255,
      "grad_norm": 31.745758056640625,
      "learning_rate": 7.942100055921195e-06,
      "loss": 0.8315,
      "step": 7367
    },
    {
      "epoch": 2.8524970963995355,
      "grad_norm": 27.255273818969727,
      "learning_rate": 7.941669892889406e-06,
      "loss": 2.0615,
      "step": 7368
    },
    {
      "epoch": 2.8528842431281456,
      "grad_norm": 44.15190505981445,
      "learning_rate": 7.941239729857617e-06,
      "loss": 1.7414,
      "step": 7369
    },
    {
      "epoch": 2.8532713898567557,
      "grad_norm": 10.931353569030762,
      "learning_rate": 7.940809566825827e-06,
      "loss": 0.5687,
      "step": 7370
    },
    {
      "epoch": 2.8536585365853657,
      "grad_norm": 35.64372253417969,
      "learning_rate": 7.940379403794039e-06,
      "loss": 2.1466,
      "step": 7371
    },
    {
      "epoch": 2.8540456833139762,
      "grad_norm": 66.4782943725586,
      "learning_rate": 7.93994924076225e-06,
      "loss": 1.7962,
      "step": 7372
    },
    {
      "epoch": 2.8544328300425863,
      "grad_norm": 12.031343460083008,
      "learning_rate": 7.939519077730461e-06,
      "loss": 0.7771,
      "step": 7373
    },
    {
      "epoch": 2.8548199767711964,
      "grad_norm": 113.78084564208984,
      "learning_rate": 7.939088914698671e-06,
      "loss": 1.2786,
      "step": 7374
    },
    {
      "epoch": 2.8552071234998064,
      "grad_norm": 13.98550796508789,
      "learning_rate": 7.938658751666882e-06,
      "loss": 1.03,
      "step": 7375
    },
    {
      "epoch": 2.8555942702284165,
      "grad_norm": 3.905940055847168,
      "learning_rate": 7.938228588635092e-06,
      "loss": 0.1158,
      "step": 7376
    },
    {
      "epoch": 2.8559814169570266,
      "grad_norm": 19.057296752929688,
      "learning_rate": 7.937798425603305e-06,
      "loss": 0.9487,
      "step": 7377
    },
    {
      "epoch": 2.8563685636856366,
      "grad_norm": 27.44889259338379,
      "learning_rate": 7.937368262571515e-06,
      "loss": 1.5425,
      "step": 7378
    },
    {
      "epoch": 2.856755710414247,
      "grad_norm": 13.65168285369873,
      "learning_rate": 7.936938099539726e-06,
      "loss": 0.8249,
      "step": 7379
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 12.1820707321167,
      "learning_rate": 7.936507936507936e-06,
      "loss": 0.6575,
      "step": 7380
    },
    {
      "epoch": 2.8575300038714673,
      "grad_norm": 12.551448822021484,
      "learning_rate": 7.93607777347615e-06,
      "loss": 0.9255,
      "step": 7381
    },
    {
      "epoch": 2.8579171506000773,
      "grad_norm": 19.2925968170166,
      "learning_rate": 7.935647610444359e-06,
      "loss": 2.1755,
      "step": 7382
    },
    {
      "epoch": 2.8583042973286874,
      "grad_norm": 19.057514190673828,
      "learning_rate": 7.93521744741257e-06,
      "loss": 1.3809,
      "step": 7383
    },
    {
      "epoch": 2.858691444057298,
      "grad_norm": 13.677562713623047,
      "learning_rate": 7.93478728438078e-06,
      "loss": 0.7541,
      "step": 7384
    },
    {
      "epoch": 2.859078590785908,
      "grad_norm": 23.285106658935547,
      "learning_rate": 7.934357121348992e-06,
      "loss": 0.8944,
      "step": 7385
    },
    {
      "epoch": 2.859465737514518,
      "grad_norm": 31.59455680847168,
      "learning_rate": 7.933926958317203e-06,
      "loss": 1.4082,
      "step": 7386
    },
    {
      "epoch": 2.859852884243128,
      "grad_norm": 38.13277816772461,
      "learning_rate": 7.933496795285414e-06,
      "loss": 2.9918,
      "step": 7387
    },
    {
      "epoch": 2.860240030971738,
      "grad_norm": 29.593042373657227,
      "learning_rate": 7.933066632253624e-06,
      "loss": 1.4079,
      "step": 7388
    },
    {
      "epoch": 2.8606271777003487,
      "grad_norm": 71.35503387451172,
      "learning_rate": 7.932636469221836e-06,
      "loss": 1.2536,
      "step": 7389
    },
    {
      "epoch": 2.8610143244289583,
      "grad_norm": 24.359895706176758,
      "learning_rate": 7.932206306190047e-06,
      "loss": 1.4949,
      "step": 7390
    },
    {
      "epoch": 2.861401471157569,
      "grad_norm": 21.383909225463867,
      "learning_rate": 7.931776143158257e-06,
      "loss": 0.8609,
      "step": 7391
    },
    {
      "epoch": 2.861788617886179,
      "grad_norm": 11.038924217224121,
      "learning_rate": 7.931345980126468e-06,
      "loss": 0.74,
      "step": 7392
    },
    {
      "epoch": 2.862175764614789,
      "grad_norm": 21.58751106262207,
      "learning_rate": 7.93091581709468e-06,
      "loss": 1.4239,
      "step": 7393
    },
    {
      "epoch": 2.862562911343399,
      "grad_norm": 34.58320999145508,
      "learning_rate": 7.930485654062891e-06,
      "loss": 1.5442,
      "step": 7394
    },
    {
      "epoch": 2.862950058072009,
      "grad_norm": 39.736412048339844,
      "learning_rate": 7.9300554910311e-06,
      "loss": 1.4517,
      "step": 7395
    },
    {
      "epoch": 2.8633372048006196,
      "grad_norm": 20.094406127929688,
      "learning_rate": 7.929625327999312e-06,
      "loss": 1.6025,
      "step": 7396
    },
    {
      "epoch": 2.8637243515292297,
      "grad_norm": 27.895315170288086,
      "learning_rate": 7.929195164967523e-06,
      "loss": 1.7698,
      "step": 7397
    },
    {
      "epoch": 2.8641114982578397,
      "grad_norm": 39.4733772277832,
      "learning_rate": 7.928765001935735e-06,
      "loss": 1.3241,
      "step": 7398
    },
    {
      "epoch": 2.86449864498645,
      "grad_norm": 19.78913116455078,
      "learning_rate": 7.928334838903945e-06,
      "loss": 2.2696,
      "step": 7399
    },
    {
      "epoch": 2.86488579171506,
      "grad_norm": 30.919567108154297,
      "learning_rate": 7.927904675872156e-06,
      "loss": 1.4873,
      "step": 7400
    },
    {
      "epoch": 2.8652729384436704,
      "grad_norm": 17.469507217407227,
      "learning_rate": 7.927474512840367e-06,
      "loss": 1.1905,
      "step": 7401
    },
    {
      "epoch": 2.8656600851722804,
      "grad_norm": 17.164142608642578,
      "learning_rate": 7.927044349808579e-06,
      "loss": 0.7247,
      "step": 7402
    },
    {
      "epoch": 2.8660472319008905,
      "grad_norm": 30.49341583251953,
      "learning_rate": 7.926614186776789e-06,
      "loss": 1.5261,
      "step": 7403
    },
    {
      "epoch": 2.8664343786295006,
      "grad_norm": 32.301265716552734,
      "learning_rate": 7.926184023745e-06,
      "loss": 0.6774,
      "step": 7404
    },
    {
      "epoch": 2.8668215253581106,
      "grad_norm": 4.295449733734131,
      "learning_rate": 7.925753860713211e-06,
      "loss": 0.1325,
      "step": 7405
    },
    {
      "epoch": 2.867208672086721,
      "grad_norm": 20.785709381103516,
      "learning_rate": 7.925323697681421e-06,
      "loss": 1.2267,
      "step": 7406
    },
    {
      "epoch": 2.8675958188153308,
      "grad_norm": 57.09174728393555,
      "learning_rate": 7.924893534649633e-06,
      "loss": 1.8025,
      "step": 7407
    },
    {
      "epoch": 2.8679829655439413,
      "grad_norm": 10.61821460723877,
      "learning_rate": 7.924463371617844e-06,
      "loss": 0.4868,
      "step": 7408
    },
    {
      "epoch": 2.8683701122725513,
      "grad_norm": 21.172927856445312,
      "learning_rate": 7.924033208586055e-06,
      "loss": 1.1719,
      "step": 7409
    },
    {
      "epoch": 2.8687572590011614,
      "grad_norm": 22.824182510375977,
      "learning_rate": 7.923603045554265e-06,
      "loss": 1.5039,
      "step": 7410
    },
    {
      "epoch": 2.8691444057297715,
      "grad_norm": 53.78929901123047,
      "learning_rate": 7.923172882522476e-06,
      "loss": 4.4524,
      "step": 7411
    },
    {
      "epoch": 2.8695315524583815,
      "grad_norm": 45.388240814208984,
      "learning_rate": 7.922742719490688e-06,
      "loss": 2.6788,
      "step": 7412
    },
    {
      "epoch": 2.869918699186992,
      "grad_norm": 21.82183265686035,
      "learning_rate": 7.9223125564589e-06,
      "loss": 1.3299,
      "step": 7413
    },
    {
      "epoch": 2.870305845915602,
      "grad_norm": 12.137054443359375,
      "learning_rate": 7.921882393427109e-06,
      "loss": 0.5258,
      "step": 7414
    },
    {
      "epoch": 2.870692992644212,
      "grad_norm": 29.033794403076172,
      "learning_rate": 7.92145223039532e-06,
      "loss": 3.1342,
      "step": 7415
    },
    {
      "epoch": 2.8710801393728222,
      "grad_norm": 17.48978042602539,
      "learning_rate": 7.921022067363532e-06,
      "loss": 1.3646,
      "step": 7416
    },
    {
      "epoch": 2.8714672861014323,
      "grad_norm": 41.15259552001953,
      "learning_rate": 7.920591904331743e-06,
      "loss": 2.6473,
      "step": 7417
    },
    {
      "epoch": 2.871854432830043,
      "grad_norm": 29.741987228393555,
      "learning_rate": 7.920161741299953e-06,
      "loss": 0.9697,
      "step": 7418
    },
    {
      "epoch": 2.872241579558653,
      "grad_norm": 25.65536880493164,
      "learning_rate": 7.919731578268164e-06,
      "loss": 2.5189,
      "step": 7419
    },
    {
      "epoch": 2.872628726287263,
      "grad_norm": 13.721902847290039,
      "learning_rate": 7.919301415236376e-06,
      "loss": 1.3695,
      "step": 7420
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 44.75494384765625,
      "learning_rate": 7.918871252204586e-06,
      "loss": 2.6512,
      "step": 7421
    },
    {
      "epoch": 2.873403019744483,
      "grad_norm": 32.58513259887695,
      "learning_rate": 7.918441089172797e-06,
      "loss": 1.1377,
      "step": 7422
    },
    {
      "epoch": 2.873790166473093,
      "grad_norm": 27.605220794677734,
      "learning_rate": 7.918010926141008e-06,
      "loss": 1.5467,
      "step": 7423
    },
    {
      "epoch": 2.874177313201703,
      "grad_norm": 11.929109573364258,
      "learning_rate": 7.91758076310922e-06,
      "loss": 0.7945,
      "step": 7424
    },
    {
      "epoch": 2.8745644599303137,
      "grad_norm": 24.440223693847656,
      "learning_rate": 7.91715060007743e-06,
      "loss": 1.5819,
      "step": 7425
    },
    {
      "epoch": 2.874951606658924,
      "grad_norm": 15.578191757202148,
      "learning_rate": 7.916720437045641e-06,
      "loss": 1.1516,
      "step": 7426
    },
    {
      "epoch": 2.875338753387534,
      "grad_norm": 19.802515029907227,
      "learning_rate": 7.91629027401385e-06,
      "loss": 1.4936,
      "step": 7427
    },
    {
      "epoch": 2.875725900116144,
      "grad_norm": 24.30876350402832,
      "learning_rate": 7.915860110982064e-06,
      "loss": 1.3615,
      "step": 7428
    },
    {
      "epoch": 2.876113046844754,
      "grad_norm": 28.624046325683594,
      "learning_rate": 7.915429947950274e-06,
      "loss": 2.4266,
      "step": 7429
    },
    {
      "epoch": 2.8765001935733645,
      "grad_norm": 17.61773681640625,
      "learning_rate": 7.914999784918485e-06,
      "loss": 1.519,
      "step": 7430
    },
    {
      "epoch": 2.8768873403019746,
      "grad_norm": 21.999263763427734,
      "learning_rate": 7.914569621886695e-06,
      "loss": 1.5851,
      "step": 7431
    },
    {
      "epoch": 2.8772744870305846,
      "grad_norm": 52.2398681640625,
      "learning_rate": 7.914139458854908e-06,
      "loss": 1.5409,
      "step": 7432
    },
    {
      "epoch": 2.8776616337591947,
      "grad_norm": 21.91822052001953,
      "learning_rate": 7.913709295823117e-06,
      "loss": 1.5888,
      "step": 7433
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 13.176401138305664,
      "learning_rate": 7.913279132791329e-06,
      "loss": 1.0051,
      "step": 7434
    },
    {
      "epoch": 2.8784359272164153,
      "grad_norm": 57.47438430786133,
      "learning_rate": 7.912848969759539e-06,
      "loss": 2.1808,
      "step": 7435
    },
    {
      "epoch": 2.878823073945025,
      "grad_norm": 22.219524383544922,
      "learning_rate": 7.91241880672775e-06,
      "loss": 1.6488,
      "step": 7436
    },
    {
      "epoch": 2.8792102206736354,
      "grad_norm": 39.43246841430664,
      "learning_rate": 7.911988643695961e-06,
      "loss": 1.3024,
      "step": 7437
    },
    {
      "epoch": 2.8795973674022455,
      "grad_norm": 31.788198471069336,
      "learning_rate": 7.911558480664173e-06,
      "loss": 1.7305,
      "step": 7438
    },
    {
      "epoch": 2.8799845141308555,
      "grad_norm": 55.65043258666992,
      "learning_rate": 7.911128317632384e-06,
      "loss": 2.6221,
      "step": 7439
    },
    {
      "epoch": 2.8803716608594656,
      "grad_norm": 32.92521667480469,
      "learning_rate": 7.910698154600594e-06,
      "loss": 1.0396,
      "step": 7440
    },
    {
      "epoch": 2.8807588075880757,
      "grad_norm": 16.675830841064453,
      "learning_rate": 7.910267991568805e-06,
      "loss": 1.144,
      "step": 7441
    },
    {
      "epoch": 2.881145954316686,
      "grad_norm": 27.308795928955078,
      "learning_rate": 7.909837828537015e-06,
      "loss": 0.8581,
      "step": 7442
    },
    {
      "epoch": 2.8815331010452963,
      "grad_norm": 20.067785263061523,
      "learning_rate": 7.909407665505228e-06,
      "loss": 1.0968,
      "step": 7443
    },
    {
      "epoch": 2.8819202477739063,
      "grad_norm": 37.445980072021484,
      "learning_rate": 7.908977502473438e-06,
      "loss": 2.7375,
      "step": 7444
    },
    {
      "epoch": 2.8823073945025164,
      "grad_norm": 68.42594909667969,
      "learning_rate": 7.90854733944165e-06,
      "loss": 1.1163,
      "step": 7445
    },
    {
      "epoch": 2.8826945412311265,
      "grad_norm": 26.279348373413086,
      "learning_rate": 7.908117176409859e-06,
      "loss": 3.2978,
      "step": 7446
    },
    {
      "epoch": 2.883081687959737,
      "grad_norm": 19.221954345703125,
      "learning_rate": 7.907687013378072e-06,
      "loss": 0.9805,
      "step": 7447
    },
    {
      "epoch": 2.883468834688347,
      "grad_norm": 34.83165740966797,
      "learning_rate": 7.907256850346282e-06,
      "loss": 1.3976,
      "step": 7448
    },
    {
      "epoch": 2.883855981416957,
      "grad_norm": 41.42762756347656,
      "learning_rate": 7.906826687314493e-06,
      "loss": 1.3082,
      "step": 7449
    },
    {
      "epoch": 2.884243128145567,
      "grad_norm": 59.82026672363281,
      "learning_rate": 7.906396524282703e-06,
      "loss": 2.0866,
      "step": 7450
    },
    {
      "epoch": 2.8846302748741772,
      "grad_norm": 52.88538360595703,
      "learning_rate": 7.905966361250914e-06,
      "loss": 1.5555,
      "step": 7451
    },
    {
      "epoch": 2.8850174216027873,
      "grad_norm": 12.823830604553223,
      "learning_rate": 7.905536198219126e-06,
      "loss": 1.4349,
      "step": 7452
    },
    {
      "epoch": 2.8854045683313974,
      "grad_norm": 16.670686721801758,
      "learning_rate": 7.905106035187337e-06,
      "loss": 1.3198,
      "step": 7453
    },
    {
      "epoch": 2.885791715060008,
      "grad_norm": 15.870999336242676,
      "learning_rate": 7.904675872155547e-06,
      "loss": 1.4183,
      "step": 7454
    },
    {
      "epoch": 2.886178861788618,
      "grad_norm": 13.996475219726562,
      "learning_rate": 7.904245709123758e-06,
      "loss": 0.5106,
      "step": 7455
    },
    {
      "epoch": 2.886566008517228,
      "grad_norm": 26.929668426513672,
      "learning_rate": 7.90381554609197e-06,
      "loss": 1.8601,
      "step": 7456
    },
    {
      "epoch": 2.886953155245838,
      "grad_norm": 20.589277267456055,
      "learning_rate": 7.90338538306018e-06,
      "loss": 1.1229,
      "step": 7457
    },
    {
      "epoch": 2.887340301974448,
      "grad_norm": 16.919536590576172,
      "learning_rate": 7.902955220028391e-06,
      "loss": 0.9311,
      "step": 7458
    },
    {
      "epoch": 2.8877274487030586,
      "grad_norm": 19.04384994506836,
      "learning_rate": 7.902525056996602e-06,
      "loss": 1.0293,
      "step": 7459
    },
    {
      "epoch": 2.8881145954316687,
      "grad_norm": 62.94681930541992,
      "learning_rate": 7.902094893964814e-06,
      "loss": 1.5753,
      "step": 7460
    },
    {
      "epoch": 2.8885017421602788,
      "grad_norm": 19.06062889099121,
      "learning_rate": 7.901664730933024e-06,
      "loss": 1.6013,
      "step": 7461
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 11.62189769744873,
      "learning_rate": 7.901234567901235e-06,
      "loss": 0.6444,
      "step": 7462
    },
    {
      "epoch": 2.889276035617499,
      "grad_norm": 13.669617652893066,
      "learning_rate": 7.900804404869446e-06,
      "loss": 0.5279,
      "step": 7463
    },
    {
      "epoch": 2.8896631823461094,
      "grad_norm": 65.77359771728516,
      "learning_rate": 7.900374241837658e-06,
      "loss": 2.6448,
      "step": 7464
    },
    {
      "epoch": 2.8900503290747195,
      "grad_norm": 32.89047622680664,
      "learning_rate": 7.899944078805868e-06,
      "loss": 1.6555,
      "step": 7465
    },
    {
      "epoch": 2.8904374758033295,
      "grad_norm": 10.329208374023438,
      "learning_rate": 7.899513915774079e-06,
      "loss": 0.674,
      "step": 7466
    },
    {
      "epoch": 2.8908246225319396,
      "grad_norm": 23.262950897216797,
      "learning_rate": 7.89908375274229e-06,
      "loss": 0.8277,
      "step": 7467
    },
    {
      "epoch": 2.8912117692605497,
      "grad_norm": 43.72859191894531,
      "learning_rate": 7.898653589710502e-06,
      "loss": 0.6197,
      "step": 7468
    },
    {
      "epoch": 2.8915989159891597,
      "grad_norm": 33.98553466796875,
      "learning_rate": 7.898223426678712e-06,
      "loss": 1.1831,
      "step": 7469
    },
    {
      "epoch": 2.89198606271777,
      "grad_norm": 13.330536842346191,
      "learning_rate": 7.897793263646923e-06,
      "loss": 0.8033,
      "step": 7470
    },
    {
      "epoch": 2.8923732094463803,
      "grad_norm": 46.97468566894531,
      "learning_rate": 7.897363100615134e-06,
      "loss": 1.2408,
      "step": 7471
    },
    {
      "epoch": 2.8927603561749904,
      "grad_norm": 40.47684097290039,
      "learning_rate": 7.896932937583344e-06,
      "loss": 0.754,
      "step": 7472
    },
    {
      "epoch": 2.8931475029036005,
      "grad_norm": 21.028635025024414,
      "learning_rate": 7.896502774551555e-06,
      "loss": 1.3938,
      "step": 7473
    },
    {
      "epoch": 2.8935346496322105,
      "grad_norm": 11.830492973327637,
      "learning_rate": 7.896072611519767e-06,
      "loss": 0.6229,
      "step": 7474
    },
    {
      "epoch": 2.8939217963608206,
      "grad_norm": 17.536874771118164,
      "learning_rate": 7.895642448487978e-06,
      "loss": 1.1112,
      "step": 7475
    },
    {
      "epoch": 2.894308943089431,
      "grad_norm": 11.80813217163086,
      "learning_rate": 7.895212285456188e-06,
      "loss": 1.3869,
      "step": 7476
    },
    {
      "epoch": 2.894696089818041,
      "grad_norm": 13.25986099243164,
      "learning_rate": 7.8947821224244e-06,
      "loss": 0.7611,
      "step": 7477
    },
    {
      "epoch": 2.8950832365466512,
      "grad_norm": 40.416873931884766,
      "learning_rate": 7.89435195939261e-06,
      "loss": 1.7199,
      "step": 7478
    },
    {
      "epoch": 2.8954703832752613,
      "grad_norm": 15.487855911254883,
      "learning_rate": 7.893921796360822e-06,
      "loss": 0.9654,
      "step": 7479
    },
    {
      "epoch": 2.8958575300038714,
      "grad_norm": 20.0258846282959,
      "learning_rate": 7.893491633329032e-06,
      "loss": 1.6397,
      "step": 7480
    },
    {
      "epoch": 2.896244676732482,
      "grad_norm": 37.77161407470703,
      "learning_rate": 7.893061470297243e-06,
      "loss": 1.8416,
      "step": 7481
    },
    {
      "epoch": 2.8966318234610915,
      "grad_norm": 13.165506362915039,
      "learning_rate": 7.892631307265455e-06,
      "loss": 1.6231,
      "step": 7482
    },
    {
      "epoch": 2.897018970189702,
      "grad_norm": 12.221134185791016,
      "learning_rate": 7.892201144233666e-06,
      "loss": 0.7562,
      "step": 7483
    },
    {
      "epoch": 2.897406116918312,
      "grad_norm": 20.16607093811035,
      "learning_rate": 7.891770981201876e-06,
      "loss": 1.05,
      "step": 7484
    },
    {
      "epoch": 2.897793263646922,
      "grad_norm": 19.979936599731445,
      "learning_rate": 7.891340818170087e-06,
      "loss": 1.565,
      "step": 7485
    },
    {
      "epoch": 2.898180410375532,
      "grad_norm": 23.567182540893555,
      "learning_rate": 7.890910655138299e-06,
      "loss": 1.7748,
      "step": 7486
    },
    {
      "epoch": 2.8985675571041423,
      "grad_norm": 17.488971710205078,
      "learning_rate": 7.890480492106509e-06,
      "loss": 0.6863,
      "step": 7487
    },
    {
      "epoch": 2.8989547038327528,
      "grad_norm": 21.128328323364258,
      "learning_rate": 7.89005032907472e-06,
      "loss": 1.1669,
      "step": 7488
    },
    {
      "epoch": 2.899341850561363,
      "grad_norm": 11.873275756835938,
      "learning_rate": 7.889620166042931e-06,
      "loss": 0.7253,
      "step": 7489
    },
    {
      "epoch": 2.899728997289973,
      "grad_norm": 12.634366989135742,
      "learning_rate": 7.889190003011143e-06,
      "loss": 0.7151,
      "step": 7490
    },
    {
      "epoch": 2.900116144018583,
      "grad_norm": 23.262584686279297,
      "learning_rate": 7.888759839979352e-06,
      "loss": 0.9333,
      "step": 7491
    },
    {
      "epoch": 2.900503290747193,
      "grad_norm": 20.063323974609375,
      "learning_rate": 7.888329676947564e-06,
      "loss": 1.337,
      "step": 7492
    },
    {
      "epoch": 2.9008904374758036,
      "grad_norm": 17.58413314819336,
      "learning_rate": 7.887899513915774e-06,
      "loss": 1.4017,
      "step": 7493
    },
    {
      "epoch": 2.9012775842044136,
      "grad_norm": 23.38795280456543,
      "learning_rate": 7.887469350883987e-06,
      "loss": 1.0943,
      "step": 7494
    },
    {
      "epoch": 2.9016647309330237,
      "grad_norm": 95.04936218261719,
      "learning_rate": 7.887039187852196e-06,
      "loss": 1.0745,
      "step": 7495
    },
    {
      "epoch": 2.9020518776616337,
      "grad_norm": 11.728462219238281,
      "learning_rate": 7.886609024820408e-06,
      "loss": 1.0217,
      "step": 7496
    },
    {
      "epoch": 2.902439024390244,
      "grad_norm": 25.093082427978516,
      "learning_rate": 7.886178861788618e-06,
      "loss": 1.2632,
      "step": 7497
    },
    {
      "epoch": 2.902826171118854,
      "grad_norm": 14.93930435180664,
      "learning_rate": 7.88574869875683e-06,
      "loss": 0.8494,
      "step": 7498
    },
    {
      "epoch": 2.903213317847464,
      "grad_norm": 13.054893493652344,
      "learning_rate": 7.88531853572504e-06,
      "loss": 0.7627,
      "step": 7499
    },
    {
      "epoch": 2.9036004645760745,
      "grad_norm": 19.643661499023438,
      "learning_rate": 7.884888372693252e-06,
      "loss": 1.5553,
      "step": 7500
    },
    {
      "epoch": 2.9039876113046845,
      "grad_norm": 15.128693580627441,
      "learning_rate": 7.884458209661462e-06,
      "loss": 0.8883,
      "step": 7501
    },
    {
      "epoch": 2.9043747580332946,
      "grad_norm": 27.20090103149414,
      "learning_rate": 7.884028046629673e-06,
      "loss": 1.4964,
      "step": 7502
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 11.558565139770508,
      "learning_rate": 7.883597883597884e-06,
      "loss": 1.269,
      "step": 7503
    },
    {
      "epoch": 2.9051490514905147,
      "grad_norm": 21.486549377441406,
      "learning_rate": 7.883167720566096e-06,
      "loss": 1.1011,
      "step": 7504
    },
    {
      "epoch": 2.9055361982191252,
      "grad_norm": 32.09672546386719,
      "learning_rate": 7.882737557534306e-06,
      "loss": 1.433,
      "step": 7505
    },
    {
      "epoch": 2.9059233449477353,
      "grad_norm": 23.555675506591797,
      "learning_rate": 7.882307394502517e-06,
      "loss": 1.0852,
      "step": 7506
    },
    {
      "epoch": 2.9063104916763454,
      "grad_norm": 12.119900703430176,
      "learning_rate": 7.881877231470728e-06,
      "loss": 1.3776,
      "step": 7507
    },
    {
      "epoch": 2.9066976384049554,
      "grad_norm": 34.57555389404297,
      "learning_rate": 7.881447068438938e-06,
      "loss": 1.2389,
      "step": 7508
    },
    {
      "epoch": 2.9070847851335655,
      "grad_norm": 23.638946533203125,
      "learning_rate": 7.88101690540715e-06,
      "loss": 1.217,
      "step": 7509
    },
    {
      "epoch": 2.907471931862176,
      "grad_norm": 5.454827785491943,
      "learning_rate": 7.880586742375361e-06,
      "loss": 0.1575,
      "step": 7510
    },
    {
      "epoch": 2.907859078590786,
      "grad_norm": 13.115774154663086,
      "learning_rate": 7.880156579343572e-06,
      "loss": 1.2643,
      "step": 7511
    },
    {
      "epoch": 2.908246225319396,
      "grad_norm": 20.90462875366211,
      "learning_rate": 7.879726416311782e-06,
      "loss": 1.1403,
      "step": 7512
    },
    {
      "epoch": 2.908633372048006,
      "grad_norm": 31.74289321899414,
      "learning_rate": 7.879296253279993e-06,
      "loss": 1.8887,
      "step": 7513
    },
    {
      "epoch": 2.9090205187766163,
      "grad_norm": 26.842302322387695,
      "learning_rate": 7.878866090248205e-06,
      "loss": 1.4524,
      "step": 7514
    },
    {
      "epoch": 2.9094076655052263,
      "grad_norm": 10.93935775756836,
      "learning_rate": 7.878435927216416e-06,
      "loss": 0.7429,
      "step": 7515
    },
    {
      "epoch": 2.9097948122338364,
      "grad_norm": 27.72509765625,
      "learning_rate": 7.878005764184626e-06,
      "loss": 1.7527,
      "step": 7516
    },
    {
      "epoch": 2.910181958962447,
      "grad_norm": 43.674888610839844,
      "learning_rate": 7.877575601152837e-06,
      "loss": 0.778,
      "step": 7517
    },
    {
      "epoch": 2.910569105691057,
      "grad_norm": 13.179491996765137,
      "learning_rate": 7.877145438121049e-06,
      "loss": 1.0028,
      "step": 7518
    },
    {
      "epoch": 2.910956252419667,
      "grad_norm": 22.81224250793457,
      "learning_rate": 7.87671527508926e-06,
      "loss": 1.6759,
      "step": 7519
    },
    {
      "epoch": 2.911343399148277,
      "grad_norm": 18.877471923828125,
      "learning_rate": 7.87628511205747e-06,
      "loss": 1.3883,
      "step": 7520
    },
    {
      "epoch": 2.911730545876887,
      "grad_norm": 13.85293197631836,
      "learning_rate": 7.875854949025681e-06,
      "loss": 0.9021,
      "step": 7521
    },
    {
      "epoch": 2.9121176926054977,
      "grad_norm": 17.433425903320312,
      "learning_rate": 7.875424785993893e-06,
      "loss": 1.3168,
      "step": 7522
    },
    {
      "epoch": 2.9125048393341078,
      "grad_norm": 11.060133934020996,
      "learning_rate": 7.874994622962103e-06,
      "loss": 0.856,
      "step": 7523
    },
    {
      "epoch": 2.912891986062718,
      "grad_norm": 14.897860527038574,
      "learning_rate": 7.874564459930314e-06,
      "loss": 1.0181,
      "step": 7524
    },
    {
      "epoch": 2.913279132791328,
      "grad_norm": 15.942066192626953,
      "learning_rate": 7.874134296898525e-06,
      "loss": 1.4823,
      "step": 7525
    },
    {
      "epoch": 2.913666279519938,
      "grad_norm": 14.29617977142334,
      "learning_rate": 7.873704133866737e-06,
      "loss": 0.8085,
      "step": 7526
    },
    {
      "epoch": 2.9140534262485485,
      "grad_norm": 19.41498565673828,
      "learning_rate": 7.873273970834947e-06,
      "loss": 1.2368,
      "step": 7527
    },
    {
      "epoch": 2.914440572977158,
      "grad_norm": 12.142794609069824,
      "learning_rate": 7.872843807803158e-06,
      "loss": 1.2619,
      "step": 7528
    },
    {
      "epoch": 2.9148277197057686,
      "grad_norm": 37.76754379272461,
      "learning_rate": 7.87241364477137e-06,
      "loss": 2.905,
      "step": 7529
    },
    {
      "epoch": 2.9152148664343787,
      "grad_norm": 14.188928604125977,
      "learning_rate": 7.87198348173958e-06,
      "loss": 1.0277,
      "step": 7530
    },
    {
      "epoch": 2.9156020131629887,
      "grad_norm": 10.407779693603516,
      "learning_rate": 7.87155331870779e-06,
      "loss": 0.4562,
      "step": 7531
    },
    {
      "epoch": 2.915989159891599,
      "grad_norm": 16.525474548339844,
      "learning_rate": 7.871123155676002e-06,
      "loss": 1.2979,
      "step": 7532
    },
    {
      "epoch": 2.916376306620209,
      "grad_norm": 45.80781173706055,
      "learning_rate": 7.870692992644213e-06,
      "loss": 0.4794,
      "step": 7533
    },
    {
      "epoch": 2.9167634533488194,
      "grad_norm": 22.973691940307617,
      "learning_rate": 7.870262829612425e-06,
      "loss": 1.3431,
      "step": 7534
    },
    {
      "epoch": 2.9171506000774294,
      "grad_norm": 13.195252418518066,
      "learning_rate": 7.869832666580634e-06,
      "loss": 0.8953,
      "step": 7535
    },
    {
      "epoch": 2.9175377468060395,
      "grad_norm": 26.05246925354004,
      "learning_rate": 7.869402503548846e-06,
      "loss": 1.4006,
      "step": 7536
    },
    {
      "epoch": 2.9179248935346496,
      "grad_norm": 17.150325775146484,
      "learning_rate": 7.868972340517057e-06,
      "loss": 0.4781,
      "step": 7537
    },
    {
      "epoch": 2.9183120402632596,
      "grad_norm": 27.498619079589844,
      "learning_rate": 7.868542177485267e-06,
      "loss": 1.5495,
      "step": 7538
    },
    {
      "epoch": 2.91869918699187,
      "grad_norm": 20.835845947265625,
      "learning_rate": 7.868112014453478e-06,
      "loss": 1.7281,
      "step": 7539
    },
    {
      "epoch": 2.91908633372048,
      "grad_norm": 30.841064453125,
      "learning_rate": 7.86768185142169e-06,
      "loss": 1.5197,
      "step": 7540
    },
    {
      "epoch": 2.9194734804490903,
      "grad_norm": 24.75371551513672,
      "learning_rate": 7.867251688389901e-06,
      "loss": 1.4939,
      "step": 7541
    },
    {
      "epoch": 2.9198606271777003,
      "grad_norm": 20.541250228881836,
      "learning_rate": 7.866821525358111e-06,
      "loss": 1.6417,
      "step": 7542
    },
    {
      "epoch": 2.9202477739063104,
      "grad_norm": 22.2336368560791,
      "learning_rate": 7.866391362326322e-06,
      "loss": 1.7669,
      "step": 7543
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 26.327068328857422,
      "learning_rate": 7.865961199294532e-06,
      "loss": 1.5786,
      "step": 7544
    },
    {
      "epoch": 2.9210220673635305,
      "grad_norm": 12.935742378234863,
      "learning_rate": 7.865531036262745e-06,
      "loss": 0.8505,
      "step": 7545
    },
    {
      "epoch": 2.921409214092141,
      "grad_norm": 17.43747901916504,
      "learning_rate": 7.865100873230955e-06,
      "loss": 0.8768,
      "step": 7546
    },
    {
      "epoch": 2.921796360820751,
      "grad_norm": 21.99923324584961,
      "learning_rate": 7.864670710199166e-06,
      "loss": 1.5355,
      "step": 7547
    },
    {
      "epoch": 2.922183507549361,
      "grad_norm": 18.23906707763672,
      "learning_rate": 7.864240547167376e-06,
      "loss": 1.5093,
      "step": 7548
    },
    {
      "epoch": 2.9225706542779712,
      "grad_norm": 11.594046592712402,
      "learning_rate": 7.86381038413559e-06,
      "loss": 0.7764,
      "step": 7549
    },
    {
      "epoch": 2.9229578010065813,
      "grad_norm": 30.295896530151367,
      "learning_rate": 7.863380221103799e-06,
      "loss": 1.4435,
      "step": 7550
    },
    {
      "epoch": 2.923344947735192,
      "grad_norm": 21.262060165405273,
      "learning_rate": 7.86295005807201e-06,
      "loss": 0.5894,
      "step": 7551
    },
    {
      "epoch": 2.923732094463802,
      "grad_norm": 16.498138427734375,
      "learning_rate": 7.86251989504022e-06,
      "loss": 0.7282,
      "step": 7552
    },
    {
      "epoch": 2.924119241192412,
      "grad_norm": 29.364686965942383,
      "learning_rate": 7.862089732008431e-06,
      "loss": 0.7065,
      "step": 7553
    },
    {
      "epoch": 2.924506387921022,
      "grad_norm": 26.37733268737793,
      "learning_rate": 7.861659568976643e-06,
      "loss": 1.6267,
      "step": 7554
    },
    {
      "epoch": 2.924893534649632,
      "grad_norm": 45.55252456665039,
      "learning_rate": 7.861229405944854e-06,
      "loss": 0.9352,
      "step": 7555
    },
    {
      "epoch": 2.9252806813782426,
      "grad_norm": 21.764244079589844,
      "learning_rate": 7.860799242913064e-06,
      "loss": 1.2924,
      "step": 7556
    },
    {
      "epoch": 2.925667828106852,
      "grad_norm": 25.731975555419922,
      "learning_rate": 7.860369079881275e-06,
      "loss": 1.0084,
      "step": 7557
    },
    {
      "epoch": 2.9260549748354627,
      "grad_norm": 18.687742233276367,
      "learning_rate": 7.859938916849487e-06,
      "loss": 1.3695,
      "step": 7558
    },
    {
      "epoch": 2.926442121564073,
      "grad_norm": 59.554595947265625,
      "learning_rate": 7.859508753817697e-06,
      "loss": 1.2751,
      "step": 7559
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 41.83247756958008,
      "learning_rate": 7.859078590785908e-06,
      "loss": 1.5781,
      "step": 7560
    },
    {
      "epoch": 2.927216415021293,
      "grad_norm": 22.513748168945312,
      "learning_rate": 7.85864842775412e-06,
      "loss": 1.7862,
      "step": 7561
    },
    {
      "epoch": 2.927603561749903,
      "grad_norm": 89.81108856201172,
      "learning_rate": 7.85821826472233e-06,
      "loss": 1.3373,
      "step": 7562
    },
    {
      "epoch": 2.9279907084785135,
      "grad_norm": 30.43745231628418,
      "learning_rate": 7.85778810169054e-06,
      "loss": 1.3471,
      "step": 7563
    },
    {
      "epoch": 2.9283778552071236,
      "grad_norm": 14.35595703125,
      "learning_rate": 7.857357938658754e-06,
      "loss": 0.9909,
      "step": 7564
    },
    {
      "epoch": 2.9287650019357336,
      "grad_norm": 23.521522521972656,
      "learning_rate": 7.856927775626963e-06,
      "loss": 2.2075,
      "step": 7565
    },
    {
      "epoch": 2.9291521486643437,
      "grad_norm": 34.04800796508789,
      "learning_rate": 7.856497612595175e-06,
      "loss": 1.0886,
      "step": 7566
    },
    {
      "epoch": 2.9295392953929538,
      "grad_norm": 26.37222671508789,
      "learning_rate": 7.856067449563385e-06,
      "loss": 2.2333,
      "step": 7567
    },
    {
      "epoch": 2.9299264421215643,
      "grad_norm": 16.62346839904785,
      "learning_rate": 7.855637286531596e-06,
      "loss": 0.8029,
      "step": 7568
    },
    {
      "epoch": 2.9303135888501743,
      "grad_norm": 4.348937034606934,
      "learning_rate": 7.855207123499807e-06,
      "loss": 0.2435,
      "step": 7569
    },
    {
      "epoch": 2.9307007355787844,
      "grad_norm": 20.297374725341797,
      "learning_rate": 7.854776960468019e-06,
      "loss": 1.0316,
      "step": 7570
    },
    {
      "epoch": 2.9310878823073945,
      "grad_norm": 12.813425064086914,
      "learning_rate": 7.854346797436228e-06,
      "loss": 0.5537,
      "step": 7571
    },
    {
      "epoch": 2.9314750290360045,
      "grad_norm": 10.68740177154541,
      "learning_rate": 7.85391663440444e-06,
      "loss": 0.6214,
      "step": 7572
    },
    {
      "epoch": 2.931862175764615,
      "grad_norm": 28.381267547607422,
      "learning_rate": 7.853486471372651e-06,
      "loss": 1.123,
      "step": 7573
    },
    {
      "epoch": 2.9322493224932247,
      "grad_norm": 29.72842025756836,
      "learning_rate": 7.853056308340861e-06,
      "loss": 1.1387,
      "step": 7574
    },
    {
      "epoch": 2.932636469221835,
      "grad_norm": 23.9423828125,
      "learning_rate": 7.852626145309072e-06,
      "loss": 1.2894,
      "step": 7575
    },
    {
      "epoch": 2.9330236159504453,
      "grad_norm": 11.93856430053711,
      "learning_rate": 7.852195982277284e-06,
      "loss": 0.7153,
      "step": 7576
    },
    {
      "epoch": 2.9334107626790553,
      "grad_norm": 58.82192611694336,
      "learning_rate": 7.851765819245495e-06,
      "loss": 1.3739,
      "step": 7577
    },
    {
      "epoch": 2.9337979094076654,
      "grad_norm": 15.57844066619873,
      "learning_rate": 7.851335656213705e-06,
      "loss": 0.8959,
      "step": 7578
    },
    {
      "epoch": 2.9341850561362754,
      "grad_norm": 14.428125381469727,
      "learning_rate": 7.850905493181916e-06,
      "loss": 1.5347,
      "step": 7579
    },
    {
      "epoch": 2.934572202864886,
      "grad_norm": 88.04529571533203,
      "learning_rate": 7.850475330150128e-06,
      "loss": 1.6636,
      "step": 7580
    },
    {
      "epoch": 2.934959349593496,
      "grad_norm": 16.810909271240234,
      "learning_rate": 7.85004516711834e-06,
      "loss": 1.2325,
      "step": 7581
    },
    {
      "epoch": 2.935346496322106,
      "grad_norm": 23.160694122314453,
      "learning_rate": 7.849615004086549e-06,
      "loss": 1.3442,
      "step": 7582
    },
    {
      "epoch": 2.935733643050716,
      "grad_norm": 22.55115509033203,
      "learning_rate": 7.84918484105476e-06,
      "loss": 1.6082,
      "step": 7583
    },
    {
      "epoch": 2.9361207897793262,
      "grad_norm": 15.965059280395508,
      "learning_rate": 7.848754678022972e-06,
      "loss": 1.1613,
      "step": 7584
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 12.494799613952637,
      "learning_rate": 7.848324514991183e-06,
      "loss": 0.7396,
      "step": 7585
    },
    {
      "epoch": 2.936895083236547,
      "grad_norm": 16.887178421020508,
      "learning_rate": 7.847894351959393e-06,
      "loss": 1.2961,
      "step": 7586
    },
    {
      "epoch": 2.937282229965157,
      "grad_norm": 13.349729537963867,
      "learning_rate": 7.847464188927604e-06,
      "loss": 1.1177,
      "step": 7587
    },
    {
      "epoch": 2.937669376693767,
      "grad_norm": 10.055732727050781,
      "learning_rate": 7.847034025895816e-06,
      "loss": 0.447,
      "step": 7588
    },
    {
      "epoch": 2.938056523422377,
      "grad_norm": 70.28915405273438,
      "learning_rate": 7.846603862864025e-06,
      "loss": 1.7503,
      "step": 7589
    },
    {
      "epoch": 2.938443670150987,
      "grad_norm": 21.154338836669922,
      "learning_rate": 7.846173699832237e-06,
      "loss": 1.5707,
      "step": 7590
    },
    {
      "epoch": 2.938830816879597,
      "grad_norm": 10.255411148071289,
      "learning_rate": 7.845743536800448e-06,
      "loss": 1.2872,
      "step": 7591
    },
    {
      "epoch": 2.9392179636082076,
      "grad_norm": 52.86012649536133,
      "learning_rate": 7.84531337376866e-06,
      "loss": 2.6864,
      "step": 7592
    },
    {
      "epoch": 2.9396051103368177,
      "grad_norm": 17.550188064575195,
      "learning_rate": 7.84488321073687e-06,
      "loss": 1.9115,
      "step": 7593
    },
    {
      "epoch": 2.9399922570654278,
      "grad_norm": 23.32989501953125,
      "learning_rate": 7.844453047705081e-06,
      "loss": 0.9718,
      "step": 7594
    },
    {
      "epoch": 2.940379403794038,
      "grad_norm": 8.765739440917969,
      "learning_rate": 7.84402288467329e-06,
      "loss": 0.4059,
      "step": 7595
    },
    {
      "epoch": 2.940766550522648,
      "grad_norm": 9.236712455749512,
      "learning_rate": 7.843592721641504e-06,
      "loss": 0.34,
      "step": 7596
    },
    {
      "epoch": 2.9411536972512584,
      "grad_norm": 35.89055252075195,
      "learning_rate": 7.843162558609713e-06,
      "loss": 2.7863,
      "step": 7597
    },
    {
      "epoch": 2.9415408439798685,
      "grad_norm": 6.481848239898682,
      "learning_rate": 7.842732395577925e-06,
      "loss": 0.3542,
      "step": 7598
    },
    {
      "epoch": 2.9419279907084785,
      "grad_norm": 17.52792739868164,
      "learning_rate": 7.842302232546135e-06,
      "loss": 1.0741,
      "step": 7599
    },
    {
      "epoch": 2.9423151374370886,
      "grad_norm": 43.391151428222656,
      "learning_rate": 7.841872069514348e-06,
      "loss": 2.3014,
      "step": 7600
    },
    {
      "epoch": 2.9427022841656987,
      "grad_norm": 17.087888717651367,
      "learning_rate": 7.841441906482557e-06,
      "loss": 1.7008,
      "step": 7601
    },
    {
      "epoch": 2.943089430894309,
      "grad_norm": 13.946194648742676,
      "learning_rate": 7.841011743450769e-06,
      "loss": 1.0133,
      "step": 7602
    },
    {
      "epoch": 2.943476577622919,
      "grad_norm": 22.17657470703125,
      "learning_rate": 7.84058158041898e-06,
      "loss": 2.7914,
      "step": 7603
    },
    {
      "epoch": 2.9438637243515293,
      "grad_norm": 25.69478988647461,
      "learning_rate": 7.84015141738719e-06,
      "loss": 0.9551,
      "step": 7604
    },
    {
      "epoch": 2.9442508710801394,
      "grad_norm": 25.71767807006836,
      "learning_rate": 7.839721254355401e-06,
      "loss": 2.1437,
      "step": 7605
    },
    {
      "epoch": 2.9446380178087495,
      "grad_norm": 14.32331657409668,
      "learning_rate": 7.839291091323613e-06,
      "loss": 0.9589,
      "step": 7606
    },
    {
      "epoch": 2.9450251645373595,
      "grad_norm": 17.4555606842041,
      "learning_rate": 7.838860928291824e-06,
      "loss": 1.3935,
      "step": 7607
    },
    {
      "epoch": 2.9454123112659696,
      "grad_norm": 21.628738403320312,
      "learning_rate": 7.838430765260034e-06,
      "loss": 1.6593,
      "step": 7608
    },
    {
      "epoch": 2.94579945799458,
      "grad_norm": 47.01352310180664,
      "learning_rate": 7.838000602228245e-06,
      "loss": 1.3904,
      "step": 7609
    },
    {
      "epoch": 2.94618660472319,
      "grad_norm": 26.731470108032227,
      "learning_rate": 7.837570439196455e-06,
      "loss": 1.0077,
      "step": 7610
    },
    {
      "epoch": 2.9465737514518002,
      "grad_norm": 77.44677734375,
      "learning_rate": 7.837140276164668e-06,
      "loss": 1.4165,
      "step": 7611
    },
    {
      "epoch": 2.9469608981804103,
      "grad_norm": 25.297651290893555,
      "learning_rate": 7.836710113132878e-06,
      "loss": 2.1059,
      "step": 7612
    },
    {
      "epoch": 2.9473480449090204,
      "grad_norm": 12.035430908203125,
      "learning_rate": 7.83627995010109e-06,
      "loss": 0.8245,
      "step": 7613
    },
    {
      "epoch": 2.947735191637631,
      "grad_norm": 20.51219367980957,
      "learning_rate": 7.835849787069299e-06,
      "loss": 1.9322,
      "step": 7614
    },
    {
      "epoch": 2.948122338366241,
      "grad_norm": 49.84027862548828,
      "learning_rate": 7.835419624037512e-06,
      "loss": 1.0444,
      "step": 7615
    },
    {
      "epoch": 2.948509485094851,
      "grad_norm": 26.643211364746094,
      "learning_rate": 7.834989461005722e-06,
      "loss": 0.9861,
      "step": 7616
    },
    {
      "epoch": 2.948896631823461,
      "grad_norm": 16.239465713500977,
      "learning_rate": 7.834559297973933e-06,
      "loss": 2.7917,
      "step": 7617
    },
    {
      "epoch": 2.949283778552071,
      "grad_norm": 24.668933868408203,
      "learning_rate": 7.834129134942143e-06,
      "loss": 1.5427,
      "step": 7618
    },
    {
      "epoch": 2.9496709252806816,
      "grad_norm": 12.775620460510254,
      "learning_rate": 7.833698971910354e-06,
      "loss": 0.705,
      "step": 7619
    },
    {
      "epoch": 2.9500580720092913,
      "grad_norm": 23.898914337158203,
      "learning_rate": 7.833268808878566e-06,
      "loss": 1.3836,
      "step": 7620
    },
    {
      "epoch": 2.9504452187379018,
      "grad_norm": 11.41406536102295,
      "learning_rate": 7.832838645846777e-06,
      "loss": 0.7207,
      "step": 7621
    },
    {
      "epoch": 2.950832365466512,
      "grad_norm": 16.80011749267578,
      "learning_rate": 7.832408482814987e-06,
      "loss": 1.1277,
      "step": 7622
    },
    {
      "epoch": 2.951219512195122,
      "grad_norm": 34.51325988769531,
      "learning_rate": 7.831978319783198e-06,
      "loss": 1.4443,
      "step": 7623
    },
    {
      "epoch": 2.951606658923732,
      "grad_norm": 44.85136413574219,
      "learning_rate": 7.83154815675141e-06,
      "loss": 1.6768,
      "step": 7624
    },
    {
      "epoch": 2.951993805652342,
      "grad_norm": 11.292993545532227,
      "learning_rate": 7.83111799371962e-06,
      "loss": 0.8313,
      "step": 7625
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 45.00840759277344,
      "learning_rate": 7.830687830687831e-06,
      "loss": 1.8899,
      "step": 7626
    },
    {
      "epoch": 2.9527680991095626,
      "grad_norm": 26.419065475463867,
      "learning_rate": 7.830257667656042e-06,
      "loss": 1.7738,
      "step": 7627
    },
    {
      "epoch": 2.9531552458381727,
      "grad_norm": 30.476673126220703,
      "learning_rate": 7.829827504624254e-06,
      "loss": 2.0102,
      "step": 7628
    },
    {
      "epoch": 2.9535423925667827,
      "grad_norm": 69.88998413085938,
      "learning_rate": 7.829397341592463e-06,
      "loss": 3.299,
      "step": 7629
    },
    {
      "epoch": 2.953929539295393,
      "grad_norm": 15.567957878112793,
      "learning_rate": 7.828967178560675e-06,
      "loss": 1.4363,
      "step": 7630
    },
    {
      "epoch": 2.9543166860240033,
      "grad_norm": 17.10489273071289,
      "learning_rate": 7.828537015528886e-06,
      "loss": 0.9096,
      "step": 7631
    },
    {
      "epoch": 2.9547038327526134,
      "grad_norm": 11.594610214233398,
      "learning_rate": 7.828106852497098e-06,
      "loss": 0.9812,
      "step": 7632
    },
    {
      "epoch": 2.9550909794812235,
      "grad_norm": 27.542848587036133,
      "learning_rate": 7.827676689465307e-06,
      "loss": 1.4238,
      "step": 7633
    },
    {
      "epoch": 2.9554781262098335,
      "grad_norm": 19.786611557006836,
      "learning_rate": 7.827246526433519e-06,
      "loss": 1.6653,
      "step": 7634
    },
    {
      "epoch": 2.9558652729384436,
      "grad_norm": 31.399694442749023,
      "learning_rate": 7.82681636340173e-06,
      "loss": 1.5543,
      "step": 7635
    },
    {
      "epoch": 2.9562524196670537,
      "grad_norm": 28.331134796142578,
      "learning_rate": 7.826386200369942e-06,
      "loss": 0.9998,
      "step": 7636
    },
    {
      "epoch": 2.9566395663956637,
      "grad_norm": 59.111331939697266,
      "learning_rate": 7.825956037338151e-06,
      "loss": 1.0387,
      "step": 7637
    },
    {
      "epoch": 2.9570267131242742,
      "grad_norm": 73.40115356445312,
      "learning_rate": 7.825525874306363e-06,
      "loss": 1.5984,
      "step": 7638
    },
    {
      "epoch": 2.9574138598528843,
      "grad_norm": 148.47483825683594,
      "learning_rate": 7.825095711274574e-06,
      "loss": 1.8602,
      "step": 7639
    },
    {
      "epoch": 2.9578010065814944,
      "grad_norm": 15.824188232421875,
      "learning_rate": 7.824665548242784e-06,
      "loss": 1.0164,
      "step": 7640
    },
    {
      "epoch": 2.9581881533101044,
      "grad_norm": 35.717254638671875,
      "learning_rate": 7.824235385210995e-06,
      "loss": 1.3575,
      "step": 7641
    },
    {
      "epoch": 2.9585753000387145,
      "grad_norm": 25.066896438598633,
      "learning_rate": 7.823805222179207e-06,
      "loss": 2.0715,
      "step": 7642
    },
    {
      "epoch": 2.958962446767325,
      "grad_norm": 82.25820922851562,
      "learning_rate": 7.823375059147418e-06,
      "loss": 1.4676,
      "step": 7643
    },
    {
      "epoch": 2.959349593495935,
      "grad_norm": 11.589202880859375,
      "learning_rate": 7.822944896115628e-06,
      "loss": 0.7655,
      "step": 7644
    },
    {
      "epoch": 2.959736740224545,
      "grad_norm": 5.196530342102051,
      "learning_rate": 7.82251473308384e-06,
      "loss": 0.2734,
      "step": 7645
    },
    {
      "epoch": 2.960123886953155,
      "grad_norm": 17.273422241210938,
      "learning_rate": 7.82208457005205e-06,
      "loss": 0.8208,
      "step": 7646
    },
    {
      "epoch": 2.9605110336817653,
      "grad_norm": 44.78836441040039,
      "learning_rate": 7.821654407020262e-06,
      "loss": 2.3593,
      "step": 7647
    },
    {
      "epoch": 2.960898180410376,
      "grad_norm": 22.03194808959961,
      "learning_rate": 7.821224243988472e-06,
      "loss": 1.339,
      "step": 7648
    },
    {
      "epoch": 2.9612853271389854,
      "grad_norm": 19.275638580322266,
      "learning_rate": 7.820794080956683e-06,
      "loss": 2.9706,
      "step": 7649
    },
    {
      "epoch": 2.961672473867596,
      "grad_norm": 43.652374267578125,
      "learning_rate": 7.820363917924895e-06,
      "loss": 1.6197,
      "step": 7650
    },
    {
      "epoch": 2.962059620596206,
      "grad_norm": 9.240982055664062,
      "learning_rate": 7.819933754893106e-06,
      "loss": 0.586,
      "step": 7651
    },
    {
      "epoch": 2.962446767324816,
      "grad_norm": 77.10714721679688,
      "learning_rate": 7.819503591861316e-06,
      "loss": 2.0924,
      "step": 7652
    },
    {
      "epoch": 2.962833914053426,
      "grad_norm": 16.98588752746582,
      "learning_rate": 7.819073428829527e-06,
      "loss": 1.1673,
      "step": 7653
    },
    {
      "epoch": 2.963221060782036,
      "grad_norm": 15.91903018951416,
      "learning_rate": 7.818643265797739e-06,
      "loss": 1.5038,
      "step": 7654
    },
    {
      "epoch": 2.9636082075106467,
      "grad_norm": 82.9590835571289,
      "learning_rate": 7.818213102765948e-06,
      "loss": 2.3049,
      "step": 7655
    },
    {
      "epoch": 2.9639953542392568,
      "grad_norm": 15.006303787231445,
      "learning_rate": 7.81778293973416e-06,
      "loss": 0.9673,
      "step": 7656
    },
    {
      "epoch": 2.964382500967867,
      "grad_norm": 27.301868438720703,
      "learning_rate": 7.817352776702371e-06,
      "loss": 0.1368,
      "step": 7657
    },
    {
      "epoch": 2.964769647696477,
      "grad_norm": 12.772469520568848,
      "learning_rate": 7.816922613670583e-06,
      "loss": 0.7674,
      "step": 7658
    },
    {
      "epoch": 2.965156794425087,
      "grad_norm": 9.646075248718262,
      "learning_rate": 7.816492450638792e-06,
      "loss": 0.6115,
      "step": 7659
    },
    {
      "epoch": 2.9655439411536975,
      "grad_norm": 23.816368103027344,
      "learning_rate": 7.816062287607004e-06,
      "loss": 1.2671,
      "step": 7660
    },
    {
      "epoch": 2.9659310878823075,
      "grad_norm": 14.153557777404785,
      "learning_rate": 7.815632124575214e-06,
      "loss": 1.4802,
      "step": 7661
    },
    {
      "epoch": 2.9663182346109176,
      "grad_norm": 27.313461303710938,
      "learning_rate": 7.815201961543427e-06,
      "loss": 1.968,
      "step": 7662
    },
    {
      "epoch": 2.9667053813395277,
      "grad_norm": 32.77643966674805,
      "learning_rate": 7.814771798511636e-06,
      "loss": 2.0709,
      "step": 7663
    },
    {
      "epoch": 2.9670925280681377,
      "grad_norm": 18.505828857421875,
      "learning_rate": 7.814341635479848e-06,
      "loss": 0.8473,
      "step": 7664
    },
    {
      "epoch": 2.9674796747967482,
      "grad_norm": 21.427196502685547,
      "learning_rate": 7.813911472448058e-06,
      "loss": 1.5508,
      "step": 7665
    },
    {
      "epoch": 2.967866821525358,
      "grad_norm": 28.640403747558594,
      "learning_rate": 7.81348130941627e-06,
      "loss": 1.4248,
      "step": 7666
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 47.117984771728516,
      "learning_rate": 7.81305114638448e-06,
      "loss": 2.0329,
      "step": 7667
    },
    {
      "epoch": 2.9686411149825784,
      "grad_norm": 18.83332061767578,
      "learning_rate": 7.812620983352692e-06,
      "loss": 0.9712,
      "step": 7668
    },
    {
      "epoch": 2.9690282617111885,
      "grad_norm": 5.2791242599487305,
      "learning_rate": 7.812190820320901e-06,
      "loss": 0.1835,
      "step": 7669
    },
    {
      "epoch": 2.9694154084397986,
      "grad_norm": 12.313138008117676,
      "learning_rate": 7.811760657289113e-06,
      "loss": 0.7042,
      "step": 7670
    },
    {
      "epoch": 2.9698025551684086,
      "grad_norm": 16.474143981933594,
      "learning_rate": 7.811330494257324e-06,
      "loss": 1.0089,
      "step": 7671
    },
    {
      "epoch": 2.970189701897019,
      "grad_norm": 36.530189514160156,
      "learning_rate": 7.810900331225536e-06,
      "loss": 2.2292,
      "step": 7672
    },
    {
      "epoch": 2.970576848625629,
      "grad_norm": 38.242515563964844,
      "learning_rate": 7.810470168193745e-06,
      "loss": 1.4064,
      "step": 7673
    },
    {
      "epoch": 2.9709639953542393,
      "grad_norm": 21.60844612121582,
      "learning_rate": 7.810040005161957e-06,
      "loss": 1.2989,
      "step": 7674
    },
    {
      "epoch": 2.9713511420828493,
      "grad_norm": 17.23124122619629,
      "learning_rate": 7.809609842130168e-06,
      "loss": 1.6613,
      "step": 7675
    },
    {
      "epoch": 2.9717382888114594,
      "grad_norm": 29.38550567626953,
      "learning_rate": 7.809179679098378e-06,
      "loss": 1.7785,
      "step": 7676
    },
    {
      "epoch": 2.97212543554007,
      "grad_norm": 36.084529876708984,
      "learning_rate": 7.80874951606659e-06,
      "loss": 1.7782,
      "step": 7677
    },
    {
      "epoch": 2.97251258226868,
      "grad_norm": 10.927180290222168,
      "learning_rate": 7.8083193530348e-06,
      "loss": 0.9234,
      "step": 7678
    },
    {
      "epoch": 2.97289972899729,
      "grad_norm": 19.092378616333008,
      "learning_rate": 7.807889190003012e-06,
      "loss": 1.2638,
      "step": 7679
    },
    {
      "epoch": 2.9732868757259,
      "grad_norm": 23.379833221435547,
      "learning_rate": 7.807459026971222e-06,
      "loss": 1.8974,
      "step": 7680
    },
    {
      "epoch": 2.97367402245451,
      "grad_norm": 20.28824806213379,
      "learning_rate": 7.807028863939433e-06,
      "loss": 1.0433,
      "step": 7681
    },
    {
      "epoch": 2.9740611691831202,
      "grad_norm": 19.18601417541504,
      "learning_rate": 7.806598700907645e-06,
      "loss": 1.1779,
      "step": 7682
    },
    {
      "epoch": 2.9744483159117303,
      "grad_norm": 129.82688903808594,
      "learning_rate": 7.806168537875856e-06,
      "loss": 1.384,
      "step": 7683
    },
    {
      "epoch": 2.974835462640341,
      "grad_norm": 43.131858825683594,
      "learning_rate": 7.805738374844066e-06,
      "loss": 1.4995,
      "step": 7684
    },
    {
      "epoch": 2.975222609368951,
      "grad_norm": 17.70462417602539,
      "learning_rate": 7.805308211812277e-06,
      "loss": 1.6879,
      "step": 7685
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 15.093474388122559,
      "learning_rate": 7.804878048780489e-06,
      "loss": 1.1208,
      "step": 7686
    },
    {
      "epoch": 2.975996902826171,
      "grad_norm": 16.016138076782227,
      "learning_rate": 7.8044478857487e-06,
      "loss": 1.5023,
      "step": 7687
    },
    {
      "epoch": 2.976384049554781,
      "grad_norm": 29.931941986083984,
      "learning_rate": 7.80401772271691e-06,
      "loss": 1.4179,
      "step": 7688
    },
    {
      "epoch": 2.9767711962833916,
      "grad_norm": 21.727596282958984,
      "learning_rate": 7.803587559685121e-06,
      "loss": 1.5661,
      "step": 7689
    },
    {
      "epoch": 2.9771583430120017,
      "grad_norm": 28.548112869262695,
      "learning_rate": 7.803157396653333e-06,
      "loss": 1.3783,
      "step": 7690
    },
    {
      "epoch": 2.9775454897406117,
      "grad_norm": 12.934340476989746,
      "learning_rate": 7.802727233621542e-06,
      "loss": 0.8766,
      "step": 7691
    },
    {
      "epoch": 2.977932636469222,
      "grad_norm": 23.93756866455078,
      "learning_rate": 7.802297070589754e-06,
      "loss": 1.0129,
      "step": 7692
    },
    {
      "epoch": 2.978319783197832,
      "grad_norm": 22.113100051879883,
      "learning_rate": 7.801866907557965e-06,
      "loss": 1.6293,
      "step": 7693
    },
    {
      "epoch": 2.9787069299264424,
      "grad_norm": 22.02153968811035,
      "learning_rate": 7.801436744526177e-06,
      "loss": 1.2684,
      "step": 7694
    },
    {
      "epoch": 2.979094076655052,
      "grad_norm": 43.82540512084961,
      "learning_rate": 7.801006581494386e-06,
      "loss": 0.9396,
      "step": 7695
    },
    {
      "epoch": 2.9794812233836625,
      "grad_norm": 32.137962341308594,
      "learning_rate": 7.800576418462598e-06,
      "loss": 1.8266,
      "step": 7696
    },
    {
      "epoch": 2.9798683701122726,
      "grad_norm": 38.128028869628906,
      "learning_rate": 7.80014625543081e-06,
      "loss": 2.2733,
      "step": 7697
    },
    {
      "epoch": 2.9802555168408826,
      "grad_norm": 31.798913955688477,
      "learning_rate": 7.79971609239902e-06,
      "loss": 1.5281,
      "step": 7698
    },
    {
      "epoch": 2.9806426635694927,
      "grad_norm": 17.188316345214844,
      "learning_rate": 7.79928592936723e-06,
      "loss": 1.5719,
      "step": 7699
    },
    {
      "epoch": 2.9810298102981028,
      "grad_norm": 54.26243591308594,
      "learning_rate": 7.798855766335442e-06,
      "loss": 1.1134,
      "step": 7700
    },
    {
      "epoch": 2.9814169570267133,
      "grad_norm": 13.169770240783691,
      "learning_rate": 7.798425603303653e-06,
      "loss": 1.2909,
      "step": 7701
    },
    {
      "epoch": 2.9818041037553233,
      "grad_norm": 26.270004272460938,
      "learning_rate": 7.797995440271865e-06,
      "loss": 1.6159,
      "step": 7702
    },
    {
      "epoch": 2.9821912504839334,
      "grad_norm": 17.638534545898438,
      "learning_rate": 7.797565277240074e-06,
      "loss": 1.2895,
      "step": 7703
    },
    {
      "epoch": 2.9825783972125435,
      "grad_norm": 26.850343704223633,
      "learning_rate": 7.797135114208286e-06,
      "loss": 1.9032,
      "step": 7704
    },
    {
      "epoch": 2.9829655439411535,
      "grad_norm": 41.008819580078125,
      "learning_rate": 7.796704951176497e-06,
      "loss": 1.451,
      "step": 7705
    },
    {
      "epoch": 2.983352690669764,
      "grad_norm": 35.8395881652832,
      "learning_rate": 7.796274788144707e-06,
      "loss": 0.8619,
      "step": 7706
    },
    {
      "epoch": 2.983739837398374,
      "grad_norm": 16.528902053833008,
      "learning_rate": 7.795844625112918e-06,
      "loss": 1.5121,
      "step": 7707
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 10.790200233459473,
      "learning_rate": 7.79541446208113e-06,
      "loss": 0.7091,
      "step": 7708
    },
    {
      "epoch": 2.9845141308555942,
      "grad_norm": 17.019336700439453,
      "learning_rate": 7.794984299049341e-06,
      "loss": 1.7264,
      "step": 7709
    },
    {
      "epoch": 2.9849012775842043,
      "grad_norm": 25.079681396484375,
      "learning_rate": 7.794554136017551e-06,
      "loss": 1.9034,
      "step": 7710
    },
    {
      "epoch": 2.985288424312815,
      "grad_norm": 18.498109817504883,
      "learning_rate": 7.794123972985762e-06,
      "loss": 1.1993,
      "step": 7711
    },
    {
      "epoch": 2.9856755710414244,
      "grad_norm": 25.541589736938477,
      "learning_rate": 7.793693809953972e-06,
      "loss": 1.5349,
      "step": 7712
    },
    {
      "epoch": 2.986062717770035,
      "grad_norm": 40.90619659423828,
      "learning_rate": 7.793263646922185e-06,
      "loss": 1.0795,
      "step": 7713
    },
    {
      "epoch": 2.986449864498645,
      "grad_norm": 30.693634033203125,
      "learning_rate": 7.792833483890395e-06,
      "loss": 2.4962,
      "step": 7714
    },
    {
      "epoch": 2.986837011227255,
      "grad_norm": 30.30902099609375,
      "learning_rate": 7.792403320858606e-06,
      "loss": 0.527,
      "step": 7715
    },
    {
      "epoch": 2.987224157955865,
      "grad_norm": 33.825889587402344,
      "learning_rate": 7.791973157826816e-06,
      "loss": 1.3322,
      "step": 7716
    },
    {
      "epoch": 2.987611304684475,
      "grad_norm": 24.192195892333984,
      "learning_rate": 7.791542994795029e-06,
      "loss": 1.4953,
      "step": 7717
    },
    {
      "epoch": 2.9879984514130857,
      "grad_norm": 24.882539749145508,
      "learning_rate": 7.791112831763239e-06,
      "loss": 1.6958,
      "step": 7718
    },
    {
      "epoch": 2.988385598141696,
      "grad_norm": 18.953907012939453,
      "learning_rate": 7.79068266873145e-06,
      "loss": 0.7709,
      "step": 7719
    },
    {
      "epoch": 2.988772744870306,
      "grad_norm": 11.8904390335083,
      "learning_rate": 7.79025250569966e-06,
      "loss": 0.8275,
      "step": 7720
    },
    {
      "epoch": 2.989159891598916,
      "grad_norm": 18.810344696044922,
      "learning_rate": 7.789822342667871e-06,
      "loss": 1.6795,
      "step": 7721
    },
    {
      "epoch": 2.989547038327526,
      "grad_norm": 18.047544479370117,
      "learning_rate": 7.789392179636083e-06,
      "loss": 2.0353,
      "step": 7722
    },
    {
      "epoch": 2.9899341850561365,
      "grad_norm": 28.835813522338867,
      "learning_rate": 7.788962016604294e-06,
      "loss": 1.639,
      "step": 7723
    },
    {
      "epoch": 2.9903213317847466,
      "grad_norm": 44.48983383178711,
      "learning_rate": 7.788531853572504e-06,
      "loss": 4.9751,
      "step": 7724
    },
    {
      "epoch": 2.9907084785133566,
      "grad_norm": 16.88945198059082,
      "learning_rate": 7.788101690540715e-06,
      "loss": 1.1643,
      "step": 7725
    },
    {
      "epoch": 2.9910956252419667,
      "grad_norm": 11.621156692504883,
      "learning_rate": 7.787671527508927e-06,
      "loss": 0.8114,
      "step": 7726
    },
    {
      "epoch": 2.9914827719705768,
      "grad_norm": 20.28190803527832,
      "learning_rate": 7.787241364477136e-06,
      "loss": 1.4851,
      "step": 7727
    },
    {
      "epoch": 2.991869918699187,
      "grad_norm": 14.883934020996094,
      "learning_rate": 7.78681120144535e-06,
      "loss": 1.2436,
      "step": 7728
    },
    {
      "epoch": 2.992257065427797,
      "grad_norm": 24.936351776123047,
      "learning_rate": 7.78638103841356e-06,
      "loss": 0.7104,
      "step": 7729
    },
    {
      "epoch": 2.9926442121564074,
      "grad_norm": 50.67856216430664,
      "learning_rate": 7.78595087538177e-06,
      "loss": 1.6678,
      "step": 7730
    },
    {
      "epoch": 2.9930313588850175,
      "grad_norm": 14.979793548583984,
      "learning_rate": 7.78552071234998e-06,
      "loss": 0.9234,
      "step": 7731
    },
    {
      "epoch": 2.9934185056136275,
      "grad_norm": 29.643400192260742,
      "learning_rate": 7.785090549318194e-06,
      "loss": 2.0833,
      "step": 7732
    },
    {
      "epoch": 2.9938056523422376,
      "grad_norm": 18.159196853637695,
      "learning_rate": 7.784660386286403e-06,
      "loss": 2.1282,
      "step": 7733
    },
    {
      "epoch": 2.9941927990708477,
      "grad_norm": 17.988203048706055,
      "learning_rate": 7.784230223254615e-06,
      "loss": 1.671,
      "step": 7734
    },
    {
      "epoch": 2.994579945799458,
      "grad_norm": 31.355192184448242,
      "learning_rate": 7.783800060222824e-06,
      "loss": 2.0197,
      "step": 7735
    },
    {
      "epoch": 2.9949670925280683,
      "grad_norm": 21.594505310058594,
      "learning_rate": 7.783369897191036e-06,
      "loss": 1.342,
      "step": 7736
    },
    {
      "epoch": 2.9953542392566783,
      "grad_norm": 8.880645751953125,
      "learning_rate": 7.782939734159247e-06,
      "loss": 0.409,
      "step": 7737
    },
    {
      "epoch": 2.9957413859852884,
      "grad_norm": 21.901714324951172,
      "learning_rate": 7.782509571127459e-06,
      "loss": 1.6835,
      "step": 7738
    },
    {
      "epoch": 2.9961285327138985,
      "grad_norm": 16.139680862426758,
      "learning_rate": 7.782079408095668e-06,
      "loss": 1.0025,
      "step": 7739
    },
    {
      "epoch": 2.996515679442509,
      "grad_norm": 29.698129653930664,
      "learning_rate": 7.78164924506388e-06,
      "loss": 2.5466,
      "step": 7740
    },
    {
      "epoch": 2.9969028261711186,
      "grad_norm": 37.663516998291016,
      "learning_rate": 7.781219082032091e-06,
      "loss": 1.6845,
      "step": 7741
    },
    {
      "epoch": 2.997289972899729,
      "grad_norm": 23.102161407470703,
      "learning_rate": 7.780788919000301e-06,
      "loss": 1.3012,
      "step": 7742
    },
    {
      "epoch": 2.997677119628339,
      "grad_norm": 17.479473114013672,
      "learning_rate": 7.780358755968512e-06,
      "loss": 0.6094,
      "step": 7743
    },
    {
      "epoch": 2.9980642663569492,
      "grad_norm": 38.64918518066406,
      "learning_rate": 7.779928592936724e-06,
      "loss": 2.0012,
      "step": 7744
    },
    {
      "epoch": 2.9984514130855593,
      "grad_norm": 19.661022186279297,
      "learning_rate": 7.779498429904935e-06,
      "loss": 1.017,
      "step": 7745
    },
    {
      "epoch": 2.9988385598141694,
      "grad_norm": 9.985368728637695,
      "learning_rate": 7.779068266873145e-06,
      "loss": 1.2527,
      "step": 7746
    },
    {
      "epoch": 2.99922570654278,
      "grad_norm": 19.406240463256836,
      "learning_rate": 7.778638103841356e-06,
      "loss": 1.2118,
      "step": 7747
    },
    {
      "epoch": 2.99961285327139,
      "grad_norm": 23.592937469482422,
      "learning_rate": 7.778207940809568e-06,
      "loss": 1.1619,
      "step": 7748
    },
    {
      "epoch": 3.0,
      "grad_norm": 51.24742126464844,
      "learning_rate": 7.77777777777778e-06,
      "loss": 4.7363,
      "step": 7749
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.4386792452830189,
      "eval_f1": 0.3958868115711651,
      "eval_loss": 1.7091596126556396,
      "eval_runtime": 383.3463,
      "eval_samples_per_second": 2.765,
      "eval_steps_per_second": 1.383,
      "step": 7749
    }
  ],
  "logging_steps": 1,
  "max_steps": 25830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.2202603067328e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
