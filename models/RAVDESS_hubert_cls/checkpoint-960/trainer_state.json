{
  "best_metric": 0.16282809420064323,
  "best_model_checkpoint": "models\\RAVDESS_hubert_cls\\checkpoint-480",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 960,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020833333333333333,
      "grad_norm": 1.3656010627746582,
      "learning_rate": 2.0833333333333335e-08,
      "loss": 1.9327,
      "step": 1
    },
    {
      "epoch": 0.004166666666666667,
      "grad_norm": 1.4255727529525757,
      "learning_rate": 4.166666666666667e-08,
      "loss": 1.968,
      "step": 2
    },
    {
      "epoch": 0.00625,
      "grad_norm": 1.5010994672775269,
      "learning_rate": 6.250000000000001e-08,
      "loss": 1.9411,
      "step": 3
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.9118722677230835,
      "learning_rate": 8.333333333333334e-08,
      "loss": 1.9608,
      "step": 4
    },
    {
      "epoch": 0.010416666666666666,
      "grad_norm": 1.3321853876113892,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 1.9254,
      "step": 5
    },
    {
      "epoch": 0.0125,
      "grad_norm": 2.400676965713501,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 1.9986,
      "step": 6
    },
    {
      "epoch": 0.014583333333333334,
      "grad_norm": 1.4379546642303467,
      "learning_rate": 1.4583333333333335e-07,
      "loss": 1.9384,
      "step": 7
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.3329111337661743,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 1.9572,
      "step": 8
    },
    {
      "epoch": 0.01875,
      "grad_norm": 1.250981092453003,
      "learning_rate": 1.875e-07,
      "loss": 1.9607,
      "step": 9
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 1.340410828590393,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 1.9308,
      "step": 10
    },
    {
      "epoch": 0.022916666666666665,
      "grad_norm": 1.474332571029663,
      "learning_rate": 2.2916666666666666e-07,
      "loss": 1.9204,
      "step": 11
    },
    {
      "epoch": 0.025,
      "grad_norm": 1.4370452165603638,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 1.9746,
      "step": 12
    },
    {
      "epoch": 0.027083333333333334,
      "grad_norm": 1.5659068822860718,
      "learning_rate": 2.7083333333333337e-07,
      "loss": 1.9237,
      "step": 13
    },
    {
      "epoch": 0.029166666666666667,
      "grad_norm": 1.6647891998291016,
      "learning_rate": 2.916666666666667e-07,
      "loss": 1.9787,
      "step": 14
    },
    {
      "epoch": 0.03125,
      "grad_norm": 1.1900396347045898,
      "learning_rate": 3.125e-07,
      "loss": 1.9424,
      "step": 15
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 2.090128183364868,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 1.9332,
      "step": 16
    },
    {
      "epoch": 0.035416666666666666,
      "grad_norm": 1.1047351360321045,
      "learning_rate": 3.541666666666667e-07,
      "loss": 1.9591,
      "step": 17
    },
    {
      "epoch": 0.0375,
      "grad_norm": 1.113924264907837,
      "learning_rate": 3.75e-07,
      "loss": 1.9185,
      "step": 18
    },
    {
      "epoch": 0.03958333333333333,
      "grad_norm": 1.4250507354736328,
      "learning_rate": 3.9583333333333334e-07,
      "loss": 1.9344,
      "step": 19
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 1.186957836151123,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 1.9861,
      "step": 20
    },
    {
      "epoch": 0.04375,
      "grad_norm": 1.1937501430511475,
      "learning_rate": 4.375e-07,
      "loss": 1.9203,
      "step": 21
    },
    {
      "epoch": 0.04583333333333333,
      "grad_norm": 1.34679114818573,
      "learning_rate": 4.583333333333333e-07,
      "loss": 1.9025,
      "step": 22
    },
    {
      "epoch": 0.04791666666666667,
      "grad_norm": 1.327927827835083,
      "learning_rate": 4.791666666666667e-07,
      "loss": 1.9803,
      "step": 23
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.2930009365081787,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9648,
      "step": 24
    },
    {
      "epoch": 0.052083333333333336,
      "grad_norm": 1.9997715950012207,
      "learning_rate": 5.208333333333334e-07,
      "loss": 1.8909,
      "step": 25
    },
    {
      "epoch": 0.05416666666666667,
      "grad_norm": 1.2654446363449097,
      "learning_rate": 5.416666666666667e-07,
      "loss": 1.9629,
      "step": 26
    },
    {
      "epoch": 0.05625,
      "grad_norm": 1.3119282722473145,
      "learning_rate": 5.625e-07,
      "loss": 1.9094,
      "step": 27
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 1.316719889640808,
      "learning_rate": 5.833333333333334e-07,
      "loss": 1.9397,
      "step": 28
    },
    {
      "epoch": 0.06041666666666667,
      "grad_norm": 1.38850736618042,
      "learning_rate": 6.041666666666667e-07,
      "loss": 1.9292,
      "step": 29
    },
    {
      "epoch": 0.0625,
      "grad_norm": 1.8557636737823486,
      "learning_rate": 6.25e-07,
      "loss": 1.9481,
      "step": 30
    },
    {
      "epoch": 0.06458333333333334,
      "grad_norm": 1.1216989755630493,
      "learning_rate": 6.458333333333334e-07,
      "loss": 1.9519,
      "step": 31
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.4565935134887695,
      "learning_rate": 6.666666666666667e-07,
      "loss": 2.006,
      "step": 32
    },
    {
      "epoch": 0.06875,
      "grad_norm": 1.4888185262680054,
      "learning_rate": 6.875000000000001e-07,
      "loss": 1.9306,
      "step": 33
    },
    {
      "epoch": 0.07083333333333333,
      "grad_norm": 1.3666950464248657,
      "learning_rate": 7.083333333333334e-07,
      "loss": 1.9474,
      "step": 34
    },
    {
      "epoch": 0.07291666666666667,
      "grad_norm": 1.453478217124939,
      "learning_rate": 7.291666666666667e-07,
      "loss": 1.9681,
      "step": 35
    },
    {
      "epoch": 0.075,
      "grad_norm": 1.4112991094589233,
      "learning_rate": 7.5e-07,
      "loss": 1.9334,
      "step": 36
    },
    {
      "epoch": 0.07708333333333334,
      "grad_norm": 1.5293419361114502,
      "learning_rate": 7.708333333333334e-07,
      "loss": 1.948,
      "step": 37
    },
    {
      "epoch": 0.07916666666666666,
      "grad_norm": 1.7969555854797363,
      "learning_rate": 7.916666666666667e-07,
      "loss": 1.9944,
      "step": 38
    },
    {
      "epoch": 0.08125,
      "grad_norm": 1.4376308917999268,
      "learning_rate": 8.125000000000001e-07,
      "loss": 1.9533,
      "step": 39
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.4313029050827026,
      "learning_rate": 8.333333333333333e-07,
      "loss": 1.9417,
      "step": 40
    },
    {
      "epoch": 0.08541666666666667,
      "grad_norm": 2.147570848464966,
      "learning_rate": 8.541666666666667e-07,
      "loss": 1.9789,
      "step": 41
    },
    {
      "epoch": 0.0875,
      "grad_norm": 1.1594797372817993,
      "learning_rate": 8.75e-07,
      "loss": 1.9405,
      "step": 42
    },
    {
      "epoch": 0.08958333333333333,
      "grad_norm": 1.6665924787521362,
      "learning_rate": 8.958333333333334e-07,
      "loss": 1.9406,
      "step": 43
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 2.037606954574585,
      "learning_rate": 9.166666666666666e-07,
      "loss": 1.924,
      "step": 44
    },
    {
      "epoch": 0.09375,
      "grad_norm": 1.2316523790359497,
      "learning_rate": 9.375000000000001e-07,
      "loss": 1.9403,
      "step": 45
    },
    {
      "epoch": 0.09583333333333334,
      "grad_norm": 1.21660578250885,
      "learning_rate": 9.583333333333334e-07,
      "loss": 1.9193,
      "step": 46
    },
    {
      "epoch": 0.09791666666666667,
      "grad_norm": 1.5828365087509155,
      "learning_rate": 9.791666666666667e-07,
      "loss": 1.9893,
      "step": 47
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.723954677581787,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.9728,
      "step": 48
    },
    {
      "epoch": 0.10208333333333333,
      "grad_norm": 1.3959202766418457,
      "learning_rate": 1.0208333333333334e-06,
      "loss": 1.9361,
      "step": 49
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 1.4296817779541016,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 1.9288,
      "step": 50
    },
    {
      "epoch": 0.10625,
      "grad_norm": 1.9942065477371216,
      "learning_rate": 1.0625e-06,
      "loss": 1.948,
      "step": 51
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 1.499065637588501,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 1.9409,
      "step": 52
    },
    {
      "epoch": 0.11041666666666666,
      "grad_norm": 1.2829371690750122,
      "learning_rate": 1.1041666666666668e-06,
      "loss": 1.912,
      "step": 53
    },
    {
      "epoch": 0.1125,
      "grad_norm": 1.4519883394241333,
      "learning_rate": 1.125e-06,
      "loss": 1.9414,
      "step": 54
    },
    {
      "epoch": 0.11458333333333333,
      "grad_norm": 1.3108900785446167,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 1.9584,
      "step": 55
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.8849138021469116,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 1.9489,
      "step": 56
    },
    {
      "epoch": 0.11875,
      "grad_norm": 1.4732887744903564,
      "learning_rate": 1.1875e-06,
      "loss": 1.9627,
      "step": 57
    },
    {
      "epoch": 0.12083333333333333,
      "grad_norm": 1.2098169326782227,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 1.9225,
      "step": 58
    },
    {
      "epoch": 0.12291666666666666,
      "grad_norm": 1.3880913257598877,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 1.931,
      "step": 59
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.284332513809204,
      "learning_rate": 1.25e-06,
      "loss": 1.9064,
      "step": 60
    },
    {
      "epoch": 0.12708333333333333,
      "grad_norm": 1.3738212585449219,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 1.9499,
      "step": 61
    },
    {
      "epoch": 0.12916666666666668,
      "grad_norm": 1.366930365562439,
      "learning_rate": 1.2916666666666669e-06,
      "loss": 1.9269,
      "step": 62
    },
    {
      "epoch": 0.13125,
      "grad_norm": 1.4547582864761353,
      "learning_rate": 1.3125000000000001e-06,
      "loss": 1.972,
      "step": 63
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.3769160509109497,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 1.9461,
      "step": 64
    },
    {
      "epoch": 0.13541666666666666,
      "grad_norm": 1.718178391456604,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 1.9872,
      "step": 65
    },
    {
      "epoch": 0.1375,
      "grad_norm": 1.8160940408706665,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 1.9294,
      "step": 66
    },
    {
      "epoch": 0.13958333333333334,
      "grad_norm": 1.7659497261047363,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 1.9772,
      "step": 67
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 2.45913028717041,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 2.0076,
      "step": 68
    },
    {
      "epoch": 0.14375,
      "grad_norm": 1.2259025573730469,
      "learning_rate": 1.4375e-06,
      "loss": 1.9295,
      "step": 69
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 1.30710768699646,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 1.9521,
      "step": 70
    },
    {
      "epoch": 0.14791666666666667,
      "grad_norm": 1.3667610883712769,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 1.9655,
      "step": 71
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.4477694034576416,
      "learning_rate": 1.5e-06,
      "loss": 1.9607,
      "step": 72
    },
    {
      "epoch": 0.15208333333333332,
      "grad_norm": 1.3650097846984863,
      "learning_rate": 1.5208333333333333e-06,
      "loss": 1.917,
      "step": 73
    },
    {
      "epoch": 0.15416666666666667,
      "grad_norm": 1.3355998992919922,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 1.9169,
      "step": 74
    },
    {
      "epoch": 0.15625,
      "grad_norm": 1.2580227851867676,
      "learning_rate": 1.5625e-06,
      "loss": 1.9224,
      "step": 75
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.6419705152511597,
      "learning_rate": 1.5833333333333333e-06,
      "loss": 1.9543,
      "step": 76
    },
    {
      "epoch": 0.16041666666666668,
      "grad_norm": 1.6093991994857788,
      "learning_rate": 1.6041666666666668e-06,
      "loss": 1.9102,
      "step": 77
    },
    {
      "epoch": 0.1625,
      "grad_norm": 1.5098154544830322,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 1.9024,
      "step": 78
    },
    {
      "epoch": 0.16458333333333333,
      "grad_norm": 1.4802966117858887,
      "learning_rate": 1.6458333333333334e-06,
      "loss": 1.991,
      "step": 79
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.492950439453125,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 1.9132,
      "step": 80
    },
    {
      "epoch": 0.16875,
      "grad_norm": 1.7119752168655396,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 1.9472,
      "step": 81
    },
    {
      "epoch": 0.17083333333333334,
      "grad_norm": 1.3862414360046387,
      "learning_rate": 1.7083333333333334e-06,
      "loss": 1.9366,
      "step": 82
    },
    {
      "epoch": 0.17291666666666666,
      "grad_norm": 1.7285840511322021,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 1.9395,
      "step": 83
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.4411652088165283,
      "learning_rate": 1.75e-06,
      "loss": 1.9125,
      "step": 84
    },
    {
      "epoch": 0.17708333333333334,
      "grad_norm": 1.2784425020217896,
      "learning_rate": 1.7708333333333337e-06,
      "loss": 1.963,
      "step": 85
    },
    {
      "epoch": 0.17916666666666667,
      "grad_norm": 1.8572155237197876,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 1.9854,
      "step": 86
    },
    {
      "epoch": 0.18125,
      "grad_norm": 2.0394866466522217,
      "learning_rate": 1.8125e-06,
      "loss": 1.9486,
      "step": 87
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.1604998111724854,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 1.9248,
      "step": 88
    },
    {
      "epoch": 0.18541666666666667,
      "grad_norm": 1.3454688787460327,
      "learning_rate": 1.854166666666667e-06,
      "loss": 1.9569,
      "step": 89
    },
    {
      "epoch": 0.1875,
      "grad_norm": 1.2562265396118164,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 1.9538,
      "step": 90
    },
    {
      "epoch": 0.18958333333333333,
      "grad_norm": 1.603117823600769,
      "learning_rate": 1.8958333333333333e-06,
      "loss": 1.9348,
      "step": 91
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.3332278728485107,
      "learning_rate": 1.916666666666667e-06,
      "loss": 1.9159,
      "step": 92
    },
    {
      "epoch": 0.19375,
      "grad_norm": 1.4468432664871216,
      "learning_rate": 1.9375e-06,
      "loss": 1.9273,
      "step": 93
    },
    {
      "epoch": 0.19583333333333333,
      "grad_norm": 1.5407981872558594,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 1.9555,
      "step": 94
    },
    {
      "epoch": 0.19791666666666666,
      "grad_norm": 1.5373258590698242,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 1.9803,
      "step": 95
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.3939034938812256,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.9028,
      "step": 96
    },
    {
      "epoch": 0.20208333333333334,
      "grad_norm": 1.5232770442962646,
      "learning_rate": 2.0208333333333336e-06,
      "loss": 1.9682,
      "step": 97
    },
    {
      "epoch": 0.20416666666666666,
      "grad_norm": 1.5902122259140015,
      "learning_rate": 2.041666666666667e-06,
      "loss": 1.9409,
      "step": 98
    },
    {
      "epoch": 0.20625,
      "grad_norm": 1.3247003555297852,
      "learning_rate": 2.0625e-06,
      "loss": 1.9401,
      "step": 99
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.5977855920791626,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 1.9686,
      "step": 100
    },
    {
      "epoch": 0.21041666666666667,
      "grad_norm": 1.405238151550293,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 1.9374,
      "step": 101
    },
    {
      "epoch": 0.2125,
      "grad_norm": 1.5895088911056519,
      "learning_rate": 2.125e-06,
      "loss": 1.9829,
      "step": 102
    },
    {
      "epoch": 0.21458333333333332,
      "grad_norm": 1.6168591976165771,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 1.9194,
      "step": 103
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.9306175708770752,
      "learning_rate": 2.166666666666667e-06,
      "loss": 1.9244,
      "step": 104
    },
    {
      "epoch": 0.21875,
      "grad_norm": 1.3008852005004883,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 1.9116,
      "step": 105
    },
    {
      "epoch": 0.22083333333333333,
      "grad_norm": 1.2496017217636108,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 1.9375,
      "step": 106
    },
    {
      "epoch": 0.22291666666666668,
      "grad_norm": 1.2698489427566528,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 1.9534,
      "step": 107
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.3460439443588257,
      "learning_rate": 2.25e-06,
      "loss": 1.9512,
      "step": 108
    },
    {
      "epoch": 0.22708333333333333,
      "grad_norm": 1.8827704191207886,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 2.0046,
      "step": 109
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 1.8271830081939697,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 2.0088,
      "step": 110
    },
    {
      "epoch": 0.23125,
      "grad_norm": 2.005007028579712,
      "learning_rate": 2.3125000000000003e-06,
      "loss": 1.9298,
      "step": 111
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.6933585405349731,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 1.9791,
      "step": 112
    },
    {
      "epoch": 0.23541666666666666,
      "grad_norm": 1.3853344917297363,
      "learning_rate": 2.354166666666667e-06,
      "loss": 1.9041,
      "step": 113
    },
    {
      "epoch": 0.2375,
      "grad_norm": 1.6051607131958008,
      "learning_rate": 2.375e-06,
      "loss": 1.9834,
      "step": 114
    },
    {
      "epoch": 0.23958333333333334,
      "grad_norm": 1.616532802581787,
      "learning_rate": 2.395833333333334e-06,
      "loss": 1.9921,
      "step": 115
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.9666160345077515,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 1.9459,
      "step": 116
    },
    {
      "epoch": 0.24375,
      "grad_norm": 2.0627338886260986,
      "learning_rate": 2.4375e-06,
      "loss": 1.9391,
      "step": 117
    },
    {
      "epoch": 0.24583333333333332,
      "grad_norm": 1.3450732231140137,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 1.9568,
      "step": 118
    },
    {
      "epoch": 0.24791666666666667,
      "grad_norm": 1.6196264028549194,
      "learning_rate": 2.479166666666667e-06,
      "loss": 1.9144,
      "step": 119
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.5735896825790405,
      "learning_rate": 2.5e-06,
      "loss": 1.9322,
      "step": 120
    },
    {
      "epoch": 0.2520833333333333,
      "grad_norm": 1.6160943508148193,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 1.9817,
      "step": 121
    },
    {
      "epoch": 0.25416666666666665,
      "grad_norm": 1.608150839805603,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 1.974,
      "step": 122
    },
    {
      "epoch": 0.25625,
      "grad_norm": 1.3417377471923828,
      "learning_rate": 2.5625e-06,
      "loss": 1.9541,
      "step": 123
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.2143162488937378,
      "learning_rate": 2.5833333333333337e-06,
      "loss": 1.9392,
      "step": 124
    },
    {
      "epoch": 0.2604166666666667,
      "grad_norm": 1.6939482688903809,
      "learning_rate": 2.604166666666667e-06,
      "loss": 1.9366,
      "step": 125
    },
    {
      "epoch": 0.2625,
      "grad_norm": 1.740329623222351,
      "learning_rate": 2.6250000000000003e-06,
      "loss": 1.9781,
      "step": 126
    },
    {
      "epoch": 0.26458333333333334,
      "grad_norm": 1.1429450511932373,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 1.9185,
      "step": 127
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.9097005128860474,
      "learning_rate": 2.666666666666667e-06,
      "loss": 1.9879,
      "step": 128
    },
    {
      "epoch": 0.26875,
      "grad_norm": 1.4480551481246948,
      "learning_rate": 2.6875e-06,
      "loss": 1.9101,
      "step": 129
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 1.4415920972824097,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 1.9234,
      "step": 130
    },
    {
      "epoch": 0.27291666666666664,
      "grad_norm": 1.4805325269699097,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 1.9103,
      "step": 131
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.4249248504638672,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 1.9625,
      "step": 132
    },
    {
      "epoch": 0.27708333333333335,
      "grad_norm": 1.273820400238037,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 1.9058,
      "step": 133
    },
    {
      "epoch": 0.2791666666666667,
      "grad_norm": 1.4991645812988281,
      "learning_rate": 2.791666666666667e-06,
      "loss": 1.9117,
      "step": 134
    },
    {
      "epoch": 0.28125,
      "grad_norm": 1.248900055885315,
      "learning_rate": 2.8125e-06,
      "loss": 1.918,
      "step": 135
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.3453645706176758,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 1.9484,
      "step": 136
    },
    {
      "epoch": 0.28541666666666665,
      "grad_norm": 1.6247049570083618,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 1.9047,
      "step": 137
    },
    {
      "epoch": 0.2875,
      "grad_norm": 1.843184471130371,
      "learning_rate": 2.875e-06,
      "loss": 1.9819,
      "step": 138
    },
    {
      "epoch": 0.28958333333333336,
      "grad_norm": 1.561177134513855,
      "learning_rate": 2.8958333333333337e-06,
      "loss": 1.9662,
      "step": 139
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.7659250497817993,
      "learning_rate": 2.916666666666667e-06,
      "loss": 1.944,
      "step": 140
    },
    {
      "epoch": 0.29375,
      "grad_norm": 1.5173490047454834,
      "learning_rate": 2.9375000000000003e-06,
      "loss": 1.9748,
      "step": 141
    },
    {
      "epoch": 0.29583333333333334,
      "grad_norm": 1.7372595071792603,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 1.9787,
      "step": 142
    },
    {
      "epoch": 0.29791666666666666,
      "grad_norm": 1.342614769935608,
      "learning_rate": 2.979166666666667e-06,
      "loss": 1.9399,
      "step": 143
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.566171407699585,
      "learning_rate": 3e-06,
      "loss": 1.9392,
      "step": 144
    },
    {
      "epoch": 0.3020833333333333,
      "grad_norm": 1.4333405494689941,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 1.954,
      "step": 145
    },
    {
      "epoch": 0.30416666666666664,
      "grad_norm": 1.7944831848144531,
      "learning_rate": 3.0416666666666666e-06,
      "loss": 1.9957,
      "step": 146
    },
    {
      "epoch": 0.30625,
      "grad_norm": 3.011237382888794,
      "learning_rate": 3.0625000000000003e-06,
      "loss": 2.0382,
      "step": 147
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 1.4312045574188232,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 1.9054,
      "step": 148
    },
    {
      "epoch": 0.3104166666666667,
      "grad_norm": 1.70932936668396,
      "learning_rate": 3.104166666666667e-06,
      "loss": 1.9834,
      "step": 149
    },
    {
      "epoch": 0.3125,
      "grad_norm": 1.5819846391677856,
      "learning_rate": 3.125e-06,
      "loss": 1.8841,
      "step": 150
    },
    {
      "epoch": 0.3145833333333333,
      "grad_norm": 1.8980270624160767,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 1.929,
      "step": 151
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 2.596450090408325,
      "learning_rate": 3.1666666666666667e-06,
      "loss": 1.9101,
      "step": 152
    },
    {
      "epoch": 0.31875,
      "grad_norm": 2.495577573776245,
      "learning_rate": 3.1875e-06,
      "loss": 1.9147,
      "step": 153
    },
    {
      "epoch": 0.32083333333333336,
      "grad_norm": 1.3611204624176025,
      "learning_rate": 3.2083333333333337e-06,
      "loss": 1.9271,
      "step": 154
    },
    {
      "epoch": 0.3229166666666667,
      "grad_norm": 1.9344487190246582,
      "learning_rate": 3.229166666666667e-06,
      "loss": 1.9114,
      "step": 155
    },
    {
      "epoch": 0.325,
      "grad_norm": 2.0817620754241943,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.9762,
      "step": 156
    },
    {
      "epoch": 0.32708333333333334,
      "grad_norm": 1.6482269763946533,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 2.0067,
      "step": 157
    },
    {
      "epoch": 0.32916666666666666,
      "grad_norm": 1.6429798603057861,
      "learning_rate": 3.2916666666666668e-06,
      "loss": 1.9578,
      "step": 158
    },
    {
      "epoch": 0.33125,
      "grad_norm": 1.3621513843536377,
      "learning_rate": 3.3125e-06,
      "loss": 1.9231,
      "step": 159
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.709699034690857,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.8905,
      "step": 160
    },
    {
      "epoch": 0.33541666666666664,
      "grad_norm": 1.424316644668579,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 1.9278,
      "step": 161
    },
    {
      "epoch": 0.3375,
      "grad_norm": 1.7434216737747192,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 1.9799,
      "step": 162
    },
    {
      "epoch": 0.33958333333333335,
      "grad_norm": 1.5084645748138428,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 1.9609,
      "step": 163
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 2.189319610595703,
      "learning_rate": 3.416666666666667e-06,
      "loss": 1.9024,
      "step": 164
    },
    {
      "epoch": 0.34375,
      "grad_norm": 1.3864644765853882,
      "learning_rate": 3.4375e-06,
      "loss": 1.9212,
      "step": 165
    },
    {
      "epoch": 0.3458333333333333,
      "grad_norm": 2.4119699001312256,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 1.9062,
      "step": 166
    },
    {
      "epoch": 0.34791666666666665,
      "grad_norm": 1.7223366498947144,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 1.9691,
      "step": 167
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.385590076446533,
      "learning_rate": 3.5e-06,
      "loss": 1.9938,
      "step": 168
    },
    {
      "epoch": 0.35208333333333336,
      "grad_norm": 1.6133110523223877,
      "learning_rate": 3.520833333333334e-06,
      "loss": 1.9105,
      "step": 169
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 2.080188274383545,
      "learning_rate": 3.5416666666666673e-06,
      "loss": 1.9444,
      "step": 170
    },
    {
      "epoch": 0.35625,
      "grad_norm": 2.002821922302246,
      "learning_rate": 3.5625e-06,
      "loss": 1.9914,
      "step": 171
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 1.4226205348968506,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 1.934,
      "step": 172
    },
    {
      "epoch": 0.36041666666666666,
      "grad_norm": 1.5380207300186157,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 1.9668,
      "step": 173
    },
    {
      "epoch": 0.3625,
      "grad_norm": 1.9901806116104126,
      "learning_rate": 3.625e-06,
      "loss": 1.9,
      "step": 174
    },
    {
      "epoch": 0.3645833333333333,
      "grad_norm": 1.6424179077148438,
      "learning_rate": 3.6458333333333333e-06,
      "loss": 1.9333,
      "step": 175
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 2.1804091930389404,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 1.8936,
      "step": 176
    },
    {
      "epoch": 0.36875,
      "grad_norm": 1.5809381008148193,
      "learning_rate": 3.6875000000000007e-06,
      "loss": 1.9643,
      "step": 177
    },
    {
      "epoch": 0.37083333333333335,
      "grad_norm": 2.197772741317749,
      "learning_rate": 3.708333333333334e-06,
      "loss": 1.9709,
      "step": 178
    },
    {
      "epoch": 0.3729166666666667,
      "grad_norm": 1.8060131072998047,
      "learning_rate": 3.7291666666666672e-06,
      "loss": 1.9395,
      "step": 179
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.7433029413223267,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 1.9848,
      "step": 180
    },
    {
      "epoch": 0.3770833333333333,
      "grad_norm": 1.6533558368682861,
      "learning_rate": 3.7708333333333334e-06,
      "loss": 1.8929,
      "step": 181
    },
    {
      "epoch": 0.37916666666666665,
      "grad_norm": 1.9880746603012085,
      "learning_rate": 3.7916666666666666e-06,
      "loss": 1.9319,
      "step": 182
    },
    {
      "epoch": 0.38125,
      "grad_norm": 1.354346752166748,
      "learning_rate": 3.8125e-06,
      "loss": 1.9491,
      "step": 183
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.6048343181610107,
      "learning_rate": 3.833333333333334e-06,
      "loss": 1.8989,
      "step": 184
    },
    {
      "epoch": 0.3854166666666667,
      "grad_norm": 2.248694896697998,
      "learning_rate": 3.854166666666667e-06,
      "loss": 1.8769,
      "step": 185
    },
    {
      "epoch": 0.3875,
      "grad_norm": 1.8974634408950806,
      "learning_rate": 3.875e-06,
      "loss": 1.966,
      "step": 186
    },
    {
      "epoch": 0.38958333333333334,
      "grad_norm": 1.554516077041626,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 1.9232,
      "step": 187
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.7448005676269531,
      "learning_rate": 3.916666666666667e-06,
      "loss": 1.9812,
      "step": 188
    },
    {
      "epoch": 0.39375,
      "grad_norm": 1.5821349620819092,
      "learning_rate": 3.9375e-06,
      "loss": 1.8925,
      "step": 189
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 1.703403115272522,
      "learning_rate": 3.958333333333333e-06,
      "loss": 1.8891,
      "step": 190
    },
    {
      "epoch": 0.39791666666666664,
      "grad_norm": 2.4912328720092773,
      "learning_rate": 3.9791666666666665e-06,
      "loss": 1.8883,
      "step": 191
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.5777997970581055,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9096,
      "step": 192
    },
    {
      "epoch": 0.40208333333333335,
      "grad_norm": 1.5086619853973389,
      "learning_rate": 4.020833333333334e-06,
      "loss": 1.9592,
      "step": 193
    },
    {
      "epoch": 0.4041666666666667,
      "grad_norm": 1.7144463062286377,
      "learning_rate": 4.041666666666667e-06,
      "loss": 1.9338,
      "step": 194
    },
    {
      "epoch": 0.40625,
      "grad_norm": 2.938366651535034,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 1.8783,
      "step": 195
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.7791860103607178,
      "learning_rate": 4.083333333333334e-06,
      "loss": 1.9534,
      "step": 196
    },
    {
      "epoch": 0.41041666666666665,
      "grad_norm": 2.0922815799713135,
      "learning_rate": 4.104166666666667e-06,
      "loss": 1.9583,
      "step": 197
    },
    {
      "epoch": 0.4125,
      "grad_norm": 1.9370859861373901,
      "learning_rate": 4.125e-06,
      "loss": 1.974,
      "step": 198
    },
    {
      "epoch": 0.41458333333333336,
      "grad_norm": 1.4685262441635132,
      "learning_rate": 4.145833333333334e-06,
      "loss": 1.9667,
      "step": 199
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.644808292388916,
      "learning_rate": 4.166666666666667e-06,
      "loss": 1.9413,
      "step": 200
    },
    {
      "epoch": 0.41875,
      "grad_norm": 1.6526079177856445,
      "learning_rate": 4.1875e-06,
      "loss": 1.9178,
      "step": 201
    },
    {
      "epoch": 0.42083333333333334,
      "grad_norm": 2.3051586151123047,
      "learning_rate": 4.208333333333333e-06,
      "loss": 2.009,
      "step": 202
    },
    {
      "epoch": 0.42291666666666666,
      "grad_norm": 1.7299785614013672,
      "learning_rate": 4.229166666666667e-06,
      "loss": 1.8726,
      "step": 203
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.9178578853607178,
      "learning_rate": 4.25e-06,
      "loss": 1.9473,
      "step": 204
    },
    {
      "epoch": 0.4270833333333333,
      "grad_norm": 1.646432876586914,
      "learning_rate": 4.270833333333333e-06,
      "loss": 1.9678,
      "step": 205
    },
    {
      "epoch": 0.42916666666666664,
      "grad_norm": 2.21234130859375,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 1.8912,
      "step": 206
    },
    {
      "epoch": 0.43125,
      "grad_norm": 1.5586419105529785,
      "learning_rate": 4.312500000000001e-06,
      "loss": 1.9335,
      "step": 207
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.7768386602401733,
      "learning_rate": 4.333333333333334e-06,
      "loss": 1.9814,
      "step": 208
    },
    {
      "epoch": 0.4354166666666667,
      "grad_norm": 2.3278260231018066,
      "learning_rate": 4.354166666666667e-06,
      "loss": 1.9237,
      "step": 209
    },
    {
      "epoch": 0.4375,
      "grad_norm": 1.7317548990249634,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 1.9659,
      "step": 210
    },
    {
      "epoch": 0.4395833333333333,
      "grad_norm": 1.8235660791397095,
      "learning_rate": 4.395833333333334e-06,
      "loss": 1.9132,
      "step": 211
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.8113242387771606,
      "learning_rate": 4.416666666666667e-06,
      "loss": 1.9345,
      "step": 212
    },
    {
      "epoch": 0.44375,
      "grad_norm": 2.174243450164795,
      "learning_rate": 4.4375e-06,
      "loss": 1.9051,
      "step": 213
    },
    {
      "epoch": 0.44583333333333336,
      "grad_norm": 1.8846261501312256,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 1.9367,
      "step": 214
    },
    {
      "epoch": 0.4479166666666667,
      "grad_norm": 2.396265983581543,
      "learning_rate": 4.479166666666667e-06,
      "loss": 1.9236,
      "step": 215
    },
    {
      "epoch": 0.45,
      "grad_norm": 2.9885406494140625,
      "learning_rate": 4.5e-06,
      "loss": 1.8675,
      "step": 216
    },
    {
      "epoch": 0.45208333333333334,
      "grad_norm": 1.5922093391418457,
      "learning_rate": 4.520833333333333e-06,
      "loss": 1.9758,
      "step": 217
    },
    {
      "epoch": 0.45416666666666666,
      "grad_norm": 1.723069190979004,
      "learning_rate": 4.541666666666667e-06,
      "loss": 1.9236,
      "step": 218
    },
    {
      "epoch": 0.45625,
      "grad_norm": 2.1999847888946533,
      "learning_rate": 4.5625e-06,
      "loss": 1.8921,
      "step": 219
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 2.267242193222046,
      "learning_rate": 4.583333333333333e-06,
      "loss": 1.8975,
      "step": 220
    },
    {
      "epoch": 0.46041666666666664,
      "grad_norm": 2.231661081314087,
      "learning_rate": 4.6041666666666665e-06,
      "loss": 1.8952,
      "step": 221
    },
    {
      "epoch": 0.4625,
      "grad_norm": 1.757491946220398,
      "learning_rate": 4.625000000000001e-06,
      "loss": 1.8975,
      "step": 222
    },
    {
      "epoch": 0.46458333333333335,
      "grad_norm": 1.862398624420166,
      "learning_rate": 4.645833333333334e-06,
      "loss": 1.897,
      "step": 223
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.939106822013855,
      "learning_rate": 4.666666666666667e-06,
      "loss": 1.9671,
      "step": 224
    },
    {
      "epoch": 0.46875,
      "grad_norm": 2.247540235519409,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 2.0449,
      "step": 225
    },
    {
      "epoch": 0.4708333333333333,
      "grad_norm": 3.573150157928467,
      "learning_rate": 4.708333333333334e-06,
      "loss": 2.0275,
      "step": 226
    },
    {
      "epoch": 0.47291666666666665,
      "grad_norm": 1.8961498737335205,
      "learning_rate": 4.729166666666667e-06,
      "loss": 1.9343,
      "step": 227
    },
    {
      "epoch": 0.475,
      "grad_norm": 2.6629021167755127,
      "learning_rate": 4.75e-06,
      "loss": 1.9605,
      "step": 228
    },
    {
      "epoch": 0.47708333333333336,
      "grad_norm": 1.9299182891845703,
      "learning_rate": 4.770833333333334e-06,
      "loss": 2.0027,
      "step": 229
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 2.2619400024414062,
      "learning_rate": 4.791666666666668e-06,
      "loss": 1.9978,
      "step": 230
    },
    {
      "epoch": 0.48125,
      "grad_norm": 1.6843916177749634,
      "learning_rate": 4.8125e-06,
      "loss": 1.9494,
      "step": 231
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.97939133644104,
      "learning_rate": 4.833333333333333e-06,
      "loss": 1.9049,
      "step": 232
    },
    {
      "epoch": 0.48541666666666666,
      "grad_norm": 2.605259656906128,
      "learning_rate": 4.854166666666667e-06,
      "loss": 1.8513,
      "step": 233
    },
    {
      "epoch": 0.4875,
      "grad_norm": 2.428267478942871,
      "learning_rate": 4.875e-06,
      "loss": 1.8604,
      "step": 234
    },
    {
      "epoch": 0.4895833333333333,
      "grad_norm": 2.2064976692199707,
      "learning_rate": 4.895833333333333e-06,
      "loss": 1.9074,
      "step": 235
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 1.8235766887664795,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 1.9042,
      "step": 236
    },
    {
      "epoch": 0.49375,
      "grad_norm": 1.7362613677978516,
      "learning_rate": 4.937500000000001e-06,
      "loss": 1.9731,
      "step": 237
    },
    {
      "epoch": 0.49583333333333335,
      "grad_norm": 1.9698946475982666,
      "learning_rate": 4.958333333333334e-06,
      "loss": 1.9606,
      "step": 238
    },
    {
      "epoch": 0.4979166666666667,
      "grad_norm": 3.645357847213745,
      "learning_rate": 4.979166666666667e-06,
      "loss": 1.8531,
      "step": 239
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.854358196258545,
      "learning_rate": 5e-06,
      "loss": 1.9671,
      "step": 240
    },
    {
      "epoch": 0.5020833333333333,
      "grad_norm": 1.843748927116394,
      "learning_rate": 5.020833333333334e-06,
      "loss": 1.9645,
      "step": 241
    },
    {
      "epoch": 0.5041666666666667,
      "grad_norm": 2.6966536045074463,
      "learning_rate": 5.041666666666667e-06,
      "loss": 1.9536,
      "step": 242
    },
    {
      "epoch": 0.50625,
      "grad_norm": 3.1641457080841064,
      "learning_rate": 5.0625e-06,
      "loss": 2.0146,
      "step": 243
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 2.738107919692993,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 1.9315,
      "step": 244
    },
    {
      "epoch": 0.5104166666666666,
      "grad_norm": 2.788121223449707,
      "learning_rate": 5.104166666666667e-06,
      "loss": 1.9572,
      "step": 245
    },
    {
      "epoch": 0.5125,
      "grad_norm": 2.3148388862609863,
      "learning_rate": 5.125e-06,
      "loss": 1.8307,
      "step": 246
    },
    {
      "epoch": 0.5145833333333333,
      "grad_norm": 2.8322653770446777,
      "learning_rate": 5.145833333333333e-06,
      "loss": 1.9568,
      "step": 247
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.8942432403564453,
      "learning_rate": 5.1666666666666675e-06,
      "loss": 1.9898,
      "step": 248
    },
    {
      "epoch": 0.51875,
      "grad_norm": 1.8595223426818848,
      "learning_rate": 5.187500000000001e-06,
      "loss": 1.894,
      "step": 249
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 2.410299301147461,
      "learning_rate": 5.208333333333334e-06,
      "loss": 1.9631,
      "step": 250
    },
    {
      "epoch": 0.5229166666666667,
      "grad_norm": 3.0616726875305176,
      "learning_rate": 5.229166666666667e-06,
      "loss": 1.9173,
      "step": 251
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.9040396213531494,
      "learning_rate": 5.2500000000000006e-06,
      "loss": 1.8926,
      "step": 252
    },
    {
      "epoch": 0.5270833333333333,
      "grad_norm": 1.8419060707092285,
      "learning_rate": 5.270833333333334e-06,
      "loss": 1.9029,
      "step": 253
    },
    {
      "epoch": 0.5291666666666667,
      "grad_norm": 3.389883279800415,
      "learning_rate": 5.291666666666667e-06,
      "loss": 2.0174,
      "step": 254
    },
    {
      "epoch": 0.53125,
      "grad_norm": 1.9361604452133179,
      "learning_rate": 5.3125e-06,
      "loss": 1.8934,
      "step": 255
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4250881671905518,
      "learning_rate": 5.333333333333334e-06,
      "loss": 2.0199,
      "step": 256
    },
    {
      "epoch": 0.5354166666666667,
      "grad_norm": 3.165557622909546,
      "learning_rate": 5.354166666666667e-06,
      "loss": 1.9131,
      "step": 257
    },
    {
      "epoch": 0.5375,
      "grad_norm": 2.1826205253601074,
      "learning_rate": 5.375e-06,
      "loss": 1.8948,
      "step": 258
    },
    {
      "epoch": 0.5395833333333333,
      "grad_norm": 2.0835843086242676,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 1.8973,
      "step": 259
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 2.1703131198883057,
      "learning_rate": 5.416666666666667e-06,
      "loss": 1.8461,
      "step": 260
    },
    {
      "epoch": 0.54375,
      "grad_norm": 2.299730062484741,
      "learning_rate": 5.4375e-06,
      "loss": 2.0039,
      "step": 261
    },
    {
      "epoch": 0.5458333333333333,
      "grad_norm": 2.1289424896240234,
      "learning_rate": 5.458333333333333e-06,
      "loss": 1.9709,
      "step": 262
    },
    {
      "epoch": 0.5479166666666667,
      "grad_norm": 3.1521549224853516,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 1.8674,
      "step": 263
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9204808473587036,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.9357,
      "step": 264
    },
    {
      "epoch": 0.5520833333333334,
      "grad_norm": 1.8873332738876343,
      "learning_rate": 5.520833333333334e-06,
      "loss": 1.9051,
      "step": 265
    },
    {
      "epoch": 0.5541666666666667,
      "grad_norm": 3.155243158340454,
      "learning_rate": 5.541666666666667e-06,
      "loss": 2.0185,
      "step": 266
    },
    {
      "epoch": 0.55625,
      "grad_norm": 2.3129990100860596,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 1.8596,
      "step": 267
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 2.3421220779418945,
      "learning_rate": 5.583333333333334e-06,
      "loss": 2.0428,
      "step": 268
    },
    {
      "epoch": 0.5604166666666667,
      "grad_norm": 3.07894229888916,
      "learning_rate": 5.604166666666667e-06,
      "loss": 1.8312,
      "step": 269
    },
    {
      "epoch": 0.5625,
      "grad_norm": 2.0167429447174072,
      "learning_rate": 5.625e-06,
      "loss": 1.919,
      "step": 270
    },
    {
      "epoch": 0.5645833333333333,
      "grad_norm": 2.1795730590820312,
      "learning_rate": 5.645833333333334e-06,
      "loss": 1.9425,
      "step": 271
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 3.5813472270965576,
      "learning_rate": 5.666666666666667e-06,
      "loss": 2.0585,
      "step": 272
    },
    {
      "epoch": 0.56875,
      "grad_norm": 2.0160906314849854,
      "learning_rate": 5.6875e-06,
      "loss": 1.9892,
      "step": 273
    },
    {
      "epoch": 0.5708333333333333,
      "grad_norm": 2.624969005584717,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 1.8676,
      "step": 274
    },
    {
      "epoch": 0.5729166666666666,
      "grad_norm": 2.2965028285980225,
      "learning_rate": 5.729166666666667e-06,
      "loss": 1.8911,
      "step": 275
    },
    {
      "epoch": 0.575,
      "grad_norm": 3.5277669429779053,
      "learning_rate": 5.75e-06,
      "loss": 2.0142,
      "step": 276
    },
    {
      "epoch": 0.5770833333333333,
      "grad_norm": 1.9311413764953613,
      "learning_rate": 5.770833333333333e-06,
      "loss": 1.9065,
      "step": 277
    },
    {
      "epoch": 0.5791666666666667,
      "grad_norm": 2.0657958984375,
      "learning_rate": 5.791666666666667e-06,
      "loss": 1.9246,
      "step": 278
    },
    {
      "epoch": 0.58125,
      "grad_norm": 2.607872247695923,
      "learning_rate": 5.812500000000001e-06,
      "loss": 2.0187,
      "step": 279
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 2.5093233585357666,
      "learning_rate": 5.833333333333334e-06,
      "loss": 1.8347,
      "step": 280
    },
    {
      "epoch": 0.5854166666666667,
      "grad_norm": 1.898473858833313,
      "learning_rate": 5.854166666666667e-06,
      "loss": 1.9006,
      "step": 281
    },
    {
      "epoch": 0.5875,
      "grad_norm": 1.9381897449493408,
      "learning_rate": 5.8750000000000005e-06,
      "loss": 1.9168,
      "step": 282
    },
    {
      "epoch": 0.5895833333333333,
      "grad_norm": 2.9250664710998535,
      "learning_rate": 5.895833333333334e-06,
      "loss": 2.0804,
      "step": 283
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 2.4667484760284424,
      "learning_rate": 5.916666666666667e-06,
      "loss": 1.9591,
      "step": 284
    },
    {
      "epoch": 0.59375,
      "grad_norm": 3.518202543258667,
      "learning_rate": 5.9375e-06,
      "loss": 2.0663,
      "step": 285
    },
    {
      "epoch": 0.5958333333333333,
      "grad_norm": 2.2965872287750244,
      "learning_rate": 5.958333333333334e-06,
      "loss": 1.9379,
      "step": 286
    },
    {
      "epoch": 0.5979166666666667,
      "grad_norm": 2.312025547027588,
      "learning_rate": 5.979166666666667e-06,
      "loss": 1.8913,
      "step": 287
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.984014630317688,
      "learning_rate": 6e-06,
      "loss": 1.9283,
      "step": 288
    },
    {
      "epoch": 0.6020833333333333,
      "grad_norm": 2.3506760597229004,
      "learning_rate": 6.0208333333333334e-06,
      "loss": 1.9983,
      "step": 289
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 3.4718809127807617,
      "learning_rate": 6.041666666666667e-06,
      "loss": 1.9049,
      "step": 290
    },
    {
      "epoch": 0.60625,
      "grad_norm": 2.399280309677124,
      "learning_rate": 6.0625e-06,
      "loss": 1.8421,
      "step": 291
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 2.5102994441986084,
      "learning_rate": 6.083333333333333e-06,
      "loss": 1.9692,
      "step": 292
    },
    {
      "epoch": 0.6104166666666667,
      "grad_norm": 2.54512619972229,
      "learning_rate": 6.104166666666667e-06,
      "loss": 1.8346,
      "step": 293
    },
    {
      "epoch": 0.6125,
      "grad_norm": 1.888302206993103,
      "learning_rate": 6.125000000000001e-06,
      "loss": 1.8693,
      "step": 294
    },
    {
      "epoch": 0.6145833333333334,
      "grad_norm": 1.730652093887329,
      "learning_rate": 6.145833333333334e-06,
      "loss": 1.9487,
      "step": 295
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 3.913667678833008,
      "learning_rate": 6.166666666666667e-06,
      "loss": 2.0249,
      "step": 296
    },
    {
      "epoch": 0.61875,
      "grad_norm": 2.5604746341705322,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 1.9091,
      "step": 297
    },
    {
      "epoch": 0.6208333333333333,
      "grad_norm": 2.047619342803955,
      "learning_rate": 6.208333333333334e-06,
      "loss": 1.8933,
      "step": 298
    },
    {
      "epoch": 0.6229166666666667,
      "grad_norm": 2.2678444385528564,
      "learning_rate": 6.229166666666667e-06,
      "loss": 1.8614,
      "step": 299
    },
    {
      "epoch": 0.625,
      "grad_norm": 2.5614569187164307,
      "learning_rate": 6.25e-06,
      "loss": 2.0174,
      "step": 300
    },
    {
      "epoch": 0.6270833333333333,
      "grad_norm": 2.5827088356018066,
      "learning_rate": 6.2708333333333336e-06,
      "loss": 1.8997,
      "step": 301
    },
    {
      "epoch": 0.6291666666666667,
      "grad_norm": 2.1900813579559326,
      "learning_rate": 6.291666666666667e-06,
      "loss": 1.8632,
      "step": 302
    },
    {
      "epoch": 0.63125,
      "grad_norm": 2.126892566680908,
      "learning_rate": 6.3125e-06,
      "loss": 1.9669,
      "step": 303
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.3701188564300537,
      "learning_rate": 6.333333333333333e-06,
      "loss": 1.8642,
      "step": 304
    },
    {
      "epoch": 0.6354166666666666,
      "grad_norm": 2.812835931777954,
      "learning_rate": 6.354166666666667e-06,
      "loss": 1.8609,
      "step": 305
    },
    {
      "epoch": 0.6375,
      "grad_norm": 2.662055492401123,
      "learning_rate": 6.375e-06,
      "loss": 2.0148,
      "step": 306
    },
    {
      "epoch": 0.6395833333333333,
      "grad_norm": 2.8154637813568115,
      "learning_rate": 6.395833333333333e-06,
      "loss": 1.8095,
      "step": 307
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 2.750066041946411,
      "learning_rate": 6.416666666666667e-06,
      "loss": 1.9756,
      "step": 308
    },
    {
      "epoch": 0.64375,
      "grad_norm": 2.7688097953796387,
      "learning_rate": 6.437500000000001e-06,
      "loss": 2.0561,
      "step": 309
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 3.8024823665618896,
      "learning_rate": 6.458333333333334e-06,
      "loss": 1.8525,
      "step": 310
    },
    {
      "epoch": 0.6479166666666667,
      "grad_norm": 2.5757503509521484,
      "learning_rate": 6.479166666666667e-06,
      "loss": 1.882,
      "step": 311
    },
    {
      "epoch": 0.65,
      "grad_norm": 2.441953659057617,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.9922,
      "step": 312
    },
    {
      "epoch": 0.6520833333333333,
      "grad_norm": 2.293457269668579,
      "learning_rate": 6.520833333333334e-06,
      "loss": 1.8927,
      "step": 313
    },
    {
      "epoch": 0.6541666666666667,
      "grad_norm": 3.0060155391693115,
      "learning_rate": 6.541666666666667e-06,
      "loss": 1.9956,
      "step": 314
    },
    {
      "epoch": 0.65625,
      "grad_norm": 2.415240526199341,
      "learning_rate": 6.5625e-06,
      "loss": 1.9537,
      "step": 315
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 2.714740514755249,
      "learning_rate": 6.5833333333333335e-06,
      "loss": 2.0724,
      "step": 316
    },
    {
      "epoch": 0.6604166666666667,
      "grad_norm": 2.6796047687530518,
      "learning_rate": 6.604166666666667e-06,
      "loss": 1.8765,
      "step": 317
    },
    {
      "epoch": 0.6625,
      "grad_norm": 2.6019749641418457,
      "learning_rate": 6.625e-06,
      "loss": 2.0555,
      "step": 318
    },
    {
      "epoch": 0.6645833333333333,
      "grad_norm": 2.2928247451782227,
      "learning_rate": 6.645833333333333e-06,
      "loss": 1.912,
      "step": 319
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.789982557296753,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.0156,
      "step": 320
    },
    {
      "epoch": 0.66875,
      "grad_norm": 2.2915830612182617,
      "learning_rate": 6.6875e-06,
      "loss": 1.896,
      "step": 321
    },
    {
      "epoch": 0.6708333333333333,
      "grad_norm": 2.2692737579345703,
      "learning_rate": 6.708333333333333e-06,
      "loss": 1.9683,
      "step": 322
    },
    {
      "epoch": 0.6729166666666667,
      "grad_norm": 2.0629639625549316,
      "learning_rate": 6.729166666666667e-06,
      "loss": 1.9431,
      "step": 323
    },
    {
      "epoch": 0.675,
      "grad_norm": 2.520183801651001,
      "learning_rate": 6.750000000000001e-06,
      "loss": 2.0044,
      "step": 324
    },
    {
      "epoch": 0.6770833333333334,
      "grad_norm": 2.4724886417388916,
      "learning_rate": 6.770833333333334e-06,
      "loss": 2.0033,
      "step": 325
    },
    {
      "epoch": 0.6791666666666667,
      "grad_norm": 2.4983415603637695,
      "learning_rate": 6.791666666666667e-06,
      "loss": 1.9768,
      "step": 326
    },
    {
      "epoch": 0.68125,
      "grad_norm": 2.4048242568969727,
      "learning_rate": 6.8125e-06,
      "loss": 2.0085,
      "step": 327
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 2.2547218799591064,
      "learning_rate": 6.833333333333334e-06,
      "loss": 1.9401,
      "step": 328
    },
    {
      "epoch": 0.6854166666666667,
      "grad_norm": 2.3316361904144287,
      "learning_rate": 6.854166666666667e-06,
      "loss": 1.9649,
      "step": 329
    },
    {
      "epoch": 0.6875,
      "grad_norm": 1.9652899503707886,
      "learning_rate": 6.875e-06,
      "loss": 1.9866,
      "step": 330
    },
    {
      "epoch": 0.6895833333333333,
      "grad_norm": 4.378288269042969,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 1.7942,
      "step": 331
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 2.1427857875823975,
      "learning_rate": 6.916666666666667e-06,
      "loss": 1.9143,
      "step": 332
    },
    {
      "epoch": 0.69375,
      "grad_norm": 2.119568109512329,
      "learning_rate": 6.9375e-06,
      "loss": 1.9713,
      "step": 333
    },
    {
      "epoch": 0.6958333333333333,
      "grad_norm": 1.9810140132904053,
      "learning_rate": 6.958333333333333e-06,
      "loss": 1.9442,
      "step": 334
    },
    {
      "epoch": 0.6979166666666666,
      "grad_norm": 2.1782877445220947,
      "learning_rate": 6.979166666666667e-06,
      "loss": 1.8447,
      "step": 335
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.250786066055298,
      "learning_rate": 7e-06,
      "loss": 1.9839,
      "step": 336
    },
    {
      "epoch": 0.7020833333333333,
      "grad_norm": 3.692650556564331,
      "learning_rate": 7.020833333333333e-06,
      "loss": 2.0693,
      "step": 337
    },
    {
      "epoch": 0.7041666666666667,
      "grad_norm": 2.408928155899048,
      "learning_rate": 7.041666666666668e-06,
      "loss": 1.9713,
      "step": 338
    },
    {
      "epoch": 0.70625,
      "grad_norm": 3.200366258621216,
      "learning_rate": 7.062500000000001e-06,
      "loss": 1.9939,
      "step": 339
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 2.39514422416687,
      "learning_rate": 7.083333333333335e-06,
      "loss": 1.9996,
      "step": 340
    },
    {
      "epoch": 0.7104166666666667,
      "grad_norm": 2.2330617904663086,
      "learning_rate": 7.104166666666668e-06,
      "loss": 1.9099,
      "step": 341
    },
    {
      "epoch": 0.7125,
      "grad_norm": 2.2347140312194824,
      "learning_rate": 7.125e-06,
      "loss": 1.8596,
      "step": 342
    },
    {
      "epoch": 0.7145833333333333,
      "grad_norm": 1.836554765701294,
      "learning_rate": 7.145833333333334e-06,
      "loss": 1.9187,
      "step": 343
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.872352123260498,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.9261,
      "step": 344
    },
    {
      "epoch": 0.71875,
      "grad_norm": 1.978493332862854,
      "learning_rate": 7.1875e-06,
      "loss": 1.9483,
      "step": 345
    },
    {
      "epoch": 0.7208333333333333,
      "grad_norm": 1.9424595832824707,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 1.9952,
      "step": 346
    },
    {
      "epoch": 0.7229166666666667,
      "grad_norm": 1.878655195236206,
      "learning_rate": 7.229166666666667e-06,
      "loss": 1.9378,
      "step": 347
    },
    {
      "epoch": 0.725,
      "grad_norm": 2.1839728355407715,
      "learning_rate": 7.25e-06,
      "loss": 1.8907,
      "step": 348
    },
    {
      "epoch": 0.7270833333333333,
      "grad_norm": 2.4479727745056152,
      "learning_rate": 7.270833333333333e-06,
      "loss": 1.9555,
      "step": 349
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 2.0386245250701904,
      "learning_rate": 7.291666666666667e-06,
      "loss": 1.971,
      "step": 350
    },
    {
      "epoch": 0.73125,
      "grad_norm": 2.078732490539551,
      "learning_rate": 7.3125e-06,
      "loss": 1.8773,
      "step": 351
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 3.3587279319763184,
      "learning_rate": 7.333333333333333e-06,
      "loss": 1.8218,
      "step": 352
    },
    {
      "epoch": 0.7354166666666667,
      "grad_norm": 1.7917520999908447,
      "learning_rate": 7.354166666666668e-06,
      "loss": 1.956,
      "step": 353
    },
    {
      "epoch": 0.7375,
      "grad_norm": 1.944817304611206,
      "learning_rate": 7.375000000000001e-06,
      "loss": 2.0219,
      "step": 354
    },
    {
      "epoch": 0.7395833333333334,
      "grad_norm": 2.1350677013397217,
      "learning_rate": 7.395833333333335e-06,
      "loss": 1.8951,
      "step": 355
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.5405153036117554,
      "learning_rate": 7.416666666666668e-06,
      "loss": 1.9479,
      "step": 356
    },
    {
      "epoch": 0.74375,
      "grad_norm": 3.1375017166137695,
      "learning_rate": 7.437500000000001e-06,
      "loss": 2.0578,
      "step": 357
    },
    {
      "epoch": 0.7458333333333333,
      "grad_norm": 2.875251293182373,
      "learning_rate": 7.4583333333333345e-06,
      "loss": 1.8975,
      "step": 358
    },
    {
      "epoch": 0.7479166666666667,
      "grad_norm": 2.002790689468384,
      "learning_rate": 7.479166666666668e-06,
      "loss": 1.8771,
      "step": 359
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.062234401702881,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.9183,
      "step": 360
    },
    {
      "epoch": 0.7520833333333333,
      "grad_norm": 1.4805147647857666,
      "learning_rate": 7.5208333333333335e-06,
      "loss": 1.9061,
      "step": 361
    },
    {
      "epoch": 0.7541666666666667,
      "grad_norm": 1.8906422853469849,
      "learning_rate": 7.541666666666667e-06,
      "loss": 1.8733,
      "step": 362
    },
    {
      "epoch": 0.75625,
      "grad_norm": 1.643729567527771,
      "learning_rate": 7.5625e-06,
      "loss": 1.9214,
      "step": 363
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 1.7192820310592651,
      "learning_rate": 7.583333333333333e-06,
      "loss": 1.9548,
      "step": 364
    },
    {
      "epoch": 0.7604166666666666,
      "grad_norm": 2.062532663345337,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 1.8739,
      "step": 365
    },
    {
      "epoch": 0.7625,
      "grad_norm": 2.0105226039886475,
      "learning_rate": 7.625e-06,
      "loss": 1.9435,
      "step": 366
    },
    {
      "epoch": 0.7645833333333333,
      "grad_norm": 3.4199979305267334,
      "learning_rate": 7.645833333333334e-06,
      "loss": 1.8157,
      "step": 367
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 2.197448492050171,
      "learning_rate": 7.666666666666667e-06,
      "loss": 1.9233,
      "step": 368
    },
    {
      "epoch": 0.76875,
      "grad_norm": 1.9707907438278198,
      "learning_rate": 7.6875e-06,
      "loss": 1.983,
      "step": 369
    },
    {
      "epoch": 0.7708333333333334,
      "grad_norm": 1.8391448259353638,
      "learning_rate": 7.708333333333334e-06,
      "loss": 1.92,
      "step": 370
    },
    {
      "epoch": 0.7729166666666667,
      "grad_norm": 1.7130945920944214,
      "learning_rate": 7.729166666666667e-06,
      "loss": 1.9186,
      "step": 371
    },
    {
      "epoch": 0.775,
      "grad_norm": 3.092482089996338,
      "learning_rate": 7.75e-06,
      "loss": 1.9811,
      "step": 372
    },
    {
      "epoch": 0.7770833333333333,
      "grad_norm": 2.120124101638794,
      "learning_rate": 7.770833333333334e-06,
      "loss": 1.9811,
      "step": 373
    },
    {
      "epoch": 0.7791666666666667,
      "grad_norm": 2.4088916778564453,
      "learning_rate": 7.791666666666667e-06,
      "loss": 1.9767,
      "step": 374
    },
    {
      "epoch": 0.78125,
      "grad_norm": 2.309558153152466,
      "learning_rate": 7.8125e-06,
      "loss": 1.8216,
      "step": 375
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 2.0857598781585693,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.9516,
      "step": 376
    },
    {
      "epoch": 0.7854166666666667,
      "grad_norm": 1.9991323947906494,
      "learning_rate": 7.854166666666667e-06,
      "loss": 1.9264,
      "step": 377
    },
    {
      "epoch": 0.7875,
      "grad_norm": 2.026289939880371,
      "learning_rate": 7.875e-06,
      "loss": 1.8872,
      "step": 378
    },
    {
      "epoch": 0.7895833333333333,
      "grad_norm": 2.5416059494018555,
      "learning_rate": 7.895833333333333e-06,
      "loss": 1.8943,
      "step": 379
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 2.088346481323242,
      "learning_rate": 7.916666666666667e-06,
      "loss": 1.8696,
      "step": 380
    },
    {
      "epoch": 0.79375,
      "grad_norm": 1.9497150182724,
      "learning_rate": 7.9375e-06,
      "loss": 1.967,
      "step": 381
    },
    {
      "epoch": 0.7958333333333333,
      "grad_norm": 1.984463095664978,
      "learning_rate": 7.958333333333333e-06,
      "loss": 1.9983,
      "step": 382
    },
    {
      "epoch": 0.7979166666666667,
      "grad_norm": 2.5847599506378174,
      "learning_rate": 7.979166666666668e-06,
      "loss": 2.0365,
      "step": 383
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.0131874084472656,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9248,
      "step": 384
    },
    {
      "epoch": 0.8020833333333334,
      "grad_norm": 2.3451294898986816,
      "learning_rate": 8.020833333333335e-06,
      "loss": 1.9017,
      "step": 385
    },
    {
      "epoch": 0.8041666666666667,
      "grad_norm": 2.1853044033050537,
      "learning_rate": 8.041666666666668e-06,
      "loss": 2.0141,
      "step": 386
    },
    {
      "epoch": 0.80625,
      "grad_norm": 3.125521421432495,
      "learning_rate": 8.062500000000001e-06,
      "loss": 1.9858,
      "step": 387
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 2.9561851024627686,
      "learning_rate": 8.083333333333334e-06,
      "loss": 1.9553,
      "step": 388
    },
    {
      "epoch": 0.8104166666666667,
      "grad_norm": 2.2265894412994385,
      "learning_rate": 8.104166666666668e-06,
      "loss": 1.8757,
      "step": 389
    },
    {
      "epoch": 0.8125,
      "grad_norm": 3.025200605392456,
      "learning_rate": 8.125000000000001e-06,
      "loss": 1.9598,
      "step": 390
    },
    {
      "epoch": 0.8145833333333333,
      "grad_norm": 3.588319778442383,
      "learning_rate": 8.145833333333334e-06,
      "loss": 1.8099,
      "step": 391
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.780010461807251,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.933,
      "step": 392
    },
    {
      "epoch": 0.81875,
      "grad_norm": 2.4813098907470703,
      "learning_rate": 8.1875e-06,
      "loss": 1.8739,
      "step": 393
    },
    {
      "epoch": 0.8208333333333333,
      "grad_norm": 2.6654679775238037,
      "learning_rate": 8.208333333333334e-06,
      "loss": 1.8741,
      "step": 394
    },
    {
      "epoch": 0.8229166666666666,
      "grad_norm": 2.364204168319702,
      "learning_rate": 8.229166666666667e-06,
      "loss": 1.9165,
      "step": 395
    },
    {
      "epoch": 0.825,
      "grad_norm": 2.3084006309509277,
      "learning_rate": 8.25e-06,
      "loss": 1.9259,
      "step": 396
    },
    {
      "epoch": 0.8270833333333333,
      "grad_norm": 2.3557353019714355,
      "learning_rate": 8.270833333333334e-06,
      "loss": 2.0113,
      "step": 397
    },
    {
      "epoch": 0.8291666666666667,
      "grad_norm": 3.0592455863952637,
      "learning_rate": 8.291666666666667e-06,
      "loss": 1.9614,
      "step": 398
    },
    {
      "epoch": 0.83125,
      "grad_norm": 2.5061662197113037,
      "learning_rate": 8.3125e-06,
      "loss": 1.8544,
      "step": 399
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.4533064365386963,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.9939,
      "step": 400
    },
    {
      "epoch": 0.8354166666666667,
      "grad_norm": 1.9671192169189453,
      "learning_rate": 8.354166666666667e-06,
      "loss": 1.9293,
      "step": 401
    },
    {
      "epoch": 0.8375,
      "grad_norm": 2.297431707382202,
      "learning_rate": 8.375e-06,
      "loss": 1.8417,
      "step": 402
    },
    {
      "epoch": 0.8395833333333333,
      "grad_norm": 2.3937785625457764,
      "learning_rate": 8.395833333333334e-06,
      "loss": 1.8477,
      "step": 403
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 2.198958158493042,
      "learning_rate": 8.416666666666667e-06,
      "loss": 1.8807,
      "step": 404
    },
    {
      "epoch": 0.84375,
      "grad_norm": 3.846808671951294,
      "learning_rate": 8.4375e-06,
      "loss": 1.983,
      "step": 405
    },
    {
      "epoch": 0.8458333333333333,
      "grad_norm": 2.3106722831726074,
      "learning_rate": 8.458333333333333e-06,
      "loss": 1.9765,
      "step": 406
    },
    {
      "epoch": 0.8479166666666667,
      "grad_norm": 3.2451775074005127,
      "learning_rate": 8.479166666666667e-06,
      "loss": 1.9076,
      "step": 407
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.9123587608337402,
      "learning_rate": 8.5e-06,
      "loss": 1.9559,
      "step": 408
    },
    {
      "epoch": 0.8520833333333333,
      "grad_norm": 1.9071195125579834,
      "learning_rate": 8.520833333333333e-06,
      "loss": 1.978,
      "step": 409
    },
    {
      "epoch": 0.8541666666666666,
      "grad_norm": 1.9764184951782227,
      "learning_rate": 8.541666666666666e-06,
      "loss": 1.9385,
      "step": 410
    },
    {
      "epoch": 0.85625,
      "grad_norm": 2.1670868396759033,
      "learning_rate": 8.5625e-06,
      "loss": 1.8938,
      "step": 411
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 3.671736478805542,
      "learning_rate": 8.583333333333333e-06,
      "loss": 1.8379,
      "step": 412
    },
    {
      "epoch": 0.8604166666666667,
      "grad_norm": 1.7840386629104614,
      "learning_rate": 8.604166666666668e-06,
      "loss": 1.9278,
      "step": 413
    },
    {
      "epoch": 0.8625,
      "grad_norm": 1.9531755447387695,
      "learning_rate": 8.625000000000001e-06,
      "loss": 1.9944,
      "step": 414
    },
    {
      "epoch": 0.8645833333333334,
      "grad_norm": 2.387019634246826,
      "learning_rate": 8.645833333333335e-06,
      "loss": 1.8599,
      "step": 415
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.8335912227630615,
      "learning_rate": 8.666666666666668e-06,
      "loss": 1.9212,
      "step": 416
    },
    {
      "epoch": 0.86875,
      "grad_norm": 1.778045892715454,
      "learning_rate": 8.687500000000001e-06,
      "loss": 1.9302,
      "step": 417
    },
    {
      "epoch": 0.8708333333333333,
      "grad_norm": 3.5218589305877686,
      "learning_rate": 8.708333333333334e-06,
      "loss": 1.8705,
      "step": 418
    },
    {
      "epoch": 0.8729166666666667,
      "grad_norm": 2.526855707168579,
      "learning_rate": 8.729166666666668e-06,
      "loss": 1.8373,
      "step": 419
    },
    {
      "epoch": 0.875,
      "grad_norm": 2.867689609527588,
      "learning_rate": 8.750000000000001e-06,
      "loss": 1.9301,
      "step": 420
    },
    {
      "epoch": 0.8770833333333333,
      "grad_norm": 2.332643508911133,
      "learning_rate": 8.770833333333334e-06,
      "loss": 1.9473,
      "step": 421
    },
    {
      "epoch": 0.8791666666666667,
      "grad_norm": 2.803417444229126,
      "learning_rate": 8.791666666666667e-06,
      "loss": 1.9552,
      "step": 422
    },
    {
      "epoch": 0.88125,
      "grad_norm": 2.178041696548462,
      "learning_rate": 8.8125e-06,
      "loss": 1.9089,
      "step": 423
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 2.666480541229248,
      "learning_rate": 8.833333333333334e-06,
      "loss": 1.8804,
      "step": 424
    },
    {
      "epoch": 0.8854166666666666,
      "grad_norm": 1.9336738586425781,
      "learning_rate": 8.854166666666667e-06,
      "loss": 1.9304,
      "step": 425
    },
    {
      "epoch": 0.8875,
      "grad_norm": 3.1005642414093018,
      "learning_rate": 8.875e-06,
      "loss": 1.882,
      "step": 426
    },
    {
      "epoch": 0.8895833333333333,
      "grad_norm": 2.1296818256378174,
      "learning_rate": 8.895833333333334e-06,
      "loss": 1.9388,
      "step": 427
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 2.2421162128448486,
      "learning_rate": 8.916666666666667e-06,
      "loss": 1.9368,
      "step": 428
    },
    {
      "epoch": 0.89375,
      "grad_norm": 3.2823519706726074,
      "learning_rate": 8.9375e-06,
      "loss": 1.9429,
      "step": 429
    },
    {
      "epoch": 0.8958333333333334,
      "grad_norm": 2.41601300239563,
      "learning_rate": 8.958333333333334e-06,
      "loss": 1.8791,
      "step": 430
    },
    {
      "epoch": 0.8979166666666667,
      "grad_norm": 2.786959171295166,
      "learning_rate": 8.979166666666667e-06,
      "loss": 1.8723,
      "step": 431
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.1478869915008545,
      "learning_rate": 9e-06,
      "loss": 1.9003,
      "step": 432
    },
    {
      "epoch": 0.9020833333333333,
      "grad_norm": 2.4939544200897217,
      "learning_rate": 9.020833333333334e-06,
      "loss": 1.9609,
      "step": 433
    },
    {
      "epoch": 0.9041666666666667,
      "grad_norm": 2.6142172813415527,
      "learning_rate": 9.041666666666667e-06,
      "loss": 2.0389,
      "step": 434
    },
    {
      "epoch": 0.90625,
      "grad_norm": 2.2247426509857178,
      "learning_rate": 9.0625e-06,
      "loss": 1.9377,
      "step": 435
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 3.047226667404175,
      "learning_rate": 9.083333333333333e-06,
      "loss": 1.8386,
      "step": 436
    },
    {
      "epoch": 0.9104166666666667,
      "grad_norm": 3.1124765872955322,
      "learning_rate": 9.104166666666667e-06,
      "loss": 1.9427,
      "step": 437
    },
    {
      "epoch": 0.9125,
      "grad_norm": 1.5476534366607666,
      "learning_rate": 9.125e-06,
      "loss": 1.9153,
      "step": 438
    },
    {
      "epoch": 0.9145833333333333,
      "grad_norm": 2.118931770324707,
      "learning_rate": 9.145833333333333e-06,
      "loss": 1.9562,
      "step": 439
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 3.014188289642334,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.8257,
      "step": 440
    },
    {
      "epoch": 0.91875,
      "grad_norm": 3.3745079040527344,
      "learning_rate": 9.1875e-06,
      "loss": 1.8304,
      "step": 441
    },
    {
      "epoch": 0.9208333333333333,
      "grad_norm": 3.294330596923828,
      "learning_rate": 9.208333333333333e-06,
      "loss": 1.9924,
      "step": 442
    },
    {
      "epoch": 0.9229166666666667,
      "grad_norm": 4.0276780128479,
      "learning_rate": 9.229166666666668e-06,
      "loss": 1.8786,
      "step": 443
    },
    {
      "epoch": 0.925,
      "grad_norm": 2.2814793586730957,
      "learning_rate": 9.250000000000001e-06,
      "loss": 1.939,
      "step": 444
    },
    {
      "epoch": 0.9270833333333334,
      "grad_norm": 2.984734535217285,
      "learning_rate": 9.270833333333334e-06,
      "loss": 1.8228,
      "step": 445
    },
    {
      "epoch": 0.9291666666666667,
      "grad_norm": 2.9730217456817627,
      "learning_rate": 9.291666666666668e-06,
      "loss": 1.8574,
      "step": 446
    },
    {
      "epoch": 0.93125,
      "grad_norm": 2.9107582569122314,
      "learning_rate": 9.312500000000001e-06,
      "loss": 1.9355,
      "step": 447
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.4466540813446045,
      "learning_rate": 9.333333333333334e-06,
      "loss": 1.9554,
      "step": 448
    },
    {
      "epoch": 0.9354166666666667,
      "grad_norm": 2.942814350128174,
      "learning_rate": 9.354166666666668e-06,
      "loss": 1.9445,
      "step": 449
    },
    {
      "epoch": 0.9375,
      "grad_norm": 2.9586877822875977,
      "learning_rate": 9.375000000000001e-06,
      "loss": 1.8989,
      "step": 450
    },
    {
      "epoch": 0.9395833333333333,
      "grad_norm": 2.871242046356201,
      "learning_rate": 9.395833333333334e-06,
      "loss": 1.8333,
      "step": 451
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 2.381859064102173,
      "learning_rate": 9.416666666666667e-06,
      "loss": 1.9094,
      "step": 452
    },
    {
      "epoch": 0.94375,
      "grad_norm": 2.848283529281616,
      "learning_rate": 9.4375e-06,
      "loss": 1.9008,
      "step": 453
    },
    {
      "epoch": 0.9458333333333333,
      "grad_norm": 3.2651169300079346,
      "learning_rate": 9.458333333333334e-06,
      "loss": 1.8077,
      "step": 454
    },
    {
      "epoch": 0.9479166666666666,
      "grad_norm": 2.916912794113159,
      "learning_rate": 9.479166666666667e-06,
      "loss": 1.9496,
      "step": 455
    },
    {
      "epoch": 0.95,
      "grad_norm": 3.27787184715271,
      "learning_rate": 9.5e-06,
      "loss": 1.8057,
      "step": 456
    },
    {
      "epoch": 0.9520833333333333,
      "grad_norm": 3.9563183784484863,
      "learning_rate": 9.520833333333334e-06,
      "loss": 1.804,
      "step": 457
    },
    {
      "epoch": 0.9541666666666667,
      "grad_norm": 3.298861026763916,
      "learning_rate": 9.541666666666669e-06,
      "loss": 1.8104,
      "step": 458
    },
    {
      "epoch": 0.95625,
      "grad_norm": 2.031202554702759,
      "learning_rate": 9.562500000000002e-06,
      "loss": 1.9307,
      "step": 459
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 3.558314561843872,
      "learning_rate": 9.583333333333335e-06,
      "loss": 2.0196,
      "step": 460
    },
    {
      "epoch": 0.9604166666666667,
      "grad_norm": 3.946951150894165,
      "learning_rate": 9.604166666666669e-06,
      "loss": 1.7687,
      "step": 461
    },
    {
      "epoch": 0.9625,
      "grad_norm": 4.042377948760986,
      "learning_rate": 9.625e-06,
      "loss": 1.8551,
      "step": 462
    },
    {
      "epoch": 0.9645833333333333,
      "grad_norm": 5.3062968254089355,
      "learning_rate": 9.645833333333333e-06,
      "loss": 1.6861,
      "step": 463
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 5.212461948394775,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.9386,
      "step": 464
    },
    {
      "epoch": 0.96875,
      "grad_norm": 2.7018473148345947,
      "learning_rate": 9.6875e-06,
      "loss": 1.8702,
      "step": 465
    },
    {
      "epoch": 0.9708333333333333,
      "grad_norm": 4.50298547744751,
      "learning_rate": 9.708333333333333e-06,
      "loss": 1.7242,
      "step": 466
    },
    {
      "epoch": 0.9729166666666667,
      "grad_norm": 3.341373920440674,
      "learning_rate": 9.729166666666667e-06,
      "loss": 1.8315,
      "step": 467
    },
    {
      "epoch": 0.975,
      "grad_norm": 4.250161170959473,
      "learning_rate": 9.75e-06,
      "loss": 1.7718,
      "step": 468
    },
    {
      "epoch": 0.9770833333333333,
      "grad_norm": 2.8732004165649414,
      "learning_rate": 9.770833333333333e-06,
      "loss": 1.9624,
      "step": 469
    },
    {
      "epoch": 0.9791666666666666,
      "grad_norm": 4.4935526847839355,
      "learning_rate": 9.791666666666666e-06,
      "loss": 1.7134,
      "step": 470
    },
    {
      "epoch": 0.98125,
      "grad_norm": 2.8006129264831543,
      "learning_rate": 9.8125e-06,
      "loss": 1.8971,
      "step": 471
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 4.489307403564453,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.901,
      "step": 472
    },
    {
      "epoch": 0.9854166666666667,
      "grad_norm": 3.762315034866333,
      "learning_rate": 9.854166666666668e-06,
      "loss": 1.8939,
      "step": 473
    },
    {
      "epoch": 0.9875,
      "grad_norm": 3.8374457359313965,
      "learning_rate": 9.875000000000001e-06,
      "loss": 1.7495,
      "step": 474
    },
    {
      "epoch": 0.9895833333333334,
      "grad_norm": 4.136640548706055,
      "learning_rate": 9.895833333333334e-06,
      "loss": 1.6997,
      "step": 475
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 6.038689613342285,
      "learning_rate": 9.916666666666668e-06,
      "loss": 1.9304,
      "step": 476
    },
    {
      "epoch": 0.99375,
      "grad_norm": 3.2505555152893066,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.8926,
      "step": 477
    },
    {
      "epoch": 0.9958333333333333,
      "grad_norm": 5.717676639556885,
      "learning_rate": 9.958333333333334e-06,
      "loss": 1.9017,
      "step": 478
    },
    {
      "epoch": 0.9979166666666667,
      "grad_norm": 5.054263114929199,
      "learning_rate": 9.979166666666668e-06,
      "loss": 1.669,
      "step": 479
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.6387014389038086,
      "learning_rate": 1e-05,
      "loss": 1.9121,
      "step": 480
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.3055555555555556,
      "eval_f1": 0.16282809420064323,
      "eval_loss": 1.81491219997406,
      "eval_runtime": 36.255,
      "eval_samples_per_second": 4.965,
      "eval_steps_per_second": 2.482,
      "step": 480
    },
    {
      "epoch": 1.0020833333333334,
      "grad_norm": 3.9957940578460693,
      "learning_rate": 9.997685185185187e-06,
      "loss": 1.8223,
      "step": 481
    },
    {
      "epoch": 1.0041666666666667,
      "grad_norm": 4.82511043548584,
      "learning_rate": 9.995370370370371e-06,
      "loss": 1.836,
      "step": 482
    },
    {
      "epoch": 1.00625,
      "grad_norm": 5.271088123321533,
      "learning_rate": 9.993055555555557e-06,
      "loss": 1.8991,
      "step": 483
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 6.162159442901611,
      "learning_rate": 9.990740740740741e-06,
      "loss": 1.71,
      "step": 484
    },
    {
      "epoch": 1.0104166666666667,
      "grad_norm": 7.287150859832764,
      "learning_rate": 9.988425925925927e-06,
      "loss": 1.969,
      "step": 485
    },
    {
      "epoch": 1.0125,
      "grad_norm": 4.839508056640625,
      "learning_rate": 9.986111111111111e-06,
      "loss": 1.9532,
      "step": 486
    },
    {
      "epoch": 1.0145833333333334,
      "grad_norm": 3.482024669647217,
      "learning_rate": 9.983796296296297e-06,
      "loss": 1.9416,
      "step": 487
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 6.045909404754639,
      "learning_rate": 9.981481481481482e-06,
      "loss": 2.0125,
      "step": 488
    },
    {
      "epoch": 1.01875,
      "grad_norm": 3.6878304481506348,
      "learning_rate": 9.979166666666668e-06,
      "loss": 1.8755,
      "step": 489
    },
    {
      "epoch": 1.0208333333333333,
      "grad_norm": 2.6828999519348145,
      "learning_rate": 9.976851851851853e-06,
      "loss": 1.8574,
      "step": 490
    },
    {
      "epoch": 1.0229166666666667,
      "grad_norm": 2.6766555309295654,
      "learning_rate": 9.974537037037038e-06,
      "loss": 1.9029,
      "step": 491
    },
    {
      "epoch": 1.025,
      "grad_norm": 5.905770778656006,
      "learning_rate": 9.972222222222224e-06,
      "loss": 1.9696,
      "step": 492
    },
    {
      "epoch": 1.0270833333333333,
      "grad_norm": 4.596978664398193,
      "learning_rate": 9.969907407407408e-06,
      "loss": 1.8437,
      "step": 493
    },
    {
      "epoch": 1.0291666666666666,
      "grad_norm": 3.7069826126098633,
      "learning_rate": 9.967592592592594e-06,
      "loss": 1.8519,
      "step": 494
    },
    {
      "epoch": 1.03125,
      "grad_norm": 2.9039828777313232,
      "learning_rate": 9.965277777777778e-06,
      "loss": 1.9705,
      "step": 495
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 3.211435317993164,
      "learning_rate": 9.962962962962964e-06,
      "loss": 1.9396,
      "step": 496
    },
    {
      "epoch": 1.0354166666666667,
      "grad_norm": 9.780223846435547,
      "learning_rate": 9.960648148148148e-06,
      "loss": 1.5244,
      "step": 497
    },
    {
      "epoch": 1.0375,
      "grad_norm": 4.749242305755615,
      "learning_rate": 9.958333333333334e-06,
      "loss": 1.8372,
      "step": 498
    },
    {
      "epoch": 1.0395833333333333,
      "grad_norm": 6.0331130027771,
      "learning_rate": 9.95601851851852e-06,
      "loss": 1.7148,
      "step": 499
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 3.7071900367736816,
      "learning_rate": 9.953703703703704e-06,
      "loss": 1.9079,
      "step": 500
    },
    {
      "epoch": 1.04375,
      "grad_norm": 5.206056118011475,
      "learning_rate": 9.95138888888889e-06,
      "loss": 1.8481,
      "step": 501
    },
    {
      "epoch": 1.0458333333333334,
      "grad_norm": 3.0550131797790527,
      "learning_rate": 9.949074074074075e-06,
      "loss": 1.921,
      "step": 502
    },
    {
      "epoch": 1.0479166666666666,
      "grad_norm": 6.710261821746826,
      "learning_rate": 9.94675925925926e-06,
      "loss": 1.7429,
      "step": 503
    },
    {
      "epoch": 1.05,
      "grad_norm": 5.953681468963623,
      "learning_rate": 9.944444444444445e-06,
      "loss": 1.8628,
      "step": 504
    },
    {
      "epoch": 1.0520833333333333,
      "grad_norm": 4.84552526473999,
      "learning_rate": 9.942129629629629e-06,
      "loss": 1.7804,
      "step": 505
    },
    {
      "epoch": 1.0541666666666667,
      "grad_norm": 5.536280155181885,
      "learning_rate": 9.939814814814815e-06,
      "loss": 1.9071,
      "step": 506
    },
    {
      "epoch": 1.05625,
      "grad_norm": 5.123532295227051,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.7412,
      "step": 507
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 4.363223552703857,
      "learning_rate": 9.935185185185185e-06,
      "loss": 1.829,
      "step": 508
    },
    {
      "epoch": 1.0604166666666666,
      "grad_norm": 6.415248394012451,
      "learning_rate": 9.932870370370371e-06,
      "loss": 1.6421,
      "step": 509
    },
    {
      "epoch": 1.0625,
      "grad_norm": 4.71692419052124,
      "learning_rate": 9.930555555555557e-06,
      "loss": 1.922,
      "step": 510
    },
    {
      "epoch": 1.0645833333333334,
      "grad_norm": 5.0998311042785645,
      "learning_rate": 9.928240740740741e-06,
      "loss": 1.7121,
      "step": 511
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 6.853371620178223,
      "learning_rate": 9.925925925925927e-06,
      "loss": 1.8206,
      "step": 512
    },
    {
      "epoch": 1.06875,
      "grad_norm": 4.425285816192627,
      "learning_rate": 9.923611111111112e-06,
      "loss": 1.7957,
      "step": 513
    },
    {
      "epoch": 1.0708333333333333,
      "grad_norm": 13.00149917602539,
      "learning_rate": 9.921296296296296e-06,
      "loss": 1.488,
      "step": 514
    },
    {
      "epoch": 1.0729166666666667,
      "grad_norm": 9.333695411682129,
      "learning_rate": 9.918981481481482e-06,
      "loss": 1.9488,
      "step": 515
    },
    {
      "epoch": 1.075,
      "grad_norm": 16.238433837890625,
      "learning_rate": 9.916666666666668e-06,
      "loss": 1.4415,
      "step": 516
    },
    {
      "epoch": 1.0770833333333334,
      "grad_norm": 4.884713172912598,
      "learning_rate": 9.914351851851852e-06,
      "loss": 1.8282,
      "step": 517
    },
    {
      "epoch": 1.0791666666666666,
      "grad_norm": 4.459047794342041,
      "learning_rate": 9.912037037037038e-06,
      "loss": 1.7382,
      "step": 518
    },
    {
      "epoch": 1.08125,
      "grad_norm": 6.032847881317139,
      "learning_rate": 9.909722222222224e-06,
      "loss": 2.0719,
      "step": 519
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 3.4831290245056152,
      "learning_rate": 9.907407407407408e-06,
      "loss": 1.9404,
      "step": 520
    },
    {
      "epoch": 1.0854166666666667,
      "grad_norm": 5.62066125869751,
      "learning_rate": 9.905092592592594e-06,
      "loss": 1.5939,
      "step": 521
    },
    {
      "epoch": 1.0875,
      "grad_norm": 11.382500648498535,
      "learning_rate": 9.902777777777778e-06,
      "loss": 2.1294,
      "step": 522
    },
    {
      "epoch": 1.0895833333333333,
      "grad_norm": 6.313187122344971,
      "learning_rate": 9.900462962962963e-06,
      "loss": 2.1261,
      "step": 523
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 7.109322547912598,
      "learning_rate": 9.898148148148148e-06,
      "loss": 1.8011,
      "step": 524
    },
    {
      "epoch": 1.09375,
      "grad_norm": 6.267740726470947,
      "learning_rate": 9.895833333333334e-06,
      "loss": 1.9473,
      "step": 525
    },
    {
      "epoch": 1.0958333333333334,
      "grad_norm": 6.748660087585449,
      "learning_rate": 9.893518518518519e-06,
      "loss": 1.7308,
      "step": 526
    },
    {
      "epoch": 1.0979166666666667,
      "grad_norm": 7.327108383178711,
      "learning_rate": 9.891203703703705e-06,
      "loss": 1.784,
      "step": 527
    },
    {
      "epoch": 1.1,
      "grad_norm": 3.7364895343780518,
      "learning_rate": 9.88888888888889e-06,
      "loss": 1.8779,
      "step": 528
    },
    {
      "epoch": 1.1020833333333333,
      "grad_norm": 6.6053009033203125,
      "learning_rate": 9.886574074074075e-06,
      "loss": 1.9554,
      "step": 529
    },
    {
      "epoch": 1.1041666666666667,
      "grad_norm": 6.762101173400879,
      "learning_rate": 9.88425925925926e-06,
      "loss": 1.8191,
      "step": 530
    },
    {
      "epoch": 1.10625,
      "grad_norm": 4.766901016235352,
      "learning_rate": 9.881944444444445e-06,
      "loss": 1.6929,
      "step": 531
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 8.623682975769043,
      "learning_rate": 9.87962962962963e-06,
      "loss": 1.9649,
      "step": 532
    },
    {
      "epoch": 1.1104166666666666,
      "grad_norm": 5.640347957611084,
      "learning_rate": 9.877314814814815e-06,
      "loss": 1.9768,
      "step": 533
    },
    {
      "epoch": 1.1125,
      "grad_norm": 10.560054779052734,
      "learning_rate": 9.875000000000001e-06,
      "loss": 2.1983,
      "step": 534
    },
    {
      "epoch": 1.1145833333333333,
      "grad_norm": 6.9880266189575195,
      "learning_rate": 9.872685185185185e-06,
      "loss": 1.5029,
      "step": 535
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 6.03161096572876,
      "learning_rate": 9.870370370370371e-06,
      "loss": 1.9606,
      "step": 536
    },
    {
      "epoch": 1.11875,
      "grad_norm": 6.219241619110107,
      "learning_rate": 9.868055555555557e-06,
      "loss": 1.8939,
      "step": 537
    },
    {
      "epoch": 1.1208333333333333,
      "grad_norm": 7.680005073547363,
      "learning_rate": 9.865740740740742e-06,
      "loss": 1.9168,
      "step": 538
    },
    {
      "epoch": 1.1229166666666666,
      "grad_norm": 3.7524220943450928,
      "learning_rate": 9.863425925925928e-06,
      "loss": 2.0279,
      "step": 539
    },
    {
      "epoch": 1.125,
      "grad_norm": 6.014187812805176,
      "learning_rate": 9.861111111111112e-06,
      "loss": 1.7289,
      "step": 540
    },
    {
      "epoch": 1.1270833333333332,
      "grad_norm": 7.165669918060303,
      "learning_rate": 9.858796296296298e-06,
      "loss": 1.3257,
      "step": 541
    },
    {
      "epoch": 1.1291666666666667,
      "grad_norm": 4.568291187286377,
      "learning_rate": 9.856481481481482e-06,
      "loss": 1.873,
      "step": 542
    },
    {
      "epoch": 1.13125,
      "grad_norm": 6.363565444946289,
      "learning_rate": 9.854166666666668e-06,
      "loss": 1.6482,
      "step": 543
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 7.158909320831299,
      "learning_rate": 9.851851851851852e-06,
      "loss": 1.4412,
      "step": 544
    },
    {
      "epoch": 1.1354166666666667,
      "grad_norm": 4.5999579429626465,
      "learning_rate": 9.849537037037038e-06,
      "loss": 1.7792,
      "step": 545
    },
    {
      "epoch": 1.1375,
      "grad_norm": 5.560220718383789,
      "learning_rate": 9.847222222222224e-06,
      "loss": 1.6991,
      "step": 546
    },
    {
      "epoch": 1.1395833333333334,
      "grad_norm": 9.544918060302734,
      "learning_rate": 9.844907407407408e-06,
      "loss": 2.2715,
      "step": 547
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 8.637630462646484,
      "learning_rate": 9.842592592592594e-06,
      "loss": 1.7003,
      "step": 548
    },
    {
      "epoch": 1.14375,
      "grad_norm": 20.41930389404297,
      "learning_rate": 9.840277777777778e-06,
      "loss": 1.6387,
      "step": 549
    },
    {
      "epoch": 1.1458333333333333,
      "grad_norm": 7.477566242218018,
      "learning_rate": 9.837962962962964e-06,
      "loss": 1.8164,
      "step": 550
    },
    {
      "epoch": 1.1479166666666667,
      "grad_norm": 9.934835433959961,
      "learning_rate": 9.835648148148149e-06,
      "loss": 1.2589,
      "step": 551
    },
    {
      "epoch": 1.15,
      "grad_norm": 7.484676361083984,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.6268,
      "step": 552
    },
    {
      "epoch": 1.1520833333333333,
      "grad_norm": 8.335282325744629,
      "learning_rate": 9.831018518518519e-06,
      "loss": 1.8808,
      "step": 553
    },
    {
      "epoch": 1.1541666666666668,
      "grad_norm": 5.433446884155273,
      "learning_rate": 9.828703703703705e-06,
      "loss": 1.5792,
      "step": 554
    },
    {
      "epoch": 1.15625,
      "grad_norm": 6.404886245727539,
      "learning_rate": 9.826388888888889e-06,
      "loss": 1.7268,
      "step": 555
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 6.680817604064941,
      "learning_rate": 9.824074074074075e-06,
      "loss": 1.9123,
      "step": 556
    },
    {
      "epoch": 1.1604166666666667,
      "grad_norm": 7.185735702514648,
      "learning_rate": 9.821759259259261e-06,
      "loss": 1.8948,
      "step": 557
    },
    {
      "epoch": 1.1625,
      "grad_norm": 10.562202453613281,
      "learning_rate": 9.819444444444445e-06,
      "loss": 1.8017,
      "step": 558
    },
    {
      "epoch": 1.1645833333333333,
      "grad_norm": 7.236743450164795,
      "learning_rate": 9.817129629629631e-06,
      "loss": 1.572,
      "step": 559
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 8.321598052978516,
      "learning_rate": 9.814814814814815e-06,
      "loss": 1.9949,
      "step": 560
    },
    {
      "epoch": 1.16875,
      "grad_norm": 35.14738082885742,
      "learning_rate": 9.8125e-06,
      "loss": 1.7215,
      "step": 561
    },
    {
      "epoch": 1.1708333333333334,
      "grad_norm": 7.668934345245361,
      "learning_rate": 9.810185185185186e-06,
      "loss": 2.151,
      "step": 562
    },
    {
      "epoch": 1.1729166666666666,
      "grad_norm": 8.58651351928711,
      "learning_rate": 9.807870370370372e-06,
      "loss": 1.6168,
      "step": 563
    },
    {
      "epoch": 1.175,
      "grad_norm": 4.863114356994629,
      "learning_rate": 9.805555555555556e-06,
      "loss": 1.8009,
      "step": 564
    },
    {
      "epoch": 1.1770833333333333,
      "grad_norm": 6.208588123321533,
      "learning_rate": 9.803240740740742e-06,
      "loss": 1.9224,
      "step": 565
    },
    {
      "epoch": 1.1791666666666667,
      "grad_norm": 8.275675773620605,
      "learning_rate": 9.800925925925928e-06,
      "loss": 1.6329,
      "step": 566
    },
    {
      "epoch": 1.18125,
      "grad_norm": 9.130366325378418,
      "learning_rate": 9.798611111111112e-06,
      "loss": 2.1039,
      "step": 567
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 7.743878364562988,
      "learning_rate": 9.796296296296298e-06,
      "loss": 1.8378,
      "step": 568
    },
    {
      "epoch": 1.1854166666666668,
      "grad_norm": 5.089112281799316,
      "learning_rate": 9.793981481481482e-06,
      "loss": 1.6882,
      "step": 569
    },
    {
      "epoch": 1.1875,
      "grad_norm": 8.366334915161133,
      "learning_rate": 9.791666666666666e-06,
      "loss": 1.8154,
      "step": 570
    },
    {
      "epoch": 1.1895833333333332,
      "grad_norm": 8.717679023742676,
      "learning_rate": 9.789351851851852e-06,
      "loss": 1.7138,
      "step": 571
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 6.941291332244873,
      "learning_rate": 9.787037037037038e-06,
      "loss": 1.8821,
      "step": 572
    },
    {
      "epoch": 1.19375,
      "grad_norm": 6.956091403961182,
      "learning_rate": 9.784722222222223e-06,
      "loss": 1.6033,
      "step": 573
    },
    {
      "epoch": 1.1958333333333333,
      "grad_norm": 32.79336166381836,
      "learning_rate": 9.782407407407408e-06,
      "loss": 1.8655,
      "step": 574
    },
    {
      "epoch": 1.1979166666666667,
      "grad_norm": 5.742950916290283,
      "learning_rate": 9.780092592592594e-06,
      "loss": 1.5505,
      "step": 575
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.779974937438965,
      "learning_rate": 9.777777777777779e-06,
      "loss": 1.664,
      "step": 576
    },
    {
      "epoch": 1.2020833333333334,
      "grad_norm": 7.3268723487854,
      "learning_rate": 9.775462962962965e-06,
      "loss": 1.8058,
      "step": 577
    },
    {
      "epoch": 1.2041666666666666,
      "grad_norm": 12.297704696655273,
      "learning_rate": 9.773148148148149e-06,
      "loss": 1.947,
      "step": 578
    },
    {
      "epoch": 1.20625,
      "grad_norm": 6.788417816162109,
      "learning_rate": 9.770833333333333e-06,
      "loss": 1.627,
      "step": 579
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 5.52682638168335,
      "learning_rate": 9.768518518518519e-06,
      "loss": 1.7198,
      "step": 580
    },
    {
      "epoch": 1.2104166666666667,
      "grad_norm": 17.729366302490234,
      "learning_rate": 9.766203703703705e-06,
      "loss": 1.9303,
      "step": 581
    },
    {
      "epoch": 1.2125,
      "grad_norm": 8.486706733703613,
      "learning_rate": 9.76388888888889e-06,
      "loss": 1.9126,
      "step": 582
    },
    {
      "epoch": 1.2145833333333333,
      "grad_norm": 6.157741069793701,
      "learning_rate": 9.761574074074075e-06,
      "loss": 1.5155,
      "step": 583
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 9.520968437194824,
      "learning_rate": 9.759259259259261e-06,
      "loss": 1.7368,
      "step": 584
    },
    {
      "epoch": 1.21875,
      "grad_norm": 8.47683334350586,
      "learning_rate": 9.756944444444445e-06,
      "loss": 1.9316,
      "step": 585
    },
    {
      "epoch": 1.2208333333333332,
      "grad_norm": 9.435464859008789,
      "learning_rate": 9.754629629629631e-06,
      "loss": 1.7868,
      "step": 586
    },
    {
      "epoch": 1.2229166666666667,
      "grad_norm": 16.3041934967041,
      "learning_rate": 9.752314814814816e-06,
      "loss": 1.2794,
      "step": 587
    },
    {
      "epoch": 1.225,
      "grad_norm": 8.665135383605957,
      "learning_rate": 9.75e-06,
      "loss": 1.9773,
      "step": 588
    },
    {
      "epoch": 1.2270833333333333,
      "grad_norm": 6.480226993560791,
      "learning_rate": 9.747685185185186e-06,
      "loss": 1.8897,
      "step": 589
    },
    {
      "epoch": 1.2291666666666667,
      "grad_norm": 7.2512969970703125,
      "learning_rate": 9.745370370370372e-06,
      "loss": 1.6018,
      "step": 590
    },
    {
      "epoch": 1.23125,
      "grad_norm": 7.501052379608154,
      "learning_rate": 9.743055555555556e-06,
      "loss": 1.4813,
      "step": 591
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 9.251667022705078,
      "learning_rate": 9.740740740740742e-06,
      "loss": 1.9874,
      "step": 592
    },
    {
      "epoch": 1.2354166666666666,
      "grad_norm": 7.004453659057617,
      "learning_rate": 9.738425925925926e-06,
      "loss": 1.6849,
      "step": 593
    },
    {
      "epoch": 1.2375,
      "grad_norm": 6.859771728515625,
      "learning_rate": 9.736111111111112e-06,
      "loss": 1.5909,
      "step": 594
    },
    {
      "epoch": 1.2395833333333333,
      "grad_norm": 8.932538986206055,
      "learning_rate": 9.733796296296298e-06,
      "loss": 2.129,
      "step": 595
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 8.326435089111328,
      "learning_rate": 9.731481481481482e-06,
      "loss": 1.5981,
      "step": 596
    },
    {
      "epoch": 1.24375,
      "grad_norm": 13.255419731140137,
      "learning_rate": 9.729166666666667e-06,
      "loss": 1.8599,
      "step": 597
    },
    {
      "epoch": 1.2458333333333333,
      "grad_norm": 31.111114501953125,
      "learning_rate": 9.726851851851852e-06,
      "loss": 1.7974,
      "step": 598
    },
    {
      "epoch": 1.2479166666666668,
      "grad_norm": 8.541772842407227,
      "learning_rate": 9.724537037037037e-06,
      "loss": 2.1316,
      "step": 599
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.618897438049316,
      "learning_rate": 9.722222222222223e-06,
      "loss": 2.1445,
      "step": 600
    },
    {
      "epoch": 1.2520833333333332,
      "grad_norm": 7.345553398132324,
      "learning_rate": 9.719907407407409e-06,
      "loss": 1.5702,
      "step": 601
    },
    {
      "epoch": 1.2541666666666667,
      "grad_norm": 7.0933709144592285,
      "learning_rate": 9.717592592592593e-06,
      "loss": 1.4742,
      "step": 602
    },
    {
      "epoch": 1.25625,
      "grad_norm": 6.940958499908447,
      "learning_rate": 9.715277777777779e-06,
      "loss": 1.7247,
      "step": 603
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 8.057994842529297,
      "learning_rate": 9.712962962962965e-06,
      "loss": 2.0939,
      "step": 604
    },
    {
      "epoch": 1.2604166666666667,
      "grad_norm": 7.662418365478516,
      "learning_rate": 9.710648148148149e-06,
      "loss": 1.8721,
      "step": 605
    },
    {
      "epoch": 1.2625,
      "grad_norm": 7.420814514160156,
      "learning_rate": 9.708333333333333e-06,
      "loss": 1.6554,
      "step": 606
    },
    {
      "epoch": 1.2645833333333334,
      "grad_norm": 8.688450813293457,
      "learning_rate": 9.70601851851852e-06,
      "loss": 2.0102,
      "step": 607
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 7.622586727142334,
      "learning_rate": 9.703703703703703e-06,
      "loss": 1.5085,
      "step": 608
    },
    {
      "epoch": 1.26875,
      "grad_norm": 8.613277435302734,
      "learning_rate": 9.70138888888889e-06,
      "loss": 2.1269,
      "step": 609
    },
    {
      "epoch": 1.2708333333333333,
      "grad_norm": 15.301783561706543,
      "learning_rate": 9.699074074074075e-06,
      "loss": 1.9017,
      "step": 610
    },
    {
      "epoch": 1.2729166666666667,
      "grad_norm": 9.329584121704102,
      "learning_rate": 9.69675925925926e-06,
      "loss": 1.5522,
      "step": 611
    },
    {
      "epoch": 1.275,
      "grad_norm": 9.622467041015625,
      "learning_rate": 9.694444444444446e-06,
      "loss": 1.4927,
      "step": 612
    },
    {
      "epoch": 1.2770833333333333,
      "grad_norm": 10.118742942810059,
      "learning_rate": 9.692129629629631e-06,
      "loss": 1.818,
      "step": 613
    },
    {
      "epoch": 1.2791666666666668,
      "grad_norm": 8.405669212341309,
      "learning_rate": 9.689814814814816e-06,
      "loss": 1.4944,
      "step": 614
    },
    {
      "epoch": 1.28125,
      "grad_norm": 9.76342487335205,
      "learning_rate": 9.6875e-06,
      "loss": 1.888,
      "step": 615
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 8.835016250610352,
      "learning_rate": 9.685185185185186e-06,
      "loss": 1.9868,
      "step": 616
    },
    {
      "epoch": 1.2854166666666667,
      "grad_norm": 11.383675575256348,
      "learning_rate": 9.68287037037037e-06,
      "loss": 1.9225,
      "step": 617
    },
    {
      "epoch": 1.2875,
      "grad_norm": 7.40202522277832,
      "learning_rate": 9.680555555555556e-06,
      "loss": 1.4878,
      "step": 618
    },
    {
      "epoch": 1.2895833333333333,
      "grad_norm": 7.351068019866943,
      "learning_rate": 9.678240740740742e-06,
      "loss": 1.514,
      "step": 619
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 7.668013572692871,
      "learning_rate": 9.675925925925926e-06,
      "loss": 1.8678,
      "step": 620
    },
    {
      "epoch": 1.29375,
      "grad_norm": 7.7469611167907715,
      "learning_rate": 9.673611111111112e-06,
      "loss": 1.6697,
      "step": 621
    },
    {
      "epoch": 1.2958333333333334,
      "grad_norm": 57.054386138916016,
      "learning_rate": 9.671296296296298e-06,
      "loss": 2.309,
      "step": 622
    },
    {
      "epoch": 1.2979166666666666,
      "grad_norm": 8.09070873260498,
      "learning_rate": 9.668981481481482e-06,
      "loss": 1.202,
      "step": 623
    },
    {
      "epoch": 1.3,
      "grad_norm": 6.925523281097412,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.4155,
      "step": 624
    },
    {
      "epoch": 1.3020833333333333,
      "grad_norm": 10.31895923614502,
      "learning_rate": 9.664351851851853e-06,
      "loss": 2.1433,
      "step": 625
    },
    {
      "epoch": 1.3041666666666667,
      "grad_norm": 7.176229953765869,
      "learning_rate": 9.662037037037037e-06,
      "loss": 1.4627,
      "step": 626
    },
    {
      "epoch": 1.30625,
      "grad_norm": 6.262925148010254,
      "learning_rate": 9.659722222222223e-06,
      "loss": 1.8339,
      "step": 627
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 24.398725509643555,
      "learning_rate": 9.657407407407409e-06,
      "loss": 1.2573,
      "step": 628
    },
    {
      "epoch": 1.3104166666666668,
      "grad_norm": 14.120368957519531,
      "learning_rate": 9.655092592592593e-06,
      "loss": 2.0746,
      "step": 629
    },
    {
      "epoch": 1.3125,
      "grad_norm": 14.32103157043457,
      "learning_rate": 9.652777777777779e-06,
      "loss": 1.902,
      "step": 630
    },
    {
      "epoch": 1.3145833333333332,
      "grad_norm": 12.424918174743652,
      "learning_rate": 9.650462962962965e-06,
      "loss": 2.4815,
      "step": 631
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 7.39402961730957,
      "learning_rate": 9.64814814814815e-06,
      "loss": 1.7484,
      "step": 632
    },
    {
      "epoch": 1.31875,
      "grad_norm": 10.450010299682617,
      "learning_rate": 9.645833333333333e-06,
      "loss": 2.0999,
      "step": 633
    },
    {
      "epoch": 1.3208333333333333,
      "grad_norm": 7.993061542510986,
      "learning_rate": 9.64351851851852e-06,
      "loss": 1.5458,
      "step": 634
    },
    {
      "epoch": 1.3229166666666667,
      "grad_norm": 7.297593593597412,
      "learning_rate": 9.641203703703704e-06,
      "loss": 1.4682,
      "step": 635
    },
    {
      "epoch": 1.325,
      "grad_norm": 10.245990753173828,
      "learning_rate": 9.63888888888889e-06,
      "loss": 1.9194,
      "step": 636
    },
    {
      "epoch": 1.3270833333333334,
      "grad_norm": 9.162091255187988,
      "learning_rate": 9.636574074074076e-06,
      "loss": 1.7946,
      "step": 637
    },
    {
      "epoch": 1.3291666666666666,
      "grad_norm": 10.001178741455078,
      "learning_rate": 9.63425925925926e-06,
      "loss": 1.8674,
      "step": 638
    },
    {
      "epoch": 1.33125,
      "grad_norm": 9.27587604522705,
      "learning_rate": 9.631944444444446e-06,
      "loss": 1.87,
      "step": 639
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 8.137566566467285,
      "learning_rate": 9.62962962962963e-06,
      "loss": 1.6741,
      "step": 640
    },
    {
      "epoch": 1.3354166666666667,
      "grad_norm": 8.478104591369629,
      "learning_rate": 9.627314814814816e-06,
      "loss": 1.9006,
      "step": 641
    },
    {
      "epoch": 1.3375,
      "grad_norm": 6.995912075042725,
      "learning_rate": 9.625e-06,
      "loss": 1.4786,
      "step": 642
    },
    {
      "epoch": 1.3395833333333333,
      "grad_norm": 10.526086807250977,
      "learning_rate": 9.622685185185186e-06,
      "loss": 1.9156,
      "step": 643
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 11.341340065002441,
      "learning_rate": 9.62037037037037e-06,
      "loss": 2.209,
      "step": 644
    },
    {
      "epoch": 1.34375,
      "grad_norm": 7.358345031738281,
      "learning_rate": 9.618055555555556e-06,
      "loss": 1.8594,
      "step": 645
    },
    {
      "epoch": 1.3458333333333332,
      "grad_norm": 8.906429290771484,
      "learning_rate": 9.61574074074074e-06,
      "loss": 2.0922,
      "step": 646
    },
    {
      "epoch": 1.3479166666666667,
      "grad_norm": 7.257223129272461,
      "learning_rate": 9.613425925925927e-06,
      "loss": 1.4282,
      "step": 647
    },
    {
      "epoch": 1.35,
      "grad_norm": 21.18498420715332,
      "learning_rate": 9.611111111111112e-06,
      "loss": 2.2221,
      "step": 648
    },
    {
      "epoch": 1.3520833333333333,
      "grad_norm": 8.555641174316406,
      "learning_rate": 9.608796296296297e-06,
      "loss": 1.6299,
      "step": 649
    },
    {
      "epoch": 1.3541666666666667,
      "grad_norm": 16.590076446533203,
      "learning_rate": 9.606481481481483e-06,
      "loss": 2.2756,
      "step": 650
    },
    {
      "epoch": 1.35625,
      "grad_norm": 8.984541893005371,
      "learning_rate": 9.604166666666669e-06,
      "loss": 2.0865,
      "step": 651
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 3.8862545490264893,
      "learning_rate": 9.601851851851853e-06,
      "loss": 2.0708,
      "step": 652
    },
    {
      "epoch": 1.3604166666666666,
      "grad_norm": 10.22523021697998,
      "learning_rate": 9.599537037037037e-06,
      "loss": 1.781,
      "step": 653
    },
    {
      "epoch": 1.3625,
      "grad_norm": 9.3724365234375,
      "learning_rate": 9.597222222222223e-06,
      "loss": 1.4334,
      "step": 654
    },
    {
      "epoch": 1.3645833333333333,
      "grad_norm": 6.981603622436523,
      "learning_rate": 9.594907407407407e-06,
      "loss": 1.8178,
      "step": 655
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 5.329166412353516,
      "learning_rate": 9.592592592592593e-06,
      "loss": 1.9871,
      "step": 656
    },
    {
      "epoch": 1.36875,
      "grad_norm": 6.148777484893799,
      "learning_rate": 9.59027777777778e-06,
      "loss": 1.5783,
      "step": 657
    },
    {
      "epoch": 1.3708333333333333,
      "grad_norm": 14.034274101257324,
      "learning_rate": 9.587962962962963e-06,
      "loss": 1.7989,
      "step": 658
    },
    {
      "epoch": 1.3729166666666668,
      "grad_norm": 9.860187530517578,
      "learning_rate": 9.58564814814815e-06,
      "loss": 1.9732,
      "step": 659
    },
    {
      "epoch": 1.375,
      "grad_norm": 5.570441722869873,
      "learning_rate": 9.583333333333335e-06,
      "loss": 1.4907,
      "step": 660
    },
    {
      "epoch": 1.3770833333333332,
      "grad_norm": 7.976370334625244,
      "learning_rate": 9.58101851851852e-06,
      "loss": 1.4347,
      "step": 661
    },
    {
      "epoch": 1.3791666666666667,
      "grad_norm": 7.761101722717285,
      "learning_rate": 9.578703703703704e-06,
      "loss": 1.6827,
      "step": 662
    },
    {
      "epoch": 1.38125,
      "grad_norm": 8.027889251708984,
      "learning_rate": 9.57638888888889e-06,
      "loss": 1.8294,
      "step": 663
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 6.265452861785889,
      "learning_rate": 9.574074074074074e-06,
      "loss": 1.4063,
      "step": 664
    },
    {
      "epoch": 1.3854166666666667,
      "grad_norm": 6.574056148529053,
      "learning_rate": 9.57175925925926e-06,
      "loss": 1.8191,
      "step": 665
    },
    {
      "epoch": 1.3875,
      "grad_norm": 6.40303373336792,
      "learning_rate": 9.569444444444446e-06,
      "loss": 1.5265,
      "step": 666
    },
    {
      "epoch": 1.3895833333333334,
      "grad_norm": 9.182129859924316,
      "learning_rate": 9.56712962962963e-06,
      "loss": 1.7922,
      "step": 667
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 6.340987682342529,
      "learning_rate": 9.564814814814816e-06,
      "loss": 1.3887,
      "step": 668
    },
    {
      "epoch": 1.39375,
      "grad_norm": 6.722743511199951,
      "learning_rate": 9.562500000000002e-06,
      "loss": 1.5832,
      "step": 669
    },
    {
      "epoch": 1.3958333333333333,
      "grad_norm": 9.126479148864746,
      "learning_rate": 9.560185185185186e-06,
      "loss": 2.0748,
      "step": 670
    },
    {
      "epoch": 1.3979166666666667,
      "grad_norm": 10.776910781860352,
      "learning_rate": 9.55787037037037e-06,
      "loss": 2.0463,
      "step": 671
    },
    {
      "epoch": 1.4,
      "grad_norm": 7.172388076782227,
      "learning_rate": 9.555555555555556e-06,
      "loss": 1.9209,
      "step": 672
    },
    {
      "epoch": 1.4020833333333333,
      "grad_norm": 6.509444713592529,
      "learning_rate": 9.55324074074074e-06,
      "loss": 1.6912,
      "step": 673
    },
    {
      "epoch": 1.4041666666666668,
      "grad_norm": 14.773183822631836,
      "learning_rate": 9.550925925925927e-06,
      "loss": 1.9896,
      "step": 674
    },
    {
      "epoch": 1.40625,
      "grad_norm": 6.680857181549072,
      "learning_rate": 9.548611111111113e-06,
      "loss": 1.414,
      "step": 675
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 11.86325454711914,
      "learning_rate": 9.546296296296297e-06,
      "loss": 0.9865,
      "step": 676
    },
    {
      "epoch": 1.4104166666666667,
      "grad_norm": 8.366303443908691,
      "learning_rate": 9.543981481481483e-06,
      "loss": 1.4973,
      "step": 677
    },
    {
      "epoch": 1.4125,
      "grad_norm": 9.766424179077148,
      "learning_rate": 9.541666666666669e-06,
      "loss": 2.1377,
      "step": 678
    },
    {
      "epoch": 1.4145833333333333,
      "grad_norm": 12.052597999572754,
      "learning_rate": 9.539351851851853e-06,
      "loss": 2.1732,
      "step": 679
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 11.112542152404785,
      "learning_rate": 9.537037037037037e-06,
      "loss": 1.6345,
      "step": 680
    },
    {
      "epoch": 1.41875,
      "grad_norm": 8.951783180236816,
      "learning_rate": 9.534722222222223e-06,
      "loss": 1.5806,
      "step": 681
    },
    {
      "epoch": 1.4208333333333334,
      "grad_norm": 6.969979286193848,
      "learning_rate": 9.532407407407407e-06,
      "loss": 1.3907,
      "step": 682
    },
    {
      "epoch": 1.4229166666666666,
      "grad_norm": 5.270254611968994,
      "learning_rate": 9.530092592592593e-06,
      "loss": 1.8601,
      "step": 683
    },
    {
      "epoch": 1.425,
      "grad_norm": 11.293233871459961,
      "learning_rate": 9.527777777777778e-06,
      "loss": 1.8047,
      "step": 684
    },
    {
      "epoch": 1.4270833333333333,
      "grad_norm": 17.436124801635742,
      "learning_rate": 9.525462962962964e-06,
      "loss": 1.6825,
      "step": 685
    },
    {
      "epoch": 1.4291666666666667,
      "grad_norm": 7.121499061584473,
      "learning_rate": 9.52314814814815e-06,
      "loss": 1.864,
      "step": 686
    },
    {
      "epoch": 1.43125,
      "grad_norm": 7.628202438354492,
      "learning_rate": 9.520833333333334e-06,
      "loss": 1.6396,
      "step": 687
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 8.12023639678955,
      "learning_rate": 9.51851851851852e-06,
      "loss": 1.2653,
      "step": 688
    },
    {
      "epoch": 1.4354166666666668,
      "grad_norm": 8.366511344909668,
      "learning_rate": 9.516203703703704e-06,
      "loss": 1.4344,
      "step": 689
    },
    {
      "epoch": 1.4375,
      "grad_norm": 11.044703483581543,
      "learning_rate": 9.51388888888889e-06,
      "loss": 1.7737,
      "step": 690
    },
    {
      "epoch": 1.4395833333333332,
      "grad_norm": 9.54362964630127,
      "learning_rate": 9.511574074074074e-06,
      "loss": 1.701,
      "step": 691
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 13.276009559631348,
      "learning_rate": 9.50925925925926e-06,
      "loss": 1.8994,
      "step": 692
    },
    {
      "epoch": 1.44375,
      "grad_norm": 9.309993743896484,
      "learning_rate": 9.506944444444444e-06,
      "loss": 1.4232,
      "step": 693
    },
    {
      "epoch": 1.4458333333333333,
      "grad_norm": 6.933876037597656,
      "learning_rate": 9.50462962962963e-06,
      "loss": 2.091,
      "step": 694
    },
    {
      "epoch": 1.4479166666666667,
      "grad_norm": 15.032843589782715,
      "learning_rate": 9.502314814814816e-06,
      "loss": 1.921,
      "step": 695
    },
    {
      "epoch": 1.45,
      "grad_norm": 7.7666850090026855,
      "learning_rate": 9.5e-06,
      "loss": 1.4793,
      "step": 696
    },
    {
      "epoch": 1.4520833333333334,
      "grad_norm": 14.989361763000488,
      "learning_rate": 9.497685185185186e-06,
      "loss": 1.9109,
      "step": 697
    },
    {
      "epoch": 1.4541666666666666,
      "grad_norm": 6.4530253410339355,
      "learning_rate": 9.49537037037037e-06,
      "loss": 2.0006,
      "step": 698
    },
    {
      "epoch": 1.45625,
      "grad_norm": 11.892557144165039,
      "learning_rate": 9.493055555555557e-06,
      "loss": 1.9771,
      "step": 699
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 12.19426441192627,
      "learning_rate": 9.490740740740741e-06,
      "loss": 1.5263,
      "step": 700
    },
    {
      "epoch": 1.4604166666666667,
      "grad_norm": 8.82660961151123,
      "learning_rate": 9.488425925925927e-06,
      "loss": 1.3468,
      "step": 701
    },
    {
      "epoch": 1.4625,
      "grad_norm": 8.971718788146973,
      "learning_rate": 9.486111111111111e-06,
      "loss": 1.829,
      "step": 702
    },
    {
      "epoch": 1.4645833333333333,
      "grad_norm": 12.563873291015625,
      "learning_rate": 9.483796296296297e-06,
      "loss": 2.213,
      "step": 703
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 9.198945999145508,
      "learning_rate": 9.481481481481483e-06,
      "loss": 1.8473,
      "step": 704
    },
    {
      "epoch": 1.46875,
      "grad_norm": 7.02931547164917,
      "learning_rate": 9.479166666666667e-06,
      "loss": 1.5598,
      "step": 705
    },
    {
      "epoch": 1.4708333333333332,
      "grad_norm": 9.198569297790527,
      "learning_rate": 9.476851851851853e-06,
      "loss": 1.8291,
      "step": 706
    },
    {
      "epoch": 1.4729166666666667,
      "grad_norm": 12.020807266235352,
      "learning_rate": 9.474537037037037e-06,
      "loss": 0.8967,
      "step": 707
    },
    {
      "epoch": 1.475,
      "grad_norm": 7.067902565002441,
      "learning_rate": 9.472222222222223e-06,
      "loss": 1.7867,
      "step": 708
    },
    {
      "epoch": 1.4770833333333333,
      "grad_norm": 5.884659767150879,
      "learning_rate": 9.469907407407408e-06,
      "loss": 1.4565,
      "step": 709
    },
    {
      "epoch": 1.4791666666666667,
      "grad_norm": 11.06344985961914,
      "learning_rate": 9.467592592592594e-06,
      "loss": 2.0035,
      "step": 710
    },
    {
      "epoch": 1.48125,
      "grad_norm": 8.631453514099121,
      "learning_rate": 9.465277777777778e-06,
      "loss": 2.1018,
      "step": 711
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 11.086047172546387,
      "learning_rate": 9.462962962962964e-06,
      "loss": 1.758,
      "step": 712
    },
    {
      "epoch": 1.4854166666666666,
      "grad_norm": 12.94863224029541,
      "learning_rate": 9.46064814814815e-06,
      "loss": 2.0371,
      "step": 713
    },
    {
      "epoch": 1.4875,
      "grad_norm": 11.810455322265625,
      "learning_rate": 9.458333333333334e-06,
      "loss": 1.8896,
      "step": 714
    },
    {
      "epoch": 1.4895833333333333,
      "grad_norm": 8.957878112792969,
      "learning_rate": 9.45601851851852e-06,
      "loss": 2.1475,
      "step": 715
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 17.543840408325195,
      "learning_rate": 9.453703703703704e-06,
      "loss": 2.2791,
      "step": 716
    },
    {
      "epoch": 1.49375,
      "grad_norm": 9.082154273986816,
      "learning_rate": 9.45138888888889e-06,
      "loss": 1.7403,
      "step": 717
    },
    {
      "epoch": 1.4958333333333333,
      "grad_norm": 10.517353057861328,
      "learning_rate": 9.449074074074074e-06,
      "loss": 1.5853,
      "step": 718
    },
    {
      "epoch": 1.4979166666666668,
      "grad_norm": 7.88033390045166,
      "learning_rate": 9.44675925925926e-06,
      "loss": 1.6378,
      "step": 719
    },
    {
      "epoch": 1.5,
      "grad_norm": 11.205703735351562,
      "learning_rate": 9.444444444444445e-06,
      "loss": 2.0816,
      "step": 720
    },
    {
      "epoch": 1.5020833333333332,
      "grad_norm": 8.25730037689209,
      "learning_rate": 9.44212962962963e-06,
      "loss": 1.3082,
      "step": 721
    },
    {
      "epoch": 1.5041666666666667,
      "grad_norm": 9.887762069702148,
      "learning_rate": 9.439814814814816e-06,
      "loss": 1.9294,
      "step": 722
    },
    {
      "epoch": 1.50625,
      "grad_norm": 7.674728870391846,
      "learning_rate": 9.4375e-06,
      "loss": 1.3562,
      "step": 723
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 9.325597763061523,
      "learning_rate": 9.435185185185187e-06,
      "loss": 1.1083,
      "step": 724
    },
    {
      "epoch": 1.5104166666666665,
      "grad_norm": 14.209789276123047,
      "learning_rate": 9.432870370370371e-06,
      "loss": 2.6763,
      "step": 725
    },
    {
      "epoch": 1.5125,
      "grad_norm": 8.615937232971191,
      "learning_rate": 9.430555555555557e-06,
      "loss": 1.1846,
      "step": 726
    },
    {
      "epoch": 1.5145833333333334,
      "grad_norm": 8.525432586669922,
      "learning_rate": 9.428240740740741e-06,
      "loss": 1.1732,
      "step": 727
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 9.47774600982666,
      "learning_rate": 9.425925925925925e-06,
      "loss": 1.7189,
      "step": 728
    },
    {
      "epoch": 1.51875,
      "grad_norm": 6.530431747436523,
      "learning_rate": 9.423611111111111e-06,
      "loss": 1.4575,
      "step": 729
    },
    {
      "epoch": 1.5208333333333335,
      "grad_norm": 7.540192127227783,
      "learning_rate": 9.421296296296297e-06,
      "loss": 1.6404,
      "step": 730
    },
    {
      "epoch": 1.5229166666666667,
      "grad_norm": 6.9873785972595215,
      "learning_rate": 9.418981481481481e-06,
      "loss": 1.3539,
      "step": 731
    },
    {
      "epoch": 1.525,
      "grad_norm": 14.077200889587402,
      "learning_rate": 9.416666666666667e-06,
      "loss": 2.0471,
      "step": 732
    },
    {
      "epoch": 1.5270833333333333,
      "grad_norm": 11.383381843566895,
      "learning_rate": 9.414351851851853e-06,
      "loss": 2.0218,
      "step": 733
    },
    {
      "epoch": 1.5291666666666668,
      "grad_norm": 9.69314956665039,
      "learning_rate": 9.412037037037038e-06,
      "loss": 1.5908,
      "step": 734
    },
    {
      "epoch": 1.53125,
      "grad_norm": 8.981706619262695,
      "learning_rate": 9.409722222222224e-06,
      "loss": 1.6957,
      "step": 735
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 5.6117706298828125,
      "learning_rate": 9.407407407407408e-06,
      "loss": 1.9673,
      "step": 736
    },
    {
      "epoch": 1.5354166666666667,
      "grad_norm": 9.590352058410645,
      "learning_rate": 9.405092592592592e-06,
      "loss": 1.8801,
      "step": 737
    },
    {
      "epoch": 1.5375,
      "grad_norm": 9.588565826416016,
      "learning_rate": 9.402777777777778e-06,
      "loss": 1.7895,
      "step": 738
    },
    {
      "epoch": 1.5395833333333333,
      "grad_norm": 21.511857986450195,
      "learning_rate": 9.400462962962964e-06,
      "loss": 1.6757,
      "step": 739
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 12.063216209411621,
      "learning_rate": 9.398148148148148e-06,
      "loss": 0.9622,
      "step": 740
    },
    {
      "epoch": 1.54375,
      "grad_norm": 9.365038871765137,
      "learning_rate": 9.395833333333334e-06,
      "loss": 2.0091,
      "step": 741
    },
    {
      "epoch": 1.5458333333333334,
      "grad_norm": 12.310638427734375,
      "learning_rate": 9.39351851851852e-06,
      "loss": 1.1551,
      "step": 742
    },
    {
      "epoch": 1.5479166666666666,
      "grad_norm": 7.5223283767700195,
      "learning_rate": 9.391203703703704e-06,
      "loss": 1.8813,
      "step": 743
    },
    {
      "epoch": 1.55,
      "grad_norm": 11.146984100341797,
      "learning_rate": 9.38888888888889e-06,
      "loss": 1.8987,
      "step": 744
    },
    {
      "epoch": 1.5520833333333335,
      "grad_norm": 9.20479679107666,
      "learning_rate": 9.386574074074075e-06,
      "loss": 1.7463,
      "step": 745
    },
    {
      "epoch": 1.5541666666666667,
      "grad_norm": 2.890655279159546,
      "learning_rate": 9.384259259259259e-06,
      "loss": 1.8881,
      "step": 746
    },
    {
      "epoch": 1.55625,
      "grad_norm": 7.359027862548828,
      "learning_rate": 9.381944444444445e-06,
      "loss": 1.8624,
      "step": 747
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 10.852561950683594,
      "learning_rate": 9.37962962962963e-06,
      "loss": 1.5413,
      "step": 748
    },
    {
      "epoch": 1.5604166666666668,
      "grad_norm": 7.757837295532227,
      "learning_rate": 9.377314814814815e-06,
      "loss": 1.4335,
      "step": 749
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.021791458129883,
      "learning_rate": 9.375000000000001e-06,
      "loss": 2.188,
      "step": 750
    },
    {
      "epoch": 1.5645833333333332,
      "grad_norm": 7.357199668884277,
      "learning_rate": 9.372685185185187e-06,
      "loss": 1.4445,
      "step": 751
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 11.117677688598633,
      "learning_rate": 9.370370370370371e-06,
      "loss": 2.2017,
      "step": 752
    },
    {
      "epoch": 1.56875,
      "grad_norm": 8.08716106414795,
      "learning_rate": 9.368055555555557e-06,
      "loss": 1.9903,
      "step": 753
    },
    {
      "epoch": 1.5708333333333333,
      "grad_norm": 6.126646518707275,
      "learning_rate": 9.365740740740741e-06,
      "loss": 1.7974,
      "step": 754
    },
    {
      "epoch": 1.5729166666666665,
      "grad_norm": 7.748610496520996,
      "learning_rate": 9.363425925925927e-06,
      "loss": 1.6856,
      "step": 755
    },
    {
      "epoch": 1.575,
      "grad_norm": 8.157474517822266,
      "learning_rate": 9.361111111111111e-06,
      "loss": 1.9049,
      "step": 756
    },
    {
      "epoch": 1.5770833333333334,
      "grad_norm": 12.281026840209961,
      "learning_rate": 9.358796296296297e-06,
      "loss": 0.8613,
      "step": 757
    },
    {
      "epoch": 1.5791666666666666,
      "grad_norm": 9.38447093963623,
      "learning_rate": 9.356481481481482e-06,
      "loss": 1.8266,
      "step": 758
    },
    {
      "epoch": 1.58125,
      "grad_norm": 7.167775630950928,
      "learning_rate": 9.354166666666668e-06,
      "loss": 1.3393,
      "step": 759
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 10.525578498840332,
      "learning_rate": 9.351851851851854e-06,
      "loss": 1.9579,
      "step": 760
    },
    {
      "epoch": 1.5854166666666667,
      "grad_norm": 10.664066314697266,
      "learning_rate": 9.349537037037038e-06,
      "loss": 1.8167,
      "step": 761
    },
    {
      "epoch": 1.5875,
      "grad_norm": 8.770933151245117,
      "learning_rate": 9.347222222222224e-06,
      "loss": 1.7369,
      "step": 762
    },
    {
      "epoch": 1.5895833333333333,
      "grad_norm": 9.524698257446289,
      "learning_rate": 9.344907407407408e-06,
      "loss": 1.8943,
      "step": 763
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 8.87808609008789,
      "learning_rate": 9.342592592592594e-06,
      "loss": 1.7792,
      "step": 764
    },
    {
      "epoch": 1.59375,
      "grad_norm": 15.112667083740234,
      "learning_rate": 9.340277777777778e-06,
      "loss": 2.4081,
      "step": 765
    },
    {
      "epoch": 1.5958333333333332,
      "grad_norm": 10.793756484985352,
      "learning_rate": 9.337962962962964e-06,
      "loss": 1.1266,
      "step": 766
    },
    {
      "epoch": 1.5979166666666667,
      "grad_norm": 12.41319751739502,
      "learning_rate": 9.335648148148148e-06,
      "loss": 2.0702,
      "step": 767
    },
    {
      "epoch": 1.6,
      "grad_norm": 7.000545978546143,
      "learning_rate": 9.333333333333334e-06,
      "loss": 2.1052,
      "step": 768
    },
    {
      "epoch": 1.6020833333333333,
      "grad_norm": 10.542059898376465,
      "learning_rate": 9.33101851851852e-06,
      "loss": 1.5722,
      "step": 769
    },
    {
      "epoch": 1.6041666666666665,
      "grad_norm": 4.133267879486084,
      "learning_rate": 9.328703703703705e-06,
      "loss": 1.9304,
      "step": 770
    },
    {
      "epoch": 1.60625,
      "grad_norm": 8.83169174194336,
      "learning_rate": 9.32638888888889e-06,
      "loss": 1.29,
      "step": 771
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 7.024064540863037,
      "learning_rate": 9.324074074074075e-06,
      "loss": 1.4015,
      "step": 772
    },
    {
      "epoch": 1.6104166666666666,
      "grad_norm": 8.097102165222168,
      "learning_rate": 9.32175925925926e-06,
      "loss": 1.4848,
      "step": 773
    },
    {
      "epoch": 1.6125,
      "grad_norm": 14.316213607788086,
      "learning_rate": 9.319444444444445e-06,
      "loss": 1.6107,
      "step": 774
    },
    {
      "epoch": 1.6145833333333335,
      "grad_norm": 9.98715877532959,
      "learning_rate": 9.31712962962963e-06,
      "loss": 1.7351,
      "step": 775
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 9.580471992492676,
      "learning_rate": 9.314814814814815e-06,
      "loss": 2.1634,
      "step": 776
    },
    {
      "epoch": 1.61875,
      "grad_norm": 8.279526710510254,
      "learning_rate": 9.312500000000001e-06,
      "loss": 1.9187,
      "step": 777
    },
    {
      "epoch": 1.6208333333333333,
      "grad_norm": 8.969770431518555,
      "learning_rate": 9.310185185185185e-06,
      "loss": 1.8998,
      "step": 778
    },
    {
      "epoch": 1.6229166666666668,
      "grad_norm": 5.969125270843506,
      "learning_rate": 9.307870370370371e-06,
      "loss": 1.4969,
      "step": 779
    },
    {
      "epoch": 1.625,
      "grad_norm": 10.428056716918945,
      "learning_rate": 9.305555555555557e-06,
      "loss": 1.6046,
      "step": 780
    },
    {
      "epoch": 1.6270833333333332,
      "grad_norm": 14.476799964904785,
      "learning_rate": 9.303240740740741e-06,
      "loss": 2.4443,
      "step": 781
    },
    {
      "epoch": 1.6291666666666667,
      "grad_norm": 29.292333602905273,
      "learning_rate": 9.300925925925927e-06,
      "loss": 1.9608,
      "step": 782
    },
    {
      "epoch": 1.63125,
      "grad_norm": 5.666156768798828,
      "learning_rate": 9.298611111111112e-06,
      "loss": 1.8887,
      "step": 783
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 8.484051704406738,
      "learning_rate": 9.296296296296296e-06,
      "loss": 1.5593,
      "step": 784
    },
    {
      "epoch": 1.6354166666666665,
      "grad_norm": 8.568696022033691,
      "learning_rate": 9.293981481481482e-06,
      "loss": 2.0549,
      "step": 785
    },
    {
      "epoch": 1.6375,
      "grad_norm": 10.110095977783203,
      "learning_rate": 9.291666666666668e-06,
      "loss": 2.1259,
      "step": 786
    },
    {
      "epoch": 1.6395833333333334,
      "grad_norm": 11.819135665893555,
      "learning_rate": 9.289351851851852e-06,
      "loss": 1.6079,
      "step": 787
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 45.78827667236328,
      "learning_rate": 9.287037037037038e-06,
      "loss": 1.5303,
      "step": 788
    },
    {
      "epoch": 1.64375,
      "grad_norm": 6.987464427947998,
      "learning_rate": 9.284722222222224e-06,
      "loss": 1.7755,
      "step": 789
    },
    {
      "epoch": 1.6458333333333335,
      "grad_norm": 7.507819175720215,
      "learning_rate": 9.282407407407408e-06,
      "loss": 1.3888,
      "step": 790
    },
    {
      "epoch": 1.6479166666666667,
      "grad_norm": 7.949976444244385,
      "learning_rate": 9.280092592592594e-06,
      "loss": 1.3991,
      "step": 791
    },
    {
      "epoch": 1.65,
      "grad_norm": 7.6637864112854,
      "learning_rate": 9.277777777777778e-06,
      "loss": 2.0348,
      "step": 792
    },
    {
      "epoch": 1.6520833333333333,
      "grad_norm": 13.634201049804688,
      "learning_rate": 9.275462962962963e-06,
      "loss": 1.4467,
      "step": 793
    },
    {
      "epoch": 1.6541666666666668,
      "grad_norm": 3.627788782119751,
      "learning_rate": 9.273148148148149e-06,
      "loss": 2.0675,
      "step": 794
    },
    {
      "epoch": 1.65625,
      "grad_norm": 9.578767776489258,
      "learning_rate": 9.270833333333334e-06,
      "loss": 1.5087,
      "step": 795
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 9.598677635192871,
      "learning_rate": 9.268518518518519e-06,
      "loss": 1.8061,
      "step": 796
    },
    {
      "epoch": 1.6604166666666667,
      "grad_norm": 9.555903434753418,
      "learning_rate": 9.266203703703705e-06,
      "loss": 2.1864,
      "step": 797
    },
    {
      "epoch": 1.6625,
      "grad_norm": 12.41916275024414,
      "learning_rate": 9.26388888888889e-06,
      "loss": 1.6232,
      "step": 798
    },
    {
      "epoch": 1.6645833333333333,
      "grad_norm": 10.031953811645508,
      "learning_rate": 9.261574074074075e-06,
      "loss": 1.9664,
      "step": 799
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 8.503414154052734,
      "learning_rate": 9.25925925925926e-06,
      "loss": 1.8826,
      "step": 800
    },
    {
      "epoch": 1.66875,
      "grad_norm": 9.925237655639648,
      "learning_rate": 9.256944444444445e-06,
      "loss": 2.0802,
      "step": 801
    },
    {
      "epoch": 1.6708333333333334,
      "grad_norm": 11.572080612182617,
      "learning_rate": 9.25462962962963e-06,
      "loss": 1.8179,
      "step": 802
    },
    {
      "epoch": 1.6729166666666666,
      "grad_norm": 6.2245774269104,
      "learning_rate": 9.252314814814815e-06,
      "loss": 1.4008,
      "step": 803
    },
    {
      "epoch": 1.675,
      "grad_norm": 7.253314971923828,
      "learning_rate": 9.250000000000001e-06,
      "loss": 1.4221,
      "step": 804
    },
    {
      "epoch": 1.6770833333333335,
      "grad_norm": 7.790045261383057,
      "learning_rate": 9.247685185185185e-06,
      "loss": 1.9939,
      "step": 805
    },
    {
      "epoch": 1.6791666666666667,
      "grad_norm": 8.688694953918457,
      "learning_rate": 9.245370370370371e-06,
      "loss": 1.6515,
      "step": 806
    },
    {
      "epoch": 1.68125,
      "grad_norm": 7.438039779663086,
      "learning_rate": 9.243055555555557e-06,
      "loss": 1.5146,
      "step": 807
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 8.703648567199707,
      "learning_rate": 9.240740740740742e-06,
      "loss": 1.4267,
      "step": 808
    },
    {
      "epoch": 1.6854166666666668,
      "grad_norm": 13.181537628173828,
      "learning_rate": 9.238425925925928e-06,
      "loss": 1.6959,
      "step": 809
    },
    {
      "epoch": 1.6875,
      "grad_norm": 11.780952453613281,
      "learning_rate": 9.236111111111112e-06,
      "loss": 1.088,
      "step": 810
    },
    {
      "epoch": 1.6895833333333332,
      "grad_norm": 6.81350040435791,
      "learning_rate": 9.233796296296296e-06,
      "loss": 1.982,
      "step": 811
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 7.014212608337402,
      "learning_rate": 9.231481481481482e-06,
      "loss": 1.2798,
      "step": 812
    },
    {
      "epoch": 1.69375,
      "grad_norm": 8.470229148864746,
      "learning_rate": 9.229166666666668e-06,
      "loss": 1.0935,
      "step": 813
    },
    {
      "epoch": 1.6958333333333333,
      "grad_norm": 12.414495468139648,
      "learning_rate": 9.226851851851852e-06,
      "loss": 2.2276,
      "step": 814
    },
    {
      "epoch": 1.6979166666666665,
      "grad_norm": 7.5014495849609375,
      "learning_rate": 9.224537037037038e-06,
      "loss": 1.2545,
      "step": 815
    },
    {
      "epoch": 1.7,
      "grad_norm": 8.31639289855957,
      "learning_rate": 9.222222222222224e-06,
      "loss": 1.0643,
      "step": 816
    },
    {
      "epoch": 1.7020833333333334,
      "grad_norm": 7.941153526306152,
      "learning_rate": 9.219907407407408e-06,
      "loss": 1.3673,
      "step": 817
    },
    {
      "epoch": 1.7041666666666666,
      "grad_norm": 7.797340393066406,
      "learning_rate": 9.217592592592594e-06,
      "loss": 1.3176,
      "step": 818
    },
    {
      "epoch": 1.70625,
      "grad_norm": 10.874963760375977,
      "learning_rate": 9.215277777777779e-06,
      "loss": 1.8769,
      "step": 819
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 14.906208992004395,
      "learning_rate": 9.212962962962963e-06,
      "loss": 2.001,
      "step": 820
    },
    {
      "epoch": 1.7104166666666667,
      "grad_norm": 11.547801971435547,
      "learning_rate": 9.210648148148149e-06,
      "loss": 0.922,
      "step": 821
    },
    {
      "epoch": 1.7125,
      "grad_norm": 8.380057334899902,
      "learning_rate": 9.208333333333333e-06,
      "loss": 1.6479,
      "step": 822
    },
    {
      "epoch": 1.7145833333333333,
      "grad_norm": 8.244202613830566,
      "learning_rate": 9.206018518518519e-06,
      "loss": 1.257,
      "step": 823
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 14.234712600708008,
      "learning_rate": 9.203703703703705e-06,
      "loss": 2.034,
      "step": 824
    },
    {
      "epoch": 1.71875,
      "grad_norm": 10.541569709777832,
      "learning_rate": 9.201388888888889e-06,
      "loss": 1.8926,
      "step": 825
    },
    {
      "epoch": 1.7208333333333332,
      "grad_norm": 17.83091163635254,
      "learning_rate": 9.199074074074075e-06,
      "loss": 2.1978,
      "step": 826
    },
    {
      "epoch": 1.7229166666666667,
      "grad_norm": 10.668956756591797,
      "learning_rate": 9.196759259259261e-06,
      "loss": 1.8311,
      "step": 827
    },
    {
      "epoch": 1.725,
      "grad_norm": 10.147337913513184,
      "learning_rate": 9.194444444444445e-06,
      "loss": 1.7958,
      "step": 828
    },
    {
      "epoch": 1.7270833333333333,
      "grad_norm": 5.7800140380859375,
      "learning_rate": 9.19212962962963e-06,
      "loss": 1.8072,
      "step": 829
    },
    {
      "epoch": 1.7291666666666665,
      "grad_norm": 10.577611923217773,
      "learning_rate": 9.189814814814815e-06,
      "loss": 1.7255,
      "step": 830
    },
    {
      "epoch": 1.73125,
      "grad_norm": 18.70315933227539,
      "learning_rate": 9.1875e-06,
      "loss": 2.21,
      "step": 831
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 10.007413864135742,
      "learning_rate": 9.185185185185186e-06,
      "loss": 1.2375,
      "step": 832
    },
    {
      "epoch": 1.7354166666666666,
      "grad_norm": 11.646224975585938,
      "learning_rate": 9.182870370370372e-06,
      "loss": 2.094,
      "step": 833
    },
    {
      "epoch": 1.7375,
      "grad_norm": 7.496830463409424,
      "learning_rate": 9.180555555555556e-06,
      "loss": 1.2717,
      "step": 834
    },
    {
      "epoch": 1.7395833333333335,
      "grad_norm": 7.183534622192383,
      "learning_rate": 9.178240740740742e-06,
      "loss": 1.427,
      "step": 835
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 9.152249336242676,
      "learning_rate": 9.175925925925928e-06,
      "loss": 1.8227,
      "step": 836
    },
    {
      "epoch": 1.74375,
      "grad_norm": 10.726395606994629,
      "learning_rate": 9.173611111111112e-06,
      "loss": 1.3648,
      "step": 837
    },
    {
      "epoch": 1.7458333333333333,
      "grad_norm": 8.860553741455078,
      "learning_rate": 9.171296296296296e-06,
      "loss": 1.2382,
      "step": 838
    },
    {
      "epoch": 1.7479166666666668,
      "grad_norm": 21.66317367553711,
      "learning_rate": 9.168981481481482e-06,
      "loss": 2.0582,
      "step": 839
    },
    {
      "epoch": 1.75,
      "grad_norm": 9.258090019226074,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.8121,
      "step": 840
    },
    {
      "epoch": 1.7520833333333332,
      "grad_norm": 9.318236351013184,
      "learning_rate": 9.164351851851852e-06,
      "loss": 1.9395,
      "step": 841
    },
    {
      "epoch": 1.7541666666666667,
      "grad_norm": 17.32600212097168,
      "learning_rate": 9.162037037037038e-06,
      "loss": 1.3953,
      "step": 842
    },
    {
      "epoch": 1.75625,
      "grad_norm": 20.78260612487793,
      "learning_rate": 9.159722222222223e-06,
      "loss": 2.0838,
      "step": 843
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 8.978339195251465,
      "learning_rate": 9.157407407407409e-06,
      "loss": 1.0031,
      "step": 844
    },
    {
      "epoch": 1.7604166666666665,
      "grad_norm": 6.961418151855469,
      "learning_rate": 9.155092592592594e-06,
      "loss": 1.3141,
      "step": 845
    },
    {
      "epoch": 1.7625,
      "grad_norm": 11.999186515808105,
      "learning_rate": 9.152777777777779e-06,
      "loss": 1.5018,
      "step": 846
    },
    {
      "epoch": 1.7645833333333334,
      "grad_norm": 8.304859161376953,
      "learning_rate": 9.150462962962963e-06,
      "loss": 1.5647,
      "step": 847
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 8.583681106567383,
      "learning_rate": 9.148148148148149e-06,
      "loss": 1.589,
      "step": 848
    },
    {
      "epoch": 1.76875,
      "grad_norm": 5.588135719299316,
      "learning_rate": 9.145833333333333e-06,
      "loss": 1.9778,
      "step": 849
    },
    {
      "epoch": 1.7708333333333335,
      "grad_norm": 9.401382446289062,
      "learning_rate": 9.143518518518519e-06,
      "loss": 2.1103,
      "step": 850
    },
    {
      "epoch": 1.7729166666666667,
      "grad_norm": 8.44985580444336,
      "learning_rate": 9.141203703703705e-06,
      "loss": 1.068,
      "step": 851
    },
    {
      "epoch": 1.775,
      "grad_norm": 7.69551420211792,
      "learning_rate": 9.13888888888889e-06,
      "loss": 1.4734,
      "step": 852
    },
    {
      "epoch": 1.7770833333333333,
      "grad_norm": 16.553653717041016,
      "learning_rate": 9.136574074074075e-06,
      "loss": 1.6663,
      "step": 853
    },
    {
      "epoch": 1.7791666666666668,
      "grad_norm": 9.779820442199707,
      "learning_rate": 9.134259259259261e-06,
      "loss": 1.2793,
      "step": 854
    },
    {
      "epoch": 1.78125,
      "grad_norm": 8.227226257324219,
      "learning_rate": 9.131944444444445e-06,
      "loss": 1.5655,
      "step": 855
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 20.566431045532227,
      "learning_rate": 9.12962962962963e-06,
      "loss": 1.9869,
      "step": 856
    },
    {
      "epoch": 1.7854166666666667,
      "grad_norm": 7.706771373748779,
      "learning_rate": 9.127314814814816e-06,
      "loss": 2.0387,
      "step": 857
    },
    {
      "epoch": 1.7875,
      "grad_norm": 9.118346214294434,
      "learning_rate": 9.125e-06,
      "loss": 1.2238,
      "step": 858
    },
    {
      "epoch": 1.7895833333333333,
      "grad_norm": 8.33463191986084,
      "learning_rate": 9.122685185185186e-06,
      "loss": 1.8076,
      "step": 859
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 6.347353458404541,
      "learning_rate": 9.120370370370372e-06,
      "loss": 1.3117,
      "step": 860
    },
    {
      "epoch": 1.79375,
      "grad_norm": 16.891090393066406,
      "learning_rate": 9.118055555555556e-06,
      "loss": 1.6341,
      "step": 861
    },
    {
      "epoch": 1.7958333333333334,
      "grad_norm": 8.815967559814453,
      "learning_rate": 9.115740740740742e-06,
      "loss": 1.891,
      "step": 862
    },
    {
      "epoch": 1.7979166666666666,
      "grad_norm": 7.940005302429199,
      "learning_rate": 9.113425925925926e-06,
      "loss": 1.4523,
      "step": 863
    },
    {
      "epoch": 1.8,
      "grad_norm": 11.420913696289062,
      "learning_rate": 9.111111111111112e-06,
      "loss": 1.8099,
      "step": 864
    },
    {
      "epoch": 1.8020833333333335,
      "grad_norm": 15.994171142578125,
      "learning_rate": 9.108796296296296e-06,
      "loss": 1.8556,
      "step": 865
    },
    {
      "epoch": 1.8041666666666667,
      "grad_norm": 11.009873390197754,
      "learning_rate": 9.106481481481482e-06,
      "loss": 1.7073,
      "step": 866
    },
    {
      "epoch": 1.80625,
      "grad_norm": 14.285979270935059,
      "learning_rate": 9.104166666666667e-06,
      "loss": 1.6787,
      "step": 867
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 8.126300811767578,
      "learning_rate": 9.101851851851853e-06,
      "loss": 1.4408,
      "step": 868
    },
    {
      "epoch": 1.8104166666666668,
      "grad_norm": 10.936728477478027,
      "learning_rate": 9.099537037037037e-06,
      "loss": 1.5312,
      "step": 869
    },
    {
      "epoch": 1.8125,
      "grad_norm": 14.285359382629395,
      "learning_rate": 9.097222222222223e-06,
      "loss": 1.2912,
      "step": 870
    },
    {
      "epoch": 1.8145833333333332,
      "grad_norm": 9.016558647155762,
      "learning_rate": 9.094907407407409e-06,
      "loss": 1.7728,
      "step": 871
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 10.40654182434082,
      "learning_rate": 9.092592592592593e-06,
      "loss": 1.3217,
      "step": 872
    },
    {
      "epoch": 1.81875,
      "grad_norm": 10.579638481140137,
      "learning_rate": 9.090277777777779e-06,
      "loss": 1.3018,
      "step": 873
    },
    {
      "epoch": 1.8208333333333333,
      "grad_norm": 10.981321334838867,
      "learning_rate": 9.087962962962965e-06,
      "loss": 1.7958,
      "step": 874
    },
    {
      "epoch": 1.8229166666666665,
      "grad_norm": 6.823093891143799,
      "learning_rate": 9.085648148148149e-06,
      "loss": 1.9156,
      "step": 875
    },
    {
      "epoch": 1.825,
      "grad_norm": 6.484595775604248,
      "learning_rate": 9.083333333333333e-06,
      "loss": 2.164,
      "step": 876
    },
    {
      "epoch": 1.8270833333333334,
      "grad_norm": 7.0883917808532715,
      "learning_rate": 9.08101851851852e-06,
      "loss": 1.277,
      "step": 877
    },
    {
      "epoch": 1.8291666666666666,
      "grad_norm": 12.533559799194336,
      "learning_rate": 9.078703703703704e-06,
      "loss": 2.0539,
      "step": 878
    },
    {
      "epoch": 1.83125,
      "grad_norm": 13.249163627624512,
      "learning_rate": 9.07638888888889e-06,
      "loss": 1.8315,
      "step": 879
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 10.202385902404785,
      "learning_rate": 9.074074074074075e-06,
      "loss": 1.1941,
      "step": 880
    },
    {
      "epoch": 1.8354166666666667,
      "grad_norm": 13.566216468811035,
      "learning_rate": 9.07175925925926e-06,
      "loss": 1.6738,
      "step": 881
    },
    {
      "epoch": 1.8375,
      "grad_norm": 10.877077102661133,
      "learning_rate": 9.069444444444446e-06,
      "loss": 1.7684,
      "step": 882
    },
    {
      "epoch": 1.8395833333333333,
      "grad_norm": 7.034998416900635,
      "learning_rate": 9.067129629629632e-06,
      "loss": 1.7539,
      "step": 883
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 14.247953414916992,
      "learning_rate": 9.064814814814816e-06,
      "loss": 2.2199,
      "step": 884
    },
    {
      "epoch": 1.84375,
      "grad_norm": 7.842029094696045,
      "learning_rate": 9.0625e-06,
      "loss": 1.1922,
      "step": 885
    },
    {
      "epoch": 1.8458333333333332,
      "grad_norm": 10.577768325805664,
      "learning_rate": 9.060185185185186e-06,
      "loss": 1.8469,
      "step": 886
    },
    {
      "epoch": 1.8479166666666667,
      "grad_norm": 8.348637580871582,
      "learning_rate": 9.05787037037037e-06,
      "loss": 1.481,
      "step": 887
    },
    {
      "epoch": 1.85,
      "grad_norm": 7.942948341369629,
      "learning_rate": 9.055555555555556e-06,
      "loss": 1.4891,
      "step": 888
    },
    {
      "epoch": 1.8520833333333333,
      "grad_norm": 9.835296630859375,
      "learning_rate": 9.053240740740742e-06,
      "loss": 2.1825,
      "step": 889
    },
    {
      "epoch": 1.8541666666666665,
      "grad_norm": 12.1279296875,
      "learning_rate": 9.050925925925926e-06,
      "loss": 0.7215,
      "step": 890
    },
    {
      "epoch": 1.85625,
      "grad_norm": 11.793085098266602,
      "learning_rate": 9.048611111111112e-06,
      "loss": 1.5518,
      "step": 891
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 8.347285270690918,
      "learning_rate": 9.046296296296298e-06,
      "loss": 1.694,
      "step": 892
    },
    {
      "epoch": 1.8604166666666666,
      "grad_norm": 8.325379371643066,
      "learning_rate": 9.043981481481483e-06,
      "loss": 1.6504,
      "step": 893
    },
    {
      "epoch": 1.8625,
      "grad_norm": 11.7201566696167,
      "learning_rate": 9.041666666666667e-06,
      "loss": 0.7612,
      "step": 894
    },
    {
      "epoch": 1.8645833333333335,
      "grad_norm": 12.91265869140625,
      "learning_rate": 9.039351851851853e-06,
      "loss": 1.6201,
      "step": 895
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 8.927043914794922,
      "learning_rate": 9.037037037037037e-06,
      "loss": 2.0395,
      "step": 896
    },
    {
      "epoch": 1.86875,
      "grad_norm": 7.528295516967773,
      "learning_rate": 9.034722222222223e-06,
      "loss": 1.796,
      "step": 897
    },
    {
      "epoch": 1.8708333333333333,
      "grad_norm": 6.009469032287598,
      "learning_rate": 9.032407407407409e-06,
      "loss": 1.306,
      "step": 898
    },
    {
      "epoch": 1.8729166666666668,
      "grad_norm": 9.730255126953125,
      "learning_rate": 9.030092592592593e-06,
      "loss": 2.0464,
      "step": 899
    },
    {
      "epoch": 1.875,
      "grad_norm": 6.693554878234863,
      "learning_rate": 9.027777777777779e-06,
      "loss": 1.621,
      "step": 900
    },
    {
      "epoch": 1.8770833333333332,
      "grad_norm": 9.36329174041748,
      "learning_rate": 9.025462962962965e-06,
      "loss": 1.3875,
      "step": 901
    },
    {
      "epoch": 1.8791666666666667,
      "grad_norm": 12.936211585998535,
      "learning_rate": 9.02314814814815e-06,
      "loss": 1.9299,
      "step": 902
    },
    {
      "epoch": 1.88125,
      "grad_norm": 12.441951751708984,
      "learning_rate": 9.020833333333334e-06,
      "loss": 2.1367,
      "step": 903
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 9.181896209716797,
      "learning_rate": 9.01851851851852e-06,
      "loss": 1.2404,
      "step": 904
    },
    {
      "epoch": 1.8854166666666665,
      "grad_norm": 8.217595100402832,
      "learning_rate": 9.016203703703704e-06,
      "loss": 1.8981,
      "step": 905
    },
    {
      "epoch": 1.8875,
      "grad_norm": 13.830297470092773,
      "learning_rate": 9.01388888888889e-06,
      "loss": 1.93,
      "step": 906
    },
    {
      "epoch": 1.8895833333333334,
      "grad_norm": 9.606034278869629,
      "learning_rate": 9.011574074074076e-06,
      "loss": 1.8762,
      "step": 907
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 13.889596939086914,
      "learning_rate": 9.00925925925926e-06,
      "loss": 1.3878,
      "step": 908
    },
    {
      "epoch": 1.89375,
      "grad_norm": 19.686172485351562,
      "learning_rate": 9.006944444444446e-06,
      "loss": 1.351,
      "step": 909
    },
    {
      "epoch": 1.8958333333333335,
      "grad_norm": 8.082128524780273,
      "learning_rate": 9.00462962962963e-06,
      "loss": 1.449,
      "step": 910
    },
    {
      "epoch": 1.8979166666666667,
      "grad_norm": 4.123544216156006,
      "learning_rate": 9.002314814814816e-06,
      "loss": 1.7994,
      "step": 911
    },
    {
      "epoch": 1.9,
      "grad_norm": 11.414502143859863,
      "learning_rate": 9e-06,
      "loss": 1.6776,
      "step": 912
    },
    {
      "epoch": 1.9020833333333333,
      "grad_norm": 4.440438747406006,
      "learning_rate": 8.997685185185186e-06,
      "loss": 1.8135,
      "step": 913
    },
    {
      "epoch": 1.9041666666666668,
      "grad_norm": 11.983895301818848,
      "learning_rate": 8.99537037037037e-06,
      "loss": 1.8351,
      "step": 914
    },
    {
      "epoch": 1.90625,
      "grad_norm": 6.560915470123291,
      "learning_rate": 8.993055555555556e-06,
      "loss": 1.8465,
      "step": 915
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 8.028507232666016,
      "learning_rate": 8.99074074074074e-06,
      "loss": 1.4409,
      "step": 916
    },
    {
      "epoch": 1.9104166666666667,
      "grad_norm": 7.359493255615234,
      "learning_rate": 8.988425925925927e-06,
      "loss": 1.1926,
      "step": 917
    },
    {
      "epoch": 1.9125,
      "grad_norm": 7.468618869781494,
      "learning_rate": 8.986111111111113e-06,
      "loss": 1.6706,
      "step": 918
    },
    {
      "epoch": 1.9145833333333333,
      "grad_norm": 8.47833251953125,
      "learning_rate": 8.983796296296297e-06,
      "loss": 1.3987,
      "step": 919
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 7.997312545776367,
      "learning_rate": 8.981481481481483e-06,
      "loss": 1.2925,
      "step": 920
    },
    {
      "epoch": 1.91875,
      "grad_norm": 9.810345649719238,
      "learning_rate": 8.979166666666667e-06,
      "loss": 1.7328,
      "step": 921
    },
    {
      "epoch": 1.9208333333333334,
      "grad_norm": 14.142459869384766,
      "learning_rate": 8.976851851851853e-06,
      "loss": 1.2843,
      "step": 922
    },
    {
      "epoch": 1.9229166666666666,
      "grad_norm": 11.938398361206055,
      "learning_rate": 8.974537037037037e-06,
      "loss": 1.723,
      "step": 923
    },
    {
      "epoch": 1.925,
      "grad_norm": 13.46699333190918,
      "learning_rate": 8.972222222222223e-06,
      "loss": 2.0628,
      "step": 924
    },
    {
      "epoch": 1.9270833333333335,
      "grad_norm": 8.581670761108398,
      "learning_rate": 8.969907407407407e-06,
      "loss": 1.6072,
      "step": 925
    },
    {
      "epoch": 1.9291666666666667,
      "grad_norm": 7.81107234954834,
      "learning_rate": 8.967592592592593e-06,
      "loss": 1.1864,
      "step": 926
    },
    {
      "epoch": 1.93125,
      "grad_norm": 8.888562202453613,
      "learning_rate": 8.96527777777778e-06,
      "loss": 1.1963,
      "step": 927
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 16.082992553710938,
      "learning_rate": 8.962962962962963e-06,
      "loss": 1.6016,
      "step": 928
    },
    {
      "epoch": 1.9354166666666668,
      "grad_norm": 8.49306583404541,
      "learning_rate": 8.96064814814815e-06,
      "loss": 1.5287,
      "step": 929
    },
    {
      "epoch": 1.9375,
      "grad_norm": 13.873140335083008,
      "learning_rate": 8.958333333333334e-06,
      "loss": 1.2438,
      "step": 930
    },
    {
      "epoch": 1.9395833333333332,
      "grad_norm": 7.612985610961914,
      "learning_rate": 8.95601851851852e-06,
      "loss": 1.7006,
      "step": 931
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 15.920992851257324,
      "learning_rate": 8.953703703703704e-06,
      "loss": 1.51,
      "step": 932
    },
    {
      "epoch": 1.94375,
      "grad_norm": 24.004409790039062,
      "learning_rate": 8.95138888888889e-06,
      "loss": 1.597,
      "step": 933
    },
    {
      "epoch": 1.9458333333333333,
      "grad_norm": 14.39924144744873,
      "learning_rate": 8.949074074074074e-06,
      "loss": 1.978,
      "step": 934
    },
    {
      "epoch": 1.9479166666666665,
      "grad_norm": 9.981633186340332,
      "learning_rate": 8.94675925925926e-06,
      "loss": 1.7472,
      "step": 935
    },
    {
      "epoch": 1.95,
      "grad_norm": 9.47550106048584,
      "learning_rate": 8.944444444444446e-06,
      "loss": 1.8064,
      "step": 936
    },
    {
      "epoch": 1.9520833333333334,
      "grad_norm": 16.91872215270996,
      "learning_rate": 8.94212962962963e-06,
      "loss": 0.8537,
      "step": 937
    },
    {
      "epoch": 1.9541666666666666,
      "grad_norm": 10.532050132751465,
      "learning_rate": 8.939814814814816e-06,
      "loss": 1.1795,
      "step": 938
    },
    {
      "epoch": 1.95625,
      "grad_norm": 10.26323413848877,
      "learning_rate": 8.9375e-06,
      "loss": 2.1788,
      "step": 939
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 8.952888488769531,
      "learning_rate": 8.935185185185186e-06,
      "loss": 1.3374,
      "step": 940
    },
    {
      "epoch": 1.9604166666666667,
      "grad_norm": 16.264251708984375,
      "learning_rate": 8.93287037037037e-06,
      "loss": 2.1409,
      "step": 941
    },
    {
      "epoch": 1.9625,
      "grad_norm": 11.512383460998535,
      "learning_rate": 8.930555555555557e-06,
      "loss": 1.7176,
      "step": 942
    },
    {
      "epoch": 1.9645833333333333,
      "grad_norm": 8.073332786560059,
      "learning_rate": 8.92824074074074e-06,
      "loss": 1.5532,
      "step": 943
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 11.432637214660645,
      "learning_rate": 8.925925925925927e-06,
      "loss": 0.6439,
      "step": 944
    },
    {
      "epoch": 1.96875,
      "grad_norm": 13.920961380004883,
      "learning_rate": 8.923611111111113e-06,
      "loss": 1.7579,
      "step": 945
    },
    {
      "epoch": 1.9708333333333332,
      "grad_norm": 9.424504280090332,
      "learning_rate": 8.921296296296297e-06,
      "loss": 1.1218,
      "step": 946
    },
    {
      "epoch": 1.9729166666666667,
      "grad_norm": 12.746904373168945,
      "learning_rate": 8.918981481481483e-06,
      "loss": 1.8139,
      "step": 947
    },
    {
      "epoch": 1.975,
      "grad_norm": 9.633855819702148,
      "learning_rate": 8.916666666666667e-06,
      "loss": 1.1521,
      "step": 948
    },
    {
      "epoch": 1.9770833333333333,
      "grad_norm": 10.285284996032715,
      "learning_rate": 8.914351851851853e-06,
      "loss": 1.8117,
      "step": 949
    },
    {
      "epoch": 1.9791666666666665,
      "grad_norm": 10.111377716064453,
      "learning_rate": 8.912037037037037e-06,
      "loss": 1.6527,
      "step": 950
    },
    {
      "epoch": 1.98125,
      "grad_norm": 6.363406181335449,
      "learning_rate": 8.909722222222223e-06,
      "loss": 1.2558,
      "step": 951
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 8.595080375671387,
      "learning_rate": 8.907407407407408e-06,
      "loss": 1.162,
      "step": 952
    },
    {
      "epoch": 1.9854166666666666,
      "grad_norm": 17.88335418701172,
      "learning_rate": 8.905092592592593e-06,
      "loss": 2.3154,
      "step": 953
    },
    {
      "epoch": 1.9875,
      "grad_norm": 11.945383071899414,
      "learning_rate": 8.902777777777778e-06,
      "loss": 1.6338,
      "step": 954
    },
    {
      "epoch": 1.9895833333333335,
      "grad_norm": 8.615144729614258,
      "learning_rate": 8.900462962962964e-06,
      "loss": 1.9074,
      "step": 955
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 11.33499813079834,
      "learning_rate": 8.89814814814815e-06,
      "loss": 0.6742,
      "step": 956
    },
    {
      "epoch": 1.99375,
      "grad_norm": 12.203349113464355,
      "learning_rate": 8.895833333333334e-06,
      "loss": 1.6212,
      "step": 957
    },
    {
      "epoch": 1.9958333333333333,
      "grad_norm": 14.368036270141602,
      "learning_rate": 8.89351851851852e-06,
      "loss": 1.754,
      "step": 958
    },
    {
      "epoch": 1.9979166666666668,
      "grad_norm": 15.935209274291992,
      "learning_rate": 8.891203703703704e-06,
      "loss": 2.4105,
      "step": 959
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.258042335510254,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.2768,
      "step": 960
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.3333333333333333,
      "eval_f1": 0.15716205837173577,
      "eval_loss": 1.6416659355163574,
      "eval_runtime": 34.3996,
      "eval_samples_per_second": 5.233,
      "eval_steps_per_second": 2.616,
      "step": 960
    }
  ],
  "logging_steps": 1,
  "max_steps": 4800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.229364174848e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
