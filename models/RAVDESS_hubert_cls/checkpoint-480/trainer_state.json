{
  "best_metric": 0.16282809420064323,
  "best_model_checkpoint": "models\\RAVDESS_hubert_cls\\checkpoint-480",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 480,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020833333333333333,
      "grad_norm": 1.3656010627746582,
      "learning_rate": 2.0833333333333335e-08,
      "loss": 1.9327,
      "step": 1
    },
    {
      "epoch": 0.004166666666666667,
      "grad_norm": 1.4255727529525757,
      "learning_rate": 4.166666666666667e-08,
      "loss": 1.968,
      "step": 2
    },
    {
      "epoch": 0.00625,
      "grad_norm": 1.5010994672775269,
      "learning_rate": 6.250000000000001e-08,
      "loss": 1.9411,
      "step": 3
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.9118722677230835,
      "learning_rate": 8.333333333333334e-08,
      "loss": 1.9608,
      "step": 4
    },
    {
      "epoch": 0.010416666666666666,
      "grad_norm": 1.3321853876113892,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 1.9254,
      "step": 5
    },
    {
      "epoch": 0.0125,
      "grad_norm": 2.400676965713501,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 1.9986,
      "step": 6
    },
    {
      "epoch": 0.014583333333333334,
      "grad_norm": 1.4379546642303467,
      "learning_rate": 1.4583333333333335e-07,
      "loss": 1.9384,
      "step": 7
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.3329111337661743,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 1.9572,
      "step": 8
    },
    {
      "epoch": 0.01875,
      "grad_norm": 1.250981092453003,
      "learning_rate": 1.875e-07,
      "loss": 1.9607,
      "step": 9
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 1.340410828590393,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 1.9308,
      "step": 10
    },
    {
      "epoch": 0.022916666666666665,
      "grad_norm": 1.474332571029663,
      "learning_rate": 2.2916666666666666e-07,
      "loss": 1.9204,
      "step": 11
    },
    {
      "epoch": 0.025,
      "grad_norm": 1.4370452165603638,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 1.9746,
      "step": 12
    },
    {
      "epoch": 0.027083333333333334,
      "grad_norm": 1.5659068822860718,
      "learning_rate": 2.7083333333333337e-07,
      "loss": 1.9237,
      "step": 13
    },
    {
      "epoch": 0.029166666666666667,
      "grad_norm": 1.6647891998291016,
      "learning_rate": 2.916666666666667e-07,
      "loss": 1.9787,
      "step": 14
    },
    {
      "epoch": 0.03125,
      "grad_norm": 1.1900396347045898,
      "learning_rate": 3.125e-07,
      "loss": 1.9424,
      "step": 15
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 2.090128183364868,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 1.9332,
      "step": 16
    },
    {
      "epoch": 0.035416666666666666,
      "grad_norm": 1.1047351360321045,
      "learning_rate": 3.541666666666667e-07,
      "loss": 1.9591,
      "step": 17
    },
    {
      "epoch": 0.0375,
      "grad_norm": 1.113924264907837,
      "learning_rate": 3.75e-07,
      "loss": 1.9185,
      "step": 18
    },
    {
      "epoch": 0.03958333333333333,
      "grad_norm": 1.4250507354736328,
      "learning_rate": 3.9583333333333334e-07,
      "loss": 1.9344,
      "step": 19
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 1.186957836151123,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 1.9861,
      "step": 20
    },
    {
      "epoch": 0.04375,
      "grad_norm": 1.1937501430511475,
      "learning_rate": 4.375e-07,
      "loss": 1.9203,
      "step": 21
    },
    {
      "epoch": 0.04583333333333333,
      "grad_norm": 1.34679114818573,
      "learning_rate": 4.583333333333333e-07,
      "loss": 1.9025,
      "step": 22
    },
    {
      "epoch": 0.04791666666666667,
      "grad_norm": 1.327927827835083,
      "learning_rate": 4.791666666666667e-07,
      "loss": 1.9803,
      "step": 23
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.2930009365081787,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9648,
      "step": 24
    },
    {
      "epoch": 0.052083333333333336,
      "grad_norm": 1.9997715950012207,
      "learning_rate": 5.208333333333334e-07,
      "loss": 1.8909,
      "step": 25
    },
    {
      "epoch": 0.05416666666666667,
      "grad_norm": 1.2654446363449097,
      "learning_rate": 5.416666666666667e-07,
      "loss": 1.9629,
      "step": 26
    },
    {
      "epoch": 0.05625,
      "grad_norm": 1.3119282722473145,
      "learning_rate": 5.625e-07,
      "loss": 1.9094,
      "step": 27
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 1.316719889640808,
      "learning_rate": 5.833333333333334e-07,
      "loss": 1.9397,
      "step": 28
    },
    {
      "epoch": 0.06041666666666667,
      "grad_norm": 1.38850736618042,
      "learning_rate": 6.041666666666667e-07,
      "loss": 1.9292,
      "step": 29
    },
    {
      "epoch": 0.0625,
      "grad_norm": 1.8557636737823486,
      "learning_rate": 6.25e-07,
      "loss": 1.9481,
      "step": 30
    },
    {
      "epoch": 0.06458333333333334,
      "grad_norm": 1.1216989755630493,
      "learning_rate": 6.458333333333334e-07,
      "loss": 1.9519,
      "step": 31
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.4565935134887695,
      "learning_rate": 6.666666666666667e-07,
      "loss": 2.006,
      "step": 32
    },
    {
      "epoch": 0.06875,
      "grad_norm": 1.4888185262680054,
      "learning_rate": 6.875000000000001e-07,
      "loss": 1.9306,
      "step": 33
    },
    {
      "epoch": 0.07083333333333333,
      "grad_norm": 1.3666950464248657,
      "learning_rate": 7.083333333333334e-07,
      "loss": 1.9474,
      "step": 34
    },
    {
      "epoch": 0.07291666666666667,
      "grad_norm": 1.453478217124939,
      "learning_rate": 7.291666666666667e-07,
      "loss": 1.9681,
      "step": 35
    },
    {
      "epoch": 0.075,
      "grad_norm": 1.4112991094589233,
      "learning_rate": 7.5e-07,
      "loss": 1.9334,
      "step": 36
    },
    {
      "epoch": 0.07708333333333334,
      "grad_norm": 1.5293419361114502,
      "learning_rate": 7.708333333333334e-07,
      "loss": 1.948,
      "step": 37
    },
    {
      "epoch": 0.07916666666666666,
      "grad_norm": 1.7969555854797363,
      "learning_rate": 7.916666666666667e-07,
      "loss": 1.9944,
      "step": 38
    },
    {
      "epoch": 0.08125,
      "grad_norm": 1.4376308917999268,
      "learning_rate": 8.125000000000001e-07,
      "loss": 1.9533,
      "step": 39
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.4313029050827026,
      "learning_rate": 8.333333333333333e-07,
      "loss": 1.9417,
      "step": 40
    },
    {
      "epoch": 0.08541666666666667,
      "grad_norm": 2.147570848464966,
      "learning_rate": 8.541666666666667e-07,
      "loss": 1.9789,
      "step": 41
    },
    {
      "epoch": 0.0875,
      "grad_norm": 1.1594797372817993,
      "learning_rate": 8.75e-07,
      "loss": 1.9405,
      "step": 42
    },
    {
      "epoch": 0.08958333333333333,
      "grad_norm": 1.6665924787521362,
      "learning_rate": 8.958333333333334e-07,
      "loss": 1.9406,
      "step": 43
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 2.037606954574585,
      "learning_rate": 9.166666666666666e-07,
      "loss": 1.924,
      "step": 44
    },
    {
      "epoch": 0.09375,
      "grad_norm": 1.2316523790359497,
      "learning_rate": 9.375000000000001e-07,
      "loss": 1.9403,
      "step": 45
    },
    {
      "epoch": 0.09583333333333334,
      "grad_norm": 1.21660578250885,
      "learning_rate": 9.583333333333334e-07,
      "loss": 1.9193,
      "step": 46
    },
    {
      "epoch": 0.09791666666666667,
      "grad_norm": 1.5828365087509155,
      "learning_rate": 9.791666666666667e-07,
      "loss": 1.9893,
      "step": 47
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.723954677581787,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.9728,
      "step": 48
    },
    {
      "epoch": 0.10208333333333333,
      "grad_norm": 1.3959202766418457,
      "learning_rate": 1.0208333333333334e-06,
      "loss": 1.9361,
      "step": 49
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 1.4296817779541016,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 1.9288,
      "step": 50
    },
    {
      "epoch": 0.10625,
      "grad_norm": 1.9942065477371216,
      "learning_rate": 1.0625e-06,
      "loss": 1.948,
      "step": 51
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 1.499065637588501,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 1.9409,
      "step": 52
    },
    {
      "epoch": 0.11041666666666666,
      "grad_norm": 1.2829371690750122,
      "learning_rate": 1.1041666666666668e-06,
      "loss": 1.912,
      "step": 53
    },
    {
      "epoch": 0.1125,
      "grad_norm": 1.4519883394241333,
      "learning_rate": 1.125e-06,
      "loss": 1.9414,
      "step": 54
    },
    {
      "epoch": 0.11458333333333333,
      "grad_norm": 1.3108900785446167,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 1.9584,
      "step": 55
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.8849138021469116,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 1.9489,
      "step": 56
    },
    {
      "epoch": 0.11875,
      "grad_norm": 1.4732887744903564,
      "learning_rate": 1.1875e-06,
      "loss": 1.9627,
      "step": 57
    },
    {
      "epoch": 0.12083333333333333,
      "grad_norm": 1.2098169326782227,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 1.9225,
      "step": 58
    },
    {
      "epoch": 0.12291666666666666,
      "grad_norm": 1.3880913257598877,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 1.931,
      "step": 59
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.284332513809204,
      "learning_rate": 1.25e-06,
      "loss": 1.9064,
      "step": 60
    },
    {
      "epoch": 0.12708333333333333,
      "grad_norm": 1.3738212585449219,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 1.9499,
      "step": 61
    },
    {
      "epoch": 0.12916666666666668,
      "grad_norm": 1.366930365562439,
      "learning_rate": 1.2916666666666669e-06,
      "loss": 1.9269,
      "step": 62
    },
    {
      "epoch": 0.13125,
      "grad_norm": 1.4547582864761353,
      "learning_rate": 1.3125000000000001e-06,
      "loss": 1.972,
      "step": 63
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.3769160509109497,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 1.9461,
      "step": 64
    },
    {
      "epoch": 0.13541666666666666,
      "grad_norm": 1.718178391456604,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 1.9872,
      "step": 65
    },
    {
      "epoch": 0.1375,
      "grad_norm": 1.8160940408706665,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 1.9294,
      "step": 66
    },
    {
      "epoch": 0.13958333333333334,
      "grad_norm": 1.7659497261047363,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 1.9772,
      "step": 67
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 2.45913028717041,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 2.0076,
      "step": 68
    },
    {
      "epoch": 0.14375,
      "grad_norm": 1.2259025573730469,
      "learning_rate": 1.4375e-06,
      "loss": 1.9295,
      "step": 69
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 1.30710768699646,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 1.9521,
      "step": 70
    },
    {
      "epoch": 0.14791666666666667,
      "grad_norm": 1.3667610883712769,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 1.9655,
      "step": 71
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.4477694034576416,
      "learning_rate": 1.5e-06,
      "loss": 1.9607,
      "step": 72
    },
    {
      "epoch": 0.15208333333333332,
      "grad_norm": 1.3650097846984863,
      "learning_rate": 1.5208333333333333e-06,
      "loss": 1.917,
      "step": 73
    },
    {
      "epoch": 0.15416666666666667,
      "grad_norm": 1.3355998992919922,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 1.9169,
      "step": 74
    },
    {
      "epoch": 0.15625,
      "grad_norm": 1.2580227851867676,
      "learning_rate": 1.5625e-06,
      "loss": 1.9224,
      "step": 75
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.6419705152511597,
      "learning_rate": 1.5833333333333333e-06,
      "loss": 1.9543,
      "step": 76
    },
    {
      "epoch": 0.16041666666666668,
      "grad_norm": 1.6093991994857788,
      "learning_rate": 1.6041666666666668e-06,
      "loss": 1.9102,
      "step": 77
    },
    {
      "epoch": 0.1625,
      "grad_norm": 1.5098154544830322,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 1.9024,
      "step": 78
    },
    {
      "epoch": 0.16458333333333333,
      "grad_norm": 1.4802966117858887,
      "learning_rate": 1.6458333333333334e-06,
      "loss": 1.991,
      "step": 79
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.492950439453125,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 1.9132,
      "step": 80
    },
    {
      "epoch": 0.16875,
      "grad_norm": 1.7119752168655396,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 1.9472,
      "step": 81
    },
    {
      "epoch": 0.17083333333333334,
      "grad_norm": 1.3862414360046387,
      "learning_rate": 1.7083333333333334e-06,
      "loss": 1.9366,
      "step": 82
    },
    {
      "epoch": 0.17291666666666666,
      "grad_norm": 1.7285840511322021,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 1.9395,
      "step": 83
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.4411652088165283,
      "learning_rate": 1.75e-06,
      "loss": 1.9125,
      "step": 84
    },
    {
      "epoch": 0.17708333333333334,
      "grad_norm": 1.2784425020217896,
      "learning_rate": 1.7708333333333337e-06,
      "loss": 1.963,
      "step": 85
    },
    {
      "epoch": 0.17916666666666667,
      "grad_norm": 1.8572155237197876,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 1.9854,
      "step": 86
    },
    {
      "epoch": 0.18125,
      "grad_norm": 2.0394866466522217,
      "learning_rate": 1.8125e-06,
      "loss": 1.9486,
      "step": 87
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.1604998111724854,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 1.9248,
      "step": 88
    },
    {
      "epoch": 0.18541666666666667,
      "grad_norm": 1.3454688787460327,
      "learning_rate": 1.854166666666667e-06,
      "loss": 1.9569,
      "step": 89
    },
    {
      "epoch": 0.1875,
      "grad_norm": 1.2562265396118164,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 1.9538,
      "step": 90
    },
    {
      "epoch": 0.18958333333333333,
      "grad_norm": 1.603117823600769,
      "learning_rate": 1.8958333333333333e-06,
      "loss": 1.9348,
      "step": 91
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.3332278728485107,
      "learning_rate": 1.916666666666667e-06,
      "loss": 1.9159,
      "step": 92
    },
    {
      "epoch": 0.19375,
      "grad_norm": 1.4468432664871216,
      "learning_rate": 1.9375e-06,
      "loss": 1.9273,
      "step": 93
    },
    {
      "epoch": 0.19583333333333333,
      "grad_norm": 1.5407981872558594,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 1.9555,
      "step": 94
    },
    {
      "epoch": 0.19791666666666666,
      "grad_norm": 1.5373258590698242,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 1.9803,
      "step": 95
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.3939034938812256,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.9028,
      "step": 96
    },
    {
      "epoch": 0.20208333333333334,
      "grad_norm": 1.5232770442962646,
      "learning_rate": 2.0208333333333336e-06,
      "loss": 1.9682,
      "step": 97
    },
    {
      "epoch": 0.20416666666666666,
      "grad_norm": 1.5902122259140015,
      "learning_rate": 2.041666666666667e-06,
      "loss": 1.9409,
      "step": 98
    },
    {
      "epoch": 0.20625,
      "grad_norm": 1.3247003555297852,
      "learning_rate": 2.0625e-06,
      "loss": 1.9401,
      "step": 99
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.5977855920791626,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 1.9686,
      "step": 100
    },
    {
      "epoch": 0.21041666666666667,
      "grad_norm": 1.405238151550293,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 1.9374,
      "step": 101
    },
    {
      "epoch": 0.2125,
      "grad_norm": 1.5895088911056519,
      "learning_rate": 2.125e-06,
      "loss": 1.9829,
      "step": 102
    },
    {
      "epoch": 0.21458333333333332,
      "grad_norm": 1.6168591976165771,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 1.9194,
      "step": 103
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.9306175708770752,
      "learning_rate": 2.166666666666667e-06,
      "loss": 1.9244,
      "step": 104
    },
    {
      "epoch": 0.21875,
      "grad_norm": 1.3008852005004883,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 1.9116,
      "step": 105
    },
    {
      "epoch": 0.22083333333333333,
      "grad_norm": 1.2496017217636108,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 1.9375,
      "step": 106
    },
    {
      "epoch": 0.22291666666666668,
      "grad_norm": 1.2698489427566528,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 1.9534,
      "step": 107
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.3460439443588257,
      "learning_rate": 2.25e-06,
      "loss": 1.9512,
      "step": 108
    },
    {
      "epoch": 0.22708333333333333,
      "grad_norm": 1.8827704191207886,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 2.0046,
      "step": 109
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 1.8271830081939697,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 2.0088,
      "step": 110
    },
    {
      "epoch": 0.23125,
      "grad_norm": 2.005007028579712,
      "learning_rate": 2.3125000000000003e-06,
      "loss": 1.9298,
      "step": 111
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.6933585405349731,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 1.9791,
      "step": 112
    },
    {
      "epoch": 0.23541666666666666,
      "grad_norm": 1.3853344917297363,
      "learning_rate": 2.354166666666667e-06,
      "loss": 1.9041,
      "step": 113
    },
    {
      "epoch": 0.2375,
      "grad_norm": 1.6051607131958008,
      "learning_rate": 2.375e-06,
      "loss": 1.9834,
      "step": 114
    },
    {
      "epoch": 0.23958333333333334,
      "grad_norm": 1.616532802581787,
      "learning_rate": 2.395833333333334e-06,
      "loss": 1.9921,
      "step": 115
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.9666160345077515,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 1.9459,
      "step": 116
    },
    {
      "epoch": 0.24375,
      "grad_norm": 2.0627338886260986,
      "learning_rate": 2.4375e-06,
      "loss": 1.9391,
      "step": 117
    },
    {
      "epoch": 0.24583333333333332,
      "grad_norm": 1.3450732231140137,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 1.9568,
      "step": 118
    },
    {
      "epoch": 0.24791666666666667,
      "grad_norm": 1.6196264028549194,
      "learning_rate": 2.479166666666667e-06,
      "loss": 1.9144,
      "step": 119
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.5735896825790405,
      "learning_rate": 2.5e-06,
      "loss": 1.9322,
      "step": 120
    },
    {
      "epoch": 0.2520833333333333,
      "grad_norm": 1.6160943508148193,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 1.9817,
      "step": 121
    },
    {
      "epoch": 0.25416666666666665,
      "grad_norm": 1.608150839805603,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 1.974,
      "step": 122
    },
    {
      "epoch": 0.25625,
      "grad_norm": 1.3417377471923828,
      "learning_rate": 2.5625e-06,
      "loss": 1.9541,
      "step": 123
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.2143162488937378,
      "learning_rate": 2.5833333333333337e-06,
      "loss": 1.9392,
      "step": 124
    },
    {
      "epoch": 0.2604166666666667,
      "grad_norm": 1.6939482688903809,
      "learning_rate": 2.604166666666667e-06,
      "loss": 1.9366,
      "step": 125
    },
    {
      "epoch": 0.2625,
      "grad_norm": 1.740329623222351,
      "learning_rate": 2.6250000000000003e-06,
      "loss": 1.9781,
      "step": 126
    },
    {
      "epoch": 0.26458333333333334,
      "grad_norm": 1.1429450511932373,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 1.9185,
      "step": 127
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.9097005128860474,
      "learning_rate": 2.666666666666667e-06,
      "loss": 1.9879,
      "step": 128
    },
    {
      "epoch": 0.26875,
      "grad_norm": 1.4480551481246948,
      "learning_rate": 2.6875e-06,
      "loss": 1.9101,
      "step": 129
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 1.4415920972824097,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 1.9234,
      "step": 130
    },
    {
      "epoch": 0.27291666666666664,
      "grad_norm": 1.4805325269699097,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 1.9103,
      "step": 131
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.4249248504638672,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 1.9625,
      "step": 132
    },
    {
      "epoch": 0.27708333333333335,
      "grad_norm": 1.273820400238037,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 1.9058,
      "step": 133
    },
    {
      "epoch": 0.2791666666666667,
      "grad_norm": 1.4991645812988281,
      "learning_rate": 2.791666666666667e-06,
      "loss": 1.9117,
      "step": 134
    },
    {
      "epoch": 0.28125,
      "grad_norm": 1.248900055885315,
      "learning_rate": 2.8125e-06,
      "loss": 1.918,
      "step": 135
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.3453645706176758,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 1.9484,
      "step": 136
    },
    {
      "epoch": 0.28541666666666665,
      "grad_norm": 1.6247049570083618,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 1.9047,
      "step": 137
    },
    {
      "epoch": 0.2875,
      "grad_norm": 1.843184471130371,
      "learning_rate": 2.875e-06,
      "loss": 1.9819,
      "step": 138
    },
    {
      "epoch": 0.28958333333333336,
      "grad_norm": 1.561177134513855,
      "learning_rate": 2.8958333333333337e-06,
      "loss": 1.9662,
      "step": 139
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.7659250497817993,
      "learning_rate": 2.916666666666667e-06,
      "loss": 1.944,
      "step": 140
    },
    {
      "epoch": 0.29375,
      "grad_norm": 1.5173490047454834,
      "learning_rate": 2.9375000000000003e-06,
      "loss": 1.9748,
      "step": 141
    },
    {
      "epoch": 0.29583333333333334,
      "grad_norm": 1.7372595071792603,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 1.9787,
      "step": 142
    },
    {
      "epoch": 0.29791666666666666,
      "grad_norm": 1.342614769935608,
      "learning_rate": 2.979166666666667e-06,
      "loss": 1.9399,
      "step": 143
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.566171407699585,
      "learning_rate": 3e-06,
      "loss": 1.9392,
      "step": 144
    },
    {
      "epoch": 0.3020833333333333,
      "grad_norm": 1.4333405494689941,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 1.954,
      "step": 145
    },
    {
      "epoch": 0.30416666666666664,
      "grad_norm": 1.7944831848144531,
      "learning_rate": 3.0416666666666666e-06,
      "loss": 1.9957,
      "step": 146
    },
    {
      "epoch": 0.30625,
      "grad_norm": 3.011237382888794,
      "learning_rate": 3.0625000000000003e-06,
      "loss": 2.0382,
      "step": 147
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 1.4312045574188232,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 1.9054,
      "step": 148
    },
    {
      "epoch": 0.3104166666666667,
      "grad_norm": 1.70932936668396,
      "learning_rate": 3.104166666666667e-06,
      "loss": 1.9834,
      "step": 149
    },
    {
      "epoch": 0.3125,
      "grad_norm": 1.5819846391677856,
      "learning_rate": 3.125e-06,
      "loss": 1.8841,
      "step": 150
    },
    {
      "epoch": 0.3145833333333333,
      "grad_norm": 1.8980270624160767,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 1.929,
      "step": 151
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 2.596450090408325,
      "learning_rate": 3.1666666666666667e-06,
      "loss": 1.9101,
      "step": 152
    },
    {
      "epoch": 0.31875,
      "grad_norm": 2.495577573776245,
      "learning_rate": 3.1875e-06,
      "loss": 1.9147,
      "step": 153
    },
    {
      "epoch": 0.32083333333333336,
      "grad_norm": 1.3611204624176025,
      "learning_rate": 3.2083333333333337e-06,
      "loss": 1.9271,
      "step": 154
    },
    {
      "epoch": 0.3229166666666667,
      "grad_norm": 1.9344487190246582,
      "learning_rate": 3.229166666666667e-06,
      "loss": 1.9114,
      "step": 155
    },
    {
      "epoch": 0.325,
      "grad_norm": 2.0817620754241943,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.9762,
      "step": 156
    },
    {
      "epoch": 0.32708333333333334,
      "grad_norm": 1.6482269763946533,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 2.0067,
      "step": 157
    },
    {
      "epoch": 0.32916666666666666,
      "grad_norm": 1.6429798603057861,
      "learning_rate": 3.2916666666666668e-06,
      "loss": 1.9578,
      "step": 158
    },
    {
      "epoch": 0.33125,
      "grad_norm": 1.3621513843536377,
      "learning_rate": 3.3125e-06,
      "loss": 1.9231,
      "step": 159
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.709699034690857,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.8905,
      "step": 160
    },
    {
      "epoch": 0.33541666666666664,
      "grad_norm": 1.424316644668579,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 1.9278,
      "step": 161
    },
    {
      "epoch": 0.3375,
      "grad_norm": 1.7434216737747192,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 1.9799,
      "step": 162
    },
    {
      "epoch": 0.33958333333333335,
      "grad_norm": 1.5084645748138428,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 1.9609,
      "step": 163
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 2.189319610595703,
      "learning_rate": 3.416666666666667e-06,
      "loss": 1.9024,
      "step": 164
    },
    {
      "epoch": 0.34375,
      "grad_norm": 1.3864644765853882,
      "learning_rate": 3.4375e-06,
      "loss": 1.9212,
      "step": 165
    },
    {
      "epoch": 0.3458333333333333,
      "grad_norm": 2.4119699001312256,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 1.9062,
      "step": 166
    },
    {
      "epoch": 0.34791666666666665,
      "grad_norm": 1.7223366498947144,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 1.9691,
      "step": 167
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.385590076446533,
      "learning_rate": 3.5e-06,
      "loss": 1.9938,
      "step": 168
    },
    {
      "epoch": 0.35208333333333336,
      "grad_norm": 1.6133110523223877,
      "learning_rate": 3.520833333333334e-06,
      "loss": 1.9105,
      "step": 169
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 2.080188274383545,
      "learning_rate": 3.5416666666666673e-06,
      "loss": 1.9444,
      "step": 170
    },
    {
      "epoch": 0.35625,
      "grad_norm": 2.002821922302246,
      "learning_rate": 3.5625e-06,
      "loss": 1.9914,
      "step": 171
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 1.4226205348968506,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 1.934,
      "step": 172
    },
    {
      "epoch": 0.36041666666666666,
      "grad_norm": 1.5380207300186157,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 1.9668,
      "step": 173
    },
    {
      "epoch": 0.3625,
      "grad_norm": 1.9901806116104126,
      "learning_rate": 3.625e-06,
      "loss": 1.9,
      "step": 174
    },
    {
      "epoch": 0.3645833333333333,
      "grad_norm": 1.6424179077148438,
      "learning_rate": 3.6458333333333333e-06,
      "loss": 1.9333,
      "step": 175
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 2.1804091930389404,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 1.8936,
      "step": 176
    },
    {
      "epoch": 0.36875,
      "grad_norm": 1.5809381008148193,
      "learning_rate": 3.6875000000000007e-06,
      "loss": 1.9643,
      "step": 177
    },
    {
      "epoch": 0.37083333333333335,
      "grad_norm": 2.197772741317749,
      "learning_rate": 3.708333333333334e-06,
      "loss": 1.9709,
      "step": 178
    },
    {
      "epoch": 0.3729166666666667,
      "grad_norm": 1.8060131072998047,
      "learning_rate": 3.7291666666666672e-06,
      "loss": 1.9395,
      "step": 179
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.7433029413223267,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 1.9848,
      "step": 180
    },
    {
      "epoch": 0.3770833333333333,
      "grad_norm": 1.6533558368682861,
      "learning_rate": 3.7708333333333334e-06,
      "loss": 1.8929,
      "step": 181
    },
    {
      "epoch": 0.37916666666666665,
      "grad_norm": 1.9880746603012085,
      "learning_rate": 3.7916666666666666e-06,
      "loss": 1.9319,
      "step": 182
    },
    {
      "epoch": 0.38125,
      "grad_norm": 1.354346752166748,
      "learning_rate": 3.8125e-06,
      "loss": 1.9491,
      "step": 183
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.6048343181610107,
      "learning_rate": 3.833333333333334e-06,
      "loss": 1.8989,
      "step": 184
    },
    {
      "epoch": 0.3854166666666667,
      "grad_norm": 2.248694896697998,
      "learning_rate": 3.854166666666667e-06,
      "loss": 1.8769,
      "step": 185
    },
    {
      "epoch": 0.3875,
      "grad_norm": 1.8974634408950806,
      "learning_rate": 3.875e-06,
      "loss": 1.966,
      "step": 186
    },
    {
      "epoch": 0.38958333333333334,
      "grad_norm": 1.554516077041626,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 1.9232,
      "step": 187
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.7448005676269531,
      "learning_rate": 3.916666666666667e-06,
      "loss": 1.9812,
      "step": 188
    },
    {
      "epoch": 0.39375,
      "grad_norm": 1.5821349620819092,
      "learning_rate": 3.9375e-06,
      "loss": 1.8925,
      "step": 189
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 1.703403115272522,
      "learning_rate": 3.958333333333333e-06,
      "loss": 1.8891,
      "step": 190
    },
    {
      "epoch": 0.39791666666666664,
      "grad_norm": 2.4912328720092773,
      "learning_rate": 3.9791666666666665e-06,
      "loss": 1.8883,
      "step": 191
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.5777997970581055,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9096,
      "step": 192
    },
    {
      "epoch": 0.40208333333333335,
      "grad_norm": 1.5086619853973389,
      "learning_rate": 4.020833333333334e-06,
      "loss": 1.9592,
      "step": 193
    },
    {
      "epoch": 0.4041666666666667,
      "grad_norm": 1.7144463062286377,
      "learning_rate": 4.041666666666667e-06,
      "loss": 1.9338,
      "step": 194
    },
    {
      "epoch": 0.40625,
      "grad_norm": 2.938366651535034,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 1.8783,
      "step": 195
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.7791860103607178,
      "learning_rate": 4.083333333333334e-06,
      "loss": 1.9534,
      "step": 196
    },
    {
      "epoch": 0.41041666666666665,
      "grad_norm": 2.0922815799713135,
      "learning_rate": 4.104166666666667e-06,
      "loss": 1.9583,
      "step": 197
    },
    {
      "epoch": 0.4125,
      "grad_norm": 1.9370859861373901,
      "learning_rate": 4.125e-06,
      "loss": 1.974,
      "step": 198
    },
    {
      "epoch": 0.41458333333333336,
      "grad_norm": 1.4685262441635132,
      "learning_rate": 4.145833333333334e-06,
      "loss": 1.9667,
      "step": 199
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.644808292388916,
      "learning_rate": 4.166666666666667e-06,
      "loss": 1.9413,
      "step": 200
    },
    {
      "epoch": 0.41875,
      "grad_norm": 1.6526079177856445,
      "learning_rate": 4.1875e-06,
      "loss": 1.9178,
      "step": 201
    },
    {
      "epoch": 0.42083333333333334,
      "grad_norm": 2.3051586151123047,
      "learning_rate": 4.208333333333333e-06,
      "loss": 2.009,
      "step": 202
    },
    {
      "epoch": 0.42291666666666666,
      "grad_norm": 1.7299785614013672,
      "learning_rate": 4.229166666666667e-06,
      "loss": 1.8726,
      "step": 203
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.9178578853607178,
      "learning_rate": 4.25e-06,
      "loss": 1.9473,
      "step": 204
    },
    {
      "epoch": 0.4270833333333333,
      "grad_norm": 1.646432876586914,
      "learning_rate": 4.270833333333333e-06,
      "loss": 1.9678,
      "step": 205
    },
    {
      "epoch": 0.42916666666666664,
      "grad_norm": 2.21234130859375,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 1.8912,
      "step": 206
    },
    {
      "epoch": 0.43125,
      "grad_norm": 1.5586419105529785,
      "learning_rate": 4.312500000000001e-06,
      "loss": 1.9335,
      "step": 207
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.7768386602401733,
      "learning_rate": 4.333333333333334e-06,
      "loss": 1.9814,
      "step": 208
    },
    {
      "epoch": 0.4354166666666667,
      "grad_norm": 2.3278260231018066,
      "learning_rate": 4.354166666666667e-06,
      "loss": 1.9237,
      "step": 209
    },
    {
      "epoch": 0.4375,
      "grad_norm": 1.7317548990249634,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 1.9659,
      "step": 210
    },
    {
      "epoch": 0.4395833333333333,
      "grad_norm": 1.8235660791397095,
      "learning_rate": 4.395833333333334e-06,
      "loss": 1.9132,
      "step": 211
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.8113242387771606,
      "learning_rate": 4.416666666666667e-06,
      "loss": 1.9345,
      "step": 212
    },
    {
      "epoch": 0.44375,
      "grad_norm": 2.174243450164795,
      "learning_rate": 4.4375e-06,
      "loss": 1.9051,
      "step": 213
    },
    {
      "epoch": 0.44583333333333336,
      "grad_norm": 1.8846261501312256,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 1.9367,
      "step": 214
    },
    {
      "epoch": 0.4479166666666667,
      "grad_norm": 2.396265983581543,
      "learning_rate": 4.479166666666667e-06,
      "loss": 1.9236,
      "step": 215
    },
    {
      "epoch": 0.45,
      "grad_norm": 2.9885406494140625,
      "learning_rate": 4.5e-06,
      "loss": 1.8675,
      "step": 216
    },
    {
      "epoch": 0.45208333333333334,
      "grad_norm": 1.5922093391418457,
      "learning_rate": 4.520833333333333e-06,
      "loss": 1.9758,
      "step": 217
    },
    {
      "epoch": 0.45416666666666666,
      "grad_norm": 1.723069190979004,
      "learning_rate": 4.541666666666667e-06,
      "loss": 1.9236,
      "step": 218
    },
    {
      "epoch": 0.45625,
      "grad_norm": 2.1999847888946533,
      "learning_rate": 4.5625e-06,
      "loss": 1.8921,
      "step": 219
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 2.267242193222046,
      "learning_rate": 4.583333333333333e-06,
      "loss": 1.8975,
      "step": 220
    },
    {
      "epoch": 0.46041666666666664,
      "grad_norm": 2.231661081314087,
      "learning_rate": 4.6041666666666665e-06,
      "loss": 1.8952,
      "step": 221
    },
    {
      "epoch": 0.4625,
      "grad_norm": 1.757491946220398,
      "learning_rate": 4.625000000000001e-06,
      "loss": 1.8975,
      "step": 222
    },
    {
      "epoch": 0.46458333333333335,
      "grad_norm": 1.862398624420166,
      "learning_rate": 4.645833333333334e-06,
      "loss": 1.897,
      "step": 223
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.939106822013855,
      "learning_rate": 4.666666666666667e-06,
      "loss": 1.9671,
      "step": 224
    },
    {
      "epoch": 0.46875,
      "grad_norm": 2.247540235519409,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 2.0449,
      "step": 225
    },
    {
      "epoch": 0.4708333333333333,
      "grad_norm": 3.573150157928467,
      "learning_rate": 4.708333333333334e-06,
      "loss": 2.0275,
      "step": 226
    },
    {
      "epoch": 0.47291666666666665,
      "grad_norm": 1.8961498737335205,
      "learning_rate": 4.729166666666667e-06,
      "loss": 1.9343,
      "step": 227
    },
    {
      "epoch": 0.475,
      "grad_norm": 2.6629021167755127,
      "learning_rate": 4.75e-06,
      "loss": 1.9605,
      "step": 228
    },
    {
      "epoch": 0.47708333333333336,
      "grad_norm": 1.9299182891845703,
      "learning_rate": 4.770833333333334e-06,
      "loss": 2.0027,
      "step": 229
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 2.2619400024414062,
      "learning_rate": 4.791666666666668e-06,
      "loss": 1.9978,
      "step": 230
    },
    {
      "epoch": 0.48125,
      "grad_norm": 1.6843916177749634,
      "learning_rate": 4.8125e-06,
      "loss": 1.9494,
      "step": 231
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.97939133644104,
      "learning_rate": 4.833333333333333e-06,
      "loss": 1.9049,
      "step": 232
    },
    {
      "epoch": 0.48541666666666666,
      "grad_norm": 2.605259656906128,
      "learning_rate": 4.854166666666667e-06,
      "loss": 1.8513,
      "step": 233
    },
    {
      "epoch": 0.4875,
      "grad_norm": 2.428267478942871,
      "learning_rate": 4.875e-06,
      "loss": 1.8604,
      "step": 234
    },
    {
      "epoch": 0.4895833333333333,
      "grad_norm": 2.2064976692199707,
      "learning_rate": 4.895833333333333e-06,
      "loss": 1.9074,
      "step": 235
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 1.8235766887664795,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 1.9042,
      "step": 236
    },
    {
      "epoch": 0.49375,
      "grad_norm": 1.7362613677978516,
      "learning_rate": 4.937500000000001e-06,
      "loss": 1.9731,
      "step": 237
    },
    {
      "epoch": 0.49583333333333335,
      "grad_norm": 1.9698946475982666,
      "learning_rate": 4.958333333333334e-06,
      "loss": 1.9606,
      "step": 238
    },
    {
      "epoch": 0.4979166666666667,
      "grad_norm": 3.645357847213745,
      "learning_rate": 4.979166666666667e-06,
      "loss": 1.8531,
      "step": 239
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.854358196258545,
      "learning_rate": 5e-06,
      "loss": 1.9671,
      "step": 240
    },
    {
      "epoch": 0.5020833333333333,
      "grad_norm": 1.843748927116394,
      "learning_rate": 5.020833333333334e-06,
      "loss": 1.9645,
      "step": 241
    },
    {
      "epoch": 0.5041666666666667,
      "grad_norm": 2.6966536045074463,
      "learning_rate": 5.041666666666667e-06,
      "loss": 1.9536,
      "step": 242
    },
    {
      "epoch": 0.50625,
      "grad_norm": 3.1641457080841064,
      "learning_rate": 5.0625e-06,
      "loss": 2.0146,
      "step": 243
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 2.738107919692993,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 1.9315,
      "step": 244
    },
    {
      "epoch": 0.5104166666666666,
      "grad_norm": 2.788121223449707,
      "learning_rate": 5.104166666666667e-06,
      "loss": 1.9572,
      "step": 245
    },
    {
      "epoch": 0.5125,
      "grad_norm": 2.3148388862609863,
      "learning_rate": 5.125e-06,
      "loss": 1.8307,
      "step": 246
    },
    {
      "epoch": 0.5145833333333333,
      "grad_norm": 2.8322653770446777,
      "learning_rate": 5.145833333333333e-06,
      "loss": 1.9568,
      "step": 247
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.8942432403564453,
      "learning_rate": 5.1666666666666675e-06,
      "loss": 1.9898,
      "step": 248
    },
    {
      "epoch": 0.51875,
      "grad_norm": 1.8595223426818848,
      "learning_rate": 5.187500000000001e-06,
      "loss": 1.894,
      "step": 249
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 2.410299301147461,
      "learning_rate": 5.208333333333334e-06,
      "loss": 1.9631,
      "step": 250
    },
    {
      "epoch": 0.5229166666666667,
      "grad_norm": 3.0616726875305176,
      "learning_rate": 5.229166666666667e-06,
      "loss": 1.9173,
      "step": 251
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.9040396213531494,
      "learning_rate": 5.2500000000000006e-06,
      "loss": 1.8926,
      "step": 252
    },
    {
      "epoch": 0.5270833333333333,
      "grad_norm": 1.8419060707092285,
      "learning_rate": 5.270833333333334e-06,
      "loss": 1.9029,
      "step": 253
    },
    {
      "epoch": 0.5291666666666667,
      "grad_norm": 3.389883279800415,
      "learning_rate": 5.291666666666667e-06,
      "loss": 2.0174,
      "step": 254
    },
    {
      "epoch": 0.53125,
      "grad_norm": 1.9361604452133179,
      "learning_rate": 5.3125e-06,
      "loss": 1.8934,
      "step": 255
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4250881671905518,
      "learning_rate": 5.333333333333334e-06,
      "loss": 2.0199,
      "step": 256
    },
    {
      "epoch": 0.5354166666666667,
      "grad_norm": 3.165557622909546,
      "learning_rate": 5.354166666666667e-06,
      "loss": 1.9131,
      "step": 257
    },
    {
      "epoch": 0.5375,
      "grad_norm": 2.1826205253601074,
      "learning_rate": 5.375e-06,
      "loss": 1.8948,
      "step": 258
    },
    {
      "epoch": 0.5395833333333333,
      "grad_norm": 2.0835843086242676,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 1.8973,
      "step": 259
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 2.1703131198883057,
      "learning_rate": 5.416666666666667e-06,
      "loss": 1.8461,
      "step": 260
    },
    {
      "epoch": 0.54375,
      "grad_norm": 2.299730062484741,
      "learning_rate": 5.4375e-06,
      "loss": 2.0039,
      "step": 261
    },
    {
      "epoch": 0.5458333333333333,
      "grad_norm": 2.1289424896240234,
      "learning_rate": 5.458333333333333e-06,
      "loss": 1.9709,
      "step": 262
    },
    {
      "epoch": 0.5479166666666667,
      "grad_norm": 3.1521549224853516,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 1.8674,
      "step": 263
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9204808473587036,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.9357,
      "step": 264
    },
    {
      "epoch": 0.5520833333333334,
      "grad_norm": 1.8873332738876343,
      "learning_rate": 5.520833333333334e-06,
      "loss": 1.9051,
      "step": 265
    },
    {
      "epoch": 0.5541666666666667,
      "grad_norm": 3.155243158340454,
      "learning_rate": 5.541666666666667e-06,
      "loss": 2.0185,
      "step": 266
    },
    {
      "epoch": 0.55625,
      "grad_norm": 2.3129990100860596,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 1.8596,
      "step": 267
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 2.3421220779418945,
      "learning_rate": 5.583333333333334e-06,
      "loss": 2.0428,
      "step": 268
    },
    {
      "epoch": 0.5604166666666667,
      "grad_norm": 3.07894229888916,
      "learning_rate": 5.604166666666667e-06,
      "loss": 1.8312,
      "step": 269
    },
    {
      "epoch": 0.5625,
      "grad_norm": 2.0167429447174072,
      "learning_rate": 5.625e-06,
      "loss": 1.919,
      "step": 270
    },
    {
      "epoch": 0.5645833333333333,
      "grad_norm": 2.1795730590820312,
      "learning_rate": 5.645833333333334e-06,
      "loss": 1.9425,
      "step": 271
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 3.5813472270965576,
      "learning_rate": 5.666666666666667e-06,
      "loss": 2.0585,
      "step": 272
    },
    {
      "epoch": 0.56875,
      "grad_norm": 2.0160906314849854,
      "learning_rate": 5.6875e-06,
      "loss": 1.9892,
      "step": 273
    },
    {
      "epoch": 0.5708333333333333,
      "grad_norm": 2.624969005584717,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 1.8676,
      "step": 274
    },
    {
      "epoch": 0.5729166666666666,
      "grad_norm": 2.2965028285980225,
      "learning_rate": 5.729166666666667e-06,
      "loss": 1.8911,
      "step": 275
    },
    {
      "epoch": 0.575,
      "grad_norm": 3.5277669429779053,
      "learning_rate": 5.75e-06,
      "loss": 2.0142,
      "step": 276
    },
    {
      "epoch": 0.5770833333333333,
      "grad_norm": 1.9311413764953613,
      "learning_rate": 5.770833333333333e-06,
      "loss": 1.9065,
      "step": 277
    },
    {
      "epoch": 0.5791666666666667,
      "grad_norm": 2.0657958984375,
      "learning_rate": 5.791666666666667e-06,
      "loss": 1.9246,
      "step": 278
    },
    {
      "epoch": 0.58125,
      "grad_norm": 2.607872247695923,
      "learning_rate": 5.812500000000001e-06,
      "loss": 2.0187,
      "step": 279
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 2.5093233585357666,
      "learning_rate": 5.833333333333334e-06,
      "loss": 1.8347,
      "step": 280
    },
    {
      "epoch": 0.5854166666666667,
      "grad_norm": 1.898473858833313,
      "learning_rate": 5.854166666666667e-06,
      "loss": 1.9006,
      "step": 281
    },
    {
      "epoch": 0.5875,
      "grad_norm": 1.9381897449493408,
      "learning_rate": 5.8750000000000005e-06,
      "loss": 1.9168,
      "step": 282
    },
    {
      "epoch": 0.5895833333333333,
      "grad_norm": 2.9250664710998535,
      "learning_rate": 5.895833333333334e-06,
      "loss": 2.0804,
      "step": 283
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 2.4667484760284424,
      "learning_rate": 5.916666666666667e-06,
      "loss": 1.9591,
      "step": 284
    },
    {
      "epoch": 0.59375,
      "grad_norm": 3.518202543258667,
      "learning_rate": 5.9375e-06,
      "loss": 2.0663,
      "step": 285
    },
    {
      "epoch": 0.5958333333333333,
      "grad_norm": 2.2965872287750244,
      "learning_rate": 5.958333333333334e-06,
      "loss": 1.9379,
      "step": 286
    },
    {
      "epoch": 0.5979166666666667,
      "grad_norm": 2.312025547027588,
      "learning_rate": 5.979166666666667e-06,
      "loss": 1.8913,
      "step": 287
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.984014630317688,
      "learning_rate": 6e-06,
      "loss": 1.9283,
      "step": 288
    },
    {
      "epoch": 0.6020833333333333,
      "grad_norm": 2.3506760597229004,
      "learning_rate": 6.0208333333333334e-06,
      "loss": 1.9983,
      "step": 289
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 3.4718809127807617,
      "learning_rate": 6.041666666666667e-06,
      "loss": 1.9049,
      "step": 290
    },
    {
      "epoch": 0.60625,
      "grad_norm": 2.399280309677124,
      "learning_rate": 6.0625e-06,
      "loss": 1.8421,
      "step": 291
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 2.5102994441986084,
      "learning_rate": 6.083333333333333e-06,
      "loss": 1.9692,
      "step": 292
    },
    {
      "epoch": 0.6104166666666667,
      "grad_norm": 2.54512619972229,
      "learning_rate": 6.104166666666667e-06,
      "loss": 1.8346,
      "step": 293
    },
    {
      "epoch": 0.6125,
      "grad_norm": 1.888302206993103,
      "learning_rate": 6.125000000000001e-06,
      "loss": 1.8693,
      "step": 294
    },
    {
      "epoch": 0.6145833333333334,
      "grad_norm": 1.730652093887329,
      "learning_rate": 6.145833333333334e-06,
      "loss": 1.9487,
      "step": 295
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 3.913667678833008,
      "learning_rate": 6.166666666666667e-06,
      "loss": 2.0249,
      "step": 296
    },
    {
      "epoch": 0.61875,
      "grad_norm": 2.5604746341705322,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 1.9091,
      "step": 297
    },
    {
      "epoch": 0.6208333333333333,
      "grad_norm": 2.047619342803955,
      "learning_rate": 6.208333333333334e-06,
      "loss": 1.8933,
      "step": 298
    },
    {
      "epoch": 0.6229166666666667,
      "grad_norm": 2.2678444385528564,
      "learning_rate": 6.229166666666667e-06,
      "loss": 1.8614,
      "step": 299
    },
    {
      "epoch": 0.625,
      "grad_norm": 2.5614569187164307,
      "learning_rate": 6.25e-06,
      "loss": 2.0174,
      "step": 300
    },
    {
      "epoch": 0.6270833333333333,
      "grad_norm": 2.5827088356018066,
      "learning_rate": 6.2708333333333336e-06,
      "loss": 1.8997,
      "step": 301
    },
    {
      "epoch": 0.6291666666666667,
      "grad_norm": 2.1900813579559326,
      "learning_rate": 6.291666666666667e-06,
      "loss": 1.8632,
      "step": 302
    },
    {
      "epoch": 0.63125,
      "grad_norm": 2.126892566680908,
      "learning_rate": 6.3125e-06,
      "loss": 1.9669,
      "step": 303
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.3701188564300537,
      "learning_rate": 6.333333333333333e-06,
      "loss": 1.8642,
      "step": 304
    },
    {
      "epoch": 0.6354166666666666,
      "grad_norm": 2.812835931777954,
      "learning_rate": 6.354166666666667e-06,
      "loss": 1.8609,
      "step": 305
    },
    {
      "epoch": 0.6375,
      "grad_norm": 2.662055492401123,
      "learning_rate": 6.375e-06,
      "loss": 2.0148,
      "step": 306
    },
    {
      "epoch": 0.6395833333333333,
      "grad_norm": 2.8154637813568115,
      "learning_rate": 6.395833333333333e-06,
      "loss": 1.8095,
      "step": 307
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 2.750066041946411,
      "learning_rate": 6.416666666666667e-06,
      "loss": 1.9756,
      "step": 308
    },
    {
      "epoch": 0.64375,
      "grad_norm": 2.7688097953796387,
      "learning_rate": 6.437500000000001e-06,
      "loss": 2.0561,
      "step": 309
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 3.8024823665618896,
      "learning_rate": 6.458333333333334e-06,
      "loss": 1.8525,
      "step": 310
    },
    {
      "epoch": 0.6479166666666667,
      "grad_norm": 2.5757503509521484,
      "learning_rate": 6.479166666666667e-06,
      "loss": 1.882,
      "step": 311
    },
    {
      "epoch": 0.65,
      "grad_norm": 2.441953659057617,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.9922,
      "step": 312
    },
    {
      "epoch": 0.6520833333333333,
      "grad_norm": 2.293457269668579,
      "learning_rate": 6.520833333333334e-06,
      "loss": 1.8927,
      "step": 313
    },
    {
      "epoch": 0.6541666666666667,
      "grad_norm": 3.0060155391693115,
      "learning_rate": 6.541666666666667e-06,
      "loss": 1.9956,
      "step": 314
    },
    {
      "epoch": 0.65625,
      "grad_norm": 2.415240526199341,
      "learning_rate": 6.5625e-06,
      "loss": 1.9537,
      "step": 315
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 2.714740514755249,
      "learning_rate": 6.5833333333333335e-06,
      "loss": 2.0724,
      "step": 316
    },
    {
      "epoch": 0.6604166666666667,
      "grad_norm": 2.6796047687530518,
      "learning_rate": 6.604166666666667e-06,
      "loss": 1.8765,
      "step": 317
    },
    {
      "epoch": 0.6625,
      "grad_norm": 2.6019749641418457,
      "learning_rate": 6.625e-06,
      "loss": 2.0555,
      "step": 318
    },
    {
      "epoch": 0.6645833333333333,
      "grad_norm": 2.2928247451782227,
      "learning_rate": 6.645833333333333e-06,
      "loss": 1.912,
      "step": 319
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.789982557296753,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.0156,
      "step": 320
    },
    {
      "epoch": 0.66875,
      "grad_norm": 2.2915830612182617,
      "learning_rate": 6.6875e-06,
      "loss": 1.896,
      "step": 321
    },
    {
      "epoch": 0.6708333333333333,
      "grad_norm": 2.2692737579345703,
      "learning_rate": 6.708333333333333e-06,
      "loss": 1.9683,
      "step": 322
    },
    {
      "epoch": 0.6729166666666667,
      "grad_norm": 2.0629639625549316,
      "learning_rate": 6.729166666666667e-06,
      "loss": 1.9431,
      "step": 323
    },
    {
      "epoch": 0.675,
      "grad_norm": 2.520183801651001,
      "learning_rate": 6.750000000000001e-06,
      "loss": 2.0044,
      "step": 324
    },
    {
      "epoch": 0.6770833333333334,
      "grad_norm": 2.4724886417388916,
      "learning_rate": 6.770833333333334e-06,
      "loss": 2.0033,
      "step": 325
    },
    {
      "epoch": 0.6791666666666667,
      "grad_norm": 2.4983415603637695,
      "learning_rate": 6.791666666666667e-06,
      "loss": 1.9768,
      "step": 326
    },
    {
      "epoch": 0.68125,
      "grad_norm": 2.4048242568969727,
      "learning_rate": 6.8125e-06,
      "loss": 2.0085,
      "step": 327
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 2.2547218799591064,
      "learning_rate": 6.833333333333334e-06,
      "loss": 1.9401,
      "step": 328
    },
    {
      "epoch": 0.6854166666666667,
      "grad_norm": 2.3316361904144287,
      "learning_rate": 6.854166666666667e-06,
      "loss": 1.9649,
      "step": 329
    },
    {
      "epoch": 0.6875,
      "grad_norm": 1.9652899503707886,
      "learning_rate": 6.875e-06,
      "loss": 1.9866,
      "step": 330
    },
    {
      "epoch": 0.6895833333333333,
      "grad_norm": 4.378288269042969,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 1.7942,
      "step": 331
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 2.1427857875823975,
      "learning_rate": 6.916666666666667e-06,
      "loss": 1.9143,
      "step": 332
    },
    {
      "epoch": 0.69375,
      "grad_norm": 2.119568109512329,
      "learning_rate": 6.9375e-06,
      "loss": 1.9713,
      "step": 333
    },
    {
      "epoch": 0.6958333333333333,
      "grad_norm": 1.9810140132904053,
      "learning_rate": 6.958333333333333e-06,
      "loss": 1.9442,
      "step": 334
    },
    {
      "epoch": 0.6979166666666666,
      "grad_norm": 2.1782877445220947,
      "learning_rate": 6.979166666666667e-06,
      "loss": 1.8447,
      "step": 335
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.250786066055298,
      "learning_rate": 7e-06,
      "loss": 1.9839,
      "step": 336
    },
    {
      "epoch": 0.7020833333333333,
      "grad_norm": 3.692650556564331,
      "learning_rate": 7.020833333333333e-06,
      "loss": 2.0693,
      "step": 337
    },
    {
      "epoch": 0.7041666666666667,
      "grad_norm": 2.408928155899048,
      "learning_rate": 7.041666666666668e-06,
      "loss": 1.9713,
      "step": 338
    },
    {
      "epoch": 0.70625,
      "grad_norm": 3.200366258621216,
      "learning_rate": 7.062500000000001e-06,
      "loss": 1.9939,
      "step": 339
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 2.39514422416687,
      "learning_rate": 7.083333333333335e-06,
      "loss": 1.9996,
      "step": 340
    },
    {
      "epoch": 0.7104166666666667,
      "grad_norm": 2.2330617904663086,
      "learning_rate": 7.104166666666668e-06,
      "loss": 1.9099,
      "step": 341
    },
    {
      "epoch": 0.7125,
      "grad_norm": 2.2347140312194824,
      "learning_rate": 7.125e-06,
      "loss": 1.8596,
      "step": 342
    },
    {
      "epoch": 0.7145833333333333,
      "grad_norm": 1.836554765701294,
      "learning_rate": 7.145833333333334e-06,
      "loss": 1.9187,
      "step": 343
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.872352123260498,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.9261,
      "step": 344
    },
    {
      "epoch": 0.71875,
      "grad_norm": 1.978493332862854,
      "learning_rate": 7.1875e-06,
      "loss": 1.9483,
      "step": 345
    },
    {
      "epoch": 0.7208333333333333,
      "grad_norm": 1.9424595832824707,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 1.9952,
      "step": 346
    },
    {
      "epoch": 0.7229166666666667,
      "grad_norm": 1.878655195236206,
      "learning_rate": 7.229166666666667e-06,
      "loss": 1.9378,
      "step": 347
    },
    {
      "epoch": 0.725,
      "grad_norm": 2.1839728355407715,
      "learning_rate": 7.25e-06,
      "loss": 1.8907,
      "step": 348
    },
    {
      "epoch": 0.7270833333333333,
      "grad_norm": 2.4479727745056152,
      "learning_rate": 7.270833333333333e-06,
      "loss": 1.9555,
      "step": 349
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 2.0386245250701904,
      "learning_rate": 7.291666666666667e-06,
      "loss": 1.971,
      "step": 350
    },
    {
      "epoch": 0.73125,
      "grad_norm": 2.078732490539551,
      "learning_rate": 7.3125e-06,
      "loss": 1.8773,
      "step": 351
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 3.3587279319763184,
      "learning_rate": 7.333333333333333e-06,
      "loss": 1.8218,
      "step": 352
    },
    {
      "epoch": 0.7354166666666667,
      "grad_norm": 1.7917520999908447,
      "learning_rate": 7.354166666666668e-06,
      "loss": 1.956,
      "step": 353
    },
    {
      "epoch": 0.7375,
      "grad_norm": 1.944817304611206,
      "learning_rate": 7.375000000000001e-06,
      "loss": 2.0219,
      "step": 354
    },
    {
      "epoch": 0.7395833333333334,
      "grad_norm": 2.1350677013397217,
      "learning_rate": 7.395833333333335e-06,
      "loss": 1.8951,
      "step": 355
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.5405153036117554,
      "learning_rate": 7.416666666666668e-06,
      "loss": 1.9479,
      "step": 356
    },
    {
      "epoch": 0.74375,
      "grad_norm": 3.1375017166137695,
      "learning_rate": 7.437500000000001e-06,
      "loss": 2.0578,
      "step": 357
    },
    {
      "epoch": 0.7458333333333333,
      "grad_norm": 2.875251293182373,
      "learning_rate": 7.4583333333333345e-06,
      "loss": 1.8975,
      "step": 358
    },
    {
      "epoch": 0.7479166666666667,
      "grad_norm": 2.002790689468384,
      "learning_rate": 7.479166666666668e-06,
      "loss": 1.8771,
      "step": 359
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.062234401702881,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.9183,
      "step": 360
    },
    {
      "epoch": 0.7520833333333333,
      "grad_norm": 1.4805147647857666,
      "learning_rate": 7.5208333333333335e-06,
      "loss": 1.9061,
      "step": 361
    },
    {
      "epoch": 0.7541666666666667,
      "grad_norm": 1.8906422853469849,
      "learning_rate": 7.541666666666667e-06,
      "loss": 1.8733,
      "step": 362
    },
    {
      "epoch": 0.75625,
      "grad_norm": 1.643729567527771,
      "learning_rate": 7.5625e-06,
      "loss": 1.9214,
      "step": 363
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 1.7192820310592651,
      "learning_rate": 7.583333333333333e-06,
      "loss": 1.9548,
      "step": 364
    },
    {
      "epoch": 0.7604166666666666,
      "grad_norm": 2.062532663345337,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 1.8739,
      "step": 365
    },
    {
      "epoch": 0.7625,
      "grad_norm": 2.0105226039886475,
      "learning_rate": 7.625e-06,
      "loss": 1.9435,
      "step": 366
    },
    {
      "epoch": 0.7645833333333333,
      "grad_norm": 3.4199979305267334,
      "learning_rate": 7.645833333333334e-06,
      "loss": 1.8157,
      "step": 367
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 2.197448492050171,
      "learning_rate": 7.666666666666667e-06,
      "loss": 1.9233,
      "step": 368
    },
    {
      "epoch": 0.76875,
      "grad_norm": 1.9707907438278198,
      "learning_rate": 7.6875e-06,
      "loss": 1.983,
      "step": 369
    },
    {
      "epoch": 0.7708333333333334,
      "grad_norm": 1.8391448259353638,
      "learning_rate": 7.708333333333334e-06,
      "loss": 1.92,
      "step": 370
    },
    {
      "epoch": 0.7729166666666667,
      "grad_norm": 1.7130945920944214,
      "learning_rate": 7.729166666666667e-06,
      "loss": 1.9186,
      "step": 371
    },
    {
      "epoch": 0.775,
      "grad_norm": 3.092482089996338,
      "learning_rate": 7.75e-06,
      "loss": 1.9811,
      "step": 372
    },
    {
      "epoch": 0.7770833333333333,
      "grad_norm": 2.120124101638794,
      "learning_rate": 7.770833333333334e-06,
      "loss": 1.9811,
      "step": 373
    },
    {
      "epoch": 0.7791666666666667,
      "grad_norm": 2.4088916778564453,
      "learning_rate": 7.791666666666667e-06,
      "loss": 1.9767,
      "step": 374
    },
    {
      "epoch": 0.78125,
      "grad_norm": 2.309558153152466,
      "learning_rate": 7.8125e-06,
      "loss": 1.8216,
      "step": 375
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 2.0857598781585693,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.9516,
      "step": 376
    },
    {
      "epoch": 0.7854166666666667,
      "grad_norm": 1.9991323947906494,
      "learning_rate": 7.854166666666667e-06,
      "loss": 1.9264,
      "step": 377
    },
    {
      "epoch": 0.7875,
      "grad_norm": 2.026289939880371,
      "learning_rate": 7.875e-06,
      "loss": 1.8872,
      "step": 378
    },
    {
      "epoch": 0.7895833333333333,
      "grad_norm": 2.5416059494018555,
      "learning_rate": 7.895833333333333e-06,
      "loss": 1.8943,
      "step": 379
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 2.088346481323242,
      "learning_rate": 7.916666666666667e-06,
      "loss": 1.8696,
      "step": 380
    },
    {
      "epoch": 0.79375,
      "grad_norm": 1.9497150182724,
      "learning_rate": 7.9375e-06,
      "loss": 1.967,
      "step": 381
    },
    {
      "epoch": 0.7958333333333333,
      "grad_norm": 1.984463095664978,
      "learning_rate": 7.958333333333333e-06,
      "loss": 1.9983,
      "step": 382
    },
    {
      "epoch": 0.7979166666666667,
      "grad_norm": 2.5847599506378174,
      "learning_rate": 7.979166666666668e-06,
      "loss": 2.0365,
      "step": 383
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.0131874084472656,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9248,
      "step": 384
    },
    {
      "epoch": 0.8020833333333334,
      "grad_norm": 2.3451294898986816,
      "learning_rate": 8.020833333333335e-06,
      "loss": 1.9017,
      "step": 385
    },
    {
      "epoch": 0.8041666666666667,
      "grad_norm": 2.1853044033050537,
      "learning_rate": 8.041666666666668e-06,
      "loss": 2.0141,
      "step": 386
    },
    {
      "epoch": 0.80625,
      "grad_norm": 3.125521421432495,
      "learning_rate": 8.062500000000001e-06,
      "loss": 1.9858,
      "step": 387
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 2.9561851024627686,
      "learning_rate": 8.083333333333334e-06,
      "loss": 1.9553,
      "step": 388
    },
    {
      "epoch": 0.8104166666666667,
      "grad_norm": 2.2265894412994385,
      "learning_rate": 8.104166666666668e-06,
      "loss": 1.8757,
      "step": 389
    },
    {
      "epoch": 0.8125,
      "grad_norm": 3.025200605392456,
      "learning_rate": 8.125000000000001e-06,
      "loss": 1.9598,
      "step": 390
    },
    {
      "epoch": 0.8145833333333333,
      "grad_norm": 3.588319778442383,
      "learning_rate": 8.145833333333334e-06,
      "loss": 1.8099,
      "step": 391
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.780010461807251,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.933,
      "step": 392
    },
    {
      "epoch": 0.81875,
      "grad_norm": 2.4813098907470703,
      "learning_rate": 8.1875e-06,
      "loss": 1.8739,
      "step": 393
    },
    {
      "epoch": 0.8208333333333333,
      "grad_norm": 2.6654679775238037,
      "learning_rate": 8.208333333333334e-06,
      "loss": 1.8741,
      "step": 394
    },
    {
      "epoch": 0.8229166666666666,
      "grad_norm": 2.364204168319702,
      "learning_rate": 8.229166666666667e-06,
      "loss": 1.9165,
      "step": 395
    },
    {
      "epoch": 0.825,
      "grad_norm": 2.3084006309509277,
      "learning_rate": 8.25e-06,
      "loss": 1.9259,
      "step": 396
    },
    {
      "epoch": 0.8270833333333333,
      "grad_norm": 2.3557353019714355,
      "learning_rate": 8.270833333333334e-06,
      "loss": 2.0113,
      "step": 397
    },
    {
      "epoch": 0.8291666666666667,
      "grad_norm": 3.0592455863952637,
      "learning_rate": 8.291666666666667e-06,
      "loss": 1.9614,
      "step": 398
    },
    {
      "epoch": 0.83125,
      "grad_norm": 2.5061662197113037,
      "learning_rate": 8.3125e-06,
      "loss": 1.8544,
      "step": 399
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.4533064365386963,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.9939,
      "step": 400
    },
    {
      "epoch": 0.8354166666666667,
      "grad_norm": 1.9671192169189453,
      "learning_rate": 8.354166666666667e-06,
      "loss": 1.9293,
      "step": 401
    },
    {
      "epoch": 0.8375,
      "grad_norm": 2.297431707382202,
      "learning_rate": 8.375e-06,
      "loss": 1.8417,
      "step": 402
    },
    {
      "epoch": 0.8395833333333333,
      "grad_norm": 2.3937785625457764,
      "learning_rate": 8.395833333333334e-06,
      "loss": 1.8477,
      "step": 403
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 2.198958158493042,
      "learning_rate": 8.416666666666667e-06,
      "loss": 1.8807,
      "step": 404
    },
    {
      "epoch": 0.84375,
      "grad_norm": 3.846808671951294,
      "learning_rate": 8.4375e-06,
      "loss": 1.983,
      "step": 405
    },
    {
      "epoch": 0.8458333333333333,
      "grad_norm": 2.3106722831726074,
      "learning_rate": 8.458333333333333e-06,
      "loss": 1.9765,
      "step": 406
    },
    {
      "epoch": 0.8479166666666667,
      "grad_norm": 3.2451775074005127,
      "learning_rate": 8.479166666666667e-06,
      "loss": 1.9076,
      "step": 407
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.9123587608337402,
      "learning_rate": 8.5e-06,
      "loss": 1.9559,
      "step": 408
    },
    {
      "epoch": 0.8520833333333333,
      "grad_norm": 1.9071195125579834,
      "learning_rate": 8.520833333333333e-06,
      "loss": 1.978,
      "step": 409
    },
    {
      "epoch": 0.8541666666666666,
      "grad_norm": 1.9764184951782227,
      "learning_rate": 8.541666666666666e-06,
      "loss": 1.9385,
      "step": 410
    },
    {
      "epoch": 0.85625,
      "grad_norm": 2.1670868396759033,
      "learning_rate": 8.5625e-06,
      "loss": 1.8938,
      "step": 411
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 3.671736478805542,
      "learning_rate": 8.583333333333333e-06,
      "loss": 1.8379,
      "step": 412
    },
    {
      "epoch": 0.8604166666666667,
      "grad_norm": 1.7840386629104614,
      "learning_rate": 8.604166666666668e-06,
      "loss": 1.9278,
      "step": 413
    },
    {
      "epoch": 0.8625,
      "grad_norm": 1.9531755447387695,
      "learning_rate": 8.625000000000001e-06,
      "loss": 1.9944,
      "step": 414
    },
    {
      "epoch": 0.8645833333333334,
      "grad_norm": 2.387019634246826,
      "learning_rate": 8.645833333333335e-06,
      "loss": 1.8599,
      "step": 415
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.8335912227630615,
      "learning_rate": 8.666666666666668e-06,
      "loss": 1.9212,
      "step": 416
    },
    {
      "epoch": 0.86875,
      "grad_norm": 1.778045892715454,
      "learning_rate": 8.687500000000001e-06,
      "loss": 1.9302,
      "step": 417
    },
    {
      "epoch": 0.8708333333333333,
      "grad_norm": 3.5218589305877686,
      "learning_rate": 8.708333333333334e-06,
      "loss": 1.8705,
      "step": 418
    },
    {
      "epoch": 0.8729166666666667,
      "grad_norm": 2.526855707168579,
      "learning_rate": 8.729166666666668e-06,
      "loss": 1.8373,
      "step": 419
    },
    {
      "epoch": 0.875,
      "grad_norm": 2.867689609527588,
      "learning_rate": 8.750000000000001e-06,
      "loss": 1.9301,
      "step": 420
    },
    {
      "epoch": 0.8770833333333333,
      "grad_norm": 2.332643508911133,
      "learning_rate": 8.770833333333334e-06,
      "loss": 1.9473,
      "step": 421
    },
    {
      "epoch": 0.8791666666666667,
      "grad_norm": 2.803417444229126,
      "learning_rate": 8.791666666666667e-06,
      "loss": 1.9552,
      "step": 422
    },
    {
      "epoch": 0.88125,
      "grad_norm": 2.178041696548462,
      "learning_rate": 8.8125e-06,
      "loss": 1.9089,
      "step": 423
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 2.666480541229248,
      "learning_rate": 8.833333333333334e-06,
      "loss": 1.8804,
      "step": 424
    },
    {
      "epoch": 0.8854166666666666,
      "grad_norm": 1.9336738586425781,
      "learning_rate": 8.854166666666667e-06,
      "loss": 1.9304,
      "step": 425
    },
    {
      "epoch": 0.8875,
      "grad_norm": 3.1005642414093018,
      "learning_rate": 8.875e-06,
      "loss": 1.882,
      "step": 426
    },
    {
      "epoch": 0.8895833333333333,
      "grad_norm": 2.1296818256378174,
      "learning_rate": 8.895833333333334e-06,
      "loss": 1.9388,
      "step": 427
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 2.2421162128448486,
      "learning_rate": 8.916666666666667e-06,
      "loss": 1.9368,
      "step": 428
    },
    {
      "epoch": 0.89375,
      "grad_norm": 3.2823519706726074,
      "learning_rate": 8.9375e-06,
      "loss": 1.9429,
      "step": 429
    },
    {
      "epoch": 0.8958333333333334,
      "grad_norm": 2.41601300239563,
      "learning_rate": 8.958333333333334e-06,
      "loss": 1.8791,
      "step": 430
    },
    {
      "epoch": 0.8979166666666667,
      "grad_norm": 2.786959171295166,
      "learning_rate": 8.979166666666667e-06,
      "loss": 1.8723,
      "step": 431
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.1478869915008545,
      "learning_rate": 9e-06,
      "loss": 1.9003,
      "step": 432
    },
    {
      "epoch": 0.9020833333333333,
      "grad_norm": 2.4939544200897217,
      "learning_rate": 9.020833333333334e-06,
      "loss": 1.9609,
      "step": 433
    },
    {
      "epoch": 0.9041666666666667,
      "grad_norm": 2.6142172813415527,
      "learning_rate": 9.041666666666667e-06,
      "loss": 2.0389,
      "step": 434
    },
    {
      "epoch": 0.90625,
      "grad_norm": 2.2247426509857178,
      "learning_rate": 9.0625e-06,
      "loss": 1.9377,
      "step": 435
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 3.047226667404175,
      "learning_rate": 9.083333333333333e-06,
      "loss": 1.8386,
      "step": 436
    },
    {
      "epoch": 0.9104166666666667,
      "grad_norm": 3.1124765872955322,
      "learning_rate": 9.104166666666667e-06,
      "loss": 1.9427,
      "step": 437
    },
    {
      "epoch": 0.9125,
      "grad_norm": 1.5476534366607666,
      "learning_rate": 9.125e-06,
      "loss": 1.9153,
      "step": 438
    },
    {
      "epoch": 0.9145833333333333,
      "grad_norm": 2.118931770324707,
      "learning_rate": 9.145833333333333e-06,
      "loss": 1.9562,
      "step": 439
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 3.014188289642334,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.8257,
      "step": 440
    },
    {
      "epoch": 0.91875,
      "grad_norm": 3.3745079040527344,
      "learning_rate": 9.1875e-06,
      "loss": 1.8304,
      "step": 441
    },
    {
      "epoch": 0.9208333333333333,
      "grad_norm": 3.294330596923828,
      "learning_rate": 9.208333333333333e-06,
      "loss": 1.9924,
      "step": 442
    },
    {
      "epoch": 0.9229166666666667,
      "grad_norm": 4.0276780128479,
      "learning_rate": 9.229166666666668e-06,
      "loss": 1.8786,
      "step": 443
    },
    {
      "epoch": 0.925,
      "grad_norm": 2.2814793586730957,
      "learning_rate": 9.250000000000001e-06,
      "loss": 1.939,
      "step": 444
    },
    {
      "epoch": 0.9270833333333334,
      "grad_norm": 2.984734535217285,
      "learning_rate": 9.270833333333334e-06,
      "loss": 1.8228,
      "step": 445
    },
    {
      "epoch": 0.9291666666666667,
      "grad_norm": 2.9730217456817627,
      "learning_rate": 9.291666666666668e-06,
      "loss": 1.8574,
      "step": 446
    },
    {
      "epoch": 0.93125,
      "grad_norm": 2.9107582569122314,
      "learning_rate": 9.312500000000001e-06,
      "loss": 1.9355,
      "step": 447
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.4466540813446045,
      "learning_rate": 9.333333333333334e-06,
      "loss": 1.9554,
      "step": 448
    },
    {
      "epoch": 0.9354166666666667,
      "grad_norm": 2.942814350128174,
      "learning_rate": 9.354166666666668e-06,
      "loss": 1.9445,
      "step": 449
    },
    {
      "epoch": 0.9375,
      "grad_norm": 2.9586877822875977,
      "learning_rate": 9.375000000000001e-06,
      "loss": 1.8989,
      "step": 450
    },
    {
      "epoch": 0.9395833333333333,
      "grad_norm": 2.871242046356201,
      "learning_rate": 9.395833333333334e-06,
      "loss": 1.8333,
      "step": 451
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 2.381859064102173,
      "learning_rate": 9.416666666666667e-06,
      "loss": 1.9094,
      "step": 452
    },
    {
      "epoch": 0.94375,
      "grad_norm": 2.848283529281616,
      "learning_rate": 9.4375e-06,
      "loss": 1.9008,
      "step": 453
    },
    {
      "epoch": 0.9458333333333333,
      "grad_norm": 3.2651169300079346,
      "learning_rate": 9.458333333333334e-06,
      "loss": 1.8077,
      "step": 454
    },
    {
      "epoch": 0.9479166666666666,
      "grad_norm": 2.916912794113159,
      "learning_rate": 9.479166666666667e-06,
      "loss": 1.9496,
      "step": 455
    },
    {
      "epoch": 0.95,
      "grad_norm": 3.27787184715271,
      "learning_rate": 9.5e-06,
      "loss": 1.8057,
      "step": 456
    },
    {
      "epoch": 0.9520833333333333,
      "grad_norm": 3.9563183784484863,
      "learning_rate": 9.520833333333334e-06,
      "loss": 1.804,
      "step": 457
    },
    {
      "epoch": 0.9541666666666667,
      "grad_norm": 3.298861026763916,
      "learning_rate": 9.541666666666669e-06,
      "loss": 1.8104,
      "step": 458
    },
    {
      "epoch": 0.95625,
      "grad_norm": 2.031202554702759,
      "learning_rate": 9.562500000000002e-06,
      "loss": 1.9307,
      "step": 459
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 3.558314561843872,
      "learning_rate": 9.583333333333335e-06,
      "loss": 2.0196,
      "step": 460
    },
    {
      "epoch": 0.9604166666666667,
      "grad_norm": 3.946951150894165,
      "learning_rate": 9.604166666666669e-06,
      "loss": 1.7687,
      "step": 461
    },
    {
      "epoch": 0.9625,
      "grad_norm": 4.042377948760986,
      "learning_rate": 9.625e-06,
      "loss": 1.8551,
      "step": 462
    },
    {
      "epoch": 0.9645833333333333,
      "grad_norm": 5.3062968254089355,
      "learning_rate": 9.645833333333333e-06,
      "loss": 1.6861,
      "step": 463
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 5.212461948394775,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.9386,
      "step": 464
    },
    {
      "epoch": 0.96875,
      "grad_norm": 2.7018473148345947,
      "learning_rate": 9.6875e-06,
      "loss": 1.8702,
      "step": 465
    },
    {
      "epoch": 0.9708333333333333,
      "grad_norm": 4.50298547744751,
      "learning_rate": 9.708333333333333e-06,
      "loss": 1.7242,
      "step": 466
    },
    {
      "epoch": 0.9729166666666667,
      "grad_norm": 3.341373920440674,
      "learning_rate": 9.729166666666667e-06,
      "loss": 1.8315,
      "step": 467
    },
    {
      "epoch": 0.975,
      "grad_norm": 4.250161170959473,
      "learning_rate": 9.75e-06,
      "loss": 1.7718,
      "step": 468
    },
    {
      "epoch": 0.9770833333333333,
      "grad_norm": 2.8732004165649414,
      "learning_rate": 9.770833333333333e-06,
      "loss": 1.9624,
      "step": 469
    },
    {
      "epoch": 0.9791666666666666,
      "grad_norm": 4.4935526847839355,
      "learning_rate": 9.791666666666666e-06,
      "loss": 1.7134,
      "step": 470
    },
    {
      "epoch": 0.98125,
      "grad_norm": 2.8006129264831543,
      "learning_rate": 9.8125e-06,
      "loss": 1.8971,
      "step": 471
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 4.489307403564453,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.901,
      "step": 472
    },
    {
      "epoch": 0.9854166666666667,
      "grad_norm": 3.762315034866333,
      "learning_rate": 9.854166666666668e-06,
      "loss": 1.8939,
      "step": 473
    },
    {
      "epoch": 0.9875,
      "grad_norm": 3.8374457359313965,
      "learning_rate": 9.875000000000001e-06,
      "loss": 1.7495,
      "step": 474
    },
    {
      "epoch": 0.9895833333333334,
      "grad_norm": 4.136640548706055,
      "learning_rate": 9.895833333333334e-06,
      "loss": 1.6997,
      "step": 475
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 6.038689613342285,
      "learning_rate": 9.916666666666668e-06,
      "loss": 1.9304,
      "step": 476
    },
    {
      "epoch": 0.99375,
      "grad_norm": 3.2505555152893066,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.8926,
      "step": 477
    },
    {
      "epoch": 0.9958333333333333,
      "grad_norm": 5.717676639556885,
      "learning_rate": 9.958333333333334e-06,
      "loss": 1.9017,
      "step": 478
    },
    {
      "epoch": 0.9979166666666667,
      "grad_norm": 5.054263114929199,
      "learning_rate": 9.979166666666668e-06,
      "loss": 1.669,
      "step": 479
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.6387014389038086,
      "learning_rate": 1e-05,
      "loss": 1.9121,
      "step": 480
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.3055555555555556,
      "eval_f1": 0.16282809420064323,
      "eval_loss": 1.81491219997406,
      "eval_runtime": 36.255,
      "eval_samples_per_second": 4.965,
      "eval_steps_per_second": 2.482,
      "step": 480
    }
  ],
  "logging_steps": 1,
  "max_steps": 4800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.614682087424e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
