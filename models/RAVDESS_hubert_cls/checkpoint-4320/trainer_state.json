{
  "best_metric": 0.5552577371509153,
  "best_model_checkpoint": "models\\RAVDESS_hubert_cls\\checkpoint-2880",
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 4320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020833333333333333,
      "grad_norm": 1.3656010627746582,
      "learning_rate": 2.0833333333333335e-08,
      "loss": 1.9327,
      "step": 1
    },
    {
      "epoch": 0.004166666666666667,
      "grad_norm": 1.4255727529525757,
      "learning_rate": 4.166666666666667e-08,
      "loss": 1.968,
      "step": 2
    },
    {
      "epoch": 0.00625,
      "grad_norm": 1.5010994672775269,
      "learning_rate": 6.250000000000001e-08,
      "loss": 1.9411,
      "step": 3
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 1.9118722677230835,
      "learning_rate": 8.333333333333334e-08,
      "loss": 1.9608,
      "step": 4
    },
    {
      "epoch": 0.010416666666666666,
      "grad_norm": 1.3321853876113892,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 1.9254,
      "step": 5
    },
    {
      "epoch": 0.0125,
      "grad_norm": 2.400676965713501,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 1.9986,
      "step": 6
    },
    {
      "epoch": 0.014583333333333334,
      "grad_norm": 1.4379546642303467,
      "learning_rate": 1.4583333333333335e-07,
      "loss": 1.9384,
      "step": 7
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.3329111337661743,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 1.9572,
      "step": 8
    },
    {
      "epoch": 0.01875,
      "grad_norm": 1.250981092453003,
      "learning_rate": 1.875e-07,
      "loss": 1.9607,
      "step": 9
    },
    {
      "epoch": 0.020833333333333332,
      "grad_norm": 1.340410828590393,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 1.9308,
      "step": 10
    },
    {
      "epoch": 0.022916666666666665,
      "grad_norm": 1.474332571029663,
      "learning_rate": 2.2916666666666666e-07,
      "loss": 1.9204,
      "step": 11
    },
    {
      "epoch": 0.025,
      "grad_norm": 1.4370452165603638,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 1.9746,
      "step": 12
    },
    {
      "epoch": 0.027083333333333334,
      "grad_norm": 1.5659068822860718,
      "learning_rate": 2.7083333333333337e-07,
      "loss": 1.9237,
      "step": 13
    },
    {
      "epoch": 0.029166666666666667,
      "grad_norm": 1.6647891998291016,
      "learning_rate": 2.916666666666667e-07,
      "loss": 1.9787,
      "step": 14
    },
    {
      "epoch": 0.03125,
      "grad_norm": 1.1900396347045898,
      "learning_rate": 3.125e-07,
      "loss": 1.9424,
      "step": 15
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 2.090128183364868,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 1.9332,
      "step": 16
    },
    {
      "epoch": 0.035416666666666666,
      "grad_norm": 1.1047351360321045,
      "learning_rate": 3.541666666666667e-07,
      "loss": 1.9591,
      "step": 17
    },
    {
      "epoch": 0.0375,
      "grad_norm": 1.113924264907837,
      "learning_rate": 3.75e-07,
      "loss": 1.9185,
      "step": 18
    },
    {
      "epoch": 0.03958333333333333,
      "grad_norm": 1.4250507354736328,
      "learning_rate": 3.9583333333333334e-07,
      "loss": 1.9344,
      "step": 19
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 1.186957836151123,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 1.9861,
      "step": 20
    },
    {
      "epoch": 0.04375,
      "grad_norm": 1.1937501430511475,
      "learning_rate": 4.375e-07,
      "loss": 1.9203,
      "step": 21
    },
    {
      "epoch": 0.04583333333333333,
      "grad_norm": 1.34679114818573,
      "learning_rate": 4.583333333333333e-07,
      "loss": 1.9025,
      "step": 22
    },
    {
      "epoch": 0.04791666666666667,
      "grad_norm": 1.327927827835083,
      "learning_rate": 4.791666666666667e-07,
      "loss": 1.9803,
      "step": 23
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.2930009365081787,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9648,
      "step": 24
    },
    {
      "epoch": 0.052083333333333336,
      "grad_norm": 1.9997715950012207,
      "learning_rate": 5.208333333333334e-07,
      "loss": 1.8909,
      "step": 25
    },
    {
      "epoch": 0.05416666666666667,
      "grad_norm": 1.2654446363449097,
      "learning_rate": 5.416666666666667e-07,
      "loss": 1.9629,
      "step": 26
    },
    {
      "epoch": 0.05625,
      "grad_norm": 1.3119282722473145,
      "learning_rate": 5.625e-07,
      "loss": 1.9094,
      "step": 27
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 1.316719889640808,
      "learning_rate": 5.833333333333334e-07,
      "loss": 1.9397,
      "step": 28
    },
    {
      "epoch": 0.06041666666666667,
      "grad_norm": 1.38850736618042,
      "learning_rate": 6.041666666666667e-07,
      "loss": 1.9292,
      "step": 29
    },
    {
      "epoch": 0.0625,
      "grad_norm": 1.8557636737823486,
      "learning_rate": 6.25e-07,
      "loss": 1.9481,
      "step": 30
    },
    {
      "epoch": 0.06458333333333334,
      "grad_norm": 1.1216989755630493,
      "learning_rate": 6.458333333333334e-07,
      "loss": 1.9519,
      "step": 31
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.4565935134887695,
      "learning_rate": 6.666666666666667e-07,
      "loss": 2.006,
      "step": 32
    },
    {
      "epoch": 0.06875,
      "grad_norm": 1.4888185262680054,
      "learning_rate": 6.875000000000001e-07,
      "loss": 1.9306,
      "step": 33
    },
    {
      "epoch": 0.07083333333333333,
      "grad_norm": 1.3666950464248657,
      "learning_rate": 7.083333333333334e-07,
      "loss": 1.9474,
      "step": 34
    },
    {
      "epoch": 0.07291666666666667,
      "grad_norm": 1.453478217124939,
      "learning_rate": 7.291666666666667e-07,
      "loss": 1.9681,
      "step": 35
    },
    {
      "epoch": 0.075,
      "grad_norm": 1.4112991094589233,
      "learning_rate": 7.5e-07,
      "loss": 1.9334,
      "step": 36
    },
    {
      "epoch": 0.07708333333333334,
      "grad_norm": 1.5293419361114502,
      "learning_rate": 7.708333333333334e-07,
      "loss": 1.948,
      "step": 37
    },
    {
      "epoch": 0.07916666666666666,
      "grad_norm": 1.7969555854797363,
      "learning_rate": 7.916666666666667e-07,
      "loss": 1.9944,
      "step": 38
    },
    {
      "epoch": 0.08125,
      "grad_norm": 1.4376308917999268,
      "learning_rate": 8.125000000000001e-07,
      "loss": 1.9533,
      "step": 39
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.4313029050827026,
      "learning_rate": 8.333333333333333e-07,
      "loss": 1.9417,
      "step": 40
    },
    {
      "epoch": 0.08541666666666667,
      "grad_norm": 2.147570848464966,
      "learning_rate": 8.541666666666667e-07,
      "loss": 1.9789,
      "step": 41
    },
    {
      "epoch": 0.0875,
      "grad_norm": 1.1594797372817993,
      "learning_rate": 8.75e-07,
      "loss": 1.9405,
      "step": 42
    },
    {
      "epoch": 0.08958333333333333,
      "grad_norm": 1.6665924787521362,
      "learning_rate": 8.958333333333334e-07,
      "loss": 1.9406,
      "step": 43
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 2.037606954574585,
      "learning_rate": 9.166666666666666e-07,
      "loss": 1.924,
      "step": 44
    },
    {
      "epoch": 0.09375,
      "grad_norm": 1.2316523790359497,
      "learning_rate": 9.375000000000001e-07,
      "loss": 1.9403,
      "step": 45
    },
    {
      "epoch": 0.09583333333333334,
      "grad_norm": 1.21660578250885,
      "learning_rate": 9.583333333333334e-07,
      "loss": 1.9193,
      "step": 46
    },
    {
      "epoch": 0.09791666666666667,
      "grad_norm": 1.5828365087509155,
      "learning_rate": 9.791666666666667e-07,
      "loss": 1.9893,
      "step": 47
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.723954677581787,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.9728,
      "step": 48
    },
    {
      "epoch": 0.10208333333333333,
      "grad_norm": 1.3959202766418457,
      "learning_rate": 1.0208333333333334e-06,
      "loss": 1.9361,
      "step": 49
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 1.4296817779541016,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 1.9288,
      "step": 50
    },
    {
      "epoch": 0.10625,
      "grad_norm": 1.9942065477371216,
      "learning_rate": 1.0625e-06,
      "loss": 1.948,
      "step": 51
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 1.499065637588501,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 1.9409,
      "step": 52
    },
    {
      "epoch": 0.11041666666666666,
      "grad_norm": 1.2829371690750122,
      "learning_rate": 1.1041666666666668e-06,
      "loss": 1.912,
      "step": 53
    },
    {
      "epoch": 0.1125,
      "grad_norm": 1.4519883394241333,
      "learning_rate": 1.125e-06,
      "loss": 1.9414,
      "step": 54
    },
    {
      "epoch": 0.11458333333333333,
      "grad_norm": 1.3108900785446167,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 1.9584,
      "step": 55
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.8849138021469116,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 1.9489,
      "step": 56
    },
    {
      "epoch": 0.11875,
      "grad_norm": 1.4732887744903564,
      "learning_rate": 1.1875e-06,
      "loss": 1.9627,
      "step": 57
    },
    {
      "epoch": 0.12083333333333333,
      "grad_norm": 1.2098169326782227,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 1.9225,
      "step": 58
    },
    {
      "epoch": 0.12291666666666666,
      "grad_norm": 1.3880913257598877,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 1.931,
      "step": 59
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.284332513809204,
      "learning_rate": 1.25e-06,
      "loss": 1.9064,
      "step": 60
    },
    {
      "epoch": 0.12708333333333333,
      "grad_norm": 1.3738212585449219,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 1.9499,
      "step": 61
    },
    {
      "epoch": 0.12916666666666668,
      "grad_norm": 1.366930365562439,
      "learning_rate": 1.2916666666666669e-06,
      "loss": 1.9269,
      "step": 62
    },
    {
      "epoch": 0.13125,
      "grad_norm": 1.4547582864761353,
      "learning_rate": 1.3125000000000001e-06,
      "loss": 1.972,
      "step": 63
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.3769160509109497,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 1.9461,
      "step": 64
    },
    {
      "epoch": 0.13541666666666666,
      "grad_norm": 1.718178391456604,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 1.9872,
      "step": 65
    },
    {
      "epoch": 0.1375,
      "grad_norm": 1.8160940408706665,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 1.9294,
      "step": 66
    },
    {
      "epoch": 0.13958333333333334,
      "grad_norm": 1.7659497261047363,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 1.9772,
      "step": 67
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 2.45913028717041,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 2.0076,
      "step": 68
    },
    {
      "epoch": 0.14375,
      "grad_norm": 1.2259025573730469,
      "learning_rate": 1.4375e-06,
      "loss": 1.9295,
      "step": 69
    },
    {
      "epoch": 0.14583333333333334,
      "grad_norm": 1.30710768699646,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 1.9521,
      "step": 70
    },
    {
      "epoch": 0.14791666666666667,
      "grad_norm": 1.3667610883712769,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 1.9655,
      "step": 71
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.4477694034576416,
      "learning_rate": 1.5e-06,
      "loss": 1.9607,
      "step": 72
    },
    {
      "epoch": 0.15208333333333332,
      "grad_norm": 1.3650097846984863,
      "learning_rate": 1.5208333333333333e-06,
      "loss": 1.917,
      "step": 73
    },
    {
      "epoch": 0.15416666666666667,
      "grad_norm": 1.3355998992919922,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 1.9169,
      "step": 74
    },
    {
      "epoch": 0.15625,
      "grad_norm": 1.2580227851867676,
      "learning_rate": 1.5625e-06,
      "loss": 1.9224,
      "step": 75
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.6419705152511597,
      "learning_rate": 1.5833333333333333e-06,
      "loss": 1.9543,
      "step": 76
    },
    {
      "epoch": 0.16041666666666668,
      "grad_norm": 1.6093991994857788,
      "learning_rate": 1.6041666666666668e-06,
      "loss": 1.9102,
      "step": 77
    },
    {
      "epoch": 0.1625,
      "grad_norm": 1.5098154544830322,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 1.9024,
      "step": 78
    },
    {
      "epoch": 0.16458333333333333,
      "grad_norm": 1.4802966117858887,
      "learning_rate": 1.6458333333333334e-06,
      "loss": 1.991,
      "step": 79
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.492950439453125,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 1.9132,
      "step": 80
    },
    {
      "epoch": 0.16875,
      "grad_norm": 1.7119752168655396,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 1.9472,
      "step": 81
    },
    {
      "epoch": 0.17083333333333334,
      "grad_norm": 1.3862414360046387,
      "learning_rate": 1.7083333333333334e-06,
      "loss": 1.9366,
      "step": 82
    },
    {
      "epoch": 0.17291666666666666,
      "grad_norm": 1.7285840511322021,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 1.9395,
      "step": 83
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.4411652088165283,
      "learning_rate": 1.75e-06,
      "loss": 1.9125,
      "step": 84
    },
    {
      "epoch": 0.17708333333333334,
      "grad_norm": 1.2784425020217896,
      "learning_rate": 1.7708333333333337e-06,
      "loss": 1.963,
      "step": 85
    },
    {
      "epoch": 0.17916666666666667,
      "grad_norm": 1.8572155237197876,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 1.9854,
      "step": 86
    },
    {
      "epoch": 0.18125,
      "grad_norm": 2.0394866466522217,
      "learning_rate": 1.8125e-06,
      "loss": 1.9486,
      "step": 87
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.1604998111724854,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 1.9248,
      "step": 88
    },
    {
      "epoch": 0.18541666666666667,
      "grad_norm": 1.3454688787460327,
      "learning_rate": 1.854166666666667e-06,
      "loss": 1.9569,
      "step": 89
    },
    {
      "epoch": 0.1875,
      "grad_norm": 1.2562265396118164,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 1.9538,
      "step": 90
    },
    {
      "epoch": 0.18958333333333333,
      "grad_norm": 1.603117823600769,
      "learning_rate": 1.8958333333333333e-06,
      "loss": 1.9348,
      "step": 91
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.3332278728485107,
      "learning_rate": 1.916666666666667e-06,
      "loss": 1.9159,
      "step": 92
    },
    {
      "epoch": 0.19375,
      "grad_norm": 1.4468432664871216,
      "learning_rate": 1.9375e-06,
      "loss": 1.9273,
      "step": 93
    },
    {
      "epoch": 0.19583333333333333,
      "grad_norm": 1.5407981872558594,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 1.9555,
      "step": 94
    },
    {
      "epoch": 0.19791666666666666,
      "grad_norm": 1.5373258590698242,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 1.9803,
      "step": 95
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.3939034938812256,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.9028,
      "step": 96
    },
    {
      "epoch": 0.20208333333333334,
      "grad_norm": 1.5232770442962646,
      "learning_rate": 2.0208333333333336e-06,
      "loss": 1.9682,
      "step": 97
    },
    {
      "epoch": 0.20416666666666666,
      "grad_norm": 1.5902122259140015,
      "learning_rate": 2.041666666666667e-06,
      "loss": 1.9409,
      "step": 98
    },
    {
      "epoch": 0.20625,
      "grad_norm": 1.3247003555297852,
      "learning_rate": 2.0625e-06,
      "loss": 1.9401,
      "step": 99
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.5977855920791626,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 1.9686,
      "step": 100
    },
    {
      "epoch": 0.21041666666666667,
      "grad_norm": 1.405238151550293,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 1.9374,
      "step": 101
    },
    {
      "epoch": 0.2125,
      "grad_norm": 1.5895088911056519,
      "learning_rate": 2.125e-06,
      "loss": 1.9829,
      "step": 102
    },
    {
      "epoch": 0.21458333333333332,
      "grad_norm": 1.6168591976165771,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 1.9194,
      "step": 103
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.9306175708770752,
      "learning_rate": 2.166666666666667e-06,
      "loss": 1.9244,
      "step": 104
    },
    {
      "epoch": 0.21875,
      "grad_norm": 1.3008852005004883,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 1.9116,
      "step": 105
    },
    {
      "epoch": 0.22083333333333333,
      "grad_norm": 1.2496017217636108,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 1.9375,
      "step": 106
    },
    {
      "epoch": 0.22291666666666668,
      "grad_norm": 1.2698489427566528,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 1.9534,
      "step": 107
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.3460439443588257,
      "learning_rate": 2.25e-06,
      "loss": 1.9512,
      "step": 108
    },
    {
      "epoch": 0.22708333333333333,
      "grad_norm": 1.8827704191207886,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 2.0046,
      "step": 109
    },
    {
      "epoch": 0.22916666666666666,
      "grad_norm": 1.8271830081939697,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 2.0088,
      "step": 110
    },
    {
      "epoch": 0.23125,
      "grad_norm": 2.005007028579712,
      "learning_rate": 2.3125000000000003e-06,
      "loss": 1.9298,
      "step": 111
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.6933585405349731,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 1.9791,
      "step": 112
    },
    {
      "epoch": 0.23541666666666666,
      "grad_norm": 1.3853344917297363,
      "learning_rate": 2.354166666666667e-06,
      "loss": 1.9041,
      "step": 113
    },
    {
      "epoch": 0.2375,
      "grad_norm": 1.6051607131958008,
      "learning_rate": 2.375e-06,
      "loss": 1.9834,
      "step": 114
    },
    {
      "epoch": 0.23958333333333334,
      "grad_norm": 1.616532802581787,
      "learning_rate": 2.395833333333334e-06,
      "loss": 1.9921,
      "step": 115
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.9666160345077515,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 1.9459,
      "step": 116
    },
    {
      "epoch": 0.24375,
      "grad_norm": 2.0627338886260986,
      "learning_rate": 2.4375e-06,
      "loss": 1.9391,
      "step": 117
    },
    {
      "epoch": 0.24583333333333332,
      "grad_norm": 1.3450732231140137,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 1.9568,
      "step": 118
    },
    {
      "epoch": 0.24791666666666667,
      "grad_norm": 1.6196264028549194,
      "learning_rate": 2.479166666666667e-06,
      "loss": 1.9144,
      "step": 119
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.5735896825790405,
      "learning_rate": 2.5e-06,
      "loss": 1.9322,
      "step": 120
    },
    {
      "epoch": 0.2520833333333333,
      "grad_norm": 1.6160943508148193,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 1.9817,
      "step": 121
    },
    {
      "epoch": 0.25416666666666665,
      "grad_norm": 1.608150839805603,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 1.974,
      "step": 122
    },
    {
      "epoch": 0.25625,
      "grad_norm": 1.3417377471923828,
      "learning_rate": 2.5625e-06,
      "loss": 1.9541,
      "step": 123
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.2143162488937378,
      "learning_rate": 2.5833333333333337e-06,
      "loss": 1.9392,
      "step": 124
    },
    {
      "epoch": 0.2604166666666667,
      "grad_norm": 1.6939482688903809,
      "learning_rate": 2.604166666666667e-06,
      "loss": 1.9366,
      "step": 125
    },
    {
      "epoch": 0.2625,
      "grad_norm": 1.740329623222351,
      "learning_rate": 2.6250000000000003e-06,
      "loss": 1.9781,
      "step": 126
    },
    {
      "epoch": 0.26458333333333334,
      "grad_norm": 1.1429450511932373,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 1.9185,
      "step": 127
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.9097005128860474,
      "learning_rate": 2.666666666666667e-06,
      "loss": 1.9879,
      "step": 128
    },
    {
      "epoch": 0.26875,
      "grad_norm": 1.4480551481246948,
      "learning_rate": 2.6875e-06,
      "loss": 1.9101,
      "step": 129
    },
    {
      "epoch": 0.2708333333333333,
      "grad_norm": 1.4415920972824097,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 1.9234,
      "step": 130
    },
    {
      "epoch": 0.27291666666666664,
      "grad_norm": 1.4805325269699097,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 1.9103,
      "step": 131
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.4249248504638672,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 1.9625,
      "step": 132
    },
    {
      "epoch": 0.27708333333333335,
      "grad_norm": 1.273820400238037,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 1.9058,
      "step": 133
    },
    {
      "epoch": 0.2791666666666667,
      "grad_norm": 1.4991645812988281,
      "learning_rate": 2.791666666666667e-06,
      "loss": 1.9117,
      "step": 134
    },
    {
      "epoch": 0.28125,
      "grad_norm": 1.248900055885315,
      "learning_rate": 2.8125e-06,
      "loss": 1.918,
      "step": 135
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.3453645706176758,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 1.9484,
      "step": 136
    },
    {
      "epoch": 0.28541666666666665,
      "grad_norm": 1.6247049570083618,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 1.9047,
      "step": 137
    },
    {
      "epoch": 0.2875,
      "grad_norm": 1.843184471130371,
      "learning_rate": 2.875e-06,
      "loss": 1.9819,
      "step": 138
    },
    {
      "epoch": 0.28958333333333336,
      "grad_norm": 1.561177134513855,
      "learning_rate": 2.8958333333333337e-06,
      "loss": 1.9662,
      "step": 139
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.7659250497817993,
      "learning_rate": 2.916666666666667e-06,
      "loss": 1.944,
      "step": 140
    },
    {
      "epoch": 0.29375,
      "grad_norm": 1.5173490047454834,
      "learning_rate": 2.9375000000000003e-06,
      "loss": 1.9748,
      "step": 141
    },
    {
      "epoch": 0.29583333333333334,
      "grad_norm": 1.7372595071792603,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 1.9787,
      "step": 142
    },
    {
      "epoch": 0.29791666666666666,
      "grad_norm": 1.342614769935608,
      "learning_rate": 2.979166666666667e-06,
      "loss": 1.9399,
      "step": 143
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.566171407699585,
      "learning_rate": 3e-06,
      "loss": 1.9392,
      "step": 144
    },
    {
      "epoch": 0.3020833333333333,
      "grad_norm": 1.4333405494689941,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 1.954,
      "step": 145
    },
    {
      "epoch": 0.30416666666666664,
      "grad_norm": 1.7944831848144531,
      "learning_rate": 3.0416666666666666e-06,
      "loss": 1.9957,
      "step": 146
    },
    {
      "epoch": 0.30625,
      "grad_norm": 3.011237382888794,
      "learning_rate": 3.0625000000000003e-06,
      "loss": 2.0382,
      "step": 147
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 1.4312045574188232,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 1.9054,
      "step": 148
    },
    {
      "epoch": 0.3104166666666667,
      "grad_norm": 1.70932936668396,
      "learning_rate": 3.104166666666667e-06,
      "loss": 1.9834,
      "step": 149
    },
    {
      "epoch": 0.3125,
      "grad_norm": 1.5819846391677856,
      "learning_rate": 3.125e-06,
      "loss": 1.8841,
      "step": 150
    },
    {
      "epoch": 0.3145833333333333,
      "grad_norm": 1.8980270624160767,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 1.929,
      "step": 151
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 2.596450090408325,
      "learning_rate": 3.1666666666666667e-06,
      "loss": 1.9101,
      "step": 152
    },
    {
      "epoch": 0.31875,
      "grad_norm": 2.495577573776245,
      "learning_rate": 3.1875e-06,
      "loss": 1.9147,
      "step": 153
    },
    {
      "epoch": 0.32083333333333336,
      "grad_norm": 1.3611204624176025,
      "learning_rate": 3.2083333333333337e-06,
      "loss": 1.9271,
      "step": 154
    },
    {
      "epoch": 0.3229166666666667,
      "grad_norm": 1.9344487190246582,
      "learning_rate": 3.229166666666667e-06,
      "loss": 1.9114,
      "step": 155
    },
    {
      "epoch": 0.325,
      "grad_norm": 2.0817620754241943,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.9762,
      "step": 156
    },
    {
      "epoch": 0.32708333333333334,
      "grad_norm": 1.6482269763946533,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 2.0067,
      "step": 157
    },
    {
      "epoch": 0.32916666666666666,
      "grad_norm": 1.6429798603057861,
      "learning_rate": 3.2916666666666668e-06,
      "loss": 1.9578,
      "step": 158
    },
    {
      "epoch": 0.33125,
      "grad_norm": 1.3621513843536377,
      "learning_rate": 3.3125e-06,
      "loss": 1.9231,
      "step": 159
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.709699034690857,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.8905,
      "step": 160
    },
    {
      "epoch": 0.33541666666666664,
      "grad_norm": 1.424316644668579,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 1.9278,
      "step": 161
    },
    {
      "epoch": 0.3375,
      "grad_norm": 1.7434216737747192,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 1.9799,
      "step": 162
    },
    {
      "epoch": 0.33958333333333335,
      "grad_norm": 1.5084645748138428,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 1.9609,
      "step": 163
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 2.189319610595703,
      "learning_rate": 3.416666666666667e-06,
      "loss": 1.9024,
      "step": 164
    },
    {
      "epoch": 0.34375,
      "grad_norm": 1.3864644765853882,
      "learning_rate": 3.4375e-06,
      "loss": 1.9212,
      "step": 165
    },
    {
      "epoch": 0.3458333333333333,
      "grad_norm": 2.4119699001312256,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 1.9062,
      "step": 166
    },
    {
      "epoch": 0.34791666666666665,
      "grad_norm": 1.7223366498947144,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 1.9691,
      "step": 167
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.385590076446533,
      "learning_rate": 3.5e-06,
      "loss": 1.9938,
      "step": 168
    },
    {
      "epoch": 0.35208333333333336,
      "grad_norm": 1.6133110523223877,
      "learning_rate": 3.520833333333334e-06,
      "loss": 1.9105,
      "step": 169
    },
    {
      "epoch": 0.3541666666666667,
      "grad_norm": 2.080188274383545,
      "learning_rate": 3.5416666666666673e-06,
      "loss": 1.9444,
      "step": 170
    },
    {
      "epoch": 0.35625,
      "grad_norm": 2.002821922302246,
      "learning_rate": 3.5625e-06,
      "loss": 1.9914,
      "step": 171
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 1.4226205348968506,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 1.934,
      "step": 172
    },
    {
      "epoch": 0.36041666666666666,
      "grad_norm": 1.5380207300186157,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 1.9668,
      "step": 173
    },
    {
      "epoch": 0.3625,
      "grad_norm": 1.9901806116104126,
      "learning_rate": 3.625e-06,
      "loss": 1.9,
      "step": 174
    },
    {
      "epoch": 0.3645833333333333,
      "grad_norm": 1.6424179077148438,
      "learning_rate": 3.6458333333333333e-06,
      "loss": 1.9333,
      "step": 175
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 2.1804091930389404,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 1.8936,
      "step": 176
    },
    {
      "epoch": 0.36875,
      "grad_norm": 1.5809381008148193,
      "learning_rate": 3.6875000000000007e-06,
      "loss": 1.9643,
      "step": 177
    },
    {
      "epoch": 0.37083333333333335,
      "grad_norm": 2.197772741317749,
      "learning_rate": 3.708333333333334e-06,
      "loss": 1.9709,
      "step": 178
    },
    {
      "epoch": 0.3729166666666667,
      "grad_norm": 1.8060131072998047,
      "learning_rate": 3.7291666666666672e-06,
      "loss": 1.9395,
      "step": 179
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.7433029413223267,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 1.9848,
      "step": 180
    },
    {
      "epoch": 0.3770833333333333,
      "grad_norm": 1.6533558368682861,
      "learning_rate": 3.7708333333333334e-06,
      "loss": 1.8929,
      "step": 181
    },
    {
      "epoch": 0.37916666666666665,
      "grad_norm": 1.9880746603012085,
      "learning_rate": 3.7916666666666666e-06,
      "loss": 1.9319,
      "step": 182
    },
    {
      "epoch": 0.38125,
      "grad_norm": 1.354346752166748,
      "learning_rate": 3.8125e-06,
      "loss": 1.9491,
      "step": 183
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.6048343181610107,
      "learning_rate": 3.833333333333334e-06,
      "loss": 1.8989,
      "step": 184
    },
    {
      "epoch": 0.3854166666666667,
      "grad_norm": 2.248694896697998,
      "learning_rate": 3.854166666666667e-06,
      "loss": 1.8769,
      "step": 185
    },
    {
      "epoch": 0.3875,
      "grad_norm": 1.8974634408950806,
      "learning_rate": 3.875e-06,
      "loss": 1.966,
      "step": 186
    },
    {
      "epoch": 0.38958333333333334,
      "grad_norm": 1.554516077041626,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 1.9232,
      "step": 187
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.7448005676269531,
      "learning_rate": 3.916666666666667e-06,
      "loss": 1.9812,
      "step": 188
    },
    {
      "epoch": 0.39375,
      "grad_norm": 1.5821349620819092,
      "learning_rate": 3.9375e-06,
      "loss": 1.8925,
      "step": 189
    },
    {
      "epoch": 0.3958333333333333,
      "grad_norm": 1.703403115272522,
      "learning_rate": 3.958333333333333e-06,
      "loss": 1.8891,
      "step": 190
    },
    {
      "epoch": 0.39791666666666664,
      "grad_norm": 2.4912328720092773,
      "learning_rate": 3.9791666666666665e-06,
      "loss": 1.8883,
      "step": 191
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.5777997970581055,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9096,
      "step": 192
    },
    {
      "epoch": 0.40208333333333335,
      "grad_norm": 1.5086619853973389,
      "learning_rate": 4.020833333333334e-06,
      "loss": 1.9592,
      "step": 193
    },
    {
      "epoch": 0.4041666666666667,
      "grad_norm": 1.7144463062286377,
      "learning_rate": 4.041666666666667e-06,
      "loss": 1.9338,
      "step": 194
    },
    {
      "epoch": 0.40625,
      "grad_norm": 2.938366651535034,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 1.8783,
      "step": 195
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.7791860103607178,
      "learning_rate": 4.083333333333334e-06,
      "loss": 1.9534,
      "step": 196
    },
    {
      "epoch": 0.41041666666666665,
      "grad_norm": 2.0922815799713135,
      "learning_rate": 4.104166666666667e-06,
      "loss": 1.9583,
      "step": 197
    },
    {
      "epoch": 0.4125,
      "grad_norm": 1.9370859861373901,
      "learning_rate": 4.125e-06,
      "loss": 1.974,
      "step": 198
    },
    {
      "epoch": 0.41458333333333336,
      "grad_norm": 1.4685262441635132,
      "learning_rate": 4.145833333333334e-06,
      "loss": 1.9667,
      "step": 199
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.644808292388916,
      "learning_rate": 4.166666666666667e-06,
      "loss": 1.9413,
      "step": 200
    },
    {
      "epoch": 0.41875,
      "grad_norm": 1.6526079177856445,
      "learning_rate": 4.1875e-06,
      "loss": 1.9178,
      "step": 201
    },
    {
      "epoch": 0.42083333333333334,
      "grad_norm": 2.3051586151123047,
      "learning_rate": 4.208333333333333e-06,
      "loss": 2.009,
      "step": 202
    },
    {
      "epoch": 0.42291666666666666,
      "grad_norm": 1.7299785614013672,
      "learning_rate": 4.229166666666667e-06,
      "loss": 1.8726,
      "step": 203
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.9178578853607178,
      "learning_rate": 4.25e-06,
      "loss": 1.9473,
      "step": 204
    },
    {
      "epoch": 0.4270833333333333,
      "grad_norm": 1.646432876586914,
      "learning_rate": 4.270833333333333e-06,
      "loss": 1.9678,
      "step": 205
    },
    {
      "epoch": 0.42916666666666664,
      "grad_norm": 2.21234130859375,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 1.8912,
      "step": 206
    },
    {
      "epoch": 0.43125,
      "grad_norm": 1.5586419105529785,
      "learning_rate": 4.312500000000001e-06,
      "loss": 1.9335,
      "step": 207
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.7768386602401733,
      "learning_rate": 4.333333333333334e-06,
      "loss": 1.9814,
      "step": 208
    },
    {
      "epoch": 0.4354166666666667,
      "grad_norm": 2.3278260231018066,
      "learning_rate": 4.354166666666667e-06,
      "loss": 1.9237,
      "step": 209
    },
    {
      "epoch": 0.4375,
      "grad_norm": 1.7317548990249634,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 1.9659,
      "step": 210
    },
    {
      "epoch": 0.4395833333333333,
      "grad_norm": 1.8235660791397095,
      "learning_rate": 4.395833333333334e-06,
      "loss": 1.9132,
      "step": 211
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.8113242387771606,
      "learning_rate": 4.416666666666667e-06,
      "loss": 1.9345,
      "step": 212
    },
    {
      "epoch": 0.44375,
      "grad_norm": 2.174243450164795,
      "learning_rate": 4.4375e-06,
      "loss": 1.9051,
      "step": 213
    },
    {
      "epoch": 0.44583333333333336,
      "grad_norm": 1.8846261501312256,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 1.9367,
      "step": 214
    },
    {
      "epoch": 0.4479166666666667,
      "grad_norm": 2.396265983581543,
      "learning_rate": 4.479166666666667e-06,
      "loss": 1.9236,
      "step": 215
    },
    {
      "epoch": 0.45,
      "grad_norm": 2.9885406494140625,
      "learning_rate": 4.5e-06,
      "loss": 1.8675,
      "step": 216
    },
    {
      "epoch": 0.45208333333333334,
      "grad_norm": 1.5922093391418457,
      "learning_rate": 4.520833333333333e-06,
      "loss": 1.9758,
      "step": 217
    },
    {
      "epoch": 0.45416666666666666,
      "grad_norm": 1.723069190979004,
      "learning_rate": 4.541666666666667e-06,
      "loss": 1.9236,
      "step": 218
    },
    {
      "epoch": 0.45625,
      "grad_norm": 2.1999847888946533,
      "learning_rate": 4.5625e-06,
      "loss": 1.8921,
      "step": 219
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 2.267242193222046,
      "learning_rate": 4.583333333333333e-06,
      "loss": 1.8975,
      "step": 220
    },
    {
      "epoch": 0.46041666666666664,
      "grad_norm": 2.231661081314087,
      "learning_rate": 4.6041666666666665e-06,
      "loss": 1.8952,
      "step": 221
    },
    {
      "epoch": 0.4625,
      "grad_norm": 1.757491946220398,
      "learning_rate": 4.625000000000001e-06,
      "loss": 1.8975,
      "step": 222
    },
    {
      "epoch": 0.46458333333333335,
      "grad_norm": 1.862398624420166,
      "learning_rate": 4.645833333333334e-06,
      "loss": 1.897,
      "step": 223
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.939106822013855,
      "learning_rate": 4.666666666666667e-06,
      "loss": 1.9671,
      "step": 224
    },
    {
      "epoch": 0.46875,
      "grad_norm": 2.247540235519409,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 2.0449,
      "step": 225
    },
    {
      "epoch": 0.4708333333333333,
      "grad_norm": 3.573150157928467,
      "learning_rate": 4.708333333333334e-06,
      "loss": 2.0275,
      "step": 226
    },
    {
      "epoch": 0.47291666666666665,
      "grad_norm": 1.8961498737335205,
      "learning_rate": 4.729166666666667e-06,
      "loss": 1.9343,
      "step": 227
    },
    {
      "epoch": 0.475,
      "grad_norm": 2.6629021167755127,
      "learning_rate": 4.75e-06,
      "loss": 1.9605,
      "step": 228
    },
    {
      "epoch": 0.47708333333333336,
      "grad_norm": 1.9299182891845703,
      "learning_rate": 4.770833333333334e-06,
      "loss": 2.0027,
      "step": 229
    },
    {
      "epoch": 0.4791666666666667,
      "grad_norm": 2.2619400024414062,
      "learning_rate": 4.791666666666668e-06,
      "loss": 1.9978,
      "step": 230
    },
    {
      "epoch": 0.48125,
      "grad_norm": 1.6843916177749634,
      "learning_rate": 4.8125e-06,
      "loss": 1.9494,
      "step": 231
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.97939133644104,
      "learning_rate": 4.833333333333333e-06,
      "loss": 1.9049,
      "step": 232
    },
    {
      "epoch": 0.48541666666666666,
      "grad_norm": 2.605259656906128,
      "learning_rate": 4.854166666666667e-06,
      "loss": 1.8513,
      "step": 233
    },
    {
      "epoch": 0.4875,
      "grad_norm": 2.428267478942871,
      "learning_rate": 4.875e-06,
      "loss": 1.8604,
      "step": 234
    },
    {
      "epoch": 0.4895833333333333,
      "grad_norm": 2.2064976692199707,
      "learning_rate": 4.895833333333333e-06,
      "loss": 1.9074,
      "step": 235
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 1.8235766887664795,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 1.9042,
      "step": 236
    },
    {
      "epoch": 0.49375,
      "grad_norm": 1.7362613677978516,
      "learning_rate": 4.937500000000001e-06,
      "loss": 1.9731,
      "step": 237
    },
    {
      "epoch": 0.49583333333333335,
      "grad_norm": 1.9698946475982666,
      "learning_rate": 4.958333333333334e-06,
      "loss": 1.9606,
      "step": 238
    },
    {
      "epoch": 0.4979166666666667,
      "grad_norm": 3.645357847213745,
      "learning_rate": 4.979166666666667e-06,
      "loss": 1.8531,
      "step": 239
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.854358196258545,
      "learning_rate": 5e-06,
      "loss": 1.9671,
      "step": 240
    },
    {
      "epoch": 0.5020833333333333,
      "grad_norm": 1.843748927116394,
      "learning_rate": 5.020833333333334e-06,
      "loss": 1.9645,
      "step": 241
    },
    {
      "epoch": 0.5041666666666667,
      "grad_norm": 2.6966536045074463,
      "learning_rate": 5.041666666666667e-06,
      "loss": 1.9536,
      "step": 242
    },
    {
      "epoch": 0.50625,
      "grad_norm": 3.1641457080841064,
      "learning_rate": 5.0625e-06,
      "loss": 2.0146,
      "step": 243
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 2.738107919692993,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 1.9315,
      "step": 244
    },
    {
      "epoch": 0.5104166666666666,
      "grad_norm": 2.788121223449707,
      "learning_rate": 5.104166666666667e-06,
      "loss": 1.9572,
      "step": 245
    },
    {
      "epoch": 0.5125,
      "grad_norm": 2.3148388862609863,
      "learning_rate": 5.125e-06,
      "loss": 1.8307,
      "step": 246
    },
    {
      "epoch": 0.5145833333333333,
      "grad_norm": 2.8322653770446777,
      "learning_rate": 5.145833333333333e-06,
      "loss": 1.9568,
      "step": 247
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.8942432403564453,
      "learning_rate": 5.1666666666666675e-06,
      "loss": 1.9898,
      "step": 248
    },
    {
      "epoch": 0.51875,
      "grad_norm": 1.8595223426818848,
      "learning_rate": 5.187500000000001e-06,
      "loss": 1.894,
      "step": 249
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 2.410299301147461,
      "learning_rate": 5.208333333333334e-06,
      "loss": 1.9631,
      "step": 250
    },
    {
      "epoch": 0.5229166666666667,
      "grad_norm": 3.0616726875305176,
      "learning_rate": 5.229166666666667e-06,
      "loss": 1.9173,
      "step": 251
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.9040396213531494,
      "learning_rate": 5.2500000000000006e-06,
      "loss": 1.8926,
      "step": 252
    },
    {
      "epoch": 0.5270833333333333,
      "grad_norm": 1.8419060707092285,
      "learning_rate": 5.270833333333334e-06,
      "loss": 1.9029,
      "step": 253
    },
    {
      "epoch": 0.5291666666666667,
      "grad_norm": 3.389883279800415,
      "learning_rate": 5.291666666666667e-06,
      "loss": 2.0174,
      "step": 254
    },
    {
      "epoch": 0.53125,
      "grad_norm": 1.9361604452133179,
      "learning_rate": 5.3125e-06,
      "loss": 1.8934,
      "step": 255
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.4250881671905518,
      "learning_rate": 5.333333333333334e-06,
      "loss": 2.0199,
      "step": 256
    },
    {
      "epoch": 0.5354166666666667,
      "grad_norm": 3.165557622909546,
      "learning_rate": 5.354166666666667e-06,
      "loss": 1.9131,
      "step": 257
    },
    {
      "epoch": 0.5375,
      "grad_norm": 2.1826205253601074,
      "learning_rate": 5.375e-06,
      "loss": 1.8948,
      "step": 258
    },
    {
      "epoch": 0.5395833333333333,
      "grad_norm": 2.0835843086242676,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 1.8973,
      "step": 259
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 2.1703131198883057,
      "learning_rate": 5.416666666666667e-06,
      "loss": 1.8461,
      "step": 260
    },
    {
      "epoch": 0.54375,
      "grad_norm": 2.299730062484741,
      "learning_rate": 5.4375e-06,
      "loss": 2.0039,
      "step": 261
    },
    {
      "epoch": 0.5458333333333333,
      "grad_norm": 2.1289424896240234,
      "learning_rate": 5.458333333333333e-06,
      "loss": 1.9709,
      "step": 262
    },
    {
      "epoch": 0.5479166666666667,
      "grad_norm": 3.1521549224853516,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 1.8674,
      "step": 263
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.9204808473587036,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.9357,
      "step": 264
    },
    {
      "epoch": 0.5520833333333334,
      "grad_norm": 1.8873332738876343,
      "learning_rate": 5.520833333333334e-06,
      "loss": 1.9051,
      "step": 265
    },
    {
      "epoch": 0.5541666666666667,
      "grad_norm": 3.155243158340454,
      "learning_rate": 5.541666666666667e-06,
      "loss": 2.0185,
      "step": 266
    },
    {
      "epoch": 0.55625,
      "grad_norm": 2.3129990100860596,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 1.8596,
      "step": 267
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 2.3421220779418945,
      "learning_rate": 5.583333333333334e-06,
      "loss": 2.0428,
      "step": 268
    },
    {
      "epoch": 0.5604166666666667,
      "grad_norm": 3.07894229888916,
      "learning_rate": 5.604166666666667e-06,
      "loss": 1.8312,
      "step": 269
    },
    {
      "epoch": 0.5625,
      "grad_norm": 2.0167429447174072,
      "learning_rate": 5.625e-06,
      "loss": 1.919,
      "step": 270
    },
    {
      "epoch": 0.5645833333333333,
      "grad_norm": 2.1795730590820312,
      "learning_rate": 5.645833333333334e-06,
      "loss": 1.9425,
      "step": 271
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 3.5813472270965576,
      "learning_rate": 5.666666666666667e-06,
      "loss": 2.0585,
      "step": 272
    },
    {
      "epoch": 0.56875,
      "grad_norm": 2.0160906314849854,
      "learning_rate": 5.6875e-06,
      "loss": 1.9892,
      "step": 273
    },
    {
      "epoch": 0.5708333333333333,
      "grad_norm": 2.624969005584717,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 1.8676,
      "step": 274
    },
    {
      "epoch": 0.5729166666666666,
      "grad_norm": 2.2965028285980225,
      "learning_rate": 5.729166666666667e-06,
      "loss": 1.8911,
      "step": 275
    },
    {
      "epoch": 0.575,
      "grad_norm": 3.5277669429779053,
      "learning_rate": 5.75e-06,
      "loss": 2.0142,
      "step": 276
    },
    {
      "epoch": 0.5770833333333333,
      "grad_norm": 1.9311413764953613,
      "learning_rate": 5.770833333333333e-06,
      "loss": 1.9065,
      "step": 277
    },
    {
      "epoch": 0.5791666666666667,
      "grad_norm": 2.0657958984375,
      "learning_rate": 5.791666666666667e-06,
      "loss": 1.9246,
      "step": 278
    },
    {
      "epoch": 0.58125,
      "grad_norm": 2.607872247695923,
      "learning_rate": 5.812500000000001e-06,
      "loss": 2.0187,
      "step": 279
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 2.5093233585357666,
      "learning_rate": 5.833333333333334e-06,
      "loss": 1.8347,
      "step": 280
    },
    {
      "epoch": 0.5854166666666667,
      "grad_norm": 1.898473858833313,
      "learning_rate": 5.854166666666667e-06,
      "loss": 1.9006,
      "step": 281
    },
    {
      "epoch": 0.5875,
      "grad_norm": 1.9381897449493408,
      "learning_rate": 5.8750000000000005e-06,
      "loss": 1.9168,
      "step": 282
    },
    {
      "epoch": 0.5895833333333333,
      "grad_norm": 2.9250664710998535,
      "learning_rate": 5.895833333333334e-06,
      "loss": 2.0804,
      "step": 283
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 2.4667484760284424,
      "learning_rate": 5.916666666666667e-06,
      "loss": 1.9591,
      "step": 284
    },
    {
      "epoch": 0.59375,
      "grad_norm": 3.518202543258667,
      "learning_rate": 5.9375e-06,
      "loss": 2.0663,
      "step": 285
    },
    {
      "epoch": 0.5958333333333333,
      "grad_norm": 2.2965872287750244,
      "learning_rate": 5.958333333333334e-06,
      "loss": 1.9379,
      "step": 286
    },
    {
      "epoch": 0.5979166666666667,
      "grad_norm": 2.312025547027588,
      "learning_rate": 5.979166666666667e-06,
      "loss": 1.8913,
      "step": 287
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.984014630317688,
      "learning_rate": 6e-06,
      "loss": 1.9283,
      "step": 288
    },
    {
      "epoch": 0.6020833333333333,
      "grad_norm": 2.3506760597229004,
      "learning_rate": 6.0208333333333334e-06,
      "loss": 1.9983,
      "step": 289
    },
    {
      "epoch": 0.6041666666666666,
      "grad_norm": 3.4718809127807617,
      "learning_rate": 6.041666666666667e-06,
      "loss": 1.9049,
      "step": 290
    },
    {
      "epoch": 0.60625,
      "grad_norm": 2.399280309677124,
      "learning_rate": 6.0625e-06,
      "loss": 1.8421,
      "step": 291
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 2.5102994441986084,
      "learning_rate": 6.083333333333333e-06,
      "loss": 1.9692,
      "step": 292
    },
    {
      "epoch": 0.6104166666666667,
      "grad_norm": 2.54512619972229,
      "learning_rate": 6.104166666666667e-06,
      "loss": 1.8346,
      "step": 293
    },
    {
      "epoch": 0.6125,
      "grad_norm": 1.888302206993103,
      "learning_rate": 6.125000000000001e-06,
      "loss": 1.8693,
      "step": 294
    },
    {
      "epoch": 0.6145833333333334,
      "grad_norm": 1.730652093887329,
      "learning_rate": 6.145833333333334e-06,
      "loss": 1.9487,
      "step": 295
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 3.913667678833008,
      "learning_rate": 6.166666666666667e-06,
      "loss": 2.0249,
      "step": 296
    },
    {
      "epoch": 0.61875,
      "grad_norm": 2.5604746341705322,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 1.9091,
      "step": 297
    },
    {
      "epoch": 0.6208333333333333,
      "grad_norm": 2.047619342803955,
      "learning_rate": 6.208333333333334e-06,
      "loss": 1.8933,
      "step": 298
    },
    {
      "epoch": 0.6229166666666667,
      "grad_norm": 2.2678444385528564,
      "learning_rate": 6.229166666666667e-06,
      "loss": 1.8614,
      "step": 299
    },
    {
      "epoch": 0.625,
      "grad_norm": 2.5614569187164307,
      "learning_rate": 6.25e-06,
      "loss": 2.0174,
      "step": 300
    },
    {
      "epoch": 0.6270833333333333,
      "grad_norm": 2.5827088356018066,
      "learning_rate": 6.2708333333333336e-06,
      "loss": 1.8997,
      "step": 301
    },
    {
      "epoch": 0.6291666666666667,
      "grad_norm": 2.1900813579559326,
      "learning_rate": 6.291666666666667e-06,
      "loss": 1.8632,
      "step": 302
    },
    {
      "epoch": 0.63125,
      "grad_norm": 2.126892566680908,
      "learning_rate": 6.3125e-06,
      "loss": 1.9669,
      "step": 303
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.3701188564300537,
      "learning_rate": 6.333333333333333e-06,
      "loss": 1.8642,
      "step": 304
    },
    {
      "epoch": 0.6354166666666666,
      "grad_norm": 2.812835931777954,
      "learning_rate": 6.354166666666667e-06,
      "loss": 1.8609,
      "step": 305
    },
    {
      "epoch": 0.6375,
      "grad_norm": 2.662055492401123,
      "learning_rate": 6.375e-06,
      "loss": 2.0148,
      "step": 306
    },
    {
      "epoch": 0.6395833333333333,
      "grad_norm": 2.8154637813568115,
      "learning_rate": 6.395833333333333e-06,
      "loss": 1.8095,
      "step": 307
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 2.750066041946411,
      "learning_rate": 6.416666666666667e-06,
      "loss": 1.9756,
      "step": 308
    },
    {
      "epoch": 0.64375,
      "grad_norm": 2.7688097953796387,
      "learning_rate": 6.437500000000001e-06,
      "loss": 2.0561,
      "step": 309
    },
    {
      "epoch": 0.6458333333333334,
      "grad_norm": 3.8024823665618896,
      "learning_rate": 6.458333333333334e-06,
      "loss": 1.8525,
      "step": 310
    },
    {
      "epoch": 0.6479166666666667,
      "grad_norm": 2.5757503509521484,
      "learning_rate": 6.479166666666667e-06,
      "loss": 1.882,
      "step": 311
    },
    {
      "epoch": 0.65,
      "grad_norm": 2.441953659057617,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.9922,
      "step": 312
    },
    {
      "epoch": 0.6520833333333333,
      "grad_norm": 2.293457269668579,
      "learning_rate": 6.520833333333334e-06,
      "loss": 1.8927,
      "step": 313
    },
    {
      "epoch": 0.6541666666666667,
      "grad_norm": 3.0060155391693115,
      "learning_rate": 6.541666666666667e-06,
      "loss": 1.9956,
      "step": 314
    },
    {
      "epoch": 0.65625,
      "grad_norm": 2.415240526199341,
      "learning_rate": 6.5625e-06,
      "loss": 1.9537,
      "step": 315
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 2.714740514755249,
      "learning_rate": 6.5833333333333335e-06,
      "loss": 2.0724,
      "step": 316
    },
    {
      "epoch": 0.6604166666666667,
      "grad_norm": 2.6796047687530518,
      "learning_rate": 6.604166666666667e-06,
      "loss": 1.8765,
      "step": 317
    },
    {
      "epoch": 0.6625,
      "grad_norm": 2.6019749641418457,
      "learning_rate": 6.625e-06,
      "loss": 2.0555,
      "step": 318
    },
    {
      "epoch": 0.6645833333333333,
      "grad_norm": 2.2928247451782227,
      "learning_rate": 6.645833333333333e-06,
      "loss": 1.912,
      "step": 319
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.789982557296753,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.0156,
      "step": 320
    },
    {
      "epoch": 0.66875,
      "grad_norm": 2.2915830612182617,
      "learning_rate": 6.6875e-06,
      "loss": 1.896,
      "step": 321
    },
    {
      "epoch": 0.6708333333333333,
      "grad_norm": 2.2692737579345703,
      "learning_rate": 6.708333333333333e-06,
      "loss": 1.9683,
      "step": 322
    },
    {
      "epoch": 0.6729166666666667,
      "grad_norm": 2.0629639625549316,
      "learning_rate": 6.729166666666667e-06,
      "loss": 1.9431,
      "step": 323
    },
    {
      "epoch": 0.675,
      "grad_norm": 2.520183801651001,
      "learning_rate": 6.750000000000001e-06,
      "loss": 2.0044,
      "step": 324
    },
    {
      "epoch": 0.6770833333333334,
      "grad_norm": 2.4724886417388916,
      "learning_rate": 6.770833333333334e-06,
      "loss": 2.0033,
      "step": 325
    },
    {
      "epoch": 0.6791666666666667,
      "grad_norm": 2.4983415603637695,
      "learning_rate": 6.791666666666667e-06,
      "loss": 1.9768,
      "step": 326
    },
    {
      "epoch": 0.68125,
      "grad_norm": 2.4048242568969727,
      "learning_rate": 6.8125e-06,
      "loss": 2.0085,
      "step": 327
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 2.2547218799591064,
      "learning_rate": 6.833333333333334e-06,
      "loss": 1.9401,
      "step": 328
    },
    {
      "epoch": 0.6854166666666667,
      "grad_norm": 2.3316361904144287,
      "learning_rate": 6.854166666666667e-06,
      "loss": 1.9649,
      "step": 329
    },
    {
      "epoch": 0.6875,
      "grad_norm": 1.9652899503707886,
      "learning_rate": 6.875e-06,
      "loss": 1.9866,
      "step": 330
    },
    {
      "epoch": 0.6895833333333333,
      "grad_norm": 4.378288269042969,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 1.7942,
      "step": 331
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 2.1427857875823975,
      "learning_rate": 6.916666666666667e-06,
      "loss": 1.9143,
      "step": 332
    },
    {
      "epoch": 0.69375,
      "grad_norm": 2.119568109512329,
      "learning_rate": 6.9375e-06,
      "loss": 1.9713,
      "step": 333
    },
    {
      "epoch": 0.6958333333333333,
      "grad_norm": 1.9810140132904053,
      "learning_rate": 6.958333333333333e-06,
      "loss": 1.9442,
      "step": 334
    },
    {
      "epoch": 0.6979166666666666,
      "grad_norm": 2.1782877445220947,
      "learning_rate": 6.979166666666667e-06,
      "loss": 1.8447,
      "step": 335
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.250786066055298,
      "learning_rate": 7e-06,
      "loss": 1.9839,
      "step": 336
    },
    {
      "epoch": 0.7020833333333333,
      "grad_norm": 3.692650556564331,
      "learning_rate": 7.020833333333333e-06,
      "loss": 2.0693,
      "step": 337
    },
    {
      "epoch": 0.7041666666666667,
      "grad_norm": 2.408928155899048,
      "learning_rate": 7.041666666666668e-06,
      "loss": 1.9713,
      "step": 338
    },
    {
      "epoch": 0.70625,
      "grad_norm": 3.200366258621216,
      "learning_rate": 7.062500000000001e-06,
      "loss": 1.9939,
      "step": 339
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 2.39514422416687,
      "learning_rate": 7.083333333333335e-06,
      "loss": 1.9996,
      "step": 340
    },
    {
      "epoch": 0.7104166666666667,
      "grad_norm": 2.2330617904663086,
      "learning_rate": 7.104166666666668e-06,
      "loss": 1.9099,
      "step": 341
    },
    {
      "epoch": 0.7125,
      "grad_norm": 2.2347140312194824,
      "learning_rate": 7.125e-06,
      "loss": 1.8596,
      "step": 342
    },
    {
      "epoch": 0.7145833333333333,
      "grad_norm": 1.836554765701294,
      "learning_rate": 7.145833333333334e-06,
      "loss": 1.9187,
      "step": 343
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.872352123260498,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.9261,
      "step": 344
    },
    {
      "epoch": 0.71875,
      "grad_norm": 1.978493332862854,
      "learning_rate": 7.1875e-06,
      "loss": 1.9483,
      "step": 345
    },
    {
      "epoch": 0.7208333333333333,
      "grad_norm": 1.9424595832824707,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 1.9952,
      "step": 346
    },
    {
      "epoch": 0.7229166666666667,
      "grad_norm": 1.878655195236206,
      "learning_rate": 7.229166666666667e-06,
      "loss": 1.9378,
      "step": 347
    },
    {
      "epoch": 0.725,
      "grad_norm": 2.1839728355407715,
      "learning_rate": 7.25e-06,
      "loss": 1.8907,
      "step": 348
    },
    {
      "epoch": 0.7270833333333333,
      "grad_norm": 2.4479727745056152,
      "learning_rate": 7.270833333333333e-06,
      "loss": 1.9555,
      "step": 349
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 2.0386245250701904,
      "learning_rate": 7.291666666666667e-06,
      "loss": 1.971,
      "step": 350
    },
    {
      "epoch": 0.73125,
      "grad_norm": 2.078732490539551,
      "learning_rate": 7.3125e-06,
      "loss": 1.8773,
      "step": 351
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 3.3587279319763184,
      "learning_rate": 7.333333333333333e-06,
      "loss": 1.8218,
      "step": 352
    },
    {
      "epoch": 0.7354166666666667,
      "grad_norm": 1.7917520999908447,
      "learning_rate": 7.354166666666668e-06,
      "loss": 1.956,
      "step": 353
    },
    {
      "epoch": 0.7375,
      "grad_norm": 1.944817304611206,
      "learning_rate": 7.375000000000001e-06,
      "loss": 2.0219,
      "step": 354
    },
    {
      "epoch": 0.7395833333333334,
      "grad_norm": 2.1350677013397217,
      "learning_rate": 7.395833333333335e-06,
      "loss": 1.8951,
      "step": 355
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.5405153036117554,
      "learning_rate": 7.416666666666668e-06,
      "loss": 1.9479,
      "step": 356
    },
    {
      "epoch": 0.74375,
      "grad_norm": 3.1375017166137695,
      "learning_rate": 7.437500000000001e-06,
      "loss": 2.0578,
      "step": 357
    },
    {
      "epoch": 0.7458333333333333,
      "grad_norm": 2.875251293182373,
      "learning_rate": 7.4583333333333345e-06,
      "loss": 1.8975,
      "step": 358
    },
    {
      "epoch": 0.7479166666666667,
      "grad_norm": 2.002790689468384,
      "learning_rate": 7.479166666666668e-06,
      "loss": 1.8771,
      "step": 359
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.062234401702881,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.9183,
      "step": 360
    },
    {
      "epoch": 0.7520833333333333,
      "grad_norm": 1.4805147647857666,
      "learning_rate": 7.5208333333333335e-06,
      "loss": 1.9061,
      "step": 361
    },
    {
      "epoch": 0.7541666666666667,
      "grad_norm": 1.8906422853469849,
      "learning_rate": 7.541666666666667e-06,
      "loss": 1.8733,
      "step": 362
    },
    {
      "epoch": 0.75625,
      "grad_norm": 1.643729567527771,
      "learning_rate": 7.5625e-06,
      "loss": 1.9214,
      "step": 363
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 1.7192820310592651,
      "learning_rate": 7.583333333333333e-06,
      "loss": 1.9548,
      "step": 364
    },
    {
      "epoch": 0.7604166666666666,
      "grad_norm": 2.062532663345337,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 1.8739,
      "step": 365
    },
    {
      "epoch": 0.7625,
      "grad_norm": 2.0105226039886475,
      "learning_rate": 7.625e-06,
      "loss": 1.9435,
      "step": 366
    },
    {
      "epoch": 0.7645833333333333,
      "grad_norm": 3.4199979305267334,
      "learning_rate": 7.645833333333334e-06,
      "loss": 1.8157,
      "step": 367
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 2.197448492050171,
      "learning_rate": 7.666666666666667e-06,
      "loss": 1.9233,
      "step": 368
    },
    {
      "epoch": 0.76875,
      "grad_norm": 1.9707907438278198,
      "learning_rate": 7.6875e-06,
      "loss": 1.983,
      "step": 369
    },
    {
      "epoch": 0.7708333333333334,
      "grad_norm": 1.8391448259353638,
      "learning_rate": 7.708333333333334e-06,
      "loss": 1.92,
      "step": 370
    },
    {
      "epoch": 0.7729166666666667,
      "grad_norm": 1.7130945920944214,
      "learning_rate": 7.729166666666667e-06,
      "loss": 1.9186,
      "step": 371
    },
    {
      "epoch": 0.775,
      "grad_norm": 3.092482089996338,
      "learning_rate": 7.75e-06,
      "loss": 1.9811,
      "step": 372
    },
    {
      "epoch": 0.7770833333333333,
      "grad_norm": 2.120124101638794,
      "learning_rate": 7.770833333333334e-06,
      "loss": 1.9811,
      "step": 373
    },
    {
      "epoch": 0.7791666666666667,
      "grad_norm": 2.4088916778564453,
      "learning_rate": 7.791666666666667e-06,
      "loss": 1.9767,
      "step": 374
    },
    {
      "epoch": 0.78125,
      "grad_norm": 2.309558153152466,
      "learning_rate": 7.8125e-06,
      "loss": 1.8216,
      "step": 375
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 2.0857598781585693,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.9516,
      "step": 376
    },
    {
      "epoch": 0.7854166666666667,
      "grad_norm": 1.9991323947906494,
      "learning_rate": 7.854166666666667e-06,
      "loss": 1.9264,
      "step": 377
    },
    {
      "epoch": 0.7875,
      "grad_norm": 2.026289939880371,
      "learning_rate": 7.875e-06,
      "loss": 1.8872,
      "step": 378
    },
    {
      "epoch": 0.7895833333333333,
      "grad_norm": 2.5416059494018555,
      "learning_rate": 7.895833333333333e-06,
      "loss": 1.8943,
      "step": 379
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 2.088346481323242,
      "learning_rate": 7.916666666666667e-06,
      "loss": 1.8696,
      "step": 380
    },
    {
      "epoch": 0.79375,
      "grad_norm": 1.9497150182724,
      "learning_rate": 7.9375e-06,
      "loss": 1.967,
      "step": 381
    },
    {
      "epoch": 0.7958333333333333,
      "grad_norm": 1.984463095664978,
      "learning_rate": 7.958333333333333e-06,
      "loss": 1.9983,
      "step": 382
    },
    {
      "epoch": 0.7979166666666667,
      "grad_norm": 2.5847599506378174,
      "learning_rate": 7.979166666666668e-06,
      "loss": 2.0365,
      "step": 383
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.0131874084472656,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9248,
      "step": 384
    },
    {
      "epoch": 0.8020833333333334,
      "grad_norm": 2.3451294898986816,
      "learning_rate": 8.020833333333335e-06,
      "loss": 1.9017,
      "step": 385
    },
    {
      "epoch": 0.8041666666666667,
      "grad_norm": 2.1853044033050537,
      "learning_rate": 8.041666666666668e-06,
      "loss": 2.0141,
      "step": 386
    },
    {
      "epoch": 0.80625,
      "grad_norm": 3.125521421432495,
      "learning_rate": 8.062500000000001e-06,
      "loss": 1.9858,
      "step": 387
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 2.9561851024627686,
      "learning_rate": 8.083333333333334e-06,
      "loss": 1.9553,
      "step": 388
    },
    {
      "epoch": 0.8104166666666667,
      "grad_norm": 2.2265894412994385,
      "learning_rate": 8.104166666666668e-06,
      "loss": 1.8757,
      "step": 389
    },
    {
      "epoch": 0.8125,
      "grad_norm": 3.025200605392456,
      "learning_rate": 8.125000000000001e-06,
      "loss": 1.9598,
      "step": 390
    },
    {
      "epoch": 0.8145833333333333,
      "grad_norm": 3.588319778442383,
      "learning_rate": 8.145833333333334e-06,
      "loss": 1.8099,
      "step": 391
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.780010461807251,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.933,
      "step": 392
    },
    {
      "epoch": 0.81875,
      "grad_norm": 2.4813098907470703,
      "learning_rate": 8.1875e-06,
      "loss": 1.8739,
      "step": 393
    },
    {
      "epoch": 0.8208333333333333,
      "grad_norm": 2.6654679775238037,
      "learning_rate": 8.208333333333334e-06,
      "loss": 1.8741,
      "step": 394
    },
    {
      "epoch": 0.8229166666666666,
      "grad_norm": 2.364204168319702,
      "learning_rate": 8.229166666666667e-06,
      "loss": 1.9165,
      "step": 395
    },
    {
      "epoch": 0.825,
      "grad_norm": 2.3084006309509277,
      "learning_rate": 8.25e-06,
      "loss": 1.9259,
      "step": 396
    },
    {
      "epoch": 0.8270833333333333,
      "grad_norm": 2.3557353019714355,
      "learning_rate": 8.270833333333334e-06,
      "loss": 2.0113,
      "step": 397
    },
    {
      "epoch": 0.8291666666666667,
      "grad_norm": 3.0592455863952637,
      "learning_rate": 8.291666666666667e-06,
      "loss": 1.9614,
      "step": 398
    },
    {
      "epoch": 0.83125,
      "grad_norm": 2.5061662197113037,
      "learning_rate": 8.3125e-06,
      "loss": 1.8544,
      "step": 399
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.4533064365386963,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.9939,
      "step": 400
    },
    {
      "epoch": 0.8354166666666667,
      "grad_norm": 1.9671192169189453,
      "learning_rate": 8.354166666666667e-06,
      "loss": 1.9293,
      "step": 401
    },
    {
      "epoch": 0.8375,
      "grad_norm": 2.297431707382202,
      "learning_rate": 8.375e-06,
      "loss": 1.8417,
      "step": 402
    },
    {
      "epoch": 0.8395833333333333,
      "grad_norm": 2.3937785625457764,
      "learning_rate": 8.395833333333334e-06,
      "loss": 1.8477,
      "step": 403
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 2.198958158493042,
      "learning_rate": 8.416666666666667e-06,
      "loss": 1.8807,
      "step": 404
    },
    {
      "epoch": 0.84375,
      "grad_norm": 3.846808671951294,
      "learning_rate": 8.4375e-06,
      "loss": 1.983,
      "step": 405
    },
    {
      "epoch": 0.8458333333333333,
      "grad_norm": 2.3106722831726074,
      "learning_rate": 8.458333333333333e-06,
      "loss": 1.9765,
      "step": 406
    },
    {
      "epoch": 0.8479166666666667,
      "grad_norm": 3.2451775074005127,
      "learning_rate": 8.479166666666667e-06,
      "loss": 1.9076,
      "step": 407
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.9123587608337402,
      "learning_rate": 8.5e-06,
      "loss": 1.9559,
      "step": 408
    },
    {
      "epoch": 0.8520833333333333,
      "grad_norm": 1.9071195125579834,
      "learning_rate": 8.520833333333333e-06,
      "loss": 1.978,
      "step": 409
    },
    {
      "epoch": 0.8541666666666666,
      "grad_norm": 1.9764184951782227,
      "learning_rate": 8.541666666666666e-06,
      "loss": 1.9385,
      "step": 410
    },
    {
      "epoch": 0.85625,
      "grad_norm": 2.1670868396759033,
      "learning_rate": 8.5625e-06,
      "loss": 1.8938,
      "step": 411
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 3.671736478805542,
      "learning_rate": 8.583333333333333e-06,
      "loss": 1.8379,
      "step": 412
    },
    {
      "epoch": 0.8604166666666667,
      "grad_norm": 1.7840386629104614,
      "learning_rate": 8.604166666666668e-06,
      "loss": 1.9278,
      "step": 413
    },
    {
      "epoch": 0.8625,
      "grad_norm": 1.9531755447387695,
      "learning_rate": 8.625000000000001e-06,
      "loss": 1.9944,
      "step": 414
    },
    {
      "epoch": 0.8645833333333334,
      "grad_norm": 2.387019634246826,
      "learning_rate": 8.645833333333335e-06,
      "loss": 1.8599,
      "step": 415
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.8335912227630615,
      "learning_rate": 8.666666666666668e-06,
      "loss": 1.9212,
      "step": 416
    },
    {
      "epoch": 0.86875,
      "grad_norm": 1.778045892715454,
      "learning_rate": 8.687500000000001e-06,
      "loss": 1.9302,
      "step": 417
    },
    {
      "epoch": 0.8708333333333333,
      "grad_norm": 3.5218589305877686,
      "learning_rate": 8.708333333333334e-06,
      "loss": 1.8705,
      "step": 418
    },
    {
      "epoch": 0.8729166666666667,
      "grad_norm": 2.526855707168579,
      "learning_rate": 8.729166666666668e-06,
      "loss": 1.8373,
      "step": 419
    },
    {
      "epoch": 0.875,
      "grad_norm": 2.867689609527588,
      "learning_rate": 8.750000000000001e-06,
      "loss": 1.9301,
      "step": 420
    },
    {
      "epoch": 0.8770833333333333,
      "grad_norm": 2.332643508911133,
      "learning_rate": 8.770833333333334e-06,
      "loss": 1.9473,
      "step": 421
    },
    {
      "epoch": 0.8791666666666667,
      "grad_norm": 2.803417444229126,
      "learning_rate": 8.791666666666667e-06,
      "loss": 1.9552,
      "step": 422
    },
    {
      "epoch": 0.88125,
      "grad_norm": 2.178041696548462,
      "learning_rate": 8.8125e-06,
      "loss": 1.9089,
      "step": 423
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 2.666480541229248,
      "learning_rate": 8.833333333333334e-06,
      "loss": 1.8804,
      "step": 424
    },
    {
      "epoch": 0.8854166666666666,
      "grad_norm": 1.9336738586425781,
      "learning_rate": 8.854166666666667e-06,
      "loss": 1.9304,
      "step": 425
    },
    {
      "epoch": 0.8875,
      "grad_norm": 3.1005642414093018,
      "learning_rate": 8.875e-06,
      "loss": 1.882,
      "step": 426
    },
    {
      "epoch": 0.8895833333333333,
      "grad_norm": 2.1296818256378174,
      "learning_rate": 8.895833333333334e-06,
      "loss": 1.9388,
      "step": 427
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 2.2421162128448486,
      "learning_rate": 8.916666666666667e-06,
      "loss": 1.9368,
      "step": 428
    },
    {
      "epoch": 0.89375,
      "grad_norm": 3.2823519706726074,
      "learning_rate": 8.9375e-06,
      "loss": 1.9429,
      "step": 429
    },
    {
      "epoch": 0.8958333333333334,
      "grad_norm": 2.41601300239563,
      "learning_rate": 8.958333333333334e-06,
      "loss": 1.8791,
      "step": 430
    },
    {
      "epoch": 0.8979166666666667,
      "grad_norm": 2.786959171295166,
      "learning_rate": 8.979166666666667e-06,
      "loss": 1.8723,
      "step": 431
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.1478869915008545,
      "learning_rate": 9e-06,
      "loss": 1.9003,
      "step": 432
    },
    {
      "epoch": 0.9020833333333333,
      "grad_norm": 2.4939544200897217,
      "learning_rate": 9.020833333333334e-06,
      "loss": 1.9609,
      "step": 433
    },
    {
      "epoch": 0.9041666666666667,
      "grad_norm": 2.6142172813415527,
      "learning_rate": 9.041666666666667e-06,
      "loss": 2.0389,
      "step": 434
    },
    {
      "epoch": 0.90625,
      "grad_norm": 2.2247426509857178,
      "learning_rate": 9.0625e-06,
      "loss": 1.9377,
      "step": 435
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 3.047226667404175,
      "learning_rate": 9.083333333333333e-06,
      "loss": 1.8386,
      "step": 436
    },
    {
      "epoch": 0.9104166666666667,
      "grad_norm": 3.1124765872955322,
      "learning_rate": 9.104166666666667e-06,
      "loss": 1.9427,
      "step": 437
    },
    {
      "epoch": 0.9125,
      "grad_norm": 1.5476534366607666,
      "learning_rate": 9.125e-06,
      "loss": 1.9153,
      "step": 438
    },
    {
      "epoch": 0.9145833333333333,
      "grad_norm": 2.118931770324707,
      "learning_rate": 9.145833333333333e-06,
      "loss": 1.9562,
      "step": 439
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 3.014188289642334,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.8257,
      "step": 440
    },
    {
      "epoch": 0.91875,
      "grad_norm": 3.3745079040527344,
      "learning_rate": 9.1875e-06,
      "loss": 1.8304,
      "step": 441
    },
    {
      "epoch": 0.9208333333333333,
      "grad_norm": 3.294330596923828,
      "learning_rate": 9.208333333333333e-06,
      "loss": 1.9924,
      "step": 442
    },
    {
      "epoch": 0.9229166666666667,
      "grad_norm": 4.0276780128479,
      "learning_rate": 9.229166666666668e-06,
      "loss": 1.8786,
      "step": 443
    },
    {
      "epoch": 0.925,
      "grad_norm": 2.2814793586730957,
      "learning_rate": 9.250000000000001e-06,
      "loss": 1.939,
      "step": 444
    },
    {
      "epoch": 0.9270833333333334,
      "grad_norm": 2.984734535217285,
      "learning_rate": 9.270833333333334e-06,
      "loss": 1.8228,
      "step": 445
    },
    {
      "epoch": 0.9291666666666667,
      "grad_norm": 2.9730217456817627,
      "learning_rate": 9.291666666666668e-06,
      "loss": 1.8574,
      "step": 446
    },
    {
      "epoch": 0.93125,
      "grad_norm": 2.9107582569122314,
      "learning_rate": 9.312500000000001e-06,
      "loss": 1.9355,
      "step": 447
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.4466540813446045,
      "learning_rate": 9.333333333333334e-06,
      "loss": 1.9554,
      "step": 448
    },
    {
      "epoch": 0.9354166666666667,
      "grad_norm": 2.942814350128174,
      "learning_rate": 9.354166666666668e-06,
      "loss": 1.9445,
      "step": 449
    },
    {
      "epoch": 0.9375,
      "grad_norm": 2.9586877822875977,
      "learning_rate": 9.375000000000001e-06,
      "loss": 1.8989,
      "step": 450
    },
    {
      "epoch": 0.9395833333333333,
      "grad_norm": 2.871242046356201,
      "learning_rate": 9.395833333333334e-06,
      "loss": 1.8333,
      "step": 451
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 2.381859064102173,
      "learning_rate": 9.416666666666667e-06,
      "loss": 1.9094,
      "step": 452
    },
    {
      "epoch": 0.94375,
      "grad_norm": 2.848283529281616,
      "learning_rate": 9.4375e-06,
      "loss": 1.9008,
      "step": 453
    },
    {
      "epoch": 0.9458333333333333,
      "grad_norm": 3.2651169300079346,
      "learning_rate": 9.458333333333334e-06,
      "loss": 1.8077,
      "step": 454
    },
    {
      "epoch": 0.9479166666666666,
      "grad_norm": 2.916912794113159,
      "learning_rate": 9.479166666666667e-06,
      "loss": 1.9496,
      "step": 455
    },
    {
      "epoch": 0.95,
      "grad_norm": 3.27787184715271,
      "learning_rate": 9.5e-06,
      "loss": 1.8057,
      "step": 456
    },
    {
      "epoch": 0.9520833333333333,
      "grad_norm": 3.9563183784484863,
      "learning_rate": 9.520833333333334e-06,
      "loss": 1.804,
      "step": 457
    },
    {
      "epoch": 0.9541666666666667,
      "grad_norm": 3.298861026763916,
      "learning_rate": 9.541666666666669e-06,
      "loss": 1.8104,
      "step": 458
    },
    {
      "epoch": 0.95625,
      "grad_norm": 2.031202554702759,
      "learning_rate": 9.562500000000002e-06,
      "loss": 1.9307,
      "step": 459
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 3.558314561843872,
      "learning_rate": 9.583333333333335e-06,
      "loss": 2.0196,
      "step": 460
    },
    {
      "epoch": 0.9604166666666667,
      "grad_norm": 3.946951150894165,
      "learning_rate": 9.604166666666669e-06,
      "loss": 1.7687,
      "step": 461
    },
    {
      "epoch": 0.9625,
      "grad_norm": 4.042377948760986,
      "learning_rate": 9.625e-06,
      "loss": 1.8551,
      "step": 462
    },
    {
      "epoch": 0.9645833333333333,
      "grad_norm": 5.3062968254089355,
      "learning_rate": 9.645833333333333e-06,
      "loss": 1.6861,
      "step": 463
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 5.212461948394775,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.9386,
      "step": 464
    },
    {
      "epoch": 0.96875,
      "grad_norm": 2.7018473148345947,
      "learning_rate": 9.6875e-06,
      "loss": 1.8702,
      "step": 465
    },
    {
      "epoch": 0.9708333333333333,
      "grad_norm": 4.50298547744751,
      "learning_rate": 9.708333333333333e-06,
      "loss": 1.7242,
      "step": 466
    },
    {
      "epoch": 0.9729166666666667,
      "grad_norm": 3.341373920440674,
      "learning_rate": 9.729166666666667e-06,
      "loss": 1.8315,
      "step": 467
    },
    {
      "epoch": 0.975,
      "grad_norm": 4.250161170959473,
      "learning_rate": 9.75e-06,
      "loss": 1.7718,
      "step": 468
    },
    {
      "epoch": 0.9770833333333333,
      "grad_norm": 2.8732004165649414,
      "learning_rate": 9.770833333333333e-06,
      "loss": 1.9624,
      "step": 469
    },
    {
      "epoch": 0.9791666666666666,
      "grad_norm": 4.4935526847839355,
      "learning_rate": 9.791666666666666e-06,
      "loss": 1.7134,
      "step": 470
    },
    {
      "epoch": 0.98125,
      "grad_norm": 2.8006129264831543,
      "learning_rate": 9.8125e-06,
      "loss": 1.8971,
      "step": 471
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 4.489307403564453,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.901,
      "step": 472
    },
    {
      "epoch": 0.9854166666666667,
      "grad_norm": 3.762315034866333,
      "learning_rate": 9.854166666666668e-06,
      "loss": 1.8939,
      "step": 473
    },
    {
      "epoch": 0.9875,
      "grad_norm": 3.8374457359313965,
      "learning_rate": 9.875000000000001e-06,
      "loss": 1.7495,
      "step": 474
    },
    {
      "epoch": 0.9895833333333334,
      "grad_norm": 4.136640548706055,
      "learning_rate": 9.895833333333334e-06,
      "loss": 1.6997,
      "step": 475
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 6.038689613342285,
      "learning_rate": 9.916666666666668e-06,
      "loss": 1.9304,
      "step": 476
    },
    {
      "epoch": 0.99375,
      "grad_norm": 3.2505555152893066,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.8926,
      "step": 477
    },
    {
      "epoch": 0.9958333333333333,
      "grad_norm": 5.717676639556885,
      "learning_rate": 9.958333333333334e-06,
      "loss": 1.9017,
      "step": 478
    },
    {
      "epoch": 0.9979166666666667,
      "grad_norm": 5.054263114929199,
      "learning_rate": 9.979166666666668e-06,
      "loss": 1.669,
      "step": 479
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.6387014389038086,
      "learning_rate": 1e-05,
      "loss": 1.9121,
      "step": 480
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.3055555555555556,
      "eval_f1": 0.16282809420064323,
      "eval_loss": 1.81491219997406,
      "eval_runtime": 36.255,
      "eval_samples_per_second": 4.965,
      "eval_steps_per_second": 2.482,
      "step": 480
    },
    {
      "epoch": 1.0020833333333334,
      "grad_norm": 3.9957940578460693,
      "learning_rate": 9.997685185185187e-06,
      "loss": 1.8223,
      "step": 481
    },
    {
      "epoch": 1.0041666666666667,
      "grad_norm": 4.82511043548584,
      "learning_rate": 9.995370370370371e-06,
      "loss": 1.836,
      "step": 482
    },
    {
      "epoch": 1.00625,
      "grad_norm": 5.271088123321533,
      "learning_rate": 9.993055555555557e-06,
      "loss": 1.8991,
      "step": 483
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 6.162159442901611,
      "learning_rate": 9.990740740740741e-06,
      "loss": 1.71,
      "step": 484
    },
    {
      "epoch": 1.0104166666666667,
      "grad_norm": 7.287150859832764,
      "learning_rate": 9.988425925925927e-06,
      "loss": 1.969,
      "step": 485
    },
    {
      "epoch": 1.0125,
      "grad_norm": 4.839508056640625,
      "learning_rate": 9.986111111111111e-06,
      "loss": 1.9532,
      "step": 486
    },
    {
      "epoch": 1.0145833333333334,
      "grad_norm": 3.482024669647217,
      "learning_rate": 9.983796296296297e-06,
      "loss": 1.9416,
      "step": 487
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 6.045909404754639,
      "learning_rate": 9.981481481481482e-06,
      "loss": 2.0125,
      "step": 488
    },
    {
      "epoch": 1.01875,
      "grad_norm": 3.6878304481506348,
      "learning_rate": 9.979166666666668e-06,
      "loss": 1.8755,
      "step": 489
    },
    {
      "epoch": 1.0208333333333333,
      "grad_norm": 2.6828999519348145,
      "learning_rate": 9.976851851851853e-06,
      "loss": 1.8574,
      "step": 490
    },
    {
      "epoch": 1.0229166666666667,
      "grad_norm": 2.6766555309295654,
      "learning_rate": 9.974537037037038e-06,
      "loss": 1.9029,
      "step": 491
    },
    {
      "epoch": 1.025,
      "grad_norm": 5.905770778656006,
      "learning_rate": 9.972222222222224e-06,
      "loss": 1.9696,
      "step": 492
    },
    {
      "epoch": 1.0270833333333333,
      "grad_norm": 4.596978664398193,
      "learning_rate": 9.969907407407408e-06,
      "loss": 1.8437,
      "step": 493
    },
    {
      "epoch": 1.0291666666666666,
      "grad_norm": 3.7069826126098633,
      "learning_rate": 9.967592592592594e-06,
      "loss": 1.8519,
      "step": 494
    },
    {
      "epoch": 1.03125,
      "grad_norm": 2.9039828777313232,
      "learning_rate": 9.965277777777778e-06,
      "loss": 1.9705,
      "step": 495
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 3.211435317993164,
      "learning_rate": 9.962962962962964e-06,
      "loss": 1.9396,
      "step": 496
    },
    {
      "epoch": 1.0354166666666667,
      "grad_norm": 9.780223846435547,
      "learning_rate": 9.960648148148148e-06,
      "loss": 1.5244,
      "step": 497
    },
    {
      "epoch": 1.0375,
      "grad_norm": 4.749242305755615,
      "learning_rate": 9.958333333333334e-06,
      "loss": 1.8372,
      "step": 498
    },
    {
      "epoch": 1.0395833333333333,
      "grad_norm": 6.0331130027771,
      "learning_rate": 9.95601851851852e-06,
      "loss": 1.7148,
      "step": 499
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 3.7071900367736816,
      "learning_rate": 9.953703703703704e-06,
      "loss": 1.9079,
      "step": 500
    },
    {
      "epoch": 1.04375,
      "grad_norm": 5.206056118011475,
      "learning_rate": 9.95138888888889e-06,
      "loss": 1.8481,
      "step": 501
    },
    {
      "epoch": 1.0458333333333334,
      "grad_norm": 3.0550131797790527,
      "learning_rate": 9.949074074074075e-06,
      "loss": 1.921,
      "step": 502
    },
    {
      "epoch": 1.0479166666666666,
      "grad_norm": 6.710261821746826,
      "learning_rate": 9.94675925925926e-06,
      "loss": 1.7429,
      "step": 503
    },
    {
      "epoch": 1.05,
      "grad_norm": 5.953681468963623,
      "learning_rate": 9.944444444444445e-06,
      "loss": 1.8628,
      "step": 504
    },
    {
      "epoch": 1.0520833333333333,
      "grad_norm": 4.84552526473999,
      "learning_rate": 9.942129629629629e-06,
      "loss": 1.7804,
      "step": 505
    },
    {
      "epoch": 1.0541666666666667,
      "grad_norm": 5.536280155181885,
      "learning_rate": 9.939814814814815e-06,
      "loss": 1.9071,
      "step": 506
    },
    {
      "epoch": 1.05625,
      "grad_norm": 5.123532295227051,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.7412,
      "step": 507
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 4.363223552703857,
      "learning_rate": 9.935185185185185e-06,
      "loss": 1.829,
      "step": 508
    },
    {
      "epoch": 1.0604166666666666,
      "grad_norm": 6.415248394012451,
      "learning_rate": 9.932870370370371e-06,
      "loss": 1.6421,
      "step": 509
    },
    {
      "epoch": 1.0625,
      "grad_norm": 4.71692419052124,
      "learning_rate": 9.930555555555557e-06,
      "loss": 1.922,
      "step": 510
    },
    {
      "epoch": 1.0645833333333334,
      "grad_norm": 5.0998311042785645,
      "learning_rate": 9.928240740740741e-06,
      "loss": 1.7121,
      "step": 511
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 6.853371620178223,
      "learning_rate": 9.925925925925927e-06,
      "loss": 1.8206,
      "step": 512
    },
    {
      "epoch": 1.06875,
      "grad_norm": 4.425285816192627,
      "learning_rate": 9.923611111111112e-06,
      "loss": 1.7957,
      "step": 513
    },
    {
      "epoch": 1.0708333333333333,
      "grad_norm": 13.00149917602539,
      "learning_rate": 9.921296296296296e-06,
      "loss": 1.488,
      "step": 514
    },
    {
      "epoch": 1.0729166666666667,
      "grad_norm": 9.333695411682129,
      "learning_rate": 9.918981481481482e-06,
      "loss": 1.9488,
      "step": 515
    },
    {
      "epoch": 1.075,
      "grad_norm": 16.238433837890625,
      "learning_rate": 9.916666666666668e-06,
      "loss": 1.4415,
      "step": 516
    },
    {
      "epoch": 1.0770833333333334,
      "grad_norm": 4.884713172912598,
      "learning_rate": 9.914351851851852e-06,
      "loss": 1.8282,
      "step": 517
    },
    {
      "epoch": 1.0791666666666666,
      "grad_norm": 4.459047794342041,
      "learning_rate": 9.912037037037038e-06,
      "loss": 1.7382,
      "step": 518
    },
    {
      "epoch": 1.08125,
      "grad_norm": 6.032847881317139,
      "learning_rate": 9.909722222222224e-06,
      "loss": 2.0719,
      "step": 519
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 3.4831290245056152,
      "learning_rate": 9.907407407407408e-06,
      "loss": 1.9404,
      "step": 520
    },
    {
      "epoch": 1.0854166666666667,
      "grad_norm": 5.62066125869751,
      "learning_rate": 9.905092592592594e-06,
      "loss": 1.5939,
      "step": 521
    },
    {
      "epoch": 1.0875,
      "grad_norm": 11.382500648498535,
      "learning_rate": 9.902777777777778e-06,
      "loss": 2.1294,
      "step": 522
    },
    {
      "epoch": 1.0895833333333333,
      "grad_norm": 6.313187122344971,
      "learning_rate": 9.900462962962963e-06,
      "loss": 2.1261,
      "step": 523
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 7.109322547912598,
      "learning_rate": 9.898148148148148e-06,
      "loss": 1.8011,
      "step": 524
    },
    {
      "epoch": 1.09375,
      "grad_norm": 6.267740726470947,
      "learning_rate": 9.895833333333334e-06,
      "loss": 1.9473,
      "step": 525
    },
    {
      "epoch": 1.0958333333333334,
      "grad_norm": 6.748660087585449,
      "learning_rate": 9.893518518518519e-06,
      "loss": 1.7308,
      "step": 526
    },
    {
      "epoch": 1.0979166666666667,
      "grad_norm": 7.327108383178711,
      "learning_rate": 9.891203703703705e-06,
      "loss": 1.784,
      "step": 527
    },
    {
      "epoch": 1.1,
      "grad_norm": 3.7364895343780518,
      "learning_rate": 9.88888888888889e-06,
      "loss": 1.8779,
      "step": 528
    },
    {
      "epoch": 1.1020833333333333,
      "grad_norm": 6.6053009033203125,
      "learning_rate": 9.886574074074075e-06,
      "loss": 1.9554,
      "step": 529
    },
    {
      "epoch": 1.1041666666666667,
      "grad_norm": 6.762101173400879,
      "learning_rate": 9.88425925925926e-06,
      "loss": 1.8191,
      "step": 530
    },
    {
      "epoch": 1.10625,
      "grad_norm": 4.766901016235352,
      "learning_rate": 9.881944444444445e-06,
      "loss": 1.6929,
      "step": 531
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 8.623682975769043,
      "learning_rate": 9.87962962962963e-06,
      "loss": 1.9649,
      "step": 532
    },
    {
      "epoch": 1.1104166666666666,
      "grad_norm": 5.640347957611084,
      "learning_rate": 9.877314814814815e-06,
      "loss": 1.9768,
      "step": 533
    },
    {
      "epoch": 1.1125,
      "grad_norm": 10.560054779052734,
      "learning_rate": 9.875000000000001e-06,
      "loss": 2.1983,
      "step": 534
    },
    {
      "epoch": 1.1145833333333333,
      "grad_norm": 6.9880266189575195,
      "learning_rate": 9.872685185185185e-06,
      "loss": 1.5029,
      "step": 535
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 6.03161096572876,
      "learning_rate": 9.870370370370371e-06,
      "loss": 1.9606,
      "step": 536
    },
    {
      "epoch": 1.11875,
      "grad_norm": 6.219241619110107,
      "learning_rate": 9.868055555555557e-06,
      "loss": 1.8939,
      "step": 537
    },
    {
      "epoch": 1.1208333333333333,
      "grad_norm": 7.680005073547363,
      "learning_rate": 9.865740740740742e-06,
      "loss": 1.9168,
      "step": 538
    },
    {
      "epoch": 1.1229166666666666,
      "grad_norm": 3.7524220943450928,
      "learning_rate": 9.863425925925928e-06,
      "loss": 2.0279,
      "step": 539
    },
    {
      "epoch": 1.125,
      "grad_norm": 6.014187812805176,
      "learning_rate": 9.861111111111112e-06,
      "loss": 1.7289,
      "step": 540
    },
    {
      "epoch": 1.1270833333333332,
      "grad_norm": 7.165669918060303,
      "learning_rate": 9.858796296296298e-06,
      "loss": 1.3257,
      "step": 541
    },
    {
      "epoch": 1.1291666666666667,
      "grad_norm": 4.568291187286377,
      "learning_rate": 9.856481481481482e-06,
      "loss": 1.873,
      "step": 542
    },
    {
      "epoch": 1.13125,
      "grad_norm": 6.363565444946289,
      "learning_rate": 9.854166666666668e-06,
      "loss": 1.6482,
      "step": 543
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 7.158909320831299,
      "learning_rate": 9.851851851851852e-06,
      "loss": 1.4412,
      "step": 544
    },
    {
      "epoch": 1.1354166666666667,
      "grad_norm": 4.5999579429626465,
      "learning_rate": 9.849537037037038e-06,
      "loss": 1.7792,
      "step": 545
    },
    {
      "epoch": 1.1375,
      "grad_norm": 5.560220718383789,
      "learning_rate": 9.847222222222224e-06,
      "loss": 1.6991,
      "step": 546
    },
    {
      "epoch": 1.1395833333333334,
      "grad_norm": 9.544918060302734,
      "learning_rate": 9.844907407407408e-06,
      "loss": 2.2715,
      "step": 547
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 8.637630462646484,
      "learning_rate": 9.842592592592594e-06,
      "loss": 1.7003,
      "step": 548
    },
    {
      "epoch": 1.14375,
      "grad_norm": 20.41930389404297,
      "learning_rate": 9.840277777777778e-06,
      "loss": 1.6387,
      "step": 549
    },
    {
      "epoch": 1.1458333333333333,
      "grad_norm": 7.477566242218018,
      "learning_rate": 9.837962962962964e-06,
      "loss": 1.8164,
      "step": 550
    },
    {
      "epoch": 1.1479166666666667,
      "grad_norm": 9.934835433959961,
      "learning_rate": 9.835648148148149e-06,
      "loss": 1.2589,
      "step": 551
    },
    {
      "epoch": 1.15,
      "grad_norm": 7.484676361083984,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.6268,
      "step": 552
    },
    {
      "epoch": 1.1520833333333333,
      "grad_norm": 8.335282325744629,
      "learning_rate": 9.831018518518519e-06,
      "loss": 1.8808,
      "step": 553
    },
    {
      "epoch": 1.1541666666666668,
      "grad_norm": 5.433446884155273,
      "learning_rate": 9.828703703703705e-06,
      "loss": 1.5792,
      "step": 554
    },
    {
      "epoch": 1.15625,
      "grad_norm": 6.404886245727539,
      "learning_rate": 9.826388888888889e-06,
      "loss": 1.7268,
      "step": 555
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 6.680817604064941,
      "learning_rate": 9.824074074074075e-06,
      "loss": 1.9123,
      "step": 556
    },
    {
      "epoch": 1.1604166666666667,
      "grad_norm": 7.185735702514648,
      "learning_rate": 9.821759259259261e-06,
      "loss": 1.8948,
      "step": 557
    },
    {
      "epoch": 1.1625,
      "grad_norm": 10.562202453613281,
      "learning_rate": 9.819444444444445e-06,
      "loss": 1.8017,
      "step": 558
    },
    {
      "epoch": 1.1645833333333333,
      "grad_norm": 7.236743450164795,
      "learning_rate": 9.817129629629631e-06,
      "loss": 1.572,
      "step": 559
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 8.321598052978516,
      "learning_rate": 9.814814814814815e-06,
      "loss": 1.9949,
      "step": 560
    },
    {
      "epoch": 1.16875,
      "grad_norm": 35.14738082885742,
      "learning_rate": 9.8125e-06,
      "loss": 1.7215,
      "step": 561
    },
    {
      "epoch": 1.1708333333333334,
      "grad_norm": 7.668934345245361,
      "learning_rate": 9.810185185185186e-06,
      "loss": 2.151,
      "step": 562
    },
    {
      "epoch": 1.1729166666666666,
      "grad_norm": 8.58651351928711,
      "learning_rate": 9.807870370370372e-06,
      "loss": 1.6168,
      "step": 563
    },
    {
      "epoch": 1.175,
      "grad_norm": 4.863114356994629,
      "learning_rate": 9.805555555555556e-06,
      "loss": 1.8009,
      "step": 564
    },
    {
      "epoch": 1.1770833333333333,
      "grad_norm": 6.208588123321533,
      "learning_rate": 9.803240740740742e-06,
      "loss": 1.9224,
      "step": 565
    },
    {
      "epoch": 1.1791666666666667,
      "grad_norm": 8.275675773620605,
      "learning_rate": 9.800925925925928e-06,
      "loss": 1.6329,
      "step": 566
    },
    {
      "epoch": 1.18125,
      "grad_norm": 9.130366325378418,
      "learning_rate": 9.798611111111112e-06,
      "loss": 2.1039,
      "step": 567
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 7.743878364562988,
      "learning_rate": 9.796296296296298e-06,
      "loss": 1.8378,
      "step": 568
    },
    {
      "epoch": 1.1854166666666668,
      "grad_norm": 5.089112281799316,
      "learning_rate": 9.793981481481482e-06,
      "loss": 1.6882,
      "step": 569
    },
    {
      "epoch": 1.1875,
      "grad_norm": 8.366334915161133,
      "learning_rate": 9.791666666666666e-06,
      "loss": 1.8154,
      "step": 570
    },
    {
      "epoch": 1.1895833333333332,
      "grad_norm": 8.717679023742676,
      "learning_rate": 9.789351851851852e-06,
      "loss": 1.7138,
      "step": 571
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 6.941291332244873,
      "learning_rate": 9.787037037037038e-06,
      "loss": 1.8821,
      "step": 572
    },
    {
      "epoch": 1.19375,
      "grad_norm": 6.956091403961182,
      "learning_rate": 9.784722222222223e-06,
      "loss": 1.6033,
      "step": 573
    },
    {
      "epoch": 1.1958333333333333,
      "grad_norm": 32.79336166381836,
      "learning_rate": 9.782407407407408e-06,
      "loss": 1.8655,
      "step": 574
    },
    {
      "epoch": 1.1979166666666667,
      "grad_norm": 5.742950916290283,
      "learning_rate": 9.780092592592594e-06,
      "loss": 1.5505,
      "step": 575
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.779974937438965,
      "learning_rate": 9.777777777777779e-06,
      "loss": 1.664,
      "step": 576
    },
    {
      "epoch": 1.2020833333333334,
      "grad_norm": 7.3268723487854,
      "learning_rate": 9.775462962962965e-06,
      "loss": 1.8058,
      "step": 577
    },
    {
      "epoch": 1.2041666666666666,
      "grad_norm": 12.297704696655273,
      "learning_rate": 9.773148148148149e-06,
      "loss": 1.947,
      "step": 578
    },
    {
      "epoch": 1.20625,
      "grad_norm": 6.788417816162109,
      "learning_rate": 9.770833333333333e-06,
      "loss": 1.627,
      "step": 579
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 5.52682638168335,
      "learning_rate": 9.768518518518519e-06,
      "loss": 1.7198,
      "step": 580
    },
    {
      "epoch": 1.2104166666666667,
      "grad_norm": 17.729366302490234,
      "learning_rate": 9.766203703703705e-06,
      "loss": 1.9303,
      "step": 581
    },
    {
      "epoch": 1.2125,
      "grad_norm": 8.486706733703613,
      "learning_rate": 9.76388888888889e-06,
      "loss": 1.9126,
      "step": 582
    },
    {
      "epoch": 1.2145833333333333,
      "grad_norm": 6.157741069793701,
      "learning_rate": 9.761574074074075e-06,
      "loss": 1.5155,
      "step": 583
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 9.520968437194824,
      "learning_rate": 9.759259259259261e-06,
      "loss": 1.7368,
      "step": 584
    },
    {
      "epoch": 1.21875,
      "grad_norm": 8.47683334350586,
      "learning_rate": 9.756944444444445e-06,
      "loss": 1.9316,
      "step": 585
    },
    {
      "epoch": 1.2208333333333332,
      "grad_norm": 9.435464859008789,
      "learning_rate": 9.754629629629631e-06,
      "loss": 1.7868,
      "step": 586
    },
    {
      "epoch": 1.2229166666666667,
      "grad_norm": 16.3041934967041,
      "learning_rate": 9.752314814814816e-06,
      "loss": 1.2794,
      "step": 587
    },
    {
      "epoch": 1.225,
      "grad_norm": 8.665135383605957,
      "learning_rate": 9.75e-06,
      "loss": 1.9773,
      "step": 588
    },
    {
      "epoch": 1.2270833333333333,
      "grad_norm": 6.480226993560791,
      "learning_rate": 9.747685185185186e-06,
      "loss": 1.8897,
      "step": 589
    },
    {
      "epoch": 1.2291666666666667,
      "grad_norm": 7.2512969970703125,
      "learning_rate": 9.745370370370372e-06,
      "loss": 1.6018,
      "step": 590
    },
    {
      "epoch": 1.23125,
      "grad_norm": 7.501052379608154,
      "learning_rate": 9.743055555555556e-06,
      "loss": 1.4813,
      "step": 591
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 9.251667022705078,
      "learning_rate": 9.740740740740742e-06,
      "loss": 1.9874,
      "step": 592
    },
    {
      "epoch": 1.2354166666666666,
      "grad_norm": 7.004453659057617,
      "learning_rate": 9.738425925925926e-06,
      "loss": 1.6849,
      "step": 593
    },
    {
      "epoch": 1.2375,
      "grad_norm": 6.859771728515625,
      "learning_rate": 9.736111111111112e-06,
      "loss": 1.5909,
      "step": 594
    },
    {
      "epoch": 1.2395833333333333,
      "grad_norm": 8.932538986206055,
      "learning_rate": 9.733796296296298e-06,
      "loss": 2.129,
      "step": 595
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 8.326435089111328,
      "learning_rate": 9.731481481481482e-06,
      "loss": 1.5981,
      "step": 596
    },
    {
      "epoch": 1.24375,
      "grad_norm": 13.255419731140137,
      "learning_rate": 9.729166666666667e-06,
      "loss": 1.8599,
      "step": 597
    },
    {
      "epoch": 1.2458333333333333,
      "grad_norm": 31.111114501953125,
      "learning_rate": 9.726851851851852e-06,
      "loss": 1.7974,
      "step": 598
    },
    {
      "epoch": 1.2479166666666668,
      "grad_norm": 8.541772842407227,
      "learning_rate": 9.724537037037037e-06,
      "loss": 2.1316,
      "step": 599
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.618897438049316,
      "learning_rate": 9.722222222222223e-06,
      "loss": 2.1445,
      "step": 600
    },
    {
      "epoch": 1.2520833333333332,
      "grad_norm": 7.345553398132324,
      "learning_rate": 9.719907407407409e-06,
      "loss": 1.5702,
      "step": 601
    },
    {
      "epoch": 1.2541666666666667,
      "grad_norm": 7.0933709144592285,
      "learning_rate": 9.717592592592593e-06,
      "loss": 1.4742,
      "step": 602
    },
    {
      "epoch": 1.25625,
      "grad_norm": 6.940958499908447,
      "learning_rate": 9.715277777777779e-06,
      "loss": 1.7247,
      "step": 603
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 8.057994842529297,
      "learning_rate": 9.712962962962965e-06,
      "loss": 2.0939,
      "step": 604
    },
    {
      "epoch": 1.2604166666666667,
      "grad_norm": 7.662418365478516,
      "learning_rate": 9.710648148148149e-06,
      "loss": 1.8721,
      "step": 605
    },
    {
      "epoch": 1.2625,
      "grad_norm": 7.420814514160156,
      "learning_rate": 9.708333333333333e-06,
      "loss": 1.6554,
      "step": 606
    },
    {
      "epoch": 1.2645833333333334,
      "grad_norm": 8.688450813293457,
      "learning_rate": 9.70601851851852e-06,
      "loss": 2.0102,
      "step": 607
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 7.622586727142334,
      "learning_rate": 9.703703703703703e-06,
      "loss": 1.5085,
      "step": 608
    },
    {
      "epoch": 1.26875,
      "grad_norm": 8.613277435302734,
      "learning_rate": 9.70138888888889e-06,
      "loss": 2.1269,
      "step": 609
    },
    {
      "epoch": 1.2708333333333333,
      "grad_norm": 15.301783561706543,
      "learning_rate": 9.699074074074075e-06,
      "loss": 1.9017,
      "step": 610
    },
    {
      "epoch": 1.2729166666666667,
      "grad_norm": 9.329584121704102,
      "learning_rate": 9.69675925925926e-06,
      "loss": 1.5522,
      "step": 611
    },
    {
      "epoch": 1.275,
      "grad_norm": 9.622467041015625,
      "learning_rate": 9.694444444444446e-06,
      "loss": 1.4927,
      "step": 612
    },
    {
      "epoch": 1.2770833333333333,
      "grad_norm": 10.118742942810059,
      "learning_rate": 9.692129629629631e-06,
      "loss": 1.818,
      "step": 613
    },
    {
      "epoch": 1.2791666666666668,
      "grad_norm": 8.405669212341309,
      "learning_rate": 9.689814814814816e-06,
      "loss": 1.4944,
      "step": 614
    },
    {
      "epoch": 1.28125,
      "grad_norm": 9.76342487335205,
      "learning_rate": 9.6875e-06,
      "loss": 1.888,
      "step": 615
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 8.835016250610352,
      "learning_rate": 9.685185185185186e-06,
      "loss": 1.9868,
      "step": 616
    },
    {
      "epoch": 1.2854166666666667,
      "grad_norm": 11.383675575256348,
      "learning_rate": 9.68287037037037e-06,
      "loss": 1.9225,
      "step": 617
    },
    {
      "epoch": 1.2875,
      "grad_norm": 7.40202522277832,
      "learning_rate": 9.680555555555556e-06,
      "loss": 1.4878,
      "step": 618
    },
    {
      "epoch": 1.2895833333333333,
      "grad_norm": 7.351068019866943,
      "learning_rate": 9.678240740740742e-06,
      "loss": 1.514,
      "step": 619
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 7.668013572692871,
      "learning_rate": 9.675925925925926e-06,
      "loss": 1.8678,
      "step": 620
    },
    {
      "epoch": 1.29375,
      "grad_norm": 7.7469611167907715,
      "learning_rate": 9.673611111111112e-06,
      "loss": 1.6697,
      "step": 621
    },
    {
      "epoch": 1.2958333333333334,
      "grad_norm": 57.054386138916016,
      "learning_rate": 9.671296296296298e-06,
      "loss": 2.309,
      "step": 622
    },
    {
      "epoch": 1.2979166666666666,
      "grad_norm": 8.09070873260498,
      "learning_rate": 9.668981481481482e-06,
      "loss": 1.202,
      "step": 623
    },
    {
      "epoch": 1.3,
      "grad_norm": 6.925523281097412,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.4155,
      "step": 624
    },
    {
      "epoch": 1.3020833333333333,
      "grad_norm": 10.31895923614502,
      "learning_rate": 9.664351851851853e-06,
      "loss": 2.1433,
      "step": 625
    },
    {
      "epoch": 1.3041666666666667,
      "grad_norm": 7.176229953765869,
      "learning_rate": 9.662037037037037e-06,
      "loss": 1.4627,
      "step": 626
    },
    {
      "epoch": 1.30625,
      "grad_norm": 6.262925148010254,
      "learning_rate": 9.659722222222223e-06,
      "loss": 1.8339,
      "step": 627
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 24.398725509643555,
      "learning_rate": 9.657407407407409e-06,
      "loss": 1.2573,
      "step": 628
    },
    {
      "epoch": 1.3104166666666668,
      "grad_norm": 14.120368957519531,
      "learning_rate": 9.655092592592593e-06,
      "loss": 2.0746,
      "step": 629
    },
    {
      "epoch": 1.3125,
      "grad_norm": 14.32103157043457,
      "learning_rate": 9.652777777777779e-06,
      "loss": 1.902,
      "step": 630
    },
    {
      "epoch": 1.3145833333333332,
      "grad_norm": 12.424918174743652,
      "learning_rate": 9.650462962962965e-06,
      "loss": 2.4815,
      "step": 631
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 7.39402961730957,
      "learning_rate": 9.64814814814815e-06,
      "loss": 1.7484,
      "step": 632
    },
    {
      "epoch": 1.31875,
      "grad_norm": 10.450010299682617,
      "learning_rate": 9.645833333333333e-06,
      "loss": 2.0999,
      "step": 633
    },
    {
      "epoch": 1.3208333333333333,
      "grad_norm": 7.993061542510986,
      "learning_rate": 9.64351851851852e-06,
      "loss": 1.5458,
      "step": 634
    },
    {
      "epoch": 1.3229166666666667,
      "grad_norm": 7.297593593597412,
      "learning_rate": 9.641203703703704e-06,
      "loss": 1.4682,
      "step": 635
    },
    {
      "epoch": 1.325,
      "grad_norm": 10.245990753173828,
      "learning_rate": 9.63888888888889e-06,
      "loss": 1.9194,
      "step": 636
    },
    {
      "epoch": 1.3270833333333334,
      "grad_norm": 9.162091255187988,
      "learning_rate": 9.636574074074076e-06,
      "loss": 1.7946,
      "step": 637
    },
    {
      "epoch": 1.3291666666666666,
      "grad_norm": 10.001178741455078,
      "learning_rate": 9.63425925925926e-06,
      "loss": 1.8674,
      "step": 638
    },
    {
      "epoch": 1.33125,
      "grad_norm": 9.27587604522705,
      "learning_rate": 9.631944444444446e-06,
      "loss": 1.87,
      "step": 639
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 8.137566566467285,
      "learning_rate": 9.62962962962963e-06,
      "loss": 1.6741,
      "step": 640
    },
    {
      "epoch": 1.3354166666666667,
      "grad_norm": 8.478104591369629,
      "learning_rate": 9.627314814814816e-06,
      "loss": 1.9006,
      "step": 641
    },
    {
      "epoch": 1.3375,
      "grad_norm": 6.995912075042725,
      "learning_rate": 9.625e-06,
      "loss": 1.4786,
      "step": 642
    },
    {
      "epoch": 1.3395833333333333,
      "grad_norm": 10.526086807250977,
      "learning_rate": 9.622685185185186e-06,
      "loss": 1.9156,
      "step": 643
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 11.341340065002441,
      "learning_rate": 9.62037037037037e-06,
      "loss": 2.209,
      "step": 644
    },
    {
      "epoch": 1.34375,
      "grad_norm": 7.358345031738281,
      "learning_rate": 9.618055555555556e-06,
      "loss": 1.8594,
      "step": 645
    },
    {
      "epoch": 1.3458333333333332,
      "grad_norm": 8.906429290771484,
      "learning_rate": 9.61574074074074e-06,
      "loss": 2.0922,
      "step": 646
    },
    {
      "epoch": 1.3479166666666667,
      "grad_norm": 7.257223129272461,
      "learning_rate": 9.613425925925927e-06,
      "loss": 1.4282,
      "step": 647
    },
    {
      "epoch": 1.35,
      "grad_norm": 21.18498420715332,
      "learning_rate": 9.611111111111112e-06,
      "loss": 2.2221,
      "step": 648
    },
    {
      "epoch": 1.3520833333333333,
      "grad_norm": 8.555641174316406,
      "learning_rate": 9.608796296296297e-06,
      "loss": 1.6299,
      "step": 649
    },
    {
      "epoch": 1.3541666666666667,
      "grad_norm": 16.590076446533203,
      "learning_rate": 9.606481481481483e-06,
      "loss": 2.2756,
      "step": 650
    },
    {
      "epoch": 1.35625,
      "grad_norm": 8.984541893005371,
      "learning_rate": 9.604166666666669e-06,
      "loss": 2.0865,
      "step": 651
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 3.8862545490264893,
      "learning_rate": 9.601851851851853e-06,
      "loss": 2.0708,
      "step": 652
    },
    {
      "epoch": 1.3604166666666666,
      "grad_norm": 10.22523021697998,
      "learning_rate": 9.599537037037037e-06,
      "loss": 1.781,
      "step": 653
    },
    {
      "epoch": 1.3625,
      "grad_norm": 9.3724365234375,
      "learning_rate": 9.597222222222223e-06,
      "loss": 1.4334,
      "step": 654
    },
    {
      "epoch": 1.3645833333333333,
      "grad_norm": 6.981603622436523,
      "learning_rate": 9.594907407407407e-06,
      "loss": 1.8178,
      "step": 655
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 5.329166412353516,
      "learning_rate": 9.592592592592593e-06,
      "loss": 1.9871,
      "step": 656
    },
    {
      "epoch": 1.36875,
      "grad_norm": 6.148777484893799,
      "learning_rate": 9.59027777777778e-06,
      "loss": 1.5783,
      "step": 657
    },
    {
      "epoch": 1.3708333333333333,
      "grad_norm": 14.034274101257324,
      "learning_rate": 9.587962962962963e-06,
      "loss": 1.7989,
      "step": 658
    },
    {
      "epoch": 1.3729166666666668,
      "grad_norm": 9.860187530517578,
      "learning_rate": 9.58564814814815e-06,
      "loss": 1.9732,
      "step": 659
    },
    {
      "epoch": 1.375,
      "grad_norm": 5.570441722869873,
      "learning_rate": 9.583333333333335e-06,
      "loss": 1.4907,
      "step": 660
    },
    {
      "epoch": 1.3770833333333332,
      "grad_norm": 7.976370334625244,
      "learning_rate": 9.58101851851852e-06,
      "loss": 1.4347,
      "step": 661
    },
    {
      "epoch": 1.3791666666666667,
      "grad_norm": 7.761101722717285,
      "learning_rate": 9.578703703703704e-06,
      "loss": 1.6827,
      "step": 662
    },
    {
      "epoch": 1.38125,
      "grad_norm": 8.027889251708984,
      "learning_rate": 9.57638888888889e-06,
      "loss": 1.8294,
      "step": 663
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 6.265452861785889,
      "learning_rate": 9.574074074074074e-06,
      "loss": 1.4063,
      "step": 664
    },
    {
      "epoch": 1.3854166666666667,
      "grad_norm": 6.574056148529053,
      "learning_rate": 9.57175925925926e-06,
      "loss": 1.8191,
      "step": 665
    },
    {
      "epoch": 1.3875,
      "grad_norm": 6.40303373336792,
      "learning_rate": 9.569444444444446e-06,
      "loss": 1.5265,
      "step": 666
    },
    {
      "epoch": 1.3895833333333334,
      "grad_norm": 9.182129859924316,
      "learning_rate": 9.56712962962963e-06,
      "loss": 1.7922,
      "step": 667
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 6.340987682342529,
      "learning_rate": 9.564814814814816e-06,
      "loss": 1.3887,
      "step": 668
    },
    {
      "epoch": 1.39375,
      "grad_norm": 6.722743511199951,
      "learning_rate": 9.562500000000002e-06,
      "loss": 1.5832,
      "step": 669
    },
    {
      "epoch": 1.3958333333333333,
      "grad_norm": 9.126479148864746,
      "learning_rate": 9.560185185185186e-06,
      "loss": 2.0748,
      "step": 670
    },
    {
      "epoch": 1.3979166666666667,
      "grad_norm": 10.776910781860352,
      "learning_rate": 9.55787037037037e-06,
      "loss": 2.0463,
      "step": 671
    },
    {
      "epoch": 1.4,
      "grad_norm": 7.172388076782227,
      "learning_rate": 9.555555555555556e-06,
      "loss": 1.9209,
      "step": 672
    },
    {
      "epoch": 1.4020833333333333,
      "grad_norm": 6.509444713592529,
      "learning_rate": 9.55324074074074e-06,
      "loss": 1.6912,
      "step": 673
    },
    {
      "epoch": 1.4041666666666668,
      "grad_norm": 14.773183822631836,
      "learning_rate": 9.550925925925927e-06,
      "loss": 1.9896,
      "step": 674
    },
    {
      "epoch": 1.40625,
      "grad_norm": 6.680857181549072,
      "learning_rate": 9.548611111111113e-06,
      "loss": 1.414,
      "step": 675
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 11.86325454711914,
      "learning_rate": 9.546296296296297e-06,
      "loss": 0.9865,
      "step": 676
    },
    {
      "epoch": 1.4104166666666667,
      "grad_norm": 8.366303443908691,
      "learning_rate": 9.543981481481483e-06,
      "loss": 1.4973,
      "step": 677
    },
    {
      "epoch": 1.4125,
      "grad_norm": 9.766424179077148,
      "learning_rate": 9.541666666666669e-06,
      "loss": 2.1377,
      "step": 678
    },
    {
      "epoch": 1.4145833333333333,
      "grad_norm": 12.052597999572754,
      "learning_rate": 9.539351851851853e-06,
      "loss": 2.1732,
      "step": 679
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 11.112542152404785,
      "learning_rate": 9.537037037037037e-06,
      "loss": 1.6345,
      "step": 680
    },
    {
      "epoch": 1.41875,
      "grad_norm": 8.951783180236816,
      "learning_rate": 9.534722222222223e-06,
      "loss": 1.5806,
      "step": 681
    },
    {
      "epoch": 1.4208333333333334,
      "grad_norm": 6.969979286193848,
      "learning_rate": 9.532407407407407e-06,
      "loss": 1.3907,
      "step": 682
    },
    {
      "epoch": 1.4229166666666666,
      "grad_norm": 5.270254611968994,
      "learning_rate": 9.530092592592593e-06,
      "loss": 1.8601,
      "step": 683
    },
    {
      "epoch": 1.425,
      "grad_norm": 11.293233871459961,
      "learning_rate": 9.527777777777778e-06,
      "loss": 1.8047,
      "step": 684
    },
    {
      "epoch": 1.4270833333333333,
      "grad_norm": 17.436124801635742,
      "learning_rate": 9.525462962962964e-06,
      "loss": 1.6825,
      "step": 685
    },
    {
      "epoch": 1.4291666666666667,
      "grad_norm": 7.121499061584473,
      "learning_rate": 9.52314814814815e-06,
      "loss": 1.864,
      "step": 686
    },
    {
      "epoch": 1.43125,
      "grad_norm": 7.628202438354492,
      "learning_rate": 9.520833333333334e-06,
      "loss": 1.6396,
      "step": 687
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 8.12023639678955,
      "learning_rate": 9.51851851851852e-06,
      "loss": 1.2653,
      "step": 688
    },
    {
      "epoch": 1.4354166666666668,
      "grad_norm": 8.366511344909668,
      "learning_rate": 9.516203703703704e-06,
      "loss": 1.4344,
      "step": 689
    },
    {
      "epoch": 1.4375,
      "grad_norm": 11.044703483581543,
      "learning_rate": 9.51388888888889e-06,
      "loss": 1.7737,
      "step": 690
    },
    {
      "epoch": 1.4395833333333332,
      "grad_norm": 9.54362964630127,
      "learning_rate": 9.511574074074074e-06,
      "loss": 1.701,
      "step": 691
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 13.276009559631348,
      "learning_rate": 9.50925925925926e-06,
      "loss": 1.8994,
      "step": 692
    },
    {
      "epoch": 1.44375,
      "grad_norm": 9.309993743896484,
      "learning_rate": 9.506944444444444e-06,
      "loss": 1.4232,
      "step": 693
    },
    {
      "epoch": 1.4458333333333333,
      "grad_norm": 6.933876037597656,
      "learning_rate": 9.50462962962963e-06,
      "loss": 2.091,
      "step": 694
    },
    {
      "epoch": 1.4479166666666667,
      "grad_norm": 15.032843589782715,
      "learning_rate": 9.502314814814816e-06,
      "loss": 1.921,
      "step": 695
    },
    {
      "epoch": 1.45,
      "grad_norm": 7.7666850090026855,
      "learning_rate": 9.5e-06,
      "loss": 1.4793,
      "step": 696
    },
    {
      "epoch": 1.4520833333333334,
      "grad_norm": 14.989361763000488,
      "learning_rate": 9.497685185185186e-06,
      "loss": 1.9109,
      "step": 697
    },
    {
      "epoch": 1.4541666666666666,
      "grad_norm": 6.4530253410339355,
      "learning_rate": 9.49537037037037e-06,
      "loss": 2.0006,
      "step": 698
    },
    {
      "epoch": 1.45625,
      "grad_norm": 11.892557144165039,
      "learning_rate": 9.493055555555557e-06,
      "loss": 1.9771,
      "step": 699
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 12.19426441192627,
      "learning_rate": 9.490740740740741e-06,
      "loss": 1.5263,
      "step": 700
    },
    {
      "epoch": 1.4604166666666667,
      "grad_norm": 8.82660961151123,
      "learning_rate": 9.488425925925927e-06,
      "loss": 1.3468,
      "step": 701
    },
    {
      "epoch": 1.4625,
      "grad_norm": 8.971718788146973,
      "learning_rate": 9.486111111111111e-06,
      "loss": 1.829,
      "step": 702
    },
    {
      "epoch": 1.4645833333333333,
      "grad_norm": 12.563873291015625,
      "learning_rate": 9.483796296296297e-06,
      "loss": 2.213,
      "step": 703
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 9.198945999145508,
      "learning_rate": 9.481481481481483e-06,
      "loss": 1.8473,
      "step": 704
    },
    {
      "epoch": 1.46875,
      "grad_norm": 7.02931547164917,
      "learning_rate": 9.479166666666667e-06,
      "loss": 1.5598,
      "step": 705
    },
    {
      "epoch": 1.4708333333333332,
      "grad_norm": 9.198569297790527,
      "learning_rate": 9.476851851851853e-06,
      "loss": 1.8291,
      "step": 706
    },
    {
      "epoch": 1.4729166666666667,
      "grad_norm": 12.020807266235352,
      "learning_rate": 9.474537037037037e-06,
      "loss": 0.8967,
      "step": 707
    },
    {
      "epoch": 1.475,
      "grad_norm": 7.067902565002441,
      "learning_rate": 9.472222222222223e-06,
      "loss": 1.7867,
      "step": 708
    },
    {
      "epoch": 1.4770833333333333,
      "grad_norm": 5.884659767150879,
      "learning_rate": 9.469907407407408e-06,
      "loss": 1.4565,
      "step": 709
    },
    {
      "epoch": 1.4791666666666667,
      "grad_norm": 11.06344985961914,
      "learning_rate": 9.467592592592594e-06,
      "loss": 2.0035,
      "step": 710
    },
    {
      "epoch": 1.48125,
      "grad_norm": 8.631453514099121,
      "learning_rate": 9.465277777777778e-06,
      "loss": 2.1018,
      "step": 711
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 11.086047172546387,
      "learning_rate": 9.462962962962964e-06,
      "loss": 1.758,
      "step": 712
    },
    {
      "epoch": 1.4854166666666666,
      "grad_norm": 12.94863224029541,
      "learning_rate": 9.46064814814815e-06,
      "loss": 2.0371,
      "step": 713
    },
    {
      "epoch": 1.4875,
      "grad_norm": 11.810455322265625,
      "learning_rate": 9.458333333333334e-06,
      "loss": 1.8896,
      "step": 714
    },
    {
      "epoch": 1.4895833333333333,
      "grad_norm": 8.957878112792969,
      "learning_rate": 9.45601851851852e-06,
      "loss": 2.1475,
      "step": 715
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 17.543840408325195,
      "learning_rate": 9.453703703703704e-06,
      "loss": 2.2791,
      "step": 716
    },
    {
      "epoch": 1.49375,
      "grad_norm": 9.082154273986816,
      "learning_rate": 9.45138888888889e-06,
      "loss": 1.7403,
      "step": 717
    },
    {
      "epoch": 1.4958333333333333,
      "grad_norm": 10.517353057861328,
      "learning_rate": 9.449074074074074e-06,
      "loss": 1.5853,
      "step": 718
    },
    {
      "epoch": 1.4979166666666668,
      "grad_norm": 7.88033390045166,
      "learning_rate": 9.44675925925926e-06,
      "loss": 1.6378,
      "step": 719
    },
    {
      "epoch": 1.5,
      "grad_norm": 11.205703735351562,
      "learning_rate": 9.444444444444445e-06,
      "loss": 2.0816,
      "step": 720
    },
    {
      "epoch": 1.5020833333333332,
      "grad_norm": 8.25730037689209,
      "learning_rate": 9.44212962962963e-06,
      "loss": 1.3082,
      "step": 721
    },
    {
      "epoch": 1.5041666666666667,
      "grad_norm": 9.887762069702148,
      "learning_rate": 9.439814814814816e-06,
      "loss": 1.9294,
      "step": 722
    },
    {
      "epoch": 1.50625,
      "grad_norm": 7.674728870391846,
      "learning_rate": 9.4375e-06,
      "loss": 1.3562,
      "step": 723
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 9.325597763061523,
      "learning_rate": 9.435185185185187e-06,
      "loss": 1.1083,
      "step": 724
    },
    {
      "epoch": 1.5104166666666665,
      "grad_norm": 14.209789276123047,
      "learning_rate": 9.432870370370371e-06,
      "loss": 2.6763,
      "step": 725
    },
    {
      "epoch": 1.5125,
      "grad_norm": 8.615937232971191,
      "learning_rate": 9.430555555555557e-06,
      "loss": 1.1846,
      "step": 726
    },
    {
      "epoch": 1.5145833333333334,
      "grad_norm": 8.525432586669922,
      "learning_rate": 9.428240740740741e-06,
      "loss": 1.1732,
      "step": 727
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 9.47774600982666,
      "learning_rate": 9.425925925925925e-06,
      "loss": 1.7189,
      "step": 728
    },
    {
      "epoch": 1.51875,
      "grad_norm": 6.530431747436523,
      "learning_rate": 9.423611111111111e-06,
      "loss": 1.4575,
      "step": 729
    },
    {
      "epoch": 1.5208333333333335,
      "grad_norm": 7.540192127227783,
      "learning_rate": 9.421296296296297e-06,
      "loss": 1.6404,
      "step": 730
    },
    {
      "epoch": 1.5229166666666667,
      "grad_norm": 6.9873785972595215,
      "learning_rate": 9.418981481481481e-06,
      "loss": 1.3539,
      "step": 731
    },
    {
      "epoch": 1.525,
      "grad_norm": 14.077200889587402,
      "learning_rate": 9.416666666666667e-06,
      "loss": 2.0471,
      "step": 732
    },
    {
      "epoch": 1.5270833333333333,
      "grad_norm": 11.383381843566895,
      "learning_rate": 9.414351851851853e-06,
      "loss": 2.0218,
      "step": 733
    },
    {
      "epoch": 1.5291666666666668,
      "grad_norm": 9.69314956665039,
      "learning_rate": 9.412037037037038e-06,
      "loss": 1.5908,
      "step": 734
    },
    {
      "epoch": 1.53125,
      "grad_norm": 8.981706619262695,
      "learning_rate": 9.409722222222224e-06,
      "loss": 1.6957,
      "step": 735
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 5.6117706298828125,
      "learning_rate": 9.407407407407408e-06,
      "loss": 1.9673,
      "step": 736
    },
    {
      "epoch": 1.5354166666666667,
      "grad_norm": 9.590352058410645,
      "learning_rate": 9.405092592592592e-06,
      "loss": 1.8801,
      "step": 737
    },
    {
      "epoch": 1.5375,
      "grad_norm": 9.588565826416016,
      "learning_rate": 9.402777777777778e-06,
      "loss": 1.7895,
      "step": 738
    },
    {
      "epoch": 1.5395833333333333,
      "grad_norm": 21.511857986450195,
      "learning_rate": 9.400462962962964e-06,
      "loss": 1.6757,
      "step": 739
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 12.063216209411621,
      "learning_rate": 9.398148148148148e-06,
      "loss": 0.9622,
      "step": 740
    },
    {
      "epoch": 1.54375,
      "grad_norm": 9.365038871765137,
      "learning_rate": 9.395833333333334e-06,
      "loss": 2.0091,
      "step": 741
    },
    {
      "epoch": 1.5458333333333334,
      "grad_norm": 12.310638427734375,
      "learning_rate": 9.39351851851852e-06,
      "loss": 1.1551,
      "step": 742
    },
    {
      "epoch": 1.5479166666666666,
      "grad_norm": 7.5223283767700195,
      "learning_rate": 9.391203703703704e-06,
      "loss": 1.8813,
      "step": 743
    },
    {
      "epoch": 1.55,
      "grad_norm": 11.146984100341797,
      "learning_rate": 9.38888888888889e-06,
      "loss": 1.8987,
      "step": 744
    },
    {
      "epoch": 1.5520833333333335,
      "grad_norm": 9.20479679107666,
      "learning_rate": 9.386574074074075e-06,
      "loss": 1.7463,
      "step": 745
    },
    {
      "epoch": 1.5541666666666667,
      "grad_norm": 2.890655279159546,
      "learning_rate": 9.384259259259259e-06,
      "loss": 1.8881,
      "step": 746
    },
    {
      "epoch": 1.55625,
      "grad_norm": 7.359027862548828,
      "learning_rate": 9.381944444444445e-06,
      "loss": 1.8624,
      "step": 747
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 10.852561950683594,
      "learning_rate": 9.37962962962963e-06,
      "loss": 1.5413,
      "step": 748
    },
    {
      "epoch": 1.5604166666666668,
      "grad_norm": 7.757837295532227,
      "learning_rate": 9.377314814814815e-06,
      "loss": 1.4335,
      "step": 749
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.021791458129883,
      "learning_rate": 9.375000000000001e-06,
      "loss": 2.188,
      "step": 750
    },
    {
      "epoch": 1.5645833333333332,
      "grad_norm": 7.357199668884277,
      "learning_rate": 9.372685185185187e-06,
      "loss": 1.4445,
      "step": 751
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 11.117677688598633,
      "learning_rate": 9.370370370370371e-06,
      "loss": 2.2017,
      "step": 752
    },
    {
      "epoch": 1.56875,
      "grad_norm": 8.08716106414795,
      "learning_rate": 9.368055555555557e-06,
      "loss": 1.9903,
      "step": 753
    },
    {
      "epoch": 1.5708333333333333,
      "grad_norm": 6.126646518707275,
      "learning_rate": 9.365740740740741e-06,
      "loss": 1.7974,
      "step": 754
    },
    {
      "epoch": 1.5729166666666665,
      "grad_norm": 7.748610496520996,
      "learning_rate": 9.363425925925927e-06,
      "loss": 1.6856,
      "step": 755
    },
    {
      "epoch": 1.575,
      "grad_norm": 8.157474517822266,
      "learning_rate": 9.361111111111111e-06,
      "loss": 1.9049,
      "step": 756
    },
    {
      "epoch": 1.5770833333333334,
      "grad_norm": 12.281026840209961,
      "learning_rate": 9.358796296296297e-06,
      "loss": 0.8613,
      "step": 757
    },
    {
      "epoch": 1.5791666666666666,
      "grad_norm": 9.38447093963623,
      "learning_rate": 9.356481481481482e-06,
      "loss": 1.8266,
      "step": 758
    },
    {
      "epoch": 1.58125,
      "grad_norm": 7.167775630950928,
      "learning_rate": 9.354166666666668e-06,
      "loss": 1.3393,
      "step": 759
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 10.525578498840332,
      "learning_rate": 9.351851851851854e-06,
      "loss": 1.9579,
      "step": 760
    },
    {
      "epoch": 1.5854166666666667,
      "grad_norm": 10.664066314697266,
      "learning_rate": 9.349537037037038e-06,
      "loss": 1.8167,
      "step": 761
    },
    {
      "epoch": 1.5875,
      "grad_norm": 8.770933151245117,
      "learning_rate": 9.347222222222224e-06,
      "loss": 1.7369,
      "step": 762
    },
    {
      "epoch": 1.5895833333333333,
      "grad_norm": 9.524698257446289,
      "learning_rate": 9.344907407407408e-06,
      "loss": 1.8943,
      "step": 763
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 8.87808609008789,
      "learning_rate": 9.342592592592594e-06,
      "loss": 1.7792,
      "step": 764
    },
    {
      "epoch": 1.59375,
      "grad_norm": 15.112667083740234,
      "learning_rate": 9.340277777777778e-06,
      "loss": 2.4081,
      "step": 765
    },
    {
      "epoch": 1.5958333333333332,
      "grad_norm": 10.793756484985352,
      "learning_rate": 9.337962962962964e-06,
      "loss": 1.1266,
      "step": 766
    },
    {
      "epoch": 1.5979166666666667,
      "grad_norm": 12.41319751739502,
      "learning_rate": 9.335648148148148e-06,
      "loss": 2.0702,
      "step": 767
    },
    {
      "epoch": 1.6,
      "grad_norm": 7.000545978546143,
      "learning_rate": 9.333333333333334e-06,
      "loss": 2.1052,
      "step": 768
    },
    {
      "epoch": 1.6020833333333333,
      "grad_norm": 10.542059898376465,
      "learning_rate": 9.33101851851852e-06,
      "loss": 1.5722,
      "step": 769
    },
    {
      "epoch": 1.6041666666666665,
      "grad_norm": 4.133267879486084,
      "learning_rate": 9.328703703703705e-06,
      "loss": 1.9304,
      "step": 770
    },
    {
      "epoch": 1.60625,
      "grad_norm": 8.83169174194336,
      "learning_rate": 9.32638888888889e-06,
      "loss": 1.29,
      "step": 771
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 7.024064540863037,
      "learning_rate": 9.324074074074075e-06,
      "loss": 1.4015,
      "step": 772
    },
    {
      "epoch": 1.6104166666666666,
      "grad_norm": 8.097102165222168,
      "learning_rate": 9.32175925925926e-06,
      "loss": 1.4848,
      "step": 773
    },
    {
      "epoch": 1.6125,
      "grad_norm": 14.316213607788086,
      "learning_rate": 9.319444444444445e-06,
      "loss": 1.6107,
      "step": 774
    },
    {
      "epoch": 1.6145833333333335,
      "grad_norm": 9.98715877532959,
      "learning_rate": 9.31712962962963e-06,
      "loss": 1.7351,
      "step": 775
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 9.580471992492676,
      "learning_rate": 9.314814814814815e-06,
      "loss": 2.1634,
      "step": 776
    },
    {
      "epoch": 1.61875,
      "grad_norm": 8.279526710510254,
      "learning_rate": 9.312500000000001e-06,
      "loss": 1.9187,
      "step": 777
    },
    {
      "epoch": 1.6208333333333333,
      "grad_norm": 8.969770431518555,
      "learning_rate": 9.310185185185185e-06,
      "loss": 1.8998,
      "step": 778
    },
    {
      "epoch": 1.6229166666666668,
      "grad_norm": 5.969125270843506,
      "learning_rate": 9.307870370370371e-06,
      "loss": 1.4969,
      "step": 779
    },
    {
      "epoch": 1.625,
      "grad_norm": 10.428056716918945,
      "learning_rate": 9.305555555555557e-06,
      "loss": 1.6046,
      "step": 780
    },
    {
      "epoch": 1.6270833333333332,
      "grad_norm": 14.476799964904785,
      "learning_rate": 9.303240740740741e-06,
      "loss": 2.4443,
      "step": 781
    },
    {
      "epoch": 1.6291666666666667,
      "grad_norm": 29.292333602905273,
      "learning_rate": 9.300925925925927e-06,
      "loss": 1.9608,
      "step": 782
    },
    {
      "epoch": 1.63125,
      "grad_norm": 5.666156768798828,
      "learning_rate": 9.298611111111112e-06,
      "loss": 1.8887,
      "step": 783
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 8.484051704406738,
      "learning_rate": 9.296296296296296e-06,
      "loss": 1.5593,
      "step": 784
    },
    {
      "epoch": 1.6354166666666665,
      "grad_norm": 8.568696022033691,
      "learning_rate": 9.293981481481482e-06,
      "loss": 2.0549,
      "step": 785
    },
    {
      "epoch": 1.6375,
      "grad_norm": 10.110095977783203,
      "learning_rate": 9.291666666666668e-06,
      "loss": 2.1259,
      "step": 786
    },
    {
      "epoch": 1.6395833333333334,
      "grad_norm": 11.819135665893555,
      "learning_rate": 9.289351851851852e-06,
      "loss": 1.6079,
      "step": 787
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 45.78827667236328,
      "learning_rate": 9.287037037037038e-06,
      "loss": 1.5303,
      "step": 788
    },
    {
      "epoch": 1.64375,
      "grad_norm": 6.987464427947998,
      "learning_rate": 9.284722222222224e-06,
      "loss": 1.7755,
      "step": 789
    },
    {
      "epoch": 1.6458333333333335,
      "grad_norm": 7.507819175720215,
      "learning_rate": 9.282407407407408e-06,
      "loss": 1.3888,
      "step": 790
    },
    {
      "epoch": 1.6479166666666667,
      "grad_norm": 7.949976444244385,
      "learning_rate": 9.280092592592594e-06,
      "loss": 1.3991,
      "step": 791
    },
    {
      "epoch": 1.65,
      "grad_norm": 7.6637864112854,
      "learning_rate": 9.277777777777778e-06,
      "loss": 2.0348,
      "step": 792
    },
    {
      "epoch": 1.6520833333333333,
      "grad_norm": 13.634201049804688,
      "learning_rate": 9.275462962962963e-06,
      "loss": 1.4467,
      "step": 793
    },
    {
      "epoch": 1.6541666666666668,
      "grad_norm": 3.627788782119751,
      "learning_rate": 9.273148148148149e-06,
      "loss": 2.0675,
      "step": 794
    },
    {
      "epoch": 1.65625,
      "grad_norm": 9.578767776489258,
      "learning_rate": 9.270833333333334e-06,
      "loss": 1.5087,
      "step": 795
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 9.598677635192871,
      "learning_rate": 9.268518518518519e-06,
      "loss": 1.8061,
      "step": 796
    },
    {
      "epoch": 1.6604166666666667,
      "grad_norm": 9.555903434753418,
      "learning_rate": 9.266203703703705e-06,
      "loss": 2.1864,
      "step": 797
    },
    {
      "epoch": 1.6625,
      "grad_norm": 12.41916275024414,
      "learning_rate": 9.26388888888889e-06,
      "loss": 1.6232,
      "step": 798
    },
    {
      "epoch": 1.6645833333333333,
      "grad_norm": 10.031953811645508,
      "learning_rate": 9.261574074074075e-06,
      "loss": 1.9664,
      "step": 799
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 8.503414154052734,
      "learning_rate": 9.25925925925926e-06,
      "loss": 1.8826,
      "step": 800
    },
    {
      "epoch": 1.66875,
      "grad_norm": 9.925237655639648,
      "learning_rate": 9.256944444444445e-06,
      "loss": 2.0802,
      "step": 801
    },
    {
      "epoch": 1.6708333333333334,
      "grad_norm": 11.572080612182617,
      "learning_rate": 9.25462962962963e-06,
      "loss": 1.8179,
      "step": 802
    },
    {
      "epoch": 1.6729166666666666,
      "grad_norm": 6.2245774269104,
      "learning_rate": 9.252314814814815e-06,
      "loss": 1.4008,
      "step": 803
    },
    {
      "epoch": 1.675,
      "grad_norm": 7.253314971923828,
      "learning_rate": 9.250000000000001e-06,
      "loss": 1.4221,
      "step": 804
    },
    {
      "epoch": 1.6770833333333335,
      "grad_norm": 7.790045261383057,
      "learning_rate": 9.247685185185185e-06,
      "loss": 1.9939,
      "step": 805
    },
    {
      "epoch": 1.6791666666666667,
      "grad_norm": 8.688694953918457,
      "learning_rate": 9.245370370370371e-06,
      "loss": 1.6515,
      "step": 806
    },
    {
      "epoch": 1.68125,
      "grad_norm": 7.438039779663086,
      "learning_rate": 9.243055555555557e-06,
      "loss": 1.5146,
      "step": 807
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 8.703648567199707,
      "learning_rate": 9.240740740740742e-06,
      "loss": 1.4267,
      "step": 808
    },
    {
      "epoch": 1.6854166666666668,
      "grad_norm": 13.181537628173828,
      "learning_rate": 9.238425925925928e-06,
      "loss": 1.6959,
      "step": 809
    },
    {
      "epoch": 1.6875,
      "grad_norm": 11.780952453613281,
      "learning_rate": 9.236111111111112e-06,
      "loss": 1.088,
      "step": 810
    },
    {
      "epoch": 1.6895833333333332,
      "grad_norm": 6.81350040435791,
      "learning_rate": 9.233796296296296e-06,
      "loss": 1.982,
      "step": 811
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 7.014212608337402,
      "learning_rate": 9.231481481481482e-06,
      "loss": 1.2798,
      "step": 812
    },
    {
      "epoch": 1.69375,
      "grad_norm": 8.470229148864746,
      "learning_rate": 9.229166666666668e-06,
      "loss": 1.0935,
      "step": 813
    },
    {
      "epoch": 1.6958333333333333,
      "grad_norm": 12.414495468139648,
      "learning_rate": 9.226851851851852e-06,
      "loss": 2.2276,
      "step": 814
    },
    {
      "epoch": 1.6979166666666665,
      "grad_norm": 7.5014495849609375,
      "learning_rate": 9.224537037037038e-06,
      "loss": 1.2545,
      "step": 815
    },
    {
      "epoch": 1.7,
      "grad_norm": 8.31639289855957,
      "learning_rate": 9.222222222222224e-06,
      "loss": 1.0643,
      "step": 816
    },
    {
      "epoch": 1.7020833333333334,
      "grad_norm": 7.941153526306152,
      "learning_rate": 9.219907407407408e-06,
      "loss": 1.3673,
      "step": 817
    },
    {
      "epoch": 1.7041666666666666,
      "grad_norm": 7.797340393066406,
      "learning_rate": 9.217592592592594e-06,
      "loss": 1.3176,
      "step": 818
    },
    {
      "epoch": 1.70625,
      "grad_norm": 10.874963760375977,
      "learning_rate": 9.215277777777779e-06,
      "loss": 1.8769,
      "step": 819
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 14.906208992004395,
      "learning_rate": 9.212962962962963e-06,
      "loss": 2.001,
      "step": 820
    },
    {
      "epoch": 1.7104166666666667,
      "grad_norm": 11.547801971435547,
      "learning_rate": 9.210648148148149e-06,
      "loss": 0.922,
      "step": 821
    },
    {
      "epoch": 1.7125,
      "grad_norm": 8.380057334899902,
      "learning_rate": 9.208333333333333e-06,
      "loss": 1.6479,
      "step": 822
    },
    {
      "epoch": 1.7145833333333333,
      "grad_norm": 8.244202613830566,
      "learning_rate": 9.206018518518519e-06,
      "loss": 1.257,
      "step": 823
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 14.234712600708008,
      "learning_rate": 9.203703703703705e-06,
      "loss": 2.034,
      "step": 824
    },
    {
      "epoch": 1.71875,
      "grad_norm": 10.541569709777832,
      "learning_rate": 9.201388888888889e-06,
      "loss": 1.8926,
      "step": 825
    },
    {
      "epoch": 1.7208333333333332,
      "grad_norm": 17.83091163635254,
      "learning_rate": 9.199074074074075e-06,
      "loss": 2.1978,
      "step": 826
    },
    {
      "epoch": 1.7229166666666667,
      "grad_norm": 10.668956756591797,
      "learning_rate": 9.196759259259261e-06,
      "loss": 1.8311,
      "step": 827
    },
    {
      "epoch": 1.725,
      "grad_norm": 10.147337913513184,
      "learning_rate": 9.194444444444445e-06,
      "loss": 1.7958,
      "step": 828
    },
    {
      "epoch": 1.7270833333333333,
      "grad_norm": 5.7800140380859375,
      "learning_rate": 9.19212962962963e-06,
      "loss": 1.8072,
      "step": 829
    },
    {
      "epoch": 1.7291666666666665,
      "grad_norm": 10.577611923217773,
      "learning_rate": 9.189814814814815e-06,
      "loss": 1.7255,
      "step": 830
    },
    {
      "epoch": 1.73125,
      "grad_norm": 18.70315933227539,
      "learning_rate": 9.1875e-06,
      "loss": 2.21,
      "step": 831
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 10.007413864135742,
      "learning_rate": 9.185185185185186e-06,
      "loss": 1.2375,
      "step": 832
    },
    {
      "epoch": 1.7354166666666666,
      "grad_norm": 11.646224975585938,
      "learning_rate": 9.182870370370372e-06,
      "loss": 2.094,
      "step": 833
    },
    {
      "epoch": 1.7375,
      "grad_norm": 7.496830463409424,
      "learning_rate": 9.180555555555556e-06,
      "loss": 1.2717,
      "step": 834
    },
    {
      "epoch": 1.7395833333333335,
      "grad_norm": 7.183534622192383,
      "learning_rate": 9.178240740740742e-06,
      "loss": 1.427,
      "step": 835
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 9.152249336242676,
      "learning_rate": 9.175925925925928e-06,
      "loss": 1.8227,
      "step": 836
    },
    {
      "epoch": 1.74375,
      "grad_norm": 10.726395606994629,
      "learning_rate": 9.173611111111112e-06,
      "loss": 1.3648,
      "step": 837
    },
    {
      "epoch": 1.7458333333333333,
      "grad_norm": 8.860553741455078,
      "learning_rate": 9.171296296296296e-06,
      "loss": 1.2382,
      "step": 838
    },
    {
      "epoch": 1.7479166666666668,
      "grad_norm": 21.66317367553711,
      "learning_rate": 9.168981481481482e-06,
      "loss": 2.0582,
      "step": 839
    },
    {
      "epoch": 1.75,
      "grad_norm": 9.258090019226074,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.8121,
      "step": 840
    },
    {
      "epoch": 1.7520833333333332,
      "grad_norm": 9.318236351013184,
      "learning_rate": 9.164351851851852e-06,
      "loss": 1.9395,
      "step": 841
    },
    {
      "epoch": 1.7541666666666667,
      "grad_norm": 17.32600212097168,
      "learning_rate": 9.162037037037038e-06,
      "loss": 1.3953,
      "step": 842
    },
    {
      "epoch": 1.75625,
      "grad_norm": 20.78260612487793,
      "learning_rate": 9.159722222222223e-06,
      "loss": 2.0838,
      "step": 843
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 8.978339195251465,
      "learning_rate": 9.157407407407409e-06,
      "loss": 1.0031,
      "step": 844
    },
    {
      "epoch": 1.7604166666666665,
      "grad_norm": 6.961418151855469,
      "learning_rate": 9.155092592592594e-06,
      "loss": 1.3141,
      "step": 845
    },
    {
      "epoch": 1.7625,
      "grad_norm": 11.999186515808105,
      "learning_rate": 9.152777777777779e-06,
      "loss": 1.5018,
      "step": 846
    },
    {
      "epoch": 1.7645833333333334,
      "grad_norm": 8.304859161376953,
      "learning_rate": 9.150462962962963e-06,
      "loss": 1.5647,
      "step": 847
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 8.583681106567383,
      "learning_rate": 9.148148148148149e-06,
      "loss": 1.589,
      "step": 848
    },
    {
      "epoch": 1.76875,
      "grad_norm": 5.588135719299316,
      "learning_rate": 9.145833333333333e-06,
      "loss": 1.9778,
      "step": 849
    },
    {
      "epoch": 1.7708333333333335,
      "grad_norm": 9.401382446289062,
      "learning_rate": 9.143518518518519e-06,
      "loss": 2.1103,
      "step": 850
    },
    {
      "epoch": 1.7729166666666667,
      "grad_norm": 8.44985580444336,
      "learning_rate": 9.141203703703705e-06,
      "loss": 1.068,
      "step": 851
    },
    {
      "epoch": 1.775,
      "grad_norm": 7.69551420211792,
      "learning_rate": 9.13888888888889e-06,
      "loss": 1.4734,
      "step": 852
    },
    {
      "epoch": 1.7770833333333333,
      "grad_norm": 16.553653717041016,
      "learning_rate": 9.136574074074075e-06,
      "loss": 1.6663,
      "step": 853
    },
    {
      "epoch": 1.7791666666666668,
      "grad_norm": 9.779820442199707,
      "learning_rate": 9.134259259259261e-06,
      "loss": 1.2793,
      "step": 854
    },
    {
      "epoch": 1.78125,
      "grad_norm": 8.227226257324219,
      "learning_rate": 9.131944444444445e-06,
      "loss": 1.5655,
      "step": 855
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 20.566431045532227,
      "learning_rate": 9.12962962962963e-06,
      "loss": 1.9869,
      "step": 856
    },
    {
      "epoch": 1.7854166666666667,
      "grad_norm": 7.706771373748779,
      "learning_rate": 9.127314814814816e-06,
      "loss": 2.0387,
      "step": 857
    },
    {
      "epoch": 1.7875,
      "grad_norm": 9.118346214294434,
      "learning_rate": 9.125e-06,
      "loss": 1.2238,
      "step": 858
    },
    {
      "epoch": 1.7895833333333333,
      "grad_norm": 8.33463191986084,
      "learning_rate": 9.122685185185186e-06,
      "loss": 1.8076,
      "step": 859
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 6.347353458404541,
      "learning_rate": 9.120370370370372e-06,
      "loss": 1.3117,
      "step": 860
    },
    {
      "epoch": 1.79375,
      "grad_norm": 16.891090393066406,
      "learning_rate": 9.118055555555556e-06,
      "loss": 1.6341,
      "step": 861
    },
    {
      "epoch": 1.7958333333333334,
      "grad_norm": 8.815967559814453,
      "learning_rate": 9.115740740740742e-06,
      "loss": 1.891,
      "step": 862
    },
    {
      "epoch": 1.7979166666666666,
      "grad_norm": 7.940005302429199,
      "learning_rate": 9.113425925925926e-06,
      "loss": 1.4523,
      "step": 863
    },
    {
      "epoch": 1.8,
      "grad_norm": 11.420913696289062,
      "learning_rate": 9.111111111111112e-06,
      "loss": 1.8099,
      "step": 864
    },
    {
      "epoch": 1.8020833333333335,
      "grad_norm": 15.994171142578125,
      "learning_rate": 9.108796296296296e-06,
      "loss": 1.8556,
      "step": 865
    },
    {
      "epoch": 1.8041666666666667,
      "grad_norm": 11.009873390197754,
      "learning_rate": 9.106481481481482e-06,
      "loss": 1.7073,
      "step": 866
    },
    {
      "epoch": 1.80625,
      "grad_norm": 14.285979270935059,
      "learning_rate": 9.104166666666667e-06,
      "loss": 1.6787,
      "step": 867
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 8.126300811767578,
      "learning_rate": 9.101851851851853e-06,
      "loss": 1.4408,
      "step": 868
    },
    {
      "epoch": 1.8104166666666668,
      "grad_norm": 10.936728477478027,
      "learning_rate": 9.099537037037037e-06,
      "loss": 1.5312,
      "step": 869
    },
    {
      "epoch": 1.8125,
      "grad_norm": 14.285359382629395,
      "learning_rate": 9.097222222222223e-06,
      "loss": 1.2912,
      "step": 870
    },
    {
      "epoch": 1.8145833333333332,
      "grad_norm": 9.016558647155762,
      "learning_rate": 9.094907407407409e-06,
      "loss": 1.7728,
      "step": 871
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 10.40654182434082,
      "learning_rate": 9.092592592592593e-06,
      "loss": 1.3217,
      "step": 872
    },
    {
      "epoch": 1.81875,
      "grad_norm": 10.579638481140137,
      "learning_rate": 9.090277777777779e-06,
      "loss": 1.3018,
      "step": 873
    },
    {
      "epoch": 1.8208333333333333,
      "grad_norm": 10.981321334838867,
      "learning_rate": 9.087962962962965e-06,
      "loss": 1.7958,
      "step": 874
    },
    {
      "epoch": 1.8229166666666665,
      "grad_norm": 6.823093891143799,
      "learning_rate": 9.085648148148149e-06,
      "loss": 1.9156,
      "step": 875
    },
    {
      "epoch": 1.825,
      "grad_norm": 6.484595775604248,
      "learning_rate": 9.083333333333333e-06,
      "loss": 2.164,
      "step": 876
    },
    {
      "epoch": 1.8270833333333334,
      "grad_norm": 7.0883917808532715,
      "learning_rate": 9.08101851851852e-06,
      "loss": 1.277,
      "step": 877
    },
    {
      "epoch": 1.8291666666666666,
      "grad_norm": 12.533559799194336,
      "learning_rate": 9.078703703703704e-06,
      "loss": 2.0539,
      "step": 878
    },
    {
      "epoch": 1.83125,
      "grad_norm": 13.249163627624512,
      "learning_rate": 9.07638888888889e-06,
      "loss": 1.8315,
      "step": 879
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 10.202385902404785,
      "learning_rate": 9.074074074074075e-06,
      "loss": 1.1941,
      "step": 880
    },
    {
      "epoch": 1.8354166666666667,
      "grad_norm": 13.566216468811035,
      "learning_rate": 9.07175925925926e-06,
      "loss": 1.6738,
      "step": 881
    },
    {
      "epoch": 1.8375,
      "grad_norm": 10.877077102661133,
      "learning_rate": 9.069444444444446e-06,
      "loss": 1.7684,
      "step": 882
    },
    {
      "epoch": 1.8395833333333333,
      "grad_norm": 7.034998416900635,
      "learning_rate": 9.067129629629632e-06,
      "loss": 1.7539,
      "step": 883
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 14.247953414916992,
      "learning_rate": 9.064814814814816e-06,
      "loss": 2.2199,
      "step": 884
    },
    {
      "epoch": 1.84375,
      "grad_norm": 7.842029094696045,
      "learning_rate": 9.0625e-06,
      "loss": 1.1922,
      "step": 885
    },
    {
      "epoch": 1.8458333333333332,
      "grad_norm": 10.577768325805664,
      "learning_rate": 9.060185185185186e-06,
      "loss": 1.8469,
      "step": 886
    },
    {
      "epoch": 1.8479166666666667,
      "grad_norm": 8.348637580871582,
      "learning_rate": 9.05787037037037e-06,
      "loss": 1.481,
      "step": 887
    },
    {
      "epoch": 1.85,
      "grad_norm": 7.942948341369629,
      "learning_rate": 9.055555555555556e-06,
      "loss": 1.4891,
      "step": 888
    },
    {
      "epoch": 1.8520833333333333,
      "grad_norm": 9.835296630859375,
      "learning_rate": 9.053240740740742e-06,
      "loss": 2.1825,
      "step": 889
    },
    {
      "epoch": 1.8541666666666665,
      "grad_norm": 12.1279296875,
      "learning_rate": 9.050925925925926e-06,
      "loss": 0.7215,
      "step": 890
    },
    {
      "epoch": 1.85625,
      "grad_norm": 11.793085098266602,
      "learning_rate": 9.048611111111112e-06,
      "loss": 1.5518,
      "step": 891
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 8.347285270690918,
      "learning_rate": 9.046296296296298e-06,
      "loss": 1.694,
      "step": 892
    },
    {
      "epoch": 1.8604166666666666,
      "grad_norm": 8.325379371643066,
      "learning_rate": 9.043981481481483e-06,
      "loss": 1.6504,
      "step": 893
    },
    {
      "epoch": 1.8625,
      "grad_norm": 11.7201566696167,
      "learning_rate": 9.041666666666667e-06,
      "loss": 0.7612,
      "step": 894
    },
    {
      "epoch": 1.8645833333333335,
      "grad_norm": 12.91265869140625,
      "learning_rate": 9.039351851851853e-06,
      "loss": 1.6201,
      "step": 895
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 8.927043914794922,
      "learning_rate": 9.037037037037037e-06,
      "loss": 2.0395,
      "step": 896
    },
    {
      "epoch": 1.86875,
      "grad_norm": 7.528295516967773,
      "learning_rate": 9.034722222222223e-06,
      "loss": 1.796,
      "step": 897
    },
    {
      "epoch": 1.8708333333333333,
      "grad_norm": 6.009469032287598,
      "learning_rate": 9.032407407407409e-06,
      "loss": 1.306,
      "step": 898
    },
    {
      "epoch": 1.8729166666666668,
      "grad_norm": 9.730255126953125,
      "learning_rate": 9.030092592592593e-06,
      "loss": 2.0464,
      "step": 899
    },
    {
      "epoch": 1.875,
      "grad_norm": 6.693554878234863,
      "learning_rate": 9.027777777777779e-06,
      "loss": 1.621,
      "step": 900
    },
    {
      "epoch": 1.8770833333333332,
      "grad_norm": 9.36329174041748,
      "learning_rate": 9.025462962962965e-06,
      "loss": 1.3875,
      "step": 901
    },
    {
      "epoch": 1.8791666666666667,
      "grad_norm": 12.936211585998535,
      "learning_rate": 9.02314814814815e-06,
      "loss": 1.9299,
      "step": 902
    },
    {
      "epoch": 1.88125,
      "grad_norm": 12.441951751708984,
      "learning_rate": 9.020833333333334e-06,
      "loss": 2.1367,
      "step": 903
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 9.181896209716797,
      "learning_rate": 9.01851851851852e-06,
      "loss": 1.2404,
      "step": 904
    },
    {
      "epoch": 1.8854166666666665,
      "grad_norm": 8.217595100402832,
      "learning_rate": 9.016203703703704e-06,
      "loss": 1.8981,
      "step": 905
    },
    {
      "epoch": 1.8875,
      "grad_norm": 13.830297470092773,
      "learning_rate": 9.01388888888889e-06,
      "loss": 1.93,
      "step": 906
    },
    {
      "epoch": 1.8895833333333334,
      "grad_norm": 9.606034278869629,
      "learning_rate": 9.011574074074076e-06,
      "loss": 1.8762,
      "step": 907
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 13.889596939086914,
      "learning_rate": 9.00925925925926e-06,
      "loss": 1.3878,
      "step": 908
    },
    {
      "epoch": 1.89375,
      "grad_norm": 19.686172485351562,
      "learning_rate": 9.006944444444446e-06,
      "loss": 1.351,
      "step": 909
    },
    {
      "epoch": 1.8958333333333335,
      "grad_norm": 8.082128524780273,
      "learning_rate": 9.00462962962963e-06,
      "loss": 1.449,
      "step": 910
    },
    {
      "epoch": 1.8979166666666667,
      "grad_norm": 4.123544216156006,
      "learning_rate": 9.002314814814816e-06,
      "loss": 1.7994,
      "step": 911
    },
    {
      "epoch": 1.9,
      "grad_norm": 11.414502143859863,
      "learning_rate": 9e-06,
      "loss": 1.6776,
      "step": 912
    },
    {
      "epoch": 1.9020833333333333,
      "grad_norm": 4.440438747406006,
      "learning_rate": 8.997685185185186e-06,
      "loss": 1.8135,
      "step": 913
    },
    {
      "epoch": 1.9041666666666668,
      "grad_norm": 11.983895301818848,
      "learning_rate": 8.99537037037037e-06,
      "loss": 1.8351,
      "step": 914
    },
    {
      "epoch": 1.90625,
      "grad_norm": 6.560915470123291,
      "learning_rate": 8.993055555555556e-06,
      "loss": 1.8465,
      "step": 915
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 8.028507232666016,
      "learning_rate": 8.99074074074074e-06,
      "loss": 1.4409,
      "step": 916
    },
    {
      "epoch": 1.9104166666666667,
      "grad_norm": 7.359493255615234,
      "learning_rate": 8.988425925925927e-06,
      "loss": 1.1926,
      "step": 917
    },
    {
      "epoch": 1.9125,
      "grad_norm": 7.468618869781494,
      "learning_rate": 8.986111111111113e-06,
      "loss": 1.6706,
      "step": 918
    },
    {
      "epoch": 1.9145833333333333,
      "grad_norm": 8.47833251953125,
      "learning_rate": 8.983796296296297e-06,
      "loss": 1.3987,
      "step": 919
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 7.997312545776367,
      "learning_rate": 8.981481481481483e-06,
      "loss": 1.2925,
      "step": 920
    },
    {
      "epoch": 1.91875,
      "grad_norm": 9.810345649719238,
      "learning_rate": 8.979166666666667e-06,
      "loss": 1.7328,
      "step": 921
    },
    {
      "epoch": 1.9208333333333334,
      "grad_norm": 14.142459869384766,
      "learning_rate": 8.976851851851853e-06,
      "loss": 1.2843,
      "step": 922
    },
    {
      "epoch": 1.9229166666666666,
      "grad_norm": 11.938398361206055,
      "learning_rate": 8.974537037037037e-06,
      "loss": 1.723,
      "step": 923
    },
    {
      "epoch": 1.925,
      "grad_norm": 13.46699333190918,
      "learning_rate": 8.972222222222223e-06,
      "loss": 2.0628,
      "step": 924
    },
    {
      "epoch": 1.9270833333333335,
      "grad_norm": 8.581670761108398,
      "learning_rate": 8.969907407407407e-06,
      "loss": 1.6072,
      "step": 925
    },
    {
      "epoch": 1.9291666666666667,
      "grad_norm": 7.81107234954834,
      "learning_rate": 8.967592592592593e-06,
      "loss": 1.1864,
      "step": 926
    },
    {
      "epoch": 1.93125,
      "grad_norm": 8.888562202453613,
      "learning_rate": 8.96527777777778e-06,
      "loss": 1.1963,
      "step": 927
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 16.082992553710938,
      "learning_rate": 8.962962962962963e-06,
      "loss": 1.6016,
      "step": 928
    },
    {
      "epoch": 1.9354166666666668,
      "grad_norm": 8.49306583404541,
      "learning_rate": 8.96064814814815e-06,
      "loss": 1.5287,
      "step": 929
    },
    {
      "epoch": 1.9375,
      "grad_norm": 13.873140335083008,
      "learning_rate": 8.958333333333334e-06,
      "loss": 1.2438,
      "step": 930
    },
    {
      "epoch": 1.9395833333333332,
      "grad_norm": 7.612985610961914,
      "learning_rate": 8.95601851851852e-06,
      "loss": 1.7006,
      "step": 931
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 15.920992851257324,
      "learning_rate": 8.953703703703704e-06,
      "loss": 1.51,
      "step": 932
    },
    {
      "epoch": 1.94375,
      "grad_norm": 24.004409790039062,
      "learning_rate": 8.95138888888889e-06,
      "loss": 1.597,
      "step": 933
    },
    {
      "epoch": 1.9458333333333333,
      "grad_norm": 14.39924144744873,
      "learning_rate": 8.949074074074074e-06,
      "loss": 1.978,
      "step": 934
    },
    {
      "epoch": 1.9479166666666665,
      "grad_norm": 9.981633186340332,
      "learning_rate": 8.94675925925926e-06,
      "loss": 1.7472,
      "step": 935
    },
    {
      "epoch": 1.95,
      "grad_norm": 9.47550106048584,
      "learning_rate": 8.944444444444446e-06,
      "loss": 1.8064,
      "step": 936
    },
    {
      "epoch": 1.9520833333333334,
      "grad_norm": 16.91872215270996,
      "learning_rate": 8.94212962962963e-06,
      "loss": 0.8537,
      "step": 937
    },
    {
      "epoch": 1.9541666666666666,
      "grad_norm": 10.532050132751465,
      "learning_rate": 8.939814814814816e-06,
      "loss": 1.1795,
      "step": 938
    },
    {
      "epoch": 1.95625,
      "grad_norm": 10.26323413848877,
      "learning_rate": 8.9375e-06,
      "loss": 2.1788,
      "step": 939
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 8.952888488769531,
      "learning_rate": 8.935185185185186e-06,
      "loss": 1.3374,
      "step": 940
    },
    {
      "epoch": 1.9604166666666667,
      "grad_norm": 16.264251708984375,
      "learning_rate": 8.93287037037037e-06,
      "loss": 2.1409,
      "step": 941
    },
    {
      "epoch": 1.9625,
      "grad_norm": 11.512383460998535,
      "learning_rate": 8.930555555555557e-06,
      "loss": 1.7176,
      "step": 942
    },
    {
      "epoch": 1.9645833333333333,
      "grad_norm": 8.073332786560059,
      "learning_rate": 8.92824074074074e-06,
      "loss": 1.5532,
      "step": 943
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 11.432637214660645,
      "learning_rate": 8.925925925925927e-06,
      "loss": 0.6439,
      "step": 944
    },
    {
      "epoch": 1.96875,
      "grad_norm": 13.920961380004883,
      "learning_rate": 8.923611111111113e-06,
      "loss": 1.7579,
      "step": 945
    },
    {
      "epoch": 1.9708333333333332,
      "grad_norm": 9.424504280090332,
      "learning_rate": 8.921296296296297e-06,
      "loss": 1.1218,
      "step": 946
    },
    {
      "epoch": 1.9729166666666667,
      "grad_norm": 12.746904373168945,
      "learning_rate": 8.918981481481483e-06,
      "loss": 1.8139,
      "step": 947
    },
    {
      "epoch": 1.975,
      "grad_norm": 9.633855819702148,
      "learning_rate": 8.916666666666667e-06,
      "loss": 1.1521,
      "step": 948
    },
    {
      "epoch": 1.9770833333333333,
      "grad_norm": 10.285284996032715,
      "learning_rate": 8.914351851851853e-06,
      "loss": 1.8117,
      "step": 949
    },
    {
      "epoch": 1.9791666666666665,
      "grad_norm": 10.111377716064453,
      "learning_rate": 8.912037037037037e-06,
      "loss": 1.6527,
      "step": 950
    },
    {
      "epoch": 1.98125,
      "grad_norm": 6.363406181335449,
      "learning_rate": 8.909722222222223e-06,
      "loss": 1.2558,
      "step": 951
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 8.595080375671387,
      "learning_rate": 8.907407407407408e-06,
      "loss": 1.162,
      "step": 952
    },
    {
      "epoch": 1.9854166666666666,
      "grad_norm": 17.88335418701172,
      "learning_rate": 8.905092592592593e-06,
      "loss": 2.3154,
      "step": 953
    },
    {
      "epoch": 1.9875,
      "grad_norm": 11.945383071899414,
      "learning_rate": 8.902777777777778e-06,
      "loss": 1.6338,
      "step": 954
    },
    {
      "epoch": 1.9895833333333335,
      "grad_norm": 8.615144729614258,
      "learning_rate": 8.900462962962964e-06,
      "loss": 1.9074,
      "step": 955
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 11.33499813079834,
      "learning_rate": 8.89814814814815e-06,
      "loss": 0.6742,
      "step": 956
    },
    {
      "epoch": 1.99375,
      "grad_norm": 12.203349113464355,
      "learning_rate": 8.895833333333334e-06,
      "loss": 1.6212,
      "step": 957
    },
    {
      "epoch": 1.9958333333333333,
      "grad_norm": 14.368036270141602,
      "learning_rate": 8.89351851851852e-06,
      "loss": 1.754,
      "step": 958
    },
    {
      "epoch": 1.9979166666666668,
      "grad_norm": 15.935209274291992,
      "learning_rate": 8.891203703703704e-06,
      "loss": 2.4105,
      "step": 959
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.258042335510254,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.2768,
      "step": 960
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.3333333333333333,
      "eval_f1": 0.15716205837173577,
      "eval_loss": 1.6416659355163574,
      "eval_runtime": 34.3996,
      "eval_samples_per_second": 5.233,
      "eval_steps_per_second": 2.616,
      "step": 960
    },
    {
      "epoch": 2.002083333333333,
      "grad_norm": 17.505138397216797,
      "learning_rate": 8.886574074074074e-06,
      "loss": 1.7279,
      "step": 961
    },
    {
      "epoch": 2.004166666666667,
      "grad_norm": 15.828010559082031,
      "learning_rate": 8.88425925925926e-06,
      "loss": 1.5297,
      "step": 962
    },
    {
      "epoch": 2.00625,
      "grad_norm": 8.615467071533203,
      "learning_rate": 8.881944444444444e-06,
      "loss": 1.1519,
      "step": 963
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 13.710670471191406,
      "learning_rate": 8.87962962962963e-06,
      "loss": 1.7666,
      "step": 964
    },
    {
      "epoch": 2.0104166666666665,
      "grad_norm": 8.831527709960938,
      "learning_rate": 8.877314814814816e-06,
      "loss": 1.2023,
      "step": 965
    },
    {
      "epoch": 2.0125,
      "grad_norm": 8.850103378295898,
      "learning_rate": 8.875e-06,
      "loss": 1.3237,
      "step": 966
    },
    {
      "epoch": 2.0145833333333334,
      "grad_norm": 16.94955825805664,
      "learning_rate": 8.872685185185187e-06,
      "loss": 2.2988,
      "step": 967
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 10.445310592651367,
      "learning_rate": 8.87037037037037e-06,
      "loss": 1.6101,
      "step": 968
    },
    {
      "epoch": 2.01875,
      "grad_norm": 13.528011322021484,
      "learning_rate": 8.868055555555555e-06,
      "loss": 1.4172,
      "step": 969
    },
    {
      "epoch": 2.0208333333333335,
      "grad_norm": 9.233993530273438,
      "learning_rate": 8.865740740740741e-06,
      "loss": 0.8443,
      "step": 970
    },
    {
      "epoch": 2.0229166666666667,
      "grad_norm": 13.101081848144531,
      "learning_rate": 8.863425925925927e-06,
      "loss": 2.1395,
      "step": 971
    },
    {
      "epoch": 2.025,
      "grad_norm": 12.63184928894043,
      "learning_rate": 8.861111111111111e-06,
      "loss": 1.8229,
      "step": 972
    },
    {
      "epoch": 2.027083333333333,
      "grad_norm": 11.180068016052246,
      "learning_rate": 8.858796296296297e-06,
      "loss": 1.7163,
      "step": 973
    },
    {
      "epoch": 2.029166666666667,
      "grad_norm": 8.473617553710938,
      "learning_rate": 8.856481481481483e-06,
      "loss": 1.7636,
      "step": 974
    },
    {
      "epoch": 2.03125,
      "grad_norm": 7.105902671813965,
      "learning_rate": 8.854166666666667e-06,
      "loss": 1.2444,
      "step": 975
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 9.455431938171387,
      "learning_rate": 8.851851851851853e-06,
      "loss": 0.9837,
      "step": 976
    },
    {
      "epoch": 2.035416666666667,
      "grad_norm": 8.007153511047363,
      "learning_rate": 8.849537037037037e-06,
      "loss": 1.7348,
      "step": 977
    },
    {
      "epoch": 2.0375,
      "grad_norm": 11.568887710571289,
      "learning_rate": 8.847222222222223e-06,
      "loss": 1.7189,
      "step": 978
    },
    {
      "epoch": 2.0395833333333333,
      "grad_norm": 9.589680671691895,
      "learning_rate": 8.844907407407408e-06,
      "loss": 1.6211,
      "step": 979
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 7.888820648193359,
      "learning_rate": 8.842592592592594e-06,
      "loss": 1.2208,
      "step": 980
    },
    {
      "epoch": 2.04375,
      "grad_norm": 11.599005699157715,
      "learning_rate": 8.840277777777778e-06,
      "loss": 1.2025,
      "step": 981
    },
    {
      "epoch": 2.0458333333333334,
      "grad_norm": 8.624143600463867,
      "learning_rate": 8.837962962962964e-06,
      "loss": 1.2305,
      "step": 982
    },
    {
      "epoch": 2.0479166666666666,
      "grad_norm": 8.003819465637207,
      "learning_rate": 8.83564814814815e-06,
      "loss": 1.2598,
      "step": 983
    },
    {
      "epoch": 2.05,
      "grad_norm": 8.585545539855957,
      "learning_rate": 8.833333333333334e-06,
      "loss": 1.2517,
      "step": 984
    },
    {
      "epoch": 2.0520833333333335,
      "grad_norm": 21.967798233032227,
      "learning_rate": 8.83101851851852e-06,
      "loss": 2.5372,
      "step": 985
    },
    {
      "epoch": 2.0541666666666667,
      "grad_norm": 11.639695167541504,
      "learning_rate": 8.828703703703704e-06,
      "loss": 1.8552,
      "step": 986
    },
    {
      "epoch": 2.05625,
      "grad_norm": 61.098731994628906,
      "learning_rate": 8.82638888888889e-06,
      "loss": 2.2642,
      "step": 987
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 8.151710510253906,
      "learning_rate": 8.824074074074074e-06,
      "loss": 1.4026,
      "step": 988
    },
    {
      "epoch": 2.060416666666667,
      "grad_norm": 11.195569038391113,
      "learning_rate": 8.82175925925926e-06,
      "loss": 2.0343,
      "step": 989
    },
    {
      "epoch": 2.0625,
      "grad_norm": 32.11823272705078,
      "learning_rate": 8.819444444444445e-06,
      "loss": 2.2804,
      "step": 990
    },
    {
      "epoch": 2.064583333333333,
      "grad_norm": 8.419768333435059,
      "learning_rate": 8.81712962962963e-06,
      "loss": 1.2487,
      "step": 991
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 7.250988960266113,
      "learning_rate": 8.814814814814817e-06,
      "loss": 1.4987,
      "step": 992
    },
    {
      "epoch": 2.06875,
      "grad_norm": 9.426046371459961,
      "learning_rate": 8.8125e-06,
      "loss": 1.588,
      "step": 993
    },
    {
      "epoch": 2.0708333333333333,
      "grad_norm": 12.07608413696289,
      "learning_rate": 8.810185185185187e-06,
      "loss": 1.7605,
      "step": 994
    },
    {
      "epoch": 2.0729166666666665,
      "grad_norm": 16.244802474975586,
      "learning_rate": 8.807870370370371e-06,
      "loss": 1.9868,
      "step": 995
    },
    {
      "epoch": 2.075,
      "grad_norm": 8.760147094726562,
      "learning_rate": 8.805555555555557e-06,
      "loss": 1.8971,
      "step": 996
    },
    {
      "epoch": 2.0770833333333334,
      "grad_norm": 13.148557662963867,
      "learning_rate": 8.803240740740741e-06,
      "loss": 1.9102,
      "step": 997
    },
    {
      "epoch": 2.0791666666666666,
      "grad_norm": 11.824660301208496,
      "learning_rate": 8.800925925925925e-06,
      "loss": 1.636,
      "step": 998
    },
    {
      "epoch": 2.08125,
      "grad_norm": 11.991376876831055,
      "learning_rate": 8.798611111111111e-06,
      "loss": 1.7731,
      "step": 999
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 9.435819625854492,
      "learning_rate": 8.796296296296297e-06,
      "loss": 1.0676,
      "step": 1000
    },
    {
      "epoch": 2.0854166666666667,
      "grad_norm": 9.568175315856934,
      "learning_rate": 8.793981481481482e-06,
      "loss": 1.5325,
      "step": 1001
    },
    {
      "epoch": 2.0875,
      "grad_norm": 9.623245239257812,
      "learning_rate": 8.791666666666667e-06,
      "loss": 1.1106,
      "step": 1002
    },
    {
      "epoch": 2.089583333333333,
      "grad_norm": 8.648161888122559,
      "learning_rate": 8.789351851851853e-06,
      "loss": 1.402,
      "step": 1003
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 8.214733123779297,
      "learning_rate": 8.787037037037038e-06,
      "loss": 1.0707,
      "step": 1004
    },
    {
      "epoch": 2.09375,
      "grad_norm": 11.947189331054688,
      "learning_rate": 8.784722222222224e-06,
      "loss": 1.8378,
      "step": 1005
    },
    {
      "epoch": 2.095833333333333,
      "grad_norm": 8.444190979003906,
      "learning_rate": 8.782407407407408e-06,
      "loss": 1.6783,
      "step": 1006
    },
    {
      "epoch": 2.097916666666667,
      "grad_norm": 12.355681419372559,
      "learning_rate": 8.780092592592592e-06,
      "loss": 1.7315,
      "step": 1007
    },
    {
      "epoch": 2.1,
      "grad_norm": 9.787135124206543,
      "learning_rate": 8.777777777777778e-06,
      "loss": 1.161,
      "step": 1008
    },
    {
      "epoch": 2.1020833333333333,
      "grad_norm": 18.69886589050293,
      "learning_rate": 8.775462962962964e-06,
      "loss": 2.4445,
      "step": 1009
    },
    {
      "epoch": 2.1041666666666665,
      "grad_norm": 8.785627365112305,
      "learning_rate": 8.773148148148148e-06,
      "loss": 2.0025,
      "step": 1010
    },
    {
      "epoch": 2.10625,
      "grad_norm": 11.318119049072266,
      "learning_rate": 8.770833333333334e-06,
      "loss": 2.1604,
      "step": 1011
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 17.639484405517578,
      "learning_rate": 8.76851851851852e-06,
      "loss": 2.5209,
      "step": 1012
    },
    {
      "epoch": 2.1104166666666666,
      "grad_norm": 9.10660457611084,
      "learning_rate": 8.766203703703704e-06,
      "loss": 1.7252,
      "step": 1013
    },
    {
      "epoch": 2.1125,
      "grad_norm": 31.707143783569336,
      "learning_rate": 8.76388888888889e-06,
      "loss": 1.8272,
      "step": 1014
    },
    {
      "epoch": 2.1145833333333335,
      "grad_norm": 12.820146560668945,
      "learning_rate": 8.761574074074075e-06,
      "loss": 1.4902,
      "step": 1015
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 8.500635147094727,
      "learning_rate": 8.759259259259259e-06,
      "loss": 1.2681,
      "step": 1016
    },
    {
      "epoch": 2.11875,
      "grad_norm": 11.613377571105957,
      "learning_rate": 8.756944444444445e-06,
      "loss": 1.8447,
      "step": 1017
    },
    {
      "epoch": 2.120833333333333,
      "grad_norm": 7.901694297790527,
      "learning_rate": 8.75462962962963e-06,
      "loss": 1.2644,
      "step": 1018
    },
    {
      "epoch": 2.122916666666667,
      "grad_norm": 10.378113746643066,
      "learning_rate": 8.752314814814815e-06,
      "loss": 1.6724,
      "step": 1019
    },
    {
      "epoch": 2.125,
      "grad_norm": 9.32404613494873,
      "learning_rate": 8.750000000000001e-06,
      "loss": 1.3837,
      "step": 1020
    },
    {
      "epoch": 2.127083333333333,
      "grad_norm": 9.051942825317383,
      "learning_rate": 8.747685185185187e-06,
      "loss": 1.0688,
      "step": 1021
    },
    {
      "epoch": 2.129166666666667,
      "grad_norm": 15.63863754272461,
      "learning_rate": 8.745370370370371e-06,
      "loss": 2.1611,
      "step": 1022
    },
    {
      "epoch": 2.13125,
      "grad_norm": 12.427881240844727,
      "learning_rate": 8.743055555555557e-06,
      "loss": 1.8647,
      "step": 1023
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 8.753654479980469,
      "learning_rate": 8.740740740740741e-06,
      "loss": 1.4002,
      "step": 1024
    },
    {
      "epoch": 2.1354166666666665,
      "grad_norm": 14.128405570983887,
      "learning_rate": 8.738425925925926e-06,
      "loss": 2.2227,
      "step": 1025
    },
    {
      "epoch": 2.1375,
      "grad_norm": 13.058379173278809,
      "learning_rate": 8.736111111111112e-06,
      "loss": 2.1179,
      "step": 1026
    },
    {
      "epoch": 2.1395833333333334,
      "grad_norm": 11.635553359985352,
      "learning_rate": 8.733796296296297e-06,
      "loss": 1.9017,
      "step": 1027
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 18.97733497619629,
      "learning_rate": 8.731481481481482e-06,
      "loss": 2.499,
      "step": 1028
    },
    {
      "epoch": 2.14375,
      "grad_norm": 20.088869094848633,
      "learning_rate": 8.729166666666668e-06,
      "loss": 2.7545,
      "step": 1029
    },
    {
      "epoch": 2.1458333333333335,
      "grad_norm": 9.801322937011719,
      "learning_rate": 8.726851851851854e-06,
      "loss": 1.7492,
      "step": 1030
    },
    {
      "epoch": 2.1479166666666667,
      "grad_norm": 15.409847259521484,
      "learning_rate": 8.724537037037038e-06,
      "loss": 1.5726,
      "step": 1031
    },
    {
      "epoch": 2.15,
      "grad_norm": 9.94548511505127,
      "learning_rate": 8.722222222222224e-06,
      "loss": 2.2664,
      "step": 1032
    },
    {
      "epoch": 2.152083333333333,
      "grad_norm": 13.050732612609863,
      "learning_rate": 8.719907407407408e-06,
      "loss": 1.7266,
      "step": 1033
    },
    {
      "epoch": 2.154166666666667,
      "grad_norm": 9.038129806518555,
      "learning_rate": 8.717592592592592e-06,
      "loss": 1.4083,
      "step": 1034
    },
    {
      "epoch": 2.15625,
      "grad_norm": 9.029563903808594,
      "learning_rate": 8.715277777777778e-06,
      "loss": 1.4425,
      "step": 1035
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 9.956282615661621,
      "learning_rate": 8.712962962962964e-06,
      "loss": 1.4183,
      "step": 1036
    },
    {
      "epoch": 2.160416666666667,
      "grad_norm": 14.937089920043945,
      "learning_rate": 8.710648148148148e-06,
      "loss": 2.4396,
      "step": 1037
    },
    {
      "epoch": 2.1625,
      "grad_norm": 11.772575378417969,
      "learning_rate": 8.708333333333334e-06,
      "loss": 1.7651,
      "step": 1038
    },
    {
      "epoch": 2.1645833333333333,
      "grad_norm": 8.125516891479492,
      "learning_rate": 8.70601851851852e-06,
      "loss": 1.9031,
      "step": 1039
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 26.11501693725586,
      "learning_rate": 8.703703703703705e-06,
      "loss": 1.3808,
      "step": 1040
    },
    {
      "epoch": 2.16875,
      "grad_norm": 10.318233489990234,
      "learning_rate": 8.70138888888889e-06,
      "loss": 1.912,
      "step": 1041
    },
    {
      "epoch": 2.1708333333333334,
      "grad_norm": 14.600889205932617,
      "learning_rate": 8.699074074074075e-06,
      "loss": 1.7945,
      "step": 1042
    },
    {
      "epoch": 2.1729166666666666,
      "grad_norm": 10.99664306640625,
      "learning_rate": 8.696759259259259e-06,
      "loss": 1.7115,
      "step": 1043
    },
    {
      "epoch": 2.175,
      "grad_norm": 11.00523567199707,
      "learning_rate": 8.694444444444445e-06,
      "loss": 1.6599,
      "step": 1044
    },
    {
      "epoch": 2.1770833333333335,
      "grad_norm": 27.966215133666992,
      "learning_rate": 8.69212962962963e-06,
      "loss": 2.1223,
      "step": 1045
    },
    {
      "epoch": 2.1791666666666667,
      "grad_norm": 10.139533996582031,
      "learning_rate": 8.689814814814815e-06,
      "loss": 1.9912,
      "step": 1046
    },
    {
      "epoch": 2.18125,
      "grad_norm": 16.689626693725586,
      "learning_rate": 8.687500000000001e-06,
      "loss": 2.1556,
      "step": 1047
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 13.48896598815918,
      "learning_rate": 8.685185185185185e-06,
      "loss": 1.7107,
      "step": 1048
    },
    {
      "epoch": 2.185416666666667,
      "grad_norm": 11.626394271850586,
      "learning_rate": 8.682870370370371e-06,
      "loss": 0.5612,
      "step": 1049
    },
    {
      "epoch": 2.1875,
      "grad_norm": 19.81576156616211,
      "learning_rate": 8.680555555555557e-06,
      "loss": 2.0001,
      "step": 1050
    },
    {
      "epoch": 2.189583333333333,
      "grad_norm": 12.047005653381348,
      "learning_rate": 8.678240740740741e-06,
      "loss": 2.0681,
      "step": 1051
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 7.131646156311035,
      "learning_rate": 8.675925925925926e-06,
      "loss": 1.7077,
      "step": 1052
    },
    {
      "epoch": 2.19375,
      "grad_norm": 10.274662971496582,
      "learning_rate": 8.673611111111112e-06,
      "loss": 1.1085,
      "step": 1053
    },
    {
      "epoch": 2.1958333333333333,
      "grad_norm": 7.994170188903809,
      "learning_rate": 8.671296296296296e-06,
      "loss": 1.8121,
      "step": 1054
    },
    {
      "epoch": 2.1979166666666665,
      "grad_norm": 9.318581581115723,
      "learning_rate": 8.668981481481482e-06,
      "loss": 2.0305,
      "step": 1055
    },
    {
      "epoch": 2.2,
      "grad_norm": 9.251644134521484,
      "learning_rate": 8.666666666666668e-06,
      "loss": 2.001,
      "step": 1056
    },
    {
      "epoch": 2.2020833333333334,
      "grad_norm": 6.571701526641846,
      "learning_rate": 8.664351851851852e-06,
      "loss": 1.2424,
      "step": 1057
    },
    {
      "epoch": 2.2041666666666666,
      "grad_norm": 11.786820411682129,
      "learning_rate": 8.662037037037038e-06,
      "loss": 0.6491,
      "step": 1058
    },
    {
      "epoch": 2.20625,
      "grad_norm": 10.022692680358887,
      "learning_rate": 8.659722222222224e-06,
      "loss": 1.9592,
      "step": 1059
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 11.003902435302734,
      "learning_rate": 8.657407407407408e-06,
      "loss": 1.2269,
      "step": 1060
    },
    {
      "epoch": 2.2104166666666667,
      "grad_norm": 14.948573112487793,
      "learning_rate": 8.655092592592592e-06,
      "loss": 1.8775,
      "step": 1061
    },
    {
      "epoch": 2.2125,
      "grad_norm": 9.355350494384766,
      "learning_rate": 8.652777777777778e-06,
      "loss": 1.7244,
      "step": 1062
    },
    {
      "epoch": 2.214583333333333,
      "grad_norm": 11.466259956359863,
      "learning_rate": 8.650462962962963e-06,
      "loss": 2.3101,
      "step": 1063
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 8.667525291442871,
      "learning_rate": 8.648148148148149e-06,
      "loss": 1.5937,
      "step": 1064
    },
    {
      "epoch": 2.21875,
      "grad_norm": 7.799858093261719,
      "learning_rate": 8.645833333333335e-06,
      "loss": 1.7353,
      "step": 1065
    },
    {
      "epoch": 2.220833333333333,
      "grad_norm": 11.262869834899902,
      "learning_rate": 8.643518518518519e-06,
      "loss": 1.8104,
      "step": 1066
    },
    {
      "epoch": 2.222916666666667,
      "grad_norm": 7.854640483856201,
      "learning_rate": 8.641203703703705e-06,
      "loss": 1.1959,
      "step": 1067
    },
    {
      "epoch": 2.225,
      "grad_norm": 10.286930084228516,
      "learning_rate": 8.63888888888889e-06,
      "loss": 1.0538,
      "step": 1068
    },
    {
      "epoch": 2.2270833333333333,
      "grad_norm": 7.2305588722229,
      "learning_rate": 8.636574074074075e-06,
      "loss": 1.1072,
      "step": 1069
    },
    {
      "epoch": 2.2291666666666665,
      "grad_norm": 23.53972625732422,
      "learning_rate": 8.63425925925926e-06,
      "loss": 2.0934,
      "step": 1070
    },
    {
      "epoch": 2.23125,
      "grad_norm": 7.177188873291016,
      "learning_rate": 8.631944444444445e-06,
      "loss": 1.8972,
      "step": 1071
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 14.440926551818848,
      "learning_rate": 8.62962962962963e-06,
      "loss": 1.8247,
      "step": 1072
    },
    {
      "epoch": 2.2354166666666666,
      "grad_norm": 8.146236419677734,
      "learning_rate": 8.627314814814815e-06,
      "loss": 1.8001,
      "step": 1073
    },
    {
      "epoch": 2.2375,
      "grad_norm": 10.972853660583496,
      "learning_rate": 8.625000000000001e-06,
      "loss": 1.2114,
      "step": 1074
    },
    {
      "epoch": 2.2395833333333335,
      "grad_norm": 15.66885757446289,
      "learning_rate": 8.622685185185186e-06,
      "loss": 1.9372,
      "step": 1075
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 9.493586540222168,
      "learning_rate": 8.620370370370371e-06,
      "loss": 1.2032,
      "step": 1076
    },
    {
      "epoch": 2.24375,
      "grad_norm": 17.676889419555664,
      "learning_rate": 8.618055555555557e-06,
      "loss": 2.0101,
      "step": 1077
    },
    {
      "epoch": 2.245833333333333,
      "grad_norm": 9.166789054870605,
      "learning_rate": 8.615740740740742e-06,
      "loss": 1.6538,
      "step": 1078
    },
    {
      "epoch": 2.247916666666667,
      "grad_norm": 11.15358829498291,
      "learning_rate": 8.613425925925926e-06,
      "loss": 0.564,
      "step": 1079
    },
    {
      "epoch": 2.25,
      "grad_norm": 11.859816551208496,
      "learning_rate": 8.611111111111112e-06,
      "loss": 1.7357,
      "step": 1080
    },
    {
      "epoch": 2.252083333333333,
      "grad_norm": 16.363523483276367,
      "learning_rate": 8.608796296296296e-06,
      "loss": 1.7869,
      "step": 1081
    },
    {
      "epoch": 2.2541666666666664,
      "grad_norm": 8.964293479919434,
      "learning_rate": 8.606481481481482e-06,
      "loss": 1.8315,
      "step": 1082
    },
    {
      "epoch": 2.25625,
      "grad_norm": 9.226705551147461,
      "learning_rate": 8.604166666666668e-06,
      "loss": 1.4544,
      "step": 1083
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 7.844170570373535,
      "learning_rate": 8.601851851851852e-06,
      "loss": 1.2504,
      "step": 1084
    },
    {
      "epoch": 2.2604166666666665,
      "grad_norm": 8.851114273071289,
      "learning_rate": 8.599537037037038e-06,
      "loss": 1.2154,
      "step": 1085
    },
    {
      "epoch": 2.2625,
      "grad_norm": 39.44384002685547,
      "learning_rate": 8.597222222222224e-06,
      "loss": 2.2322,
      "step": 1086
    },
    {
      "epoch": 2.2645833333333334,
      "grad_norm": 10.909282684326172,
      "learning_rate": 8.594907407407408e-06,
      "loss": 2.0231,
      "step": 1087
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 11.40078353881836,
      "learning_rate": 8.592592592592593e-06,
      "loss": 1.6571,
      "step": 1088
    },
    {
      "epoch": 2.26875,
      "grad_norm": 8.38719367980957,
      "learning_rate": 8.590277777777779e-06,
      "loss": 1.4682,
      "step": 1089
    },
    {
      "epoch": 2.2708333333333335,
      "grad_norm": 13.693059921264648,
      "learning_rate": 8.587962962962963e-06,
      "loss": 1.8023,
      "step": 1090
    },
    {
      "epoch": 2.2729166666666667,
      "grad_norm": 8.946663856506348,
      "learning_rate": 8.585648148148149e-06,
      "loss": 1.0279,
      "step": 1091
    },
    {
      "epoch": 2.275,
      "grad_norm": 15.73279857635498,
      "learning_rate": 8.583333333333333e-06,
      "loss": 1.277,
      "step": 1092
    },
    {
      "epoch": 2.2770833333333336,
      "grad_norm": 8.608046531677246,
      "learning_rate": 8.581018518518519e-06,
      "loss": 1.475,
      "step": 1093
    },
    {
      "epoch": 2.279166666666667,
      "grad_norm": 12.72873306274414,
      "learning_rate": 8.578703703703705e-06,
      "loss": 1.7934,
      "step": 1094
    },
    {
      "epoch": 2.28125,
      "grad_norm": 8.177120208740234,
      "learning_rate": 8.57638888888889e-06,
      "loss": 1.7392,
      "step": 1095
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 8.830512046813965,
      "learning_rate": 8.574074074074075e-06,
      "loss": 1.5528,
      "step": 1096
    },
    {
      "epoch": 2.2854166666666664,
      "grad_norm": 12.225396156311035,
      "learning_rate": 8.571759259259261e-06,
      "loss": 1.6144,
      "step": 1097
    },
    {
      "epoch": 2.2875,
      "grad_norm": 9.11170482635498,
      "learning_rate": 8.569444444444445e-06,
      "loss": 1.8883,
      "step": 1098
    },
    {
      "epoch": 2.2895833333333333,
      "grad_norm": 8.226595878601074,
      "learning_rate": 8.56712962962963e-06,
      "loss": 1.0981,
      "step": 1099
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 8.02590560913086,
      "learning_rate": 8.564814814814816e-06,
      "loss": 1.0823,
      "step": 1100
    },
    {
      "epoch": 2.29375,
      "grad_norm": 10.228697776794434,
      "learning_rate": 8.5625e-06,
      "loss": 2.1621,
      "step": 1101
    },
    {
      "epoch": 2.2958333333333334,
      "grad_norm": 6.7761335372924805,
      "learning_rate": 8.560185185185186e-06,
      "loss": 1.5721,
      "step": 1102
    },
    {
      "epoch": 2.2979166666666666,
      "grad_norm": 10.957170486450195,
      "learning_rate": 8.557870370370372e-06,
      "loss": 0.5649,
      "step": 1103
    },
    {
      "epoch": 2.3,
      "grad_norm": 17.946434020996094,
      "learning_rate": 8.555555555555556e-06,
      "loss": 0.7376,
      "step": 1104
    },
    {
      "epoch": 2.3020833333333335,
      "grad_norm": 10.481464385986328,
      "learning_rate": 8.553240740740742e-06,
      "loss": 1.1206,
      "step": 1105
    },
    {
      "epoch": 2.3041666666666667,
      "grad_norm": 14.930051803588867,
      "learning_rate": 8.550925925925928e-06,
      "loss": 1.888,
      "step": 1106
    },
    {
      "epoch": 2.30625,
      "grad_norm": 9.05218505859375,
      "learning_rate": 8.548611111111112e-06,
      "loss": 1.9903,
      "step": 1107
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 9.88666820526123,
      "learning_rate": 8.546296296296296e-06,
      "loss": 1.0584,
      "step": 1108
    },
    {
      "epoch": 2.310416666666667,
      "grad_norm": 12.817476272583008,
      "learning_rate": 8.543981481481482e-06,
      "loss": 1.3405,
      "step": 1109
    },
    {
      "epoch": 2.3125,
      "grad_norm": 13.553704261779785,
      "learning_rate": 8.541666666666666e-06,
      "loss": 1.806,
      "step": 1110
    },
    {
      "epoch": 2.314583333333333,
      "grad_norm": 6.008396625518799,
      "learning_rate": 8.539351851851852e-06,
      "loss": 1.7842,
      "step": 1111
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 10.426676750183105,
      "learning_rate": 8.537037037037038e-06,
      "loss": 1.8818,
      "step": 1112
    },
    {
      "epoch": 2.31875,
      "grad_norm": 16.070737838745117,
      "learning_rate": 8.534722222222223e-06,
      "loss": 1.7804,
      "step": 1113
    },
    {
      "epoch": 2.3208333333333333,
      "grad_norm": 11.112672805786133,
      "learning_rate": 8.532407407407409e-06,
      "loss": 0.5886,
      "step": 1114
    },
    {
      "epoch": 2.3229166666666665,
      "grad_norm": 19.53879737854004,
      "learning_rate": 8.530092592592595e-06,
      "loss": 1.9825,
      "step": 1115
    },
    {
      "epoch": 2.325,
      "grad_norm": 9.824698448181152,
      "learning_rate": 8.527777777777779e-06,
      "loss": 1.6216,
      "step": 1116
    },
    {
      "epoch": 2.3270833333333334,
      "grad_norm": 8.294476509094238,
      "learning_rate": 8.525462962962963e-06,
      "loss": 1.449,
      "step": 1117
    },
    {
      "epoch": 2.3291666666666666,
      "grad_norm": 12.571111679077148,
      "learning_rate": 8.523148148148149e-06,
      "loss": 1.5923,
      "step": 1118
    },
    {
      "epoch": 2.33125,
      "grad_norm": 6.474946022033691,
      "learning_rate": 8.520833333333333e-06,
      "loss": 1.227,
      "step": 1119
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 13.779489517211914,
      "learning_rate": 8.518518518518519e-06,
      "loss": 1.5792,
      "step": 1120
    },
    {
      "epoch": 2.3354166666666667,
      "grad_norm": 13.454984664916992,
      "learning_rate": 8.516203703703705e-06,
      "loss": 1.7922,
      "step": 1121
    },
    {
      "epoch": 2.3375,
      "grad_norm": 8.819160461425781,
      "learning_rate": 8.51388888888889e-06,
      "loss": 1.6711,
      "step": 1122
    },
    {
      "epoch": 2.3395833333333336,
      "grad_norm": 15.81314468383789,
      "learning_rate": 8.511574074074075e-06,
      "loss": 1.6902,
      "step": 1123
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 24.90575408935547,
      "learning_rate": 8.509259259259261e-06,
      "loss": 1.9451,
      "step": 1124
    },
    {
      "epoch": 2.34375,
      "grad_norm": 13.764931678771973,
      "learning_rate": 8.506944444444445e-06,
      "loss": 1.7561,
      "step": 1125
    },
    {
      "epoch": 2.345833333333333,
      "grad_norm": 15.479278564453125,
      "learning_rate": 8.50462962962963e-06,
      "loss": 2.436,
      "step": 1126
    },
    {
      "epoch": 2.3479166666666664,
      "grad_norm": 9.473422050476074,
      "learning_rate": 8.502314814814816e-06,
      "loss": 1.8305,
      "step": 1127
    },
    {
      "epoch": 2.35,
      "grad_norm": 6.369125843048096,
      "learning_rate": 8.5e-06,
      "loss": 1.9619,
      "step": 1128
    },
    {
      "epoch": 2.3520833333333333,
      "grad_norm": 10.63737964630127,
      "learning_rate": 8.497685185185186e-06,
      "loss": 0.5307,
      "step": 1129
    },
    {
      "epoch": 2.3541666666666665,
      "grad_norm": 20.078401565551758,
      "learning_rate": 8.495370370370372e-06,
      "loss": 2.1097,
      "step": 1130
    },
    {
      "epoch": 2.35625,
      "grad_norm": 12.422049522399902,
      "learning_rate": 8.493055555555556e-06,
      "loss": 2.0191,
      "step": 1131
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 17.020130157470703,
      "learning_rate": 8.490740740740742e-06,
      "loss": 1.998,
      "step": 1132
    },
    {
      "epoch": 2.3604166666666666,
      "grad_norm": 21.899930953979492,
      "learning_rate": 8.488425925925926e-06,
      "loss": 2.3456,
      "step": 1133
    },
    {
      "epoch": 2.3625,
      "grad_norm": 13.208564758300781,
      "learning_rate": 8.486111111111112e-06,
      "loss": 1.9165,
      "step": 1134
    },
    {
      "epoch": 2.3645833333333335,
      "grad_norm": 20.680850982666016,
      "learning_rate": 8.483796296296296e-06,
      "loss": 2.3025,
      "step": 1135
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 7.751391410827637,
      "learning_rate": 8.481481481481482e-06,
      "loss": 1.1642,
      "step": 1136
    },
    {
      "epoch": 2.36875,
      "grad_norm": 10.405619621276855,
      "learning_rate": 8.479166666666667e-06,
      "loss": 1.5422,
      "step": 1137
    },
    {
      "epoch": 2.3708333333333336,
      "grad_norm": 8.90152359008789,
      "learning_rate": 8.476851851851853e-06,
      "loss": 1.5019,
      "step": 1138
    },
    {
      "epoch": 2.372916666666667,
      "grad_norm": 11.284436225891113,
      "learning_rate": 8.474537037037037e-06,
      "loss": 1.702,
      "step": 1139
    },
    {
      "epoch": 2.375,
      "grad_norm": 12.490365982055664,
      "learning_rate": 8.472222222222223e-06,
      "loss": 1.8442,
      "step": 1140
    },
    {
      "epoch": 2.377083333333333,
      "grad_norm": 7.685700416564941,
      "learning_rate": 8.469907407407409e-06,
      "loss": 1.287,
      "step": 1141
    },
    {
      "epoch": 2.3791666666666664,
      "grad_norm": 10.64561939239502,
      "learning_rate": 8.467592592592593e-06,
      "loss": 1.5643,
      "step": 1142
    },
    {
      "epoch": 2.38125,
      "grad_norm": 8.646292686462402,
      "learning_rate": 8.465277777777779e-06,
      "loss": 1.6681,
      "step": 1143
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 7.388561725616455,
      "learning_rate": 8.462962962962963e-06,
      "loss": 1.7828,
      "step": 1144
    },
    {
      "epoch": 2.3854166666666665,
      "grad_norm": 13.410447120666504,
      "learning_rate": 8.460648148148149e-06,
      "loss": 1.266,
      "step": 1145
    },
    {
      "epoch": 2.3875,
      "grad_norm": 9.328567504882812,
      "learning_rate": 8.458333333333333e-06,
      "loss": 1.2317,
      "step": 1146
    },
    {
      "epoch": 2.3895833333333334,
      "grad_norm": 11.148921012878418,
      "learning_rate": 8.45601851851852e-06,
      "loss": 1.1627,
      "step": 1147
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 7.5279765129089355,
      "learning_rate": 8.453703703703704e-06,
      "loss": 1.6849,
      "step": 1148
    },
    {
      "epoch": 2.39375,
      "grad_norm": 9.579207420349121,
      "learning_rate": 8.45138888888889e-06,
      "loss": 1.6829,
      "step": 1149
    },
    {
      "epoch": 2.3958333333333335,
      "grad_norm": 15.094261169433594,
      "learning_rate": 8.449074074074075e-06,
      "loss": 1.2646,
      "step": 1150
    },
    {
      "epoch": 2.3979166666666667,
      "grad_norm": 19.201276779174805,
      "learning_rate": 8.44675925925926e-06,
      "loss": 2.7349,
      "step": 1151
    },
    {
      "epoch": 2.4,
      "grad_norm": 17.091293334960938,
      "learning_rate": 8.444444444444446e-06,
      "loss": 1.5774,
      "step": 1152
    },
    {
      "epoch": 2.4020833333333336,
      "grad_norm": 14.454021453857422,
      "learning_rate": 8.44212962962963e-06,
      "loss": 1.6978,
      "step": 1153
    },
    {
      "epoch": 2.404166666666667,
      "grad_norm": 10.111045837402344,
      "learning_rate": 8.439814814814816e-06,
      "loss": 0.9965,
      "step": 1154
    },
    {
      "epoch": 2.40625,
      "grad_norm": 8.932780265808105,
      "learning_rate": 8.4375e-06,
      "loss": 1.599,
      "step": 1155
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 9.152031898498535,
      "learning_rate": 8.435185185185186e-06,
      "loss": 1.7668,
      "step": 1156
    },
    {
      "epoch": 2.4104166666666664,
      "grad_norm": 11.569371223449707,
      "learning_rate": 8.43287037037037e-06,
      "loss": 1.4614,
      "step": 1157
    },
    {
      "epoch": 2.4125,
      "grad_norm": 39.43797302246094,
      "learning_rate": 8.430555555555556e-06,
      "loss": 2.4643,
      "step": 1158
    },
    {
      "epoch": 2.4145833333333333,
      "grad_norm": 9.867234230041504,
      "learning_rate": 8.428240740740742e-06,
      "loss": 1.4838,
      "step": 1159
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 9.882936477661133,
      "learning_rate": 8.425925925925926e-06,
      "loss": 1.0417,
      "step": 1160
    },
    {
      "epoch": 2.41875,
      "grad_norm": 10.584673881530762,
      "learning_rate": 8.423611111111112e-06,
      "loss": 1.4668,
      "step": 1161
    },
    {
      "epoch": 2.4208333333333334,
      "grad_norm": 16.572031021118164,
      "learning_rate": 8.421296296296297e-06,
      "loss": 1.472,
      "step": 1162
    },
    {
      "epoch": 2.4229166666666666,
      "grad_norm": 7.1535491943359375,
      "learning_rate": 8.418981481481483e-06,
      "loss": 2.3533,
      "step": 1163
    },
    {
      "epoch": 2.425,
      "grad_norm": 10.53370189666748,
      "learning_rate": 8.416666666666667e-06,
      "loss": 1.6806,
      "step": 1164
    },
    {
      "epoch": 2.4270833333333335,
      "grad_norm": 7.845157623291016,
      "learning_rate": 8.414351851851853e-06,
      "loss": 1.0488,
      "step": 1165
    },
    {
      "epoch": 2.4291666666666667,
      "grad_norm": 17.470008850097656,
      "learning_rate": 8.412037037037037e-06,
      "loss": 1.8842,
      "step": 1166
    },
    {
      "epoch": 2.43125,
      "grad_norm": 9.547412872314453,
      "learning_rate": 8.409722222222223e-06,
      "loss": 1.7572,
      "step": 1167
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 9.658899307250977,
      "learning_rate": 8.407407407407409e-06,
      "loss": 1.685,
      "step": 1168
    },
    {
      "epoch": 2.435416666666667,
      "grad_norm": 12.294544219970703,
      "learning_rate": 8.405092592592593e-06,
      "loss": 2.0077,
      "step": 1169
    },
    {
      "epoch": 2.4375,
      "grad_norm": 7.628542900085449,
      "learning_rate": 8.402777777777779e-06,
      "loss": 1.6387,
      "step": 1170
    },
    {
      "epoch": 2.439583333333333,
      "grad_norm": 9.468656539916992,
      "learning_rate": 8.400462962962963e-06,
      "loss": 1.7035,
      "step": 1171
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 9.449274063110352,
      "learning_rate": 8.39814814814815e-06,
      "loss": 1.7355,
      "step": 1172
    },
    {
      "epoch": 2.44375,
      "grad_norm": 15.584009170532227,
      "learning_rate": 8.395833333333334e-06,
      "loss": 1.6812,
      "step": 1173
    },
    {
      "epoch": 2.4458333333333333,
      "grad_norm": 17.59484100341797,
      "learning_rate": 8.39351851851852e-06,
      "loss": 1.9082,
      "step": 1174
    },
    {
      "epoch": 2.4479166666666665,
      "grad_norm": 11.812171936035156,
      "learning_rate": 8.391203703703704e-06,
      "loss": 1.9685,
      "step": 1175
    },
    {
      "epoch": 2.45,
      "grad_norm": 8.863175392150879,
      "learning_rate": 8.38888888888889e-06,
      "loss": 1.6255,
      "step": 1176
    },
    {
      "epoch": 2.4520833333333334,
      "grad_norm": 13.422099113464355,
      "learning_rate": 8.386574074074076e-06,
      "loss": 1.6075,
      "step": 1177
    },
    {
      "epoch": 2.4541666666666666,
      "grad_norm": 8.18246841430664,
      "learning_rate": 8.38425925925926e-06,
      "loss": 1.8179,
      "step": 1178
    },
    {
      "epoch": 2.45625,
      "grad_norm": 10.269169807434082,
      "learning_rate": 8.381944444444446e-06,
      "loss": 1.0702,
      "step": 1179
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 14.596285820007324,
      "learning_rate": 8.37962962962963e-06,
      "loss": 2.4177,
      "step": 1180
    },
    {
      "epoch": 2.4604166666666667,
      "grad_norm": 7.224671840667725,
      "learning_rate": 8.377314814814816e-06,
      "loss": 1.7267,
      "step": 1181
    },
    {
      "epoch": 2.4625,
      "grad_norm": 10.94937515258789,
      "learning_rate": 8.375e-06,
      "loss": 1.6536,
      "step": 1182
    },
    {
      "epoch": 2.4645833333333336,
      "grad_norm": 11.389430046081543,
      "learning_rate": 8.372685185185185e-06,
      "loss": 1.8055,
      "step": 1183
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 9.178133010864258,
      "learning_rate": 8.37037037037037e-06,
      "loss": 1.801,
      "step": 1184
    },
    {
      "epoch": 2.46875,
      "grad_norm": 8.055488586425781,
      "learning_rate": 8.368055555555556e-06,
      "loss": 1.2328,
      "step": 1185
    },
    {
      "epoch": 2.470833333333333,
      "grad_norm": 17.950702667236328,
      "learning_rate": 8.36574074074074e-06,
      "loss": 1.4279,
      "step": 1186
    },
    {
      "epoch": 2.4729166666666664,
      "grad_norm": 12.474373817443848,
      "learning_rate": 8.363425925925927e-06,
      "loss": 1.7026,
      "step": 1187
    },
    {
      "epoch": 2.475,
      "grad_norm": 7.335087299346924,
      "learning_rate": 8.361111111111113e-06,
      "loss": 1.5623,
      "step": 1188
    },
    {
      "epoch": 2.4770833333333333,
      "grad_norm": 15.906096458435059,
      "learning_rate": 8.358796296296297e-06,
      "loss": 1.4244,
      "step": 1189
    },
    {
      "epoch": 2.4791666666666665,
      "grad_norm": 7.049061298370361,
      "learning_rate": 8.356481481481483e-06,
      "loss": 1.2351,
      "step": 1190
    },
    {
      "epoch": 2.48125,
      "grad_norm": 14.200840950012207,
      "learning_rate": 8.354166666666667e-06,
      "loss": 1.8271,
      "step": 1191
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 9.512900352478027,
      "learning_rate": 8.351851851851851e-06,
      "loss": 1.7616,
      "step": 1192
    },
    {
      "epoch": 2.4854166666666666,
      "grad_norm": 19.416955947875977,
      "learning_rate": 8.349537037037037e-06,
      "loss": 0.7186,
      "step": 1193
    },
    {
      "epoch": 2.4875,
      "grad_norm": 8.822894096374512,
      "learning_rate": 8.347222222222223e-06,
      "loss": 1.6212,
      "step": 1194
    },
    {
      "epoch": 2.4895833333333335,
      "grad_norm": 12.722311973571777,
      "learning_rate": 8.344907407407407e-06,
      "loss": 0.6856,
      "step": 1195
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 17.736003875732422,
      "learning_rate": 8.342592592592593e-06,
      "loss": 1.5846,
      "step": 1196
    },
    {
      "epoch": 2.49375,
      "grad_norm": 12.81029987335205,
      "learning_rate": 8.34027777777778e-06,
      "loss": 1.6331,
      "step": 1197
    },
    {
      "epoch": 2.4958333333333336,
      "grad_norm": 34.34999084472656,
      "learning_rate": 8.337962962962964e-06,
      "loss": 1.7032,
      "step": 1198
    },
    {
      "epoch": 2.497916666666667,
      "grad_norm": 21.06304168701172,
      "learning_rate": 8.33564814814815e-06,
      "loss": 1.4256,
      "step": 1199
    },
    {
      "epoch": 2.5,
      "grad_norm": 21.016094207763672,
      "learning_rate": 8.333333333333334e-06,
      "loss": 2.3003,
      "step": 1200
    },
    {
      "epoch": 2.502083333333333,
      "grad_norm": 8.997235298156738,
      "learning_rate": 8.33101851851852e-06,
      "loss": 1.5173,
      "step": 1201
    },
    {
      "epoch": 2.5041666666666664,
      "grad_norm": 8.57131290435791,
      "learning_rate": 8.328703703703704e-06,
      "loss": 1.15,
      "step": 1202
    },
    {
      "epoch": 2.50625,
      "grad_norm": 13.015962600708008,
      "learning_rate": 8.32638888888889e-06,
      "loss": 1.5412,
      "step": 1203
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 11.598355293273926,
      "learning_rate": 8.324074074074074e-06,
      "loss": 1.7273,
      "step": 1204
    },
    {
      "epoch": 2.5104166666666665,
      "grad_norm": 9.410616874694824,
      "learning_rate": 8.32175925925926e-06,
      "loss": 1.3015,
      "step": 1205
    },
    {
      "epoch": 2.5125,
      "grad_norm": 11.995646476745605,
      "learning_rate": 8.319444444444446e-06,
      "loss": 2.2217,
      "step": 1206
    },
    {
      "epoch": 2.5145833333333334,
      "grad_norm": 12.367525100708008,
      "learning_rate": 8.31712962962963e-06,
      "loss": 1.7221,
      "step": 1207
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 7.784179210662842,
      "learning_rate": 8.314814814814816e-06,
      "loss": 1.9252,
      "step": 1208
    },
    {
      "epoch": 2.51875,
      "grad_norm": 25.05213737487793,
      "learning_rate": 8.3125e-06,
      "loss": 1.9001,
      "step": 1209
    },
    {
      "epoch": 2.5208333333333335,
      "grad_norm": 9.194501876831055,
      "learning_rate": 8.310185185185186e-06,
      "loss": 1.1578,
      "step": 1210
    },
    {
      "epoch": 2.5229166666666667,
      "grad_norm": 11.161489486694336,
      "learning_rate": 8.30787037037037e-06,
      "loss": 0.574,
      "step": 1211
    },
    {
      "epoch": 2.525,
      "grad_norm": 10.086304664611816,
      "learning_rate": 8.305555555555557e-06,
      "loss": 1.3927,
      "step": 1212
    },
    {
      "epoch": 2.5270833333333336,
      "grad_norm": 11.035353660583496,
      "learning_rate": 8.303240740740741e-06,
      "loss": 1.5794,
      "step": 1213
    },
    {
      "epoch": 2.529166666666667,
      "grad_norm": 9.292789459228516,
      "learning_rate": 8.300925925925927e-06,
      "loss": 1.3964,
      "step": 1214
    },
    {
      "epoch": 2.53125,
      "grad_norm": 9.62117862701416,
      "learning_rate": 8.298611111111113e-06,
      "loss": 1.6882,
      "step": 1215
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 7.850563049316406,
      "learning_rate": 8.296296296296297e-06,
      "loss": 1.1213,
      "step": 1216
    },
    {
      "epoch": 2.5354166666666664,
      "grad_norm": 12.361413955688477,
      "learning_rate": 8.293981481481483e-06,
      "loss": 1.7843,
      "step": 1217
    },
    {
      "epoch": 2.5375,
      "grad_norm": 10.588074684143066,
      "learning_rate": 8.291666666666667e-06,
      "loss": 1.9948,
      "step": 1218
    },
    {
      "epoch": 2.5395833333333333,
      "grad_norm": 6.471216678619385,
      "learning_rate": 8.289351851851853e-06,
      "loss": 1.1412,
      "step": 1219
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 26.79412269592285,
      "learning_rate": 8.287037037037037e-06,
      "loss": 2.0195,
      "step": 1220
    },
    {
      "epoch": 2.54375,
      "grad_norm": 11.03252124786377,
      "learning_rate": 8.284722222222223e-06,
      "loss": 0.8863,
      "step": 1221
    },
    {
      "epoch": 2.5458333333333334,
      "grad_norm": 23.00309181213379,
      "learning_rate": 8.282407407407408e-06,
      "loss": 1.4356,
      "step": 1222
    },
    {
      "epoch": 2.5479166666666666,
      "grad_norm": 10.04065990447998,
      "learning_rate": 8.280092592592594e-06,
      "loss": 1.6733,
      "step": 1223
    },
    {
      "epoch": 2.55,
      "grad_norm": 10.292444229125977,
      "learning_rate": 8.277777777777778e-06,
      "loss": 1.6256,
      "step": 1224
    },
    {
      "epoch": 2.5520833333333335,
      "grad_norm": 11.15899658203125,
      "learning_rate": 8.275462962962964e-06,
      "loss": 0.4884,
      "step": 1225
    },
    {
      "epoch": 2.5541666666666667,
      "grad_norm": 8.769241333007812,
      "learning_rate": 8.27314814814815e-06,
      "loss": 1.7496,
      "step": 1226
    },
    {
      "epoch": 2.55625,
      "grad_norm": 10.096172332763672,
      "learning_rate": 8.270833333333334e-06,
      "loss": 0.8597,
      "step": 1227
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 9.509612083435059,
      "learning_rate": 8.26851851851852e-06,
      "loss": 1.2769,
      "step": 1228
    },
    {
      "epoch": 2.560416666666667,
      "grad_norm": 18.74850082397461,
      "learning_rate": 8.266203703703704e-06,
      "loss": 2.0371,
      "step": 1229
    },
    {
      "epoch": 2.5625,
      "grad_norm": 15.35952091217041,
      "learning_rate": 8.263888888888888e-06,
      "loss": 1.6637,
      "step": 1230
    },
    {
      "epoch": 2.564583333333333,
      "grad_norm": 6.856828689575195,
      "learning_rate": 8.261574074074074e-06,
      "loss": 1.5349,
      "step": 1231
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 9.199795722961426,
      "learning_rate": 8.25925925925926e-06,
      "loss": 1.3595,
      "step": 1232
    },
    {
      "epoch": 2.56875,
      "grad_norm": 9.529607772827148,
      "learning_rate": 8.256944444444444e-06,
      "loss": 1.3212,
      "step": 1233
    },
    {
      "epoch": 2.5708333333333333,
      "grad_norm": 27.65325164794922,
      "learning_rate": 8.25462962962963e-06,
      "loss": 1.3392,
      "step": 1234
    },
    {
      "epoch": 2.5729166666666665,
      "grad_norm": 11.970836639404297,
      "learning_rate": 8.252314814814816e-06,
      "loss": 1.4061,
      "step": 1235
    },
    {
      "epoch": 2.575,
      "grad_norm": 66.14676666259766,
      "learning_rate": 8.25e-06,
      "loss": 1.9471,
      "step": 1236
    },
    {
      "epoch": 2.5770833333333334,
      "grad_norm": 13.504956245422363,
      "learning_rate": 8.247685185185187e-06,
      "loss": 1.0418,
      "step": 1237
    },
    {
      "epoch": 2.5791666666666666,
      "grad_norm": 10.019413948059082,
      "learning_rate": 8.24537037037037e-06,
      "loss": 1.169,
      "step": 1238
    },
    {
      "epoch": 2.58125,
      "grad_norm": 29.249860763549805,
      "learning_rate": 8.243055555555555e-06,
      "loss": 1.2263,
      "step": 1239
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 7.952947616577148,
      "learning_rate": 8.240740740740741e-06,
      "loss": 1.5342,
      "step": 1240
    },
    {
      "epoch": 2.5854166666666667,
      "grad_norm": 18.765335083007812,
      "learning_rate": 8.238425925925927e-06,
      "loss": 2.9262,
      "step": 1241
    },
    {
      "epoch": 2.5875,
      "grad_norm": 11.418972969055176,
      "learning_rate": 8.236111111111111e-06,
      "loss": 1.1299,
      "step": 1242
    },
    {
      "epoch": 2.5895833333333336,
      "grad_norm": 11.383533477783203,
      "learning_rate": 8.233796296296297e-06,
      "loss": 1.4165,
      "step": 1243
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 14.503146171569824,
      "learning_rate": 8.231481481481483e-06,
      "loss": 2.0711,
      "step": 1244
    },
    {
      "epoch": 2.59375,
      "grad_norm": 12.627099990844727,
      "learning_rate": 8.229166666666667e-06,
      "loss": 1.6964,
      "step": 1245
    },
    {
      "epoch": 2.595833333333333,
      "grad_norm": 9.701997756958008,
      "learning_rate": 8.226851851851853e-06,
      "loss": 0.4114,
      "step": 1246
    },
    {
      "epoch": 2.5979166666666664,
      "grad_norm": 8.883659362792969,
      "learning_rate": 8.224537037037038e-06,
      "loss": 1.7354,
      "step": 1247
    },
    {
      "epoch": 2.6,
      "grad_norm": 21.744863510131836,
      "learning_rate": 8.222222222222222e-06,
      "loss": 1.6546,
      "step": 1248
    },
    {
      "epoch": 2.6020833333333333,
      "grad_norm": 10.579623222351074,
      "learning_rate": 8.219907407407408e-06,
      "loss": 0.8976,
      "step": 1249
    },
    {
      "epoch": 2.6041666666666665,
      "grad_norm": 17.711694717407227,
      "learning_rate": 8.217592592592594e-06,
      "loss": 1.6078,
      "step": 1250
    },
    {
      "epoch": 2.60625,
      "grad_norm": 10.723112106323242,
      "learning_rate": 8.215277777777778e-06,
      "loss": 1.8858,
      "step": 1251
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 11.922599792480469,
      "learning_rate": 8.212962962962964e-06,
      "loss": 1.8472,
      "step": 1252
    },
    {
      "epoch": 2.6104166666666666,
      "grad_norm": 13.550307273864746,
      "learning_rate": 8.21064814814815e-06,
      "loss": 1.8583,
      "step": 1253
    },
    {
      "epoch": 2.6125,
      "grad_norm": 9.376236915588379,
      "learning_rate": 8.208333333333334e-06,
      "loss": 1.0249,
      "step": 1254
    },
    {
      "epoch": 2.6145833333333335,
      "grad_norm": 14.683326721191406,
      "learning_rate": 8.20601851851852e-06,
      "loss": 1.9005,
      "step": 1255
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 16.899473190307617,
      "learning_rate": 8.203703703703704e-06,
      "loss": 1.0733,
      "step": 1256
    },
    {
      "epoch": 2.61875,
      "grad_norm": 16.936193466186523,
      "learning_rate": 8.201388888888889e-06,
      "loss": 1.5876,
      "step": 1257
    },
    {
      "epoch": 2.6208333333333336,
      "grad_norm": 16.01904296875,
      "learning_rate": 8.199074074074074e-06,
      "loss": 2.0115,
      "step": 1258
    },
    {
      "epoch": 2.622916666666667,
      "grad_norm": 16.180017471313477,
      "learning_rate": 8.19675925925926e-06,
      "loss": 1.953,
      "step": 1259
    },
    {
      "epoch": 2.625,
      "grad_norm": 14.453916549682617,
      "learning_rate": 8.194444444444445e-06,
      "loss": 1.7525,
      "step": 1260
    },
    {
      "epoch": 2.627083333333333,
      "grad_norm": 14.210854530334473,
      "learning_rate": 8.19212962962963e-06,
      "loss": 1.4534,
      "step": 1261
    },
    {
      "epoch": 2.6291666666666664,
      "grad_norm": 14.280570983886719,
      "learning_rate": 8.189814814814817e-06,
      "loss": 1.9732,
      "step": 1262
    },
    {
      "epoch": 2.63125,
      "grad_norm": 11.861852645874023,
      "learning_rate": 8.1875e-06,
      "loss": 1.7826,
      "step": 1263
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 5.930116653442383,
      "learning_rate": 8.185185185185187e-06,
      "loss": 1.2215,
      "step": 1264
    },
    {
      "epoch": 2.6354166666666665,
      "grad_norm": 9.991823196411133,
      "learning_rate": 8.182870370370371e-06,
      "loss": 1.4934,
      "step": 1265
    },
    {
      "epoch": 2.6375,
      "grad_norm": 17.25125503540039,
      "learning_rate": 8.180555555555555e-06,
      "loss": 1.4634,
      "step": 1266
    },
    {
      "epoch": 2.6395833333333334,
      "grad_norm": 13.784278869628906,
      "learning_rate": 8.178240740740741e-06,
      "loss": 1.96,
      "step": 1267
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 10.662077903747559,
      "learning_rate": 8.175925925925925e-06,
      "loss": 0.7907,
      "step": 1268
    },
    {
      "epoch": 2.64375,
      "grad_norm": 20.0089168548584,
      "learning_rate": 8.173611111111111e-06,
      "loss": 1.7711,
      "step": 1269
    },
    {
      "epoch": 2.6458333333333335,
      "grad_norm": 8.348695755004883,
      "learning_rate": 8.171296296296297e-06,
      "loss": 0.819,
      "step": 1270
    },
    {
      "epoch": 2.6479166666666667,
      "grad_norm": 21.170555114746094,
      "learning_rate": 8.168981481481482e-06,
      "loss": 1.1414,
      "step": 1271
    },
    {
      "epoch": 2.65,
      "grad_norm": 9.503641128540039,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.4198,
      "step": 1272
    },
    {
      "epoch": 2.6520833333333336,
      "grad_norm": 13.870600700378418,
      "learning_rate": 8.164351851851853e-06,
      "loss": 1.6962,
      "step": 1273
    },
    {
      "epoch": 2.654166666666667,
      "grad_norm": 42.49225616455078,
      "learning_rate": 8.162037037037038e-06,
      "loss": 1.5133,
      "step": 1274
    },
    {
      "epoch": 2.65625,
      "grad_norm": 9.530725479125977,
      "learning_rate": 8.159722222222222e-06,
      "loss": 1.393,
      "step": 1275
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 9.991826057434082,
      "learning_rate": 8.157407407407408e-06,
      "loss": 0.427,
      "step": 1276
    },
    {
      "epoch": 2.6604166666666664,
      "grad_norm": 16.748546600341797,
      "learning_rate": 8.155092592592592e-06,
      "loss": 1.7667,
      "step": 1277
    },
    {
      "epoch": 2.6625,
      "grad_norm": 10.812652587890625,
      "learning_rate": 8.152777777777778e-06,
      "loss": 0.5297,
      "step": 1278
    },
    {
      "epoch": 2.6645833333333333,
      "grad_norm": 16.300491333007812,
      "learning_rate": 8.150462962962964e-06,
      "loss": 1.7027,
      "step": 1279
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 13.328343391418457,
      "learning_rate": 8.148148148148148e-06,
      "loss": 2.3326,
      "step": 1280
    },
    {
      "epoch": 2.66875,
      "grad_norm": 14.047361373901367,
      "learning_rate": 8.145833333333334e-06,
      "loss": 1.9963,
      "step": 1281
    },
    {
      "epoch": 2.6708333333333334,
      "grad_norm": 15.001729011535645,
      "learning_rate": 8.14351851851852e-06,
      "loss": 1.6797,
      "step": 1282
    },
    {
      "epoch": 2.6729166666666666,
      "grad_norm": 11.220824241638184,
      "learning_rate": 8.141203703703704e-06,
      "loss": 1.7803,
      "step": 1283
    },
    {
      "epoch": 2.675,
      "grad_norm": 7.474020481109619,
      "learning_rate": 8.138888888888889e-06,
      "loss": 1.4505,
      "step": 1284
    },
    {
      "epoch": 2.6770833333333335,
      "grad_norm": 18.482173919677734,
      "learning_rate": 8.136574074074075e-06,
      "loss": 2.5058,
      "step": 1285
    },
    {
      "epoch": 2.6791666666666667,
      "grad_norm": 18.212961196899414,
      "learning_rate": 8.134259259259259e-06,
      "loss": 2.5832,
      "step": 1286
    },
    {
      "epoch": 2.68125,
      "grad_norm": 16.013940811157227,
      "learning_rate": 8.131944444444445e-06,
      "loss": 1.827,
      "step": 1287
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 15.189193725585938,
      "learning_rate": 8.12962962962963e-06,
      "loss": 2.4948,
      "step": 1288
    },
    {
      "epoch": 2.685416666666667,
      "grad_norm": 9.53752326965332,
      "learning_rate": 8.127314814814815e-06,
      "loss": 1.0476,
      "step": 1289
    },
    {
      "epoch": 2.6875,
      "grad_norm": 10.317638397216797,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.4508,
      "step": 1290
    },
    {
      "epoch": 2.689583333333333,
      "grad_norm": 14.071438789367676,
      "learning_rate": 8.122685185185187e-06,
      "loss": 1.6742,
      "step": 1291
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 15.908955574035645,
      "learning_rate": 8.120370370370371e-06,
      "loss": 1.5452,
      "step": 1292
    },
    {
      "epoch": 2.69375,
      "grad_norm": 16.60049057006836,
      "learning_rate": 8.118055555555555e-06,
      "loss": 1.7983,
      "step": 1293
    },
    {
      "epoch": 2.6958333333333333,
      "grad_norm": 10.590672492980957,
      "learning_rate": 8.115740740740741e-06,
      "loss": 1.3854,
      "step": 1294
    },
    {
      "epoch": 2.6979166666666665,
      "grad_norm": 9.584092140197754,
      "learning_rate": 8.113425925925926e-06,
      "loss": 1.2136,
      "step": 1295
    },
    {
      "epoch": 2.7,
      "grad_norm": 15.220026016235352,
      "learning_rate": 8.111111111111112e-06,
      "loss": 1.9017,
      "step": 1296
    },
    {
      "epoch": 2.7020833333333334,
      "grad_norm": 52.292232513427734,
      "learning_rate": 8.108796296296298e-06,
      "loss": 2.2737,
      "step": 1297
    },
    {
      "epoch": 2.7041666666666666,
      "grad_norm": 32.47677993774414,
      "learning_rate": 8.106481481481482e-06,
      "loss": 1.9871,
      "step": 1298
    },
    {
      "epoch": 2.70625,
      "grad_norm": 9.650799751281738,
      "learning_rate": 8.104166666666668e-06,
      "loss": 1.1101,
      "step": 1299
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 20.946548461914062,
      "learning_rate": 8.101851851851854e-06,
      "loss": 2.7336,
      "step": 1300
    },
    {
      "epoch": 2.7104166666666667,
      "grad_norm": 15.74068546295166,
      "learning_rate": 8.099537037037038e-06,
      "loss": 1.7275,
      "step": 1301
    },
    {
      "epoch": 2.7125,
      "grad_norm": 19.723207473754883,
      "learning_rate": 8.097222222222222e-06,
      "loss": 1.9477,
      "step": 1302
    },
    {
      "epoch": 2.7145833333333336,
      "grad_norm": 8.177547454833984,
      "learning_rate": 8.094907407407408e-06,
      "loss": 0.7833,
      "step": 1303
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 17.921781539916992,
      "learning_rate": 8.092592592592592e-06,
      "loss": 1.8503,
      "step": 1304
    },
    {
      "epoch": 2.71875,
      "grad_norm": 15.416362762451172,
      "learning_rate": 8.090277777777778e-06,
      "loss": 1.6418,
      "step": 1305
    },
    {
      "epoch": 2.720833333333333,
      "grad_norm": 8.386033058166504,
      "learning_rate": 8.087962962962964e-06,
      "loss": 1.3721,
      "step": 1306
    },
    {
      "epoch": 2.7229166666666664,
      "grad_norm": 22.382312774658203,
      "learning_rate": 8.085648148148148e-06,
      "loss": 0.6417,
      "step": 1307
    },
    {
      "epoch": 2.725,
      "grad_norm": 17.69202995300293,
      "learning_rate": 8.083333333333334e-06,
      "loss": 1.8342,
      "step": 1308
    },
    {
      "epoch": 2.7270833333333333,
      "grad_norm": 8.00488567352295,
      "learning_rate": 8.08101851851852e-06,
      "loss": 0.7464,
      "step": 1309
    },
    {
      "epoch": 2.7291666666666665,
      "grad_norm": 14.410529136657715,
      "learning_rate": 8.078703703703705e-06,
      "loss": 1.4947,
      "step": 1310
    },
    {
      "epoch": 2.73125,
      "grad_norm": 14.377093315124512,
      "learning_rate": 8.076388888888889e-06,
      "loss": 2.1564,
      "step": 1311
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 10.557861328125,
      "learning_rate": 8.074074074074075e-06,
      "loss": 1.1472,
      "step": 1312
    },
    {
      "epoch": 2.7354166666666666,
      "grad_norm": 14.609647750854492,
      "learning_rate": 8.071759259259259e-06,
      "loss": 1.9625,
      "step": 1313
    },
    {
      "epoch": 2.7375,
      "grad_norm": 6.6745500564575195,
      "learning_rate": 8.069444444444445e-06,
      "loss": 1.9053,
      "step": 1314
    },
    {
      "epoch": 2.7395833333333335,
      "grad_norm": 6.697928428649902,
      "learning_rate": 8.06712962962963e-06,
      "loss": 1.7918,
      "step": 1315
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 16.10703468322754,
      "learning_rate": 8.064814814814815e-06,
      "loss": 2.1077,
      "step": 1316
    },
    {
      "epoch": 2.74375,
      "grad_norm": 10.841408729553223,
      "learning_rate": 8.062500000000001e-06,
      "loss": 1.3738,
      "step": 1317
    },
    {
      "epoch": 2.7458333333333336,
      "grad_norm": 5.701089382171631,
      "learning_rate": 8.060185185185185e-06,
      "loss": 1.7809,
      "step": 1318
    },
    {
      "epoch": 2.747916666666667,
      "grad_norm": 14.050878524780273,
      "learning_rate": 8.057870370370371e-06,
      "loss": 1.7184,
      "step": 1319
    },
    {
      "epoch": 2.75,
      "grad_norm": 9.736824035644531,
      "learning_rate": 8.055555555555557e-06,
      "loss": 1.07,
      "step": 1320
    },
    {
      "epoch": 2.752083333333333,
      "grad_norm": 16.533119201660156,
      "learning_rate": 8.053240740740742e-06,
      "loss": 1.6233,
      "step": 1321
    },
    {
      "epoch": 2.7541666666666664,
      "grad_norm": 6.943735122680664,
      "learning_rate": 8.050925925925926e-06,
      "loss": 1.0908,
      "step": 1322
    },
    {
      "epoch": 2.75625,
      "grad_norm": 7.932961463928223,
      "learning_rate": 8.048611111111112e-06,
      "loss": 1.6993,
      "step": 1323
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 6.589171409606934,
      "learning_rate": 8.046296296296296e-06,
      "loss": 1.1141,
      "step": 1324
    },
    {
      "epoch": 2.7604166666666665,
      "grad_norm": 10.83792495727539,
      "learning_rate": 8.043981481481482e-06,
      "loss": 2.0698,
      "step": 1325
    },
    {
      "epoch": 2.7625,
      "grad_norm": 15.07831859588623,
      "learning_rate": 8.041666666666668e-06,
      "loss": 1.5944,
      "step": 1326
    },
    {
      "epoch": 2.7645833333333334,
      "grad_norm": 7.762652397155762,
      "learning_rate": 8.039351851851852e-06,
      "loss": 1.0107,
      "step": 1327
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 16.3038272857666,
      "learning_rate": 8.037037037037038e-06,
      "loss": 2.2785,
      "step": 1328
    },
    {
      "epoch": 2.76875,
      "grad_norm": 17.916419982910156,
      "learning_rate": 8.034722222222224e-06,
      "loss": 1.7363,
      "step": 1329
    },
    {
      "epoch": 2.7708333333333335,
      "grad_norm": 16.480243682861328,
      "learning_rate": 8.032407407407408e-06,
      "loss": 1.7488,
      "step": 1330
    },
    {
      "epoch": 2.7729166666666667,
      "grad_norm": 22.76009750366211,
      "learning_rate": 8.030092592592593e-06,
      "loss": 0.9185,
      "step": 1331
    },
    {
      "epoch": 2.775,
      "grad_norm": 16.806411743164062,
      "learning_rate": 8.027777777777778e-06,
      "loss": 1.7726,
      "step": 1332
    },
    {
      "epoch": 2.7770833333333336,
      "grad_norm": 18.20438003540039,
      "learning_rate": 8.025462962962963e-06,
      "loss": 1.7624,
      "step": 1333
    },
    {
      "epoch": 2.779166666666667,
      "grad_norm": 22.18476104736328,
      "learning_rate": 8.023148148148149e-06,
      "loss": 1.873,
      "step": 1334
    },
    {
      "epoch": 2.78125,
      "grad_norm": 10.260299682617188,
      "learning_rate": 8.020833333333335e-06,
      "loss": 1.5344,
      "step": 1335
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 33.307899475097656,
      "learning_rate": 8.018518518518519e-06,
      "loss": 2.3481,
      "step": 1336
    },
    {
      "epoch": 2.7854166666666664,
      "grad_norm": 10.008955001831055,
      "learning_rate": 8.016203703703705e-06,
      "loss": 2.0357,
      "step": 1337
    },
    {
      "epoch": 2.7875,
      "grad_norm": 12.589130401611328,
      "learning_rate": 8.01388888888889e-06,
      "loss": 1.652,
      "step": 1338
    },
    {
      "epoch": 2.7895833333333333,
      "grad_norm": 40.896297454833984,
      "learning_rate": 8.011574074074075e-06,
      "loss": 1.5811,
      "step": 1339
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 13.891815185546875,
      "learning_rate": 8.00925925925926e-06,
      "loss": 1.2014,
      "step": 1340
    },
    {
      "epoch": 2.79375,
      "grad_norm": 7.68464994430542,
      "learning_rate": 8.006944444444445e-06,
      "loss": 1.3356,
      "step": 1341
    },
    {
      "epoch": 2.7958333333333334,
      "grad_norm": 13.35292911529541,
      "learning_rate": 8.00462962962963e-06,
      "loss": 1.8556,
      "step": 1342
    },
    {
      "epoch": 2.7979166666666666,
      "grad_norm": 9.12899112701416,
      "learning_rate": 8.002314814814815e-06,
      "loss": 0.756,
      "step": 1343
    },
    {
      "epoch": 2.8,
      "grad_norm": 14.694924354553223,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.43,
      "step": 1344
    },
    {
      "epoch": 2.8020833333333335,
      "grad_norm": 11.130921363830566,
      "learning_rate": 7.997685185185186e-06,
      "loss": 2.1129,
      "step": 1345
    },
    {
      "epoch": 2.8041666666666667,
      "grad_norm": 31.93353271484375,
      "learning_rate": 7.995370370370372e-06,
      "loss": 1.1128,
      "step": 1346
    },
    {
      "epoch": 2.80625,
      "grad_norm": 12.079771995544434,
      "learning_rate": 7.993055555555557e-06,
      "loss": 1.8084,
      "step": 1347
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 22.313766479492188,
      "learning_rate": 7.990740740740742e-06,
      "loss": 2.1924,
      "step": 1348
    },
    {
      "epoch": 2.810416666666667,
      "grad_norm": 10.00877571105957,
      "learning_rate": 7.988425925925926e-06,
      "loss": 1.86,
      "step": 1349
    },
    {
      "epoch": 2.8125,
      "grad_norm": 7.665529727935791,
      "learning_rate": 7.986111111111112e-06,
      "loss": 1.6143,
      "step": 1350
    },
    {
      "epoch": 2.814583333333333,
      "grad_norm": 10.358628273010254,
      "learning_rate": 7.983796296296296e-06,
      "loss": 1.6772,
      "step": 1351
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 14.740657806396484,
      "learning_rate": 7.981481481481482e-06,
      "loss": 2.4825,
      "step": 1352
    },
    {
      "epoch": 2.81875,
      "grad_norm": 10.783858299255371,
      "learning_rate": 7.979166666666668e-06,
      "loss": 1.7198,
      "step": 1353
    },
    {
      "epoch": 2.8208333333333333,
      "grad_norm": 12.506735801696777,
      "learning_rate": 7.976851851851852e-06,
      "loss": 2.0208,
      "step": 1354
    },
    {
      "epoch": 2.8229166666666665,
      "grad_norm": 11.936141967773438,
      "learning_rate": 7.974537037037038e-06,
      "loss": 2.044,
      "step": 1355
    },
    {
      "epoch": 2.825,
      "grad_norm": 9.015995025634766,
      "learning_rate": 7.972222222222224e-06,
      "loss": 0.823,
      "step": 1356
    },
    {
      "epoch": 2.8270833333333334,
      "grad_norm": 8.479978561401367,
      "learning_rate": 7.969907407407408e-06,
      "loss": 1.386,
      "step": 1357
    },
    {
      "epoch": 2.8291666666666666,
      "grad_norm": 13.587721824645996,
      "learning_rate": 7.967592592592593e-06,
      "loss": 2.1958,
      "step": 1358
    },
    {
      "epoch": 2.83125,
      "grad_norm": 11.902953147888184,
      "learning_rate": 7.965277777777779e-06,
      "loss": 0.6147,
      "step": 1359
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 8.818395614624023,
      "learning_rate": 7.962962962962963e-06,
      "loss": 1.3598,
      "step": 1360
    },
    {
      "epoch": 2.8354166666666667,
      "grad_norm": 12.245823860168457,
      "learning_rate": 7.960648148148149e-06,
      "loss": 1.2588,
      "step": 1361
    },
    {
      "epoch": 2.8375,
      "grad_norm": 14.086878776550293,
      "learning_rate": 7.958333333333333e-06,
      "loss": 0.6003,
      "step": 1362
    },
    {
      "epoch": 2.8395833333333336,
      "grad_norm": 13.801023483276367,
      "learning_rate": 7.956018518518519e-06,
      "loss": 1.1103,
      "step": 1363
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 17.10738182067871,
      "learning_rate": 7.953703703703705e-06,
      "loss": 2.0224,
      "step": 1364
    },
    {
      "epoch": 2.84375,
      "grad_norm": 44.51094055175781,
      "learning_rate": 7.95138888888889e-06,
      "loss": 1.3606,
      "step": 1365
    },
    {
      "epoch": 2.845833333333333,
      "grad_norm": 9.559381484985352,
      "learning_rate": 7.949074074074075e-06,
      "loss": 1.1269,
      "step": 1366
    },
    {
      "epoch": 2.8479166666666664,
      "grad_norm": 8.32861328125,
      "learning_rate": 7.94675925925926e-06,
      "loss": 2.3821,
      "step": 1367
    },
    {
      "epoch": 2.85,
      "grad_norm": 8.004071235656738,
      "learning_rate": 7.944444444444445e-06,
      "loss": 0.7425,
      "step": 1368
    },
    {
      "epoch": 2.8520833333333333,
      "grad_norm": 7.87039041519165,
      "learning_rate": 7.94212962962963e-06,
      "loss": 1.3415,
      "step": 1369
    },
    {
      "epoch": 2.8541666666666665,
      "grad_norm": 14.247236251831055,
      "learning_rate": 7.939814814814816e-06,
      "loss": 1.7466,
      "step": 1370
    },
    {
      "epoch": 2.85625,
      "grad_norm": 11.121127128601074,
      "learning_rate": 7.9375e-06,
      "loss": 0.4656,
      "step": 1371
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 8.561418533325195,
      "learning_rate": 7.935185185185186e-06,
      "loss": 0.6995,
      "step": 1372
    },
    {
      "epoch": 2.8604166666666666,
      "grad_norm": 6.960025787353516,
      "learning_rate": 7.932870370370372e-06,
      "loss": 1.2206,
      "step": 1373
    },
    {
      "epoch": 2.8625,
      "grad_norm": 10.601712226867676,
      "learning_rate": 7.930555555555556e-06,
      "loss": 1.4096,
      "step": 1374
    },
    {
      "epoch": 2.8645833333333335,
      "grad_norm": 11.01677131652832,
      "learning_rate": 7.928240740740742e-06,
      "loss": 1.9229,
      "step": 1375
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 8.7813138961792,
      "learning_rate": 7.925925925925926e-06,
      "loss": 1.0743,
      "step": 1376
    },
    {
      "epoch": 2.86875,
      "grad_norm": 15.223567008972168,
      "learning_rate": 7.923611111111112e-06,
      "loss": 1.6061,
      "step": 1377
    },
    {
      "epoch": 2.8708333333333336,
      "grad_norm": 13.153427124023438,
      "learning_rate": 7.921296296296296e-06,
      "loss": 1.7622,
      "step": 1378
    },
    {
      "epoch": 2.872916666666667,
      "grad_norm": 16.204524993896484,
      "learning_rate": 7.918981481481482e-06,
      "loss": 1.8266,
      "step": 1379
    },
    {
      "epoch": 2.875,
      "grad_norm": 7.440621852874756,
      "learning_rate": 7.916666666666667e-06,
      "loss": 1.1455,
      "step": 1380
    },
    {
      "epoch": 2.877083333333333,
      "grad_norm": 25.51666831970215,
      "learning_rate": 7.914351851851852e-06,
      "loss": 1.9985,
      "step": 1381
    },
    {
      "epoch": 2.8791666666666664,
      "grad_norm": 16.36136245727539,
      "learning_rate": 7.912037037037038e-06,
      "loss": 2.0747,
      "step": 1382
    },
    {
      "epoch": 2.88125,
      "grad_norm": 31.506465911865234,
      "learning_rate": 7.909722222222223e-06,
      "loss": 1.8964,
      "step": 1383
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 9.58450698852539,
      "learning_rate": 7.907407407407409e-06,
      "loss": 1.6167,
      "step": 1384
    },
    {
      "epoch": 2.8854166666666665,
      "grad_norm": 14.15600299835205,
      "learning_rate": 7.905092592592593e-06,
      "loss": 1.9176,
      "step": 1385
    },
    {
      "epoch": 2.8875,
      "grad_norm": 7.005581855773926,
      "learning_rate": 7.902777777777779e-06,
      "loss": 0.9644,
      "step": 1386
    },
    {
      "epoch": 2.8895833333333334,
      "grad_norm": 7.596829891204834,
      "learning_rate": 7.900462962962963e-06,
      "loss": 0.9953,
      "step": 1387
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 12.856324195861816,
      "learning_rate": 7.898148148148149e-06,
      "loss": 1.8682,
      "step": 1388
    },
    {
      "epoch": 2.89375,
      "grad_norm": 9.332005500793457,
      "learning_rate": 7.895833333333333e-06,
      "loss": 0.3807,
      "step": 1389
    },
    {
      "epoch": 2.8958333333333335,
      "grad_norm": 11.335540771484375,
      "learning_rate": 7.89351851851852e-06,
      "loss": 1.113,
      "step": 1390
    },
    {
      "epoch": 2.8979166666666667,
      "grad_norm": 6.862778663635254,
      "learning_rate": 7.891203703703705e-06,
      "loss": 0.954,
      "step": 1391
    },
    {
      "epoch": 2.9,
      "grad_norm": 8.252827644348145,
      "learning_rate": 7.88888888888889e-06,
      "loss": 1.0201,
      "step": 1392
    },
    {
      "epoch": 2.9020833333333336,
      "grad_norm": 15.432798385620117,
      "learning_rate": 7.886574074074075e-06,
      "loss": 1.8076,
      "step": 1393
    },
    {
      "epoch": 2.904166666666667,
      "grad_norm": 13.086578369140625,
      "learning_rate": 7.88425925925926e-06,
      "loss": 1.6483,
      "step": 1394
    },
    {
      "epoch": 2.90625,
      "grad_norm": 8.492554664611816,
      "learning_rate": 7.881944444444446e-06,
      "loss": 1.2465,
      "step": 1395
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 8.558693885803223,
      "learning_rate": 7.87962962962963e-06,
      "loss": 1.8196,
      "step": 1396
    },
    {
      "epoch": 2.9104166666666664,
      "grad_norm": 9.07071304321289,
      "learning_rate": 7.877314814814816e-06,
      "loss": 1.5408,
      "step": 1397
    },
    {
      "epoch": 2.9125,
      "grad_norm": 9.555488586425781,
      "learning_rate": 7.875e-06,
      "loss": 1.3449,
      "step": 1398
    },
    {
      "epoch": 2.9145833333333333,
      "grad_norm": 9.199796676635742,
      "learning_rate": 7.872685185185186e-06,
      "loss": 2.0298,
      "step": 1399
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 10.359968185424805,
      "learning_rate": 7.870370370370372e-06,
      "loss": 1.8098,
      "step": 1400
    },
    {
      "epoch": 2.91875,
      "grad_norm": 16.051565170288086,
      "learning_rate": 7.868055555555556e-06,
      "loss": 0.7213,
      "step": 1401
    },
    {
      "epoch": 2.9208333333333334,
      "grad_norm": 9.30400276184082,
      "learning_rate": 7.865740740740742e-06,
      "loss": 1.6954,
      "step": 1402
    },
    {
      "epoch": 2.9229166666666666,
      "grad_norm": 10.813753128051758,
      "learning_rate": 7.863425925925926e-06,
      "loss": 1.7304,
      "step": 1403
    },
    {
      "epoch": 2.925,
      "grad_norm": 17.837011337280273,
      "learning_rate": 7.861111111111112e-06,
      "loss": 1.5934,
      "step": 1404
    },
    {
      "epoch": 2.9270833333333335,
      "grad_norm": 11.686570167541504,
      "learning_rate": 7.858796296296297e-06,
      "loss": 1.6849,
      "step": 1405
    },
    {
      "epoch": 2.9291666666666667,
      "grad_norm": 9.426356315612793,
      "learning_rate": 7.85648148148148e-06,
      "loss": 2.1727,
      "step": 1406
    },
    {
      "epoch": 2.93125,
      "grad_norm": 17.528644561767578,
      "learning_rate": 7.854166666666667e-06,
      "loss": 1.7253,
      "step": 1407
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 9.579368591308594,
      "learning_rate": 7.851851851851853e-06,
      "loss": 0.4096,
      "step": 1408
    },
    {
      "epoch": 2.935416666666667,
      "grad_norm": 15.592487335205078,
      "learning_rate": 7.849537037037037e-06,
      "loss": 2.0009,
      "step": 1409
    },
    {
      "epoch": 2.9375,
      "grad_norm": 7.652840614318848,
      "learning_rate": 7.847222222222223e-06,
      "loss": 1.686,
      "step": 1410
    },
    {
      "epoch": 2.939583333333333,
      "grad_norm": 9.701273918151855,
      "learning_rate": 7.844907407407409e-06,
      "loss": 2.2363,
      "step": 1411
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 10.064667701721191,
      "learning_rate": 7.842592592592593e-06,
      "loss": 1.8433,
      "step": 1412
    },
    {
      "epoch": 2.94375,
      "grad_norm": 9.984945297241211,
      "learning_rate": 7.840277777777779e-06,
      "loss": 0.8677,
      "step": 1413
    },
    {
      "epoch": 2.9458333333333333,
      "grad_norm": 9.162389755249023,
      "learning_rate": 7.837962962962963e-06,
      "loss": 1.8986,
      "step": 1414
    },
    {
      "epoch": 2.9479166666666665,
      "grad_norm": 16.488862991333008,
      "learning_rate": 7.835648148148147e-06,
      "loss": 1.41,
      "step": 1415
    },
    {
      "epoch": 2.95,
      "grad_norm": 11.00195598602295,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.5994,
      "step": 1416
    },
    {
      "epoch": 2.9520833333333334,
      "grad_norm": 17.687562942504883,
      "learning_rate": 7.83101851851852e-06,
      "loss": 0.8943,
      "step": 1417
    },
    {
      "epoch": 2.9541666666666666,
      "grad_norm": 21.51258087158203,
      "learning_rate": 7.828703703703704e-06,
      "loss": 1.6523,
      "step": 1418
    },
    {
      "epoch": 2.95625,
      "grad_norm": 6.4251179695129395,
      "learning_rate": 7.82638888888889e-06,
      "loss": 1.5562,
      "step": 1419
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 10.956955909729004,
      "learning_rate": 7.824074074074076e-06,
      "loss": 1.6222,
      "step": 1420
    },
    {
      "epoch": 2.9604166666666667,
      "grad_norm": 9.906082153320312,
      "learning_rate": 7.82175925925926e-06,
      "loss": 1.9112,
      "step": 1421
    },
    {
      "epoch": 2.9625,
      "grad_norm": 49.47096252441406,
      "learning_rate": 7.819444444444446e-06,
      "loss": 1.2056,
      "step": 1422
    },
    {
      "epoch": 2.9645833333333336,
      "grad_norm": 13.442893981933594,
      "learning_rate": 7.81712962962963e-06,
      "loss": 1.0698,
      "step": 1423
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 11.051499366760254,
      "learning_rate": 7.814814814814816e-06,
      "loss": 1.9418,
      "step": 1424
    },
    {
      "epoch": 2.96875,
      "grad_norm": 9.570570945739746,
      "learning_rate": 7.8125e-06,
      "loss": 1.6449,
      "step": 1425
    },
    {
      "epoch": 2.970833333333333,
      "grad_norm": 12.110183715820312,
      "learning_rate": 7.810185185185186e-06,
      "loss": 1.8501,
      "step": 1426
    },
    {
      "epoch": 2.9729166666666664,
      "grad_norm": 9.276840209960938,
      "learning_rate": 7.80787037037037e-06,
      "loss": 0.4009,
      "step": 1427
    },
    {
      "epoch": 2.975,
      "grad_norm": 22.013601303100586,
      "learning_rate": 7.805555555555556e-06,
      "loss": 1.6837,
      "step": 1428
    },
    {
      "epoch": 2.9770833333333333,
      "grad_norm": 12.308402061462402,
      "learning_rate": 7.803240740740742e-06,
      "loss": 1.4001,
      "step": 1429
    },
    {
      "epoch": 2.9791666666666665,
      "grad_norm": 11.008816719055176,
      "learning_rate": 7.800925925925926e-06,
      "loss": 1.5668,
      "step": 1430
    },
    {
      "epoch": 2.98125,
      "grad_norm": 14.664021492004395,
      "learning_rate": 7.798611111111112e-06,
      "loss": 1.5282,
      "step": 1431
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 21.679033279418945,
      "learning_rate": 7.796296296296297e-06,
      "loss": 1.4816,
      "step": 1432
    },
    {
      "epoch": 2.9854166666666666,
      "grad_norm": 18.341217041015625,
      "learning_rate": 7.793981481481483e-06,
      "loss": 2.3104,
      "step": 1433
    },
    {
      "epoch": 2.9875,
      "grad_norm": 35.42647933959961,
      "learning_rate": 7.791666666666667e-06,
      "loss": 1.4304,
      "step": 1434
    },
    {
      "epoch": 2.9895833333333335,
      "grad_norm": 12.812570571899414,
      "learning_rate": 7.789351851851853e-06,
      "loss": 1.4669,
      "step": 1435
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 9.573634147644043,
      "learning_rate": 7.787037037037037e-06,
      "loss": 1.6682,
      "step": 1436
    },
    {
      "epoch": 2.99375,
      "grad_norm": 12.424708366394043,
      "learning_rate": 7.784722222222223e-06,
      "loss": 1.7329,
      "step": 1437
    },
    {
      "epoch": 2.9958333333333336,
      "grad_norm": 18.9138240814209,
      "learning_rate": 7.782407407407409e-06,
      "loss": 2.5256,
      "step": 1438
    },
    {
      "epoch": 2.997916666666667,
      "grad_norm": 11.2137451171875,
      "learning_rate": 7.780092592592593e-06,
      "loss": 1.7375,
      "step": 1439
    },
    {
      "epoch": 3.0,
      "grad_norm": 14.151691436767578,
      "learning_rate": 7.77777777777778e-06,
      "loss": 1.8465,
      "step": 1440
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.38333333333333336,
      "eval_f1": 0.23544787499006217,
      "eval_loss": 1.543938159942627,
      "eval_runtime": 34.3857,
      "eval_samples_per_second": 5.235,
      "eval_steps_per_second": 2.617,
      "step": 1440
    },
    {
      "epoch": 3.002083333333333,
      "grad_norm": 11.11097240447998,
      "learning_rate": 7.775462962962963e-06,
      "loss": 1.4113,
      "step": 1441
    },
    {
      "epoch": 3.004166666666667,
      "grad_norm": 9.83458137512207,
      "learning_rate": 7.77314814814815e-06,
      "loss": 1.3048,
      "step": 1442
    },
    {
      "epoch": 3.00625,
      "grad_norm": 8.958370208740234,
      "learning_rate": 7.770833333333334e-06,
      "loss": 1.2965,
      "step": 1443
    },
    {
      "epoch": 3.0083333333333333,
      "grad_norm": 6.1371283531188965,
      "learning_rate": 7.76851851851852e-06,
      "loss": 1.8469,
      "step": 1444
    },
    {
      "epoch": 3.0104166666666665,
      "grad_norm": 8.522350311279297,
      "learning_rate": 7.766203703703704e-06,
      "loss": 1.3947,
      "step": 1445
    },
    {
      "epoch": 3.0125,
      "grad_norm": 9.662973403930664,
      "learning_rate": 7.76388888888889e-06,
      "loss": 1.6165,
      "step": 1446
    },
    {
      "epoch": 3.0145833333333334,
      "grad_norm": 5.811525821685791,
      "learning_rate": 7.761574074074076e-06,
      "loss": 1.8166,
      "step": 1447
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 8.687488555908203,
      "learning_rate": 7.75925925925926e-06,
      "loss": 1.6287,
      "step": 1448
    },
    {
      "epoch": 3.01875,
      "grad_norm": 6.594512939453125,
      "learning_rate": 7.756944444444446e-06,
      "loss": 1.0921,
      "step": 1449
    },
    {
      "epoch": 3.0208333333333335,
      "grad_norm": 15.939640045166016,
      "learning_rate": 7.75462962962963e-06,
      "loss": 1.9136,
      "step": 1450
    },
    {
      "epoch": 3.0229166666666667,
      "grad_norm": 8.201277732849121,
      "learning_rate": 7.752314814814816e-06,
      "loss": 1.7968,
      "step": 1451
    },
    {
      "epoch": 3.025,
      "grad_norm": 19.03200340270996,
      "learning_rate": 7.75e-06,
      "loss": 1.2791,
      "step": 1452
    },
    {
      "epoch": 3.027083333333333,
      "grad_norm": 8.861605644226074,
      "learning_rate": 7.747685185185185e-06,
      "loss": 0.9699,
      "step": 1453
    },
    {
      "epoch": 3.029166666666667,
      "grad_norm": 8.831127166748047,
      "learning_rate": 7.74537037037037e-06,
      "loss": 1.077,
      "step": 1454
    },
    {
      "epoch": 3.03125,
      "grad_norm": 7.257228374481201,
      "learning_rate": 7.743055555555556e-06,
      "loss": 1.4005,
      "step": 1455
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 16.8408260345459,
      "learning_rate": 7.74074074074074e-06,
      "loss": 2.3409,
      "step": 1456
    },
    {
      "epoch": 3.035416666666667,
      "grad_norm": 21.297779083251953,
      "learning_rate": 7.738425925925927e-06,
      "loss": 1.7474,
      "step": 1457
    },
    {
      "epoch": 3.0375,
      "grad_norm": 14.084864616394043,
      "learning_rate": 7.736111111111113e-06,
      "loss": 1.4998,
      "step": 1458
    },
    {
      "epoch": 3.0395833333333333,
      "grad_norm": 6.99440860748291,
      "learning_rate": 7.733796296296297e-06,
      "loss": 0.9795,
      "step": 1459
    },
    {
      "epoch": 3.0416666666666665,
      "grad_norm": 15.19703483581543,
      "learning_rate": 7.731481481481483e-06,
      "loss": 1.3488,
      "step": 1460
    },
    {
      "epoch": 3.04375,
      "grad_norm": 11.183158874511719,
      "learning_rate": 7.729166666666667e-06,
      "loss": 1.623,
      "step": 1461
    },
    {
      "epoch": 3.0458333333333334,
      "grad_norm": 8.241318702697754,
      "learning_rate": 7.726851851851851e-06,
      "loss": 1.5185,
      "step": 1462
    },
    {
      "epoch": 3.0479166666666666,
      "grad_norm": 25.15695571899414,
      "learning_rate": 7.724537037037037e-06,
      "loss": 2.0582,
      "step": 1463
    },
    {
      "epoch": 3.05,
      "grad_norm": 8.546344757080078,
      "learning_rate": 7.722222222222223e-06,
      "loss": 1.687,
      "step": 1464
    },
    {
      "epoch": 3.0520833333333335,
      "grad_norm": 11.50838851928711,
      "learning_rate": 7.719907407407407e-06,
      "loss": 1.661,
      "step": 1465
    },
    {
      "epoch": 3.0541666666666667,
      "grad_norm": 36.91523361206055,
      "learning_rate": 7.717592592592593e-06,
      "loss": 2.006,
      "step": 1466
    },
    {
      "epoch": 3.05625,
      "grad_norm": 11.011968612670898,
      "learning_rate": 7.71527777777778e-06,
      "loss": 1.1832,
      "step": 1467
    },
    {
      "epoch": 3.058333333333333,
      "grad_norm": 10.50777816772461,
      "learning_rate": 7.712962962962964e-06,
      "loss": 1.7364,
      "step": 1468
    },
    {
      "epoch": 3.060416666666667,
      "grad_norm": 11.35183048248291,
      "learning_rate": 7.71064814814815e-06,
      "loss": 1.8552,
      "step": 1469
    },
    {
      "epoch": 3.0625,
      "grad_norm": 12.887102127075195,
      "learning_rate": 7.708333333333334e-06,
      "loss": 1.5582,
      "step": 1470
    },
    {
      "epoch": 3.064583333333333,
      "grad_norm": 14.966692924499512,
      "learning_rate": 7.706018518518518e-06,
      "loss": 1.5914,
      "step": 1471
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 79.82857513427734,
      "learning_rate": 7.703703703703704e-06,
      "loss": 1.6821,
      "step": 1472
    },
    {
      "epoch": 3.06875,
      "grad_norm": 13.252111434936523,
      "learning_rate": 7.70138888888889e-06,
      "loss": 1.8263,
      "step": 1473
    },
    {
      "epoch": 3.0708333333333333,
      "grad_norm": 9.075307846069336,
      "learning_rate": 7.699074074074074e-06,
      "loss": 1.3194,
      "step": 1474
    },
    {
      "epoch": 3.0729166666666665,
      "grad_norm": 10.034220695495605,
      "learning_rate": 7.69675925925926e-06,
      "loss": 1.1077,
      "step": 1475
    },
    {
      "epoch": 3.075,
      "grad_norm": 32.250526428222656,
      "learning_rate": 7.694444444444446e-06,
      "loss": 2.7025,
      "step": 1476
    },
    {
      "epoch": 3.0770833333333334,
      "grad_norm": 14.153480529785156,
      "learning_rate": 7.69212962962963e-06,
      "loss": 1.0386,
      "step": 1477
    },
    {
      "epoch": 3.0791666666666666,
      "grad_norm": 12.391898155212402,
      "learning_rate": 7.689814814814816e-06,
      "loss": 2.0674,
      "step": 1478
    },
    {
      "epoch": 3.08125,
      "grad_norm": 20.039716720581055,
      "learning_rate": 7.6875e-06,
      "loss": 1.8524,
      "step": 1479
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 9.79759407043457,
      "learning_rate": 7.685185185185185e-06,
      "loss": 1.6527,
      "step": 1480
    },
    {
      "epoch": 3.0854166666666667,
      "grad_norm": 15.71152114868164,
      "learning_rate": 7.68287037037037e-06,
      "loss": 1.7267,
      "step": 1481
    },
    {
      "epoch": 3.0875,
      "grad_norm": 16.563426971435547,
      "learning_rate": 7.680555555555557e-06,
      "loss": 1.6574,
      "step": 1482
    },
    {
      "epoch": 3.089583333333333,
      "grad_norm": 8.087890625,
      "learning_rate": 7.678240740740741e-06,
      "loss": 1.2386,
      "step": 1483
    },
    {
      "epoch": 3.091666666666667,
      "grad_norm": 10.413450241088867,
      "learning_rate": 7.675925925925927e-06,
      "loss": 2.3483,
      "step": 1484
    },
    {
      "epoch": 3.09375,
      "grad_norm": 9.0812406539917,
      "learning_rate": 7.673611111111113e-06,
      "loss": 1.192,
      "step": 1485
    },
    {
      "epoch": 3.095833333333333,
      "grad_norm": 20.153289794921875,
      "learning_rate": 7.671296296296297e-06,
      "loss": 1.3245,
      "step": 1486
    },
    {
      "epoch": 3.097916666666667,
      "grad_norm": 9.103425025939941,
      "learning_rate": 7.668981481481483e-06,
      "loss": 1.3687,
      "step": 1487
    },
    {
      "epoch": 3.1,
      "grad_norm": 9.633724212646484,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.6807,
      "step": 1488
    },
    {
      "epoch": 3.1020833333333333,
      "grad_norm": 24.803842544555664,
      "learning_rate": 7.664351851851851e-06,
      "loss": 1.7801,
      "step": 1489
    },
    {
      "epoch": 3.1041666666666665,
      "grad_norm": 11.576142311096191,
      "learning_rate": 7.662037037037037e-06,
      "loss": 1.8441,
      "step": 1490
    },
    {
      "epoch": 3.10625,
      "grad_norm": 16.726364135742188,
      "learning_rate": 7.659722222222223e-06,
      "loss": 1.2571,
      "step": 1491
    },
    {
      "epoch": 3.1083333333333334,
      "grad_norm": 19.424882888793945,
      "learning_rate": 7.657407407407408e-06,
      "loss": 1.6815,
      "step": 1492
    },
    {
      "epoch": 3.1104166666666666,
      "grad_norm": 8.6419095993042,
      "learning_rate": 7.655092592592594e-06,
      "loss": 1.1839,
      "step": 1493
    },
    {
      "epoch": 3.1125,
      "grad_norm": 14.59862995147705,
      "learning_rate": 7.652777777777778e-06,
      "loss": 1.4136,
      "step": 1494
    },
    {
      "epoch": 3.1145833333333335,
      "grad_norm": 8.831308364868164,
      "learning_rate": 7.650462962962964e-06,
      "loss": 0.705,
      "step": 1495
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 32.279727935791016,
      "learning_rate": 7.64814814814815e-06,
      "loss": 1.711,
      "step": 1496
    },
    {
      "epoch": 3.11875,
      "grad_norm": 11.966588020324707,
      "learning_rate": 7.645833333333334e-06,
      "loss": 1.4876,
      "step": 1497
    },
    {
      "epoch": 3.120833333333333,
      "grad_norm": 14.670621871948242,
      "learning_rate": 7.643518518518518e-06,
      "loss": 1.2298,
      "step": 1498
    },
    {
      "epoch": 3.122916666666667,
      "grad_norm": 8.149364471435547,
      "learning_rate": 7.641203703703704e-06,
      "loss": 0.6995,
      "step": 1499
    },
    {
      "epoch": 3.125,
      "grad_norm": 14.684659957885742,
      "learning_rate": 7.638888888888888e-06,
      "loss": 1.7469,
      "step": 1500
    },
    {
      "epoch": 3.127083333333333,
      "grad_norm": 11.777621269226074,
      "learning_rate": 7.636574074074074e-06,
      "loss": 1.7557,
      "step": 1501
    },
    {
      "epoch": 3.129166666666667,
      "grad_norm": 6.113039016723633,
      "learning_rate": 7.63425925925926e-06,
      "loss": 1.7028,
      "step": 1502
    },
    {
      "epoch": 3.13125,
      "grad_norm": 27.4244327545166,
      "learning_rate": 7.631944444444445e-06,
      "loss": 1.7541,
      "step": 1503
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 10.564238548278809,
      "learning_rate": 7.62962962962963e-06,
      "loss": 1.1468,
      "step": 1504
    },
    {
      "epoch": 3.1354166666666665,
      "grad_norm": 9.673091888427734,
      "learning_rate": 7.627314814814816e-06,
      "loss": 0.9652,
      "step": 1505
    },
    {
      "epoch": 3.1375,
      "grad_norm": 5.0172319412231445,
      "learning_rate": 7.625e-06,
      "loss": 1.0587,
      "step": 1506
    },
    {
      "epoch": 3.1395833333333334,
      "grad_norm": 10.034624099731445,
      "learning_rate": 7.622685185185186e-06,
      "loss": 1.1035,
      "step": 1507
    },
    {
      "epoch": 3.1416666666666666,
      "grad_norm": 21.209163665771484,
      "learning_rate": 7.620370370370372e-06,
      "loss": 1.0357,
      "step": 1508
    },
    {
      "epoch": 3.14375,
      "grad_norm": 14.757760047912598,
      "learning_rate": 7.618055555555556e-06,
      "loss": 2.0438,
      "step": 1509
    },
    {
      "epoch": 3.1458333333333335,
      "grad_norm": 13.722759246826172,
      "learning_rate": 7.615740740740741e-06,
      "loss": 1.8323,
      "step": 1510
    },
    {
      "epoch": 3.1479166666666667,
      "grad_norm": 11.490903854370117,
      "learning_rate": 7.613425925925927e-06,
      "loss": 1.4552,
      "step": 1511
    },
    {
      "epoch": 3.15,
      "grad_norm": 10.824515342712402,
      "learning_rate": 7.611111111111111e-06,
      "loss": 1.0406,
      "step": 1512
    },
    {
      "epoch": 3.152083333333333,
      "grad_norm": 9.279813766479492,
      "learning_rate": 7.608796296296297e-06,
      "loss": 1.4316,
      "step": 1513
    },
    {
      "epoch": 3.154166666666667,
      "grad_norm": 10.1582612991333,
      "learning_rate": 7.606481481481482e-06,
      "loss": 2.0681,
      "step": 1514
    },
    {
      "epoch": 3.15625,
      "grad_norm": 20.695091247558594,
      "learning_rate": 7.6041666666666666e-06,
      "loss": 2.1656,
      "step": 1515
    },
    {
      "epoch": 3.158333333333333,
      "grad_norm": 5.8067779541015625,
      "learning_rate": 7.6018518518518525e-06,
      "loss": 1.1605,
      "step": 1516
    },
    {
      "epoch": 3.160416666666667,
      "grad_norm": 17.93328285217285,
      "learning_rate": 7.5995370370370385e-06,
      "loss": 1.923,
      "step": 1517
    },
    {
      "epoch": 3.1625,
      "grad_norm": 7.587504863739014,
      "learning_rate": 7.597222222222223e-06,
      "loss": 0.9394,
      "step": 1518
    },
    {
      "epoch": 3.1645833333333333,
      "grad_norm": 9.542258262634277,
      "learning_rate": 7.594907407407408e-06,
      "loss": 0.4448,
      "step": 1519
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 15.414895057678223,
      "learning_rate": 7.592592592592594e-06,
      "loss": 1.7441,
      "step": 1520
    },
    {
      "epoch": 3.16875,
      "grad_norm": 6.293943881988525,
      "learning_rate": 7.590277777777778e-06,
      "loss": 1.1588,
      "step": 1521
    },
    {
      "epoch": 3.1708333333333334,
      "grad_norm": 14.181828498840332,
      "learning_rate": 7.587962962962964e-06,
      "loss": 1.6013,
      "step": 1522
    },
    {
      "epoch": 3.1729166666666666,
      "grad_norm": 14.997909545898438,
      "learning_rate": 7.585648148148149e-06,
      "loss": 1.4998,
      "step": 1523
    },
    {
      "epoch": 3.175,
      "grad_norm": 15.732763290405273,
      "learning_rate": 7.583333333333333e-06,
      "loss": 1.6192,
      "step": 1524
    },
    {
      "epoch": 3.1770833333333335,
      "grad_norm": 10.513632774353027,
      "learning_rate": 7.581018518518519e-06,
      "loss": 1.6338,
      "step": 1525
    },
    {
      "epoch": 3.1791666666666667,
      "grad_norm": 14.355449676513672,
      "learning_rate": 7.578703703703705e-06,
      "loss": 1.846,
      "step": 1526
    },
    {
      "epoch": 3.18125,
      "grad_norm": 24.00659942626953,
      "learning_rate": 7.5763888888888894e-06,
      "loss": 1.7366,
      "step": 1527
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 19.750446319580078,
      "learning_rate": 7.5740740740740745e-06,
      "loss": 1.41,
      "step": 1528
    },
    {
      "epoch": 3.185416666666667,
      "grad_norm": 11.673609733581543,
      "learning_rate": 7.5717592592592605e-06,
      "loss": 1.6694,
      "step": 1529
    },
    {
      "epoch": 3.1875,
      "grad_norm": 57.473880767822266,
      "learning_rate": 7.569444444444445e-06,
      "loss": 1.1219,
      "step": 1530
    },
    {
      "epoch": 3.189583333333333,
      "grad_norm": 9.481151580810547,
      "learning_rate": 7.567129629629631e-06,
      "loss": 0.3824,
      "step": 1531
    },
    {
      "epoch": 3.191666666666667,
      "grad_norm": 14.210219383239746,
      "learning_rate": 7.564814814814816e-06,
      "loss": 1.5263,
      "step": 1532
    },
    {
      "epoch": 3.19375,
      "grad_norm": 8.335572242736816,
      "learning_rate": 7.5625e-06,
      "loss": 0.6486,
      "step": 1533
    },
    {
      "epoch": 3.1958333333333333,
      "grad_norm": 14.973611831665039,
      "learning_rate": 7.560185185185186e-06,
      "loss": 1.7538,
      "step": 1534
    },
    {
      "epoch": 3.1979166666666665,
      "grad_norm": 13.874602317810059,
      "learning_rate": 7.557870370370372e-06,
      "loss": 1.9991,
      "step": 1535
    },
    {
      "epoch": 3.2,
      "grad_norm": 14.684089660644531,
      "learning_rate": 7.555555555555556e-06,
      "loss": 1.9826,
      "step": 1536
    },
    {
      "epoch": 3.2020833333333334,
      "grad_norm": 9.320372581481934,
      "learning_rate": 7.553240740740741e-06,
      "loss": 1.6393,
      "step": 1537
    },
    {
      "epoch": 3.2041666666666666,
      "grad_norm": 9.298287391662598,
      "learning_rate": 7.550925925925926e-06,
      "loss": 1.6419,
      "step": 1538
    },
    {
      "epoch": 3.20625,
      "grad_norm": 16.442546844482422,
      "learning_rate": 7.5486111111111114e-06,
      "loss": 1.9113,
      "step": 1539
    },
    {
      "epoch": 3.2083333333333335,
      "grad_norm": 8.085906982421875,
      "learning_rate": 7.546296296296297e-06,
      "loss": 0.9503,
      "step": 1540
    },
    {
      "epoch": 3.2104166666666667,
      "grad_norm": 11.802702903747559,
      "learning_rate": 7.543981481481482e-06,
      "loss": 1.8369,
      "step": 1541
    },
    {
      "epoch": 3.2125,
      "grad_norm": 9.771520614624023,
      "learning_rate": 7.541666666666667e-06,
      "loss": 1.4373,
      "step": 1542
    },
    {
      "epoch": 3.214583333333333,
      "grad_norm": 17.274822235107422,
      "learning_rate": 7.539351851851853e-06,
      "loss": 1.7356,
      "step": 1543
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 9.114928245544434,
      "learning_rate": 7.537037037037037e-06,
      "loss": 0.9651,
      "step": 1544
    },
    {
      "epoch": 3.21875,
      "grad_norm": 6.809887409210205,
      "learning_rate": 7.534722222222223e-06,
      "loss": 1.6175,
      "step": 1545
    },
    {
      "epoch": 3.220833333333333,
      "grad_norm": 9.645605087280273,
      "learning_rate": 7.532407407407408e-06,
      "loss": 1.6838,
      "step": 1546
    },
    {
      "epoch": 3.222916666666667,
      "grad_norm": 8.373592376708984,
      "learning_rate": 7.530092592592593e-06,
      "loss": 1.249,
      "step": 1547
    },
    {
      "epoch": 3.225,
      "grad_norm": 9.409747123718262,
      "learning_rate": 7.527777777777778e-06,
      "loss": 1.5772,
      "step": 1548
    },
    {
      "epoch": 3.2270833333333333,
      "grad_norm": 11.152097702026367,
      "learning_rate": 7.525462962962964e-06,
      "loss": 1.0724,
      "step": 1549
    },
    {
      "epoch": 3.2291666666666665,
      "grad_norm": 8.573594093322754,
      "learning_rate": 7.523148148148148e-06,
      "loss": 0.3231,
      "step": 1550
    },
    {
      "epoch": 3.23125,
      "grad_norm": 16.784666061401367,
      "learning_rate": 7.5208333333333335e-06,
      "loss": 2.005,
      "step": 1551
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 6.79121732711792,
      "learning_rate": 7.518518518518519e-06,
      "loss": 1.7129,
      "step": 1552
    },
    {
      "epoch": 3.2354166666666666,
      "grad_norm": 12.629203796386719,
      "learning_rate": 7.516203703703704e-06,
      "loss": 2.2928,
      "step": 1553
    },
    {
      "epoch": 3.2375,
      "grad_norm": 14.161707878112793,
      "learning_rate": 7.51388888888889e-06,
      "loss": 1.4301,
      "step": 1554
    },
    {
      "epoch": 3.2395833333333335,
      "grad_norm": 14.840030670166016,
      "learning_rate": 7.511574074074075e-06,
      "loss": 1.5768,
      "step": 1555
    },
    {
      "epoch": 3.2416666666666667,
      "grad_norm": 14.7915678024292,
      "learning_rate": 7.50925925925926e-06,
      "loss": 1.7694,
      "step": 1556
    },
    {
      "epoch": 3.24375,
      "grad_norm": 11.507760047912598,
      "learning_rate": 7.506944444444445e-06,
      "loss": 2.0875,
      "step": 1557
    },
    {
      "epoch": 3.245833333333333,
      "grad_norm": 7.641468048095703,
      "learning_rate": 7.504629629629631e-06,
      "loss": 0.8924,
      "step": 1558
    },
    {
      "epoch": 3.247916666666667,
      "grad_norm": 9.933541297912598,
      "learning_rate": 7.502314814814815e-06,
      "loss": 0.8785,
      "step": 1559
    },
    {
      "epoch": 3.25,
      "grad_norm": 8.755648612976074,
      "learning_rate": 7.500000000000001e-06,
      "loss": 2.0841,
      "step": 1560
    },
    {
      "epoch": 3.252083333333333,
      "grad_norm": 9.650700569152832,
      "learning_rate": 7.497685185185186e-06,
      "loss": 1.5697,
      "step": 1561
    },
    {
      "epoch": 3.2541666666666664,
      "grad_norm": 10.012605667114258,
      "learning_rate": 7.49537037037037e-06,
      "loss": 1.5995,
      "step": 1562
    },
    {
      "epoch": 3.25625,
      "grad_norm": 26.51213836669922,
      "learning_rate": 7.493055555555556e-06,
      "loss": 1.7491,
      "step": 1563
    },
    {
      "epoch": 3.2583333333333333,
      "grad_norm": 8.43543815612793,
      "learning_rate": 7.4907407407407414e-06,
      "loss": 1.6438,
      "step": 1564
    },
    {
      "epoch": 3.2604166666666665,
      "grad_norm": 13.937514305114746,
      "learning_rate": 7.4884259259259265e-06,
      "loss": 1.3385,
      "step": 1565
    },
    {
      "epoch": 3.2625,
      "grad_norm": 5.736406326293945,
      "learning_rate": 7.486111111111112e-06,
      "loss": 1.7463,
      "step": 1566
    },
    {
      "epoch": 3.2645833333333334,
      "grad_norm": 16.080251693725586,
      "learning_rate": 7.4837962962962976e-06,
      "loss": 1.6186,
      "step": 1567
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 7.547855377197266,
      "learning_rate": 7.481481481481482e-06,
      "loss": 0.9444,
      "step": 1568
    },
    {
      "epoch": 3.26875,
      "grad_norm": 17.10163116455078,
      "learning_rate": 7.479166666666668e-06,
      "loss": 1.9013,
      "step": 1569
    },
    {
      "epoch": 3.2708333333333335,
      "grad_norm": 14.835648536682129,
      "learning_rate": 7.476851851851853e-06,
      "loss": 1.7368,
      "step": 1570
    },
    {
      "epoch": 3.2729166666666667,
      "grad_norm": 10.834020614624023,
      "learning_rate": 7.474537037037037e-06,
      "loss": 1.1643,
      "step": 1571
    },
    {
      "epoch": 3.275,
      "grad_norm": 19.578550338745117,
      "learning_rate": 7.472222222222223e-06,
      "loss": 1.7627,
      "step": 1572
    },
    {
      "epoch": 3.2770833333333336,
      "grad_norm": 44.571937561035156,
      "learning_rate": 7.469907407407408e-06,
      "loss": 1.9678,
      "step": 1573
    },
    {
      "epoch": 3.279166666666667,
      "grad_norm": 9.677410125732422,
      "learning_rate": 7.467592592592593e-06,
      "loss": 1.661,
      "step": 1574
    },
    {
      "epoch": 3.28125,
      "grad_norm": 9.858698844909668,
      "learning_rate": 7.465277777777778e-06,
      "loss": 1.7471,
      "step": 1575
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 36.677711486816406,
      "learning_rate": 7.462962962962964e-06,
      "loss": 1.9514,
      "step": 1576
    },
    {
      "epoch": 3.2854166666666664,
      "grad_norm": 8.807937622070312,
      "learning_rate": 7.4606481481481485e-06,
      "loss": 1.3699,
      "step": 1577
    },
    {
      "epoch": 3.2875,
      "grad_norm": 7.394006729125977,
      "learning_rate": 7.4583333333333345e-06,
      "loss": 1.6894,
      "step": 1578
    },
    {
      "epoch": 3.2895833333333333,
      "grad_norm": 13.909343719482422,
      "learning_rate": 7.45601851851852e-06,
      "loss": 1.4029,
      "step": 1579
    },
    {
      "epoch": 3.2916666666666665,
      "grad_norm": 11.338515281677246,
      "learning_rate": 7.453703703703704e-06,
      "loss": 1.6826,
      "step": 1580
    },
    {
      "epoch": 3.29375,
      "grad_norm": 16.424779891967773,
      "learning_rate": 7.45138888888889e-06,
      "loss": 2.4926,
      "step": 1581
    },
    {
      "epoch": 3.2958333333333334,
      "grad_norm": 6.375638961791992,
      "learning_rate": 7.449074074074075e-06,
      "loss": 1.6913,
      "step": 1582
    },
    {
      "epoch": 3.2979166666666666,
      "grad_norm": 18.018815994262695,
      "learning_rate": 7.44675925925926e-06,
      "loss": 1.8181,
      "step": 1583
    },
    {
      "epoch": 3.3,
      "grad_norm": 7.878902435302734,
      "learning_rate": 7.444444444444445e-06,
      "loss": 1.3957,
      "step": 1584
    },
    {
      "epoch": 3.3020833333333335,
      "grad_norm": 14.018404960632324,
      "learning_rate": 7.442129629629629e-06,
      "loss": 1.7356,
      "step": 1585
    },
    {
      "epoch": 3.3041666666666667,
      "grad_norm": 17.591201782226562,
      "learning_rate": 7.439814814814815e-06,
      "loss": 1.7966,
      "step": 1586
    },
    {
      "epoch": 3.30625,
      "grad_norm": 10.548720359802246,
      "learning_rate": 7.437500000000001e-06,
      "loss": 1.0219,
      "step": 1587
    },
    {
      "epoch": 3.3083333333333336,
      "grad_norm": 10.608027458190918,
      "learning_rate": 7.4351851851851855e-06,
      "loss": 1.0804,
      "step": 1588
    },
    {
      "epoch": 3.310416666666667,
      "grad_norm": 8.771062850952148,
      "learning_rate": 7.4328703703703706e-06,
      "loss": 2.0086,
      "step": 1589
    },
    {
      "epoch": 3.3125,
      "grad_norm": 21.4487361907959,
      "learning_rate": 7.4305555555555565e-06,
      "loss": 1.1547,
      "step": 1590
    },
    {
      "epoch": 3.314583333333333,
      "grad_norm": 8.415531158447266,
      "learning_rate": 7.428240740740741e-06,
      "loss": 0.3152,
      "step": 1591
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 21.63953971862793,
      "learning_rate": 7.425925925925927e-06,
      "loss": 1.6218,
      "step": 1592
    },
    {
      "epoch": 3.31875,
      "grad_norm": 11.98835277557373,
      "learning_rate": 7.423611111111112e-06,
      "loss": 1.3875,
      "step": 1593
    },
    {
      "epoch": 3.3208333333333333,
      "grad_norm": 8.89303970336914,
      "learning_rate": 7.421296296296296e-06,
      "loss": 0.8606,
      "step": 1594
    },
    {
      "epoch": 3.3229166666666665,
      "grad_norm": 9.209890365600586,
      "learning_rate": 7.418981481481482e-06,
      "loss": 1.3506,
      "step": 1595
    },
    {
      "epoch": 3.325,
      "grad_norm": 22.61867904663086,
      "learning_rate": 7.416666666666668e-06,
      "loss": 1.3663,
      "step": 1596
    },
    {
      "epoch": 3.3270833333333334,
      "grad_norm": 8.256967544555664,
      "learning_rate": 7.414351851851852e-06,
      "loss": 1.7161,
      "step": 1597
    },
    {
      "epoch": 3.3291666666666666,
      "grad_norm": 10.00704288482666,
      "learning_rate": 7.412037037037037e-06,
      "loss": 1.6142,
      "step": 1598
    },
    {
      "epoch": 3.33125,
      "grad_norm": 18.178945541381836,
      "learning_rate": 7.409722222222223e-06,
      "loss": 1.874,
      "step": 1599
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 10.068429946899414,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.9889,
      "step": 1600
    },
    {
      "epoch": 3.3354166666666667,
      "grad_norm": 27.309974670410156,
      "learning_rate": 7.4050925925925934e-06,
      "loss": 1.8958,
      "step": 1601
    },
    {
      "epoch": 3.3375,
      "grad_norm": 18.568273544311523,
      "learning_rate": 7.4027777777777785e-06,
      "loss": 1.5701,
      "step": 1602
    },
    {
      "epoch": 3.3395833333333336,
      "grad_norm": 12.982307434082031,
      "learning_rate": 7.400462962962963e-06,
      "loss": 1.6528,
      "step": 1603
    },
    {
      "epoch": 3.341666666666667,
      "grad_norm": 9.356151580810547,
      "learning_rate": 7.398148148148149e-06,
      "loss": 1.0598,
      "step": 1604
    },
    {
      "epoch": 3.34375,
      "grad_norm": 8.072667121887207,
      "learning_rate": 7.395833333333335e-06,
      "loss": 0.76,
      "step": 1605
    },
    {
      "epoch": 3.345833333333333,
      "grad_norm": 38.873233795166016,
      "learning_rate": 7.393518518518519e-06,
      "loss": 1.801,
      "step": 1606
    },
    {
      "epoch": 3.3479166666666664,
      "grad_norm": 7.00020694732666,
      "learning_rate": 7.391203703703704e-06,
      "loss": 0.9467,
      "step": 1607
    },
    {
      "epoch": 3.35,
      "grad_norm": 30.37868881225586,
      "learning_rate": 7.38888888888889e-06,
      "loss": 1.6885,
      "step": 1608
    },
    {
      "epoch": 3.3520833333333333,
      "grad_norm": 6.744537830352783,
      "learning_rate": 7.386574074074074e-06,
      "loss": 1.0166,
      "step": 1609
    },
    {
      "epoch": 3.3541666666666665,
      "grad_norm": 17.484420776367188,
      "learning_rate": 7.38425925925926e-06,
      "loss": 1.8736,
      "step": 1610
    },
    {
      "epoch": 3.35625,
      "grad_norm": 28.259212493896484,
      "learning_rate": 7.381944444444445e-06,
      "loss": 2.8837,
      "step": 1611
    },
    {
      "epoch": 3.3583333333333334,
      "grad_norm": 11.746487617492676,
      "learning_rate": 7.3796296296296295e-06,
      "loss": 1.3968,
      "step": 1612
    },
    {
      "epoch": 3.3604166666666666,
      "grad_norm": 9.01839542388916,
      "learning_rate": 7.3773148148148154e-06,
      "loss": 1.3336,
      "step": 1613
    },
    {
      "epoch": 3.3625,
      "grad_norm": 8.018553733825684,
      "learning_rate": 7.375000000000001e-06,
      "loss": 0.2884,
      "step": 1614
    },
    {
      "epoch": 3.3645833333333335,
      "grad_norm": 15.201492309570312,
      "learning_rate": 7.372685185185186e-06,
      "loss": 1.7241,
      "step": 1615
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 9.645254135131836,
      "learning_rate": 7.370370370370371e-06,
      "loss": 1.7776,
      "step": 1616
    },
    {
      "epoch": 3.36875,
      "grad_norm": 28.836811065673828,
      "learning_rate": 7.368055555555557e-06,
      "loss": 1.8165,
      "step": 1617
    },
    {
      "epoch": 3.3708333333333336,
      "grad_norm": 7.875435829162598,
      "learning_rate": 7.365740740740741e-06,
      "loss": 1.1385,
      "step": 1618
    },
    {
      "epoch": 3.372916666666667,
      "grad_norm": 8.772188186645508,
      "learning_rate": 7.363425925925927e-06,
      "loss": 0.9775,
      "step": 1619
    },
    {
      "epoch": 3.375,
      "grad_norm": 12.345734596252441,
      "learning_rate": 7.361111111111112e-06,
      "loss": 0.9569,
      "step": 1620
    },
    {
      "epoch": 3.377083333333333,
      "grad_norm": 7.353253364562988,
      "learning_rate": 7.358796296296297e-06,
      "loss": 1.1224,
      "step": 1621
    },
    {
      "epoch": 3.3791666666666664,
      "grad_norm": 7.627312660217285,
      "learning_rate": 7.356481481481482e-06,
      "loss": 1.6345,
      "step": 1622
    },
    {
      "epoch": 3.38125,
      "grad_norm": 7.261421203613281,
      "learning_rate": 7.354166666666668e-06,
      "loss": 0.9426,
      "step": 1623
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 10.258609771728516,
      "learning_rate": 7.351851851851852e-06,
      "loss": 1.6117,
      "step": 1624
    },
    {
      "epoch": 3.3854166666666665,
      "grad_norm": 9.316633224487305,
      "learning_rate": 7.3495370370370375e-06,
      "loss": 1.6309,
      "step": 1625
    },
    {
      "epoch": 3.3875,
      "grad_norm": 16.68070411682129,
      "learning_rate": 7.347222222222223e-06,
      "loss": 1.4416,
      "step": 1626
    },
    {
      "epoch": 3.3895833333333334,
      "grad_norm": 5.863217353820801,
      "learning_rate": 7.344907407407408e-06,
      "loss": 1.7482,
      "step": 1627
    },
    {
      "epoch": 3.3916666666666666,
      "grad_norm": 6.835014343261719,
      "learning_rate": 7.342592592592594e-06,
      "loss": 1.0547,
      "step": 1628
    },
    {
      "epoch": 3.39375,
      "grad_norm": 10.086164474487305,
      "learning_rate": 7.340277777777778e-06,
      "loss": 0.9827,
      "step": 1629
    },
    {
      "epoch": 3.3958333333333335,
      "grad_norm": 11.915184020996094,
      "learning_rate": 7.337962962962964e-06,
      "loss": 1.4128,
      "step": 1630
    },
    {
      "epoch": 3.3979166666666667,
      "grad_norm": 19.501697540283203,
      "learning_rate": 7.335648148148149e-06,
      "loss": 1.4627,
      "step": 1631
    },
    {
      "epoch": 3.4,
      "grad_norm": 21.5625057220459,
      "learning_rate": 7.333333333333333e-06,
      "loss": 2.4624,
      "step": 1632
    },
    {
      "epoch": 3.4020833333333336,
      "grad_norm": 10.715518951416016,
      "learning_rate": 7.331018518518519e-06,
      "loss": 0.9577,
      "step": 1633
    },
    {
      "epoch": 3.404166666666667,
      "grad_norm": 7.497332572937012,
      "learning_rate": 7.328703703703704e-06,
      "loss": 0.2629,
      "step": 1634
    },
    {
      "epoch": 3.40625,
      "grad_norm": 18.989225387573242,
      "learning_rate": 7.326388888888889e-06,
      "loss": 2.0901,
      "step": 1635
    },
    {
      "epoch": 3.408333333333333,
      "grad_norm": 7.810740947723389,
      "learning_rate": 7.324074074074074e-06,
      "loss": 1.0685,
      "step": 1636
    },
    {
      "epoch": 3.4104166666666664,
      "grad_norm": 17.906980514526367,
      "learning_rate": 7.32175925925926e-06,
      "loss": 1.5912,
      "step": 1637
    },
    {
      "epoch": 3.4125,
      "grad_norm": 16.026880264282227,
      "learning_rate": 7.3194444444444446e-06,
      "loss": 1.6939,
      "step": 1638
    },
    {
      "epoch": 3.4145833333333333,
      "grad_norm": 9.959946632385254,
      "learning_rate": 7.3171296296296305e-06,
      "loss": 1.5253,
      "step": 1639
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 7.504960536956787,
      "learning_rate": 7.314814814814816e-06,
      "loss": 0.2687,
      "step": 1640
    },
    {
      "epoch": 3.41875,
      "grad_norm": 13.508217811584473,
      "learning_rate": 7.3125e-06,
      "loss": 1.7424,
      "step": 1641
    },
    {
      "epoch": 3.4208333333333334,
      "grad_norm": 8.41077995300293,
      "learning_rate": 7.310185185185186e-06,
      "loss": 1.3078,
      "step": 1642
    },
    {
      "epoch": 3.4229166666666666,
      "grad_norm": 12.450629234313965,
      "learning_rate": 7.307870370370371e-06,
      "loss": 1.7006,
      "step": 1643
    },
    {
      "epoch": 3.425,
      "grad_norm": 20.538532257080078,
      "learning_rate": 7.305555555555556e-06,
      "loss": 1.0175,
      "step": 1644
    },
    {
      "epoch": 3.4270833333333335,
      "grad_norm": 18.254602432250977,
      "learning_rate": 7.303240740740741e-06,
      "loss": 2.0085,
      "step": 1645
    },
    {
      "epoch": 3.4291666666666667,
      "grad_norm": 12.898154258728027,
      "learning_rate": 7.300925925925927e-06,
      "loss": 1.1693,
      "step": 1646
    },
    {
      "epoch": 3.43125,
      "grad_norm": 9.666025161743164,
      "learning_rate": 7.298611111111111e-06,
      "loss": 1.8145,
      "step": 1647
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 113.5013656616211,
      "learning_rate": 7.296296296296297e-06,
      "loss": 0.9161,
      "step": 1648
    },
    {
      "epoch": 3.435416666666667,
      "grad_norm": 29.34650993347168,
      "learning_rate": 7.293981481481482e-06,
      "loss": 1.7215,
      "step": 1649
    },
    {
      "epoch": 3.4375,
      "grad_norm": 13.662503242492676,
      "learning_rate": 7.291666666666667e-06,
      "loss": 1.5714,
      "step": 1650
    },
    {
      "epoch": 3.439583333333333,
      "grad_norm": 23.6654109954834,
      "learning_rate": 7.2893518518518525e-06,
      "loss": 2.1716,
      "step": 1651
    },
    {
      "epoch": 3.4416666666666664,
      "grad_norm": 63.714290618896484,
      "learning_rate": 7.287037037037038e-06,
      "loss": 2.5473,
      "step": 1652
    },
    {
      "epoch": 3.44375,
      "grad_norm": 15.02548885345459,
      "learning_rate": 7.284722222222223e-06,
      "loss": 0.6598,
      "step": 1653
    },
    {
      "epoch": 3.4458333333333333,
      "grad_norm": 19.08040428161621,
      "learning_rate": 7.282407407407408e-06,
      "loss": 1.3547,
      "step": 1654
    },
    {
      "epoch": 3.4479166666666665,
      "grad_norm": 17.60173225402832,
      "learning_rate": 7.280092592592594e-06,
      "loss": 1.6185,
      "step": 1655
    },
    {
      "epoch": 3.45,
      "grad_norm": 10.10284423828125,
      "learning_rate": 7.277777777777778e-06,
      "loss": 1.0307,
      "step": 1656
    },
    {
      "epoch": 3.4520833333333334,
      "grad_norm": 16.611562728881836,
      "learning_rate": 7.275462962962964e-06,
      "loss": 1.732,
      "step": 1657
    },
    {
      "epoch": 3.4541666666666666,
      "grad_norm": 5.175429821014404,
      "learning_rate": 7.273148148148149e-06,
      "loss": 0.9735,
      "step": 1658
    },
    {
      "epoch": 3.45625,
      "grad_norm": 18.039793014526367,
      "learning_rate": 7.270833333333333e-06,
      "loss": 2.1694,
      "step": 1659
    },
    {
      "epoch": 3.4583333333333335,
      "grad_norm": 16.88463592529297,
      "learning_rate": 7.268518518518519e-06,
      "loss": 2.0776,
      "step": 1660
    },
    {
      "epoch": 3.4604166666666667,
      "grad_norm": 15.792427062988281,
      "learning_rate": 7.266203703703704e-06,
      "loss": 1.7847,
      "step": 1661
    },
    {
      "epoch": 3.4625,
      "grad_norm": 18.555753707885742,
      "learning_rate": 7.2638888888888895e-06,
      "loss": 1.8071,
      "step": 1662
    },
    {
      "epoch": 3.4645833333333336,
      "grad_norm": 8.177962303161621,
      "learning_rate": 7.2615740740740746e-06,
      "loss": 1.1273,
      "step": 1663
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 27.433286666870117,
      "learning_rate": 7.2592592592592605e-06,
      "loss": 1.5041,
      "step": 1664
    },
    {
      "epoch": 3.46875,
      "grad_norm": 14.41543960571289,
      "learning_rate": 7.256944444444445e-06,
      "loss": 1.3153,
      "step": 1665
    },
    {
      "epoch": 3.470833333333333,
      "grad_norm": 22.87526512145996,
      "learning_rate": 7.254629629629631e-06,
      "loss": 1.6143,
      "step": 1666
    },
    {
      "epoch": 3.4729166666666664,
      "grad_norm": 17.882108688354492,
      "learning_rate": 7.252314814814816e-06,
      "loss": 2.2876,
      "step": 1667
    },
    {
      "epoch": 3.475,
      "grad_norm": 12.09583568572998,
      "learning_rate": 7.25e-06,
      "loss": 1.4332,
      "step": 1668
    },
    {
      "epoch": 3.4770833333333333,
      "grad_norm": 12.05620288848877,
      "learning_rate": 7.247685185185186e-06,
      "loss": 1.8658,
      "step": 1669
    },
    {
      "epoch": 3.4791666666666665,
      "grad_norm": 16.801897048950195,
      "learning_rate": 7.245370370370371e-06,
      "loss": 2.1664,
      "step": 1670
    },
    {
      "epoch": 3.48125,
      "grad_norm": 33.802696228027344,
      "learning_rate": 7.243055555555556e-06,
      "loss": 1.8423,
      "step": 1671
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 35.96992874145508,
      "learning_rate": 7.240740740740741e-06,
      "loss": 1.5462,
      "step": 1672
    },
    {
      "epoch": 3.4854166666666666,
      "grad_norm": 12.071066856384277,
      "learning_rate": 7.238425925925926e-06,
      "loss": 1.5422,
      "step": 1673
    },
    {
      "epoch": 3.4875,
      "grad_norm": 35.127445220947266,
      "learning_rate": 7.2361111111111115e-06,
      "loss": 1.736,
      "step": 1674
    },
    {
      "epoch": 3.4895833333333335,
      "grad_norm": 16.82675552368164,
      "learning_rate": 7.233796296296297e-06,
      "loss": 1.2449,
      "step": 1675
    },
    {
      "epoch": 3.4916666666666667,
      "grad_norm": 11.151525497436523,
      "learning_rate": 7.231481481481482e-06,
      "loss": 1.4057,
      "step": 1676
    },
    {
      "epoch": 3.49375,
      "grad_norm": 15.060898780822754,
      "learning_rate": 7.229166666666667e-06,
      "loss": 1.0882,
      "step": 1677
    },
    {
      "epoch": 3.4958333333333336,
      "grad_norm": 7.994150161743164,
      "learning_rate": 7.226851851851853e-06,
      "loss": 0.6099,
      "step": 1678
    },
    {
      "epoch": 3.497916666666667,
      "grad_norm": 9.501409530639648,
      "learning_rate": 7.224537037037037e-06,
      "loss": 1.9782,
      "step": 1679
    },
    {
      "epoch": 3.5,
      "grad_norm": 8.918756484985352,
      "learning_rate": 7.222222222222223e-06,
      "loss": 1.2228,
      "step": 1680
    },
    {
      "epoch": 3.502083333333333,
      "grad_norm": 12.125460624694824,
      "learning_rate": 7.219907407407408e-06,
      "loss": 1.0783,
      "step": 1681
    },
    {
      "epoch": 3.5041666666666664,
      "grad_norm": 11.220775604248047,
      "learning_rate": 7.217592592592593e-06,
      "loss": 1.6226,
      "step": 1682
    },
    {
      "epoch": 3.50625,
      "grad_norm": 8.545475006103516,
      "learning_rate": 7.215277777777778e-06,
      "loss": 0.9836,
      "step": 1683
    },
    {
      "epoch": 3.5083333333333333,
      "grad_norm": 8.05124282836914,
      "learning_rate": 7.212962962962964e-06,
      "loss": 1.7657,
      "step": 1684
    },
    {
      "epoch": 3.5104166666666665,
      "grad_norm": 15.647439956665039,
      "learning_rate": 7.210648148148148e-06,
      "loss": 1.0831,
      "step": 1685
    },
    {
      "epoch": 3.5125,
      "grad_norm": 10.877839088439941,
      "learning_rate": 7.2083333333333335e-06,
      "loss": 1.0311,
      "step": 1686
    },
    {
      "epoch": 3.5145833333333334,
      "grad_norm": 16.00374412536621,
      "learning_rate": 7.2060185185185194e-06,
      "loss": 0.9601,
      "step": 1687
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 11.09376335144043,
      "learning_rate": 7.203703703703704e-06,
      "loss": 1.079,
      "step": 1688
    },
    {
      "epoch": 3.51875,
      "grad_norm": 5.309576511383057,
      "learning_rate": 7.20138888888889e-06,
      "loss": 0.9389,
      "step": 1689
    },
    {
      "epoch": 3.5208333333333335,
      "grad_norm": 42.95549392700195,
      "learning_rate": 7.199074074074075e-06,
      "loss": 2.1036,
      "step": 1690
    },
    {
      "epoch": 3.5229166666666667,
      "grad_norm": 11.648248672485352,
      "learning_rate": 7.19675925925926e-06,
      "loss": 1.6452,
      "step": 1691
    },
    {
      "epoch": 3.525,
      "grad_norm": 7.986398696899414,
      "learning_rate": 7.194444444444445e-06,
      "loss": 0.9458,
      "step": 1692
    },
    {
      "epoch": 3.5270833333333336,
      "grad_norm": 20.00924301147461,
      "learning_rate": 7.192129629629631e-06,
      "loss": 1.4043,
      "step": 1693
    },
    {
      "epoch": 3.529166666666667,
      "grad_norm": 18.44000816345215,
      "learning_rate": 7.189814814814815e-06,
      "loss": 1.6352,
      "step": 1694
    },
    {
      "epoch": 3.53125,
      "grad_norm": 12.93783950805664,
      "learning_rate": 7.1875e-06,
      "loss": 1.9867,
      "step": 1695
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 9.340569496154785,
      "learning_rate": 7.185185185185186e-06,
      "loss": 1.7988,
      "step": 1696
    },
    {
      "epoch": 3.5354166666666664,
      "grad_norm": 10.716985702514648,
      "learning_rate": 7.18287037037037e-06,
      "loss": 1.0078,
      "step": 1697
    },
    {
      "epoch": 3.5375,
      "grad_norm": 9.3206205368042,
      "learning_rate": 7.180555555555556e-06,
      "loss": 1.7022,
      "step": 1698
    },
    {
      "epoch": 3.5395833333333333,
      "grad_norm": 10.463186264038086,
      "learning_rate": 7.1782407407407415e-06,
      "loss": 1.6713,
      "step": 1699
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 11.422173500061035,
      "learning_rate": 7.1759259259259266e-06,
      "loss": 1.3773,
      "step": 1700
    },
    {
      "epoch": 3.54375,
      "grad_norm": 9.12143611907959,
      "learning_rate": 7.173611111111112e-06,
      "loss": 1.5016,
      "step": 1701
    },
    {
      "epoch": 3.5458333333333334,
      "grad_norm": 42.4495849609375,
      "learning_rate": 7.171296296296298e-06,
      "loss": 1.7248,
      "step": 1702
    },
    {
      "epoch": 3.5479166666666666,
      "grad_norm": 13.56595230102539,
      "learning_rate": 7.168981481481482e-06,
      "loss": 2.3498,
      "step": 1703
    },
    {
      "epoch": 3.55,
      "grad_norm": 7.2852959632873535,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.9905,
      "step": 1704
    },
    {
      "epoch": 3.5520833333333335,
      "grad_norm": 25.04623031616211,
      "learning_rate": 7.164351851851853e-06,
      "loss": 2.5668,
      "step": 1705
    },
    {
      "epoch": 3.5541666666666667,
      "grad_norm": 15.122292518615723,
      "learning_rate": 7.162037037037037e-06,
      "loss": 2.2227,
      "step": 1706
    },
    {
      "epoch": 3.55625,
      "grad_norm": 8.861183166503906,
      "learning_rate": 7.159722222222223e-06,
      "loss": 1.3418,
      "step": 1707
    },
    {
      "epoch": 3.5583333333333336,
      "grad_norm": 9.481142044067383,
      "learning_rate": 7.157407407407408e-06,
      "loss": 1.806,
      "step": 1708
    },
    {
      "epoch": 3.560416666666667,
      "grad_norm": 6.52583646774292,
      "learning_rate": 7.155092592592593e-06,
      "loss": 0.9261,
      "step": 1709
    },
    {
      "epoch": 3.5625,
      "grad_norm": 10.82502555847168,
      "learning_rate": 7.152777777777778e-06,
      "loss": 2.1039,
      "step": 1710
    },
    {
      "epoch": 3.564583333333333,
      "grad_norm": 38.751712799072266,
      "learning_rate": 7.150462962962964e-06,
      "loss": 1.2896,
      "step": 1711
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 11.618090629577637,
      "learning_rate": 7.1481481481481486e-06,
      "loss": 0.9118,
      "step": 1712
    },
    {
      "epoch": 3.56875,
      "grad_norm": 13.726027488708496,
      "learning_rate": 7.145833333333334e-06,
      "loss": 2.0339,
      "step": 1713
    },
    {
      "epoch": 3.5708333333333333,
      "grad_norm": 9.178050994873047,
      "learning_rate": 7.14351851851852e-06,
      "loss": 1.3539,
      "step": 1714
    },
    {
      "epoch": 3.5729166666666665,
      "grad_norm": 19.068790435791016,
      "learning_rate": 7.141203703703704e-06,
      "loss": 1.9064,
      "step": 1715
    },
    {
      "epoch": 3.575,
      "grad_norm": 10.174125671386719,
      "learning_rate": 7.13888888888889e-06,
      "loss": 1.582,
      "step": 1716
    },
    {
      "epoch": 3.5770833333333334,
      "grad_norm": 22.242557525634766,
      "learning_rate": 7.136574074074075e-06,
      "loss": 1.4102,
      "step": 1717
    },
    {
      "epoch": 3.5791666666666666,
      "grad_norm": 15.62907886505127,
      "learning_rate": 7.13425925925926e-06,
      "loss": 1.6562,
      "step": 1718
    },
    {
      "epoch": 3.58125,
      "grad_norm": 17.581300735473633,
      "learning_rate": 7.131944444444445e-06,
      "loss": 1.6271,
      "step": 1719
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 10.664396286010742,
      "learning_rate": 7.129629629629629e-06,
      "loss": 0.9265,
      "step": 1720
    },
    {
      "epoch": 3.5854166666666667,
      "grad_norm": 20.200008392333984,
      "learning_rate": 7.127314814814815e-06,
      "loss": 1.3721,
      "step": 1721
    },
    {
      "epoch": 3.5875,
      "grad_norm": 11.978306770324707,
      "learning_rate": 7.125e-06,
      "loss": 1.7728,
      "step": 1722
    },
    {
      "epoch": 3.5895833333333336,
      "grad_norm": 11.945725440979004,
      "learning_rate": 7.1226851851851855e-06,
      "loss": 1.5782,
      "step": 1723
    },
    {
      "epoch": 3.591666666666667,
      "grad_norm": 9.04812240600586,
      "learning_rate": 7.120370370370371e-06,
      "loss": 0.6243,
      "step": 1724
    },
    {
      "epoch": 3.59375,
      "grad_norm": 9.195188522338867,
      "learning_rate": 7.1180555555555565e-06,
      "loss": 1.5552,
      "step": 1725
    },
    {
      "epoch": 3.595833333333333,
      "grad_norm": 9.611793518066406,
      "learning_rate": 7.115740740740741e-06,
      "loss": 1.1591,
      "step": 1726
    },
    {
      "epoch": 3.5979166666666664,
      "grad_norm": 9.493340492248535,
      "learning_rate": 7.113425925925927e-06,
      "loss": 1.454,
      "step": 1727
    },
    {
      "epoch": 3.6,
      "grad_norm": 9.464757919311523,
      "learning_rate": 7.111111111111112e-06,
      "loss": 1.8245,
      "step": 1728
    },
    {
      "epoch": 3.6020833333333333,
      "grad_norm": 8.262813568115234,
      "learning_rate": 7.108796296296296e-06,
      "loss": 1.5837,
      "step": 1729
    },
    {
      "epoch": 3.6041666666666665,
      "grad_norm": 53.886905670166016,
      "learning_rate": 7.106481481481482e-06,
      "loss": 1.817,
      "step": 1730
    },
    {
      "epoch": 3.60625,
      "grad_norm": 8.614126205444336,
      "learning_rate": 7.104166666666668e-06,
      "loss": 1.5679,
      "step": 1731
    },
    {
      "epoch": 3.6083333333333334,
      "grad_norm": 7.301322937011719,
      "learning_rate": 7.101851851851852e-06,
      "loss": 0.9374,
      "step": 1732
    },
    {
      "epoch": 3.6104166666666666,
      "grad_norm": 10.323362350463867,
      "learning_rate": 7.099537037037037e-06,
      "loss": 2.0793,
      "step": 1733
    },
    {
      "epoch": 3.6125,
      "grad_norm": 8.073936462402344,
      "learning_rate": 7.097222222222223e-06,
      "loss": 1.6039,
      "step": 1734
    },
    {
      "epoch": 3.6145833333333335,
      "grad_norm": 36.45586395263672,
      "learning_rate": 7.0949074074074075e-06,
      "loss": 1.6667,
      "step": 1735
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 10.070120811462402,
      "learning_rate": 7.0925925925925935e-06,
      "loss": 2.1432,
      "step": 1736
    },
    {
      "epoch": 3.61875,
      "grad_norm": 9.082972526550293,
      "learning_rate": 7.0902777777777785e-06,
      "loss": 1.8336,
      "step": 1737
    },
    {
      "epoch": 3.6208333333333336,
      "grad_norm": 27.682191848754883,
      "learning_rate": 7.087962962962963e-06,
      "loss": 0.7906,
      "step": 1738
    },
    {
      "epoch": 3.622916666666667,
      "grad_norm": 5.99149751663208,
      "learning_rate": 7.085648148148149e-06,
      "loss": 0.9475,
      "step": 1739
    },
    {
      "epoch": 3.625,
      "grad_norm": 9.11334228515625,
      "learning_rate": 7.083333333333335e-06,
      "loss": 1.8534,
      "step": 1740
    },
    {
      "epoch": 3.627083333333333,
      "grad_norm": 8.236690521240234,
      "learning_rate": 7.081018518518519e-06,
      "loss": 1.7708,
      "step": 1741
    },
    {
      "epoch": 3.6291666666666664,
      "grad_norm": 18.511293411254883,
      "learning_rate": 7.078703703703704e-06,
      "loss": 2.2234,
      "step": 1742
    },
    {
      "epoch": 3.63125,
      "grad_norm": 9.710458755493164,
      "learning_rate": 7.07638888888889e-06,
      "loss": 1.4332,
      "step": 1743
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 62.15922546386719,
      "learning_rate": 7.074074074074074e-06,
      "loss": 1.3128,
      "step": 1744
    },
    {
      "epoch": 3.6354166666666665,
      "grad_norm": 18.75208854675293,
      "learning_rate": 7.07175925925926e-06,
      "loss": 1.9081,
      "step": 1745
    },
    {
      "epoch": 3.6375,
      "grad_norm": 7.74569034576416,
      "learning_rate": 7.069444444444445e-06,
      "loss": 1.7694,
      "step": 1746
    },
    {
      "epoch": 3.6395833333333334,
      "grad_norm": 16.188257217407227,
      "learning_rate": 7.0671296296296295e-06,
      "loss": 2.2032,
      "step": 1747
    },
    {
      "epoch": 3.6416666666666666,
      "grad_norm": 7.437633037567139,
      "learning_rate": 7.0648148148148155e-06,
      "loss": 1.4968,
      "step": 1748
    },
    {
      "epoch": 3.64375,
      "grad_norm": 9.086793899536133,
      "learning_rate": 7.062500000000001e-06,
      "loss": 1.528,
      "step": 1749
    },
    {
      "epoch": 3.6458333333333335,
      "grad_norm": 10.87898063659668,
      "learning_rate": 7.060185185185186e-06,
      "loss": 1.4276,
      "step": 1750
    },
    {
      "epoch": 3.6479166666666667,
      "grad_norm": 7.917608261108398,
      "learning_rate": 7.057870370370371e-06,
      "loss": 0.6937,
      "step": 1751
    },
    {
      "epoch": 3.65,
      "grad_norm": 20.569194793701172,
      "learning_rate": 7.055555555555557e-06,
      "loss": 1.7238,
      "step": 1752
    },
    {
      "epoch": 3.6520833333333336,
      "grad_norm": 15.466102600097656,
      "learning_rate": 7.053240740740741e-06,
      "loss": 1.3163,
      "step": 1753
    },
    {
      "epoch": 3.654166666666667,
      "grad_norm": 12.56051254272461,
      "learning_rate": 7.050925925925927e-06,
      "loss": 1.3211,
      "step": 1754
    },
    {
      "epoch": 3.65625,
      "grad_norm": 10.892282485961914,
      "learning_rate": 7.048611111111112e-06,
      "loss": 1.7559,
      "step": 1755
    },
    {
      "epoch": 3.658333333333333,
      "grad_norm": 12.73090648651123,
      "learning_rate": 7.046296296296296e-06,
      "loss": 1.3667,
      "step": 1756
    },
    {
      "epoch": 3.6604166666666664,
      "grad_norm": 15.616012573242188,
      "learning_rate": 7.043981481481482e-06,
      "loss": 1.9968,
      "step": 1757
    },
    {
      "epoch": 3.6625,
      "grad_norm": 11.60941219329834,
      "learning_rate": 7.041666666666668e-06,
      "loss": 1.9175,
      "step": 1758
    },
    {
      "epoch": 3.6645833333333333,
      "grad_norm": 8.892830848693848,
      "learning_rate": 7.039351851851852e-06,
      "loss": 1.3109,
      "step": 1759
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 7.843093395233154,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 0.9124,
      "step": 1760
    },
    {
      "epoch": 3.66875,
      "grad_norm": 18.679758071899414,
      "learning_rate": 7.0347222222222234e-06,
      "loss": 1.7546,
      "step": 1761
    },
    {
      "epoch": 3.6708333333333334,
      "grad_norm": 28.129150390625,
      "learning_rate": 7.032407407407408e-06,
      "loss": 1.7537,
      "step": 1762
    },
    {
      "epoch": 3.6729166666666666,
      "grad_norm": 7.220090389251709,
      "learning_rate": 7.030092592592594e-06,
      "loss": 0.9046,
      "step": 1763
    },
    {
      "epoch": 3.675,
      "grad_norm": 10.676445960998535,
      "learning_rate": 7.027777777777778e-06,
      "loss": 0.6715,
      "step": 1764
    },
    {
      "epoch": 3.6770833333333335,
      "grad_norm": 13.961736679077148,
      "learning_rate": 7.025462962962963e-06,
      "loss": 2.0197,
      "step": 1765
    },
    {
      "epoch": 3.6791666666666667,
      "grad_norm": 16.857168197631836,
      "learning_rate": 7.023148148148149e-06,
      "loss": 1.243,
      "step": 1766
    },
    {
      "epoch": 3.68125,
      "grad_norm": 19.83894157409668,
      "learning_rate": 7.020833333333333e-06,
      "loss": 1.8228,
      "step": 1767
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 24.525136947631836,
      "learning_rate": 7.018518518518519e-06,
      "loss": 1.4376,
      "step": 1768
    },
    {
      "epoch": 3.685416666666667,
      "grad_norm": 11.972535133361816,
      "learning_rate": 7.016203703703704e-06,
      "loss": 1.8047,
      "step": 1769
    },
    {
      "epoch": 3.6875,
      "grad_norm": 10.043950080871582,
      "learning_rate": 7.013888888888889e-06,
      "loss": 1.2057,
      "step": 1770
    },
    {
      "epoch": 3.689583333333333,
      "grad_norm": 12.13235855102539,
      "learning_rate": 7.011574074074074e-06,
      "loss": 1.8833,
      "step": 1771
    },
    {
      "epoch": 3.6916666666666664,
      "grad_norm": 11.921090126037598,
      "learning_rate": 7.00925925925926e-06,
      "loss": 1.4659,
      "step": 1772
    },
    {
      "epoch": 3.69375,
      "grad_norm": 11.628682136535645,
      "learning_rate": 7.006944444444445e-06,
      "loss": 1.5751,
      "step": 1773
    },
    {
      "epoch": 3.6958333333333333,
      "grad_norm": 19.928428649902344,
      "learning_rate": 7.00462962962963e-06,
      "loss": 2.3853,
      "step": 1774
    },
    {
      "epoch": 3.6979166666666665,
      "grad_norm": 8.933985710144043,
      "learning_rate": 7.002314814814816e-06,
      "loss": 1.6506,
      "step": 1775
    },
    {
      "epoch": 3.7,
      "grad_norm": 31.431150436401367,
      "learning_rate": 7e-06,
      "loss": 1.9763,
      "step": 1776
    },
    {
      "epoch": 3.7020833333333334,
      "grad_norm": 25.67710304260254,
      "learning_rate": 6.997685185185186e-06,
      "loss": 0.9352,
      "step": 1777
    },
    {
      "epoch": 3.7041666666666666,
      "grad_norm": 30.859416961669922,
      "learning_rate": 6.995370370370371e-06,
      "loss": 2.1742,
      "step": 1778
    },
    {
      "epoch": 3.70625,
      "grad_norm": 7.8485188484191895,
      "learning_rate": 6.993055555555556e-06,
      "loss": 0.5536,
      "step": 1779
    },
    {
      "epoch": 3.7083333333333335,
      "grad_norm": 12.44090747833252,
      "learning_rate": 6.990740740740741e-06,
      "loss": 1.1646,
      "step": 1780
    },
    {
      "epoch": 3.7104166666666667,
      "grad_norm": 17.661897659301758,
      "learning_rate": 6.988425925925927e-06,
      "loss": 1.5733,
      "step": 1781
    },
    {
      "epoch": 3.7125,
      "grad_norm": 9.220908164978027,
      "learning_rate": 6.986111111111111e-06,
      "loss": 1.9243,
      "step": 1782
    },
    {
      "epoch": 3.7145833333333336,
      "grad_norm": 17.5721435546875,
      "learning_rate": 6.983796296296297e-06,
      "loss": 1.2025,
      "step": 1783
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 8.003365516662598,
      "learning_rate": 6.981481481481482e-06,
      "loss": 0.9683,
      "step": 1784
    },
    {
      "epoch": 3.71875,
      "grad_norm": 11.194072723388672,
      "learning_rate": 6.979166666666667e-06,
      "loss": 1.0722,
      "step": 1785
    },
    {
      "epoch": 3.720833333333333,
      "grad_norm": 10.632672309875488,
      "learning_rate": 6.9768518518518526e-06,
      "loss": 1.1406,
      "step": 1786
    },
    {
      "epoch": 3.7229166666666664,
      "grad_norm": 8.878738403320312,
      "learning_rate": 6.974537037037038e-06,
      "loss": 1.5136,
      "step": 1787
    },
    {
      "epoch": 3.725,
      "grad_norm": 7.569488048553467,
      "learning_rate": 6.972222222222223e-06,
      "loss": 0.8038,
      "step": 1788
    },
    {
      "epoch": 3.7270833333333333,
      "grad_norm": 6.3354105949401855,
      "learning_rate": 6.969907407407408e-06,
      "loss": 0.958,
      "step": 1789
    },
    {
      "epoch": 3.7291666666666665,
      "grad_norm": 16.041419982910156,
      "learning_rate": 6.967592592592594e-06,
      "loss": 0.9966,
      "step": 1790
    },
    {
      "epoch": 3.73125,
      "grad_norm": 10.260173797607422,
      "learning_rate": 6.965277777777778e-06,
      "loss": 0.9265,
      "step": 1791
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 14.229849815368652,
      "learning_rate": 6.962962962962964e-06,
      "loss": 0.8759,
      "step": 1792
    },
    {
      "epoch": 3.7354166666666666,
      "grad_norm": 9.171324729919434,
      "learning_rate": 6.960648148148149e-06,
      "loss": 1.7445,
      "step": 1793
    },
    {
      "epoch": 3.7375,
      "grad_norm": 7.802009105682373,
      "learning_rate": 6.958333333333333e-06,
      "loss": 0.3008,
      "step": 1794
    },
    {
      "epoch": 3.7395833333333335,
      "grad_norm": 10.226055145263672,
      "learning_rate": 6.956018518518519e-06,
      "loss": 0.8605,
      "step": 1795
    },
    {
      "epoch": 3.7416666666666667,
      "grad_norm": 8.112367630004883,
      "learning_rate": 6.953703703703704e-06,
      "loss": 0.8557,
      "step": 1796
    },
    {
      "epoch": 3.74375,
      "grad_norm": 9.99953556060791,
      "learning_rate": 6.9513888888888895e-06,
      "loss": 1.3876,
      "step": 1797
    },
    {
      "epoch": 3.7458333333333336,
      "grad_norm": 23.032299041748047,
      "learning_rate": 6.949074074074075e-06,
      "loss": 1.1843,
      "step": 1798
    },
    {
      "epoch": 3.747916666666667,
      "grad_norm": 50.304622650146484,
      "learning_rate": 6.9467592592592605e-06,
      "loss": 1.3717,
      "step": 1799
    },
    {
      "epoch": 3.75,
      "grad_norm": 12.991585731506348,
      "learning_rate": 6.944444444444445e-06,
      "loss": 1.7534,
      "step": 1800
    },
    {
      "epoch": 3.752083333333333,
      "grad_norm": 13.63359546661377,
      "learning_rate": 6.942129629629631e-06,
      "loss": 1.0829,
      "step": 1801
    },
    {
      "epoch": 3.7541666666666664,
      "grad_norm": 10.296820640563965,
      "learning_rate": 6.939814814814816e-06,
      "loss": 1.453,
      "step": 1802
    },
    {
      "epoch": 3.75625,
      "grad_norm": 7.623169422149658,
      "learning_rate": 6.9375e-06,
      "loss": 1.1516,
      "step": 1803
    },
    {
      "epoch": 3.7583333333333333,
      "grad_norm": 9.138689041137695,
      "learning_rate": 6.935185185185186e-06,
      "loss": 1.0307,
      "step": 1804
    },
    {
      "epoch": 3.7604166666666665,
      "grad_norm": 41.55263137817383,
      "learning_rate": 6.932870370370371e-06,
      "loss": 2.0101,
      "step": 1805
    },
    {
      "epoch": 3.7625,
      "grad_norm": 24.794466018676758,
      "learning_rate": 6.930555555555556e-06,
      "loss": 1.6561,
      "step": 1806
    },
    {
      "epoch": 3.7645833333333334,
      "grad_norm": 18.56903076171875,
      "learning_rate": 6.928240740740741e-06,
      "loss": 2.1164,
      "step": 1807
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 13.012846946716309,
      "learning_rate": 6.9259259259259256e-06,
      "loss": 1.1441,
      "step": 1808
    },
    {
      "epoch": 3.76875,
      "grad_norm": 9.75301742553711,
      "learning_rate": 6.9236111111111115e-06,
      "loss": 0.948,
      "step": 1809
    },
    {
      "epoch": 3.7708333333333335,
      "grad_norm": 12.151175498962402,
      "learning_rate": 6.9212962962962974e-06,
      "loss": 1.5613,
      "step": 1810
    },
    {
      "epoch": 3.7729166666666667,
      "grad_norm": 22.250125885009766,
      "learning_rate": 6.918981481481482e-06,
      "loss": 1.8135,
      "step": 1811
    },
    {
      "epoch": 3.775,
      "grad_norm": 7.919724941253662,
      "learning_rate": 6.916666666666667e-06,
      "loss": 1.2952,
      "step": 1812
    },
    {
      "epoch": 3.7770833333333336,
      "grad_norm": 11.750845909118652,
      "learning_rate": 6.914351851851853e-06,
      "loss": 1.467,
      "step": 1813
    },
    {
      "epoch": 3.779166666666667,
      "grad_norm": 12.012130737304688,
      "learning_rate": 6.912037037037037e-06,
      "loss": 1.3254,
      "step": 1814
    },
    {
      "epoch": 3.78125,
      "grad_norm": 14.354424476623535,
      "learning_rate": 6.909722222222223e-06,
      "loss": 1.8224,
      "step": 1815
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 40.46702575683594,
      "learning_rate": 6.907407407407408e-06,
      "loss": 0.5972,
      "step": 1816
    },
    {
      "epoch": 3.7854166666666664,
      "grad_norm": 11.911059379577637,
      "learning_rate": 6.905092592592592e-06,
      "loss": 2.0055,
      "step": 1817
    },
    {
      "epoch": 3.7875,
      "grad_norm": 9.059321403503418,
      "learning_rate": 6.902777777777778e-06,
      "loss": 1.6043,
      "step": 1818
    },
    {
      "epoch": 3.7895833333333333,
      "grad_norm": 13.530099868774414,
      "learning_rate": 6.900462962962964e-06,
      "loss": 1.8206,
      "step": 1819
    },
    {
      "epoch": 3.7916666666666665,
      "grad_norm": 5.522034168243408,
      "learning_rate": 6.898148148148148e-06,
      "loss": 0.95,
      "step": 1820
    },
    {
      "epoch": 3.79375,
      "grad_norm": 10.512953758239746,
      "learning_rate": 6.8958333333333335e-06,
      "loss": 1.9635,
      "step": 1821
    },
    {
      "epoch": 3.7958333333333334,
      "grad_norm": 10.036212921142578,
      "learning_rate": 6.8935185185185195e-06,
      "loss": 0.8779,
      "step": 1822
    },
    {
      "epoch": 3.7979166666666666,
      "grad_norm": 14.06045913696289,
      "learning_rate": 6.891203703703704e-06,
      "loss": 1.5555,
      "step": 1823
    },
    {
      "epoch": 3.8,
      "grad_norm": 44.981937408447266,
      "learning_rate": 6.88888888888889e-06,
      "loss": 1.7594,
      "step": 1824
    },
    {
      "epoch": 3.8020833333333335,
      "grad_norm": 10.939713478088379,
      "learning_rate": 6.886574074074075e-06,
      "loss": 1.5938,
      "step": 1825
    },
    {
      "epoch": 3.8041666666666667,
      "grad_norm": 8.973058700561523,
      "learning_rate": 6.884259259259259e-06,
      "loss": 1.5997,
      "step": 1826
    },
    {
      "epoch": 3.80625,
      "grad_norm": 21.239290237426758,
      "learning_rate": 6.881944444444445e-06,
      "loss": 1.3414,
      "step": 1827
    },
    {
      "epoch": 3.8083333333333336,
      "grad_norm": 26.64497947692871,
      "learning_rate": 6.879629629629631e-06,
      "loss": 1.7631,
      "step": 1828
    },
    {
      "epoch": 3.810416666666667,
      "grad_norm": 62.59910202026367,
      "learning_rate": 6.877314814814815e-06,
      "loss": 1.5271,
      "step": 1829
    },
    {
      "epoch": 3.8125,
      "grad_norm": 7.4893388748168945,
      "learning_rate": 6.875e-06,
      "loss": 1.2591,
      "step": 1830
    },
    {
      "epoch": 3.814583333333333,
      "grad_norm": 38.378509521484375,
      "learning_rate": 6.872685185185186e-06,
      "loss": 1.2144,
      "step": 1831
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 19.01032257080078,
      "learning_rate": 6.8703703703703704e-06,
      "loss": 1.8074,
      "step": 1832
    },
    {
      "epoch": 3.81875,
      "grad_norm": 23.66621208190918,
      "learning_rate": 6.868055555555556e-06,
      "loss": 1.7849,
      "step": 1833
    },
    {
      "epoch": 3.8208333333333333,
      "grad_norm": 9.14863395690918,
      "learning_rate": 6.8657407407407415e-06,
      "loss": 0.6148,
      "step": 1834
    },
    {
      "epoch": 3.8229166666666665,
      "grad_norm": 17.28691864013672,
      "learning_rate": 6.863425925925927e-06,
      "loss": 1.6284,
      "step": 1835
    },
    {
      "epoch": 3.825,
      "grad_norm": 23.85458755493164,
      "learning_rate": 6.861111111111112e-06,
      "loss": 1.7626,
      "step": 1836
    },
    {
      "epoch": 3.8270833333333334,
      "grad_norm": 135.8699188232422,
      "learning_rate": 6.858796296296298e-06,
      "loss": 1.9702,
      "step": 1837
    },
    {
      "epoch": 3.8291666666666666,
      "grad_norm": 36.37723922729492,
      "learning_rate": 6.856481481481482e-06,
      "loss": 2.0202,
      "step": 1838
    },
    {
      "epoch": 3.83125,
      "grad_norm": 19.909299850463867,
      "learning_rate": 6.854166666666667e-06,
      "loss": 1.4964,
      "step": 1839
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 15.833413124084473,
      "learning_rate": 6.851851851851853e-06,
      "loss": 1.9287,
      "step": 1840
    },
    {
      "epoch": 3.8354166666666667,
      "grad_norm": 15.529512405395508,
      "learning_rate": 6.849537037037037e-06,
      "loss": 1.821,
      "step": 1841
    },
    {
      "epoch": 3.8375,
      "grad_norm": 9.3222017288208,
      "learning_rate": 6.847222222222223e-06,
      "loss": 1.1396,
      "step": 1842
    },
    {
      "epoch": 3.8395833333333336,
      "grad_norm": 38.51394271850586,
      "learning_rate": 6.844907407407408e-06,
      "loss": 1.9558,
      "step": 1843
    },
    {
      "epoch": 3.841666666666667,
      "grad_norm": 14.813787460327148,
      "learning_rate": 6.842592592592593e-06,
      "loss": 1.1446,
      "step": 1844
    },
    {
      "epoch": 3.84375,
      "grad_norm": 11.81872272491455,
      "learning_rate": 6.840277777777778e-06,
      "loss": 0.7306,
      "step": 1845
    },
    {
      "epoch": 3.845833333333333,
      "grad_norm": 19.75581932067871,
      "learning_rate": 6.837962962962964e-06,
      "loss": 1.8583,
      "step": 1846
    },
    {
      "epoch": 3.8479166666666664,
      "grad_norm": 10.471599578857422,
      "learning_rate": 6.835648148148149e-06,
      "loss": 1.7969,
      "step": 1847
    },
    {
      "epoch": 3.85,
      "grad_norm": 8.787249565124512,
      "learning_rate": 6.833333333333334e-06,
      "loss": 0.9447,
      "step": 1848
    },
    {
      "epoch": 3.8520833333333333,
      "grad_norm": 53.80881881713867,
      "learning_rate": 6.83101851851852e-06,
      "loss": 2.8879,
      "step": 1849
    },
    {
      "epoch": 3.8541666666666665,
      "grad_norm": 8.31037712097168,
      "learning_rate": 6.828703703703704e-06,
      "loss": 1.0053,
      "step": 1850
    },
    {
      "epoch": 3.85625,
      "grad_norm": 8.792478561401367,
      "learning_rate": 6.82638888888889e-06,
      "loss": 1.3876,
      "step": 1851
    },
    {
      "epoch": 3.8583333333333334,
      "grad_norm": 8.36783218383789,
      "learning_rate": 6.824074074074075e-06,
      "loss": 1.6552,
      "step": 1852
    },
    {
      "epoch": 3.8604166666666666,
      "grad_norm": 8.918842315673828,
      "learning_rate": 6.82175925925926e-06,
      "loss": 1.5567,
      "step": 1853
    },
    {
      "epoch": 3.8625,
      "grad_norm": 12.696499824523926,
      "learning_rate": 6.819444444444445e-06,
      "loss": 1.0275,
      "step": 1854
    },
    {
      "epoch": 3.8645833333333335,
      "grad_norm": 13.42149543762207,
      "learning_rate": 6.817129629629629e-06,
      "loss": 1.1258,
      "step": 1855
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 15.190305709838867,
      "learning_rate": 6.814814814814815e-06,
      "loss": 2.234,
      "step": 1856
    },
    {
      "epoch": 3.86875,
      "grad_norm": 8.366508483886719,
      "learning_rate": 6.8125e-06,
      "loss": 1.2005,
      "step": 1857
    },
    {
      "epoch": 3.8708333333333336,
      "grad_norm": 17.554325103759766,
      "learning_rate": 6.8101851851851855e-06,
      "loss": 1.7102,
      "step": 1858
    },
    {
      "epoch": 3.872916666666667,
      "grad_norm": 7.961248874664307,
      "learning_rate": 6.807870370370371e-06,
      "loss": 0.6346,
      "step": 1859
    },
    {
      "epoch": 3.875,
      "grad_norm": 16.7261962890625,
      "learning_rate": 6.8055555555555566e-06,
      "loss": 1.1239,
      "step": 1860
    },
    {
      "epoch": 3.877083333333333,
      "grad_norm": 12.530271530151367,
      "learning_rate": 6.803240740740741e-06,
      "loss": 1.5598,
      "step": 1861
    },
    {
      "epoch": 3.8791666666666664,
      "grad_norm": 13.192801475524902,
      "learning_rate": 6.800925925925927e-06,
      "loss": 1.6615,
      "step": 1862
    },
    {
      "epoch": 3.88125,
      "grad_norm": 9.321806907653809,
      "learning_rate": 6.798611111111112e-06,
      "loss": 1.1674,
      "step": 1863
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 5.019077301025391,
      "learning_rate": 6.796296296296296e-06,
      "loss": 1.874,
      "step": 1864
    },
    {
      "epoch": 3.8854166666666665,
      "grad_norm": 11.528986930847168,
      "learning_rate": 6.793981481481482e-06,
      "loss": 1.7437,
      "step": 1865
    },
    {
      "epoch": 3.8875,
      "grad_norm": 14.58928108215332,
      "learning_rate": 6.791666666666667e-06,
      "loss": 1.0389,
      "step": 1866
    },
    {
      "epoch": 3.8895833333333334,
      "grad_norm": 17.67219352722168,
      "learning_rate": 6.789351851851852e-06,
      "loss": 1.966,
      "step": 1867
    },
    {
      "epoch": 3.8916666666666666,
      "grad_norm": 19.97287368774414,
      "learning_rate": 6.787037037037037e-06,
      "loss": 2.1605,
      "step": 1868
    },
    {
      "epoch": 3.89375,
      "grad_norm": 7.986180305480957,
      "learning_rate": 6.784722222222223e-06,
      "loss": 0.9583,
      "step": 1869
    },
    {
      "epoch": 3.8958333333333335,
      "grad_norm": 14.437077522277832,
      "learning_rate": 6.7824074074074075e-06,
      "loss": 1.4022,
      "step": 1870
    },
    {
      "epoch": 3.8979166666666667,
      "grad_norm": 16.1430721282959,
      "learning_rate": 6.7800925925925935e-06,
      "loss": 1.7302,
      "step": 1871
    },
    {
      "epoch": 3.9,
      "grad_norm": 60.87017822265625,
      "learning_rate": 6.777777777777779e-06,
      "loss": 2.5161,
      "step": 1872
    },
    {
      "epoch": 3.9020833333333336,
      "grad_norm": 19.255210876464844,
      "learning_rate": 6.775462962962963e-06,
      "loss": 1.8433,
      "step": 1873
    },
    {
      "epoch": 3.904166666666667,
      "grad_norm": 24.109375,
      "learning_rate": 6.773148148148149e-06,
      "loss": 0.3177,
      "step": 1874
    },
    {
      "epoch": 3.90625,
      "grad_norm": 7.150232315063477,
      "learning_rate": 6.770833333333334e-06,
      "loss": 0.711,
      "step": 1875
    },
    {
      "epoch": 3.908333333333333,
      "grad_norm": 9.495348930358887,
      "learning_rate": 6.768518518518519e-06,
      "loss": 0.7916,
      "step": 1876
    },
    {
      "epoch": 3.9104166666666664,
      "grad_norm": 21.471355438232422,
      "learning_rate": 6.766203703703704e-06,
      "loss": 1.9892,
      "step": 1877
    },
    {
      "epoch": 3.9125,
      "grad_norm": 27.938583374023438,
      "learning_rate": 6.76388888888889e-06,
      "loss": 2.0571,
      "step": 1878
    },
    {
      "epoch": 3.9145833333333333,
      "grad_norm": 9.477163314819336,
      "learning_rate": 6.761574074074074e-06,
      "loss": 1.6307,
      "step": 1879
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 8.781667709350586,
      "learning_rate": 6.75925925925926e-06,
      "loss": 0.3781,
      "step": 1880
    },
    {
      "epoch": 3.91875,
      "grad_norm": 28.51422691345215,
      "learning_rate": 6.756944444444445e-06,
      "loss": 1.249,
      "step": 1881
    },
    {
      "epoch": 3.9208333333333334,
      "grad_norm": 12.86593246459961,
      "learning_rate": 6.7546296296296296e-06,
      "loss": 1.0478,
      "step": 1882
    },
    {
      "epoch": 3.9229166666666666,
      "grad_norm": 9.412169456481934,
      "learning_rate": 6.7523148148148155e-06,
      "loss": 1.5418,
      "step": 1883
    },
    {
      "epoch": 3.925,
      "grad_norm": 6.218998432159424,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.9928,
      "step": 1884
    },
    {
      "epoch": 3.9270833333333335,
      "grad_norm": 19.290132522583008,
      "learning_rate": 6.747685185185186e-06,
      "loss": 2.1362,
      "step": 1885
    },
    {
      "epoch": 3.9291666666666667,
      "grad_norm": 16.033710479736328,
      "learning_rate": 6.745370370370371e-06,
      "loss": 1.4759,
      "step": 1886
    },
    {
      "epoch": 3.93125,
      "grad_norm": 7.812376499176025,
      "learning_rate": 6.743055555555557e-06,
      "loss": 0.5786,
      "step": 1887
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 5.445054531097412,
      "learning_rate": 6.740740740740741e-06,
      "loss": 0.9605,
      "step": 1888
    },
    {
      "epoch": 3.935416666666667,
      "grad_norm": 18.09309959411621,
      "learning_rate": 6.738425925925927e-06,
      "loss": 1.8208,
      "step": 1889
    },
    {
      "epoch": 3.9375,
      "grad_norm": 51.52994155883789,
      "learning_rate": 6.736111111111112e-06,
      "loss": 2.111,
      "step": 1890
    },
    {
      "epoch": 3.939583333333333,
      "grad_norm": 28.93613624572754,
      "learning_rate": 6.733796296296296e-06,
      "loss": 2.3997,
      "step": 1891
    },
    {
      "epoch": 3.9416666666666664,
      "grad_norm": 10.187775611877441,
      "learning_rate": 6.731481481481482e-06,
      "loss": 1.9185,
      "step": 1892
    },
    {
      "epoch": 3.94375,
      "grad_norm": 15.290033340454102,
      "learning_rate": 6.729166666666667e-06,
      "loss": 1.3951,
      "step": 1893
    },
    {
      "epoch": 3.9458333333333333,
      "grad_norm": 8.870434761047363,
      "learning_rate": 6.726851851851852e-06,
      "loss": 1.0145,
      "step": 1894
    },
    {
      "epoch": 3.9479166666666665,
      "grad_norm": 45.810489654541016,
      "learning_rate": 6.7245370370370375e-06,
      "loss": 0.7839,
      "step": 1895
    },
    {
      "epoch": 3.95,
      "grad_norm": 31.416772842407227,
      "learning_rate": 6.7222222222222235e-06,
      "loss": 1.8263,
      "step": 1896
    },
    {
      "epoch": 3.9520833333333334,
      "grad_norm": 18.28611183166504,
      "learning_rate": 6.719907407407408e-06,
      "loss": 1.5095,
      "step": 1897
    },
    {
      "epoch": 3.9541666666666666,
      "grad_norm": 6.3008551597595215,
      "learning_rate": 6.717592592592594e-06,
      "loss": 1.7231,
      "step": 1898
    },
    {
      "epoch": 3.95625,
      "grad_norm": 28.869592666625977,
      "learning_rate": 6.715277777777778e-06,
      "loss": 2.3128,
      "step": 1899
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 11.522998809814453,
      "learning_rate": 6.712962962962963e-06,
      "loss": 1.2486,
      "step": 1900
    },
    {
      "epoch": 3.9604166666666667,
      "grad_norm": 9.24240779876709,
      "learning_rate": 6.710648148148149e-06,
      "loss": 1.5144,
      "step": 1901
    },
    {
      "epoch": 3.9625,
      "grad_norm": 24.494068145751953,
      "learning_rate": 6.708333333333333e-06,
      "loss": 1.6691,
      "step": 1902
    },
    {
      "epoch": 3.9645833333333336,
      "grad_norm": 60.22758483886719,
      "learning_rate": 6.706018518518519e-06,
      "loss": 1.1757,
      "step": 1903
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 8.146350860595703,
      "learning_rate": 6.703703703703704e-06,
      "loss": 0.7984,
      "step": 1904
    },
    {
      "epoch": 3.96875,
      "grad_norm": 16.367965698242188,
      "learning_rate": 6.701388888888889e-06,
      "loss": 1.7528,
      "step": 1905
    },
    {
      "epoch": 3.970833333333333,
      "grad_norm": 9.883387565612793,
      "learning_rate": 6.6990740740740744e-06,
      "loss": 1.0129,
      "step": 1906
    },
    {
      "epoch": 3.9729166666666664,
      "grad_norm": 5.646484375,
      "learning_rate": 6.69675925925926e-06,
      "loss": 0.961,
      "step": 1907
    },
    {
      "epoch": 3.975,
      "grad_norm": 17.678695678710938,
      "learning_rate": 6.694444444444445e-06,
      "loss": 1.7571,
      "step": 1908
    },
    {
      "epoch": 3.9770833333333333,
      "grad_norm": 15.019099235534668,
      "learning_rate": 6.69212962962963e-06,
      "loss": 1.2572,
      "step": 1909
    },
    {
      "epoch": 3.9791666666666665,
      "grad_norm": 9.413469314575195,
      "learning_rate": 6.689814814814816e-06,
      "loss": 1.0722,
      "step": 1910
    },
    {
      "epoch": 3.98125,
      "grad_norm": 12.754682540893555,
      "learning_rate": 6.6875e-06,
      "loss": 1.6976,
      "step": 1911
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 46.725563049316406,
      "learning_rate": 6.685185185185186e-06,
      "loss": 1.0592,
      "step": 1912
    },
    {
      "epoch": 3.9854166666666666,
      "grad_norm": 12.086416244506836,
      "learning_rate": 6.682870370370371e-06,
      "loss": 1.4173,
      "step": 1913
    },
    {
      "epoch": 3.9875,
      "grad_norm": 20.4746150970459,
      "learning_rate": 6.680555555555556e-06,
      "loss": 1.6991,
      "step": 1914
    },
    {
      "epoch": 3.9895833333333335,
      "grad_norm": 24.059329986572266,
      "learning_rate": 6.678240740740741e-06,
      "loss": 1.6173,
      "step": 1915
    },
    {
      "epoch": 3.9916666666666667,
      "grad_norm": 10.247199058532715,
      "learning_rate": 6.675925925925927e-06,
      "loss": 1.7204,
      "step": 1916
    },
    {
      "epoch": 3.99375,
      "grad_norm": 10.58751106262207,
      "learning_rate": 6.673611111111111e-06,
      "loss": 1.6867,
      "step": 1917
    },
    {
      "epoch": 3.9958333333333336,
      "grad_norm": 11.01524543762207,
      "learning_rate": 6.6712962962962965e-06,
      "loss": 0.509,
      "step": 1918
    },
    {
      "epoch": 3.997916666666667,
      "grad_norm": 8.1679105758667,
      "learning_rate": 6.668981481481482e-06,
      "loss": 1.3093,
      "step": 1919
    },
    {
      "epoch": 4.0,
      "grad_norm": 12.384806632995605,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.5335,
      "step": 1920
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.46111111111111114,
      "eval_f1": 0.3128319385341857,
      "eval_loss": 1.452092170715332,
      "eval_runtime": 34.5078,
      "eval_samples_per_second": 5.216,
      "eval_steps_per_second": 2.608,
      "step": 1920
    },
    {
      "epoch": 4.002083333333333,
      "grad_norm": 11.425989151000977,
      "learning_rate": 6.664351851851853e-06,
      "loss": 1.4921,
      "step": 1921
    },
    {
      "epoch": 4.004166666666666,
      "grad_norm": 16.230501174926758,
      "learning_rate": 6.662037037037038e-06,
      "loss": 2.1885,
      "step": 1922
    },
    {
      "epoch": 4.00625,
      "grad_norm": 14.151776313781738,
      "learning_rate": 6.659722222222223e-06,
      "loss": 1.6765,
      "step": 1923
    },
    {
      "epoch": 4.008333333333334,
      "grad_norm": 10.822608947753906,
      "learning_rate": 6.657407407407408e-06,
      "loss": 1.653,
      "step": 1924
    },
    {
      "epoch": 4.010416666666667,
      "grad_norm": 37.285892486572266,
      "learning_rate": 6.655092592592594e-06,
      "loss": 1.1519,
      "step": 1925
    },
    {
      "epoch": 4.0125,
      "grad_norm": 16.91108512878418,
      "learning_rate": 6.652777777777778e-06,
      "loss": 0.9265,
      "step": 1926
    },
    {
      "epoch": 4.014583333333333,
      "grad_norm": 13.487555503845215,
      "learning_rate": 6.650462962962963e-06,
      "loss": 1.1462,
      "step": 1927
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 15.151947975158691,
      "learning_rate": 6.648148148148149e-06,
      "loss": 0.8987,
      "step": 1928
    },
    {
      "epoch": 4.01875,
      "grad_norm": 13.549955368041992,
      "learning_rate": 6.645833333333333e-06,
      "loss": 1.8594,
      "step": 1929
    },
    {
      "epoch": 4.020833333333333,
      "grad_norm": 9.216269493103027,
      "learning_rate": 6.643518518518519e-06,
      "loss": 1.7719,
      "step": 1930
    },
    {
      "epoch": 4.022916666666666,
      "grad_norm": 9.781072616577148,
      "learning_rate": 6.641203703703704e-06,
      "loss": 0.9189,
      "step": 1931
    },
    {
      "epoch": 4.025,
      "grad_norm": 32.518699645996094,
      "learning_rate": 6.6388888888888895e-06,
      "loss": 1.4011,
      "step": 1932
    },
    {
      "epoch": 4.027083333333334,
      "grad_norm": 7.76431131362915,
      "learning_rate": 6.636574074074075e-06,
      "loss": 0.2528,
      "step": 1933
    },
    {
      "epoch": 4.029166666666667,
      "grad_norm": 11.538046836853027,
      "learning_rate": 6.6342592592592606e-06,
      "loss": 1.8721,
      "step": 1934
    },
    {
      "epoch": 4.03125,
      "grad_norm": 8.48763656616211,
      "learning_rate": 6.631944444444445e-06,
      "loss": 1.1917,
      "step": 1935
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 17.25863265991211,
      "learning_rate": 6.62962962962963e-06,
      "loss": 1.8698,
      "step": 1936
    },
    {
      "epoch": 4.035416666666666,
      "grad_norm": 9.042269706726074,
      "learning_rate": 6.627314814814816e-06,
      "loss": 1.1119,
      "step": 1937
    },
    {
      "epoch": 4.0375,
      "grad_norm": 25.47389793395996,
      "learning_rate": 6.625e-06,
      "loss": 1.7103,
      "step": 1938
    },
    {
      "epoch": 4.039583333333334,
      "grad_norm": 7.220300674438477,
      "learning_rate": 6.622685185185186e-06,
      "loss": 1.5609,
      "step": 1939
    },
    {
      "epoch": 4.041666666666667,
      "grad_norm": 8.827157020568848,
      "learning_rate": 6.620370370370371e-06,
      "loss": 1.408,
      "step": 1940
    },
    {
      "epoch": 4.04375,
      "grad_norm": 13.882715225219727,
      "learning_rate": 6.618055555555556e-06,
      "loss": 1.1827,
      "step": 1941
    },
    {
      "epoch": 4.045833333333333,
      "grad_norm": 9.256722450256348,
      "learning_rate": 6.615740740740741e-06,
      "loss": 1.0854,
      "step": 1942
    },
    {
      "epoch": 4.047916666666667,
      "grad_norm": 6.635024547576904,
      "learning_rate": 6.613425925925926e-06,
      "loss": 0.8433,
      "step": 1943
    },
    {
      "epoch": 4.05,
      "grad_norm": 30.597013473510742,
      "learning_rate": 6.6111111111111115e-06,
      "loss": 1.3982,
      "step": 1944
    },
    {
      "epoch": 4.052083333333333,
      "grad_norm": 5.383212089538574,
      "learning_rate": 6.608796296296297e-06,
      "loss": 0.9218,
      "step": 1945
    },
    {
      "epoch": 4.054166666666666,
      "grad_norm": 26.849933624267578,
      "learning_rate": 6.606481481481482e-06,
      "loss": 1.4725,
      "step": 1946
    },
    {
      "epoch": 4.05625,
      "grad_norm": 7.993929386138916,
      "learning_rate": 6.604166666666667e-06,
      "loss": 1.7755,
      "step": 1947
    },
    {
      "epoch": 4.058333333333334,
      "grad_norm": 17.22638511657715,
      "learning_rate": 6.601851851851853e-06,
      "loss": 1.9975,
      "step": 1948
    },
    {
      "epoch": 4.060416666666667,
      "grad_norm": 15.573455810546875,
      "learning_rate": 6.599537037037037e-06,
      "loss": 0.9326,
      "step": 1949
    },
    {
      "epoch": 4.0625,
      "grad_norm": 13.571311950683594,
      "learning_rate": 6.597222222222223e-06,
      "loss": 2.7499,
      "step": 1950
    },
    {
      "epoch": 4.064583333333333,
      "grad_norm": 12.601579666137695,
      "learning_rate": 6.594907407407408e-06,
      "loss": 1.5919,
      "step": 1951
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 6.787235260009766,
      "learning_rate": 6.592592592592592e-06,
      "loss": 0.2255,
      "step": 1952
    },
    {
      "epoch": 4.06875,
      "grad_norm": 27.630237579345703,
      "learning_rate": 6.590277777777778e-06,
      "loss": 1.0908,
      "step": 1953
    },
    {
      "epoch": 4.070833333333334,
      "grad_norm": 8.784195899963379,
      "learning_rate": 6.587962962962964e-06,
      "loss": 1.1012,
      "step": 1954
    },
    {
      "epoch": 4.072916666666667,
      "grad_norm": 12.637838363647461,
      "learning_rate": 6.5856481481481484e-06,
      "loss": 1.2306,
      "step": 1955
    },
    {
      "epoch": 4.075,
      "grad_norm": 15.798023223876953,
      "learning_rate": 6.5833333333333335e-06,
      "loss": 1.7679,
      "step": 1956
    },
    {
      "epoch": 4.077083333333333,
      "grad_norm": 9.386298179626465,
      "learning_rate": 6.5810185185185195e-06,
      "loss": 1.4555,
      "step": 1957
    },
    {
      "epoch": 4.079166666666667,
      "grad_norm": 22.4061279296875,
      "learning_rate": 6.578703703703704e-06,
      "loss": 0.3992,
      "step": 1958
    },
    {
      "epoch": 4.08125,
      "grad_norm": 12.433460235595703,
      "learning_rate": 6.57638888888889e-06,
      "loss": 1.8102,
      "step": 1959
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 15.827547073364258,
      "learning_rate": 6.574074074074075e-06,
      "loss": 2.018,
      "step": 1960
    },
    {
      "epoch": 4.085416666666666,
      "grad_norm": 6.9446024894714355,
      "learning_rate": 6.571759259259259e-06,
      "loss": 1.457,
      "step": 1961
    },
    {
      "epoch": 4.0875,
      "grad_norm": 8.101552963256836,
      "learning_rate": 6.569444444444445e-06,
      "loss": 0.8323,
      "step": 1962
    },
    {
      "epoch": 4.089583333333334,
      "grad_norm": 9.345844268798828,
      "learning_rate": 6.567129629629631e-06,
      "loss": 1.0702,
      "step": 1963
    },
    {
      "epoch": 4.091666666666667,
      "grad_norm": 7.7658610343933105,
      "learning_rate": 6.564814814814815e-06,
      "loss": 0.2981,
      "step": 1964
    },
    {
      "epoch": 4.09375,
      "grad_norm": 8.294095039367676,
      "learning_rate": 6.5625e-06,
      "loss": 0.4931,
      "step": 1965
    },
    {
      "epoch": 4.095833333333333,
      "grad_norm": 9.249003410339355,
      "learning_rate": 6.560185185185186e-06,
      "loss": 1.5331,
      "step": 1966
    },
    {
      "epoch": 4.097916666666666,
      "grad_norm": 23.032583236694336,
      "learning_rate": 6.5578703703703705e-06,
      "loss": 1.5929,
      "step": 1967
    },
    {
      "epoch": 4.1,
      "grad_norm": 8.424482345581055,
      "learning_rate": 6.555555555555556e-06,
      "loss": 1.5692,
      "step": 1968
    },
    {
      "epoch": 4.102083333333334,
      "grad_norm": 20.779094696044922,
      "learning_rate": 6.5532407407407415e-06,
      "loss": 1.37,
      "step": 1969
    },
    {
      "epoch": 4.104166666666667,
      "grad_norm": 24.157442092895508,
      "learning_rate": 6.550925925925926e-06,
      "loss": 1.558,
      "step": 1970
    },
    {
      "epoch": 4.10625,
      "grad_norm": 42.0416259765625,
      "learning_rate": 6.548611111111112e-06,
      "loss": 1.9327,
      "step": 1971
    },
    {
      "epoch": 4.108333333333333,
      "grad_norm": 9.870932579040527,
      "learning_rate": 6.546296296296298e-06,
      "loss": 1.2567,
      "step": 1972
    },
    {
      "epoch": 4.110416666666667,
      "grad_norm": 53.23614501953125,
      "learning_rate": 6.543981481481482e-06,
      "loss": 1.5259,
      "step": 1973
    },
    {
      "epoch": 4.1125,
      "grad_norm": 23.287763595581055,
      "learning_rate": 6.541666666666667e-06,
      "loss": 1.0006,
      "step": 1974
    },
    {
      "epoch": 4.114583333333333,
      "grad_norm": 23.329668045043945,
      "learning_rate": 6.539351851851853e-06,
      "loss": 1.5695,
      "step": 1975
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 11.4314546585083,
      "learning_rate": 6.537037037037037e-06,
      "loss": 1.6155,
      "step": 1976
    },
    {
      "epoch": 4.11875,
      "grad_norm": 17.215335845947266,
      "learning_rate": 6.534722222222223e-06,
      "loss": 1.1029,
      "step": 1977
    },
    {
      "epoch": 4.120833333333334,
      "grad_norm": 14.838798522949219,
      "learning_rate": 6.532407407407408e-06,
      "loss": 1.678,
      "step": 1978
    },
    {
      "epoch": 4.122916666666667,
      "grad_norm": 9.55412483215332,
      "learning_rate": 6.5300925925925925e-06,
      "loss": 0.4881,
      "step": 1979
    },
    {
      "epoch": 4.125,
      "grad_norm": 20.728483200073242,
      "learning_rate": 6.5277777777777784e-06,
      "loss": 1.1558,
      "step": 1980
    },
    {
      "epoch": 4.127083333333333,
      "grad_norm": 9.159552574157715,
      "learning_rate": 6.525462962962964e-06,
      "loss": 0.7886,
      "step": 1981
    },
    {
      "epoch": 4.129166666666666,
      "grad_norm": 60.69621658325195,
      "learning_rate": 6.523148148148149e-06,
      "loss": 2.1928,
      "step": 1982
    },
    {
      "epoch": 4.13125,
      "grad_norm": 14.160812377929688,
      "learning_rate": 6.520833333333334e-06,
      "loss": 2.0674,
      "step": 1983
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 10.562731742858887,
      "learning_rate": 6.51851851851852e-06,
      "loss": 0.7054,
      "step": 1984
    },
    {
      "epoch": 4.135416666666667,
      "grad_norm": 16.30321502685547,
      "learning_rate": 6.516203703703704e-06,
      "loss": 1.3789,
      "step": 1985
    },
    {
      "epoch": 4.1375,
      "grad_norm": 16.88483238220215,
      "learning_rate": 6.51388888888889e-06,
      "loss": 1.5579,
      "step": 1986
    },
    {
      "epoch": 4.139583333333333,
      "grad_norm": 18.0698184967041,
      "learning_rate": 6.511574074074075e-06,
      "loss": 1.9049,
      "step": 1987
    },
    {
      "epoch": 4.141666666666667,
      "grad_norm": 13.284558296203613,
      "learning_rate": 6.509259259259259e-06,
      "loss": 1.8808,
      "step": 1988
    },
    {
      "epoch": 4.14375,
      "grad_norm": 29.489952087402344,
      "learning_rate": 6.506944444444445e-06,
      "loss": 1.7155,
      "step": 1989
    },
    {
      "epoch": 4.145833333333333,
      "grad_norm": 14.642143249511719,
      "learning_rate": 6.504629629629629e-06,
      "loss": 2.2972,
      "step": 1990
    },
    {
      "epoch": 4.147916666666666,
      "grad_norm": 16.498838424682617,
      "learning_rate": 6.502314814814815e-06,
      "loss": 1.6754,
      "step": 1991
    },
    {
      "epoch": 4.15,
      "grad_norm": 11.70040225982666,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.3604,
      "step": 1992
    },
    {
      "epoch": 4.152083333333334,
      "grad_norm": 40.9976806640625,
      "learning_rate": 6.4976851851851855e-06,
      "loss": 2.7296,
      "step": 1993
    },
    {
      "epoch": 4.154166666666667,
      "grad_norm": 10.49081802368164,
      "learning_rate": 6.495370370370371e-06,
      "loss": 0.9213,
      "step": 1994
    },
    {
      "epoch": 4.15625,
      "grad_norm": 15.554535865783691,
      "learning_rate": 6.493055555555557e-06,
      "loss": 1.6084,
      "step": 1995
    },
    {
      "epoch": 4.158333333333333,
      "grad_norm": 7.048669338226318,
      "learning_rate": 6.490740740740741e-06,
      "loss": 0.467,
      "step": 1996
    },
    {
      "epoch": 4.160416666666666,
      "grad_norm": 11.17845630645752,
      "learning_rate": 6.488425925925926e-06,
      "loss": 0.9834,
      "step": 1997
    },
    {
      "epoch": 4.1625,
      "grad_norm": 6.81462287902832,
      "learning_rate": 6.486111111111112e-06,
      "loss": 0.9625,
      "step": 1998
    },
    {
      "epoch": 4.164583333333334,
      "grad_norm": 11.130240440368652,
      "learning_rate": 6.483796296296296e-06,
      "loss": 1.5205,
      "step": 1999
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 12.951207160949707,
      "learning_rate": 6.481481481481482e-06,
      "loss": 1.5923,
      "step": 2000
    },
    {
      "epoch": 4.16875,
      "grad_norm": 11.289643287658691,
      "learning_rate": 6.479166666666667e-06,
      "loss": 1.4912,
      "step": 2001
    },
    {
      "epoch": 4.170833333333333,
      "grad_norm": 71.06492614746094,
      "learning_rate": 6.476851851851852e-06,
      "loss": 1.7481,
      "step": 2002
    },
    {
      "epoch": 4.172916666666667,
      "grad_norm": 46.31784439086914,
      "learning_rate": 6.474537037037037e-06,
      "loss": 1.5588,
      "step": 2003
    },
    {
      "epoch": 4.175,
      "grad_norm": 7.732725620269775,
      "learning_rate": 6.472222222222223e-06,
      "loss": 0.878,
      "step": 2004
    },
    {
      "epoch": 4.177083333333333,
      "grad_norm": 8.619267463684082,
      "learning_rate": 6.4699074074074076e-06,
      "loss": 0.8265,
      "step": 2005
    },
    {
      "epoch": 4.179166666666666,
      "grad_norm": 53.96435546875,
      "learning_rate": 6.4675925925925935e-06,
      "loss": 1.7432,
      "step": 2006
    },
    {
      "epoch": 4.18125,
      "grad_norm": 6.5270771980285645,
      "learning_rate": 6.465277777777779e-06,
      "loss": 1.5384,
      "step": 2007
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 18.824901580810547,
      "learning_rate": 6.462962962962963e-06,
      "loss": 0.3818,
      "step": 2008
    },
    {
      "epoch": 4.185416666666667,
      "grad_norm": 11.541891098022461,
      "learning_rate": 6.460648148148149e-06,
      "loss": 1.4679,
      "step": 2009
    },
    {
      "epoch": 4.1875,
      "grad_norm": 23.849489212036133,
      "learning_rate": 6.458333333333334e-06,
      "loss": 1.8067,
      "step": 2010
    },
    {
      "epoch": 4.189583333333333,
      "grad_norm": 17.182811737060547,
      "learning_rate": 6.456018518518519e-06,
      "loss": 1.7487,
      "step": 2011
    },
    {
      "epoch": 4.191666666666666,
      "grad_norm": 11.475032806396484,
      "learning_rate": 6.453703703703704e-06,
      "loss": 1.6285,
      "step": 2012
    },
    {
      "epoch": 4.19375,
      "grad_norm": 16.706077575683594,
      "learning_rate": 6.45138888888889e-06,
      "loss": 1.786,
      "step": 2013
    },
    {
      "epoch": 4.195833333333334,
      "grad_norm": 7.918870449066162,
      "learning_rate": 6.449074074074074e-06,
      "loss": 0.9286,
      "step": 2014
    },
    {
      "epoch": 4.197916666666667,
      "grad_norm": 7.780755996704102,
      "learning_rate": 6.44675925925926e-06,
      "loss": 1.3824,
      "step": 2015
    },
    {
      "epoch": 4.2,
      "grad_norm": 14.306114196777344,
      "learning_rate": 6.444444444444445e-06,
      "loss": 0.8428,
      "step": 2016
    },
    {
      "epoch": 4.202083333333333,
      "grad_norm": 18.9295711517334,
      "learning_rate": 6.44212962962963e-06,
      "loss": 1.6791,
      "step": 2017
    },
    {
      "epoch": 4.204166666666667,
      "grad_norm": 22.792259216308594,
      "learning_rate": 6.4398148148148155e-06,
      "loss": 1.9871,
      "step": 2018
    },
    {
      "epoch": 4.20625,
      "grad_norm": 7.326399803161621,
      "learning_rate": 6.437500000000001e-06,
      "loss": 0.4753,
      "step": 2019
    },
    {
      "epoch": 4.208333333333333,
      "grad_norm": 63.466678619384766,
      "learning_rate": 6.435185185185186e-06,
      "loss": 2.3977,
      "step": 2020
    },
    {
      "epoch": 4.210416666666666,
      "grad_norm": 16.96965789794922,
      "learning_rate": 6.432870370370371e-06,
      "loss": 0.9362,
      "step": 2021
    },
    {
      "epoch": 4.2125,
      "grad_norm": 62.61829376220703,
      "learning_rate": 6.430555555555557e-06,
      "loss": 1.4711,
      "step": 2022
    },
    {
      "epoch": 4.214583333333334,
      "grad_norm": 17.926359176635742,
      "learning_rate": 6.428240740740741e-06,
      "loss": 1.0134,
      "step": 2023
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 8.378107070922852,
      "learning_rate": 6.425925925925927e-06,
      "loss": 0.9988,
      "step": 2024
    },
    {
      "epoch": 4.21875,
      "grad_norm": 52.04790115356445,
      "learning_rate": 6.423611111111112e-06,
      "loss": 1.9155,
      "step": 2025
    },
    {
      "epoch": 4.220833333333333,
      "grad_norm": 46.05841064453125,
      "learning_rate": 6.421296296296296e-06,
      "loss": 1.2577,
      "step": 2026
    },
    {
      "epoch": 4.222916666666666,
      "grad_norm": 11.354354858398438,
      "learning_rate": 6.418981481481482e-06,
      "loss": 0.9535,
      "step": 2027
    },
    {
      "epoch": 4.225,
      "grad_norm": 11.933024406433105,
      "learning_rate": 6.416666666666667e-06,
      "loss": 0.965,
      "step": 2028
    },
    {
      "epoch": 4.227083333333334,
      "grad_norm": 12.003023147583008,
      "learning_rate": 6.4143518518518524e-06,
      "loss": 1.9231,
      "step": 2029
    },
    {
      "epoch": 4.229166666666667,
      "grad_norm": 9.521496772766113,
      "learning_rate": 6.4120370370370375e-06,
      "loss": 1.0431,
      "step": 2030
    },
    {
      "epoch": 4.23125,
      "grad_norm": 19.00130271911621,
      "learning_rate": 6.4097222222222235e-06,
      "loss": 1.2472,
      "step": 2031
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 15.87392807006836,
      "learning_rate": 6.407407407407408e-06,
      "loss": 0.9561,
      "step": 2032
    },
    {
      "epoch": 4.235416666666667,
      "grad_norm": 12.194890022277832,
      "learning_rate": 6.405092592592594e-06,
      "loss": 1.6183,
      "step": 2033
    },
    {
      "epoch": 4.2375,
      "grad_norm": 10.941765785217285,
      "learning_rate": 6.402777777777778e-06,
      "loss": 1.5168,
      "step": 2034
    },
    {
      "epoch": 4.239583333333333,
      "grad_norm": 7.642538547515869,
      "learning_rate": 6.400462962962963e-06,
      "loss": 1.3316,
      "step": 2035
    },
    {
      "epoch": 4.241666666666666,
      "grad_norm": 26.906461715698242,
      "learning_rate": 6.398148148148149e-06,
      "loss": 1.7093,
      "step": 2036
    },
    {
      "epoch": 4.24375,
      "grad_norm": 19.67803382873535,
      "learning_rate": 6.395833333333333e-06,
      "loss": 1.8053,
      "step": 2037
    },
    {
      "epoch": 4.245833333333334,
      "grad_norm": 19.664674758911133,
      "learning_rate": 6.393518518518519e-06,
      "loss": 1.8136,
      "step": 2038
    },
    {
      "epoch": 4.247916666666667,
      "grad_norm": 9.139124870300293,
      "learning_rate": 6.391203703703704e-06,
      "loss": 1.1997,
      "step": 2039
    },
    {
      "epoch": 4.25,
      "grad_norm": 11.107626914978027,
      "learning_rate": 6.3888888888888885e-06,
      "loss": 1.4841,
      "step": 2040
    },
    {
      "epoch": 4.252083333333333,
      "grad_norm": 13.411825180053711,
      "learning_rate": 6.3865740740740745e-06,
      "loss": 1.788,
      "step": 2041
    },
    {
      "epoch": 4.254166666666666,
      "grad_norm": 7.95448637008667,
      "learning_rate": 6.38425925925926e-06,
      "loss": 1.2168,
      "step": 2042
    },
    {
      "epoch": 4.25625,
      "grad_norm": 12.804543495178223,
      "learning_rate": 6.381944444444445e-06,
      "loss": 1.6021,
      "step": 2043
    },
    {
      "epoch": 4.258333333333334,
      "grad_norm": 14.646589279174805,
      "learning_rate": 6.37962962962963e-06,
      "loss": 1.4742,
      "step": 2044
    },
    {
      "epoch": 4.260416666666667,
      "grad_norm": 6.829981327056885,
      "learning_rate": 6.377314814814816e-06,
      "loss": 0.8734,
      "step": 2045
    },
    {
      "epoch": 4.2625,
      "grad_norm": 17.898582458496094,
      "learning_rate": 6.375e-06,
      "loss": 1.6304,
      "step": 2046
    },
    {
      "epoch": 4.264583333333333,
      "grad_norm": 6.396843910217285,
      "learning_rate": 6.372685185185186e-06,
      "loss": 0.8522,
      "step": 2047
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 9.76581859588623,
      "learning_rate": 6.370370370370371e-06,
      "loss": 1.0502,
      "step": 2048
    },
    {
      "epoch": 4.26875,
      "grad_norm": 8.459752082824707,
      "learning_rate": 6.368055555555555e-06,
      "loss": 1.2842,
      "step": 2049
    },
    {
      "epoch": 4.270833333333333,
      "grad_norm": 6.226346969604492,
      "learning_rate": 6.365740740740741e-06,
      "loss": 0.2037,
      "step": 2050
    },
    {
      "epoch": 4.272916666666666,
      "grad_norm": 48.53620147705078,
      "learning_rate": 6.363425925925927e-06,
      "loss": 1.3707,
      "step": 2051
    },
    {
      "epoch": 4.275,
      "grad_norm": 13.003081321716309,
      "learning_rate": 6.361111111111111e-06,
      "loss": 1.7298,
      "step": 2052
    },
    {
      "epoch": 4.277083333333334,
      "grad_norm": 19.19310760498047,
      "learning_rate": 6.3587962962962965e-06,
      "loss": 1.1149,
      "step": 2053
    },
    {
      "epoch": 4.279166666666667,
      "grad_norm": 10.155881881713867,
      "learning_rate": 6.3564814814814824e-06,
      "loss": 1.5543,
      "step": 2054
    },
    {
      "epoch": 4.28125,
      "grad_norm": 15.472383499145508,
      "learning_rate": 6.354166666666667e-06,
      "loss": 1.7343,
      "step": 2055
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 16.317480087280273,
      "learning_rate": 6.351851851851853e-06,
      "loss": 0.9237,
      "step": 2056
    },
    {
      "epoch": 4.285416666666666,
      "grad_norm": 10.119112968444824,
      "learning_rate": 6.349537037037038e-06,
      "loss": 1.2454,
      "step": 2057
    },
    {
      "epoch": 4.2875,
      "grad_norm": 11.484914779663086,
      "learning_rate": 6.347222222222223e-06,
      "loss": 1.0279,
      "step": 2058
    },
    {
      "epoch": 4.289583333333334,
      "grad_norm": 55.85091781616211,
      "learning_rate": 6.344907407407408e-06,
      "loss": 1.3799,
      "step": 2059
    },
    {
      "epoch": 4.291666666666667,
      "grad_norm": 52.287967681884766,
      "learning_rate": 6.342592592592594e-06,
      "loss": 2.492,
      "step": 2060
    },
    {
      "epoch": 4.29375,
      "grad_norm": 9.447396278381348,
      "learning_rate": 6.340277777777778e-06,
      "loss": 1.1282,
      "step": 2061
    },
    {
      "epoch": 4.295833333333333,
      "grad_norm": 10.437567710876465,
      "learning_rate": 6.337962962962963e-06,
      "loss": 1.2413,
      "step": 2062
    },
    {
      "epoch": 4.297916666666667,
      "grad_norm": 7.220652103424072,
      "learning_rate": 6.335648148148149e-06,
      "loss": 0.5353,
      "step": 2063
    },
    {
      "epoch": 4.3,
      "grad_norm": 8.67070198059082,
      "learning_rate": 6.333333333333333e-06,
      "loss": 1.6292,
      "step": 2064
    },
    {
      "epoch": 4.302083333333333,
      "grad_norm": 10.873047828674316,
      "learning_rate": 6.331018518518519e-06,
      "loss": 1.6236,
      "step": 2065
    },
    {
      "epoch": 4.304166666666666,
      "grad_norm": 12.68132495880127,
      "learning_rate": 6.3287037037037044e-06,
      "loss": 1.8285,
      "step": 2066
    },
    {
      "epoch": 4.30625,
      "grad_norm": 10.642812728881836,
      "learning_rate": 6.3263888888888895e-06,
      "loss": 1.8431,
      "step": 2067
    },
    {
      "epoch": 4.308333333333334,
      "grad_norm": 14.034026145935059,
      "learning_rate": 6.324074074074075e-06,
      "loss": 1.4293,
      "step": 2068
    },
    {
      "epoch": 4.310416666666667,
      "grad_norm": 15.200785636901855,
      "learning_rate": 6.321759259259261e-06,
      "loss": 0.5557,
      "step": 2069
    },
    {
      "epoch": 4.3125,
      "grad_norm": 7.803839206695557,
      "learning_rate": 6.319444444444445e-06,
      "loss": 0.6181,
      "step": 2070
    },
    {
      "epoch": 4.314583333333333,
      "grad_norm": 17.627342224121094,
      "learning_rate": 6.31712962962963e-06,
      "loss": 1.8611,
      "step": 2071
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 15.726095199584961,
      "learning_rate": 6.314814814814816e-06,
      "loss": 1.5935,
      "step": 2072
    },
    {
      "epoch": 4.31875,
      "grad_norm": 29.542118072509766,
      "learning_rate": 6.3125e-06,
      "loss": 2.0556,
      "step": 2073
    },
    {
      "epoch": 4.320833333333334,
      "grad_norm": 45.23333740234375,
      "learning_rate": 6.310185185185186e-06,
      "loss": 1.9605,
      "step": 2074
    },
    {
      "epoch": 4.322916666666667,
      "grad_norm": 15.136726379394531,
      "learning_rate": 6.307870370370371e-06,
      "loss": 1.7378,
      "step": 2075
    },
    {
      "epoch": 4.325,
      "grad_norm": 7.393916606903076,
      "learning_rate": 6.305555555555556e-06,
      "loss": 0.254,
      "step": 2076
    },
    {
      "epoch": 4.327083333333333,
      "grad_norm": 8.14498519897461,
      "learning_rate": 6.303240740740741e-06,
      "loss": 0.7826,
      "step": 2077
    },
    {
      "epoch": 4.329166666666667,
      "grad_norm": 10.492380142211914,
      "learning_rate": 6.300925925925926e-06,
      "loss": 1.4231,
      "step": 2078
    },
    {
      "epoch": 4.33125,
      "grad_norm": 22.342729568481445,
      "learning_rate": 6.2986111111111116e-06,
      "loss": 1.8375,
      "step": 2079
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 12.62850570678711,
      "learning_rate": 6.296296296296297e-06,
      "loss": 1.545,
      "step": 2080
    },
    {
      "epoch": 4.335416666666666,
      "grad_norm": 55.96995162963867,
      "learning_rate": 6.293981481481482e-06,
      "loss": 1.8351,
      "step": 2081
    },
    {
      "epoch": 4.3375,
      "grad_norm": 30.57046890258789,
      "learning_rate": 6.291666666666667e-06,
      "loss": 1.7628,
      "step": 2082
    },
    {
      "epoch": 4.339583333333334,
      "grad_norm": 19.395462036132812,
      "learning_rate": 6.289351851851853e-06,
      "loss": 1.7776,
      "step": 2083
    },
    {
      "epoch": 4.341666666666667,
      "grad_norm": 14.008026123046875,
      "learning_rate": 6.287037037037037e-06,
      "loss": 1.6127,
      "step": 2084
    },
    {
      "epoch": 4.34375,
      "grad_norm": 29.34648895263672,
      "learning_rate": 6.284722222222223e-06,
      "loss": 2.3662,
      "step": 2085
    },
    {
      "epoch": 4.345833333333333,
      "grad_norm": 31.345748901367188,
      "learning_rate": 6.282407407407408e-06,
      "loss": 1.6282,
      "step": 2086
    },
    {
      "epoch": 4.347916666666666,
      "grad_norm": 63.436973571777344,
      "learning_rate": 6.280092592592592e-06,
      "loss": 2.0299,
      "step": 2087
    },
    {
      "epoch": 4.35,
      "grad_norm": 13.23257827758789,
      "learning_rate": 6.277777777777778e-06,
      "loss": 1.596,
      "step": 2088
    },
    {
      "epoch": 4.352083333333334,
      "grad_norm": 8.000581741333008,
      "learning_rate": 6.275462962962963e-06,
      "loss": 0.5526,
      "step": 2089
    },
    {
      "epoch": 4.354166666666667,
      "grad_norm": 33.43376922607422,
      "learning_rate": 6.2731481481481485e-06,
      "loss": 1.6606,
      "step": 2090
    },
    {
      "epoch": 4.35625,
      "grad_norm": 25.109119415283203,
      "learning_rate": 6.2708333333333336e-06,
      "loss": 1.2522,
      "step": 2091
    },
    {
      "epoch": 4.358333333333333,
      "grad_norm": 85.55793762207031,
      "learning_rate": 6.2685185185185195e-06,
      "loss": 1.6214,
      "step": 2092
    },
    {
      "epoch": 4.360416666666667,
      "grad_norm": 22.354787826538086,
      "learning_rate": 6.266203703703704e-06,
      "loss": 1.9478,
      "step": 2093
    },
    {
      "epoch": 4.3625,
      "grad_norm": 142.5971221923828,
      "learning_rate": 6.26388888888889e-06,
      "loss": 1.9335,
      "step": 2094
    },
    {
      "epoch": 4.364583333333333,
      "grad_norm": 10.090405464172363,
      "learning_rate": 6.261574074074075e-06,
      "loss": 0.5997,
      "step": 2095
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 8.181794166564941,
      "learning_rate": 6.259259259259259e-06,
      "loss": 0.7931,
      "step": 2096
    },
    {
      "epoch": 4.36875,
      "grad_norm": 8.074870109558105,
      "learning_rate": 6.256944444444445e-06,
      "loss": 1.5746,
      "step": 2097
    },
    {
      "epoch": 4.370833333333334,
      "grad_norm": 8.626800537109375,
      "learning_rate": 6.25462962962963e-06,
      "loss": 1.5188,
      "step": 2098
    },
    {
      "epoch": 4.372916666666667,
      "grad_norm": 11.964887619018555,
      "learning_rate": 6.252314814814815e-06,
      "loss": 1.4164,
      "step": 2099
    },
    {
      "epoch": 4.375,
      "grad_norm": 20.382734298706055,
      "learning_rate": 6.25e-06,
      "loss": 1.44,
      "step": 2100
    },
    {
      "epoch": 4.377083333333333,
      "grad_norm": 45.84011459350586,
      "learning_rate": 6.247685185185186e-06,
      "loss": 3.2936,
      "step": 2101
    },
    {
      "epoch": 4.379166666666666,
      "grad_norm": 10.27885627746582,
      "learning_rate": 6.2453703703703705e-06,
      "loss": 1.9147,
      "step": 2102
    },
    {
      "epoch": 4.38125,
      "grad_norm": 10.307806015014648,
      "learning_rate": 6.2430555555555564e-06,
      "loss": 1.0097,
      "step": 2103
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 84.5711669921875,
      "learning_rate": 6.2407407407407415e-06,
      "loss": 1.0653,
      "step": 2104
    },
    {
      "epoch": 4.385416666666667,
      "grad_norm": 6.725643634796143,
      "learning_rate": 6.238425925925926e-06,
      "loss": 0.4477,
      "step": 2105
    },
    {
      "epoch": 4.3875,
      "grad_norm": 27.945106506347656,
      "learning_rate": 6.236111111111112e-06,
      "loss": 1.5605,
      "step": 2106
    },
    {
      "epoch": 4.389583333333333,
      "grad_norm": 8.184907913208008,
      "learning_rate": 6.233796296296297e-06,
      "loss": 0.8571,
      "step": 2107
    },
    {
      "epoch": 4.391666666666667,
      "grad_norm": 20.966821670532227,
      "learning_rate": 6.231481481481482e-06,
      "loss": 2.6261,
      "step": 2108
    },
    {
      "epoch": 4.39375,
      "grad_norm": 10.37118911743164,
      "learning_rate": 6.229166666666667e-06,
      "loss": 1.6532,
      "step": 2109
    },
    {
      "epoch": 4.395833333333333,
      "grad_norm": 13.622419357299805,
      "learning_rate": 6.226851851851853e-06,
      "loss": 1.7235,
      "step": 2110
    },
    {
      "epoch": 4.397916666666666,
      "grad_norm": 11.774707794189453,
      "learning_rate": 6.224537037037037e-06,
      "loss": 0.876,
      "step": 2111
    },
    {
      "epoch": 4.4,
      "grad_norm": 123.86788177490234,
      "learning_rate": 6.222222222222223e-06,
      "loss": 1.2855,
      "step": 2112
    },
    {
      "epoch": 4.402083333333334,
      "grad_norm": 7.054524898529053,
      "learning_rate": 6.219907407407408e-06,
      "loss": 0.8746,
      "step": 2113
    },
    {
      "epoch": 4.404166666666667,
      "grad_norm": 16.480775833129883,
      "learning_rate": 6.2175925925925925e-06,
      "loss": 1.4598,
      "step": 2114
    },
    {
      "epoch": 4.40625,
      "grad_norm": 9.754695892333984,
      "learning_rate": 6.2152777777777785e-06,
      "loss": 1.4745,
      "step": 2115
    },
    {
      "epoch": 4.408333333333333,
      "grad_norm": 6.002870082855225,
      "learning_rate": 6.2129629629629636e-06,
      "loss": 0.1983,
      "step": 2116
    },
    {
      "epoch": 4.410416666666666,
      "grad_norm": 14.035653114318848,
      "learning_rate": 6.210648148148149e-06,
      "loss": 1.2637,
      "step": 2117
    },
    {
      "epoch": 4.4125,
      "grad_norm": 18.534809112548828,
      "learning_rate": 6.208333333333334e-06,
      "loss": 1.769,
      "step": 2118
    },
    {
      "epoch": 4.414583333333334,
      "grad_norm": 12.45778751373291,
      "learning_rate": 6.20601851851852e-06,
      "loss": 0.7949,
      "step": 2119
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 11.55521011352539,
      "learning_rate": 6.203703703703704e-06,
      "loss": 1.3104,
      "step": 2120
    },
    {
      "epoch": 4.41875,
      "grad_norm": 13.763940811157227,
      "learning_rate": 6.20138888888889e-06,
      "loss": 1.6316,
      "step": 2121
    },
    {
      "epoch": 4.420833333333333,
      "grad_norm": 47.44227981567383,
      "learning_rate": 6.199074074074075e-06,
      "loss": 1.9842,
      "step": 2122
    },
    {
      "epoch": 4.422916666666667,
      "grad_norm": 10.592692375183105,
      "learning_rate": 6.196759259259259e-06,
      "loss": 1.5205,
      "step": 2123
    },
    {
      "epoch": 4.425,
      "grad_norm": 15.964728355407715,
      "learning_rate": 6.194444444444445e-06,
      "loss": 1.6911,
      "step": 2124
    },
    {
      "epoch": 4.427083333333333,
      "grad_norm": 11.848603248596191,
      "learning_rate": 6.1921296296296294e-06,
      "loss": 1.7587,
      "step": 2125
    },
    {
      "epoch": 4.429166666666666,
      "grad_norm": 14.447319984436035,
      "learning_rate": 6.189814814814815e-06,
      "loss": 1.6253,
      "step": 2126
    },
    {
      "epoch": 4.43125,
      "grad_norm": 22.66695785522461,
      "learning_rate": 6.1875000000000005e-06,
      "loss": 2.2615,
      "step": 2127
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 28.298742294311523,
      "learning_rate": 6.1851851851851856e-06,
      "loss": 1.6239,
      "step": 2128
    },
    {
      "epoch": 4.435416666666667,
      "grad_norm": 40.23784637451172,
      "learning_rate": 6.182870370370371e-06,
      "loss": 1.4217,
      "step": 2129
    },
    {
      "epoch": 4.4375,
      "grad_norm": 7.206608772277832,
      "learning_rate": 6.180555555555557e-06,
      "loss": 1.2781,
      "step": 2130
    },
    {
      "epoch": 4.439583333333333,
      "grad_norm": 21.19150733947754,
      "learning_rate": 6.178240740740741e-06,
      "loss": 0.6967,
      "step": 2131
    },
    {
      "epoch": 4.441666666666666,
      "grad_norm": 23.727035522460938,
      "learning_rate": 6.175925925925926e-06,
      "loss": 1.7083,
      "step": 2132
    },
    {
      "epoch": 4.44375,
      "grad_norm": 10.358406066894531,
      "learning_rate": 6.173611111111112e-06,
      "loss": 1.614,
      "step": 2133
    },
    {
      "epoch": 4.445833333333334,
      "grad_norm": 17.455944061279297,
      "learning_rate": 6.171296296296296e-06,
      "loss": 2.3332,
      "step": 2134
    },
    {
      "epoch": 4.447916666666667,
      "grad_norm": 14.177191734313965,
      "learning_rate": 6.168981481481482e-06,
      "loss": 1.2556,
      "step": 2135
    },
    {
      "epoch": 4.45,
      "grad_norm": 12.944197654724121,
      "learning_rate": 6.166666666666667e-06,
      "loss": 1.4837,
      "step": 2136
    },
    {
      "epoch": 4.452083333333333,
      "grad_norm": 16.697467803955078,
      "learning_rate": 6.164351851851852e-06,
      "loss": 1.4988,
      "step": 2137
    },
    {
      "epoch": 4.454166666666667,
      "grad_norm": 40.228057861328125,
      "learning_rate": 6.162037037037037e-06,
      "loss": 1.4525,
      "step": 2138
    },
    {
      "epoch": 4.45625,
      "grad_norm": 76.68539428710938,
      "learning_rate": 6.159722222222223e-06,
      "loss": 2.4299,
      "step": 2139
    },
    {
      "epoch": 4.458333333333333,
      "grad_norm": 9.785361289978027,
      "learning_rate": 6.157407407407408e-06,
      "loss": 1.5318,
      "step": 2140
    },
    {
      "epoch": 4.460416666666666,
      "grad_norm": 12.747964859008789,
      "learning_rate": 6.155092592592593e-06,
      "loss": 0.855,
      "step": 2141
    },
    {
      "epoch": 4.4625,
      "grad_norm": 16.253318786621094,
      "learning_rate": 6.152777777777779e-06,
      "loss": 1.3794,
      "step": 2142
    },
    {
      "epoch": 4.464583333333334,
      "grad_norm": 11.724430084228516,
      "learning_rate": 6.150462962962963e-06,
      "loss": 1.6964,
      "step": 2143
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 19.454076766967773,
      "learning_rate": 6.148148148148149e-06,
      "loss": 1.8972,
      "step": 2144
    },
    {
      "epoch": 4.46875,
      "grad_norm": 75.41490936279297,
      "learning_rate": 6.145833333333334e-06,
      "loss": 1.1349,
      "step": 2145
    },
    {
      "epoch": 4.470833333333333,
      "grad_norm": 13.971029281616211,
      "learning_rate": 6.143518518518519e-06,
      "loss": 1.1757,
      "step": 2146
    },
    {
      "epoch": 4.472916666666666,
      "grad_norm": 15.084277153015137,
      "learning_rate": 6.141203703703704e-06,
      "loss": 1.176,
      "step": 2147
    },
    {
      "epoch": 4.475,
      "grad_norm": 19.07908058166504,
      "learning_rate": 6.13888888888889e-06,
      "loss": 1.6423,
      "step": 2148
    },
    {
      "epoch": 4.477083333333334,
      "grad_norm": 31.39302635192871,
      "learning_rate": 6.136574074074074e-06,
      "loss": 0.9442,
      "step": 2149
    },
    {
      "epoch": 4.479166666666667,
      "grad_norm": 9.894124984741211,
      "learning_rate": 6.134259259259259e-06,
      "loss": 1.4101,
      "step": 2150
    },
    {
      "epoch": 4.48125,
      "grad_norm": 12.362871170043945,
      "learning_rate": 6.131944444444445e-06,
      "loss": 1.9327,
      "step": 2151
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 20.464357376098633,
      "learning_rate": 6.12962962962963e-06,
      "loss": 0.7587,
      "step": 2152
    },
    {
      "epoch": 4.485416666666667,
      "grad_norm": 17.880144119262695,
      "learning_rate": 6.1273148148148156e-06,
      "loss": 1.5069,
      "step": 2153
    },
    {
      "epoch": 4.4875,
      "grad_norm": 18.859445571899414,
      "learning_rate": 6.125000000000001e-06,
      "loss": 1.2335,
      "step": 2154
    },
    {
      "epoch": 4.489583333333333,
      "grad_norm": 13.06994342803955,
      "learning_rate": 6.122685185185186e-06,
      "loss": 1.1193,
      "step": 2155
    },
    {
      "epoch": 4.491666666666666,
      "grad_norm": 18.723337173461914,
      "learning_rate": 6.120370370370371e-06,
      "loss": 1.7511,
      "step": 2156
    },
    {
      "epoch": 4.49375,
      "grad_norm": 15.430583000183105,
      "learning_rate": 6.118055555555557e-06,
      "loss": 0.6468,
      "step": 2157
    },
    {
      "epoch": 4.495833333333334,
      "grad_norm": 47.73383331298828,
      "learning_rate": 6.115740740740741e-06,
      "loss": 1.5808,
      "step": 2158
    },
    {
      "epoch": 4.497916666666667,
      "grad_norm": 19.336742401123047,
      "learning_rate": 6.113425925925926e-06,
      "loss": 2.0171,
      "step": 2159
    },
    {
      "epoch": 4.5,
      "grad_norm": 15.053077697753906,
      "learning_rate": 6.111111111111112e-06,
      "loss": 2.2158,
      "step": 2160
    },
    {
      "epoch": 4.502083333333333,
      "grad_norm": 11.977940559387207,
      "learning_rate": 6.108796296296296e-06,
      "loss": 0.5768,
      "step": 2161
    },
    {
      "epoch": 4.504166666666666,
      "grad_norm": 11.937105178833008,
      "learning_rate": 6.106481481481482e-06,
      "loss": 1.7442,
      "step": 2162
    },
    {
      "epoch": 4.50625,
      "grad_norm": 7.243692398071289,
      "learning_rate": 6.104166666666667e-06,
      "loss": 0.9201,
      "step": 2163
    },
    {
      "epoch": 4.508333333333333,
      "grad_norm": 9.407855033874512,
      "learning_rate": 6.1018518518518525e-06,
      "loss": 1.0317,
      "step": 2164
    },
    {
      "epoch": 4.510416666666667,
      "grad_norm": 8.580540657043457,
      "learning_rate": 6.0995370370370376e-06,
      "loss": 0.7894,
      "step": 2165
    },
    {
      "epoch": 4.5125,
      "grad_norm": 7.987398147583008,
      "learning_rate": 6.0972222222222235e-06,
      "loss": 0.9891,
      "step": 2166
    },
    {
      "epoch": 4.514583333333333,
      "grad_norm": 12.531664848327637,
      "learning_rate": 6.094907407407408e-06,
      "loss": 1.0448,
      "step": 2167
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 15.260637283325195,
      "learning_rate": 6.092592592592593e-06,
      "loss": 1.2376,
      "step": 2168
    },
    {
      "epoch": 4.51875,
      "grad_norm": 18.999094009399414,
      "learning_rate": 6.090277777777778e-06,
      "loss": 0.9971,
      "step": 2169
    },
    {
      "epoch": 4.520833333333333,
      "grad_norm": 23.114887237548828,
      "learning_rate": 6.087962962962963e-06,
      "loss": 1.3604,
      "step": 2170
    },
    {
      "epoch": 4.522916666666667,
      "grad_norm": 33.769100189208984,
      "learning_rate": 6.085648148148149e-06,
      "loss": 1.5259,
      "step": 2171
    },
    {
      "epoch": 4.525,
      "grad_norm": 7.805113792419434,
      "learning_rate": 6.083333333333333e-06,
      "loss": 0.8825,
      "step": 2172
    },
    {
      "epoch": 4.527083333333334,
      "grad_norm": 14.881510734558105,
      "learning_rate": 6.081018518518519e-06,
      "loss": 1.636,
      "step": 2173
    },
    {
      "epoch": 4.529166666666667,
      "grad_norm": 8.05339527130127,
      "learning_rate": 6.078703703703704e-06,
      "loss": 1.3066,
      "step": 2174
    },
    {
      "epoch": 4.53125,
      "grad_norm": 17.20087242126465,
      "learning_rate": 6.0763888888888885e-06,
      "loss": 1.9813,
      "step": 2175
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 9.874761581420898,
      "learning_rate": 6.0740740740740745e-06,
      "loss": 1.6096,
      "step": 2176
    },
    {
      "epoch": 4.535416666666666,
      "grad_norm": 21.543115615844727,
      "learning_rate": 6.0717592592592604e-06,
      "loss": 0.9855,
      "step": 2177
    },
    {
      "epoch": 4.5375,
      "grad_norm": 8.784880638122559,
      "learning_rate": 6.069444444444445e-06,
      "loss": 0.7591,
      "step": 2178
    },
    {
      "epoch": 4.539583333333333,
      "grad_norm": 12.200404167175293,
      "learning_rate": 6.06712962962963e-06,
      "loss": 1.5054,
      "step": 2179
    },
    {
      "epoch": 4.541666666666667,
      "grad_norm": 19.614208221435547,
      "learning_rate": 6.064814814814816e-06,
      "loss": 0.8496,
      "step": 2180
    },
    {
      "epoch": 4.54375,
      "grad_norm": 11.705280303955078,
      "learning_rate": 6.0625e-06,
      "loss": 1.1435,
      "step": 2181
    },
    {
      "epoch": 4.545833333333333,
      "grad_norm": 21.12213706970215,
      "learning_rate": 6.060185185185186e-06,
      "loss": 1.2393,
      "step": 2182
    },
    {
      "epoch": 4.547916666666667,
      "grad_norm": 14.595428466796875,
      "learning_rate": 6.057870370370371e-06,
      "loss": 1.8265,
      "step": 2183
    },
    {
      "epoch": 4.55,
      "grad_norm": 8.190144538879395,
      "learning_rate": 6.055555555555555e-06,
      "loss": 1.181,
      "step": 2184
    },
    {
      "epoch": 4.552083333333333,
      "grad_norm": 36.323822021484375,
      "learning_rate": 6.053240740740741e-06,
      "loss": 0.8926,
      "step": 2185
    },
    {
      "epoch": 4.554166666666667,
      "grad_norm": 9.215431213378906,
      "learning_rate": 6.050925925925927e-06,
      "loss": 1.3423,
      "step": 2186
    },
    {
      "epoch": 4.55625,
      "grad_norm": 10.828312873840332,
      "learning_rate": 6.048611111111111e-06,
      "loss": 0.7398,
      "step": 2187
    },
    {
      "epoch": 4.558333333333334,
      "grad_norm": 24.27911376953125,
      "learning_rate": 6.0462962962962965e-06,
      "loss": 1.2708,
      "step": 2188
    },
    {
      "epoch": 4.560416666666667,
      "grad_norm": 12.340912818908691,
      "learning_rate": 6.0439814814814825e-06,
      "loss": 1.441,
      "step": 2189
    },
    {
      "epoch": 4.5625,
      "grad_norm": 11.55566120147705,
      "learning_rate": 6.041666666666667e-06,
      "loss": 1.0195,
      "step": 2190
    },
    {
      "epoch": 4.564583333333333,
      "grad_norm": 22.33478546142578,
      "learning_rate": 6.039351851851853e-06,
      "loss": 1.7443,
      "step": 2191
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 9.37181282043457,
      "learning_rate": 6.037037037037038e-06,
      "loss": 0.9788,
      "step": 2192
    },
    {
      "epoch": 4.56875,
      "grad_norm": 11.222620964050293,
      "learning_rate": 6.034722222222222e-06,
      "loss": 1.4539,
      "step": 2193
    },
    {
      "epoch": 4.570833333333333,
      "grad_norm": 15.134090423583984,
      "learning_rate": 6.032407407407408e-06,
      "loss": 2.0036,
      "step": 2194
    },
    {
      "epoch": 4.572916666666667,
      "grad_norm": 40.60890579223633,
      "learning_rate": 6.030092592592594e-06,
      "loss": 1.1052,
      "step": 2195
    },
    {
      "epoch": 4.575,
      "grad_norm": 13.025261878967285,
      "learning_rate": 6.027777777777778e-06,
      "loss": 1.5304,
      "step": 2196
    },
    {
      "epoch": 4.577083333333333,
      "grad_norm": 20.02141761779785,
      "learning_rate": 6.025462962962963e-06,
      "loss": 1.846,
      "step": 2197
    },
    {
      "epoch": 4.579166666666667,
      "grad_norm": 19.67865562438965,
      "learning_rate": 6.023148148148149e-06,
      "loss": 1.7972,
      "step": 2198
    },
    {
      "epoch": 4.58125,
      "grad_norm": 5.71108865737915,
      "learning_rate": 6.0208333333333334e-06,
      "loss": 0.8895,
      "step": 2199
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 18.233631134033203,
      "learning_rate": 6.018518518518519e-06,
      "loss": 1.4661,
      "step": 2200
    },
    {
      "epoch": 4.585416666666667,
      "grad_norm": 6.068849086761475,
      "learning_rate": 6.0162037037037045e-06,
      "loss": 0.1896,
      "step": 2201
    },
    {
      "epoch": 4.5875,
      "grad_norm": 33.7081184387207,
      "learning_rate": 6.013888888888889e-06,
      "loss": 0.3621,
      "step": 2202
    },
    {
      "epoch": 4.589583333333334,
      "grad_norm": 33.05038070678711,
      "learning_rate": 6.011574074074075e-06,
      "loss": 1.7003,
      "step": 2203
    },
    {
      "epoch": 4.591666666666667,
      "grad_norm": 29.16761016845703,
      "learning_rate": 6.009259259259261e-06,
      "loss": 1.9178,
      "step": 2204
    },
    {
      "epoch": 4.59375,
      "grad_norm": 47.19743347167969,
      "learning_rate": 6.006944444444445e-06,
      "loss": 0.9768,
      "step": 2205
    },
    {
      "epoch": 4.595833333333333,
      "grad_norm": 13.837318420410156,
      "learning_rate": 6.00462962962963e-06,
      "loss": 1.7558,
      "step": 2206
    },
    {
      "epoch": 4.597916666666666,
      "grad_norm": 12.716615676879883,
      "learning_rate": 6.002314814814816e-06,
      "loss": 1.5892,
      "step": 2207
    },
    {
      "epoch": 4.6,
      "grad_norm": 24.241893768310547,
      "learning_rate": 6e-06,
      "loss": 1.2484,
      "step": 2208
    },
    {
      "epoch": 4.602083333333333,
      "grad_norm": 9.342123031616211,
      "learning_rate": 5.997685185185186e-06,
      "loss": 0.8141,
      "step": 2209
    },
    {
      "epoch": 4.604166666666667,
      "grad_norm": 9.707500457763672,
      "learning_rate": 5.995370370370371e-06,
      "loss": 1.4405,
      "step": 2210
    },
    {
      "epoch": 4.60625,
      "grad_norm": 42.05936813354492,
      "learning_rate": 5.9930555555555554e-06,
      "loss": 2.0593,
      "step": 2211
    },
    {
      "epoch": 4.608333333333333,
      "grad_norm": 11.772245407104492,
      "learning_rate": 5.990740740740741e-06,
      "loss": 1.4775,
      "step": 2212
    },
    {
      "epoch": 4.610416666666667,
      "grad_norm": 7.655596733093262,
      "learning_rate": 5.988425925925926e-06,
      "loss": 1.1146,
      "step": 2213
    },
    {
      "epoch": 4.6125,
      "grad_norm": 26.14360809326172,
      "learning_rate": 5.986111111111112e-06,
      "loss": 2.6322,
      "step": 2214
    },
    {
      "epoch": 4.614583333333333,
      "grad_norm": 7.625472068786621,
      "learning_rate": 5.983796296296297e-06,
      "loss": 1.2733,
      "step": 2215
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 5.073493957519531,
      "learning_rate": 5.981481481481482e-06,
      "loss": 0.9131,
      "step": 2216
    },
    {
      "epoch": 4.61875,
      "grad_norm": 29.669086456298828,
      "learning_rate": 5.979166666666667e-06,
      "loss": 1.6548,
      "step": 2217
    },
    {
      "epoch": 4.620833333333334,
      "grad_norm": 9.724892616271973,
      "learning_rate": 5.976851851851853e-06,
      "loss": 0.9264,
      "step": 2218
    },
    {
      "epoch": 4.622916666666667,
      "grad_norm": 38.93110656738281,
      "learning_rate": 5.974537037037037e-06,
      "loss": 2.2393,
      "step": 2219
    },
    {
      "epoch": 4.625,
      "grad_norm": 7.407297134399414,
      "learning_rate": 5.972222222222222e-06,
      "loss": 1.1448,
      "step": 2220
    },
    {
      "epoch": 4.627083333333333,
      "grad_norm": 10.328550338745117,
      "learning_rate": 5.969907407407408e-06,
      "loss": 1.7928,
      "step": 2221
    },
    {
      "epoch": 4.629166666666666,
      "grad_norm": 9.43832778930664,
      "learning_rate": 5.967592592592592e-06,
      "loss": 1.0676,
      "step": 2222
    },
    {
      "epoch": 4.63125,
      "grad_norm": 17.84616470336914,
      "learning_rate": 5.965277777777778e-06,
      "loss": 0.8886,
      "step": 2223
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 6.543632984161377,
      "learning_rate": 5.962962962962963e-06,
      "loss": 0.4366,
      "step": 2224
    },
    {
      "epoch": 4.635416666666667,
      "grad_norm": 10.922250747680664,
      "learning_rate": 5.9606481481481485e-06,
      "loss": 1.2649,
      "step": 2225
    },
    {
      "epoch": 4.6375,
      "grad_norm": 35.31210708618164,
      "learning_rate": 5.958333333333334e-06,
      "loss": 1.0321,
      "step": 2226
    },
    {
      "epoch": 4.639583333333333,
      "grad_norm": 8.783602714538574,
      "learning_rate": 5.9560185185185195e-06,
      "loss": 1.072,
      "step": 2227
    },
    {
      "epoch": 4.641666666666667,
      "grad_norm": 18.34201431274414,
      "learning_rate": 5.953703703703704e-06,
      "loss": 1.8239,
      "step": 2228
    },
    {
      "epoch": 4.64375,
      "grad_norm": 39.18507385253906,
      "learning_rate": 5.95138888888889e-06,
      "loss": 1.7562,
      "step": 2229
    },
    {
      "epoch": 4.645833333333333,
      "grad_norm": 17.62548828125,
      "learning_rate": 5.949074074074075e-06,
      "loss": 0.7485,
      "step": 2230
    },
    {
      "epoch": 4.647916666666667,
      "grad_norm": 12.517934799194336,
      "learning_rate": 5.946759259259259e-06,
      "loss": 1.7583,
      "step": 2231
    },
    {
      "epoch": 4.65,
      "grad_norm": 14.170804023742676,
      "learning_rate": 5.944444444444445e-06,
      "loss": 0.9107,
      "step": 2232
    },
    {
      "epoch": 4.652083333333334,
      "grad_norm": 15.537534713745117,
      "learning_rate": 5.94212962962963e-06,
      "loss": 2.0603,
      "step": 2233
    },
    {
      "epoch": 4.654166666666667,
      "grad_norm": 11.38555908203125,
      "learning_rate": 5.939814814814815e-06,
      "loss": 1.7745,
      "step": 2234
    },
    {
      "epoch": 4.65625,
      "grad_norm": 54.38972091674805,
      "learning_rate": 5.9375e-06,
      "loss": 0.8988,
      "step": 2235
    },
    {
      "epoch": 4.658333333333333,
      "grad_norm": 6.543935775756836,
      "learning_rate": 5.935185185185186e-06,
      "loss": 0.228,
      "step": 2236
    },
    {
      "epoch": 4.660416666666666,
      "grad_norm": 12.63474178314209,
      "learning_rate": 5.9328703703703705e-06,
      "loss": 1.1205,
      "step": 2237
    },
    {
      "epoch": 4.6625,
      "grad_norm": 34.31620788574219,
      "learning_rate": 5.9305555555555565e-06,
      "loss": 2.2821,
      "step": 2238
    },
    {
      "epoch": 4.664583333333333,
      "grad_norm": 14.0016508102417,
      "learning_rate": 5.9282407407407416e-06,
      "loss": 1.2395,
      "step": 2239
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 27.38361358642578,
      "learning_rate": 5.925925925925926e-06,
      "loss": 1.6953,
      "step": 2240
    },
    {
      "epoch": 4.66875,
      "grad_norm": 7.201634407043457,
      "learning_rate": 5.923611111111112e-06,
      "loss": 1.0538,
      "step": 2241
    },
    {
      "epoch": 4.670833333333333,
      "grad_norm": 7.393752098083496,
      "learning_rate": 5.921296296296297e-06,
      "loss": 0.878,
      "step": 2242
    },
    {
      "epoch": 4.672916666666667,
      "grad_norm": 8.89948844909668,
      "learning_rate": 5.918981481481482e-06,
      "loss": 1.421,
      "step": 2243
    },
    {
      "epoch": 4.675,
      "grad_norm": 16.29271697998047,
      "learning_rate": 5.916666666666667e-06,
      "loss": 1.769,
      "step": 2244
    },
    {
      "epoch": 4.677083333333333,
      "grad_norm": 21.35405921936035,
      "learning_rate": 5.914351851851853e-06,
      "loss": 0.5926,
      "step": 2245
    },
    {
      "epoch": 4.679166666666667,
      "grad_norm": 11.661188125610352,
      "learning_rate": 5.912037037037037e-06,
      "loss": 1.2958,
      "step": 2246
    },
    {
      "epoch": 4.68125,
      "grad_norm": 6.468634128570557,
      "learning_rate": 5.909722222222223e-06,
      "loss": 0.3971,
      "step": 2247
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 9.64465045928955,
      "learning_rate": 5.907407407407408e-06,
      "loss": 1.7584,
      "step": 2248
    },
    {
      "epoch": 4.685416666666667,
      "grad_norm": 27.60015296936035,
      "learning_rate": 5.9050925925925925e-06,
      "loss": 1.8256,
      "step": 2249
    },
    {
      "epoch": 4.6875,
      "grad_norm": 7.774744033813477,
      "learning_rate": 5.9027777777777785e-06,
      "loss": 0.2161,
      "step": 2250
    },
    {
      "epoch": 4.689583333333333,
      "grad_norm": 31.319686889648438,
      "learning_rate": 5.900462962962964e-06,
      "loss": 1.5595,
      "step": 2251
    },
    {
      "epoch": 4.691666666666666,
      "grad_norm": 22.866825103759766,
      "learning_rate": 5.898148148148149e-06,
      "loss": 1.0913,
      "step": 2252
    },
    {
      "epoch": 4.69375,
      "grad_norm": 9.70372486114502,
      "learning_rate": 5.895833333333334e-06,
      "loss": 0.7472,
      "step": 2253
    },
    {
      "epoch": 4.695833333333333,
      "grad_norm": 17.721269607543945,
      "learning_rate": 5.89351851851852e-06,
      "loss": 1.8104,
      "step": 2254
    },
    {
      "epoch": 4.697916666666667,
      "grad_norm": 6.356375217437744,
      "learning_rate": 5.891203703703704e-06,
      "loss": 0.4151,
      "step": 2255
    },
    {
      "epoch": 4.7,
      "grad_norm": 52.65383529663086,
      "learning_rate": 5.88888888888889e-06,
      "loss": 1.4373,
      "step": 2256
    },
    {
      "epoch": 4.702083333333333,
      "grad_norm": 9.020687103271484,
      "learning_rate": 5.886574074074075e-06,
      "loss": 0.3042,
      "step": 2257
    },
    {
      "epoch": 4.704166666666667,
      "grad_norm": 11.793582916259766,
      "learning_rate": 5.884259259259259e-06,
      "loss": 1.4721,
      "step": 2258
    },
    {
      "epoch": 4.70625,
      "grad_norm": 25.919456481933594,
      "learning_rate": 5.881944444444445e-06,
      "loss": 1.8156,
      "step": 2259
    },
    {
      "epoch": 4.708333333333333,
      "grad_norm": 36.42392349243164,
      "learning_rate": 5.8796296296296295e-06,
      "loss": 2.247,
      "step": 2260
    },
    {
      "epoch": 4.710416666666667,
      "grad_norm": 13.128159523010254,
      "learning_rate": 5.877314814814815e-06,
      "loss": 1.5881,
      "step": 2261
    },
    {
      "epoch": 4.7125,
      "grad_norm": 7.8193159103393555,
      "learning_rate": 5.8750000000000005e-06,
      "loss": 0.8997,
      "step": 2262
    },
    {
      "epoch": 4.714583333333334,
      "grad_norm": 8.707308769226074,
      "learning_rate": 5.872685185185185e-06,
      "loss": 1.249,
      "step": 2263
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 17.142169952392578,
      "learning_rate": 5.870370370370371e-06,
      "loss": 1.3477,
      "step": 2264
    },
    {
      "epoch": 4.71875,
      "grad_norm": 11.091864585876465,
      "learning_rate": 5.868055555555557e-06,
      "loss": 1.5414,
      "step": 2265
    },
    {
      "epoch": 4.720833333333333,
      "grad_norm": 7.606781959533691,
      "learning_rate": 5.865740740740741e-06,
      "loss": 0.7573,
      "step": 2266
    },
    {
      "epoch": 4.722916666666666,
      "grad_norm": 16.981189727783203,
      "learning_rate": 5.863425925925926e-06,
      "loss": 1.6061,
      "step": 2267
    },
    {
      "epoch": 4.725,
      "grad_norm": 12.811884880065918,
      "learning_rate": 5.861111111111112e-06,
      "loss": 1.3812,
      "step": 2268
    },
    {
      "epoch": 4.727083333333333,
      "grad_norm": 5.944606781005859,
      "learning_rate": 5.858796296296296e-06,
      "loss": 0.3613,
      "step": 2269
    },
    {
      "epoch": 4.729166666666667,
      "grad_norm": 22.905675888061523,
      "learning_rate": 5.856481481481482e-06,
      "loss": 1.5785,
      "step": 2270
    },
    {
      "epoch": 4.73125,
      "grad_norm": 86.8507308959961,
      "learning_rate": 5.854166666666667e-06,
      "loss": 2.1081,
      "step": 2271
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 29.87213706970215,
      "learning_rate": 5.8518518518518515e-06,
      "loss": 2.1089,
      "step": 2272
    },
    {
      "epoch": 4.735416666666667,
      "grad_norm": 15.838976860046387,
      "learning_rate": 5.849537037037037e-06,
      "loss": 1.2845,
      "step": 2273
    },
    {
      "epoch": 4.7375,
      "grad_norm": 10.022429466247559,
      "learning_rate": 5.847222222222223e-06,
      "loss": 1.6654,
      "step": 2274
    },
    {
      "epoch": 4.739583333333333,
      "grad_norm": 28.775867462158203,
      "learning_rate": 5.844907407407408e-06,
      "loss": 0.8396,
      "step": 2275
    },
    {
      "epoch": 4.741666666666667,
      "grad_norm": 13.106331825256348,
      "learning_rate": 5.842592592592593e-06,
      "loss": 1.6096,
      "step": 2276
    },
    {
      "epoch": 4.74375,
      "grad_norm": 34.24641036987305,
      "learning_rate": 5.840277777777779e-06,
      "loss": 1.1302,
      "step": 2277
    },
    {
      "epoch": 4.745833333333334,
      "grad_norm": 12.516181945800781,
      "learning_rate": 5.837962962962963e-06,
      "loss": 1.0395,
      "step": 2278
    },
    {
      "epoch": 4.747916666666667,
      "grad_norm": 19.340681076049805,
      "learning_rate": 5.835648148148149e-06,
      "loss": 2.0202,
      "step": 2279
    },
    {
      "epoch": 4.75,
      "grad_norm": 7.683032512664795,
      "learning_rate": 5.833333333333334e-06,
      "loss": 1.3342,
      "step": 2280
    },
    {
      "epoch": 4.752083333333333,
      "grad_norm": 15.43045711517334,
      "learning_rate": 5.831018518518519e-06,
      "loss": 1.2973,
      "step": 2281
    },
    {
      "epoch": 4.754166666666666,
      "grad_norm": 7.244968414306641,
      "learning_rate": 5.828703703703704e-06,
      "loss": 1.5394,
      "step": 2282
    },
    {
      "epoch": 4.75625,
      "grad_norm": 29.14324378967285,
      "learning_rate": 5.82638888888889e-06,
      "loss": 1.5656,
      "step": 2283
    },
    {
      "epoch": 4.758333333333333,
      "grad_norm": 16.129308700561523,
      "learning_rate": 5.824074074074074e-06,
      "loss": 0.9553,
      "step": 2284
    },
    {
      "epoch": 4.760416666666667,
      "grad_norm": 47.71052932739258,
      "learning_rate": 5.8217592592592594e-06,
      "loss": 1.1776,
      "step": 2285
    },
    {
      "epoch": 4.7625,
      "grad_norm": 15.718233108520508,
      "learning_rate": 5.819444444444445e-06,
      "loss": 1.6138,
      "step": 2286
    },
    {
      "epoch": 4.764583333333333,
      "grad_norm": 19.69511604309082,
      "learning_rate": 5.81712962962963e-06,
      "loss": 1.7678,
      "step": 2287
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 9.577218055725098,
      "learning_rate": 5.814814814814816e-06,
      "loss": 0.7675,
      "step": 2288
    },
    {
      "epoch": 4.76875,
      "grad_norm": 15.726700782775879,
      "learning_rate": 5.812500000000001e-06,
      "loss": 1.3909,
      "step": 2289
    },
    {
      "epoch": 4.770833333333333,
      "grad_norm": 13.865132331848145,
      "learning_rate": 5.810185185185186e-06,
      "loss": 2.1341,
      "step": 2290
    },
    {
      "epoch": 4.772916666666667,
      "grad_norm": 13.806978225708008,
      "learning_rate": 5.807870370370371e-06,
      "loss": 1.5018,
      "step": 2291
    },
    {
      "epoch": 4.775,
      "grad_norm": 7.905600070953369,
      "learning_rate": 5.805555555555557e-06,
      "loss": 1.0203,
      "step": 2292
    },
    {
      "epoch": 4.777083333333334,
      "grad_norm": 9.512924194335938,
      "learning_rate": 5.803240740740741e-06,
      "loss": 1.0035,
      "step": 2293
    },
    {
      "epoch": 4.779166666666667,
      "grad_norm": 21.212646484375,
      "learning_rate": 5.800925925925926e-06,
      "loss": 1.9243,
      "step": 2294
    },
    {
      "epoch": 4.78125,
      "grad_norm": 7.915952682495117,
      "learning_rate": 5.798611111111112e-06,
      "loss": 0.685,
      "step": 2295
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 6.90902853012085,
      "learning_rate": 5.796296296296296e-06,
      "loss": 1.3466,
      "step": 2296
    },
    {
      "epoch": 4.785416666666666,
      "grad_norm": 7.523875713348389,
      "learning_rate": 5.793981481481482e-06,
      "loss": 1.375,
      "step": 2297
    },
    {
      "epoch": 4.7875,
      "grad_norm": 16.504329681396484,
      "learning_rate": 5.791666666666667e-06,
      "loss": 1.7988,
      "step": 2298
    },
    {
      "epoch": 4.789583333333333,
      "grad_norm": 21.536169052124023,
      "learning_rate": 5.7893518518518525e-06,
      "loss": 2.2441,
      "step": 2299
    },
    {
      "epoch": 4.791666666666667,
      "grad_norm": 14.673602104187012,
      "learning_rate": 5.787037037037038e-06,
      "loss": 1.6171,
      "step": 2300
    },
    {
      "epoch": 4.79375,
      "grad_norm": 25.16899871826172,
      "learning_rate": 5.7847222222222235e-06,
      "loss": 2.002,
      "step": 2301
    },
    {
      "epoch": 4.795833333333333,
      "grad_norm": 17.831256866455078,
      "learning_rate": 5.782407407407408e-06,
      "loss": 1.5943,
      "step": 2302
    },
    {
      "epoch": 4.797916666666667,
      "grad_norm": 14.0972318649292,
      "learning_rate": 5.780092592592593e-06,
      "loss": 1.4627,
      "step": 2303
    },
    {
      "epoch": 4.8,
      "grad_norm": 7.874513626098633,
      "learning_rate": 5.777777777777778e-06,
      "loss": 1.3217,
      "step": 2304
    },
    {
      "epoch": 4.802083333333333,
      "grad_norm": 10.514586448669434,
      "learning_rate": 5.775462962962963e-06,
      "loss": 1.5767,
      "step": 2305
    },
    {
      "epoch": 4.804166666666667,
      "grad_norm": 7.568463325500488,
      "learning_rate": 5.773148148148149e-06,
      "loss": 0.7401,
      "step": 2306
    },
    {
      "epoch": 4.80625,
      "grad_norm": 14.442937850952148,
      "learning_rate": 5.770833333333333e-06,
      "loss": 1.4564,
      "step": 2307
    },
    {
      "epoch": 4.808333333333334,
      "grad_norm": 14.638589859008789,
      "learning_rate": 5.768518518518519e-06,
      "loss": 2.2345,
      "step": 2308
    },
    {
      "epoch": 4.810416666666667,
      "grad_norm": 81.49636840820312,
      "learning_rate": 5.766203703703704e-06,
      "loss": 1.6172,
      "step": 2309
    },
    {
      "epoch": 4.8125,
      "grad_norm": 97.60637664794922,
      "learning_rate": 5.7638888888888886e-06,
      "loss": 1.7664,
      "step": 2310
    },
    {
      "epoch": 4.814583333333333,
      "grad_norm": 18.00996971130371,
      "learning_rate": 5.7615740740740745e-06,
      "loss": 1.8444,
      "step": 2311
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 8.893617630004883,
      "learning_rate": 5.75925925925926e-06,
      "loss": 0.8689,
      "step": 2312
    },
    {
      "epoch": 4.81875,
      "grad_norm": 7.042656421661377,
      "learning_rate": 5.756944444444445e-06,
      "loss": 1.3201,
      "step": 2313
    },
    {
      "epoch": 4.820833333333333,
      "grad_norm": 11.343087196350098,
      "learning_rate": 5.75462962962963e-06,
      "loss": 1.55,
      "step": 2314
    },
    {
      "epoch": 4.822916666666667,
      "grad_norm": 15.716872215270996,
      "learning_rate": 5.752314814814816e-06,
      "loss": 1.6907,
      "step": 2315
    },
    {
      "epoch": 4.825,
      "grad_norm": 11.179835319519043,
      "learning_rate": 5.75e-06,
      "loss": 0.9393,
      "step": 2316
    },
    {
      "epoch": 4.827083333333333,
      "grad_norm": 12.746673583984375,
      "learning_rate": 5.747685185185186e-06,
      "loss": 1.398,
      "step": 2317
    },
    {
      "epoch": 4.829166666666667,
      "grad_norm": 30.870101928710938,
      "learning_rate": 5.745370370370371e-06,
      "loss": 1.9424,
      "step": 2318
    },
    {
      "epoch": 4.83125,
      "grad_norm": 10.106986045837402,
      "learning_rate": 5.743055555555555e-06,
      "loss": 0.8574,
      "step": 2319
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 25.645912170410156,
      "learning_rate": 5.740740740740741e-06,
      "loss": 1.5943,
      "step": 2320
    },
    {
      "epoch": 4.835416666666667,
      "grad_norm": 25.713804244995117,
      "learning_rate": 5.738425925925926e-06,
      "loss": 2.208,
      "step": 2321
    },
    {
      "epoch": 4.8375,
      "grad_norm": 19.684770584106445,
      "learning_rate": 5.7361111111111114e-06,
      "loss": 1.5231,
      "step": 2322
    },
    {
      "epoch": 4.839583333333334,
      "grad_norm": 67.33335876464844,
      "learning_rate": 5.7337962962962965e-06,
      "loss": 1.3391,
      "step": 2323
    },
    {
      "epoch": 4.841666666666667,
      "grad_norm": 15.424273490905762,
      "learning_rate": 5.7314814814814825e-06,
      "loss": 1.3738,
      "step": 2324
    },
    {
      "epoch": 4.84375,
      "grad_norm": 15.149456977844238,
      "learning_rate": 5.729166666666667e-06,
      "loss": 1.488,
      "step": 2325
    },
    {
      "epoch": 4.845833333333333,
      "grad_norm": 10.0455961227417,
      "learning_rate": 5.726851851851853e-06,
      "loss": 0.5305,
      "step": 2326
    },
    {
      "epoch": 4.847916666666666,
      "grad_norm": 6.228156089782715,
      "learning_rate": 5.724537037037038e-06,
      "loss": 0.9217,
      "step": 2327
    },
    {
      "epoch": 4.85,
      "grad_norm": 5.346049785614014,
      "learning_rate": 5.722222222222222e-06,
      "loss": 0.864,
      "step": 2328
    },
    {
      "epoch": 4.852083333333333,
      "grad_norm": 51.145423889160156,
      "learning_rate": 5.719907407407408e-06,
      "loss": 0.6432,
      "step": 2329
    },
    {
      "epoch": 4.854166666666667,
      "grad_norm": 6.943887233734131,
      "learning_rate": 5.717592592592593e-06,
      "loss": 0.2453,
      "step": 2330
    },
    {
      "epoch": 4.85625,
      "grad_norm": 9.23423957824707,
      "learning_rate": 5.715277777777778e-06,
      "loss": 1.0527,
      "step": 2331
    },
    {
      "epoch": 4.858333333333333,
      "grad_norm": 33.84592819213867,
      "learning_rate": 5.712962962962963e-06,
      "loss": 1.3829,
      "step": 2332
    },
    {
      "epoch": 4.860416666666667,
      "grad_norm": 7.897585868835449,
      "learning_rate": 5.710648148148149e-06,
      "loss": 0.8127,
      "step": 2333
    },
    {
      "epoch": 4.8625,
      "grad_norm": 10.424864768981934,
      "learning_rate": 5.7083333333333335e-06,
      "loss": 1.4376,
      "step": 2334
    },
    {
      "epoch": 4.864583333333333,
      "grad_norm": 13.548065185546875,
      "learning_rate": 5.706018518518519e-06,
      "loss": 1.2311,
      "step": 2335
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 10.030242919921875,
      "learning_rate": 5.7037037037037045e-06,
      "loss": 1.5164,
      "step": 2336
    },
    {
      "epoch": 4.86875,
      "grad_norm": 14.866825103759766,
      "learning_rate": 5.701388888888889e-06,
      "loss": 1.1263,
      "step": 2337
    },
    {
      "epoch": 4.870833333333334,
      "grad_norm": 13.002333641052246,
      "learning_rate": 5.699074074074075e-06,
      "loss": 1.6858,
      "step": 2338
    },
    {
      "epoch": 4.872916666666667,
      "grad_norm": 27.418010711669922,
      "learning_rate": 5.69675925925926e-06,
      "loss": 1.7882,
      "step": 2339
    },
    {
      "epoch": 4.875,
      "grad_norm": 46.3333625793457,
      "learning_rate": 5.694444444444445e-06,
      "loss": 2.6883,
      "step": 2340
    },
    {
      "epoch": 4.877083333333333,
      "grad_norm": 15.188676834106445,
      "learning_rate": 5.69212962962963e-06,
      "loss": 1.4465,
      "step": 2341
    },
    {
      "epoch": 4.879166666666666,
      "grad_norm": 35.44849395751953,
      "learning_rate": 5.689814814814816e-06,
      "loss": 3.1897,
      "step": 2342
    },
    {
      "epoch": 4.88125,
      "grad_norm": 10.539538383483887,
      "learning_rate": 5.6875e-06,
      "loss": 1.4646,
      "step": 2343
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 10.22052001953125,
      "learning_rate": 5.685185185185186e-06,
      "loss": 0.9259,
      "step": 2344
    },
    {
      "epoch": 4.885416666666667,
      "grad_norm": 6.3278913497924805,
      "learning_rate": 5.682870370370371e-06,
      "loss": 0.8938,
      "step": 2345
    },
    {
      "epoch": 4.8875,
      "grad_norm": 10.954561233520508,
      "learning_rate": 5.6805555555555555e-06,
      "loss": 0.9119,
      "step": 2346
    },
    {
      "epoch": 4.889583333333333,
      "grad_norm": 8.912076950073242,
      "learning_rate": 5.678240740740741e-06,
      "loss": 1.7227,
      "step": 2347
    },
    {
      "epoch": 4.891666666666667,
      "grad_norm": 21.262964248657227,
      "learning_rate": 5.675925925925926e-06,
      "loss": 1.2999,
      "step": 2348
    },
    {
      "epoch": 4.89375,
      "grad_norm": 17.978883743286133,
      "learning_rate": 5.673611111111112e-06,
      "loss": 0.7958,
      "step": 2349
    },
    {
      "epoch": 4.895833333333333,
      "grad_norm": 26.76020050048828,
      "learning_rate": 5.671296296296297e-06,
      "loss": 1.7356,
      "step": 2350
    },
    {
      "epoch": 4.897916666666667,
      "grad_norm": 9.04733657836914,
      "learning_rate": 5.668981481481482e-06,
      "loss": 0.8756,
      "step": 2351
    },
    {
      "epoch": 4.9,
      "grad_norm": 32.009071350097656,
      "learning_rate": 5.666666666666667e-06,
      "loss": 1.8415,
      "step": 2352
    },
    {
      "epoch": 4.902083333333334,
      "grad_norm": 6.274364471435547,
      "learning_rate": 5.664351851851853e-06,
      "loss": 0.8055,
      "step": 2353
    },
    {
      "epoch": 4.904166666666667,
      "grad_norm": 20.452436447143555,
      "learning_rate": 5.662037037037037e-06,
      "loss": 1.2071,
      "step": 2354
    },
    {
      "epoch": 4.90625,
      "grad_norm": 8.001954078674316,
      "learning_rate": 5.659722222222222e-06,
      "loss": 1.1489,
      "step": 2355
    },
    {
      "epoch": 4.908333333333333,
      "grad_norm": 7.602586269378662,
      "learning_rate": 5.657407407407408e-06,
      "loss": 1.0253,
      "step": 2356
    },
    {
      "epoch": 4.910416666666666,
      "grad_norm": 8.125762939453125,
      "learning_rate": 5.655092592592592e-06,
      "loss": 1.0019,
      "step": 2357
    },
    {
      "epoch": 4.9125,
      "grad_norm": 10.527449607849121,
      "learning_rate": 5.652777777777778e-06,
      "loss": 1.4108,
      "step": 2358
    },
    {
      "epoch": 4.914583333333333,
      "grad_norm": 8.041557312011719,
      "learning_rate": 5.6504629629629634e-06,
      "loss": 1.5385,
      "step": 2359
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 10.581329345703125,
      "learning_rate": 5.6481481481481485e-06,
      "loss": 1.0021,
      "step": 2360
    },
    {
      "epoch": 4.91875,
      "grad_norm": 26.489877700805664,
      "learning_rate": 5.645833333333334e-06,
      "loss": 1.4974,
      "step": 2361
    },
    {
      "epoch": 4.920833333333333,
      "grad_norm": 13.124752044677734,
      "learning_rate": 5.6435185185185196e-06,
      "loss": 1.3658,
      "step": 2362
    },
    {
      "epoch": 4.922916666666667,
      "grad_norm": 9.217416763305664,
      "learning_rate": 5.641203703703704e-06,
      "loss": 0.8651,
      "step": 2363
    },
    {
      "epoch": 4.925,
      "grad_norm": 38.12167739868164,
      "learning_rate": 5.638888888888889e-06,
      "loss": 1.6869,
      "step": 2364
    },
    {
      "epoch": 4.927083333333333,
      "grad_norm": 24.901546478271484,
      "learning_rate": 5.636574074074075e-06,
      "loss": 1.6989,
      "step": 2365
    },
    {
      "epoch": 4.929166666666667,
      "grad_norm": 109.31017303466797,
      "learning_rate": 5.634259259259259e-06,
      "loss": 2.2309,
      "step": 2366
    },
    {
      "epoch": 4.93125,
      "grad_norm": 11.152612686157227,
      "learning_rate": 5.631944444444445e-06,
      "loss": 1.9034,
      "step": 2367
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 6.147723197937012,
      "learning_rate": 5.62962962962963e-06,
      "loss": 0.3541,
      "step": 2368
    },
    {
      "epoch": 4.935416666666667,
      "grad_norm": 14.01083755493164,
      "learning_rate": 5.627314814814815e-06,
      "loss": 0.9868,
      "step": 2369
    },
    {
      "epoch": 4.9375,
      "grad_norm": 31.712682723999023,
      "learning_rate": 5.625e-06,
      "loss": 1.577,
      "step": 2370
    },
    {
      "epoch": 4.939583333333333,
      "grad_norm": 14.212387084960938,
      "learning_rate": 5.622685185185186e-06,
      "loss": 2.2646,
      "step": 2371
    },
    {
      "epoch": 4.941666666666666,
      "grad_norm": 28.37847137451172,
      "learning_rate": 5.6203703703703705e-06,
      "loss": 0.5357,
      "step": 2372
    },
    {
      "epoch": 4.94375,
      "grad_norm": 8.13167667388916,
      "learning_rate": 5.618055555555556e-06,
      "loss": 0.8852,
      "step": 2373
    },
    {
      "epoch": 4.945833333333333,
      "grad_norm": 10.938289642333984,
      "learning_rate": 5.615740740740742e-06,
      "loss": 0.4941,
      "step": 2374
    },
    {
      "epoch": 4.947916666666667,
      "grad_norm": 17.04151153564453,
      "learning_rate": 5.613425925925926e-06,
      "loss": 0.3055,
      "step": 2375
    },
    {
      "epoch": 4.95,
      "grad_norm": 11.342246055603027,
      "learning_rate": 5.611111111111112e-06,
      "loss": 1.5326,
      "step": 2376
    },
    {
      "epoch": 4.952083333333333,
      "grad_norm": 16.057992935180664,
      "learning_rate": 5.608796296296297e-06,
      "loss": 1.656,
      "step": 2377
    },
    {
      "epoch": 4.954166666666667,
      "grad_norm": 16.466754913330078,
      "learning_rate": 5.606481481481482e-06,
      "loss": 1.3512,
      "step": 2378
    },
    {
      "epoch": 4.95625,
      "grad_norm": 13.321749687194824,
      "learning_rate": 5.604166666666667e-06,
      "loss": 1.3687,
      "step": 2379
    },
    {
      "epoch": 4.958333333333333,
      "grad_norm": 15.785557746887207,
      "learning_rate": 5.601851851851853e-06,
      "loss": 0.9262,
      "step": 2380
    },
    {
      "epoch": 4.960416666666667,
      "grad_norm": 7.6308274269104,
      "learning_rate": 5.599537037037037e-06,
      "loss": 0.7646,
      "step": 2381
    },
    {
      "epoch": 4.9625,
      "grad_norm": 13.027934074401855,
      "learning_rate": 5.597222222222222e-06,
      "loss": 1.1381,
      "step": 2382
    },
    {
      "epoch": 4.964583333333334,
      "grad_norm": 9.31263256072998,
      "learning_rate": 5.594907407407408e-06,
      "loss": 0.9378,
      "step": 2383
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 22.762937545776367,
      "learning_rate": 5.5925925925925926e-06,
      "loss": 1.4494,
      "step": 2384
    },
    {
      "epoch": 4.96875,
      "grad_norm": 34.870033264160156,
      "learning_rate": 5.5902777777777785e-06,
      "loss": 0.8939,
      "step": 2385
    },
    {
      "epoch": 4.970833333333333,
      "grad_norm": 26.926292419433594,
      "learning_rate": 5.587962962962964e-06,
      "loss": 2.0426,
      "step": 2386
    },
    {
      "epoch": 4.972916666666666,
      "grad_norm": 17.4298152923584,
      "learning_rate": 5.585648148148149e-06,
      "loss": 1.8976,
      "step": 2387
    },
    {
      "epoch": 4.975,
      "grad_norm": 29.13998794555664,
      "learning_rate": 5.583333333333334e-06,
      "loss": 1.7397,
      "step": 2388
    },
    {
      "epoch": 4.977083333333333,
      "grad_norm": 56.630123138427734,
      "learning_rate": 5.58101851851852e-06,
      "loss": 1.6691,
      "step": 2389
    },
    {
      "epoch": 4.979166666666667,
      "grad_norm": 10.118419647216797,
      "learning_rate": 5.578703703703704e-06,
      "loss": 1.4011,
      "step": 2390
    },
    {
      "epoch": 4.98125,
      "grad_norm": 33.375431060791016,
      "learning_rate": 5.576388888888889e-06,
      "loss": 2.0609,
      "step": 2391
    },
    {
      "epoch": 4.983333333333333,
      "grad_norm": 20.910524368286133,
      "learning_rate": 5.574074074074075e-06,
      "loss": 1.5349,
      "step": 2392
    },
    {
      "epoch": 4.985416666666667,
      "grad_norm": 13.062352180480957,
      "learning_rate": 5.571759259259259e-06,
      "loss": 0.7223,
      "step": 2393
    },
    {
      "epoch": 4.9875,
      "grad_norm": 40.221763610839844,
      "learning_rate": 5.569444444444445e-06,
      "loss": 0.6983,
      "step": 2394
    },
    {
      "epoch": 4.989583333333333,
      "grad_norm": 51.11137390136719,
      "learning_rate": 5.5671296296296295e-06,
      "loss": 1.5431,
      "step": 2395
    },
    {
      "epoch": 4.991666666666667,
      "grad_norm": 13.707793235778809,
      "learning_rate": 5.5648148148148154e-06,
      "loss": 1.5467,
      "step": 2396
    },
    {
      "epoch": 4.99375,
      "grad_norm": 9.283713340759277,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 0.8618,
      "step": 2397
    },
    {
      "epoch": 4.995833333333334,
      "grad_norm": 24.680133819580078,
      "learning_rate": 5.560185185185185e-06,
      "loss": 1.3869,
      "step": 2398
    },
    {
      "epoch": 4.997916666666667,
      "grad_norm": 20.97077751159668,
      "learning_rate": 5.557870370370371e-06,
      "loss": 1.6323,
      "step": 2399
    },
    {
      "epoch": 5.0,
      "grad_norm": 9.042167663574219,
      "learning_rate": 5.555555555555557e-06,
      "loss": 0.9397,
      "step": 2400
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.5,
      "eval_f1": 0.4330532693557903,
      "eval_loss": 1.4849778413772583,
      "eval_runtime": 34.4389,
      "eval_samples_per_second": 5.227,
      "eval_steps_per_second": 2.613,
      "step": 2400
    },
    {
      "epoch": 5.002083333333333,
      "grad_norm": 26.091075897216797,
      "learning_rate": 5.553240740740741e-06,
      "loss": 1.9526,
      "step": 2401
    },
    {
      "epoch": 5.004166666666666,
      "grad_norm": 39.759647369384766,
      "learning_rate": 5.550925925925926e-06,
      "loss": 0.7437,
      "step": 2402
    },
    {
      "epoch": 5.00625,
      "grad_norm": 29.354188919067383,
      "learning_rate": 5.548611111111112e-06,
      "loss": 1.7405,
      "step": 2403
    },
    {
      "epoch": 5.008333333333334,
      "grad_norm": 8.202302932739258,
      "learning_rate": 5.546296296296296e-06,
      "loss": 1.8744,
      "step": 2404
    },
    {
      "epoch": 5.010416666666667,
      "grad_norm": 12.671952247619629,
      "learning_rate": 5.543981481481482e-06,
      "loss": 1.58,
      "step": 2405
    },
    {
      "epoch": 5.0125,
      "grad_norm": 10.079906463623047,
      "learning_rate": 5.541666666666667e-06,
      "loss": 0.945,
      "step": 2406
    },
    {
      "epoch": 5.014583333333333,
      "grad_norm": 10.342391967773438,
      "learning_rate": 5.5393518518518515e-06,
      "loss": 0.5365,
      "step": 2407
    },
    {
      "epoch": 5.016666666666667,
      "grad_norm": 41.5678596496582,
      "learning_rate": 5.5370370370370374e-06,
      "loss": 1.2975,
      "step": 2408
    },
    {
      "epoch": 5.01875,
      "grad_norm": 15.391339302062988,
      "learning_rate": 5.534722222222223e-06,
      "loss": 0.7858,
      "step": 2409
    },
    {
      "epoch": 5.020833333333333,
      "grad_norm": 10.709272384643555,
      "learning_rate": 5.532407407407408e-06,
      "loss": 1.6697,
      "step": 2410
    },
    {
      "epoch": 5.022916666666666,
      "grad_norm": 27.416227340698242,
      "learning_rate": 5.530092592592593e-06,
      "loss": 1.5024,
      "step": 2411
    },
    {
      "epoch": 5.025,
      "grad_norm": 26.855634689331055,
      "learning_rate": 5.527777777777779e-06,
      "loss": 1.7992,
      "step": 2412
    },
    {
      "epoch": 5.027083333333334,
      "grad_norm": 14.220117568969727,
      "learning_rate": 5.525462962962963e-06,
      "loss": 1.5292,
      "step": 2413
    },
    {
      "epoch": 5.029166666666667,
      "grad_norm": 13.153670310974121,
      "learning_rate": 5.523148148148149e-06,
      "loss": 1.8976,
      "step": 2414
    },
    {
      "epoch": 5.03125,
      "grad_norm": 33.35939025878906,
      "learning_rate": 5.520833333333334e-06,
      "loss": 1.7269,
      "step": 2415
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 5.91060733795166,
      "learning_rate": 5.518518518518518e-06,
      "loss": 0.3449,
      "step": 2416
    },
    {
      "epoch": 5.035416666666666,
      "grad_norm": 19.195777893066406,
      "learning_rate": 5.516203703703704e-06,
      "loss": 0.2587,
      "step": 2417
    },
    {
      "epoch": 5.0375,
      "grad_norm": 23.745094299316406,
      "learning_rate": 5.51388888888889e-06,
      "loss": 0.8166,
      "step": 2418
    },
    {
      "epoch": 5.039583333333334,
      "grad_norm": 7.060279846191406,
      "learning_rate": 5.511574074074074e-06,
      "loss": 0.4125,
      "step": 2419
    },
    {
      "epoch": 5.041666666666667,
      "grad_norm": 6.475322246551514,
      "learning_rate": 5.5092592592592595e-06,
      "loss": 0.9592,
      "step": 2420
    },
    {
      "epoch": 5.04375,
      "grad_norm": 21.35100746154785,
      "learning_rate": 5.506944444444445e-06,
      "loss": 1.3999,
      "step": 2421
    },
    {
      "epoch": 5.045833333333333,
      "grad_norm": 14.68435001373291,
      "learning_rate": 5.50462962962963e-06,
      "loss": 0.999,
      "step": 2422
    },
    {
      "epoch": 5.047916666666667,
      "grad_norm": 63.78774642944336,
      "learning_rate": 5.502314814814816e-06,
      "loss": 1.6838,
      "step": 2423
    },
    {
      "epoch": 5.05,
      "grad_norm": 11.297195434570312,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.8923,
      "step": 2424
    },
    {
      "epoch": 5.052083333333333,
      "grad_norm": 16.60530662536621,
      "learning_rate": 5.497685185185185e-06,
      "loss": 1.263,
      "step": 2425
    },
    {
      "epoch": 5.054166666666666,
      "grad_norm": 29.022253036499023,
      "learning_rate": 5.495370370370371e-06,
      "loss": 1.058,
      "step": 2426
    },
    {
      "epoch": 5.05625,
      "grad_norm": 14.912104606628418,
      "learning_rate": 5.493055555555557e-06,
      "loss": 0.7661,
      "step": 2427
    },
    {
      "epoch": 5.058333333333334,
      "grad_norm": 17.881969451904297,
      "learning_rate": 5.490740740740741e-06,
      "loss": 1.0335,
      "step": 2428
    },
    {
      "epoch": 5.060416666666667,
      "grad_norm": 32.352943420410156,
      "learning_rate": 5.488425925925926e-06,
      "loss": 2.2538,
      "step": 2429
    },
    {
      "epoch": 5.0625,
      "grad_norm": 10.760896682739258,
      "learning_rate": 5.486111111111112e-06,
      "loss": 1.414,
      "step": 2430
    },
    {
      "epoch": 5.064583333333333,
      "grad_norm": 11.862736701965332,
      "learning_rate": 5.483796296296296e-06,
      "loss": 0.7498,
      "step": 2431
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 10.313759803771973,
      "learning_rate": 5.481481481481482e-06,
      "loss": 1.4528,
      "step": 2432
    },
    {
      "epoch": 5.06875,
      "grad_norm": 8.958220481872559,
      "learning_rate": 5.4791666666666674e-06,
      "loss": 1.0263,
      "step": 2433
    },
    {
      "epoch": 5.070833333333334,
      "grad_norm": 23.641094207763672,
      "learning_rate": 5.476851851851852e-06,
      "loss": 1.784,
      "step": 2434
    },
    {
      "epoch": 5.072916666666667,
      "grad_norm": 11.32951545715332,
      "learning_rate": 5.474537037037038e-06,
      "loss": 0.8568,
      "step": 2435
    },
    {
      "epoch": 5.075,
      "grad_norm": 11.861825942993164,
      "learning_rate": 5.4722222222222236e-06,
      "loss": 0.9036,
      "step": 2436
    },
    {
      "epoch": 5.077083333333333,
      "grad_norm": 9.613275527954102,
      "learning_rate": 5.469907407407408e-06,
      "loss": 0.8583,
      "step": 2437
    },
    {
      "epoch": 5.079166666666667,
      "grad_norm": 7.559279918670654,
      "learning_rate": 5.467592592592593e-06,
      "loss": 0.7011,
      "step": 2438
    },
    {
      "epoch": 5.08125,
      "grad_norm": 6.092142105102539,
      "learning_rate": 5.465277777777778e-06,
      "loss": 0.2197,
      "step": 2439
    },
    {
      "epoch": 5.083333333333333,
      "grad_norm": 20.319446563720703,
      "learning_rate": 5.462962962962963e-06,
      "loss": 1.3178,
      "step": 2440
    },
    {
      "epoch": 5.085416666666666,
      "grad_norm": 13.827178955078125,
      "learning_rate": 5.460648148148149e-06,
      "loss": 1.4041,
      "step": 2441
    },
    {
      "epoch": 5.0875,
      "grad_norm": 19.29379653930664,
      "learning_rate": 5.458333333333333e-06,
      "loss": 1.4844,
      "step": 2442
    },
    {
      "epoch": 5.089583333333334,
      "grad_norm": 11.881185531616211,
      "learning_rate": 5.456018518518518e-06,
      "loss": 1.1068,
      "step": 2443
    },
    {
      "epoch": 5.091666666666667,
      "grad_norm": 42.76203536987305,
      "learning_rate": 5.453703703703704e-06,
      "loss": 1.1562,
      "step": 2444
    },
    {
      "epoch": 5.09375,
      "grad_norm": 25.44115447998047,
      "learning_rate": 5.451388888888889e-06,
      "loss": 1.4791,
      "step": 2445
    },
    {
      "epoch": 5.095833333333333,
      "grad_norm": 25.718631744384766,
      "learning_rate": 5.4490740740740745e-06,
      "loss": 1.5594,
      "step": 2446
    },
    {
      "epoch": 5.097916666666666,
      "grad_norm": 7.263510227203369,
      "learning_rate": 5.44675925925926e-06,
      "loss": 0.8023,
      "step": 2447
    },
    {
      "epoch": 5.1,
      "grad_norm": 18.23914909362793,
      "learning_rate": 5.444444444444445e-06,
      "loss": 1.1497,
      "step": 2448
    },
    {
      "epoch": 5.102083333333334,
      "grad_norm": 6.321397304534912,
      "learning_rate": 5.44212962962963e-06,
      "loss": 0.8616,
      "step": 2449
    },
    {
      "epoch": 5.104166666666667,
      "grad_norm": 6.886681079864502,
      "learning_rate": 5.439814814814816e-06,
      "loss": 1.2461,
      "step": 2450
    },
    {
      "epoch": 5.10625,
      "grad_norm": 12.10020637512207,
      "learning_rate": 5.4375e-06,
      "loss": 0.7966,
      "step": 2451
    },
    {
      "epoch": 5.108333333333333,
      "grad_norm": 12.245318412780762,
      "learning_rate": 5.435185185185186e-06,
      "loss": 1.0821,
      "step": 2452
    },
    {
      "epoch": 5.110416666666667,
      "grad_norm": 20.527374267578125,
      "learning_rate": 5.432870370370371e-06,
      "loss": 1.4157,
      "step": 2453
    },
    {
      "epoch": 5.1125,
      "grad_norm": 25.506086349487305,
      "learning_rate": 5.430555555555555e-06,
      "loss": 2.1344,
      "step": 2454
    },
    {
      "epoch": 5.114583333333333,
      "grad_norm": 15.243223190307617,
      "learning_rate": 5.428240740740741e-06,
      "loss": 1.4808,
      "step": 2455
    },
    {
      "epoch": 5.116666666666666,
      "grad_norm": 11.450864791870117,
      "learning_rate": 5.425925925925926e-06,
      "loss": 1.1134,
      "step": 2456
    },
    {
      "epoch": 5.11875,
      "grad_norm": 9.461973190307617,
      "learning_rate": 5.4236111111111115e-06,
      "loss": 0.7933,
      "step": 2457
    },
    {
      "epoch": 5.120833333333334,
      "grad_norm": 33.56102752685547,
      "learning_rate": 5.4212962962962966e-06,
      "loss": 1.6235,
      "step": 2458
    },
    {
      "epoch": 5.122916666666667,
      "grad_norm": 17.07001304626465,
      "learning_rate": 5.4189814814814825e-06,
      "loss": 1.2365,
      "step": 2459
    },
    {
      "epoch": 5.125,
      "grad_norm": 10.180513381958008,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.8935,
      "step": 2460
    },
    {
      "epoch": 5.127083333333333,
      "grad_norm": 15.869024276733398,
      "learning_rate": 5.414351851851853e-06,
      "loss": 1.4071,
      "step": 2461
    },
    {
      "epoch": 5.129166666666666,
      "grad_norm": 14.011829376220703,
      "learning_rate": 5.412037037037038e-06,
      "loss": 1.4662,
      "step": 2462
    },
    {
      "epoch": 5.13125,
      "grad_norm": 8.699251174926758,
      "learning_rate": 5.409722222222222e-06,
      "loss": 0.8727,
      "step": 2463
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 16.033935546875,
      "learning_rate": 5.407407407407408e-06,
      "loss": 1.0192,
      "step": 2464
    },
    {
      "epoch": 5.135416666666667,
      "grad_norm": 13.71031665802002,
      "learning_rate": 5.405092592592593e-06,
      "loss": 1.3509,
      "step": 2465
    },
    {
      "epoch": 5.1375,
      "grad_norm": 12.176542282104492,
      "learning_rate": 5.402777777777778e-06,
      "loss": 1.1749,
      "step": 2466
    },
    {
      "epoch": 5.139583333333333,
      "grad_norm": 21.378841400146484,
      "learning_rate": 5.400462962962963e-06,
      "loss": 0.6393,
      "step": 2467
    },
    {
      "epoch": 5.141666666666667,
      "grad_norm": 9.353116035461426,
      "learning_rate": 5.398148148148149e-06,
      "loss": 1.5605,
      "step": 2468
    },
    {
      "epoch": 5.14375,
      "grad_norm": 24.161331176757812,
      "learning_rate": 5.3958333333333335e-06,
      "loss": 1.78,
      "step": 2469
    },
    {
      "epoch": 5.145833333333333,
      "grad_norm": 9.648232460021973,
      "learning_rate": 5.3935185185185194e-06,
      "loss": 0.5007,
      "step": 2470
    },
    {
      "epoch": 5.147916666666666,
      "grad_norm": 79.54261016845703,
      "learning_rate": 5.3912037037037045e-06,
      "loss": 1.4221,
      "step": 2471
    },
    {
      "epoch": 5.15,
      "grad_norm": 58.82863235473633,
      "learning_rate": 5.388888888888889e-06,
      "loss": 2.0712,
      "step": 2472
    },
    {
      "epoch": 5.152083333333334,
      "grad_norm": 8.43664836883545,
      "learning_rate": 5.386574074074075e-06,
      "loss": 1.3865,
      "step": 2473
    },
    {
      "epoch": 5.154166666666667,
      "grad_norm": 25.168331146240234,
      "learning_rate": 5.38425925925926e-06,
      "loss": 1.5566,
      "step": 2474
    },
    {
      "epoch": 5.15625,
      "grad_norm": 39.78693771362305,
      "learning_rate": 5.381944444444445e-06,
      "loss": 1.0296,
      "step": 2475
    },
    {
      "epoch": 5.158333333333333,
      "grad_norm": 9.126579284667969,
      "learning_rate": 5.37962962962963e-06,
      "loss": 1.3071,
      "step": 2476
    },
    {
      "epoch": 5.160416666666666,
      "grad_norm": 17.439895629882812,
      "learning_rate": 5.377314814814816e-06,
      "loss": 1.0322,
      "step": 2477
    },
    {
      "epoch": 5.1625,
      "grad_norm": 26.58399200439453,
      "learning_rate": 5.375e-06,
      "loss": 1.0244,
      "step": 2478
    },
    {
      "epoch": 5.164583333333334,
      "grad_norm": 12.943514823913574,
      "learning_rate": 5.372685185185186e-06,
      "loss": 1.4216,
      "step": 2479
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 23.48861312866211,
      "learning_rate": 5.370370370370371e-06,
      "loss": 1.5014,
      "step": 2480
    },
    {
      "epoch": 5.16875,
      "grad_norm": 8.230313301086426,
      "learning_rate": 5.3680555555555555e-06,
      "loss": 1.3663,
      "step": 2481
    },
    {
      "epoch": 5.170833333333333,
      "grad_norm": 34.2312126159668,
      "learning_rate": 5.3657407407407414e-06,
      "loss": 0.7879,
      "step": 2482
    },
    {
      "epoch": 5.172916666666667,
      "grad_norm": 14.253656387329102,
      "learning_rate": 5.363425925925926e-06,
      "loss": 1.5999,
      "step": 2483
    },
    {
      "epoch": 5.175,
      "grad_norm": 12.177654266357422,
      "learning_rate": 5.361111111111112e-06,
      "loss": 1.4034,
      "step": 2484
    },
    {
      "epoch": 5.177083333333333,
      "grad_norm": 31.353862762451172,
      "learning_rate": 5.358796296296297e-06,
      "loss": 1.9043,
      "step": 2485
    },
    {
      "epoch": 5.179166666666666,
      "grad_norm": 5.459516525268555,
      "learning_rate": 5.356481481481481e-06,
      "loss": 0.1699,
      "step": 2486
    },
    {
      "epoch": 5.18125,
      "grad_norm": 72.71556091308594,
      "learning_rate": 5.354166666666667e-06,
      "loss": 2.5073,
      "step": 2487
    },
    {
      "epoch": 5.183333333333334,
      "grad_norm": 10.071552276611328,
      "learning_rate": 5.351851851851853e-06,
      "loss": 1.0763,
      "step": 2488
    },
    {
      "epoch": 5.185416666666667,
      "grad_norm": 38.19832229614258,
      "learning_rate": 5.349537037037037e-06,
      "loss": 1.1459,
      "step": 2489
    },
    {
      "epoch": 5.1875,
      "grad_norm": 70.68492889404297,
      "learning_rate": 5.347222222222222e-06,
      "loss": 1.6339,
      "step": 2490
    },
    {
      "epoch": 5.189583333333333,
      "grad_norm": 38.12232971191406,
      "learning_rate": 5.344907407407408e-06,
      "loss": 1.0339,
      "step": 2491
    },
    {
      "epoch": 5.191666666666666,
      "grad_norm": 4.450287342071533,
      "learning_rate": 5.342592592592592e-06,
      "loss": 0.7976,
      "step": 2492
    },
    {
      "epoch": 5.19375,
      "grad_norm": 8.957085609436035,
      "learning_rate": 5.340277777777778e-06,
      "loss": 1.0431,
      "step": 2493
    },
    {
      "epoch": 5.195833333333334,
      "grad_norm": 6.344326019287109,
      "learning_rate": 5.3379629629629635e-06,
      "loss": 0.8713,
      "step": 2494
    },
    {
      "epoch": 5.197916666666667,
      "grad_norm": 20.879894256591797,
      "learning_rate": 5.335648148148148e-06,
      "loss": 1.9762,
      "step": 2495
    },
    {
      "epoch": 5.2,
      "grad_norm": 21.20952606201172,
      "learning_rate": 5.333333333333334e-06,
      "loss": 1.8318,
      "step": 2496
    },
    {
      "epoch": 5.202083333333333,
      "grad_norm": 10.267348289489746,
      "learning_rate": 5.33101851851852e-06,
      "loss": 0.9415,
      "step": 2497
    },
    {
      "epoch": 5.204166666666667,
      "grad_norm": 12.220629692077637,
      "learning_rate": 5.328703703703704e-06,
      "loss": 1.4262,
      "step": 2498
    },
    {
      "epoch": 5.20625,
      "grad_norm": 13.0584716796875,
      "learning_rate": 5.326388888888889e-06,
      "loss": 1.0533,
      "step": 2499
    },
    {
      "epoch": 5.208333333333333,
      "grad_norm": 18.450769424438477,
      "learning_rate": 5.324074074074075e-06,
      "loss": 1.831,
      "step": 2500
    },
    {
      "epoch": 5.210416666666666,
      "grad_norm": 35.1368522644043,
      "learning_rate": 5.321759259259259e-06,
      "loss": 1.5358,
      "step": 2501
    },
    {
      "epoch": 5.2125,
      "grad_norm": 14.140311241149902,
      "learning_rate": 5.319444444444445e-06,
      "loss": 1.7264,
      "step": 2502
    },
    {
      "epoch": 5.214583333333334,
      "grad_norm": 7.442519664764404,
      "learning_rate": 5.31712962962963e-06,
      "loss": 0.6507,
      "step": 2503
    },
    {
      "epoch": 5.216666666666667,
      "grad_norm": 9.479486465454102,
      "learning_rate": 5.314814814814815e-06,
      "loss": 1.4023,
      "step": 2504
    },
    {
      "epoch": 5.21875,
      "grad_norm": 13.397074699401855,
      "learning_rate": 5.3125e-06,
      "loss": 1.7714,
      "step": 2505
    },
    {
      "epoch": 5.220833333333333,
      "grad_norm": 42.99559783935547,
      "learning_rate": 5.310185185185186e-06,
      "loss": 1.7553,
      "step": 2506
    },
    {
      "epoch": 5.222916666666666,
      "grad_norm": 59.17335510253906,
      "learning_rate": 5.307870370370371e-06,
      "loss": 1.6052,
      "step": 2507
    },
    {
      "epoch": 5.225,
      "grad_norm": 10.893010139465332,
      "learning_rate": 5.305555555555556e-06,
      "loss": 1.5232,
      "step": 2508
    },
    {
      "epoch": 5.227083333333334,
      "grad_norm": 52.66289520263672,
      "learning_rate": 5.303240740740742e-06,
      "loss": 2.4586,
      "step": 2509
    },
    {
      "epoch": 5.229166666666667,
      "grad_norm": 29.45952606201172,
      "learning_rate": 5.300925925925926e-06,
      "loss": 2.6768,
      "step": 2510
    },
    {
      "epoch": 5.23125,
      "grad_norm": 47.20001983642578,
      "learning_rate": 5.298611111111112e-06,
      "loss": 0.7865,
      "step": 2511
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 14.267560958862305,
      "learning_rate": 5.296296296296297e-06,
      "loss": 1.12,
      "step": 2512
    },
    {
      "epoch": 5.235416666666667,
      "grad_norm": 5.424026966094971,
      "learning_rate": 5.293981481481482e-06,
      "loss": 0.3084,
      "step": 2513
    },
    {
      "epoch": 5.2375,
      "grad_norm": 8.76870346069336,
      "learning_rate": 5.291666666666667e-06,
      "loss": 0.8559,
      "step": 2514
    },
    {
      "epoch": 5.239583333333333,
      "grad_norm": 6.224038600921631,
      "learning_rate": 5.289351851851853e-06,
      "loss": 0.7964,
      "step": 2515
    },
    {
      "epoch": 5.241666666666666,
      "grad_norm": 20.032833099365234,
      "learning_rate": 5.287037037037037e-06,
      "loss": 1.4278,
      "step": 2516
    },
    {
      "epoch": 5.24375,
      "grad_norm": 79.70973205566406,
      "learning_rate": 5.284722222222222e-06,
      "loss": 1.8396,
      "step": 2517
    },
    {
      "epoch": 5.245833333333334,
      "grad_norm": 12.517065048217773,
      "learning_rate": 5.282407407407408e-06,
      "loss": 1.4431,
      "step": 2518
    },
    {
      "epoch": 5.247916666666667,
      "grad_norm": 20.957355499267578,
      "learning_rate": 5.280092592592593e-06,
      "loss": 1.0039,
      "step": 2519
    },
    {
      "epoch": 5.25,
      "grad_norm": 16.558067321777344,
      "learning_rate": 5.2777777777777785e-06,
      "loss": 0.8827,
      "step": 2520
    },
    {
      "epoch": 5.252083333333333,
      "grad_norm": 22.765104293823242,
      "learning_rate": 5.275462962962964e-06,
      "loss": 2.2633,
      "step": 2521
    },
    {
      "epoch": 5.254166666666666,
      "grad_norm": 5.330506324768066,
      "learning_rate": 5.273148148148149e-06,
      "loss": 0.2957,
      "step": 2522
    },
    {
      "epoch": 5.25625,
      "grad_norm": 56.43267822265625,
      "learning_rate": 5.270833333333334e-06,
      "loss": 1.2056,
      "step": 2523
    },
    {
      "epoch": 5.258333333333334,
      "grad_norm": 10.009536743164062,
      "learning_rate": 5.26851851851852e-06,
      "loss": 1.1831,
      "step": 2524
    },
    {
      "epoch": 5.260416666666667,
      "grad_norm": 103.74614715576172,
      "learning_rate": 5.266203703703704e-06,
      "loss": 1.4481,
      "step": 2525
    },
    {
      "epoch": 5.2625,
      "grad_norm": 10.828999519348145,
      "learning_rate": 5.263888888888889e-06,
      "loss": 0.7591,
      "step": 2526
    },
    {
      "epoch": 5.264583333333333,
      "grad_norm": 38.7667121887207,
      "learning_rate": 5.261574074074075e-06,
      "loss": 1.6778,
      "step": 2527
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 18.71773910522461,
      "learning_rate": 5.259259259259259e-06,
      "loss": 1.1885,
      "step": 2528
    },
    {
      "epoch": 5.26875,
      "grad_norm": 16.332294464111328,
      "learning_rate": 5.256944444444445e-06,
      "loss": 1.474,
      "step": 2529
    },
    {
      "epoch": 5.270833333333333,
      "grad_norm": 65.38679504394531,
      "learning_rate": 5.2546296296296295e-06,
      "loss": 0.9595,
      "step": 2530
    },
    {
      "epoch": 5.272916666666666,
      "grad_norm": 13.99173355102539,
      "learning_rate": 5.2523148148148155e-06,
      "loss": 1.1804,
      "step": 2531
    },
    {
      "epoch": 5.275,
      "grad_norm": 11.387557029724121,
      "learning_rate": 5.2500000000000006e-06,
      "loss": 0.816,
      "step": 2532
    },
    {
      "epoch": 5.277083333333334,
      "grad_norm": 7.8638081550598145,
      "learning_rate": 5.247685185185185e-06,
      "loss": 0.6096,
      "step": 2533
    },
    {
      "epoch": 5.279166666666667,
      "grad_norm": 5.7428364753723145,
      "learning_rate": 5.245370370370371e-06,
      "loss": 1.0545,
      "step": 2534
    },
    {
      "epoch": 5.28125,
      "grad_norm": 10.713848114013672,
      "learning_rate": 5.243055555555556e-06,
      "loss": 0.7497,
      "step": 2535
    },
    {
      "epoch": 5.283333333333333,
      "grad_norm": 19.816160202026367,
      "learning_rate": 5.240740740740741e-06,
      "loss": 1.7199,
      "step": 2536
    },
    {
      "epoch": 5.285416666666666,
      "grad_norm": 28.456003189086914,
      "learning_rate": 5.238425925925926e-06,
      "loss": 0.4331,
      "step": 2537
    },
    {
      "epoch": 5.2875,
      "grad_norm": 25.57274627685547,
      "learning_rate": 5.236111111111112e-06,
      "loss": 1.5148,
      "step": 2538
    },
    {
      "epoch": 5.289583333333334,
      "grad_norm": 12.671449661254883,
      "learning_rate": 5.233796296296296e-06,
      "loss": 1.375,
      "step": 2539
    },
    {
      "epoch": 5.291666666666667,
      "grad_norm": 33.54765319824219,
      "learning_rate": 5.231481481481482e-06,
      "loss": 1.6896,
      "step": 2540
    },
    {
      "epoch": 5.29375,
      "grad_norm": 17.00873374938965,
      "learning_rate": 5.229166666666667e-06,
      "loss": 1.7154,
      "step": 2541
    },
    {
      "epoch": 5.295833333333333,
      "grad_norm": 12.860584259033203,
      "learning_rate": 5.2268518518518515e-06,
      "loss": 1.4362,
      "step": 2542
    },
    {
      "epoch": 5.297916666666667,
      "grad_norm": 24.52130699157715,
      "learning_rate": 5.2245370370370375e-06,
      "loss": 0.875,
      "step": 2543
    },
    {
      "epoch": 5.3,
      "grad_norm": 7.810794830322266,
      "learning_rate": 5.2222222222222226e-06,
      "loss": 0.5998,
      "step": 2544
    },
    {
      "epoch": 5.302083333333333,
      "grad_norm": 35.853939056396484,
      "learning_rate": 5.219907407407408e-06,
      "loss": 1.7811,
      "step": 2545
    },
    {
      "epoch": 5.304166666666666,
      "grad_norm": 13.550134658813477,
      "learning_rate": 5.217592592592593e-06,
      "loss": 0.9151,
      "step": 2546
    },
    {
      "epoch": 5.30625,
      "grad_norm": 27.720983505249023,
      "learning_rate": 5.215277777777779e-06,
      "loss": 1.6261,
      "step": 2547
    },
    {
      "epoch": 5.308333333333334,
      "grad_norm": 30.59981346130371,
      "learning_rate": 5.212962962962963e-06,
      "loss": 0.6351,
      "step": 2548
    },
    {
      "epoch": 5.310416666666667,
      "grad_norm": 22.80454444885254,
      "learning_rate": 5.210648148148149e-06,
      "loss": 1.1828,
      "step": 2549
    },
    {
      "epoch": 5.3125,
      "grad_norm": 28.781965255737305,
      "learning_rate": 5.208333333333334e-06,
      "loss": 1.4867,
      "step": 2550
    },
    {
      "epoch": 5.314583333333333,
      "grad_norm": 11.349363327026367,
      "learning_rate": 5.206018518518518e-06,
      "loss": 1.3972,
      "step": 2551
    },
    {
      "epoch": 5.316666666666666,
      "grad_norm": 14.896028518676758,
      "learning_rate": 5.203703703703704e-06,
      "loss": 1.8957,
      "step": 2552
    },
    {
      "epoch": 5.31875,
      "grad_norm": 74.6836166381836,
      "learning_rate": 5.201388888888889e-06,
      "loss": 0.5717,
      "step": 2553
    },
    {
      "epoch": 5.320833333333334,
      "grad_norm": 19.956148147583008,
      "learning_rate": 5.199074074074074e-06,
      "loss": 1.6547,
      "step": 2554
    },
    {
      "epoch": 5.322916666666667,
      "grad_norm": 9.124159812927246,
      "learning_rate": 5.1967592592592595e-06,
      "loss": 1.8385,
      "step": 2555
    },
    {
      "epoch": 5.325,
      "grad_norm": 8.826103210449219,
      "learning_rate": 5.1944444444444454e-06,
      "loss": 0.7197,
      "step": 2556
    },
    {
      "epoch": 5.327083333333333,
      "grad_norm": 35.56348419189453,
      "learning_rate": 5.19212962962963e-06,
      "loss": 1.2088,
      "step": 2557
    },
    {
      "epoch": 5.329166666666667,
      "grad_norm": 5.649712085723877,
      "learning_rate": 5.189814814814816e-06,
      "loss": 0.8043,
      "step": 2558
    },
    {
      "epoch": 5.33125,
      "grad_norm": 79.64521789550781,
      "learning_rate": 5.187500000000001e-06,
      "loss": 1.6895,
      "step": 2559
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 21.247825622558594,
      "learning_rate": 5.185185185185185e-06,
      "loss": 1.876,
      "step": 2560
    },
    {
      "epoch": 5.335416666666666,
      "grad_norm": 16.63654136657715,
      "learning_rate": 5.182870370370371e-06,
      "loss": 1.1947,
      "step": 2561
    },
    {
      "epoch": 5.3375,
      "grad_norm": 30.27962875366211,
      "learning_rate": 5.180555555555557e-06,
      "loss": 2.9061,
      "step": 2562
    },
    {
      "epoch": 5.339583333333334,
      "grad_norm": 11.340497016906738,
      "learning_rate": 5.178240740740741e-06,
      "loss": 1.5288,
      "step": 2563
    },
    {
      "epoch": 5.341666666666667,
      "grad_norm": 24.54805564880371,
      "learning_rate": 5.175925925925926e-06,
      "loss": 1.8236,
      "step": 2564
    },
    {
      "epoch": 5.34375,
      "grad_norm": 49.563568115234375,
      "learning_rate": 5.173611111111112e-06,
      "loss": 1.7885,
      "step": 2565
    },
    {
      "epoch": 5.345833333333333,
      "grad_norm": 16.023435592651367,
      "learning_rate": 5.171296296296296e-06,
      "loss": 2.0911,
      "step": 2566
    },
    {
      "epoch": 5.347916666666666,
      "grad_norm": 16.43158531188965,
      "learning_rate": 5.168981481481482e-06,
      "loss": 0.8577,
      "step": 2567
    },
    {
      "epoch": 5.35,
      "grad_norm": 11.605490684509277,
      "learning_rate": 5.1666666666666675e-06,
      "loss": 1.4456,
      "step": 2568
    },
    {
      "epoch": 5.352083333333334,
      "grad_norm": 46.017051696777344,
      "learning_rate": 5.164351851851852e-06,
      "loss": 1.1836,
      "step": 2569
    },
    {
      "epoch": 5.354166666666667,
      "grad_norm": 21.3666934967041,
      "learning_rate": 5.162037037037038e-06,
      "loss": 1.5284,
      "step": 2570
    },
    {
      "epoch": 5.35625,
      "grad_norm": 9.310201644897461,
      "learning_rate": 5.159722222222224e-06,
      "loss": 1.4165,
      "step": 2571
    },
    {
      "epoch": 5.358333333333333,
      "grad_norm": 10.591164588928223,
      "learning_rate": 5.157407407407408e-06,
      "loss": 1.0636,
      "step": 2572
    },
    {
      "epoch": 5.360416666666667,
      "grad_norm": 10.49531078338623,
      "learning_rate": 5.155092592592593e-06,
      "loss": 0.6847,
      "step": 2573
    },
    {
      "epoch": 5.3625,
      "grad_norm": 13.431807518005371,
      "learning_rate": 5.152777777777778e-06,
      "loss": 1.3691,
      "step": 2574
    },
    {
      "epoch": 5.364583333333333,
      "grad_norm": 27.911787033081055,
      "learning_rate": 5.150462962962963e-06,
      "loss": 1.6874,
      "step": 2575
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 12.693527221679688,
      "learning_rate": 5.148148148148149e-06,
      "loss": 1.776,
      "step": 2576
    },
    {
      "epoch": 5.36875,
      "grad_norm": 11.486164093017578,
      "learning_rate": 5.145833333333333e-06,
      "loss": 1.415,
      "step": 2577
    },
    {
      "epoch": 5.370833333333334,
      "grad_norm": 21.559803009033203,
      "learning_rate": 5.1435185185185184e-06,
      "loss": 1.1226,
      "step": 2578
    },
    {
      "epoch": 5.372916666666667,
      "grad_norm": 12.944748878479004,
      "learning_rate": 5.141203703703704e-06,
      "loss": 0.8787,
      "step": 2579
    },
    {
      "epoch": 5.375,
      "grad_norm": 32.43836975097656,
      "learning_rate": 5.138888888888889e-06,
      "loss": 0.8484,
      "step": 2580
    },
    {
      "epoch": 5.377083333333333,
      "grad_norm": 11.091135025024414,
      "learning_rate": 5.1365740740740746e-06,
      "loss": 1.2479,
      "step": 2581
    },
    {
      "epoch": 5.379166666666666,
      "grad_norm": 6.925507068634033,
      "learning_rate": 5.13425925925926e-06,
      "loss": 0.3797,
      "step": 2582
    },
    {
      "epoch": 5.38125,
      "grad_norm": 9.211427688598633,
      "learning_rate": 5.131944444444445e-06,
      "loss": 0.8583,
      "step": 2583
    },
    {
      "epoch": 5.383333333333334,
      "grad_norm": 8.896913528442383,
      "learning_rate": 5.12962962962963e-06,
      "loss": 1.0851,
      "step": 2584
    },
    {
      "epoch": 5.385416666666667,
      "grad_norm": 8.779794692993164,
      "learning_rate": 5.127314814814816e-06,
      "loss": 0.7359,
      "step": 2585
    },
    {
      "epoch": 5.3875,
      "grad_norm": 13.211627006530762,
      "learning_rate": 5.125e-06,
      "loss": 0.7487,
      "step": 2586
    },
    {
      "epoch": 5.389583333333333,
      "grad_norm": 10.851419448852539,
      "learning_rate": 5.122685185185185e-06,
      "loss": 0.8826,
      "step": 2587
    },
    {
      "epoch": 5.391666666666667,
      "grad_norm": 12.930837631225586,
      "learning_rate": 5.120370370370371e-06,
      "loss": 1.4698,
      "step": 2588
    },
    {
      "epoch": 5.39375,
      "grad_norm": 21.671255111694336,
      "learning_rate": 5.118055555555555e-06,
      "loss": 1.3189,
      "step": 2589
    },
    {
      "epoch": 5.395833333333333,
      "grad_norm": 5.674430847167969,
      "learning_rate": 5.115740740740741e-06,
      "loss": 0.7822,
      "step": 2590
    },
    {
      "epoch": 5.397916666666666,
      "grad_norm": 18.956623077392578,
      "learning_rate": 5.113425925925926e-06,
      "loss": 1.4665,
      "step": 2591
    },
    {
      "epoch": 5.4,
      "grad_norm": 27.236021041870117,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 1.9761,
      "step": 2592
    },
    {
      "epoch": 5.402083333333334,
      "grad_norm": 24.177953720092773,
      "learning_rate": 5.108796296296297e-06,
      "loss": 1.3118,
      "step": 2593
    },
    {
      "epoch": 5.404166666666667,
      "grad_norm": 9.315220832824707,
      "learning_rate": 5.1064814814814825e-06,
      "loss": 1.0667,
      "step": 2594
    },
    {
      "epoch": 5.40625,
      "grad_norm": 11.641572952270508,
      "learning_rate": 5.104166666666667e-06,
      "loss": 1.4569,
      "step": 2595
    },
    {
      "epoch": 5.408333333333333,
      "grad_norm": 10.59091567993164,
      "learning_rate": 5.101851851851852e-06,
      "loss": 0.7928,
      "step": 2596
    },
    {
      "epoch": 5.410416666666666,
      "grad_norm": 44.61759948730469,
      "learning_rate": 5.099537037037038e-06,
      "loss": 1.6509,
      "step": 2597
    },
    {
      "epoch": 5.4125,
      "grad_norm": 25.77570343017578,
      "learning_rate": 5.097222222222222e-06,
      "loss": 1.2691,
      "step": 2598
    },
    {
      "epoch": 5.414583333333334,
      "grad_norm": 39.96112060546875,
      "learning_rate": 5.094907407407408e-06,
      "loss": 1.5241,
      "step": 2599
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 10.381351470947266,
      "learning_rate": 5.092592592592593e-06,
      "loss": 1.6791,
      "step": 2600
    },
    {
      "epoch": 5.41875,
      "grad_norm": 21.54079246520996,
      "learning_rate": 5.090277777777778e-06,
      "loss": 1.0463,
      "step": 2601
    },
    {
      "epoch": 5.420833333333333,
      "grad_norm": 32.637176513671875,
      "learning_rate": 5.087962962962963e-06,
      "loss": 0.9267,
      "step": 2602
    },
    {
      "epoch": 5.422916666666667,
      "grad_norm": 9.615255355834961,
      "learning_rate": 5.085648148148149e-06,
      "loss": 0.8404,
      "step": 2603
    },
    {
      "epoch": 5.425,
      "grad_norm": 13.794840812683105,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.3347,
      "step": 2604
    },
    {
      "epoch": 5.427083333333333,
      "grad_norm": 7.242059230804443,
      "learning_rate": 5.081018518518519e-06,
      "loss": 0.5898,
      "step": 2605
    },
    {
      "epoch": 5.429166666666666,
      "grad_norm": 14.304383277893066,
      "learning_rate": 5.0787037037037046e-06,
      "loss": 1.4748,
      "step": 2606
    },
    {
      "epoch": 5.43125,
      "grad_norm": 19.06270408630371,
      "learning_rate": 5.076388888888889e-06,
      "loss": 1.1911,
      "step": 2607
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 6.2875471115112305,
      "learning_rate": 5.074074074074075e-06,
      "loss": 0.7809,
      "step": 2608
    },
    {
      "epoch": 5.435416666666667,
      "grad_norm": 12.416386604309082,
      "learning_rate": 5.07175925925926e-06,
      "loss": 1.538,
      "step": 2609
    },
    {
      "epoch": 5.4375,
      "grad_norm": 17.685455322265625,
      "learning_rate": 5.069444444444445e-06,
      "loss": 1.6823,
      "step": 2610
    },
    {
      "epoch": 5.439583333333333,
      "grad_norm": 26.006160736083984,
      "learning_rate": 5.06712962962963e-06,
      "loss": 1.1663,
      "step": 2611
    },
    {
      "epoch": 5.441666666666666,
      "grad_norm": 7.5777587890625,
      "learning_rate": 5.064814814814816e-06,
      "loss": 0.4579,
      "step": 2612
    },
    {
      "epoch": 5.44375,
      "grad_norm": 13.232856750488281,
      "learning_rate": 5.0625e-06,
      "loss": 1.3187,
      "step": 2613
    },
    {
      "epoch": 5.445833333333334,
      "grad_norm": 13.02490234375,
      "learning_rate": 5.060185185185186e-06,
      "loss": 0.7026,
      "step": 2614
    },
    {
      "epoch": 5.447916666666667,
      "grad_norm": 21.636220932006836,
      "learning_rate": 5.057870370370371e-06,
      "loss": 2.1662,
      "step": 2615
    },
    {
      "epoch": 5.45,
      "grad_norm": 19.481843948364258,
      "learning_rate": 5.0555555555555555e-06,
      "loss": 1.5514,
      "step": 2616
    },
    {
      "epoch": 5.452083333333333,
      "grad_norm": 14.71524429321289,
      "learning_rate": 5.0532407407407415e-06,
      "loss": 1.433,
      "step": 2617
    },
    {
      "epoch": 5.454166666666667,
      "grad_norm": 15.984670639038086,
      "learning_rate": 5.050925925925926e-06,
      "loss": 0.8996,
      "step": 2618
    },
    {
      "epoch": 5.45625,
      "grad_norm": 10.94148063659668,
      "learning_rate": 5.048611111111112e-06,
      "loss": 1.0186,
      "step": 2619
    },
    {
      "epoch": 5.458333333333333,
      "grad_norm": 11.864581108093262,
      "learning_rate": 5.046296296296297e-06,
      "loss": 1.4091,
      "step": 2620
    },
    {
      "epoch": 5.460416666666666,
      "grad_norm": 13.617907524108887,
      "learning_rate": 5.043981481481481e-06,
      "loss": 1.4861,
      "step": 2621
    },
    {
      "epoch": 5.4625,
      "grad_norm": 54.58378219604492,
      "learning_rate": 5.041666666666667e-06,
      "loss": 1.9865,
      "step": 2622
    },
    {
      "epoch": 5.464583333333334,
      "grad_norm": 45.8775634765625,
      "learning_rate": 5.039351851851853e-06,
      "loss": 1.3631,
      "step": 2623
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 21.693561553955078,
      "learning_rate": 5.037037037037037e-06,
      "loss": 1.2561,
      "step": 2624
    },
    {
      "epoch": 5.46875,
      "grad_norm": 10.245893478393555,
      "learning_rate": 5.034722222222222e-06,
      "loss": 0.8561,
      "step": 2625
    },
    {
      "epoch": 5.470833333333333,
      "grad_norm": 10.471824645996094,
      "learning_rate": 5.032407407407408e-06,
      "loss": 1.0495,
      "step": 2626
    },
    {
      "epoch": 5.472916666666666,
      "grad_norm": 63.514652252197266,
      "learning_rate": 5.0300925925925924e-06,
      "loss": 1.1329,
      "step": 2627
    },
    {
      "epoch": 5.475,
      "grad_norm": 70.20559692382812,
      "learning_rate": 5.027777777777778e-06,
      "loss": 1.1461,
      "step": 2628
    },
    {
      "epoch": 5.477083333333334,
      "grad_norm": 25.22254180908203,
      "learning_rate": 5.0254629629629635e-06,
      "loss": 1.4083,
      "step": 2629
    },
    {
      "epoch": 5.479166666666667,
      "grad_norm": 10.815972328186035,
      "learning_rate": 5.023148148148148e-06,
      "loss": 1.3285,
      "step": 2630
    },
    {
      "epoch": 5.48125,
      "grad_norm": 24.819570541381836,
      "learning_rate": 5.020833333333334e-06,
      "loss": 1.5137,
      "step": 2631
    },
    {
      "epoch": 5.483333333333333,
      "grad_norm": 13.346161842346191,
      "learning_rate": 5.01851851851852e-06,
      "loss": 1.2367,
      "step": 2632
    },
    {
      "epoch": 5.485416666666667,
      "grad_norm": 19.776233673095703,
      "learning_rate": 5.016203703703704e-06,
      "loss": 1.5478,
      "step": 2633
    },
    {
      "epoch": 5.4875,
      "grad_norm": 22.303220748901367,
      "learning_rate": 5.013888888888889e-06,
      "loss": 1.4913,
      "step": 2634
    },
    {
      "epoch": 5.489583333333333,
      "grad_norm": 9.736674308776855,
      "learning_rate": 5.011574074074075e-06,
      "loss": 1.4012,
      "step": 2635
    },
    {
      "epoch": 5.491666666666666,
      "grad_norm": 33.33921813964844,
      "learning_rate": 5.009259259259259e-06,
      "loss": 1.5995,
      "step": 2636
    },
    {
      "epoch": 5.49375,
      "grad_norm": 10.0142240524292,
      "learning_rate": 5.006944444444445e-06,
      "loss": 0.7004,
      "step": 2637
    },
    {
      "epoch": 5.495833333333334,
      "grad_norm": 6.708818435668945,
      "learning_rate": 5.00462962962963e-06,
      "loss": 0.6919,
      "step": 2638
    },
    {
      "epoch": 5.497916666666667,
      "grad_norm": 27.87676239013672,
      "learning_rate": 5.0023148148148145e-06,
      "loss": 1.5379,
      "step": 2639
    },
    {
      "epoch": 5.5,
      "grad_norm": 7.722331523895264,
      "learning_rate": 5e-06,
      "loss": 0.5953,
      "step": 2640
    },
    {
      "epoch": 5.502083333333333,
      "grad_norm": 54.63054656982422,
      "learning_rate": 4.9976851851851855e-06,
      "loss": 2.157,
      "step": 2641
    },
    {
      "epoch": 5.504166666666666,
      "grad_norm": 71.82231903076172,
      "learning_rate": 4.995370370370371e-06,
      "loss": 1.4819,
      "step": 2642
    },
    {
      "epoch": 5.50625,
      "grad_norm": 11.962904930114746,
      "learning_rate": 4.993055555555556e-06,
      "loss": 1.8548,
      "step": 2643
    },
    {
      "epoch": 5.508333333333333,
      "grad_norm": 10.381650924682617,
      "learning_rate": 4.990740740740741e-06,
      "loss": 1.8035,
      "step": 2644
    },
    {
      "epoch": 5.510416666666667,
      "grad_norm": 6.329341888427734,
      "learning_rate": 4.988425925925927e-06,
      "loss": 0.3992,
      "step": 2645
    },
    {
      "epoch": 5.5125,
      "grad_norm": 19.279584884643555,
      "learning_rate": 4.986111111111112e-06,
      "loss": 1.4568,
      "step": 2646
    },
    {
      "epoch": 5.514583333333333,
      "grad_norm": 6.174511432647705,
      "learning_rate": 4.983796296296297e-06,
      "loss": 0.3744,
      "step": 2647
    },
    {
      "epoch": 5.516666666666667,
      "grad_norm": 16.618356704711914,
      "learning_rate": 4.981481481481482e-06,
      "loss": 1.2601,
      "step": 2648
    },
    {
      "epoch": 5.51875,
      "grad_norm": 20.909353256225586,
      "learning_rate": 4.979166666666667e-06,
      "loss": 1.1307,
      "step": 2649
    },
    {
      "epoch": 5.520833333333333,
      "grad_norm": 12.739359855651855,
      "learning_rate": 4.976851851851852e-06,
      "loss": 1.152,
      "step": 2650
    },
    {
      "epoch": 5.522916666666667,
      "grad_norm": 15.799741744995117,
      "learning_rate": 4.974537037037037e-06,
      "loss": 1.1101,
      "step": 2651
    },
    {
      "epoch": 5.525,
      "grad_norm": 21.48428726196289,
      "learning_rate": 4.9722222222222224e-06,
      "loss": 2.4172,
      "step": 2652
    },
    {
      "epoch": 5.527083333333334,
      "grad_norm": 32.30295944213867,
      "learning_rate": 4.9699074074074075e-06,
      "loss": 1.6054,
      "step": 2653
    },
    {
      "epoch": 5.529166666666667,
      "grad_norm": 14.463225364685059,
      "learning_rate": 4.967592592592593e-06,
      "loss": 1.7888,
      "step": 2654
    },
    {
      "epoch": 5.53125,
      "grad_norm": 22.120250701904297,
      "learning_rate": 4.9652777777777786e-06,
      "loss": 1.5843,
      "step": 2655
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 12.975700378417969,
      "learning_rate": 4.962962962962964e-06,
      "loss": 1.4167,
      "step": 2656
    },
    {
      "epoch": 5.535416666666666,
      "grad_norm": 30.959407806396484,
      "learning_rate": 4.960648148148148e-06,
      "loss": 0.3499,
      "step": 2657
    },
    {
      "epoch": 5.5375,
      "grad_norm": 98.95115661621094,
      "learning_rate": 4.958333333333334e-06,
      "loss": 1.3071,
      "step": 2658
    },
    {
      "epoch": 5.539583333333333,
      "grad_norm": 31.42974281311035,
      "learning_rate": 4.956018518518519e-06,
      "loss": 1.9163,
      "step": 2659
    },
    {
      "epoch": 5.541666666666667,
      "grad_norm": 14.90132999420166,
      "learning_rate": 4.953703703703704e-06,
      "loss": 1.4524,
      "step": 2660
    },
    {
      "epoch": 5.54375,
      "grad_norm": 46.120792388916016,
      "learning_rate": 4.951388888888889e-06,
      "loss": 1.7619,
      "step": 2661
    },
    {
      "epoch": 5.545833333333333,
      "grad_norm": 12.947464942932129,
      "learning_rate": 4.949074074074074e-06,
      "loss": 1.408,
      "step": 2662
    },
    {
      "epoch": 5.547916666666667,
      "grad_norm": 7.406569480895996,
      "learning_rate": 4.946759259259259e-06,
      "loss": 1.1542,
      "step": 2663
    },
    {
      "epoch": 5.55,
      "grad_norm": 10.364300727844238,
      "learning_rate": 4.944444444444445e-06,
      "loss": 1.529,
      "step": 2664
    },
    {
      "epoch": 5.552083333333333,
      "grad_norm": 10.28520393371582,
      "learning_rate": 4.94212962962963e-06,
      "loss": 0.9794,
      "step": 2665
    },
    {
      "epoch": 5.554166666666667,
      "grad_norm": 8.11608600616455,
      "learning_rate": 4.939814814814815e-06,
      "loss": 1.5179,
      "step": 2666
    },
    {
      "epoch": 5.55625,
      "grad_norm": 13.180359840393066,
      "learning_rate": 4.937500000000001e-06,
      "loss": 1.7358,
      "step": 2667
    },
    {
      "epoch": 5.558333333333334,
      "grad_norm": 40.05775833129883,
      "learning_rate": 4.935185185185186e-06,
      "loss": 1.5783,
      "step": 2668
    },
    {
      "epoch": 5.560416666666667,
      "grad_norm": 42.391300201416016,
      "learning_rate": 4.932870370370371e-06,
      "loss": 0.906,
      "step": 2669
    },
    {
      "epoch": 5.5625,
      "grad_norm": 6.677165508270264,
      "learning_rate": 4.930555555555556e-06,
      "loss": 0.8717,
      "step": 2670
    },
    {
      "epoch": 5.564583333333333,
      "grad_norm": 92.42413330078125,
      "learning_rate": 4.928240740740741e-06,
      "loss": 1.1895,
      "step": 2671
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 18.44675636291504,
      "learning_rate": 4.925925925925926e-06,
      "loss": 1.3304,
      "step": 2672
    },
    {
      "epoch": 5.56875,
      "grad_norm": 14.558174133300781,
      "learning_rate": 4.923611111111112e-06,
      "loss": 1.329,
      "step": 2673
    },
    {
      "epoch": 5.570833333333333,
      "grad_norm": 22.20636558532715,
      "learning_rate": 4.921296296296297e-06,
      "loss": 1.5984,
      "step": 2674
    },
    {
      "epoch": 5.572916666666667,
      "grad_norm": 20.868270874023438,
      "learning_rate": 4.918981481481482e-06,
      "loss": 1.9624,
      "step": 2675
    },
    {
      "epoch": 5.575,
      "grad_norm": 49.440956115722656,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 1.4793,
      "step": 2676
    },
    {
      "epoch": 5.577083333333333,
      "grad_norm": 13.74746322631836,
      "learning_rate": 4.914351851851852e-06,
      "loss": 1.0703,
      "step": 2677
    },
    {
      "epoch": 5.579166666666667,
      "grad_norm": 20.15099334716797,
      "learning_rate": 4.9120370370370375e-06,
      "loss": 1.8108,
      "step": 2678
    },
    {
      "epoch": 5.58125,
      "grad_norm": 8.858895301818848,
      "learning_rate": 4.909722222222223e-06,
      "loss": 1.3461,
      "step": 2679
    },
    {
      "epoch": 5.583333333333333,
      "grad_norm": 93.66341400146484,
      "learning_rate": 4.907407407407408e-06,
      "loss": 2.308,
      "step": 2680
    },
    {
      "epoch": 5.585416666666667,
      "grad_norm": 17.531526565551758,
      "learning_rate": 4.905092592592593e-06,
      "loss": 0.7988,
      "step": 2681
    },
    {
      "epoch": 5.5875,
      "grad_norm": 18.36700439453125,
      "learning_rate": 4.902777777777778e-06,
      "loss": 1.7984,
      "step": 2682
    },
    {
      "epoch": 5.589583333333334,
      "grad_norm": 17.736433029174805,
      "learning_rate": 4.900462962962964e-06,
      "loss": 1.9017,
      "step": 2683
    },
    {
      "epoch": 5.591666666666667,
      "grad_norm": 14.710372924804688,
      "learning_rate": 4.898148148148149e-06,
      "loss": 1.0977,
      "step": 2684
    },
    {
      "epoch": 5.59375,
      "grad_norm": 16.400339126586914,
      "learning_rate": 4.895833333333333e-06,
      "loss": 1.3098,
      "step": 2685
    },
    {
      "epoch": 5.595833333333333,
      "grad_norm": 20.536975860595703,
      "learning_rate": 4.893518518518519e-06,
      "loss": 1.8296,
      "step": 2686
    },
    {
      "epoch": 5.597916666666666,
      "grad_norm": 5.093532562255859,
      "learning_rate": 4.891203703703704e-06,
      "loss": 0.145,
      "step": 2687
    },
    {
      "epoch": 5.6,
      "grad_norm": 50.154640197753906,
      "learning_rate": 4.888888888888889e-06,
      "loss": 1.0938,
      "step": 2688
    },
    {
      "epoch": 5.602083333333333,
      "grad_norm": 17.60889434814453,
      "learning_rate": 4.8865740740740744e-06,
      "loss": 1.3527,
      "step": 2689
    },
    {
      "epoch": 5.604166666666667,
      "grad_norm": 10.148580551147461,
      "learning_rate": 4.8842592592592595e-06,
      "loss": 0.8156,
      "step": 2690
    },
    {
      "epoch": 5.60625,
      "grad_norm": 41.47946548461914,
      "learning_rate": 4.881944444444445e-06,
      "loss": 1.9535,
      "step": 2691
    },
    {
      "epoch": 5.608333333333333,
      "grad_norm": 47.35991287231445,
      "learning_rate": 4.8796296296296306e-06,
      "loss": 1.2834,
      "step": 2692
    },
    {
      "epoch": 5.610416666666667,
      "grad_norm": 38.260318756103516,
      "learning_rate": 4.877314814814816e-06,
      "loss": 1.4198,
      "step": 2693
    },
    {
      "epoch": 5.6125,
      "grad_norm": 47.36173629760742,
      "learning_rate": 4.875e-06,
      "loss": 1.9445,
      "step": 2694
    },
    {
      "epoch": 5.614583333333333,
      "grad_norm": 16.184120178222656,
      "learning_rate": 4.872685185185186e-06,
      "loss": 0.9675,
      "step": 2695
    },
    {
      "epoch": 5.616666666666667,
      "grad_norm": 31.81885528564453,
      "learning_rate": 4.870370370370371e-06,
      "loss": 1.7085,
      "step": 2696
    },
    {
      "epoch": 5.61875,
      "grad_norm": 70.76019287109375,
      "learning_rate": 4.868055555555556e-06,
      "loss": 1.6492,
      "step": 2697
    },
    {
      "epoch": 5.620833333333334,
      "grad_norm": 5.323526859283447,
      "learning_rate": 4.865740740740741e-06,
      "loss": 0.7438,
      "step": 2698
    },
    {
      "epoch": 5.622916666666667,
      "grad_norm": 20.914579391479492,
      "learning_rate": 4.863425925925926e-06,
      "loss": 1.723,
      "step": 2699
    },
    {
      "epoch": 5.625,
      "grad_norm": 17.29517936706543,
      "learning_rate": 4.861111111111111e-06,
      "loss": 1.3534,
      "step": 2700
    },
    {
      "epoch": 5.627083333333333,
      "grad_norm": 13.121288299560547,
      "learning_rate": 4.8587962962962964e-06,
      "loss": 1.4088,
      "step": 2701
    },
    {
      "epoch": 5.629166666666666,
      "grad_norm": 69.67288208007812,
      "learning_rate": 4.856481481481482e-06,
      "loss": 0.5799,
      "step": 2702
    },
    {
      "epoch": 5.63125,
      "grad_norm": 18.16193962097168,
      "learning_rate": 4.854166666666667e-06,
      "loss": 1.0211,
      "step": 2703
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 7.395769119262695,
      "learning_rate": 4.851851851851852e-06,
      "loss": 0.1791,
      "step": 2704
    },
    {
      "epoch": 5.635416666666667,
      "grad_norm": 36.44109344482422,
      "learning_rate": 4.849537037037038e-06,
      "loss": 1.9183,
      "step": 2705
    },
    {
      "epoch": 5.6375,
      "grad_norm": 16.014089584350586,
      "learning_rate": 4.847222222222223e-06,
      "loss": 1.5895,
      "step": 2706
    },
    {
      "epoch": 5.639583333333333,
      "grad_norm": 7.402563571929932,
      "learning_rate": 4.844907407407408e-06,
      "loss": 0.6425,
      "step": 2707
    },
    {
      "epoch": 5.641666666666667,
      "grad_norm": 20.14811134338379,
      "learning_rate": 4.842592592592593e-06,
      "loss": 0.3819,
      "step": 2708
    },
    {
      "epoch": 5.64375,
      "grad_norm": 13.705748558044434,
      "learning_rate": 4.840277777777778e-06,
      "loss": 1.2833,
      "step": 2709
    },
    {
      "epoch": 5.645833333333333,
      "grad_norm": 18.984376907348633,
      "learning_rate": 4.837962962962963e-06,
      "loss": 1.3569,
      "step": 2710
    },
    {
      "epoch": 5.647916666666667,
      "grad_norm": 10.848931312561035,
      "learning_rate": 4.835648148148149e-06,
      "loss": 1.5924,
      "step": 2711
    },
    {
      "epoch": 5.65,
      "grad_norm": 16.151845932006836,
      "learning_rate": 4.833333333333333e-06,
      "loss": 1.1199,
      "step": 2712
    },
    {
      "epoch": 5.652083333333334,
      "grad_norm": 11.131166458129883,
      "learning_rate": 4.8310185185185185e-06,
      "loss": 1.2484,
      "step": 2713
    },
    {
      "epoch": 5.654166666666667,
      "grad_norm": 74.39852905273438,
      "learning_rate": 4.828703703703704e-06,
      "loss": 1.3565,
      "step": 2714
    },
    {
      "epoch": 5.65625,
      "grad_norm": 10.222107887268066,
      "learning_rate": 4.8263888888888895e-06,
      "loss": 0.8662,
      "step": 2715
    },
    {
      "epoch": 5.658333333333333,
      "grad_norm": 22.820058822631836,
      "learning_rate": 4.824074074074075e-06,
      "loss": 0.8971,
      "step": 2716
    },
    {
      "epoch": 5.660416666666666,
      "grad_norm": 15.94127368927002,
      "learning_rate": 4.82175925925926e-06,
      "loss": 0.4503,
      "step": 2717
    },
    {
      "epoch": 5.6625,
      "grad_norm": 8.647547721862793,
      "learning_rate": 4.819444444444445e-06,
      "loss": 0.726,
      "step": 2718
    },
    {
      "epoch": 5.664583333333333,
      "grad_norm": 9.172589302062988,
      "learning_rate": 4.81712962962963e-06,
      "loss": 1.1414,
      "step": 2719
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 25.381601333618164,
      "learning_rate": 4.814814814814815e-06,
      "loss": 1.0008,
      "step": 2720
    },
    {
      "epoch": 5.66875,
      "grad_norm": 34.0145378112793,
      "learning_rate": 4.8125e-06,
      "loss": 1.3854,
      "step": 2721
    },
    {
      "epoch": 5.670833333333333,
      "grad_norm": 19.67780303955078,
      "learning_rate": 4.810185185185185e-06,
      "loss": 1.4223,
      "step": 2722
    },
    {
      "epoch": 5.672916666666667,
      "grad_norm": 11.36728286743164,
      "learning_rate": 4.80787037037037e-06,
      "loss": 1.6313,
      "step": 2723
    },
    {
      "epoch": 5.675,
      "grad_norm": 100.0208740234375,
      "learning_rate": 4.805555555555556e-06,
      "loss": 1.7985,
      "step": 2724
    },
    {
      "epoch": 5.677083333333333,
      "grad_norm": 63.722129821777344,
      "learning_rate": 4.803240740740741e-06,
      "loss": 1.7546,
      "step": 2725
    },
    {
      "epoch": 5.679166666666667,
      "grad_norm": 24.30217742919922,
      "learning_rate": 4.800925925925926e-06,
      "loss": 0.8458,
      "step": 2726
    },
    {
      "epoch": 5.68125,
      "grad_norm": 11.020339012145996,
      "learning_rate": 4.7986111111111115e-06,
      "loss": 0.8205,
      "step": 2727
    },
    {
      "epoch": 5.683333333333334,
      "grad_norm": 12.064104080200195,
      "learning_rate": 4.796296296296297e-06,
      "loss": 1.294,
      "step": 2728
    },
    {
      "epoch": 5.685416666666667,
      "grad_norm": 14.991999626159668,
      "learning_rate": 4.793981481481482e-06,
      "loss": 1.6195,
      "step": 2729
    },
    {
      "epoch": 5.6875,
      "grad_norm": 9.409924507141113,
      "learning_rate": 4.791666666666668e-06,
      "loss": 1.2345,
      "step": 2730
    },
    {
      "epoch": 5.689583333333333,
      "grad_norm": 25.544370651245117,
      "learning_rate": 4.789351851851852e-06,
      "loss": 1.5834,
      "step": 2731
    },
    {
      "epoch": 5.691666666666666,
      "grad_norm": 9.986680030822754,
      "learning_rate": 4.787037037037037e-06,
      "loss": 1.3282,
      "step": 2732
    },
    {
      "epoch": 5.69375,
      "grad_norm": 14.789471626281738,
      "learning_rate": 4.784722222222223e-06,
      "loss": 1.3824,
      "step": 2733
    },
    {
      "epoch": 5.695833333333333,
      "grad_norm": 9.228778839111328,
      "learning_rate": 4.782407407407408e-06,
      "loss": 1.2208,
      "step": 2734
    },
    {
      "epoch": 5.697916666666667,
      "grad_norm": 11.305465698242188,
      "learning_rate": 4.780092592592593e-06,
      "loss": 1.4171,
      "step": 2735
    },
    {
      "epoch": 5.7,
      "grad_norm": 9.34859561920166,
      "learning_rate": 4.777777777777778e-06,
      "loss": 0.2848,
      "step": 2736
    },
    {
      "epoch": 5.702083333333333,
      "grad_norm": 8.088231086730957,
      "learning_rate": 4.775462962962963e-06,
      "loss": 0.855,
      "step": 2737
    },
    {
      "epoch": 5.704166666666667,
      "grad_norm": 15.573601722717285,
      "learning_rate": 4.7731481481481484e-06,
      "loss": 1.4206,
      "step": 2738
    },
    {
      "epoch": 5.70625,
      "grad_norm": 11.420559883117676,
      "learning_rate": 4.770833333333334e-06,
      "loss": 0.9741,
      "step": 2739
    },
    {
      "epoch": 5.708333333333333,
      "grad_norm": 13.151726722717285,
      "learning_rate": 4.768518518518519e-06,
      "loss": 1.4495,
      "step": 2740
    },
    {
      "epoch": 5.710416666666667,
      "grad_norm": 13.465779304504395,
      "learning_rate": 4.766203703703704e-06,
      "loss": 1.3034,
      "step": 2741
    },
    {
      "epoch": 5.7125,
      "grad_norm": 11.68692684173584,
      "learning_rate": 4.763888888888889e-06,
      "loss": 0.6898,
      "step": 2742
    },
    {
      "epoch": 5.714583333333334,
      "grad_norm": 10.719058990478516,
      "learning_rate": 4.761574074074075e-06,
      "loss": 0.8566,
      "step": 2743
    },
    {
      "epoch": 5.716666666666667,
      "grad_norm": 15.222496032714844,
      "learning_rate": 4.75925925925926e-06,
      "loss": 1.6222,
      "step": 2744
    },
    {
      "epoch": 5.71875,
      "grad_norm": 10.82061767578125,
      "learning_rate": 4.756944444444445e-06,
      "loss": 0.8203,
      "step": 2745
    },
    {
      "epoch": 5.720833333333333,
      "grad_norm": 75.06658172607422,
      "learning_rate": 4.75462962962963e-06,
      "loss": 1.9543,
      "step": 2746
    },
    {
      "epoch": 5.722916666666666,
      "grad_norm": 5.324199199676514,
      "learning_rate": 4.752314814814815e-06,
      "loss": 0.7484,
      "step": 2747
    },
    {
      "epoch": 5.725,
      "grad_norm": 31.637611389160156,
      "learning_rate": 4.75e-06,
      "loss": 1.3145,
      "step": 2748
    },
    {
      "epoch": 5.727083333333333,
      "grad_norm": 15.159177780151367,
      "learning_rate": 4.747685185185185e-06,
      "loss": 1.1868,
      "step": 2749
    },
    {
      "epoch": 5.729166666666667,
      "grad_norm": 82.2568359375,
      "learning_rate": 4.7453703703703705e-06,
      "loss": 1.1656,
      "step": 2750
    },
    {
      "epoch": 5.73125,
      "grad_norm": 22.938228607177734,
      "learning_rate": 4.7430555555555556e-06,
      "loss": 1.6118,
      "step": 2751
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 18.433643341064453,
      "learning_rate": 4.7407407407407415e-06,
      "loss": 0.9385,
      "step": 2752
    },
    {
      "epoch": 5.735416666666667,
      "grad_norm": 12.503287315368652,
      "learning_rate": 4.738425925925927e-06,
      "loss": 1.629,
      "step": 2753
    },
    {
      "epoch": 5.7375,
      "grad_norm": 10.227115631103516,
      "learning_rate": 4.736111111111112e-06,
      "loss": 0.8887,
      "step": 2754
    },
    {
      "epoch": 5.739583333333333,
      "grad_norm": 8.285503387451172,
      "learning_rate": 4.733796296296297e-06,
      "loss": 0.6759,
      "step": 2755
    },
    {
      "epoch": 5.741666666666667,
      "grad_norm": 33.6363639831543,
      "learning_rate": 4.731481481481482e-06,
      "loss": 1.012,
      "step": 2756
    },
    {
      "epoch": 5.74375,
      "grad_norm": 18.447002410888672,
      "learning_rate": 4.729166666666667e-06,
      "loss": 1.5772,
      "step": 2757
    },
    {
      "epoch": 5.745833333333334,
      "grad_norm": 10.931351661682129,
      "learning_rate": 4.726851851851852e-06,
      "loss": 1.5897,
      "step": 2758
    },
    {
      "epoch": 5.747916666666667,
      "grad_norm": 10.151145935058594,
      "learning_rate": 4.724537037037037e-06,
      "loss": 0.7218,
      "step": 2759
    },
    {
      "epoch": 5.75,
      "grad_norm": 40.45800018310547,
      "learning_rate": 4.722222222222222e-06,
      "loss": 2.5795,
      "step": 2760
    },
    {
      "epoch": 5.752083333333333,
      "grad_norm": 91.92813873291016,
      "learning_rate": 4.719907407407408e-06,
      "loss": 1.259,
      "step": 2761
    },
    {
      "epoch": 5.754166666666666,
      "grad_norm": 28.56720542907715,
      "learning_rate": 4.717592592592593e-06,
      "loss": 1.6053,
      "step": 2762
    },
    {
      "epoch": 5.75625,
      "grad_norm": 8.7391357421875,
      "learning_rate": 4.715277777777778e-06,
      "loss": 0.7975,
      "step": 2763
    },
    {
      "epoch": 5.758333333333333,
      "grad_norm": 16.745603561401367,
      "learning_rate": 4.712962962962963e-06,
      "loss": 1.458,
      "step": 2764
    },
    {
      "epoch": 5.760416666666667,
      "grad_norm": 16.011640548706055,
      "learning_rate": 4.710648148148149e-06,
      "loss": 1.3155,
      "step": 2765
    },
    {
      "epoch": 5.7625,
      "grad_norm": 8.36845874786377,
      "learning_rate": 4.708333333333334e-06,
      "loss": 0.7577,
      "step": 2766
    },
    {
      "epoch": 5.764583333333333,
      "grad_norm": 13.200312614440918,
      "learning_rate": 4.706018518518519e-06,
      "loss": 1.3191,
      "step": 2767
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 9.79698371887207,
      "learning_rate": 4.703703703703704e-06,
      "loss": 1.0455,
      "step": 2768
    },
    {
      "epoch": 5.76875,
      "grad_norm": 10.652952194213867,
      "learning_rate": 4.701388888888889e-06,
      "loss": 1.4033,
      "step": 2769
    },
    {
      "epoch": 5.770833333333333,
      "grad_norm": 12.25667667388916,
      "learning_rate": 4.699074074074074e-06,
      "loss": 0.8414,
      "step": 2770
    },
    {
      "epoch": 5.772916666666667,
      "grad_norm": 50.347957611083984,
      "learning_rate": 4.69675925925926e-06,
      "loss": 1.8646,
      "step": 2771
    },
    {
      "epoch": 5.775,
      "grad_norm": 6.261440753936768,
      "learning_rate": 4.694444444444445e-06,
      "loss": 0.2091,
      "step": 2772
    },
    {
      "epoch": 5.777083333333334,
      "grad_norm": 80.68749237060547,
      "learning_rate": 4.692129629629629e-06,
      "loss": 1.7022,
      "step": 2773
    },
    {
      "epoch": 5.779166666666667,
      "grad_norm": 11.416617393493652,
      "learning_rate": 4.689814814814815e-06,
      "loss": 0.8474,
      "step": 2774
    },
    {
      "epoch": 5.78125,
      "grad_norm": 23.10707664489746,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 1.5658,
      "step": 2775
    },
    {
      "epoch": 5.783333333333333,
      "grad_norm": 36.660987854003906,
      "learning_rate": 4.6851851851851855e-06,
      "loss": 1.5528,
      "step": 2776
    },
    {
      "epoch": 5.785416666666666,
      "grad_norm": 10.91586685180664,
      "learning_rate": 4.682870370370371e-06,
      "loss": 0.8152,
      "step": 2777
    },
    {
      "epoch": 5.7875,
      "grad_norm": 9.52579402923584,
      "learning_rate": 4.680555555555556e-06,
      "loss": 0.696,
      "step": 2778
    },
    {
      "epoch": 5.789583333333333,
      "grad_norm": 17.222640991210938,
      "learning_rate": 4.678240740740741e-06,
      "loss": 1.6724,
      "step": 2779
    },
    {
      "epoch": 5.791666666666667,
      "grad_norm": 38.87371063232422,
      "learning_rate": 4.675925925925927e-06,
      "loss": 0.8772,
      "step": 2780
    },
    {
      "epoch": 5.79375,
      "grad_norm": 22.440950393676758,
      "learning_rate": 4.673611111111112e-06,
      "loss": 0.909,
      "step": 2781
    },
    {
      "epoch": 5.795833333333333,
      "grad_norm": 16.41139030456543,
      "learning_rate": 4.671296296296297e-06,
      "loss": 1.5132,
      "step": 2782
    },
    {
      "epoch": 5.797916666666667,
      "grad_norm": 22.71930503845215,
      "learning_rate": 4.668981481481482e-06,
      "loss": 2.3483,
      "step": 2783
    },
    {
      "epoch": 5.8,
      "grad_norm": 8.545278549194336,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.5082,
      "step": 2784
    },
    {
      "epoch": 5.802083333333333,
      "grad_norm": 12.190606117248535,
      "learning_rate": 4.664351851851852e-06,
      "loss": 0.8535,
      "step": 2785
    },
    {
      "epoch": 5.804166666666667,
      "grad_norm": 18.131261825561523,
      "learning_rate": 4.662037037037037e-06,
      "loss": 1.4853,
      "step": 2786
    },
    {
      "epoch": 5.80625,
      "grad_norm": 14.210735321044922,
      "learning_rate": 4.6597222222222225e-06,
      "loss": 1.4976,
      "step": 2787
    },
    {
      "epoch": 5.808333333333334,
      "grad_norm": 9.014070510864258,
      "learning_rate": 4.6574074074074076e-06,
      "loss": 0.9188,
      "step": 2788
    },
    {
      "epoch": 5.810416666666667,
      "grad_norm": 17.529766082763672,
      "learning_rate": 4.655092592592593e-06,
      "loss": 1.643,
      "step": 2789
    },
    {
      "epoch": 5.8125,
      "grad_norm": 36.62855911254883,
      "learning_rate": 4.652777777777779e-06,
      "loss": 1.8776,
      "step": 2790
    },
    {
      "epoch": 5.814583333333333,
      "grad_norm": 37.78445053100586,
      "learning_rate": 4.650462962962964e-06,
      "loss": 0.7529,
      "step": 2791
    },
    {
      "epoch": 5.816666666666666,
      "grad_norm": 44.37175369262695,
      "learning_rate": 4.648148148148148e-06,
      "loss": 1.4392,
      "step": 2792
    },
    {
      "epoch": 5.81875,
      "grad_norm": 18.77364730834961,
      "learning_rate": 4.645833333333334e-06,
      "loss": 1.711,
      "step": 2793
    },
    {
      "epoch": 5.820833333333333,
      "grad_norm": 29.634809494018555,
      "learning_rate": 4.643518518518519e-06,
      "loss": 2.0448,
      "step": 2794
    },
    {
      "epoch": 5.822916666666667,
      "grad_norm": 7.573497295379639,
      "learning_rate": 4.641203703703704e-06,
      "loss": 0.2251,
      "step": 2795
    },
    {
      "epoch": 5.825,
      "grad_norm": 8.90792465209961,
      "learning_rate": 4.638888888888889e-06,
      "loss": 1.31,
      "step": 2796
    },
    {
      "epoch": 5.827083333333333,
      "grad_norm": 9.384858131408691,
      "learning_rate": 4.636574074074074e-06,
      "loss": 1.0139,
      "step": 2797
    },
    {
      "epoch": 5.829166666666667,
      "grad_norm": 95.63099670410156,
      "learning_rate": 4.634259259259259e-06,
      "loss": 0.7048,
      "step": 2798
    },
    {
      "epoch": 5.83125,
      "grad_norm": 9.258630752563477,
      "learning_rate": 4.631944444444445e-06,
      "loss": 0.699,
      "step": 2799
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 24.314348220825195,
      "learning_rate": 4.62962962962963e-06,
      "loss": 1.5071,
      "step": 2800
    },
    {
      "epoch": 5.835416666666667,
      "grad_norm": 8.217039108276367,
      "learning_rate": 4.627314814814815e-06,
      "loss": 1.1051,
      "step": 2801
    },
    {
      "epoch": 5.8375,
      "grad_norm": 18.228958129882812,
      "learning_rate": 4.625000000000001e-06,
      "loss": 1.4597,
      "step": 2802
    },
    {
      "epoch": 5.839583333333334,
      "grad_norm": 107.06999969482422,
      "learning_rate": 4.622685185185186e-06,
      "loss": 1.9304,
      "step": 2803
    },
    {
      "epoch": 5.841666666666667,
      "grad_norm": 20.98653793334961,
      "learning_rate": 4.620370370370371e-06,
      "loss": 1.3928,
      "step": 2804
    },
    {
      "epoch": 5.84375,
      "grad_norm": 9.025920867919922,
      "learning_rate": 4.618055555555556e-06,
      "loss": 1.1624,
      "step": 2805
    },
    {
      "epoch": 5.845833333333333,
      "grad_norm": 22.999303817749023,
      "learning_rate": 4.615740740740741e-06,
      "loss": 1.4807,
      "step": 2806
    },
    {
      "epoch": 5.847916666666666,
      "grad_norm": 29.290010452270508,
      "learning_rate": 4.613425925925926e-06,
      "loss": 1.5059,
      "step": 2807
    },
    {
      "epoch": 5.85,
      "grad_norm": 22.569719314575195,
      "learning_rate": 4.611111111111112e-06,
      "loss": 1.4885,
      "step": 2808
    },
    {
      "epoch": 5.852083333333333,
      "grad_norm": 4.860459804534912,
      "learning_rate": 4.608796296296297e-06,
      "loss": 0.1433,
      "step": 2809
    },
    {
      "epoch": 5.854166666666667,
      "grad_norm": 16.546104431152344,
      "learning_rate": 4.606481481481481e-06,
      "loss": 1.3051,
      "step": 2810
    },
    {
      "epoch": 5.85625,
      "grad_norm": 22.820817947387695,
      "learning_rate": 4.6041666666666665e-06,
      "loss": 1.3014,
      "step": 2811
    },
    {
      "epoch": 5.858333333333333,
      "grad_norm": 60.06962203979492,
      "learning_rate": 4.6018518518518524e-06,
      "loss": 1.1868,
      "step": 2812
    },
    {
      "epoch": 5.860416666666667,
      "grad_norm": 10.715192794799805,
      "learning_rate": 4.5995370370370375e-06,
      "loss": 0.5705,
      "step": 2813
    },
    {
      "epoch": 5.8625,
      "grad_norm": 28.731229782104492,
      "learning_rate": 4.597222222222223e-06,
      "loss": 1.3239,
      "step": 2814
    },
    {
      "epoch": 5.864583333333333,
      "grad_norm": 32.56473922729492,
      "learning_rate": 4.594907407407408e-06,
      "loss": 0.9592,
      "step": 2815
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 37.466487884521484,
      "learning_rate": 4.592592592592593e-06,
      "loss": 1.8241,
      "step": 2816
    },
    {
      "epoch": 5.86875,
      "grad_norm": 17.02167510986328,
      "learning_rate": 4.590277777777778e-06,
      "loss": 0.7791,
      "step": 2817
    },
    {
      "epoch": 5.870833333333334,
      "grad_norm": 24.635900497436523,
      "learning_rate": 4.587962962962964e-06,
      "loss": 1.4461,
      "step": 2818
    },
    {
      "epoch": 5.872916666666667,
      "grad_norm": 9.575042724609375,
      "learning_rate": 4.585648148148148e-06,
      "loss": 1.2845,
      "step": 2819
    },
    {
      "epoch": 5.875,
      "grad_norm": 19.365406036376953,
      "learning_rate": 4.583333333333333e-06,
      "loss": 1.2842,
      "step": 2820
    },
    {
      "epoch": 5.877083333333333,
      "grad_norm": 25.735794067382812,
      "learning_rate": 4.581018518518519e-06,
      "loss": 1.3947,
      "step": 2821
    },
    {
      "epoch": 5.879166666666666,
      "grad_norm": 10.562206268310547,
      "learning_rate": 4.578703703703704e-06,
      "loss": 0.4249,
      "step": 2822
    },
    {
      "epoch": 5.88125,
      "grad_norm": 10.528447151184082,
      "learning_rate": 4.576388888888889e-06,
      "loss": 1.1166,
      "step": 2823
    },
    {
      "epoch": 5.883333333333333,
      "grad_norm": 8.0790433883667,
      "learning_rate": 4.5740740740740745e-06,
      "loss": 0.6555,
      "step": 2824
    },
    {
      "epoch": 5.885416666666667,
      "grad_norm": 14.826759338378906,
      "learning_rate": 4.5717592592592595e-06,
      "loss": 1.3778,
      "step": 2825
    },
    {
      "epoch": 5.8875,
      "grad_norm": 18.33205223083496,
      "learning_rate": 4.569444444444445e-06,
      "loss": 1.815,
      "step": 2826
    },
    {
      "epoch": 5.889583333333333,
      "grad_norm": 9.231634140014648,
      "learning_rate": 4.567129629629631e-06,
      "loss": 1.0141,
      "step": 2827
    },
    {
      "epoch": 5.891666666666667,
      "grad_norm": 22.980802536010742,
      "learning_rate": 4.564814814814815e-06,
      "loss": 1.6013,
      "step": 2828
    },
    {
      "epoch": 5.89375,
      "grad_norm": 14.74045181274414,
      "learning_rate": 4.5625e-06,
      "loss": 1.5521,
      "step": 2829
    },
    {
      "epoch": 5.895833333333333,
      "grad_norm": 8.686094284057617,
      "learning_rate": 4.560185185185186e-06,
      "loss": 0.8074,
      "step": 2830
    },
    {
      "epoch": 5.897916666666667,
      "grad_norm": 7.739081859588623,
      "learning_rate": 4.557870370370371e-06,
      "loss": 0.5972,
      "step": 2831
    },
    {
      "epoch": 5.9,
      "grad_norm": 19.689043045043945,
      "learning_rate": 4.555555555555556e-06,
      "loss": 1.3172,
      "step": 2832
    },
    {
      "epoch": 5.902083333333334,
      "grad_norm": 24.22858428955078,
      "learning_rate": 4.553240740740741e-06,
      "loss": 1.3273,
      "step": 2833
    },
    {
      "epoch": 5.904166666666667,
      "grad_norm": 15.804856300354004,
      "learning_rate": 4.550925925925926e-06,
      "loss": 1.0668,
      "step": 2834
    },
    {
      "epoch": 5.90625,
      "grad_norm": 13.817487716674805,
      "learning_rate": 4.548611111111111e-06,
      "loss": 0.8506,
      "step": 2835
    },
    {
      "epoch": 5.908333333333333,
      "grad_norm": 16.48790740966797,
      "learning_rate": 4.5462962962962965e-06,
      "loss": 1.2127,
      "step": 2836
    },
    {
      "epoch": 5.910416666666666,
      "grad_norm": 16.132413864135742,
      "learning_rate": 4.543981481481482e-06,
      "loss": 1.3943,
      "step": 2837
    },
    {
      "epoch": 5.9125,
      "grad_norm": 6.889290809631348,
      "learning_rate": 4.541666666666667e-06,
      "loss": 0.7362,
      "step": 2838
    },
    {
      "epoch": 5.914583333333333,
      "grad_norm": 12.339731216430664,
      "learning_rate": 4.539351851851852e-06,
      "loss": 1.4297,
      "step": 2839
    },
    {
      "epoch": 5.916666666666667,
      "grad_norm": 9.169382095336914,
      "learning_rate": 4.537037037037038e-06,
      "loss": 1.1471,
      "step": 2840
    },
    {
      "epoch": 5.91875,
      "grad_norm": 21.453214645385742,
      "learning_rate": 4.534722222222223e-06,
      "loss": 1.7495,
      "step": 2841
    },
    {
      "epoch": 5.920833333333333,
      "grad_norm": 42.65414810180664,
      "learning_rate": 4.532407407407408e-06,
      "loss": 1.1805,
      "step": 2842
    },
    {
      "epoch": 5.922916666666667,
      "grad_norm": 15.233748435974121,
      "learning_rate": 4.530092592592593e-06,
      "loss": 1.5192,
      "step": 2843
    },
    {
      "epoch": 5.925,
      "grad_norm": 8.981579780578613,
      "learning_rate": 4.527777777777778e-06,
      "loss": 0.656,
      "step": 2844
    },
    {
      "epoch": 5.927083333333333,
      "grad_norm": 21.206544876098633,
      "learning_rate": 4.525462962962963e-06,
      "loss": 0.1863,
      "step": 2845
    },
    {
      "epoch": 5.929166666666667,
      "grad_norm": 25.460651397705078,
      "learning_rate": 4.523148148148149e-06,
      "loss": 1.6707,
      "step": 2846
    },
    {
      "epoch": 5.93125,
      "grad_norm": 10.335920333862305,
      "learning_rate": 4.520833333333333e-06,
      "loss": 1.2639,
      "step": 2847
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 10.045199394226074,
      "learning_rate": 4.5185185185185185e-06,
      "loss": 0.7406,
      "step": 2848
    },
    {
      "epoch": 5.935416666666667,
      "grad_norm": 12.78046989440918,
      "learning_rate": 4.5162037037037044e-06,
      "loss": 0.9435,
      "step": 2849
    },
    {
      "epoch": 5.9375,
      "grad_norm": 10.512784957885742,
      "learning_rate": 4.5138888888888895e-06,
      "loss": 0.6839,
      "step": 2850
    },
    {
      "epoch": 5.939583333333333,
      "grad_norm": 45.936885833740234,
      "learning_rate": 4.511574074074075e-06,
      "loss": 0.9728,
      "step": 2851
    },
    {
      "epoch": 5.941666666666666,
      "grad_norm": 11.837201118469238,
      "learning_rate": 4.50925925925926e-06,
      "loss": 0.7062,
      "step": 2852
    },
    {
      "epoch": 5.94375,
      "grad_norm": 12.291709899902344,
      "learning_rate": 4.506944444444445e-06,
      "loss": 1.2584,
      "step": 2853
    },
    {
      "epoch": 5.945833333333333,
      "grad_norm": 17.9326114654541,
      "learning_rate": 4.50462962962963e-06,
      "loss": 1.0827,
      "step": 2854
    },
    {
      "epoch": 5.947916666666667,
      "grad_norm": 13.595171928405762,
      "learning_rate": 4.502314814814815e-06,
      "loss": 0.8324,
      "step": 2855
    },
    {
      "epoch": 5.95,
      "grad_norm": 6.602126598358154,
      "learning_rate": 4.5e-06,
      "loss": 0.9177,
      "step": 2856
    },
    {
      "epoch": 5.952083333333333,
      "grad_norm": 32.136356353759766,
      "learning_rate": 4.497685185185185e-06,
      "loss": 1.7277,
      "step": 2857
    },
    {
      "epoch": 5.954166666666667,
      "grad_norm": 20.238574981689453,
      "learning_rate": 4.49537037037037e-06,
      "loss": 0.9128,
      "step": 2858
    },
    {
      "epoch": 5.95625,
      "grad_norm": 16.04736328125,
      "learning_rate": 4.493055555555556e-06,
      "loss": 2.0569,
      "step": 2859
    },
    {
      "epoch": 5.958333333333333,
      "grad_norm": 25.44434356689453,
      "learning_rate": 4.490740740740741e-06,
      "loss": 1.4513,
      "step": 2860
    },
    {
      "epoch": 5.960416666666667,
      "grad_norm": 4.920794486999512,
      "learning_rate": 4.4884259259259264e-06,
      "loss": 0.733,
      "step": 2861
    },
    {
      "epoch": 5.9625,
      "grad_norm": 26.77866554260254,
      "learning_rate": 4.4861111111111115e-06,
      "loss": 0.8563,
      "step": 2862
    },
    {
      "epoch": 5.964583333333334,
      "grad_norm": 21.517976760864258,
      "learning_rate": 4.483796296296297e-06,
      "loss": 1.8548,
      "step": 2863
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 8.774205207824707,
      "learning_rate": 4.481481481481482e-06,
      "loss": 0.4909,
      "step": 2864
    },
    {
      "epoch": 5.96875,
      "grad_norm": 111.09510803222656,
      "learning_rate": 4.479166666666667e-06,
      "loss": 0.873,
      "step": 2865
    },
    {
      "epoch": 5.970833333333333,
      "grad_norm": 15.209474563598633,
      "learning_rate": 4.476851851851852e-06,
      "loss": 0.8177,
      "step": 2866
    },
    {
      "epoch": 5.972916666666666,
      "grad_norm": 24.27428436279297,
      "learning_rate": 4.474537037037037e-06,
      "loss": 0.8887,
      "step": 2867
    },
    {
      "epoch": 5.975,
      "grad_norm": 7.383584976196289,
      "learning_rate": 4.472222222222223e-06,
      "loss": 0.9184,
      "step": 2868
    },
    {
      "epoch": 5.977083333333333,
      "grad_norm": 5.191170692443848,
      "learning_rate": 4.469907407407408e-06,
      "loss": 0.1595,
      "step": 2869
    },
    {
      "epoch": 5.979166666666667,
      "grad_norm": 10.012822151184082,
      "learning_rate": 4.467592592592593e-06,
      "loss": 0.4335,
      "step": 2870
    },
    {
      "epoch": 5.98125,
      "grad_norm": 7.80019998550415,
      "learning_rate": 4.465277777777778e-06,
      "loss": 0.5835,
      "step": 2871
    },
    {
      "epoch": 5.983333333333333,
      "grad_norm": 16.510177612304688,
      "learning_rate": 4.462962962962963e-06,
      "loss": 1.8946,
      "step": 2872
    },
    {
      "epoch": 5.985416666666667,
      "grad_norm": 37.30329132080078,
      "learning_rate": 4.4606481481481485e-06,
      "loss": 1.5822,
      "step": 2873
    },
    {
      "epoch": 5.9875,
      "grad_norm": 24.282039642333984,
      "learning_rate": 4.4583333333333336e-06,
      "loss": 0.932,
      "step": 2874
    },
    {
      "epoch": 5.989583333333333,
      "grad_norm": 31.674753189086914,
      "learning_rate": 4.456018518518519e-06,
      "loss": 1.9051,
      "step": 2875
    },
    {
      "epoch": 5.991666666666667,
      "grad_norm": 17.48206329345703,
      "learning_rate": 4.453703703703704e-06,
      "loss": 0.9833,
      "step": 2876
    },
    {
      "epoch": 5.99375,
      "grad_norm": 31.02043914794922,
      "learning_rate": 4.451388888888889e-06,
      "loss": 0.9553,
      "step": 2877
    },
    {
      "epoch": 5.995833333333334,
      "grad_norm": 5.142011642456055,
      "learning_rate": 4.449074074074075e-06,
      "loss": 0.1574,
      "step": 2878
    },
    {
      "epoch": 5.997916666666667,
      "grad_norm": 19.52954864501953,
      "learning_rate": 4.44675925925926e-06,
      "loss": 0.9216,
      "step": 2879
    },
    {
      "epoch": 6.0,
      "grad_norm": 17.905029296875,
      "learning_rate": 4.444444444444444e-06,
      "loss": 1.0844,
      "step": 2880
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.6277777777777778,
      "eval_f1": 0.5552577371509153,
      "eval_loss": 1.2895019054412842,
      "eval_runtime": 34.5134,
      "eval_samples_per_second": 5.215,
      "eval_steps_per_second": 2.608,
      "step": 2880
    },
    {
      "epoch": 6.002083333333333,
      "grad_norm": 8.7665433883667,
      "learning_rate": 4.44212962962963e-06,
      "loss": 0.6799,
      "step": 2881
    },
    {
      "epoch": 6.004166666666666,
      "grad_norm": 11.939504623413086,
      "learning_rate": 4.439814814814815e-06,
      "loss": 0.882,
      "step": 2882
    },
    {
      "epoch": 6.00625,
      "grad_norm": 18.8917236328125,
      "learning_rate": 4.4375e-06,
      "loss": 1.7767,
      "step": 2883
    },
    {
      "epoch": 6.008333333333334,
      "grad_norm": 23.400270462036133,
      "learning_rate": 4.435185185185185e-06,
      "loss": 0.9103,
      "step": 2884
    },
    {
      "epoch": 6.010416666666667,
      "grad_norm": 12.123885154724121,
      "learning_rate": 4.4328703703703705e-06,
      "loss": 0.9652,
      "step": 2885
    },
    {
      "epoch": 6.0125,
      "grad_norm": 4.983949184417725,
      "learning_rate": 4.430555555555556e-06,
      "loss": 0.272,
      "step": 2886
    },
    {
      "epoch": 6.014583333333333,
      "grad_norm": 11.62995433807373,
      "learning_rate": 4.4282407407407415e-06,
      "loss": 0.9058,
      "step": 2887
    },
    {
      "epoch": 6.016666666666667,
      "grad_norm": 20.12734031677246,
      "learning_rate": 4.425925925925927e-06,
      "loss": 1.5884,
      "step": 2888
    },
    {
      "epoch": 6.01875,
      "grad_norm": 5.826562404632568,
      "learning_rate": 4.423611111111112e-06,
      "loss": 0.8335,
      "step": 2889
    },
    {
      "epoch": 6.020833333333333,
      "grad_norm": 7.424048900604248,
      "learning_rate": 4.421296296296297e-06,
      "loss": 1.0957,
      "step": 2890
    },
    {
      "epoch": 6.022916666666666,
      "grad_norm": 8.860517501831055,
      "learning_rate": 4.418981481481482e-06,
      "loss": 0.743,
      "step": 2891
    },
    {
      "epoch": 6.025,
      "grad_norm": 16.2850284576416,
      "learning_rate": 4.416666666666667e-06,
      "loss": 1.3304,
      "step": 2892
    },
    {
      "epoch": 6.027083333333334,
      "grad_norm": 15.404389381408691,
      "learning_rate": 4.414351851851852e-06,
      "loss": 0.6676,
      "step": 2893
    },
    {
      "epoch": 6.029166666666667,
      "grad_norm": 9.890554428100586,
      "learning_rate": 4.412037037037037e-06,
      "loss": 1.1398,
      "step": 2894
    },
    {
      "epoch": 6.03125,
      "grad_norm": 16.619054794311523,
      "learning_rate": 4.409722222222222e-06,
      "loss": 2.1522,
      "step": 2895
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 13.402583122253418,
      "learning_rate": 4.407407407407408e-06,
      "loss": 1.3134,
      "step": 2896
    },
    {
      "epoch": 6.035416666666666,
      "grad_norm": 36.22852325439453,
      "learning_rate": 4.405092592592593e-06,
      "loss": 0.4927,
      "step": 2897
    },
    {
      "epoch": 6.0375,
      "grad_norm": 12.019258499145508,
      "learning_rate": 4.4027777777777784e-06,
      "loss": 1.2741,
      "step": 2898
    },
    {
      "epoch": 6.039583333333334,
      "grad_norm": 12.671988487243652,
      "learning_rate": 4.400462962962963e-06,
      "loss": 1.4725,
      "step": 2899
    },
    {
      "epoch": 6.041666666666667,
      "grad_norm": 9.951126098632812,
      "learning_rate": 4.398148148148149e-06,
      "loss": 0.4116,
      "step": 2900
    },
    {
      "epoch": 6.04375,
      "grad_norm": 22.39997100830078,
      "learning_rate": 4.395833333333334e-06,
      "loss": 0.8242,
      "step": 2901
    },
    {
      "epoch": 6.045833333333333,
      "grad_norm": 28.2477970123291,
      "learning_rate": 4.393518518518519e-06,
      "loss": 1.6115,
      "step": 2902
    },
    {
      "epoch": 6.047916666666667,
      "grad_norm": 53.823055267333984,
      "learning_rate": 4.391203703703704e-06,
      "loss": 1.5474,
      "step": 2903
    },
    {
      "epoch": 6.05,
      "grad_norm": 38.35490036010742,
      "learning_rate": 4.388888888888889e-06,
      "loss": 0.8727,
      "step": 2904
    },
    {
      "epoch": 6.052083333333333,
      "grad_norm": 160.52297973632812,
      "learning_rate": 4.386574074074074e-06,
      "loss": 1.5923,
      "step": 2905
    },
    {
      "epoch": 6.054166666666666,
      "grad_norm": 29.945802688598633,
      "learning_rate": 4.38425925925926e-06,
      "loss": 1.4146,
      "step": 2906
    },
    {
      "epoch": 6.05625,
      "grad_norm": 18.497787475585938,
      "learning_rate": 4.381944444444445e-06,
      "loss": 1.1925,
      "step": 2907
    },
    {
      "epoch": 6.058333333333334,
      "grad_norm": 27.0083065032959,
      "learning_rate": 4.379629629629629e-06,
      "loss": 1.4803,
      "step": 2908
    },
    {
      "epoch": 6.060416666666667,
      "grad_norm": 59.66283416748047,
      "learning_rate": 4.377314814814815e-06,
      "loss": 1.5072,
      "step": 2909
    },
    {
      "epoch": 6.0625,
      "grad_norm": 11.978654861450195,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 0.7883,
      "step": 2910
    },
    {
      "epoch": 6.064583333333333,
      "grad_norm": 30.198665618896484,
      "learning_rate": 4.3726851851851856e-06,
      "loss": 0.7979,
      "step": 2911
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 29.121559143066406,
      "learning_rate": 4.370370370370371e-06,
      "loss": 1.6698,
      "step": 2912
    },
    {
      "epoch": 6.06875,
      "grad_norm": 36.873252868652344,
      "learning_rate": 4.368055555555556e-06,
      "loss": 1.1478,
      "step": 2913
    },
    {
      "epoch": 6.070833333333334,
      "grad_norm": 13.539698600769043,
      "learning_rate": 4.365740740740741e-06,
      "loss": 1.4481,
      "step": 2914
    },
    {
      "epoch": 6.072916666666667,
      "grad_norm": 11.989889144897461,
      "learning_rate": 4.363425925925927e-06,
      "loss": 1.1692,
      "step": 2915
    },
    {
      "epoch": 6.075,
      "grad_norm": 12.610368728637695,
      "learning_rate": 4.361111111111112e-06,
      "loss": 0.8804,
      "step": 2916
    },
    {
      "epoch": 6.077083333333333,
      "grad_norm": 63.6255989074707,
      "learning_rate": 4.358796296296296e-06,
      "loss": 0.2207,
      "step": 2917
    },
    {
      "epoch": 6.079166666666667,
      "grad_norm": 15.103975296020508,
      "learning_rate": 4.356481481481482e-06,
      "loss": 1.2909,
      "step": 2918
    },
    {
      "epoch": 6.08125,
      "grad_norm": 23.69550895690918,
      "learning_rate": 4.354166666666667e-06,
      "loss": 1.4642,
      "step": 2919
    },
    {
      "epoch": 6.083333333333333,
      "grad_norm": 4.9653449058532715,
      "learning_rate": 4.351851851851852e-06,
      "loss": 0.1494,
      "step": 2920
    },
    {
      "epoch": 6.085416666666666,
      "grad_norm": 15.97114372253418,
      "learning_rate": 4.349537037037037e-06,
      "loss": 1.0343,
      "step": 2921
    },
    {
      "epoch": 6.0875,
      "grad_norm": 13.527135848999023,
      "learning_rate": 4.3472222222222225e-06,
      "loss": 1.4726,
      "step": 2922
    },
    {
      "epoch": 6.089583333333334,
      "grad_norm": 30.885793685913086,
      "learning_rate": 4.344907407407408e-06,
      "loss": 1.2406,
      "step": 2923
    },
    {
      "epoch": 6.091666666666667,
      "grad_norm": 7.63778829574585,
      "learning_rate": 4.342592592592593e-06,
      "loss": 1.0694,
      "step": 2924
    },
    {
      "epoch": 6.09375,
      "grad_norm": 5.1197710037231445,
      "learning_rate": 4.340277777777779e-06,
      "loss": 0.2637,
      "step": 2925
    },
    {
      "epoch": 6.095833333333333,
      "grad_norm": 5.568576812744141,
      "learning_rate": 4.337962962962963e-06,
      "loss": 0.7691,
      "step": 2926
    },
    {
      "epoch": 6.097916666666666,
      "grad_norm": 20.48776626586914,
      "learning_rate": 4.335648148148148e-06,
      "loss": 1.3023,
      "step": 2927
    },
    {
      "epoch": 6.1,
      "grad_norm": 18.01279640197754,
      "learning_rate": 4.333333333333334e-06,
      "loss": 1.5459,
      "step": 2928
    },
    {
      "epoch": 6.102083333333334,
      "grad_norm": 8.244665145874023,
      "learning_rate": 4.331018518518519e-06,
      "loss": 1.211,
      "step": 2929
    },
    {
      "epoch": 6.104166666666667,
      "grad_norm": 21.76506805419922,
      "learning_rate": 4.328703703703704e-06,
      "loss": 0.7405,
      "step": 2930
    },
    {
      "epoch": 6.10625,
      "grad_norm": 30.90446662902832,
      "learning_rate": 4.326388888888889e-06,
      "loss": 1.534,
      "step": 2931
    },
    {
      "epoch": 6.108333333333333,
      "grad_norm": 17.115453720092773,
      "learning_rate": 4.324074074074074e-06,
      "loss": 0.7043,
      "step": 2932
    },
    {
      "epoch": 6.110416666666667,
      "grad_norm": 11.737861633300781,
      "learning_rate": 4.321759259259259e-06,
      "loss": 0.9036,
      "step": 2933
    },
    {
      "epoch": 6.1125,
      "grad_norm": 36.08675765991211,
      "learning_rate": 4.319444444444445e-06,
      "loss": 1.5075,
      "step": 2934
    },
    {
      "epoch": 6.114583333333333,
      "grad_norm": 5.799654006958008,
      "learning_rate": 4.31712962962963e-06,
      "loss": 0.7062,
      "step": 2935
    },
    {
      "epoch": 6.116666666666666,
      "grad_norm": 14.385897636413574,
      "learning_rate": 4.314814814814815e-06,
      "loss": 1.237,
      "step": 2936
    },
    {
      "epoch": 6.11875,
      "grad_norm": 15.112515449523926,
      "learning_rate": 4.312500000000001e-06,
      "loss": 1.8683,
      "step": 2937
    },
    {
      "epoch": 6.120833333333334,
      "grad_norm": 8.756675720214844,
      "learning_rate": 4.310185185185186e-06,
      "loss": 0.8335,
      "step": 2938
    },
    {
      "epoch": 6.122916666666667,
      "grad_norm": 9.553284645080566,
      "learning_rate": 4.307870370370371e-06,
      "loss": 0.7079,
      "step": 2939
    },
    {
      "epoch": 6.125,
      "grad_norm": 11.395082473754883,
      "learning_rate": 4.305555555555556e-06,
      "loss": 1.3181,
      "step": 2940
    },
    {
      "epoch": 6.127083333333333,
      "grad_norm": 27.97689437866211,
      "learning_rate": 4.303240740740741e-06,
      "loss": 1.9844,
      "step": 2941
    },
    {
      "epoch": 6.129166666666666,
      "grad_norm": 9.822296142578125,
      "learning_rate": 4.300925925925926e-06,
      "loss": 1.2388,
      "step": 2942
    },
    {
      "epoch": 6.13125,
      "grad_norm": 6.5377116203308105,
      "learning_rate": 4.298611111111112e-06,
      "loss": 0.6721,
      "step": 2943
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 9.982207298278809,
      "learning_rate": 4.296296296296296e-06,
      "loss": 1.0144,
      "step": 2944
    },
    {
      "epoch": 6.135416666666667,
      "grad_norm": 62.7738037109375,
      "learning_rate": 4.293981481481481e-06,
      "loss": 1.0852,
      "step": 2945
    },
    {
      "epoch": 6.1375,
      "grad_norm": 16.856422424316406,
      "learning_rate": 4.2916666666666665e-06,
      "loss": 1.0317,
      "step": 2946
    },
    {
      "epoch": 6.139583333333333,
      "grad_norm": 13.126379013061523,
      "learning_rate": 4.2893518518518525e-06,
      "loss": 0.8481,
      "step": 2947
    },
    {
      "epoch": 6.141666666666667,
      "grad_norm": 15.274932861328125,
      "learning_rate": 4.2870370370370376e-06,
      "loss": 0.8592,
      "step": 2948
    },
    {
      "epoch": 6.14375,
      "grad_norm": 12.743634223937988,
      "learning_rate": 4.284722222222223e-06,
      "loss": 1.1584,
      "step": 2949
    },
    {
      "epoch": 6.145833333333333,
      "grad_norm": 25.491640090942383,
      "learning_rate": 4.282407407407408e-06,
      "loss": 1.7011,
      "step": 2950
    },
    {
      "epoch": 6.147916666666666,
      "grad_norm": 10.494083404541016,
      "learning_rate": 4.280092592592593e-06,
      "loss": 1.1728,
      "step": 2951
    },
    {
      "epoch": 6.15,
      "grad_norm": 14.355101585388184,
      "learning_rate": 4.277777777777778e-06,
      "loss": 1.0598,
      "step": 2952
    },
    {
      "epoch": 6.152083333333334,
      "grad_norm": 4.257461071014404,
      "learning_rate": 4.275462962962964e-06,
      "loss": 0.1241,
      "step": 2953
    },
    {
      "epoch": 6.154166666666667,
      "grad_norm": 21.296586990356445,
      "learning_rate": 4.273148148148148e-06,
      "loss": 1.3963,
      "step": 2954
    },
    {
      "epoch": 6.15625,
      "grad_norm": 16.465360641479492,
      "learning_rate": 4.270833333333333e-06,
      "loss": 1.4871,
      "step": 2955
    },
    {
      "epoch": 6.158333333333333,
      "grad_norm": 7.191075325012207,
      "learning_rate": 4.268518518518519e-06,
      "loss": 0.1805,
      "step": 2956
    },
    {
      "epoch": 6.160416666666666,
      "grad_norm": 11.167840003967285,
      "learning_rate": 4.266203703703704e-06,
      "loss": 0.9537,
      "step": 2957
    },
    {
      "epoch": 6.1625,
      "grad_norm": 12.910266876220703,
      "learning_rate": 4.263888888888889e-06,
      "loss": 1.7322,
      "step": 2958
    },
    {
      "epoch": 6.164583333333334,
      "grad_norm": 67.86544036865234,
      "learning_rate": 4.2615740740740745e-06,
      "loss": 0.351,
      "step": 2959
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 42.12648010253906,
      "learning_rate": 4.2592592592592596e-06,
      "loss": 1.1051,
      "step": 2960
    },
    {
      "epoch": 6.16875,
      "grad_norm": 14.071833610534668,
      "learning_rate": 4.256944444444445e-06,
      "loss": 0.8128,
      "step": 2961
    },
    {
      "epoch": 6.170833333333333,
      "grad_norm": 25.526153564453125,
      "learning_rate": 4.254629629629631e-06,
      "loss": 1.8887,
      "step": 2962
    },
    {
      "epoch": 6.172916666666667,
      "grad_norm": 70.84503173828125,
      "learning_rate": 4.252314814814815e-06,
      "loss": 1.5052,
      "step": 2963
    },
    {
      "epoch": 6.175,
      "grad_norm": 17.89327621459961,
      "learning_rate": 4.25e-06,
      "loss": 1.406,
      "step": 2964
    },
    {
      "epoch": 6.177083333333333,
      "grad_norm": 5.140458583831787,
      "learning_rate": 4.247685185185186e-06,
      "loss": 0.6902,
      "step": 2965
    },
    {
      "epoch": 6.179166666666666,
      "grad_norm": 8.645159721374512,
      "learning_rate": 4.245370370370371e-06,
      "loss": 0.7212,
      "step": 2966
    },
    {
      "epoch": 6.18125,
      "grad_norm": 8.283159255981445,
      "learning_rate": 4.243055555555556e-06,
      "loss": 1.0437,
      "step": 2967
    },
    {
      "epoch": 6.183333333333334,
      "grad_norm": 22.45296859741211,
      "learning_rate": 4.240740740740741e-06,
      "loss": 1.9751,
      "step": 2968
    },
    {
      "epoch": 6.185416666666667,
      "grad_norm": 46.009300231933594,
      "learning_rate": 4.238425925925926e-06,
      "loss": 1.2869,
      "step": 2969
    },
    {
      "epoch": 6.1875,
      "grad_norm": 8.965004920959473,
      "learning_rate": 4.236111111111111e-06,
      "loss": 0.7429,
      "step": 2970
    },
    {
      "epoch": 6.189583333333333,
      "grad_norm": 10.372340202331543,
      "learning_rate": 4.2337962962962965e-06,
      "loss": 0.3071,
      "step": 2971
    },
    {
      "epoch": 6.191666666666666,
      "grad_norm": 14.535524368286133,
      "learning_rate": 4.231481481481482e-06,
      "loss": 1.3821,
      "step": 2972
    },
    {
      "epoch": 6.19375,
      "grad_norm": 108.0166015625,
      "learning_rate": 4.229166666666667e-06,
      "loss": 1.2871,
      "step": 2973
    },
    {
      "epoch": 6.195833333333334,
      "grad_norm": 36.01094055175781,
      "learning_rate": 4.226851851851852e-06,
      "loss": 1.4352,
      "step": 2974
    },
    {
      "epoch": 6.197916666666667,
      "grad_norm": 9.58947467803955,
      "learning_rate": 4.224537037037038e-06,
      "loss": 0.7031,
      "step": 2975
    },
    {
      "epoch": 6.2,
      "grad_norm": 15.325373649597168,
      "learning_rate": 4.222222222222223e-06,
      "loss": 1.0788,
      "step": 2976
    },
    {
      "epoch": 6.202083333333333,
      "grad_norm": 9.242880821228027,
      "learning_rate": 4.219907407407408e-06,
      "loss": 0.7857,
      "step": 2977
    },
    {
      "epoch": 6.204166666666667,
      "grad_norm": 20.11946678161621,
      "learning_rate": 4.217592592592593e-06,
      "loss": 1.9669,
      "step": 2978
    },
    {
      "epoch": 6.20625,
      "grad_norm": 46.97515869140625,
      "learning_rate": 4.215277777777778e-06,
      "loss": 1.5338,
      "step": 2979
    },
    {
      "epoch": 6.208333333333333,
      "grad_norm": 4.632361888885498,
      "learning_rate": 4.212962962962963e-06,
      "loss": 0.1437,
      "step": 2980
    },
    {
      "epoch": 6.210416666666666,
      "grad_norm": 54.89546203613281,
      "learning_rate": 4.210648148148148e-06,
      "loss": 1.9568,
      "step": 2981
    },
    {
      "epoch": 6.2125,
      "grad_norm": 11.442172050476074,
      "learning_rate": 4.208333333333333e-06,
      "loss": 1.4893,
      "step": 2982
    },
    {
      "epoch": 6.214583333333334,
      "grad_norm": 8.517104148864746,
      "learning_rate": 4.2060185185185185e-06,
      "loss": 0.6904,
      "step": 2983
    },
    {
      "epoch": 6.216666666666667,
      "grad_norm": 19.831621170043945,
      "learning_rate": 4.2037037037037045e-06,
      "loss": 1.5716,
      "step": 2984
    },
    {
      "epoch": 6.21875,
      "grad_norm": 8.767678260803223,
      "learning_rate": 4.2013888888888896e-06,
      "loss": 0.655,
      "step": 2985
    },
    {
      "epoch": 6.220833333333333,
      "grad_norm": 27.32539176940918,
      "learning_rate": 4.199074074074075e-06,
      "loss": 0.8615,
      "step": 2986
    },
    {
      "epoch": 6.222916666666666,
      "grad_norm": 38.89568328857422,
      "learning_rate": 4.19675925925926e-06,
      "loss": 1.3512,
      "step": 2987
    },
    {
      "epoch": 6.225,
      "grad_norm": 12.564358711242676,
      "learning_rate": 4.194444444444445e-06,
      "loss": 1.3001,
      "step": 2988
    },
    {
      "epoch": 6.227083333333334,
      "grad_norm": 40.29719543457031,
      "learning_rate": 4.19212962962963e-06,
      "loss": 1.0746,
      "step": 2989
    },
    {
      "epoch": 6.229166666666667,
      "grad_norm": 34.91436767578125,
      "learning_rate": 4.189814814814815e-06,
      "loss": 1.4043,
      "step": 2990
    },
    {
      "epoch": 6.23125,
      "grad_norm": 33.94633483886719,
      "learning_rate": 4.1875e-06,
      "loss": 0.9992,
      "step": 2991
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 11.650480270385742,
      "learning_rate": 4.185185185185185e-06,
      "loss": 0.4496,
      "step": 2992
    },
    {
      "epoch": 6.235416666666667,
      "grad_norm": 22.932706832885742,
      "learning_rate": 4.18287037037037e-06,
      "loss": 0.1683,
      "step": 2993
    },
    {
      "epoch": 6.2375,
      "grad_norm": 19.98859405517578,
      "learning_rate": 4.180555555555556e-06,
      "loss": 1.8551,
      "step": 2994
    },
    {
      "epoch": 6.239583333333333,
      "grad_norm": 16.5396728515625,
      "learning_rate": 4.178240740740741e-06,
      "loss": 1.2581,
      "step": 2995
    },
    {
      "epoch": 6.241666666666666,
      "grad_norm": 40.34981918334961,
      "learning_rate": 4.175925925925926e-06,
      "loss": 1.829,
      "step": 2996
    },
    {
      "epoch": 6.24375,
      "grad_norm": 8.862003326416016,
      "learning_rate": 4.1736111111111116e-06,
      "loss": 0.6148,
      "step": 2997
    },
    {
      "epoch": 6.245833333333334,
      "grad_norm": 4.719166278839111,
      "learning_rate": 4.171296296296297e-06,
      "loss": 0.1413,
      "step": 2998
    },
    {
      "epoch": 6.247916666666667,
      "grad_norm": 16.89748764038086,
      "learning_rate": 4.168981481481482e-06,
      "loss": 1.0401,
      "step": 2999
    },
    {
      "epoch": 6.25,
      "grad_norm": 11.696582794189453,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.9152,
      "step": 3000
    },
    {
      "epoch": 6.252083333333333,
      "grad_norm": 12.423206329345703,
      "learning_rate": 4.164351851851852e-06,
      "loss": 1.1486,
      "step": 3001
    },
    {
      "epoch": 6.254166666666666,
      "grad_norm": 24.42853355407715,
      "learning_rate": 4.162037037037037e-06,
      "loss": 1.4652,
      "step": 3002
    },
    {
      "epoch": 6.25625,
      "grad_norm": 9.07982349395752,
      "learning_rate": 4.159722222222223e-06,
      "loss": 0.6302,
      "step": 3003
    },
    {
      "epoch": 6.258333333333334,
      "grad_norm": 19.893081665039062,
      "learning_rate": 4.157407407407408e-06,
      "loss": 1.294,
      "step": 3004
    },
    {
      "epoch": 6.260416666666667,
      "grad_norm": 161.09129333496094,
      "learning_rate": 4.155092592592593e-06,
      "loss": 1.3262,
      "step": 3005
    },
    {
      "epoch": 6.2625,
      "grad_norm": 7.767256259918213,
      "learning_rate": 4.152777777777778e-06,
      "loss": 0.6532,
      "step": 3006
    },
    {
      "epoch": 6.264583333333333,
      "grad_norm": 37.459938049316406,
      "learning_rate": 4.150462962962963e-06,
      "loss": 1.7967,
      "step": 3007
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 45.317405700683594,
      "learning_rate": 4.1481481481481485e-06,
      "loss": 1.9415,
      "step": 3008
    },
    {
      "epoch": 6.26875,
      "grad_norm": 14.803206443786621,
      "learning_rate": 4.145833333333334e-06,
      "loss": 1.7547,
      "step": 3009
    },
    {
      "epoch": 6.270833333333333,
      "grad_norm": 16.865493774414062,
      "learning_rate": 4.143518518518519e-06,
      "loss": 1.0155,
      "step": 3010
    },
    {
      "epoch": 6.272916666666666,
      "grad_norm": 13.490784645080566,
      "learning_rate": 4.141203703703704e-06,
      "loss": 0.898,
      "step": 3011
    },
    {
      "epoch": 6.275,
      "grad_norm": 31.798460006713867,
      "learning_rate": 4.138888888888889e-06,
      "loss": 1.9458,
      "step": 3012
    },
    {
      "epoch": 6.277083333333334,
      "grad_norm": 26.672563552856445,
      "learning_rate": 4.136574074074075e-06,
      "loss": 1.3887,
      "step": 3013
    },
    {
      "epoch": 6.279166666666667,
      "grad_norm": 46.58518600463867,
      "learning_rate": 4.13425925925926e-06,
      "loss": 0.6772,
      "step": 3014
    },
    {
      "epoch": 6.28125,
      "grad_norm": 8.119582176208496,
      "learning_rate": 4.131944444444444e-06,
      "loss": 0.8011,
      "step": 3015
    },
    {
      "epoch": 6.283333333333333,
      "grad_norm": 54.33451461791992,
      "learning_rate": 4.12962962962963e-06,
      "loss": 1.7296,
      "step": 3016
    },
    {
      "epoch": 6.285416666666666,
      "grad_norm": 14.032449722290039,
      "learning_rate": 4.127314814814815e-06,
      "loss": 1.3554,
      "step": 3017
    },
    {
      "epoch": 6.2875,
      "grad_norm": 3.9723615646362305,
      "learning_rate": 4.125e-06,
      "loss": 0.1147,
      "step": 3018
    },
    {
      "epoch": 6.289583333333334,
      "grad_norm": 7.03874397277832,
      "learning_rate": 4.122685185185185e-06,
      "loss": 0.7907,
      "step": 3019
    },
    {
      "epoch": 6.291666666666667,
      "grad_norm": 8.94016170501709,
      "learning_rate": 4.1203703703703705e-06,
      "loss": 0.8385,
      "step": 3020
    },
    {
      "epoch": 6.29375,
      "grad_norm": 19.25103759765625,
      "learning_rate": 4.118055555555556e-06,
      "loss": 1.7276,
      "step": 3021
    },
    {
      "epoch": 6.295833333333333,
      "grad_norm": 11.615815162658691,
      "learning_rate": 4.1157407407407416e-06,
      "loss": 0.8874,
      "step": 3022
    },
    {
      "epoch": 6.297916666666667,
      "grad_norm": 16.02294158935547,
      "learning_rate": 4.113425925925927e-06,
      "loss": 0.9945,
      "step": 3023
    },
    {
      "epoch": 6.3,
      "grad_norm": 30.493412017822266,
      "learning_rate": 4.111111111111111e-06,
      "loss": 1.1827,
      "step": 3024
    },
    {
      "epoch": 6.302083333333333,
      "grad_norm": 32.04960632324219,
      "learning_rate": 4.108796296296297e-06,
      "loss": 1.5302,
      "step": 3025
    },
    {
      "epoch": 6.304166666666666,
      "grad_norm": 18.20003890991211,
      "learning_rate": 4.106481481481482e-06,
      "loss": 1.4644,
      "step": 3026
    },
    {
      "epoch": 6.30625,
      "grad_norm": 9.199517250061035,
      "learning_rate": 4.104166666666667e-06,
      "loss": 0.6128,
      "step": 3027
    },
    {
      "epoch": 6.308333333333334,
      "grad_norm": 43.7747802734375,
      "learning_rate": 4.101851851851852e-06,
      "loss": 1.6319,
      "step": 3028
    },
    {
      "epoch": 6.310416666666667,
      "grad_norm": 6.507363319396973,
      "learning_rate": 4.099537037037037e-06,
      "loss": 0.6795,
      "step": 3029
    },
    {
      "epoch": 6.3125,
      "grad_norm": 16.57674789428711,
      "learning_rate": 4.097222222222222e-06,
      "loss": 1.2997,
      "step": 3030
    },
    {
      "epoch": 6.314583333333333,
      "grad_norm": 10.211713790893555,
      "learning_rate": 4.094907407407408e-06,
      "loss": 1.2022,
      "step": 3031
    },
    {
      "epoch": 6.316666666666666,
      "grad_norm": 27.377578735351562,
      "learning_rate": 4.092592592592593e-06,
      "loss": 2.0187,
      "step": 3032
    },
    {
      "epoch": 6.31875,
      "grad_norm": 21.955184936523438,
      "learning_rate": 4.090277777777778e-06,
      "loss": 0.874,
      "step": 3033
    },
    {
      "epoch": 6.320833333333334,
      "grad_norm": 16.72500228881836,
      "learning_rate": 4.087962962962963e-06,
      "loss": 1.2423,
      "step": 3034
    },
    {
      "epoch": 6.322916666666667,
      "grad_norm": 12.388529777526855,
      "learning_rate": 4.085648148148149e-06,
      "loss": 0.7974,
      "step": 3035
    },
    {
      "epoch": 6.325,
      "grad_norm": 15.674114227294922,
      "learning_rate": 4.083333333333334e-06,
      "loss": 1.2444,
      "step": 3036
    },
    {
      "epoch": 6.327083333333333,
      "grad_norm": 192.74395751953125,
      "learning_rate": 4.081018518518519e-06,
      "loss": 1.0837,
      "step": 3037
    },
    {
      "epoch": 6.329166666666667,
      "grad_norm": 42.291160583496094,
      "learning_rate": 4.078703703703704e-06,
      "loss": 1.4182,
      "step": 3038
    },
    {
      "epoch": 6.33125,
      "grad_norm": 11.909017562866211,
      "learning_rate": 4.076388888888889e-06,
      "loss": 1.0104,
      "step": 3039
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 21.019304275512695,
      "learning_rate": 4.074074074074074e-06,
      "loss": 1.2728,
      "step": 3040
    },
    {
      "epoch": 6.335416666666666,
      "grad_norm": 17.902551651000977,
      "learning_rate": 4.07175925925926e-06,
      "loss": 1.4134,
      "step": 3041
    },
    {
      "epoch": 6.3375,
      "grad_norm": 3.8514199256896973,
      "learning_rate": 4.069444444444444e-06,
      "loss": 0.1088,
      "step": 3042
    },
    {
      "epoch": 6.339583333333334,
      "grad_norm": 7.137731075286865,
      "learning_rate": 4.0671296296296294e-06,
      "loss": 0.1774,
      "step": 3043
    },
    {
      "epoch": 6.341666666666667,
      "grad_norm": 22.254671096801758,
      "learning_rate": 4.064814814814815e-06,
      "loss": 1.4297,
      "step": 3044
    },
    {
      "epoch": 6.34375,
      "grad_norm": 10.58816146850586,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 0.7443,
      "step": 3045
    },
    {
      "epoch": 6.345833333333333,
      "grad_norm": 7.073588848114014,
      "learning_rate": 4.060185185185186e-06,
      "loss": 1.0454,
      "step": 3046
    },
    {
      "epoch": 6.347916666666666,
      "grad_norm": 10.509124755859375,
      "learning_rate": 4.057870370370371e-06,
      "loss": 0.8161,
      "step": 3047
    },
    {
      "epoch": 6.35,
      "grad_norm": 89.71697235107422,
      "learning_rate": 4.055555555555556e-06,
      "loss": 1.5987,
      "step": 3048
    },
    {
      "epoch": 6.352083333333334,
      "grad_norm": 21.7656307220459,
      "learning_rate": 4.053240740740741e-06,
      "loss": 2.0784,
      "step": 3049
    },
    {
      "epoch": 6.354166666666667,
      "grad_norm": 33.875789642333984,
      "learning_rate": 4.050925925925927e-06,
      "loss": 2.3744,
      "step": 3050
    },
    {
      "epoch": 6.35625,
      "grad_norm": 30.910913467407227,
      "learning_rate": 4.048611111111111e-06,
      "loss": 1.4741,
      "step": 3051
    },
    {
      "epoch": 6.358333333333333,
      "grad_norm": 13.228984832763672,
      "learning_rate": 4.046296296296296e-06,
      "loss": 1.549,
      "step": 3052
    },
    {
      "epoch": 6.360416666666667,
      "grad_norm": 50.930545806884766,
      "learning_rate": 4.043981481481482e-06,
      "loss": 1.4238,
      "step": 3053
    },
    {
      "epoch": 6.3625,
      "grad_norm": 12.986011505126953,
      "learning_rate": 4.041666666666667e-06,
      "loss": 1.3003,
      "step": 3054
    },
    {
      "epoch": 6.364583333333333,
      "grad_norm": 41.580875396728516,
      "learning_rate": 4.039351851851852e-06,
      "loss": 0.6902,
      "step": 3055
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 12.024432182312012,
      "learning_rate": 4.037037037037037e-06,
      "loss": 0.8345,
      "step": 3056
    },
    {
      "epoch": 6.36875,
      "grad_norm": 51.78447723388672,
      "learning_rate": 4.0347222222222225e-06,
      "loss": 2.3067,
      "step": 3057
    },
    {
      "epoch": 6.370833333333334,
      "grad_norm": 5.579226016998291,
      "learning_rate": 4.032407407407408e-06,
      "loss": 0.7526,
      "step": 3058
    },
    {
      "epoch": 6.372916666666667,
      "grad_norm": 8.77208423614502,
      "learning_rate": 4.030092592592593e-06,
      "loss": 1.1295,
      "step": 3059
    },
    {
      "epoch": 6.375,
      "grad_norm": 12.138517379760742,
      "learning_rate": 4.027777777777779e-06,
      "loss": 1.324,
      "step": 3060
    },
    {
      "epoch": 6.377083333333333,
      "grad_norm": 74.01473236083984,
      "learning_rate": 4.025462962962963e-06,
      "loss": 1.5243,
      "step": 3061
    },
    {
      "epoch": 6.379166666666666,
      "grad_norm": 17.655925750732422,
      "learning_rate": 4.023148148148148e-06,
      "loss": 1.2907,
      "step": 3062
    },
    {
      "epoch": 6.38125,
      "grad_norm": 24.636232376098633,
      "learning_rate": 4.020833333333334e-06,
      "loss": 1.2235,
      "step": 3063
    },
    {
      "epoch": 6.383333333333334,
      "grad_norm": 8.351906776428223,
      "learning_rate": 4.018518518518519e-06,
      "loss": 0.9395,
      "step": 3064
    },
    {
      "epoch": 6.385416666666667,
      "grad_norm": 6.717743396759033,
      "learning_rate": 4.016203703703704e-06,
      "loss": 0.9107,
      "step": 3065
    },
    {
      "epoch": 6.3875,
      "grad_norm": 34.780128479003906,
      "learning_rate": 4.013888888888889e-06,
      "loss": 2.1343,
      "step": 3066
    },
    {
      "epoch": 6.389583333333333,
      "grad_norm": 17.478952407836914,
      "learning_rate": 4.011574074074074e-06,
      "loss": 0.7797,
      "step": 3067
    },
    {
      "epoch": 6.391666666666667,
      "grad_norm": 10.02418041229248,
      "learning_rate": 4.0092592592592594e-06,
      "loss": 0.7519,
      "step": 3068
    },
    {
      "epoch": 6.39375,
      "grad_norm": 8.975188255310059,
      "learning_rate": 4.006944444444445e-06,
      "loss": 0.6292,
      "step": 3069
    },
    {
      "epoch": 6.395833333333333,
      "grad_norm": 47.92416000366211,
      "learning_rate": 4.00462962962963e-06,
      "loss": 1.8936,
      "step": 3070
    },
    {
      "epoch": 6.397916666666666,
      "grad_norm": 40.85355758666992,
      "learning_rate": 4.002314814814815e-06,
      "loss": 1.7384,
      "step": 3071
    },
    {
      "epoch": 6.4,
      "grad_norm": 25.59403419494629,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.1165,
      "step": 3072
    },
    {
      "epoch": 6.402083333333334,
      "grad_norm": 9.891407012939453,
      "learning_rate": 3.997685185185186e-06,
      "loss": 1.6052,
      "step": 3073
    },
    {
      "epoch": 6.404166666666667,
      "grad_norm": 53.8932991027832,
      "learning_rate": 3.995370370370371e-06,
      "loss": 1.6915,
      "step": 3074
    },
    {
      "epoch": 6.40625,
      "grad_norm": 12.588505744934082,
      "learning_rate": 3.993055555555556e-06,
      "loss": 0.8663,
      "step": 3075
    },
    {
      "epoch": 6.408333333333333,
      "grad_norm": 55.854835510253906,
      "learning_rate": 3.990740740740741e-06,
      "loss": 1.4569,
      "step": 3076
    },
    {
      "epoch": 6.410416666666666,
      "grad_norm": 9.723152160644531,
      "learning_rate": 3.988425925925926e-06,
      "loss": 0.7777,
      "step": 3077
    },
    {
      "epoch": 6.4125,
      "grad_norm": 21.60789680480957,
      "learning_rate": 3.986111111111112e-06,
      "loss": 1.9359,
      "step": 3078
    },
    {
      "epoch": 6.414583333333334,
      "grad_norm": 8.212793350219727,
      "learning_rate": 3.983796296296296e-06,
      "loss": 0.6102,
      "step": 3079
    },
    {
      "epoch": 6.416666666666667,
      "grad_norm": 22.692155838012695,
      "learning_rate": 3.9814814814814814e-06,
      "loss": 1.021,
      "step": 3080
    },
    {
      "epoch": 6.41875,
      "grad_norm": 41.59294128417969,
      "learning_rate": 3.9791666666666665e-06,
      "loss": 1.311,
      "step": 3081
    },
    {
      "epoch": 6.420833333333333,
      "grad_norm": 4.876103401184082,
      "learning_rate": 3.9768518518518525e-06,
      "loss": 0.711,
      "step": 3082
    },
    {
      "epoch": 6.422916666666667,
      "grad_norm": 15.853819847106934,
      "learning_rate": 3.974537037037038e-06,
      "loss": 1.2191,
      "step": 3083
    },
    {
      "epoch": 6.425,
      "grad_norm": 25.310686111450195,
      "learning_rate": 3.972222222222223e-06,
      "loss": 0.9969,
      "step": 3084
    },
    {
      "epoch": 6.427083333333333,
      "grad_norm": 23.24853515625,
      "learning_rate": 3.969907407407408e-06,
      "loss": 0.8515,
      "step": 3085
    },
    {
      "epoch": 6.429166666666666,
      "grad_norm": 4.038562297821045,
      "learning_rate": 3.967592592592593e-06,
      "loss": 0.1136,
      "step": 3086
    },
    {
      "epoch": 6.43125,
      "grad_norm": 10.447298049926758,
      "learning_rate": 3.965277777777778e-06,
      "loss": 1.6567,
      "step": 3087
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 23.570068359375,
      "learning_rate": 3.962962962962963e-06,
      "loss": 0.9404,
      "step": 3088
    },
    {
      "epoch": 6.435416666666667,
      "grad_norm": 48.013893127441406,
      "learning_rate": 3.960648148148148e-06,
      "loss": 1.6996,
      "step": 3089
    },
    {
      "epoch": 6.4375,
      "grad_norm": 13.942845344543457,
      "learning_rate": 3.958333333333333e-06,
      "loss": 1.1588,
      "step": 3090
    },
    {
      "epoch": 6.439583333333333,
      "grad_norm": 15.869194030761719,
      "learning_rate": 3.956018518518519e-06,
      "loss": 0.9863,
      "step": 3091
    },
    {
      "epoch": 6.441666666666666,
      "grad_norm": 52.96528244018555,
      "learning_rate": 3.953703703703704e-06,
      "loss": 1.9313,
      "step": 3092
    },
    {
      "epoch": 6.44375,
      "grad_norm": 12.512566566467285,
      "learning_rate": 3.951388888888889e-06,
      "loss": 1.6972,
      "step": 3093
    },
    {
      "epoch": 6.445833333333334,
      "grad_norm": 12.524683952331543,
      "learning_rate": 3.9490740740740745e-06,
      "loss": 1.2715,
      "step": 3094
    },
    {
      "epoch": 6.447916666666667,
      "grad_norm": 28.0964298248291,
      "learning_rate": 3.94675925925926e-06,
      "loss": 1.2207,
      "step": 3095
    },
    {
      "epoch": 6.45,
      "grad_norm": 8.42618179321289,
      "learning_rate": 3.944444444444445e-06,
      "loss": 0.6534,
      "step": 3096
    },
    {
      "epoch": 6.452083333333333,
      "grad_norm": 24.37434959411621,
      "learning_rate": 3.94212962962963e-06,
      "loss": 1.7867,
      "step": 3097
    },
    {
      "epoch": 6.454166666666667,
      "grad_norm": 9.133563041687012,
      "learning_rate": 3.939814814814815e-06,
      "loss": 0.4398,
      "step": 3098
    },
    {
      "epoch": 6.45625,
      "grad_norm": 33.531700134277344,
      "learning_rate": 3.9375e-06,
      "loss": 1.3921,
      "step": 3099
    },
    {
      "epoch": 6.458333333333333,
      "grad_norm": 8.400415420532227,
      "learning_rate": 3.935185185185186e-06,
      "loss": 0.931,
      "step": 3100
    },
    {
      "epoch": 6.460416666666666,
      "grad_norm": 14.709303855895996,
      "learning_rate": 3.932870370370371e-06,
      "loss": 2.0965,
      "step": 3101
    },
    {
      "epoch": 6.4625,
      "grad_norm": 27.24773597717285,
      "learning_rate": 3.930555555555556e-06,
      "loss": 1.5343,
      "step": 3102
    },
    {
      "epoch": 6.464583333333334,
      "grad_norm": 18.133949279785156,
      "learning_rate": 3.92824074074074e-06,
      "loss": 1.1931,
      "step": 3103
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 9.887325286865234,
      "learning_rate": 3.925925925925926e-06,
      "loss": 0.7629,
      "step": 3104
    },
    {
      "epoch": 6.46875,
      "grad_norm": 15.874180793762207,
      "learning_rate": 3.9236111111111114e-06,
      "loss": 0.8575,
      "step": 3105
    },
    {
      "epoch": 6.470833333333333,
      "grad_norm": 43.647552490234375,
      "learning_rate": 3.9212962962962965e-06,
      "loss": 1.5821,
      "step": 3106
    },
    {
      "epoch": 6.472916666666666,
      "grad_norm": 106.28409576416016,
      "learning_rate": 3.918981481481482e-06,
      "loss": 1.5954,
      "step": 3107
    },
    {
      "epoch": 6.475,
      "grad_norm": 65.84088897705078,
      "learning_rate": 3.916666666666667e-06,
      "loss": 1.0176,
      "step": 3108
    },
    {
      "epoch": 6.477083333333334,
      "grad_norm": 9.294798851013184,
      "learning_rate": 3.914351851851852e-06,
      "loss": 0.5601,
      "step": 3109
    },
    {
      "epoch": 6.479166666666667,
      "grad_norm": 14.913192749023438,
      "learning_rate": 3.912037037037038e-06,
      "loss": 1.4416,
      "step": 3110
    },
    {
      "epoch": 6.48125,
      "grad_norm": 11.069836616516113,
      "learning_rate": 3.909722222222223e-06,
      "loss": 1.2506,
      "step": 3111
    },
    {
      "epoch": 6.483333333333333,
      "grad_norm": 9.352299690246582,
      "learning_rate": 3.907407407407408e-06,
      "loss": 0.6989,
      "step": 3112
    },
    {
      "epoch": 6.485416666666667,
      "grad_norm": 12.327125549316406,
      "learning_rate": 3.905092592592593e-06,
      "loss": 1.545,
      "step": 3113
    },
    {
      "epoch": 6.4875,
      "grad_norm": 9.942193031311035,
      "learning_rate": 3.902777777777778e-06,
      "loss": 0.768,
      "step": 3114
    },
    {
      "epoch": 6.489583333333333,
      "grad_norm": 16.24945831298828,
      "learning_rate": 3.900462962962963e-06,
      "loss": 0.6424,
      "step": 3115
    },
    {
      "epoch": 6.491666666666666,
      "grad_norm": 13.412854194641113,
      "learning_rate": 3.898148148148148e-06,
      "loss": 0.8706,
      "step": 3116
    },
    {
      "epoch": 6.49375,
      "grad_norm": 12.631383895874023,
      "learning_rate": 3.8958333333333334e-06,
      "loss": 1.0461,
      "step": 3117
    },
    {
      "epoch": 6.495833333333334,
      "grad_norm": 23.215360641479492,
      "learning_rate": 3.8935185185185185e-06,
      "loss": 1.4785,
      "step": 3118
    },
    {
      "epoch": 6.497916666666667,
      "grad_norm": 21.55328941345215,
      "learning_rate": 3.8912037037037045e-06,
      "loss": 1.1031,
      "step": 3119
    },
    {
      "epoch": 6.5,
      "grad_norm": 7.762716293334961,
      "learning_rate": 3.88888888888889e-06,
      "loss": 1.0214,
      "step": 3120
    },
    {
      "epoch": 6.502083333333333,
      "grad_norm": 23.83938980102539,
      "learning_rate": 3.886574074074075e-06,
      "loss": 1.3701,
      "step": 3121
    },
    {
      "epoch": 6.504166666666666,
      "grad_norm": 26.548227310180664,
      "learning_rate": 3.88425925925926e-06,
      "loss": 1.3455,
      "step": 3122
    },
    {
      "epoch": 6.50625,
      "grad_norm": 24.12537956237793,
      "learning_rate": 3.881944444444445e-06,
      "loss": 1.3197,
      "step": 3123
    },
    {
      "epoch": 6.508333333333333,
      "grad_norm": 5.030806541442871,
      "learning_rate": 3.87962962962963e-06,
      "loss": 0.1329,
      "step": 3124
    },
    {
      "epoch": 6.510416666666667,
      "grad_norm": 12.417332649230957,
      "learning_rate": 3.877314814814815e-06,
      "loss": 1.6098,
      "step": 3125
    },
    {
      "epoch": 6.5125,
      "grad_norm": 24.37752342224121,
      "learning_rate": 3.875e-06,
      "loss": 0.5841,
      "step": 3126
    },
    {
      "epoch": 6.514583333333333,
      "grad_norm": 9.084753036499023,
      "learning_rate": 3.872685185185185e-06,
      "loss": 0.7199,
      "step": 3127
    },
    {
      "epoch": 6.516666666666667,
      "grad_norm": 13.290911674499512,
      "learning_rate": 3.87037037037037e-06,
      "loss": 0.2323,
      "step": 3128
    },
    {
      "epoch": 6.51875,
      "grad_norm": 20.957843780517578,
      "learning_rate": 3.868055555555556e-06,
      "loss": 0.7606,
      "step": 3129
    },
    {
      "epoch": 6.520833333333333,
      "grad_norm": 18.046207427978516,
      "learning_rate": 3.865740740740741e-06,
      "loss": 0.8836,
      "step": 3130
    },
    {
      "epoch": 6.522916666666667,
      "grad_norm": 53.225547790527344,
      "learning_rate": 3.863425925925926e-06,
      "loss": 0.781,
      "step": 3131
    },
    {
      "epoch": 6.525,
      "grad_norm": 50.041404724121094,
      "learning_rate": 3.861111111111112e-06,
      "loss": 1.1381,
      "step": 3132
    },
    {
      "epoch": 6.527083333333334,
      "grad_norm": 14.705060005187988,
      "learning_rate": 3.858796296296297e-06,
      "loss": 1.4029,
      "step": 3133
    },
    {
      "epoch": 6.529166666666667,
      "grad_norm": 15.025606155395508,
      "learning_rate": 3.856481481481482e-06,
      "loss": 1.2521,
      "step": 3134
    },
    {
      "epoch": 6.53125,
      "grad_norm": 14.704590797424316,
      "learning_rate": 3.854166666666667e-06,
      "loss": 0.6897,
      "step": 3135
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 9.585648536682129,
      "learning_rate": 3.851851851851852e-06,
      "loss": 0.7121,
      "step": 3136
    },
    {
      "epoch": 6.535416666666666,
      "grad_norm": 9.542668342590332,
      "learning_rate": 3.849537037037037e-06,
      "loss": 1.1116,
      "step": 3137
    },
    {
      "epoch": 6.5375,
      "grad_norm": 10.891653060913086,
      "learning_rate": 3.847222222222223e-06,
      "loss": 1.0309,
      "step": 3138
    },
    {
      "epoch": 6.539583333333333,
      "grad_norm": 381.3133850097656,
      "learning_rate": 3.844907407407408e-06,
      "loss": 1.637,
      "step": 3139
    },
    {
      "epoch": 6.541666666666667,
      "grad_norm": 22.192609786987305,
      "learning_rate": 3.842592592592592e-06,
      "loss": 1.4782,
      "step": 3140
    },
    {
      "epoch": 6.54375,
      "grad_norm": 5.83043098449707,
      "learning_rate": 3.840277777777778e-06,
      "loss": 0.279,
      "step": 3141
    },
    {
      "epoch": 6.545833333333333,
      "grad_norm": 32.33832550048828,
      "learning_rate": 3.8379629629629634e-06,
      "loss": 1.6561,
      "step": 3142
    },
    {
      "epoch": 6.547916666666667,
      "grad_norm": 18.114267349243164,
      "learning_rate": 3.8356481481481485e-06,
      "loss": 1.3627,
      "step": 3143
    },
    {
      "epoch": 6.55,
      "grad_norm": 99.05934143066406,
      "learning_rate": 3.833333333333334e-06,
      "loss": 1.639,
      "step": 3144
    },
    {
      "epoch": 6.552083333333333,
      "grad_norm": 16.667644500732422,
      "learning_rate": 3.831018518518519e-06,
      "loss": 1.4645,
      "step": 3145
    },
    {
      "epoch": 6.554166666666667,
      "grad_norm": 214.62852478027344,
      "learning_rate": 3.828703703703704e-06,
      "loss": 2.017,
      "step": 3146
    },
    {
      "epoch": 6.55625,
      "grad_norm": 9.25743579864502,
      "learning_rate": 3.826388888888889e-06,
      "loss": 1.0589,
      "step": 3147
    },
    {
      "epoch": 6.558333333333334,
      "grad_norm": 16.858083724975586,
      "learning_rate": 3.824074074074075e-06,
      "loss": 1.4973,
      "step": 3148
    },
    {
      "epoch": 6.560416666666667,
      "grad_norm": 234.104248046875,
      "learning_rate": 3.821759259259259e-06,
      "loss": 0.9721,
      "step": 3149
    },
    {
      "epoch": 6.5625,
      "grad_norm": 32.89097595214844,
      "learning_rate": 3.819444444444444e-06,
      "loss": 1.537,
      "step": 3150
    },
    {
      "epoch": 6.564583333333333,
      "grad_norm": 16.3020076751709,
      "learning_rate": 3.81712962962963e-06,
      "loss": 1.2818,
      "step": 3151
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 9.71627426147461,
      "learning_rate": 3.814814814814815e-06,
      "loss": 0.8033,
      "step": 3152
    },
    {
      "epoch": 6.56875,
      "grad_norm": 9.325955390930176,
      "learning_rate": 3.8125e-06,
      "loss": 0.7775,
      "step": 3153
    },
    {
      "epoch": 6.570833333333333,
      "grad_norm": 10.196456909179688,
      "learning_rate": 3.810185185185186e-06,
      "loss": 0.6994,
      "step": 3154
    },
    {
      "epoch": 6.572916666666667,
      "grad_norm": 26.604707717895508,
      "learning_rate": 3.8078703703703705e-06,
      "loss": 1.2715,
      "step": 3155
    },
    {
      "epoch": 6.575,
      "grad_norm": 50.21124267578125,
      "learning_rate": 3.8055555555555556e-06,
      "loss": 1.0698,
      "step": 3156
    },
    {
      "epoch": 6.577083333333333,
      "grad_norm": 7.802194595336914,
      "learning_rate": 3.803240740740741e-06,
      "loss": 1.1948,
      "step": 3157
    },
    {
      "epoch": 6.579166666666667,
      "grad_norm": 7.332223415374756,
      "learning_rate": 3.8009259259259263e-06,
      "loss": 0.3332,
      "step": 3158
    },
    {
      "epoch": 6.58125,
      "grad_norm": 60.37825393676758,
      "learning_rate": 3.7986111111111114e-06,
      "loss": 1.8291,
      "step": 3159
    },
    {
      "epoch": 6.583333333333333,
      "grad_norm": 30.78860092163086,
      "learning_rate": 3.796296296296297e-06,
      "loss": 1.0789,
      "step": 3160
    },
    {
      "epoch": 6.585416666666667,
      "grad_norm": 41.769649505615234,
      "learning_rate": 3.793981481481482e-06,
      "loss": 1.7414,
      "step": 3161
    },
    {
      "epoch": 6.5875,
      "grad_norm": 34.72856140136719,
      "learning_rate": 3.7916666666666666e-06,
      "loss": 1.4208,
      "step": 3162
    },
    {
      "epoch": 6.589583333333334,
      "grad_norm": 48.73365020751953,
      "learning_rate": 3.7893518518518526e-06,
      "loss": 1.6335,
      "step": 3163
    },
    {
      "epoch": 6.591666666666667,
      "grad_norm": 10.102899551391602,
      "learning_rate": 3.7870370370370373e-06,
      "loss": 1.148,
      "step": 3164
    },
    {
      "epoch": 6.59375,
      "grad_norm": 17.926010131835938,
      "learning_rate": 3.7847222222222224e-06,
      "loss": 1.3432,
      "step": 3165
    },
    {
      "epoch": 6.595833333333333,
      "grad_norm": 26.61530113220215,
      "learning_rate": 3.782407407407408e-06,
      "loss": 2.6893,
      "step": 3166
    },
    {
      "epoch": 6.597916666666666,
      "grad_norm": 14.183167457580566,
      "learning_rate": 3.780092592592593e-06,
      "loss": 0.512,
      "step": 3167
    },
    {
      "epoch": 6.6,
      "grad_norm": 7.8257155418396,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.1758,
      "step": 3168
    },
    {
      "epoch": 6.602083333333333,
      "grad_norm": 17.50019073486328,
      "learning_rate": 3.775462962962963e-06,
      "loss": 1.5193,
      "step": 3169
    },
    {
      "epoch": 6.604166666666667,
      "grad_norm": 4.362432479858398,
      "learning_rate": 3.7731481481481487e-06,
      "loss": 0.1309,
      "step": 3170
    },
    {
      "epoch": 6.60625,
      "grad_norm": 60.20115661621094,
      "learning_rate": 3.7708333333333334e-06,
      "loss": 1.4273,
      "step": 3171
    },
    {
      "epoch": 6.608333333333333,
      "grad_norm": 21.098262786865234,
      "learning_rate": 3.7685185185185185e-06,
      "loss": 1.3878,
      "step": 3172
    },
    {
      "epoch": 6.610416666666667,
      "grad_norm": 4.705079555511475,
      "learning_rate": 3.766203703703704e-06,
      "loss": 0.137,
      "step": 3173
    },
    {
      "epoch": 6.6125,
      "grad_norm": 8.64424991607666,
      "learning_rate": 3.763888888888889e-06,
      "loss": 0.685,
      "step": 3174
    },
    {
      "epoch": 6.614583333333333,
      "grad_norm": 13.100790023803711,
      "learning_rate": 3.761574074074074e-06,
      "loss": 0.7582,
      "step": 3175
    },
    {
      "epoch": 6.616666666666667,
      "grad_norm": 28.38286018371582,
      "learning_rate": 3.7592592592592597e-06,
      "loss": 1.329,
      "step": 3176
    },
    {
      "epoch": 6.61875,
      "grad_norm": 62.53010177612305,
      "learning_rate": 3.756944444444445e-06,
      "loss": 1.2675,
      "step": 3177
    },
    {
      "epoch": 6.620833333333334,
      "grad_norm": 11.108541488647461,
      "learning_rate": 3.75462962962963e-06,
      "loss": 0.647,
      "step": 3178
    },
    {
      "epoch": 6.622916666666667,
      "grad_norm": 56.67062759399414,
      "learning_rate": 3.7523148148148154e-06,
      "loss": 1.124,
      "step": 3179
    },
    {
      "epoch": 6.625,
      "grad_norm": 10.689018249511719,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 0.7629,
      "step": 3180
    },
    {
      "epoch": 6.627083333333333,
      "grad_norm": 46.247161865234375,
      "learning_rate": 3.747685185185185e-06,
      "loss": 1.0773,
      "step": 3181
    },
    {
      "epoch": 6.629166666666666,
      "grad_norm": 16.113561630249023,
      "learning_rate": 3.7453703703703707e-06,
      "loss": 0.7612,
      "step": 3182
    },
    {
      "epoch": 6.63125,
      "grad_norm": 10.398716926574707,
      "learning_rate": 3.743055555555556e-06,
      "loss": 1.2657,
      "step": 3183
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 10.979317665100098,
      "learning_rate": 3.740740740740741e-06,
      "loss": 1.3494,
      "step": 3184
    },
    {
      "epoch": 6.635416666666667,
      "grad_norm": 13.942102432250977,
      "learning_rate": 3.7384259259259264e-06,
      "loss": 1.2961,
      "step": 3185
    },
    {
      "epoch": 6.6375,
      "grad_norm": 7.2743706703186035,
      "learning_rate": 3.7361111111111115e-06,
      "loss": 0.8086,
      "step": 3186
    },
    {
      "epoch": 6.639583333333333,
      "grad_norm": 8.883919715881348,
      "learning_rate": 3.7337962962962966e-06,
      "loss": 0.395,
      "step": 3187
    },
    {
      "epoch": 6.641666666666667,
      "grad_norm": 97.61827850341797,
      "learning_rate": 3.731481481481482e-06,
      "loss": 1.6303,
      "step": 3188
    },
    {
      "epoch": 6.64375,
      "grad_norm": 6.180157661437988,
      "learning_rate": 3.7291666666666672e-06,
      "loss": 0.9702,
      "step": 3189
    },
    {
      "epoch": 6.645833333333333,
      "grad_norm": 15.812389373779297,
      "learning_rate": 3.726851851851852e-06,
      "loss": 1.1827,
      "step": 3190
    },
    {
      "epoch": 6.647916666666667,
      "grad_norm": 28.753820419311523,
      "learning_rate": 3.7245370370370374e-06,
      "loss": 1.9396,
      "step": 3191
    },
    {
      "epoch": 6.65,
      "grad_norm": 27.263385772705078,
      "learning_rate": 3.7222222222222225e-06,
      "loss": 0.7656,
      "step": 3192
    },
    {
      "epoch": 6.652083333333334,
      "grad_norm": 5.524958610534668,
      "learning_rate": 3.7199074074074076e-06,
      "loss": 0.7263,
      "step": 3193
    },
    {
      "epoch": 6.654166666666667,
      "grad_norm": 15.03187084197998,
      "learning_rate": 3.7175925925925927e-06,
      "loss": 0.9324,
      "step": 3194
    },
    {
      "epoch": 6.65625,
      "grad_norm": 14.492324829101562,
      "learning_rate": 3.7152777777777783e-06,
      "loss": 0.9638,
      "step": 3195
    },
    {
      "epoch": 6.658333333333333,
      "grad_norm": 24.612289428710938,
      "learning_rate": 3.7129629629629633e-06,
      "loss": 0.9149,
      "step": 3196
    },
    {
      "epoch": 6.660416666666666,
      "grad_norm": 27.305442810058594,
      "learning_rate": 3.710648148148148e-06,
      "loss": 0.8196,
      "step": 3197
    },
    {
      "epoch": 6.6625,
      "grad_norm": 13.359007835388184,
      "learning_rate": 3.708333333333334e-06,
      "loss": 1.6194,
      "step": 3198
    },
    {
      "epoch": 6.664583333333333,
      "grad_norm": 50.810726165771484,
      "learning_rate": 3.7060185185185186e-06,
      "loss": 0.7206,
      "step": 3199
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 9.186217308044434,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.6023,
      "step": 3200
    },
    {
      "epoch": 6.66875,
      "grad_norm": 8.85235595703125,
      "learning_rate": 3.7013888888888893e-06,
      "loss": 0.622,
      "step": 3201
    },
    {
      "epoch": 6.670833333333333,
      "grad_norm": 49.57365417480469,
      "learning_rate": 3.6990740740740744e-06,
      "loss": 1.0658,
      "step": 3202
    },
    {
      "epoch": 6.672916666666667,
      "grad_norm": 135.17251586914062,
      "learning_rate": 3.6967592592592595e-06,
      "loss": 1.858,
      "step": 3203
    },
    {
      "epoch": 6.675,
      "grad_norm": 18.308094024658203,
      "learning_rate": 3.694444444444445e-06,
      "loss": 1.4028,
      "step": 3204
    },
    {
      "epoch": 6.677083333333333,
      "grad_norm": 14.477751731872559,
      "learning_rate": 3.69212962962963e-06,
      "loss": 1.3958,
      "step": 3205
    },
    {
      "epoch": 6.679166666666667,
      "grad_norm": 45.32059860229492,
      "learning_rate": 3.6898148148148147e-06,
      "loss": 1.7639,
      "step": 3206
    },
    {
      "epoch": 6.68125,
      "grad_norm": 35.7365837097168,
      "learning_rate": 3.6875000000000007e-06,
      "loss": 1.7596,
      "step": 3207
    },
    {
      "epoch": 6.683333333333334,
      "grad_norm": 78.60963439941406,
      "learning_rate": 3.6851851851851854e-06,
      "loss": 1.1349,
      "step": 3208
    },
    {
      "epoch": 6.685416666666667,
      "grad_norm": 32.01190948486328,
      "learning_rate": 3.6828703703703705e-06,
      "loss": 0.9015,
      "step": 3209
    },
    {
      "epoch": 6.6875,
      "grad_norm": 15.631245613098145,
      "learning_rate": 3.680555555555556e-06,
      "loss": 1.0931,
      "step": 3210
    },
    {
      "epoch": 6.689583333333333,
      "grad_norm": 30.918354034423828,
      "learning_rate": 3.678240740740741e-06,
      "loss": 1.7844,
      "step": 3211
    },
    {
      "epoch": 6.691666666666666,
      "grad_norm": 7.252867698669434,
      "learning_rate": 3.675925925925926e-06,
      "loss": 0.5744,
      "step": 3212
    },
    {
      "epoch": 6.69375,
      "grad_norm": 3.5772218704223633,
      "learning_rate": 3.6736111111111117e-06,
      "loss": 0.1005,
      "step": 3213
    },
    {
      "epoch": 6.695833333333333,
      "grad_norm": 44.01997375488281,
      "learning_rate": 3.671296296296297e-06,
      "loss": 1.347,
      "step": 3214
    },
    {
      "epoch": 6.697916666666667,
      "grad_norm": 15.260616302490234,
      "learning_rate": 3.668981481481482e-06,
      "loss": 1.2898,
      "step": 3215
    },
    {
      "epoch": 6.7,
      "grad_norm": 17.83347511291504,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 0.5567,
      "step": 3216
    },
    {
      "epoch": 6.702083333333333,
      "grad_norm": 24.688344955444336,
      "learning_rate": 3.664351851851852e-06,
      "loss": 1.9945,
      "step": 3217
    },
    {
      "epoch": 6.704166666666667,
      "grad_norm": 23.986852645874023,
      "learning_rate": 3.662037037037037e-06,
      "loss": 0.9268,
      "step": 3218
    },
    {
      "epoch": 6.70625,
      "grad_norm": 21.602567672729492,
      "learning_rate": 3.6597222222222223e-06,
      "loss": 1.9424,
      "step": 3219
    },
    {
      "epoch": 6.708333333333333,
      "grad_norm": 14.04456615447998,
      "learning_rate": 3.657407407407408e-06,
      "loss": 1.2176,
      "step": 3220
    },
    {
      "epoch": 6.710416666666667,
      "grad_norm": 32.39487075805664,
      "learning_rate": 3.655092592592593e-06,
      "loss": 0.4148,
      "step": 3221
    },
    {
      "epoch": 6.7125,
      "grad_norm": 71.57600402832031,
      "learning_rate": 3.652777777777778e-06,
      "loss": 0.8258,
      "step": 3222
    },
    {
      "epoch": 6.714583333333334,
      "grad_norm": 83.36609649658203,
      "learning_rate": 3.6504629629629635e-06,
      "loss": 1.3749,
      "step": 3223
    },
    {
      "epoch": 6.716666666666667,
      "grad_norm": 17.349721908569336,
      "learning_rate": 3.6481481481481486e-06,
      "loss": 0.3485,
      "step": 3224
    },
    {
      "epoch": 6.71875,
      "grad_norm": 8.825255393981934,
      "learning_rate": 3.6458333333333333e-06,
      "loss": 0.2917,
      "step": 3225
    },
    {
      "epoch": 6.720833333333333,
      "grad_norm": 5.946669578552246,
      "learning_rate": 3.643518518518519e-06,
      "loss": 0.2862,
      "step": 3226
    },
    {
      "epoch": 6.722916666666666,
      "grad_norm": 6.614584445953369,
      "learning_rate": 3.641203703703704e-06,
      "loss": 0.9172,
      "step": 3227
    },
    {
      "epoch": 6.725,
      "grad_norm": 6.688609600067139,
      "learning_rate": 3.638888888888889e-06,
      "loss": 0.678,
      "step": 3228
    },
    {
      "epoch": 6.727083333333333,
      "grad_norm": 47.440975189208984,
      "learning_rate": 3.6365740740740745e-06,
      "loss": 1.5595,
      "step": 3229
    },
    {
      "epoch": 6.729166666666667,
      "grad_norm": 39.33890914916992,
      "learning_rate": 3.6342592592592596e-06,
      "loss": 0.7356,
      "step": 3230
    },
    {
      "epoch": 6.73125,
      "grad_norm": 19.184324264526367,
      "learning_rate": 3.6319444444444447e-06,
      "loss": 1.1771,
      "step": 3231
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 21.98124885559082,
      "learning_rate": 3.6296296296296302e-06,
      "loss": 0.8861,
      "step": 3232
    },
    {
      "epoch": 6.735416666666667,
      "grad_norm": 19.30852508544922,
      "learning_rate": 3.6273148148148153e-06,
      "loss": 1.4218,
      "step": 3233
    },
    {
      "epoch": 6.7375,
      "grad_norm": 17.10179901123047,
      "learning_rate": 3.625e-06,
      "loss": 1.3163,
      "step": 3234
    },
    {
      "epoch": 6.739583333333333,
      "grad_norm": 25.344467163085938,
      "learning_rate": 3.6226851851851855e-06,
      "loss": 1.3945,
      "step": 3235
    },
    {
      "epoch": 6.741666666666667,
      "grad_norm": 10.404577255249023,
      "learning_rate": 3.6203703703703706e-06,
      "loss": 0.4295,
      "step": 3236
    },
    {
      "epoch": 6.74375,
      "grad_norm": 56.04524612426758,
      "learning_rate": 3.6180555555555557e-06,
      "loss": 1.6411,
      "step": 3237
    },
    {
      "epoch": 6.745833333333334,
      "grad_norm": 15.230528831481934,
      "learning_rate": 3.615740740740741e-06,
      "loss": 1.1884,
      "step": 3238
    },
    {
      "epoch": 6.747916666666667,
      "grad_norm": 31.103845596313477,
      "learning_rate": 3.6134259259259264e-06,
      "loss": 0.8102,
      "step": 3239
    },
    {
      "epoch": 6.75,
      "grad_norm": 12.130995750427246,
      "learning_rate": 3.6111111111111115e-06,
      "loss": 1.5778,
      "step": 3240
    },
    {
      "epoch": 6.752083333333333,
      "grad_norm": 13.774418830871582,
      "learning_rate": 3.6087962962962966e-06,
      "loss": 0.7981,
      "step": 3241
    },
    {
      "epoch": 6.754166666666666,
      "grad_norm": 25.932723999023438,
      "learning_rate": 3.606481481481482e-06,
      "loss": 1.4014,
      "step": 3242
    },
    {
      "epoch": 6.75625,
      "grad_norm": 93.55744171142578,
      "learning_rate": 3.6041666666666667e-06,
      "loss": 1.9828,
      "step": 3243
    },
    {
      "epoch": 6.758333333333333,
      "grad_norm": 15.166474342346191,
      "learning_rate": 3.601851851851852e-06,
      "loss": 1.4448,
      "step": 3244
    },
    {
      "epoch": 6.760416666666667,
      "grad_norm": 25.166919708251953,
      "learning_rate": 3.5995370370370374e-06,
      "loss": 2.1462,
      "step": 3245
    },
    {
      "epoch": 6.7625,
      "grad_norm": 19.10688018798828,
      "learning_rate": 3.5972222222222225e-06,
      "loss": 0.4005,
      "step": 3246
    },
    {
      "epoch": 6.764583333333333,
      "grad_norm": 8.921961784362793,
      "learning_rate": 3.5949074074074076e-06,
      "loss": 0.1754,
      "step": 3247
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 10.085277557373047,
      "learning_rate": 3.592592592592593e-06,
      "loss": 0.7949,
      "step": 3248
    },
    {
      "epoch": 6.76875,
      "grad_norm": 40.57072067260742,
      "learning_rate": 3.590277777777778e-06,
      "loss": 0.9716,
      "step": 3249
    },
    {
      "epoch": 6.770833333333333,
      "grad_norm": 14.05185317993164,
      "learning_rate": 3.5879629629629633e-06,
      "loss": 1.4917,
      "step": 3250
    },
    {
      "epoch": 6.772916666666667,
      "grad_norm": 19.160072326660156,
      "learning_rate": 3.585648148148149e-06,
      "loss": 1.4786,
      "step": 3251
    },
    {
      "epoch": 6.775,
      "grad_norm": 36.564422607421875,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 1.5846,
      "step": 3252
    },
    {
      "epoch": 6.777083333333334,
      "grad_norm": 4.524210453033447,
      "learning_rate": 3.5810185185185186e-06,
      "loss": 0.2317,
      "step": 3253
    },
    {
      "epoch": 6.779166666666667,
      "grad_norm": 48.026039123535156,
      "learning_rate": 3.578703703703704e-06,
      "loss": 1.7045,
      "step": 3254
    },
    {
      "epoch": 6.78125,
      "grad_norm": 9.145730018615723,
      "learning_rate": 3.576388888888889e-06,
      "loss": 0.6387,
      "step": 3255
    },
    {
      "epoch": 6.783333333333333,
      "grad_norm": 8.752179145812988,
      "learning_rate": 3.5740740740740743e-06,
      "loss": 1.0379,
      "step": 3256
    },
    {
      "epoch": 6.785416666666666,
      "grad_norm": 22.17493438720703,
      "learning_rate": 3.57175925925926e-06,
      "loss": 1.4578,
      "step": 3257
    },
    {
      "epoch": 6.7875,
      "grad_norm": 13.70743179321289,
      "learning_rate": 3.569444444444445e-06,
      "loss": 1.5182,
      "step": 3258
    },
    {
      "epoch": 6.789583333333333,
      "grad_norm": 28.32833480834961,
      "learning_rate": 3.56712962962963e-06,
      "loss": 0.525,
      "step": 3259
    },
    {
      "epoch": 6.791666666666667,
      "grad_norm": 10.135669708251953,
      "learning_rate": 3.5648148148148147e-06,
      "loss": 1.3758,
      "step": 3260
    },
    {
      "epoch": 6.79375,
      "grad_norm": 7.9253411293029785,
      "learning_rate": 3.5625e-06,
      "loss": 0.7806,
      "step": 3261
    },
    {
      "epoch": 6.795833333333333,
      "grad_norm": 13.817299842834473,
      "learning_rate": 3.5601851851851853e-06,
      "loss": 1.4986,
      "step": 3262
    },
    {
      "epoch": 6.797916666666667,
      "grad_norm": 13.29871654510498,
      "learning_rate": 3.5578703703703704e-06,
      "loss": 1.25,
      "step": 3263
    },
    {
      "epoch": 6.8,
      "grad_norm": 73.25630187988281,
      "learning_rate": 3.555555555555556e-06,
      "loss": 1.7573,
      "step": 3264
    },
    {
      "epoch": 6.802083333333333,
      "grad_norm": 51.821022033691406,
      "learning_rate": 3.553240740740741e-06,
      "loss": 1.2108,
      "step": 3265
    },
    {
      "epoch": 6.804166666666667,
      "grad_norm": 10.613123893737793,
      "learning_rate": 3.550925925925926e-06,
      "loss": 1.2687,
      "step": 3266
    },
    {
      "epoch": 6.80625,
      "grad_norm": 22.544742584228516,
      "learning_rate": 3.5486111111111116e-06,
      "loss": 1.1916,
      "step": 3267
    },
    {
      "epoch": 6.808333333333334,
      "grad_norm": 30.68342399597168,
      "learning_rate": 3.5462962962962967e-06,
      "loss": 1.8272,
      "step": 3268
    },
    {
      "epoch": 6.810416666666667,
      "grad_norm": 29.40431022644043,
      "learning_rate": 3.5439814814814814e-06,
      "loss": 0.7876,
      "step": 3269
    },
    {
      "epoch": 6.8125,
      "grad_norm": 11.706995010375977,
      "learning_rate": 3.5416666666666673e-06,
      "loss": 1.2238,
      "step": 3270
    },
    {
      "epoch": 6.814583333333333,
      "grad_norm": 46.405601501464844,
      "learning_rate": 3.539351851851852e-06,
      "loss": 1.9104,
      "step": 3271
    },
    {
      "epoch": 6.816666666666666,
      "grad_norm": 15.87962818145752,
      "learning_rate": 3.537037037037037e-06,
      "loss": 1.765,
      "step": 3272
    },
    {
      "epoch": 6.81875,
      "grad_norm": 21.89082145690918,
      "learning_rate": 3.5347222222222226e-06,
      "loss": 1.5504,
      "step": 3273
    },
    {
      "epoch": 6.820833333333333,
      "grad_norm": 27.138439178466797,
      "learning_rate": 3.5324074074074077e-06,
      "loss": 1.6104,
      "step": 3274
    },
    {
      "epoch": 6.822916666666667,
      "grad_norm": 10.645215034484863,
      "learning_rate": 3.530092592592593e-06,
      "loss": 0.659,
      "step": 3275
    },
    {
      "epoch": 6.825,
      "grad_norm": 13.62840747833252,
      "learning_rate": 3.5277777777777784e-06,
      "loss": 1.6851,
      "step": 3276
    },
    {
      "epoch": 6.827083333333333,
      "grad_norm": 18.656435012817383,
      "learning_rate": 3.5254629629629635e-06,
      "loss": 1.2629,
      "step": 3277
    },
    {
      "epoch": 6.829166666666667,
      "grad_norm": 17.988128662109375,
      "learning_rate": 3.523148148148148e-06,
      "loss": 1.0936,
      "step": 3278
    },
    {
      "epoch": 6.83125,
      "grad_norm": 7.875549793243408,
      "learning_rate": 3.520833333333334e-06,
      "loss": 0.6963,
      "step": 3279
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 9.238173484802246,
      "learning_rate": 3.5185185185185187e-06,
      "loss": 1.0474,
      "step": 3280
    },
    {
      "epoch": 6.835416666666667,
      "grad_norm": 58.91511917114258,
      "learning_rate": 3.516203703703704e-06,
      "loss": 2.1335,
      "step": 3281
    },
    {
      "epoch": 6.8375,
      "grad_norm": 43.701656341552734,
      "learning_rate": 3.513888888888889e-06,
      "loss": 1.4564,
      "step": 3282
    },
    {
      "epoch": 6.839583333333334,
      "grad_norm": 15.122160911560059,
      "learning_rate": 3.5115740740740745e-06,
      "loss": 1.7703,
      "step": 3283
    },
    {
      "epoch": 6.841666666666667,
      "grad_norm": 58.50334930419922,
      "learning_rate": 3.5092592592592596e-06,
      "loss": 1.1549,
      "step": 3284
    },
    {
      "epoch": 6.84375,
      "grad_norm": 28.527624130249023,
      "learning_rate": 3.5069444444444447e-06,
      "loss": 1.4181,
      "step": 3285
    },
    {
      "epoch": 6.845833333333333,
      "grad_norm": 22.663068771362305,
      "learning_rate": 3.50462962962963e-06,
      "loss": 1.3615,
      "step": 3286
    },
    {
      "epoch": 6.847916666666666,
      "grad_norm": 27.191316604614258,
      "learning_rate": 3.502314814814815e-06,
      "loss": 2.0287,
      "step": 3287
    },
    {
      "epoch": 6.85,
      "grad_norm": 22.461318969726562,
      "learning_rate": 3.5e-06,
      "loss": 0.8313,
      "step": 3288
    },
    {
      "epoch": 6.852083333333333,
      "grad_norm": 10.278899192810059,
      "learning_rate": 3.4976851851851855e-06,
      "loss": 1.2122,
      "step": 3289
    },
    {
      "epoch": 6.854166666666667,
      "grad_norm": 55.5306396484375,
      "learning_rate": 3.4953703703703706e-06,
      "loss": 1.2405,
      "step": 3290
    },
    {
      "epoch": 6.85625,
      "grad_norm": 27.53511619567871,
      "learning_rate": 3.4930555555555557e-06,
      "loss": 1.5249,
      "step": 3291
    },
    {
      "epoch": 6.858333333333333,
      "grad_norm": 8.916313171386719,
      "learning_rate": 3.490740740740741e-06,
      "loss": 0.8039,
      "step": 3292
    },
    {
      "epoch": 6.860416666666667,
      "grad_norm": 54.02458572387695,
      "learning_rate": 3.4884259259259263e-06,
      "loss": 1.0793,
      "step": 3293
    },
    {
      "epoch": 6.8625,
      "grad_norm": 12.275239944458008,
      "learning_rate": 3.4861111111111114e-06,
      "loss": 0.7808,
      "step": 3294
    },
    {
      "epoch": 6.864583333333333,
      "grad_norm": 52.367103576660156,
      "learning_rate": 3.483796296296297e-06,
      "loss": 0.2635,
      "step": 3295
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 13.2603759765625,
      "learning_rate": 3.481481481481482e-06,
      "loss": 1.3205,
      "step": 3296
    },
    {
      "epoch": 6.86875,
      "grad_norm": 47.02769088745117,
      "learning_rate": 3.4791666666666667e-06,
      "loss": 0.2185,
      "step": 3297
    },
    {
      "epoch": 6.870833333333334,
      "grad_norm": 15.440442085266113,
      "learning_rate": 3.476851851851852e-06,
      "loss": 0.9839,
      "step": 3298
    },
    {
      "epoch": 6.872916666666667,
      "grad_norm": 8.263128280639648,
      "learning_rate": 3.4745370370370373e-06,
      "loss": 0.523,
      "step": 3299
    },
    {
      "epoch": 6.875,
      "grad_norm": 21.485687255859375,
      "learning_rate": 3.4722222222222224e-06,
      "loss": 1.6023,
      "step": 3300
    },
    {
      "epoch": 6.877083333333333,
      "grad_norm": 27.351606369018555,
      "learning_rate": 3.469907407407408e-06,
      "loss": 0.95,
      "step": 3301
    },
    {
      "epoch": 6.879166666666666,
      "grad_norm": 29.12397575378418,
      "learning_rate": 3.467592592592593e-06,
      "loss": 1.286,
      "step": 3302
    },
    {
      "epoch": 6.88125,
      "grad_norm": 12.333878517150879,
      "learning_rate": 3.465277777777778e-06,
      "loss": 1.1254,
      "step": 3303
    },
    {
      "epoch": 6.883333333333333,
      "grad_norm": 9.695281028747559,
      "learning_rate": 3.4629629629629628e-06,
      "loss": 1.212,
      "step": 3304
    },
    {
      "epoch": 6.885416666666667,
      "grad_norm": 8.928084373474121,
      "learning_rate": 3.4606481481481487e-06,
      "loss": 1.2126,
      "step": 3305
    },
    {
      "epoch": 6.8875,
      "grad_norm": 17.970739364624023,
      "learning_rate": 3.4583333333333334e-06,
      "loss": 1.1007,
      "step": 3306
    },
    {
      "epoch": 6.889583333333333,
      "grad_norm": 9.49393081665039,
      "learning_rate": 3.4560185185185185e-06,
      "loss": 1.6339,
      "step": 3307
    },
    {
      "epoch": 6.891666666666667,
      "grad_norm": 20.000530242919922,
      "learning_rate": 3.453703703703704e-06,
      "loss": 1.7798,
      "step": 3308
    },
    {
      "epoch": 6.89375,
      "grad_norm": 22.811077117919922,
      "learning_rate": 3.451388888888889e-06,
      "loss": 1.2323,
      "step": 3309
    },
    {
      "epoch": 6.895833333333333,
      "grad_norm": 6.870283603668213,
      "learning_rate": 3.449074074074074e-06,
      "loss": 0.9966,
      "step": 3310
    },
    {
      "epoch": 6.897916666666667,
      "grad_norm": 31.136878967285156,
      "learning_rate": 3.4467592592592597e-06,
      "loss": 1.9557,
      "step": 3311
    },
    {
      "epoch": 6.9,
      "grad_norm": 19.587617874145508,
      "learning_rate": 3.444444444444445e-06,
      "loss": 0.8028,
      "step": 3312
    },
    {
      "epoch": 6.902083333333334,
      "grad_norm": 18.286161422729492,
      "learning_rate": 3.4421296296296295e-06,
      "loss": 0.478,
      "step": 3313
    },
    {
      "epoch": 6.904166666666667,
      "grad_norm": 9.948156356811523,
      "learning_rate": 3.4398148148148154e-06,
      "loss": 0.6524,
      "step": 3314
    },
    {
      "epoch": 6.90625,
      "grad_norm": 32.595924377441406,
      "learning_rate": 3.4375e-06,
      "loss": 1.4575,
      "step": 3315
    },
    {
      "epoch": 6.908333333333333,
      "grad_norm": 92.31397247314453,
      "learning_rate": 3.4351851851851852e-06,
      "loss": 1.977,
      "step": 3316
    },
    {
      "epoch": 6.910416666666666,
      "grad_norm": 23.956995010375977,
      "learning_rate": 3.4328703703703707e-06,
      "loss": 1.3468,
      "step": 3317
    },
    {
      "epoch": 6.9125,
      "grad_norm": 6.155465126037598,
      "learning_rate": 3.430555555555556e-06,
      "loss": 0.7803,
      "step": 3318
    },
    {
      "epoch": 6.914583333333333,
      "grad_norm": 12.847746849060059,
      "learning_rate": 3.428240740740741e-06,
      "loss": 0.8809,
      "step": 3319
    },
    {
      "epoch": 6.916666666666667,
      "grad_norm": 74.65428924560547,
      "learning_rate": 3.4259259259259265e-06,
      "loss": 1.2858,
      "step": 3320
    },
    {
      "epoch": 6.91875,
      "grad_norm": 38.193790435791016,
      "learning_rate": 3.4236111111111116e-06,
      "loss": 1.5545,
      "step": 3321
    },
    {
      "epoch": 6.920833333333333,
      "grad_norm": 18.071542739868164,
      "learning_rate": 3.4212962962962967e-06,
      "loss": 1.2666,
      "step": 3322
    },
    {
      "epoch": 6.922916666666667,
      "grad_norm": 41.49395751953125,
      "learning_rate": 3.418981481481482e-06,
      "loss": 2.0727,
      "step": 3323
    },
    {
      "epoch": 6.925,
      "grad_norm": 4.7191162109375,
      "learning_rate": 3.416666666666667e-06,
      "loss": 0.2417,
      "step": 3324
    },
    {
      "epoch": 6.927083333333333,
      "grad_norm": 49.15475845336914,
      "learning_rate": 3.414351851851852e-06,
      "loss": 1.9221,
      "step": 3325
    },
    {
      "epoch": 6.929166666666667,
      "grad_norm": 17.54848289489746,
      "learning_rate": 3.4120370370370375e-06,
      "loss": 1.1211,
      "step": 3326
    },
    {
      "epoch": 6.93125,
      "grad_norm": 18.139799118041992,
      "learning_rate": 3.4097222222222226e-06,
      "loss": 1.2365,
      "step": 3327
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 12.805261611938477,
      "learning_rate": 3.4074074074074077e-06,
      "loss": 1.3165,
      "step": 3328
    },
    {
      "epoch": 6.935416666666667,
      "grad_norm": 14.149245262145996,
      "learning_rate": 3.4050925925925928e-06,
      "loss": 0.7371,
      "step": 3329
    },
    {
      "epoch": 6.9375,
      "grad_norm": 9.88827896118164,
      "learning_rate": 3.4027777777777783e-06,
      "loss": 0.7594,
      "step": 3330
    },
    {
      "epoch": 6.939583333333333,
      "grad_norm": 11.395557403564453,
      "learning_rate": 3.4004629629629634e-06,
      "loss": 0.835,
      "step": 3331
    },
    {
      "epoch": 6.941666666666666,
      "grad_norm": 11.18151569366455,
      "learning_rate": 3.398148148148148e-06,
      "loss": 1.2286,
      "step": 3332
    },
    {
      "epoch": 6.94375,
      "grad_norm": 51.537044525146484,
      "learning_rate": 3.3958333333333336e-06,
      "loss": 1.6471,
      "step": 3333
    },
    {
      "epoch": 6.945833333333333,
      "grad_norm": 21.661645889282227,
      "learning_rate": 3.3935185185185187e-06,
      "loss": 1.3684,
      "step": 3334
    },
    {
      "epoch": 6.947916666666667,
      "grad_norm": 9.132303237915039,
      "learning_rate": 3.3912037037037038e-06,
      "loss": 1.197,
      "step": 3335
    },
    {
      "epoch": 6.95,
      "grad_norm": 5.980513095855713,
      "learning_rate": 3.3888888888888893e-06,
      "loss": 0.5873,
      "step": 3336
    },
    {
      "epoch": 6.952083333333333,
      "grad_norm": 39.69879150390625,
      "learning_rate": 3.3865740740740744e-06,
      "loss": 0.8352,
      "step": 3337
    },
    {
      "epoch": 6.954166666666667,
      "grad_norm": 5.813734531402588,
      "learning_rate": 3.3842592592592595e-06,
      "loss": 0.5879,
      "step": 3338
    },
    {
      "epoch": 6.95625,
      "grad_norm": 59.72212219238281,
      "learning_rate": 3.381944444444445e-06,
      "loss": 1.8622,
      "step": 3339
    },
    {
      "epoch": 6.958333333333333,
      "grad_norm": 17.1998233795166,
      "learning_rate": 3.37962962962963e-06,
      "loss": 1.3373,
      "step": 3340
    },
    {
      "epoch": 6.960416666666667,
      "grad_norm": 26.325267791748047,
      "learning_rate": 3.3773148148148148e-06,
      "loss": 1.0074,
      "step": 3341
    },
    {
      "epoch": 6.9625,
      "grad_norm": 10.064627647399902,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.7517,
      "step": 3342
    },
    {
      "epoch": 6.964583333333334,
      "grad_norm": 23.229949951171875,
      "learning_rate": 3.3726851851851854e-06,
      "loss": 0.8411,
      "step": 3343
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 17.802597045898438,
      "learning_rate": 3.3703703703703705e-06,
      "loss": 1.4169,
      "step": 3344
    },
    {
      "epoch": 6.96875,
      "grad_norm": 41.063514709472656,
      "learning_rate": 3.368055555555556e-06,
      "loss": 1.205,
      "step": 3345
    },
    {
      "epoch": 6.970833333333333,
      "grad_norm": 114.28053283691406,
      "learning_rate": 3.365740740740741e-06,
      "loss": 1.655,
      "step": 3346
    },
    {
      "epoch": 6.972916666666666,
      "grad_norm": 85.98180389404297,
      "learning_rate": 3.363425925925926e-06,
      "loss": 1.5883,
      "step": 3347
    },
    {
      "epoch": 6.975,
      "grad_norm": 13.076519966125488,
      "learning_rate": 3.3611111111111117e-06,
      "loss": 0.7334,
      "step": 3348
    },
    {
      "epoch": 6.977083333333333,
      "grad_norm": 52.3630485534668,
      "learning_rate": 3.358796296296297e-06,
      "loss": 1.585,
      "step": 3349
    },
    {
      "epoch": 6.979166666666667,
      "grad_norm": 11.502852439880371,
      "learning_rate": 3.3564814814814815e-06,
      "loss": 1.177,
      "step": 3350
    },
    {
      "epoch": 6.98125,
      "grad_norm": 33.171958923339844,
      "learning_rate": 3.3541666666666666e-06,
      "loss": 1.5692,
      "step": 3351
    },
    {
      "epoch": 6.983333333333333,
      "grad_norm": 30.006744384765625,
      "learning_rate": 3.351851851851852e-06,
      "loss": 1.9373,
      "step": 3352
    },
    {
      "epoch": 6.985416666666667,
      "grad_norm": 11.732643127441406,
      "learning_rate": 3.3495370370370372e-06,
      "loss": 0.7273,
      "step": 3353
    },
    {
      "epoch": 6.9875,
      "grad_norm": 18.103702545166016,
      "learning_rate": 3.3472222222222223e-06,
      "loss": 0.8115,
      "step": 3354
    },
    {
      "epoch": 6.989583333333333,
      "grad_norm": 16.05026626586914,
      "learning_rate": 3.344907407407408e-06,
      "loss": 0.6243,
      "step": 3355
    },
    {
      "epoch": 6.991666666666667,
      "grad_norm": 28.095603942871094,
      "learning_rate": 3.342592592592593e-06,
      "loss": 1.4076,
      "step": 3356
    },
    {
      "epoch": 6.99375,
      "grad_norm": 8.267290115356445,
      "learning_rate": 3.340277777777778e-06,
      "loss": 1.1122,
      "step": 3357
    },
    {
      "epoch": 6.995833333333334,
      "grad_norm": 6.728207111358643,
      "learning_rate": 3.3379629629629636e-06,
      "loss": 0.5788,
      "step": 3358
    },
    {
      "epoch": 6.997916666666667,
      "grad_norm": 25.274587631225586,
      "learning_rate": 3.3356481481481482e-06,
      "loss": 0.7486,
      "step": 3359
    },
    {
      "epoch": 7.0,
      "grad_norm": 17.30194854736328,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.3266,
      "step": 3360
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.6222222222222222,
      "eval_f1": 0.5408511688587677,
      "eval_loss": 1.19895339012146,
      "eval_runtime": 34.3361,
      "eval_samples_per_second": 5.242,
      "eval_steps_per_second": 2.621,
      "step": 3360
    },
    {
      "epoch": 7.002083333333333,
      "grad_norm": 5.06330680847168,
      "learning_rate": 3.331018518518519e-06,
      "loss": 0.6528,
      "step": 3361
    },
    {
      "epoch": 7.004166666666666,
      "grad_norm": 53.959007263183594,
      "learning_rate": 3.328703703703704e-06,
      "loss": 0.8126,
      "step": 3362
    },
    {
      "epoch": 7.00625,
      "grad_norm": 38.49995422363281,
      "learning_rate": 3.326388888888889e-06,
      "loss": 1.8611,
      "step": 3363
    },
    {
      "epoch": 7.008333333333334,
      "grad_norm": 17.96354103088379,
      "learning_rate": 3.3240740740740746e-06,
      "loss": 1.1255,
      "step": 3364
    },
    {
      "epoch": 7.010416666666667,
      "grad_norm": 6.84883451461792,
      "learning_rate": 3.3217592592592597e-06,
      "loss": 0.6673,
      "step": 3365
    },
    {
      "epoch": 7.0125,
      "grad_norm": 47.61046600341797,
      "learning_rate": 3.3194444444444448e-06,
      "loss": 0.8403,
      "step": 3366
    },
    {
      "epoch": 7.014583333333333,
      "grad_norm": 17.070844650268555,
      "learning_rate": 3.3171296296296303e-06,
      "loss": 1.1633,
      "step": 3367
    },
    {
      "epoch": 7.016666666666667,
      "grad_norm": 5.609095096588135,
      "learning_rate": 3.314814814814815e-06,
      "loss": 0.2885,
      "step": 3368
    },
    {
      "epoch": 7.01875,
      "grad_norm": 82.65998077392578,
      "learning_rate": 3.3125e-06,
      "loss": 1.1206,
      "step": 3369
    },
    {
      "epoch": 7.020833333333333,
      "grad_norm": 43.87882995605469,
      "learning_rate": 3.3101851851851856e-06,
      "loss": 1.2846,
      "step": 3370
    },
    {
      "epoch": 7.022916666666666,
      "grad_norm": 13.799749374389648,
      "learning_rate": 3.3078703703703707e-06,
      "loss": 1.7574,
      "step": 3371
    },
    {
      "epoch": 7.025,
      "grad_norm": 58.00347900390625,
      "learning_rate": 3.3055555555555558e-06,
      "loss": 1.0834,
      "step": 3372
    },
    {
      "epoch": 7.027083333333334,
      "grad_norm": 23.14607810974121,
      "learning_rate": 3.303240740740741e-06,
      "loss": 2.3806,
      "step": 3373
    },
    {
      "epoch": 7.029166666666667,
      "grad_norm": 15.706398963928223,
      "learning_rate": 3.3009259259259264e-06,
      "loss": 0.8641,
      "step": 3374
    },
    {
      "epoch": 7.03125,
      "grad_norm": 7.089739799499512,
      "learning_rate": 3.2986111111111115e-06,
      "loss": 0.5634,
      "step": 3375
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 25.1265926361084,
      "learning_rate": 3.296296296296296e-06,
      "loss": 1.1513,
      "step": 3376
    },
    {
      "epoch": 7.035416666666666,
      "grad_norm": 20.91440200805664,
      "learning_rate": 3.293981481481482e-06,
      "loss": 1.5935,
      "step": 3377
    },
    {
      "epoch": 7.0375,
      "grad_norm": 10.323622703552246,
      "learning_rate": 3.2916666666666668e-06,
      "loss": 1.1462,
      "step": 3378
    },
    {
      "epoch": 7.039583333333334,
      "grad_norm": 41.67070007324219,
      "learning_rate": 3.289351851851852e-06,
      "loss": 1.4442,
      "step": 3379
    },
    {
      "epoch": 7.041666666666667,
      "grad_norm": 16.894704818725586,
      "learning_rate": 3.2870370370370374e-06,
      "loss": 1.017,
      "step": 3380
    },
    {
      "epoch": 7.04375,
      "grad_norm": 21.486894607543945,
      "learning_rate": 3.2847222222222225e-06,
      "loss": 1.5272,
      "step": 3381
    },
    {
      "epoch": 7.045833333333333,
      "grad_norm": 90.15560150146484,
      "learning_rate": 3.2824074074074076e-06,
      "loss": 1.429,
      "step": 3382
    },
    {
      "epoch": 7.047916666666667,
      "grad_norm": 16.83997917175293,
      "learning_rate": 3.280092592592593e-06,
      "loss": 1.792,
      "step": 3383
    },
    {
      "epoch": 7.05,
      "grad_norm": 9.346016883850098,
      "learning_rate": 3.277777777777778e-06,
      "loss": 0.6452,
      "step": 3384
    },
    {
      "epoch": 7.052083333333333,
      "grad_norm": 20.886295318603516,
      "learning_rate": 3.275462962962963e-06,
      "loss": 1.2567,
      "step": 3385
    },
    {
      "epoch": 7.054166666666666,
      "grad_norm": 9.561849594116211,
      "learning_rate": 3.273148148148149e-06,
      "loss": 1.0933,
      "step": 3386
    },
    {
      "epoch": 7.05625,
      "grad_norm": 16.50163459777832,
      "learning_rate": 3.2708333333333335e-06,
      "loss": 1.4274,
      "step": 3387
    },
    {
      "epoch": 7.058333333333334,
      "grad_norm": 18.41683578491211,
      "learning_rate": 3.2685185185185186e-06,
      "loss": 1.1286,
      "step": 3388
    },
    {
      "epoch": 7.060416666666667,
      "grad_norm": 90.64448547363281,
      "learning_rate": 3.266203703703704e-06,
      "loss": 1.2103,
      "step": 3389
    },
    {
      "epoch": 7.0625,
      "grad_norm": 10.151415824890137,
      "learning_rate": 3.2638888888888892e-06,
      "loss": 0.9672,
      "step": 3390
    },
    {
      "epoch": 7.064583333333333,
      "grad_norm": 8.008759498596191,
      "learning_rate": 3.2615740740740743e-06,
      "loss": 0.1953,
      "step": 3391
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 26.486454010009766,
      "learning_rate": 3.25925925925926e-06,
      "loss": 1.0722,
      "step": 3392
    },
    {
      "epoch": 7.06875,
      "grad_norm": 18.1334171295166,
      "learning_rate": 3.256944444444445e-06,
      "loss": 0.9396,
      "step": 3393
    },
    {
      "epoch": 7.070833333333334,
      "grad_norm": 13.598011016845703,
      "learning_rate": 3.2546296296296296e-06,
      "loss": 0.3766,
      "step": 3394
    },
    {
      "epoch": 7.072916666666667,
      "grad_norm": 10.1068696975708,
      "learning_rate": 3.2523148148148147e-06,
      "loss": 1.35,
      "step": 3395
    },
    {
      "epoch": 7.075,
      "grad_norm": 18.1624813079834,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.6985,
      "step": 3396
    },
    {
      "epoch": 7.077083333333333,
      "grad_norm": 10.33638858795166,
      "learning_rate": 3.2476851851851853e-06,
      "loss": 0.7131,
      "step": 3397
    },
    {
      "epoch": 7.079166666666667,
      "grad_norm": 27.657365798950195,
      "learning_rate": 3.2453703703703704e-06,
      "loss": 0.7738,
      "step": 3398
    },
    {
      "epoch": 7.08125,
      "grad_norm": 25.059043884277344,
      "learning_rate": 3.243055555555556e-06,
      "loss": 1.7818,
      "step": 3399
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 6.867362022399902,
      "learning_rate": 3.240740740740741e-06,
      "loss": 0.9318,
      "step": 3400
    },
    {
      "epoch": 7.085416666666666,
      "grad_norm": 84.4830322265625,
      "learning_rate": 3.238425925925926e-06,
      "loss": 2.21,
      "step": 3401
    },
    {
      "epoch": 7.0875,
      "grad_norm": 6.676488876342773,
      "learning_rate": 3.2361111111111117e-06,
      "loss": 0.3282,
      "step": 3402
    },
    {
      "epoch": 7.089583333333334,
      "grad_norm": 14.978672981262207,
      "learning_rate": 3.2337962962962968e-06,
      "loss": 0.7296,
      "step": 3403
    },
    {
      "epoch": 7.091666666666667,
      "grad_norm": 29.738895416259766,
      "learning_rate": 3.2314814814814814e-06,
      "loss": 1.9516,
      "step": 3404
    },
    {
      "epoch": 7.09375,
      "grad_norm": 8.84156322479248,
      "learning_rate": 3.229166666666667e-06,
      "loss": 0.806,
      "step": 3405
    },
    {
      "epoch": 7.095833333333333,
      "grad_norm": 57.461097717285156,
      "learning_rate": 3.226851851851852e-06,
      "loss": 1.7609,
      "step": 3406
    },
    {
      "epoch": 7.097916666666666,
      "grad_norm": 12.188767433166504,
      "learning_rate": 3.224537037037037e-06,
      "loss": 1.0931,
      "step": 3407
    },
    {
      "epoch": 7.1,
      "grad_norm": 26.22235870361328,
      "learning_rate": 3.2222222222222227e-06,
      "loss": 1.2763,
      "step": 3408
    },
    {
      "epoch": 7.102083333333334,
      "grad_norm": 22.023723602294922,
      "learning_rate": 3.2199074074074078e-06,
      "loss": 1.3284,
      "step": 3409
    },
    {
      "epoch": 7.104166666666667,
      "grad_norm": 63.72650146484375,
      "learning_rate": 3.217592592592593e-06,
      "loss": 0.8688,
      "step": 3410
    },
    {
      "epoch": 7.10625,
      "grad_norm": 6.8457489013671875,
      "learning_rate": 3.2152777777777784e-06,
      "loss": 1.8035,
      "step": 3411
    },
    {
      "epoch": 7.108333333333333,
      "grad_norm": 6.49717378616333,
      "learning_rate": 3.2129629629629635e-06,
      "loss": 0.9616,
      "step": 3412
    },
    {
      "epoch": 7.110416666666667,
      "grad_norm": 48.601806640625,
      "learning_rate": 3.210648148148148e-06,
      "loss": 1.1544,
      "step": 3413
    },
    {
      "epoch": 7.1125,
      "grad_norm": 8.976884841918945,
      "learning_rate": 3.2083333333333337e-06,
      "loss": 0.63,
      "step": 3414
    },
    {
      "epoch": 7.114583333333333,
      "grad_norm": 48.63069534301758,
      "learning_rate": 3.2060185185185188e-06,
      "loss": 0.635,
      "step": 3415
    },
    {
      "epoch": 7.116666666666666,
      "grad_norm": 30.877601623535156,
      "learning_rate": 3.203703703703704e-06,
      "loss": 0.7736,
      "step": 3416
    },
    {
      "epoch": 7.11875,
      "grad_norm": 25.672883987426758,
      "learning_rate": 3.201388888888889e-06,
      "loss": 0.8534,
      "step": 3417
    },
    {
      "epoch": 7.120833333333334,
      "grad_norm": 54.079952239990234,
      "learning_rate": 3.1990740740740745e-06,
      "loss": 1.7891,
      "step": 3418
    },
    {
      "epoch": 7.122916666666667,
      "grad_norm": 59.905391693115234,
      "learning_rate": 3.1967592592592596e-06,
      "loss": 1.693,
      "step": 3419
    },
    {
      "epoch": 7.125,
      "grad_norm": 17.694538116455078,
      "learning_rate": 3.1944444444444443e-06,
      "loss": 2.3288,
      "step": 3420
    },
    {
      "epoch": 7.127083333333333,
      "grad_norm": 20.09495735168457,
      "learning_rate": 3.19212962962963e-06,
      "loss": 1.0005,
      "step": 3421
    },
    {
      "epoch": 7.129166666666666,
      "grad_norm": 47.44550323486328,
      "learning_rate": 3.189814814814815e-06,
      "loss": 0.8174,
      "step": 3422
    },
    {
      "epoch": 7.13125,
      "grad_norm": 28.213165283203125,
      "learning_rate": 3.1875e-06,
      "loss": 1.517,
      "step": 3423
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 6.079566955566406,
      "learning_rate": 3.1851851851851855e-06,
      "loss": 0.7226,
      "step": 3424
    },
    {
      "epoch": 7.135416666666667,
      "grad_norm": 37.52182388305664,
      "learning_rate": 3.1828703703703706e-06,
      "loss": 1.4112,
      "step": 3425
    },
    {
      "epoch": 7.1375,
      "grad_norm": 57.54835510253906,
      "learning_rate": 3.1805555555555557e-06,
      "loss": 1.5144,
      "step": 3426
    },
    {
      "epoch": 7.139583333333333,
      "grad_norm": 7.331507205963135,
      "learning_rate": 3.1782407407407412e-06,
      "loss": 0.5997,
      "step": 3427
    },
    {
      "epoch": 7.141666666666667,
      "grad_norm": 10.81025218963623,
      "learning_rate": 3.1759259259259263e-06,
      "loss": 1.2764,
      "step": 3428
    },
    {
      "epoch": 7.14375,
      "grad_norm": 6.609555721282959,
      "learning_rate": 3.1736111111111114e-06,
      "loss": 1.0155,
      "step": 3429
    },
    {
      "epoch": 7.145833333333333,
      "grad_norm": 12.93248462677002,
      "learning_rate": 3.171296296296297e-06,
      "loss": 0.7735,
      "step": 3430
    },
    {
      "epoch": 7.147916666666666,
      "grad_norm": 9.112159729003906,
      "learning_rate": 3.1689814814814816e-06,
      "loss": 0.9524,
      "step": 3431
    },
    {
      "epoch": 7.15,
      "grad_norm": 5.634632587432861,
      "learning_rate": 3.1666666666666667e-06,
      "loss": 0.5791,
      "step": 3432
    },
    {
      "epoch": 7.152083333333334,
      "grad_norm": 11.316228866577148,
      "learning_rate": 3.1643518518518522e-06,
      "loss": 1.1102,
      "step": 3433
    },
    {
      "epoch": 7.154166666666667,
      "grad_norm": 6.348420143127441,
      "learning_rate": 3.1620370370370373e-06,
      "loss": 0.7768,
      "step": 3434
    },
    {
      "epoch": 7.15625,
      "grad_norm": 25.620758056640625,
      "learning_rate": 3.1597222222222224e-06,
      "loss": 1.3079,
      "step": 3435
    },
    {
      "epoch": 7.158333333333333,
      "grad_norm": 13.271943092346191,
      "learning_rate": 3.157407407407408e-06,
      "loss": 1.6479,
      "step": 3436
    },
    {
      "epoch": 7.160416666666666,
      "grad_norm": 10.208378791809082,
      "learning_rate": 3.155092592592593e-06,
      "loss": 1.1176,
      "step": 3437
    },
    {
      "epoch": 7.1625,
      "grad_norm": 9.847487449645996,
      "learning_rate": 3.152777777777778e-06,
      "loss": 1.5048,
      "step": 3438
    },
    {
      "epoch": 7.164583333333334,
      "grad_norm": 8.497209548950195,
      "learning_rate": 3.150462962962963e-06,
      "loss": 0.5698,
      "step": 3439
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 11.589018821716309,
      "learning_rate": 3.1481481481481483e-06,
      "loss": 1.0516,
      "step": 3440
    },
    {
      "epoch": 7.16875,
      "grad_norm": 19.49235725402832,
      "learning_rate": 3.1458333333333334e-06,
      "loss": 1.2398,
      "step": 3441
    },
    {
      "epoch": 7.170833333333333,
      "grad_norm": 9.663986206054688,
      "learning_rate": 3.1435185185185185e-06,
      "loss": 0.7378,
      "step": 3442
    },
    {
      "epoch": 7.172916666666667,
      "grad_norm": 17.975297927856445,
      "learning_rate": 3.141203703703704e-06,
      "loss": 1.8069,
      "step": 3443
    },
    {
      "epoch": 7.175,
      "grad_norm": 22.14020538330078,
      "learning_rate": 3.138888888888889e-06,
      "loss": 0.9196,
      "step": 3444
    },
    {
      "epoch": 7.177083333333333,
      "grad_norm": 23.63958740234375,
      "learning_rate": 3.1365740740740742e-06,
      "loss": 1.1467,
      "step": 3445
    },
    {
      "epoch": 7.179166666666666,
      "grad_norm": 24.539857864379883,
      "learning_rate": 3.1342592592592598e-06,
      "loss": 1.5508,
      "step": 3446
    },
    {
      "epoch": 7.18125,
      "grad_norm": 21.879623413085938,
      "learning_rate": 3.131944444444445e-06,
      "loss": 1.1179,
      "step": 3447
    },
    {
      "epoch": 7.183333333333334,
      "grad_norm": 18.019433975219727,
      "learning_rate": 3.1296296296296295e-06,
      "loss": 1.049,
      "step": 3448
    },
    {
      "epoch": 7.185416666666667,
      "grad_norm": 11.64961051940918,
      "learning_rate": 3.127314814814815e-06,
      "loss": 1.204,
      "step": 3449
    },
    {
      "epoch": 7.1875,
      "grad_norm": 13.97445297241211,
      "learning_rate": 3.125e-06,
      "loss": 1.2421,
      "step": 3450
    },
    {
      "epoch": 7.189583333333333,
      "grad_norm": 14.103302001953125,
      "learning_rate": 3.1226851851851852e-06,
      "loss": 1.2744,
      "step": 3451
    },
    {
      "epoch": 7.191666666666666,
      "grad_norm": 33.19229507446289,
      "learning_rate": 3.1203703703703708e-06,
      "loss": 1.0584,
      "step": 3452
    },
    {
      "epoch": 7.19375,
      "grad_norm": 154.79913330078125,
      "learning_rate": 3.118055555555556e-06,
      "loss": 1.4354,
      "step": 3453
    },
    {
      "epoch": 7.195833333333334,
      "grad_norm": 19.578624725341797,
      "learning_rate": 3.115740740740741e-06,
      "loss": 1.208,
      "step": 3454
    },
    {
      "epoch": 7.197916666666667,
      "grad_norm": 3.84724760055542,
      "learning_rate": 3.1134259259259265e-06,
      "loss": 0.1036,
      "step": 3455
    },
    {
      "epoch": 7.2,
      "grad_norm": 25.749292373657227,
      "learning_rate": 3.1111111111111116e-06,
      "loss": 1.4021,
      "step": 3456
    },
    {
      "epoch": 7.202083333333333,
      "grad_norm": 8.999017715454102,
      "learning_rate": 3.1087962962962963e-06,
      "loss": 0.6064,
      "step": 3457
    },
    {
      "epoch": 7.204166666666667,
      "grad_norm": 19.232973098754883,
      "learning_rate": 3.1064814814814818e-06,
      "loss": 1.5528,
      "step": 3458
    },
    {
      "epoch": 7.20625,
      "grad_norm": 69.09200286865234,
      "learning_rate": 3.104166666666667e-06,
      "loss": 1.5015,
      "step": 3459
    },
    {
      "epoch": 7.208333333333333,
      "grad_norm": 93.4761962890625,
      "learning_rate": 3.101851851851852e-06,
      "loss": 1.6956,
      "step": 3460
    },
    {
      "epoch": 7.210416666666666,
      "grad_norm": 8.12049674987793,
      "learning_rate": 3.0995370370370375e-06,
      "loss": 0.7342,
      "step": 3461
    },
    {
      "epoch": 7.2125,
      "grad_norm": 35.46592712402344,
      "learning_rate": 3.0972222222222226e-06,
      "loss": 1.0436,
      "step": 3462
    },
    {
      "epoch": 7.214583333333334,
      "grad_norm": 25.62652587890625,
      "learning_rate": 3.0949074074074077e-06,
      "loss": 1.4924,
      "step": 3463
    },
    {
      "epoch": 7.216666666666667,
      "grad_norm": 12.271061897277832,
      "learning_rate": 3.0925925925925928e-06,
      "loss": 1.2702,
      "step": 3464
    },
    {
      "epoch": 7.21875,
      "grad_norm": 14.003610610961914,
      "learning_rate": 3.0902777777777783e-06,
      "loss": 1.4839,
      "step": 3465
    },
    {
      "epoch": 7.220833333333333,
      "grad_norm": 9.064988136291504,
      "learning_rate": 3.087962962962963e-06,
      "loss": 1.1236,
      "step": 3466
    },
    {
      "epoch": 7.222916666666666,
      "grad_norm": 35.42302703857422,
      "learning_rate": 3.085648148148148e-06,
      "loss": 1.1875,
      "step": 3467
    },
    {
      "epoch": 7.225,
      "grad_norm": 11.462800979614258,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.7912,
      "step": 3468
    },
    {
      "epoch": 7.227083333333334,
      "grad_norm": 49.53496170043945,
      "learning_rate": 3.0810185185185187e-06,
      "loss": 0.8444,
      "step": 3469
    },
    {
      "epoch": 7.229166666666667,
      "grad_norm": 10.303289413452148,
      "learning_rate": 3.078703703703704e-06,
      "loss": 1.1903,
      "step": 3470
    },
    {
      "epoch": 7.23125,
      "grad_norm": 29.796470642089844,
      "learning_rate": 3.0763888888888893e-06,
      "loss": 1.8017,
      "step": 3471
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 19.35892105102539,
      "learning_rate": 3.0740740740740744e-06,
      "loss": 1.1323,
      "step": 3472
    },
    {
      "epoch": 7.235416666666667,
      "grad_norm": 25.12809181213379,
      "learning_rate": 3.0717592592592595e-06,
      "loss": 1.855,
      "step": 3473
    },
    {
      "epoch": 7.2375,
      "grad_norm": 75.85609436035156,
      "learning_rate": 3.069444444444445e-06,
      "loss": 0.878,
      "step": 3474
    },
    {
      "epoch": 7.239583333333333,
      "grad_norm": 7.802272796630859,
      "learning_rate": 3.0671296296296297e-06,
      "loss": 0.3009,
      "step": 3475
    },
    {
      "epoch": 7.241666666666666,
      "grad_norm": 7.577181816101074,
      "learning_rate": 3.064814814814815e-06,
      "loss": 1.0971,
      "step": 3476
    },
    {
      "epoch": 7.24375,
      "grad_norm": 25.58934211730957,
      "learning_rate": 3.0625000000000003e-06,
      "loss": 0.7138,
      "step": 3477
    },
    {
      "epoch": 7.245833333333334,
      "grad_norm": 17.411794662475586,
      "learning_rate": 3.0601851851851854e-06,
      "loss": 1.0928,
      "step": 3478
    },
    {
      "epoch": 7.247916666666667,
      "grad_norm": 9.078028678894043,
      "learning_rate": 3.0578703703703705e-06,
      "loss": 0.7181,
      "step": 3479
    },
    {
      "epoch": 7.25,
      "grad_norm": 37.909149169921875,
      "learning_rate": 3.055555555555556e-06,
      "loss": 0.6625,
      "step": 3480
    },
    {
      "epoch": 7.252083333333333,
      "grad_norm": 14.850003242492676,
      "learning_rate": 3.053240740740741e-06,
      "loss": 1.3066,
      "step": 3481
    },
    {
      "epoch": 7.254166666666666,
      "grad_norm": 4.942549705505371,
      "learning_rate": 3.0509259259259262e-06,
      "loss": 0.1558,
      "step": 3482
    },
    {
      "epoch": 7.25625,
      "grad_norm": 90.73078155517578,
      "learning_rate": 3.0486111111111118e-06,
      "loss": 0.5814,
      "step": 3483
    },
    {
      "epoch": 7.258333333333334,
      "grad_norm": 58.45635223388672,
      "learning_rate": 3.0462962962962964e-06,
      "loss": 0.9629,
      "step": 3484
    },
    {
      "epoch": 7.260416666666667,
      "grad_norm": 37.36188888549805,
      "learning_rate": 3.0439814814814815e-06,
      "loss": 1.06,
      "step": 3485
    },
    {
      "epoch": 7.2625,
      "grad_norm": 84.77528381347656,
      "learning_rate": 3.0416666666666666e-06,
      "loss": 1.4077,
      "step": 3486
    },
    {
      "epoch": 7.264583333333333,
      "grad_norm": 10.636208534240723,
      "learning_rate": 3.039351851851852e-06,
      "loss": 0.8861,
      "step": 3487
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 8.978046417236328,
      "learning_rate": 3.0370370370370372e-06,
      "loss": 0.7044,
      "step": 3488
    },
    {
      "epoch": 7.26875,
      "grad_norm": 109.21783447265625,
      "learning_rate": 3.0347222222222223e-06,
      "loss": 1.1247,
      "step": 3489
    },
    {
      "epoch": 7.270833333333333,
      "grad_norm": 11.589775085449219,
      "learning_rate": 3.032407407407408e-06,
      "loss": 1.0983,
      "step": 3490
    },
    {
      "epoch": 7.272916666666666,
      "grad_norm": 29.487201690673828,
      "learning_rate": 3.030092592592593e-06,
      "loss": 1.347,
      "step": 3491
    },
    {
      "epoch": 7.275,
      "grad_norm": 24.259435653686523,
      "learning_rate": 3.0277777777777776e-06,
      "loss": 1.0195,
      "step": 3492
    },
    {
      "epoch": 7.277083333333334,
      "grad_norm": 8.726410865783691,
      "learning_rate": 3.0254629629629636e-06,
      "loss": 1.4197,
      "step": 3493
    },
    {
      "epoch": 7.279166666666667,
      "grad_norm": 6.781492710113525,
      "learning_rate": 3.0231481481481483e-06,
      "loss": 0.1406,
      "step": 3494
    },
    {
      "epoch": 7.28125,
      "grad_norm": 3.828138589859009,
      "learning_rate": 3.0208333333333334e-06,
      "loss": 0.1063,
      "step": 3495
    },
    {
      "epoch": 7.283333333333333,
      "grad_norm": 11.029924392700195,
      "learning_rate": 3.018518518518519e-06,
      "loss": 0.7278,
      "step": 3496
    },
    {
      "epoch": 7.285416666666666,
      "grad_norm": 8.241766929626465,
      "learning_rate": 3.016203703703704e-06,
      "loss": 0.5747,
      "step": 3497
    },
    {
      "epoch": 7.2875,
      "grad_norm": 24.452180862426758,
      "learning_rate": 3.013888888888889e-06,
      "loss": 1.1152,
      "step": 3498
    },
    {
      "epoch": 7.289583333333334,
      "grad_norm": 44.958736419677734,
      "learning_rate": 3.0115740740740746e-06,
      "loss": 1.1639,
      "step": 3499
    },
    {
      "epoch": 7.291666666666667,
      "grad_norm": 18.17034912109375,
      "learning_rate": 3.0092592592592597e-06,
      "loss": 0.7936,
      "step": 3500
    },
    {
      "epoch": 7.29375,
      "grad_norm": 21.219724655151367,
      "learning_rate": 3.0069444444444444e-06,
      "loss": 0.5622,
      "step": 3501
    },
    {
      "epoch": 7.295833333333333,
      "grad_norm": 9.286829948425293,
      "learning_rate": 3.0046296296296303e-06,
      "loss": 0.8159,
      "step": 3502
    },
    {
      "epoch": 7.297916666666667,
      "grad_norm": 4.767704486846924,
      "learning_rate": 3.002314814814815e-06,
      "loss": 0.2219,
      "step": 3503
    },
    {
      "epoch": 7.3,
      "grad_norm": 6.298069000244141,
      "learning_rate": 3e-06,
      "loss": 0.1653,
      "step": 3504
    },
    {
      "epoch": 7.302083333333333,
      "grad_norm": 3.9867992401123047,
      "learning_rate": 2.9976851851851856e-06,
      "loss": 0.1186,
      "step": 3505
    },
    {
      "epoch": 7.304166666666666,
      "grad_norm": 49.88336944580078,
      "learning_rate": 2.9953703703703707e-06,
      "loss": 1.6515,
      "step": 3506
    },
    {
      "epoch": 7.30625,
      "grad_norm": 16.890443801879883,
      "learning_rate": 2.993055555555556e-06,
      "loss": 0.947,
      "step": 3507
    },
    {
      "epoch": 7.308333333333334,
      "grad_norm": 6.785431385040283,
      "learning_rate": 2.990740740740741e-06,
      "loss": 0.6181,
      "step": 3508
    },
    {
      "epoch": 7.310416666666667,
      "grad_norm": 12.716791152954102,
      "learning_rate": 2.9884259259259264e-06,
      "loss": 0.6275,
      "step": 3509
    },
    {
      "epoch": 7.3125,
      "grad_norm": 26.37067985534668,
      "learning_rate": 2.986111111111111e-06,
      "loss": 1.7445,
      "step": 3510
    },
    {
      "epoch": 7.314583333333333,
      "grad_norm": 128.7861785888672,
      "learning_rate": 2.983796296296296e-06,
      "loss": 1.297,
      "step": 3511
    },
    {
      "epoch": 7.316666666666666,
      "grad_norm": 8.469250679016113,
      "learning_rate": 2.9814814814814817e-06,
      "loss": 0.6174,
      "step": 3512
    },
    {
      "epoch": 7.31875,
      "grad_norm": 8.093239784240723,
      "learning_rate": 2.979166666666667e-06,
      "loss": 0.6814,
      "step": 3513
    },
    {
      "epoch": 7.320833333333334,
      "grad_norm": 29.086816787719727,
      "learning_rate": 2.976851851851852e-06,
      "loss": 0.6722,
      "step": 3514
    },
    {
      "epoch": 7.322916666666667,
      "grad_norm": 8.331524848937988,
      "learning_rate": 2.9745370370370374e-06,
      "loss": 0.6088,
      "step": 3515
    },
    {
      "epoch": 7.325,
      "grad_norm": 8.605606079101562,
      "learning_rate": 2.9722222222222225e-06,
      "loss": 1.1139,
      "step": 3516
    },
    {
      "epoch": 7.327083333333333,
      "grad_norm": 26.224851608276367,
      "learning_rate": 2.9699074074074076e-06,
      "loss": 0.9379,
      "step": 3517
    },
    {
      "epoch": 7.329166666666667,
      "grad_norm": 20.15190887451172,
      "learning_rate": 2.967592592592593e-06,
      "loss": 1.5152,
      "step": 3518
    },
    {
      "epoch": 7.33125,
      "grad_norm": 27.654769897460938,
      "learning_rate": 2.9652777777777782e-06,
      "loss": 1.3238,
      "step": 3519
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 17.314186096191406,
      "learning_rate": 2.962962962962963e-06,
      "loss": 0.8324,
      "step": 3520
    },
    {
      "epoch": 7.335416666666666,
      "grad_norm": 16.662017822265625,
      "learning_rate": 2.9606481481481484e-06,
      "loss": 1.2073,
      "step": 3521
    },
    {
      "epoch": 7.3375,
      "grad_norm": 12.621149063110352,
      "learning_rate": 2.9583333333333335e-06,
      "loss": 1.1915,
      "step": 3522
    },
    {
      "epoch": 7.339583333333334,
      "grad_norm": 18.533628463745117,
      "learning_rate": 2.9560185185185186e-06,
      "loss": 1.0563,
      "step": 3523
    },
    {
      "epoch": 7.341666666666667,
      "grad_norm": 47.84166717529297,
      "learning_rate": 2.953703703703704e-06,
      "loss": 1.4851,
      "step": 3524
    },
    {
      "epoch": 7.34375,
      "grad_norm": 104.65959167480469,
      "learning_rate": 2.9513888888888892e-06,
      "loss": 1.8232,
      "step": 3525
    },
    {
      "epoch": 7.345833333333333,
      "grad_norm": 9.948877334594727,
      "learning_rate": 2.9490740740740743e-06,
      "loss": 0.6065,
      "step": 3526
    },
    {
      "epoch": 7.347916666666666,
      "grad_norm": 29.463010787963867,
      "learning_rate": 2.94675925925926e-06,
      "loss": 1.2393,
      "step": 3527
    },
    {
      "epoch": 7.35,
      "grad_norm": 39.495296478271484,
      "learning_rate": 2.944444444444445e-06,
      "loss": 1.399,
      "step": 3528
    },
    {
      "epoch": 7.352083333333334,
      "grad_norm": 16.48163414001465,
      "learning_rate": 2.9421296296296296e-06,
      "loss": 1.3779,
      "step": 3529
    },
    {
      "epoch": 7.354166666666667,
      "grad_norm": 7.910675525665283,
      "learning_rate": 2.9398148148148147e-06,
      "loss": 0.5597,
      "step": 3530
    },
    {
      "epoch": 7.35625,
      "grad_norm": 111.27947998046875,
      "learning_rate": 2.9375000000000003e-06,
      "loss": 2.6521,
      "step": 3531
    },
    {
      "epoch": 7.358333333333333,
      "grad_norm": 7.4371843338012695,
      "learning_rate": 2.9351851851851853e-06,
      "loss": 0.997,
      "step": 3532
    },
    {
      "epoch": 7.360416666666667,
      "grad_norm": 36.11269760131836,
      "learning_rate": 2.9328703703703704e-06,
      "loss": 0.7859,
      "step": 3533
    },
    {
      "epoch": 7.3625,
      "grad_norm": 50.536808013916016,
      "learning_rate": 2.930555555555556e-06,
      "loss": 1.6814,
      "step": 3534
    },
    {
      "epoch": 7.364583333333333,
      "grad_norm": 8.277294158935547,
      "learning_rate": 2.928240740740741e-06,
      "loss": 0.5634,
      "step": 3535
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 64.3221435546875,
      "learning_rate": 2.9259259259259257e-06,
      "loss": 1.112,
      "step": 3536
    },
    {
      "epoch": 7.36875,
      "grad_norm": 14.406845092773438,
      "learning_rate": 2.9236111111111117e-06,
      "loss": 0.5931,
      "step": 3537
    },
    {
      "epoch": 7.370833333333334,
      "grad_norm": 7.551350116729736,
      "learning_rate": 2.9212962962962964e-06,
      "loss": 0.5408,
      "step": 3538
    },
    {
      "epoch": 7.372916666666667,
      "grad_norm": 47.05802536010742,
      "learning_rate": 2.9189814814814815e-06,
      "loss": 1.7997,
      "step": 3539
    },
    {
      "epoch": 7.375,
      "grad_norm": 89.13943481445312,
      "learning_rate": 2.916666666666667e-06,
      "loss": 1.5691,
      "step": 3540
    },
    {
      "epoch": 7.377083333333333,
      "grad_norm": 16.382253646850586,
      "learning_rate": 2.914351851851852e-06,
      "loss": 1.0069,
      "step": 3541
    },
    {
      "epoch": 7.379166666666666,
      "grad_norm": 45.52473449707031,
      "learning_rate": 2.912037037037037e-06,
      "loss": 1.62,
      "step": 3542
    },
    {
      "epoch": 7.38125,
      "grad_norm": 10.8056058883667,
      "learning_rate": 2.9097222222222227e-06,
      "loss": 1.123,
      "step": 3543
    },
    {
      "epoch": 7.383333333333334,
      "grad_norm": 12.020273208618164,
      "learning_rate": 2.907407407407408e-06,
      "loss": 0.7148,
      "step": 3544
    },
    {
      "epoch": 7.385416666666667,
      "grad_norm": 29.081756591796875,
      "learning_rate": 2.905092592592593e-06,
      "loss": 0.6839,
      "step": 3545
    },
    {
      "epoch": 7.3875,
      "grad_norm": 9.693109512329102,
      "learning_rate": 2.9027777777777784e-06,
      "loss": 1.1001,
      "step": 3546
    },
    {
      "epoch": 7.389583333333333,
      "grad_norm": 13.636918067932129,
      "learning_rate": 2.900462962962963e-06,
      "loss": 0.7075,
      "step": 3547
    },
    {
      "epoch": 7.391666666666667,
      "grad_norm": 17.856063842773438,
      "learning_rate": 2.898148148148148e-06,
      "loss": 0.8561,
      "step": 3548
    },
    {
      "epoch": 7.39375,
      "grad_norm": 16.09902572631836,
      "learning_rate": 2.8958333333333337e-06,
      "loss": 1.1087,
      "step": 3549
    },
    {
      "epoch": 7.395833333333333,
      "grad_norm": 23.229549407958984,
      "learning_rate": 2.893518518518519e-06,
      "loss": 0.882,
      "step": 3550
    },
    {
      "epoch": 7.397916666666666,
      "grad_norm": 6.533660411834717,
      "learning_rate": 2.891203703703704e-06,
      "loss": 0.9734,
      "step": 3551
    },
    {
      "epoch": 7.4,
      "grad_norm": 27.238229751586914,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.6459,
      "step": 3552
    },
    {
      "epoch": 7.402083333333334,
      "grad_norm": 14.411246299743652,
      "learning_rate": 2.8865740740740745e-06,
      "loss": 1.1705,
      "step": 3553
    },
    {
      "epoch": 7.404166666666667,
      "grad_norm": 28.1679744720459,
      "learning_rate": 2.8842592592592596e-06,
      "loss": 2.1074,
      "step": 3554
    },
    {
      "epoch": 7.40625,
      "grad_norm": 63.99889373779297,
      "learning_rate": 2.8819444444444443e-06,
      "loss": 1.3256,
      "step": 3555
    },
    {
      "epoch": 7.408333333333333,
      "grad_norm": 10.442880630493164,
      "learning_rate": 2.87962962962963e-06,
      "loss": 1.1626,
      "step": 3556
    },
    {
      "epoch": 7.410416666666666,
      "grad_norm": 10.398323059082031,
      "learning_rate": 2.877314814814815e-06,
      "loss": 0.6519,
      "step": 3557
    },
    {
      "epoch": 7.4125,
      "grad_norm": 30.54114532470703,
      "learning_rate": 2.875e-06,
      "loss": 0.6929,
      "step": 3558
    },
    {
      "epoch": 7.414583333333334,
      "grad_norm": 34.66029739379883,
      "learning_rate": 2.8726851851851855e-06,
      "loss": 0.3411,
      "step": 3559
    },
    {
      "epoch": 7.416666666666667,
      "grad_norm": 11.385039329528809,
      "learning_rate": 2.8703703703703706e-06,
      "loss": 0.8032,
      "step": 3560
    },
    {
      "epoch": 7.41875,
      "grad_norm": 6.388288974761963,
      "learning_rate": 2.8680555555555557e-06,
      "loss": 0.577,
      "step": 3561
    },
    {
      "epoch": 7.420833333333333,
      "grad_norm": 24.513931274414062,
      "learning_rate": 2.8657407407407412e-06,
      "loss": 1.0419,
      "step": 3562
    },
    {
      "epoch": 7.422916666666667,
      "grad_norm": 10.473139762878418,
      "learning_rate": 2.8634259259259263e-06,
      "loss": 0.9048,
      "step": 3563
    },
    {
      "epoch": 7.425,
      "grad_norm": 9.079346656799316,
      "learning_rate": 2.861111111111111e-06,
      "loss": 1.033,
      "step": 3564
    },
    {
      "epoch": 7.427083333333333,
      "grad_norm": 11.698452949523926,
      "learning_rate": 2.8587962962962965e-06,
      "loss": 1.4015,
      "step": 3565
    },
    {
      "epoch": 7.429166666666666,
      "grad_norm": 14.619104385375977,
      "learning_rate": 2.8564814814814816e-06,
      "loss": 0.7223,
      "step": 3566
    },
    {
      "epoch": 7.43125,
      "grad_norm": 63.21839904785156,
      "learning_rate": 2.8541666666666667e-06,
      "loss": 1.5545,
      "step": 3567
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 9.438141822814941,
      "learning_rate": 2.8518518518518522e-06,
      "loss": 0.6743,
      "step": 3568
    },
    {
      "epoch": 7.435416666666667,
      "grad_norm": 16.878799438476562,
      "learning_rate": 2.8495370370370373e-06,
      "loss": 0.826,
      "step": 3569
    },
    {
      "epoch": 7.4375,
      "grad_norm": 32.45354461669922,
      "learning_rate": 2.8472222222222224e-06,
      "loss": 1.6443,
      "step": 3570
    },
    {
      "epoch": 7.439583333333333,
      "grad_norm": 9.528642654418945,
      "learning_rate": 2.844907407407408e-06,
      "loss": 0.7145,
      "step": 3571
    },
    {
      "epoch": 7.441666666666666,
      "grad_norm": 6.142322063446045,
      "learning_rate": 2.842592592592593e-06,
      "loss": 0.5475,
      "step": 3572
    },
    {
      "epoch": 7.44375,
      "grad_norm": 22.1881160736084,
      "learning_rate": 2.8402777777777777e-06,
      "loss": 1.4339,
      "step": 3573
    },
    {
      "epoch": 7.445833333333334,
      "grad_norm": 18.800722122192383,
      "learning_rate": 2.837962962962963e-06,
      "loss": 1.4734,
      "step": 3574
    },
    {
      "epoch": 7.447916666666667,
      "grad_norm": 36.226219177246094,
      "learning_rate": 2.8356481481481484e-06,
      "loss": 0.7438,
      "step": 3575
    },
    {
      "epoch": 7.45,
      "grad_norm": 21.430200576782227,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 1.5095,
      "step": 3576
    },
    {
      "epoch": 7.452083333333333,
      "grad_norm": 11.954110145568848,
      "learning_rate": 2.8310185185185185e-06,
      "loss": 0.3679,
      "step": 3577
    },
    {
      "epoch": 7.454166666666667,
      "grad_norm": 16.954336166381836,
      "learning_rate": 2.828703703703704e-06,
      "loss": 0.8987,
      "step": 3578
    },
    {
      "epoch": 7.45625,
      "grad_norm": 9.661752700805664,
      "learning_rate": 2.826388888888889e-06,
      "loss": 1.0173,
      "step": 3579
    },
    {
      "epoch": 7.458333333333333,
      "grad_norm": 50.473175048828125,
      "learning_rate": 2.8240740740740743e-06,
      "loss": 0.9803,
      "step": 3580
    },
    {
      "epoch": 7.460416666666666,
      "grad_norm": 4.994325637817383,
      "learning_rate": 2.8217592592592598e-06,
      "loss": 0.6301,
      "step": 3581
    },
    {
      "epoch": 7.4625,
      "grad_norm": 4.5017900466918945,
      "learning_rate": 2.8194444444444445e-06,
      "loss": 0.1318,
      "step": 3582
    },
    {
      "epoch": 7.464583333333334,
      "grad_norm": 35.329532623291016,
      "learning_rate": 2.8171296296296296e-06,
      "loss": 1.6933,
      "step": 3583
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 28.23699188232422,
      "learning_rate": 2.814814814814815e-06,
      "loss": 1.1398,
      "step": 3584
    },
    {
      "epoch": 7.46875,
      "grad_norm": 19.575191497802734,
      "learning_rate": 2.8125e-06,
      "loss": 1.1723,
      "step": 3585
    },
    {
      "epoch": 7.470833333333333,
      "grad_norm": 61.199317932128906,
      "learning_rate": 2.8101851851851853e-06,
      "loss": 0.3487,
      "step": 3586
    },
    {
      "epoch": 7.472916666666666,
      "grad_norm": 15.524656295776367,
      "learning_rate": 2.807870370370371e-06,
      "loss": 1.8895,
      "step": 3587
    },
    {
      "epoch": 7.475,
      "grad_norm": 22.050758361816406,
      "learning_rate": 2.805555555555556e-06,
      "loss": 1.4793,
      "step": 3588
    },
    {
      "epoch": 7.477083333333334,
      "grad_norm": 17.021808624267578,
      "learning_rate": 2.803240740740741e-06,
      "loss": 1.1347,
      "step": 3589
    },
    {
      "epoch": 7.479166666666667,
      "grad_norm": 6.82460880279541,
      "learning_rate": 2.8009259259259265e-06,
      "loss": 0.6104,
      "step": 3590
    },
    {
      "epoch": 7.48125,
      "grad_norm": 63.80146789550781,
      "learning_rate": 2.798611111111111e-06,
      "loss": 2.513,
      "step": 3591
    },
    {
      "epoch": 7.483333333333333,
      "grad_norm": 8.063558578491211,
      "learning_rate": 2.7962962962962963e-06,
      "loss": 0.7252,
      "step": 3592
    },
    {
      "epoch": 7.485416666666667,
      "grad_norm": 18.50151252746582,
      "learning_rate": 2.793981481481482e-06,
      "loss": 1.1158,
      "step": 3593
    },
    {
      "epoch": 7.4875,
      "grad_norm": 15.425544738769531,
      "learning_rate": 2.791666666666667e-06,
      "loss": 0.6031,
      "step": 3594
    },
    {
      "epoch": 7.489583333333333,
      "grad_norm": 64.28814697265625,
      "learning_rate": 2.789351851851852e-06,
      "loss": 1.9832,
      "step": 3595
    },
    {
      "epoch": 7.491666666666666,
      "grad_norm": 31.60550308227539,
      "learning_rate": 2.7870370370370375e-06,
      "loss": 1.2806,
      "step": 3596
    },
    {
      "epoch": 7.49375,
      "grad_norm": 6.6384782791137695,
      "learning_rate": 2.7847222222222226e-06,
      "loss": 0.2606,
      "step": 3597
    },
    {
      "epoch": 7.495833333333334,
      "grad_norm": 39.34120178222656,
      "learning_rate": 2.7824074074074077e-06,
      "loss": 1.0942,
      "step": 3598
    },
    {
      "epoch": 7.497916666666667,
      "grad_norm": 39.6353645324707,
      "learning_rate": 2.7800925925925924e-06,
      "loss": 1.1524,
      "step": 3599
    },
    {
      "epoch": 7.5,
      "grad_norm": 43.6107292175293,
      "learning_rate": 2.7777777777777783e-06,
      "loss": 1.1457,
      "step": 3600
    },
    {
      "epoch": 7.502083333333333,
      "grad_norm": 43.630706787109375,
      "learning_rate": 2.775462962962963e-06,
      "loss": 0.9551,
      "step": 3601
    },
    {
      "epoch": 7.504166666666666,
      "grad_norm": 26.709754943847656,
      "learning_rate": 2.773148148148148e-06,
      "loss": 1.2818,
      "step": 3602
    },
    {
      "epoch": 7.50625,
      "grad_norm": 7.062775611877441,
      "learning_rate": 2.7708333333333336e-06,
      "loss": 0.5405,
      "step": 3603
    },
    {
      "epoch": 7.508333333333333,
      "grad_norm": 9.4700345993042,
      "learning_rate": 2.7685185185185187e-06,
      "loss": 0.7018,
      "step": 3604
    },
    {
      "epoch": 7.510416666666667,
      "grad_norm": 18.6095027923584,
      "learning_rate": 2.766203703703704e-06,
      "loss": 0.8832,
      "step": 3605
    },
    {
      "epoch": 7.5125,
      "grad_norm": 16.723291397094727,
      "learning_rate": 2.7638888888888893e-06,
      "loss": 0.8943,
      "step": 3606
    },
    {
      "epoch": 7.514583333333333,
      "grad_norm": 15.560973167419434,
      "learning_rate": 2.7615740740740744e-06,
      "loss": 0.6095,
      "step": 3607
    },
    {
      "epoch": 7.516666666666667,
      "grad_norm": 23.072843551635742,
      "learning_rate": 2.759259259259259e-06,
      "loss": 1.0816,
      "step": 3608
    },
    {
      "epoch": 7.51875,
      "grad_norm": 17.414987564086914,
      "learning_rate": 2.756944444444445e-06,
      "loss": 1.7404,
      "step": 3609
    },
    {
      "epoch": 7.520833333333333,
      "grad_norm": 61.077632904052734,
      "learning_rate": 2.7546296296296297e-06,
      "loss": 1.7038,
      "step": 3610
    },
    {
      "epoch": 7.522916666666667,
      "grad_norm": 48.93572235107422,
      "learning_rate": 2.752314814814815e-06,
      "loss": 1.3374,
      "step": 3611
    },
    {
      "epoch": 7.525,
      "grad_norm": 13.25523853302002,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.8736,
      "step": 3612
    },
    {
      "epoch": 7.527083333333334,
      "grad_norm": 85.74697875976562,
      "learning_rate": 2.7476851851851854e-06,
      "loss": 1.178,
      "step": 3613
    },
    {
      "epoch": 7.529166666666667,
      "grad_norm": 61.35190963745117,
      "learning_rate": 2.7453703703703705e-06,
      "loss": 0.4922,
      "step": 3614
    },
    {
      "epoch": 7.53125,
      "grad_norm": 9.581509590148926,
      "learning_rate": 2.743055555555556e-06,
      "loss": 1.2063,
      "step": 3615
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 4.626865386962891,
      "learning_rate": 2.740740740740741e-06,
      "loss": 0.2397,
      "step": 3616
    },
    {
      "epoch": 7.535416666666666,
      "grad_norm": 9.549103736877441,
      "learning_rate": 2.738425925925926e-06,
      "loss": 0.8088,
      "step": 3617
    },
    {
      "epoch": 7.5375,
      "grad_norm": 65.00791931152344,
      "learning_rate": 2.7361111111111118e-06,
      "loss": 1.7845,
      "step": 3618
    },
    {
      "epoch": 7.539583333333333,
      "grad_norm": 8.852416038513184,
      "learning_rate": 2.7337962962962965e-06,
      "loss": 0.9802,
      "step": 3619
    },
    {
      "epoch": 7.541666666666667,
      "grad_norm": 33.84933853149414,
      "learning_rate": 2.7314814814814816e-06,
      "loss": 1.1799,
      "step": 3620
    },
    {
      "epoch": 7.54375,
      "grad_norm": 11.703516006469727,
      "learning_rate": 2.7291666666666667e-06,
      "loss": 1.0087,
      "step": 3621
    },
    {
      "epoch": 7.545833333333333,
      "grad_norm": 9.866474151611328,
      "learning_rate": 2.726851851851852e-06,
      "loss": 0.6291,
      "step": 3622
    },
    {
      "epoch": 7.547916666666667,
      "grad_norm": 9.78606128692627,
      "learning_rate": 2.7245370370370373e-06,
      "loss": 0.9699,
      "step": 3623
    },
    {
      "epoch": 7.55,
      "grad_norm": 16.08931541442871,
      "learning_rate": 2.7222222222222224e-06,
      "loss": 1.2742,
      "step": 3624
    },
    {
      "epoch": 7.552083333333333,
      "grad_norm": 31.352039337158203,
      "learning_rate": 2.719907407407408e-06,
      "loss": 1.7669,
      "step": 3625
    },
    {
      "epoch": 7.554166666666667,
      "grad_norm": 15.011707305908203,
      "learning_rate": 2.717592592592593e-06,
      "loss": 0.6018,
      "step": 3626
    },
    {
      "epoch": 7.55625,
      "grad_norm": 29.135913848876953,
      "learning_rate": 2.7152777777777777e-06,
      "loss": 1.1283,
      "step": 3627
    },
    {
      "epoch": 7.558333333333334,
      "grad_norm": 10.541122436523438,
      "learning_rate": 2.712962962962963e-06,
      "loss": 0.7125,
      "step": 3628
    },
    {
      "epoch": 7.560416666666667,
      "grad_norm": 33.781558990478516,
      "learning_rate": 2.7106481481481483e-06,
      "loss": 1.3421,
      "step": 3629
    },
    {
      "epoch": 7.5625,
      "grad_norm": 56.8535041809082,
      "learning_rate": 2.7083333333333334e-06,
      "loss": 1.5338,
      "step": 3630
    },
    {
      "epoch": 7.564583333333333,
      "grad_norm": 4.420912742614746,
      "learning_rate": 2.706018518518519e-06,
      "loss": 0.1191,
      "step": 3631
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 38.70248031616211,
      "learning_rate": 2.703703703703704e-06,
      "loss": 0.939,
      "step": 3632
    },
    {
      "epoch": 7.56875,
      "grad_norm": 4.9753851890563965,
      "learning_rate": 2.701388888888889e-06,
      "loss": 0.6688,
      "step": 3633
    },
    {
      "epoch": 7.570833333333333,
      "grad_norm": 11.929903030395508,
      "learning_rate": 2.6990740740740746e-06,
      "loss": 0.7761,
      "step": 3634
    },
    {
      "epoch": 7.572916666666667,
      "grad_norm": 78.59593963623047,
      "learning_rate": 2.6967592592592597e-06,
      "loss": 1.4224,
      "step": 3635
    },
    {
      "epoch": 7.575,
      "grad_norm": 28.014522552490234,
      "learning_rate": 2.6944444444444444e-06,
      "loss": 1.5733,
      "step": 3636
    },
    {
      "epoch": 7.577083333333333,
      "grad_norm": 7.683343410491943,
      "learning_rate": 2.69212962962963e-06,
      "loss": 0.5401,
      "step": 3637
    },
    {
      "epoch": 7.579166666666667,
      "grad_norm": 14.671492576599121,
      "learning_rate": 2.689814814814815e-06,
      "loss": 0.7885,
      "step": 3638
    },
    {
      "epoch": 7.58125,
      "grad_norm": 67.80731964111328,
      "learning_rate": 2.6875e-06,
      "loss": 0.3833,
      "step": 3639
    },
    {
      "epoch": 7.583333333333333,
      "grad_norm": 14.007455825805664,
      "learning_rate": 2.6851851851851856e-06,
      "loss": 0.7087,
      "step": 3640
    },
    {
      "epoch": 7.585416666666667,
      "grad_norm": 8.689462661743164,
      "learning_rate": 2.6828703703703707e-06,
      "loss": 0.639,
      "step": 3641
    },
    {
      "epoch": 7.5875,
      "grad_norm": 23.035734176635742,
      "learning_rate": 2.680555555555556e-06,
      "loss": 1.0985,
      "step": 3642
    },
    {
      "epoch": 7.589583333333334,
      "grad_norm": 12.485694885253906,
      "learning_rate": 2.6782407407407405e-06,
      "loss": 1.0057,
      "step": 3643
    },
    {
      "epoch": 7.591666666666667,
      "grad_norm": 5.170112133026123,
      "learning_rate": 2.6759259259259264e-06,
      "loss": 0.1402,
      "step": 3644
    },
    {
      "epoch": 7.59375,
      "grad_norm": 10.138998985290527,
      "learning_rate": 2.673611111111111e-06,
      "loss": 0.6892,
      "step": 3645
    },
    {
      "epoch": 7.595833333333333,
      "grad_norm": 13.795212745666504,
      "learning_rate": 2.671296296296296e-06,
      "loss": 1.1199,
      "step": 3646
    },
    {
      "epoch": 7.597916666666666,
      "grad_norm": 26.716331481933594,
      "learning_rate": 2.6689814814814817e-06,
      "loss": 1.6298,
      "step": 3647
    },
    {
      "epoch": 7.6,
      "grad_norm": 21.03572654724121,
      "learning_rate": 2.666666666666667e-06,
      "loss": 1.6641,
      "step": 3648
    },
    {
      "epoch": 7.602083333333333,
      "grad_norm": 50.22358322143555,
      "learning_rate": 2.664351851851852e-06,
      "loss": 1.964,
      "step": 3649
    },
    {
      "epoch": 7.604166666666667,
      "grad_norm": 15.947169303894043,
      "learning_rate": 2.6620370370370374e-06,
      "loss": 0.181,
      "step": 3650
    },
    {
      "epoch": 7.60625,
      "grad_norm": 8.016609191894531,
      "learning_rate": 2.6597222222222225e-06,
      "loss": 0.5467,
      "step": 3651
    },
    {
      "epoch": 7.608333333333333,
      "grad_norm": 71.48843383789062,
      "learning_rate": 2.6574074074074076e-06,
      "loss": 1.3791,
      "step": 3652
    },
    {
      "epoch": 7.610416666666667,
      "grad_norm": 46.69275665283203,
      "learning_rate": 2.655092592592593e-06,
      "loss": 1.6621,
      "step": 3653
    },
    {
      "epoch": 7.6125,
      "grad_norm": 26.64859962463379,
      "learning_rate": 2.652777777777778e-06,
      "loss": 1.2127,
      "step": 3654
    },
    {
      "epoch": 7.614583333333333,
      "grad_norm": 11.44262409210205,
      "learning_rate": 2.650462962962963e-06,
      "loss": 1.0842,
      "step": 3655
    },
    {
      "epoch": 7.616666666666667,
      "grad_norm": 9.88675308227539,
      "learning_rate": 2.6481481481481485e-06,
      "loss": 1.1077,
      "step": 3656
    },
    {
      "epoch": 7.61875,
      "grad_norm": 18.466934204101562,
      "learning_rate": 2.6458333333333336e-06,
      "loss": 0.9674,
      "step": 3657
    },
    {
      "epoch": 7.620833333333334,
      "grad_norm": 9.845658302307129,
      "learning_rate": 2.6435185185185187e-06,
      "loss": 0.6397,
      "step": 3658
    },
    {
      "epoch": 7.622916666666667,
      "grad_norm": 19.73640251159668,
      "learning_rate": 2.641203703703704e-06,
      "loss": 1.3895,
      "step": 3659
    },
    {
      "epoch": 7.625,
      "grad_norm": 50.98579788208008,
      "learning_rate": 2.6388888888888893e-06,
      "loss": 2.3587,
      "step": 3660
    },
    {
      "epoch": 7.627083333333333,
      "grad_norm": 14.075255393981934,
      "learning_rate": 2.6365740740740744e-06,
      "loss": 0.6911,
      "step": 3661
    },
    {
      "epoch": 7.629166666666666,
      "grad_norm": 32.2586784362793,
      "learning_rate": 2.63425925925926e-06,
      "loss": 1.7027,
      "step": 3662
    },
    {
      "epoch": 7.63125,
      "grad_norm": 5.3256635665893555,
      "learning_rate": 2.6319444444444446e-06,
      "loss": 0.6382,
      "step": 3663
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 20.054317474365234,
      "learning_rate": 2.6296296296296297e-06,
      "loss": 1.3593,
      "step": 3664
    },
    {
      "epoch": 7.635416666666667,
      "grad_norm": 34.6110954284668,
      "learning_rate": 2.6273148148148148e-06,
      "loss": 0.8325,
      "step": 3665
    },
    {
      "epoch": 7.6375,
      "grad_norm": 3.4850454330444336,
      "learning_rate": 2.6250000000000003e-06,
      "loss": 0.0963,
      "step": 3666
    },
    {
      "epoch": 7.639583333333333,
      "grad_norm": 32.979244232177734,
      "learning_rate": 2.6226851851851854e-06,
      "loss": 1.0899,
      "step": 3667
    },
    {
      "epoch": 7.641666666666667,
      "grad_norm": 40.68037033081055,
      "learning_rate": 2.6203703703703705e-06,
      "loss": 1.2913,
      "step": 3668
    },
    {
      "epoch": 7.64375,
      "grad_norm": 7.92669153213501,
      "learning_rate": 2.618055555555556e-06,
      "loss": 0.6852,
      "step": 3669
    },
    {
      "epoch": 7.645833333333333,
      "grad_norm": 33.37407684326172,
      "learning_rate": 2.615740740740741e-06,
      "loss": 1.427,
      "step": 3670
    },
    {
      "epoch": 7.647916666666667,
      "grad_norm": 30.740026473999023,
      "learning_rate": 2.6134259259259258e-06,
      "loss": 0.8704,
      "step": 3671
    },
    {
      "epoch": 7.65,
      "grad_norm": 14.844690322875977,
      "learning_rate": 2.6111111111111113e-06,
      "loss": 1.1242,
      "step": 3672
    },
    {
      "epoch": 7.652083333333334,
      "grad_norm": 16.16402816772461,
      "learning_rate": 2.6087962962962964e-06,
      "loss": 1.5048,
      "step": 3673
    },
    {
      "epoch": 7.654166666666667,
      "grad_norm": 42.87295150756836,
      "learning_rate": 2.6064814814814815e-06,
      "loss": 1.2585,
      "step": 3674
    },
    {
      "epoch": 7.65625,
      "grad_norm": 26.74138069152832,
      "learning_rate": 2.604166666666667e-06,
      "loss": 3.2612,
      "step": 3675
    },
    {
      "epoch": 7.658333333333333,
      "grad_norm": 11.684232711791992,
      "learning_rate": 2.601851851851852e-06,
      "loss": 1.2303,
      "step": 3676
    },
    {
      "epoch": 7.660416666666666,
      "grad_norm": 34.33918762207031,
      "learning_rate": 2.599537037037037e-06,
      "loss": 1.2176,
      "step": 3677
    },
    {
      "epoch": 7.6625,
      "grad_norm": 17.07019805908203,
      "learning_rate": 2.5972222222222227e-06,
      "loss": 0.6079,
      "step": 3678
    },
    {
      "epoch": 7.664583333333333,
      "grad_norm": 48.806888580322266,
      "learning_rate": 2.594907407407408e-06,
      "loss": 1.4919,
      "step": 3679
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 27.140483856201172,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 1.2328,
      "step": 3680
    },
    {
      "epoch": 7.66875,
      "grad_norm": 12.444490432739258,
      "learning_rate": 2.5902777777777784e-06,
      "loss": 0.7957,
      "step": 3681
    },
    {
      "epoch": 7.670833333333333,
      "grad_norm": 20.484487533569336,
      "learning_rate": 2.587962962962963e-06,
      "loss": 1.2276,
      "step": 3682
    },
    {
      "epoch": 7.672916666666667,
      "grad_norm": 14.100120544433594,
      "learning_rate": 2.585648148148148e-06,
      "loss": 0.9875,
      "step": 3683
    },
    {
      "epoch": 7.675,
      "grad_norm": 10.496613502502441,
      "learning_rate": 2.5833333333333337e-06,
      "loss": 1.2938,
      "step": 3684
    },
    {
      "epoch": 7.677083333333333,
      "grad_norm": 25.0545654296875,
      "learning_rate": 2.581018518518519e-06,
      "loss": 0.744,
      "step": 3685
    },
    {
      "epoch": 7.679166666666667,
      "grad_norm": 13.747700691223145,
      "learning_rate": 2.578703703703704e-06,
      "loss": 1.3024,
      "step": 3686
    },
    {
      "epoch": 7.68125,
      "grad_norm": 25.869230270385742,
      "learning_rate": 2.576388888888889e-06,
      "loss": 0.2774,
      "step": 3687
    },
    {
      "epoch": 7.683333333333334,
      "grad_norm": 27.11687660217285,
      "learning_rate": 2.5740740740740745e-06,
      "loss": 1.7495,
      "step": 3688
    },
    {
      "epoch": 7.685416666666667,
      "grad_norm": 28.670658111572266,
      "learning_rate": 2.5717592592592592e-06,
      "loss": 1.3999,
      "step": 3689
    },
    {
      "epoch": 7.6875,
      "grad_norm": 8.541465759277344,
      "learning_rate": 2.5694444444444443e-06,
      "loss": 0.6482,
      "step": 3690
    },
    {
      "epoch": 7.689583333333333,
      "grad_norm": 12.827608108520508,
      "learning_rate": 2.56712962962963e-06,
      "loss": 0.6992,
      "step": 3691
    },
    {
      "epoch": 7.691666666666666,
      "grad_norm": 56.524330139160156,
      "learning_rate": 2.564814814814815e-06,
      "loss": 1.3306,
      "step": 3692
    },
    {
      "epoch": 7.69375,
      "grad_norm": 24.459308624267578,
      "learning_rate": 2.5625e-06,
      "loss": 1.7112,
      "step": 3693
    },
    {
      "epoch": 7.695833333333333,
      "grad_norm": 22.422088623046875,
      "learning_rate": 2.5601851851851856e-06,
      "loss": 1.2338,
      "step": 3694
    },
    {
      "epoch": 7.697916666666667,
      "grad_norm": 15.261307716369629,
      "learning_rate": 2.5578703703703706e-06,
      "loss": 1.0813,
      "step": 3695
    },
    {
      "epoch": 7.7,
      "grad_norm": 18.09872817993164,
      "learning_rate": 2.5555555555555557e-06,
      "loss": 1.0425,
      "step": 3696
    },
    {
      "epoch": 7.702083333333333,
      "grad_norm": 20.202713012695312,
      "learning_rate": 2.5532407407407413e-06,
      "loss": 0.832,
      "step": 3697
    },
    {
      "epoch": 7.704166666666667,
      "grad_norm": 10.371318817138672,
      "learning_rate": 2.550925925925926e-06,
      "loss": 0.7931,
      "step": 3698
    },
    {
      "epoch": 7.70625,
      "grad_norm": 27.74633026123047,
      "learning_rate": 2.548611111111111e-06,
      "loss": 1.0767,
      "step": 3699
    },
    {
      "epoch": 7.708333333333333,
      "grad_norm": 36.07676315307617,
      "learning_rate": 2.5462962962962966e-06,
      "loss": 1.4368,
      "step": 3700
    },
    {
      "epoch": 7.710416666666667,
      "grad_norm": 33.620853424072266,
      "learning_rate": 2.5439814814814817e-06,
      "loss": 1.5669,
      "step": 3701
    },
    {
      "epoch": 7.7125,
      "grad_norm": 22.857547760009766,
      "learning_rate": 2.5416666666666668e-06,
      "loss": 1.0901,
      "step": 3702
    },
    {
      "epoch": 7.714583333333334,
      "grad_norm": 28.18770980834961,
      "learning_rate": 2.5393518518518523e-06,
      "loss": 1.5074,
      "step": 3703
    },
    {
      "epoch": 7.716666666666667,
      "grad_norm": 29.22012710571289,
      "learning_rate": 2.5370370370370374e-06,
      "loss": 1.644,
      "step": 3704
    },
    {
      "epoch": 7.71875,
      "grad_norm": 16.901033401489258,
      "learning_rate": 2.5347222222222225e-06,
      "loss": 1.8706,
      "step": 3705
    },
    {
      "epoch": 7.720833333333333,
      "grad_norm": 55.43427276611328,
      "learning_rate": 2.532407407407408e-06,
      "loss": 0.8281,
      "step": 3706
    },
    {
      "epoch": 7.722916666666666,
      "grad_norm": 9.304278373718262,
      "learning_rate": 2.530092592592593e-06,
      "loss": 1.0743,
      "step": 3707
    },
    {
      "epoch": 7.725,
      "grad_norm": 7.070235729217529,
      "learning_rate": 2.5277777777777778e-06,
      "loss": 0.2799,
      "step": 3708
    },
    {
      "epoch": 7.727083333333333,
      "grad_norm": 8.70744514465332,
      "learning_rate": 2.525462962962963e-06,
      "loss": 0.6242,
      "step": 3709
    },
    {
      "epoch": 7.729166666666667,
      "grad_norm": 241.35682678222656,
      "learning_rate": 2.5231481481481484e-06,
      "loss": 1.2823,
      "step": 3710
    },
    {
      "epoch": 7.73125,
      "grad_norm": 7.432450771331787,
      "learning_rate": 2.5208333333333335e-06,
      "loss": 0.7784,
      "step": 3711
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 26.152074813842773,
      "learning_rate": 2.5185185185185186e-06,
      "loss": 1.1825,
      "step": 3712
    },
    {
      "epoch": 7.735416666666667,
      "grad_norm": 6.648736953735352,
      "learning_rate": 2.516203703703704e-06,
      "loss": 0.6815,
      "step": 3713
    },
    {
      "epoch": 7.7375,
      "grad_norm": 5.962923526763916,
      "learning_rate": 2.513888888888889e-06,
      "loss": 0.1225,
      "step": 3714
    },
    {
      "epoch": 7.739583333333333,
      "grad_norm": 14.758429527282715,
      "learning_rate": 2.511574074074074e-06,
      "loss": 1.3239,
      "step": 3715
    },
    {
      "epoch": 7.741666666666667,
      "grad_norm": 48.796138763427734,
      "learning_rate": 2.50925925925926e-06,
      "loss": 1.0831,
      "step": 3716
    },
    {
      "epoch": 7.74375,
      "grad_norm": 16.637237548828125,
      "learning_rate": 2.5069444444444445e-06,
      "loss": 1.1965,
      "step": 3717
    },
    {
      "epoch": 7.745833333333334,
      "grad_norm": 28.114656448364258,
      "learning_rate": 2.5046296296296296e-06,
      "loss": 1.1549,
      "step": 3718
    },
    {
      "epoch": 7.747916666666667,
      "grad_norm": 20.040449142456055,
      "learning_rate": 2.502314814814815e-06,
      "loss": 0.8199,
      "step": 3719
    },
    {
      "epoch": 7.75,
      "grad_norm": 53.102134704589844,
      "learning_rate": 2.5e-06,
      "loss": 1.5123,
      "step": 3720
    },
    {
      "epoch": 7.752083333333333,
      "grad_norm": 27.62825584411621,
      "learning_rate": 2.4976851851851853e-06,
      "loss": 1.0271,
      "step": 3721
    },
    {
      "epoch": 7.754166666666666,
      "grad_norm": 35.78841018676758,
      "learning_rate": 2.4953703703703704e-06,
      "loss": 2.2926,
      "step": 3722
    },
    {
      "epoch": 7.75625,
      "grad_norm": 10.163921356201172,
      "learning_rate": 2.493055555555556e-06,
      "loss": 0.6334,
      "step": 3723
    },
    {
      "epoch": 7.758333333333333,
      "grad_norm": 54.24650192260742,
      "learning_rate": 2.490740740740741e-06,
      "loss": 1.3448,
      "step": 3724
    },
    {
      "epoch": 7.760416666666667,
      "grad_norm": 56.98100280761719,
      "learning_rate": 2.488425925925926e-06,
      "loss": 0.6431,
      "step": 3725
    },
    {
      "epoch": 7.7625,
      "grad_norm": 50.20996856689453,
      "learning_rate": 2.4861111111111112e-06,
      "loss": 1.2925,
      "step": 3726
    },
    {
      "epoch": 7.764583333333333,
      "grad_norm": 6.780822277069092,
      "learning_rate": 2.4837962962962963e-06,
      "loss": 1.0477,
      "step": 3727
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 11.582083702087402,
      "learning_rate": 2.481481481481482e-06,
      "loss": 1.101,
      "step": 3728
    },
    {
      "epoch": 7.76875,
      "grad_norm": 98.79605865478516,
      "learning_rate": 2.479166666666667e-06,
      "loss": 1.2729,
      "step": 3729
    },
    {
      "epoch": 7.770833333333333,
      "grad_norm": 25.700040817260742,
      "learning_rate": 2.476851851851852e-06,
      "loss": 1.6472,
      "step": 3730
    },
    {
      "epoch": 7.772916666666667,
      "grad_norm": 155.0523681640625,
      "learning_rate": 2.474537037037037e-06,
      "loss": 2.8713,
      "step": 3731
    },
    {
      "epoch": 7.775,
      "grad_norm": 21.313669204711914,
      "learning_rate": 2.4722222222222226e-06,
      "loss": 1.1049,
      "step": 3732
    },
    {
      "epoch": 7.777083333333334,
      "grad_norm": 36.77714920043945,
      "learning_rate": 2.4699074074074073e-06,
      "loss": 0.4396,
      "step": 3733
    },
    {
      "epoch": 7.779166666666667,
      "grad_norm": 11.479266166687012,
      "learning_rate": 2.467592592592593e-06,
      "loss": 1.0207,
      "step": 3734
    },
    {
      "epoch": 7.78125,
      "grad_norm": 5.531317234039307,
      "learning_rate": 2.465277777777778e-06,
      "loss": 0.2297,
      "step": 3735
    },
    {
      "epoch": 7.783333333333333,
      "grad_norm": 35.66225814819336,
      "learning_rate": 2.462962962962963e-06,
      "loss": 0.7448,
      "step": 3736
    },
    {
      "epoch": 7.785416666666666,
      "grad_norm": 23.17045783996582,
      "learning_rate": 2.4606481481481486e-06,
      "loss": 1.7817,
      "step": 3737
    },
    {
      "epoch": 7.7875,
      "grad_norm": 11.661620140075684,
      "learning_rate": 2.4583333333333332e-06,
      "loss": 1.2625,
      "step": 3738
    },
    {
      "epoch": 7.789583333333333,
      "grad_norm": 19.949575424194336,
      "learning_rate": 2.4560185185185188e-06,
      "loss": 1.7924,
      "step": 3739
    },
    {
      "epoch": 7.791666666666667,
      "grad_norm": 8.463611602783203,
      "learning_rate": 2.453703703703704e-06,
      "loss": 0.7871,
      "step": 3740
    },
    {
      "epoch": 7.79375,
      "grad_norm": 4.422572135925293,
      "learning_rate": 2.451388888888889e-06,
      "loss": 0.2185,
      "step": 3741
    },
    {
      "epoch": 7.795833333333333,
      "grad_norm": 74.68665313720703,
      "learning_rate": 2.4490740740740745e-06,
      "loss": 0.7448,
      "step": 3742
    },
    {
      "epoch": 7.797916666666667,
      "grad_norm": 27.021484375,
      "learning_rate": 2.4467592592592596e-06,
      "loss": 2.0571,
      "step": 3743
    },
    {
      "epoch": 7.8,
      "grad_norm": 10.548443794250488,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 1.052,
      "step": 3744
    },
    {
      "epoch": 7.802083333333333,
      "grad_norm": 8.85108470916748,
      "learning_rate": 2.4421296296296298e-06,
      "loss": 0.3804,
      "step": 3745
    },
    {
      "epoch": 7.804166666666667,
      "grad_norm": 51.05646896362305,
      "learning_rate": 2.4398148148148153e-06,
      "loss": 2.095,
      "step": 3746
    },
    {
      "epoch": 7.80625,
      "grad_norm": 17.454017639160156,
      "learning_rate": 2.4375e-06,
      "loss": 1.242,
      "step": 3747
    },
    {
      "epoch": 7.808333333333334,
      "grad_norm": 9.7086763381958,
      "learning_rate": 2.4351851851851855e-06,
      "loss": 0.9708,
      "step": 3748
    },
    {
      "epoch": 7.810416666666667,
      "grad_norm": 19.949506759643555,
      "learning_rate": 2.4328703703703706e-06,
      "loss": 1.1853,
      "step": 3749
    },
    {
      "epoch": 7.8125,
      "grad_norm": 13.152909278869629,
      "learning_rate": 2.4305555555555557e-06,
      "loss": 1.0247,
      "step": 3750
    },
    {
      "epoch": 7.814583333333333,
      "grad_norm": 99.12299346923828,
      "learning_rate": 2.428240740740741e-06,
      "loss": 2.3295,
      "step": 3751
    },
    {
      "epoch": 7.816666666666666,
      "grad_norm": 29.573183059692383,
      "learning_rate": 2.425925925925926e-06,
      "loss": 1.6714,
      "step": 3752
    },
    {
      "epoch": 7.81875,
      "grad_norm": 26.952255249023438,
      "learning_rate": 2.4236111111111114e-06,
      "loss": 1.6876,
      "step": 3753
    },
    {
      "epoch": 7.820833333333333,
      "grad_norm": 9.416098594665527,
      "learning_rate": 2.4212962962962965e-06,
      "loss": 0.5953,
      "step": 3754
    },
    {
      "epoch": 7.822916666666667,
      "grad_norm": 16.95435905456543,
      "learning_rate": 2.4189814814814816e-06,
      "loss": 1.3999,
      "step": 3755
    },
    {
      "epoch": 7.825,
      "grad_norm": 13.606197357177734,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.6686,
      "step": 3756
    },
    {
      "epoch": 7.827083333333333,
      "grad_norm": 13.198713302612305,
      "learning_rate": 2.414351851851852e-06,
      "loss": 0.68,
      "step": 3757
    },
    {
      "epoch": 7.829166666666667,
      "grad_norm": 11.282671928405762,
      "learning_rate": 2.4120370370370373e-06,
      "loss": 1.0384,
      "step": 3758
    },
    {
      "epoch": 7.83125,
      "grad_norm": 30.92683219909668,
      "learning_rate": 2.4097222222222224e-06,
      "loss": 2.2595,
      "step": 3759
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 8.003619194030762,
      "learning_rate": 2.4074074074074075e-06,
      "loss": 0.9676,
      "step": 3760
    },
    {
      "epoch": 7.835416666666667,
      "grad_norm": 6.948476314544678,
      "learning_rate": 2.4050925925925926e-06,
      "loss": 0.7099,
      "step": 3761
    },
    {
      "epoch": 7.8375,
      "grad_norm": 8.110392570495605,
      "learning_rate": 2.402777777777778e-06,
      "loss": 1.0106,
      "step": 3762
    },
    {
      "epoch": 7.839583333333334,
      "grad_norm": 19.62311363220215,
      "learning_rate": 2.400462962962963e-06,
      "loss": 1.3176,
      "step": 3763
    },
    {
      "epoch": 7.841666666666667,
      "grad_norm": 5.136992931365967,
      "learning_rate": 2.3981481481481483e-06,
      "loss": 0.5731,
      "step": 3764
    },
    {
      "epoch": 7.84375,
      "grad_norm": 49.64826583862305,
      "learning_rate": 2.395833333333334e-06,
      "loss": 1.2607,
      "step": 3765
    },
    {
      "epoch": 7.845833333333333,
      "grad_norm": 20.35418128967285,
      "learning_rate": 2.3935185185185185e-06,
      "loss": 1.272,
      "step": 3766
    },
    {
      "epoch": 7.847916666666666,
      "grad_norm": 153.44444274902344,
      "learning_rate": 2.391203703703704e-06,
      "loss": 1.0408,
      "step": 3767
    },
    {
      "epoch": 7.85,
      "grad_norm": 61.47704315185547,
      "learning_rate": 2.388888888888889e-06,
      "loss": 0.7793,
      "step": 3768
    },
    {
      "epoch": 7.852083333333333,
      "grad_norm": 9.396506309509277,
      "learning_rate": 2.3865740740740742e-06,
      "loss": 0.6474,
      "step": 3769
    },
    {
      "epoch": 7.854166666666667,
      "grad_norm": 5.00730037689209,
      "learning_rate": 2.3842592592592593e-06,
      "loss": 0.5542,
      "step": 3770
    },
    {
      "epoch": 7.85625,
      "grad_norm": 15.470006942749023,
      "learning_rate": 2.3819444444444444e-06,
      "loss": 0.9102,
      "step": 3771
    },
    {
      "epoch": 7.858333333333333,
      "grad_norm": 36.54035949707031,
      "learning_rate": 2.37962962962963e-06,
      "loss": 1.2239,
      "step": 3772
    },
    {
      "epoch": 7.860416666666667,
      "grad_norm": 9.12225341796875,
      "learning_rate": 2.377314814814815e-06,
      "loss": 0.584,
      "step": 3773
    },
    {
      "epoch": 7.8625,
      "grad_norm": 80.91447448730469,
      "learning_rate": 2.375e-06,
      "loss": 1.1335,
      "step": 3774
    },
    {
      "epoch": 7.864583333333333,
      "grad_norm": 20.631675720214844,
      "learning_rate": 2.3726851851851852e-06,
      "loss": 1.2677,
      "step": 3775
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 17.382293701171875,
      "learning_rate": 2.3703703703703707e-06,
      "loss": 1.0655,
      "step": 3776
    },
    {
      "epoch": 7.86875,
      "grad_norm": 8.76994800567627,
      "learning_rate": 2.368055555555556e-06,
      "loss": 0.5792,
      "step": 3777
    },
    {
      "epoch": 7.870833333333334,
      "grad_norm": 40.96208190917969,
      "learning_rate": 2.365740740740741e-06,
      "loss": 0.7135,
      "step": 3778
    },
    {
      "epoch": 7.872916666666667,
      "grad_norm": 8.935606002807617,
      "learning_rate": 2.363425925925926e-06,
      "loss": 0.5716,
      "step": 3779
    },
    {
      "epoch": 7.875,
      "grad_norm": 10.232242584228516,
      "learning_rate": 2.361111111111111e-06,
      "loss": 1.0068,
      "step": 3780
    },
    {
      "epoch": 7.877083333333333,
      "grad_norm": 53.719573974609375,
      "learning_rate": 2.3587962962962967e-06,
      "loss": 1.8889,
      "step": 3781
    },
    {
      "epoch": 7.879166666666666,
      "grad_norm": 9.730996131896973,
      "learning_rate": 2.3564814814814813e-06,
      "loss": 0.738,
      "step": 3782
    },
    {
      "epoch": 7.88125,
      "grad_norm": 25.200960159301758,
      "learning_rate": 2.354166666666667e-06,
      "loss": 1.081,
      "step": 3783
    },
    {
      "epoch": 7.883333333333333,
      "grad_norm": 38.63057327270508,
      "learning_rate": 2.351851851851852e-06,
      "loss": 1.0184,
      "step": 3784
    },
    {
      "epoch": 7.885416666666667,
      "grad_norm": 7.223288059234619,
      "learning_rate": 2.349537037037037e-06,
      "loss": 0.54,
      "step": 3785
    },
    {
      "epoch": 7.8875,
      "grad_norm": 20.89580726623535,
      "learning_rate": 2.3472222222222226e-06,
      "loss": 0.4419,
      "step": 3786
    },
    {
      "epoch": 7.889583333333333,
      "grad_norm": 12.392614364624023,
      "learning_rate": 2.3449074074074077e-06,
      "loss": 1.2595,
      "step": 3787
    },
    {
      "epoch": 7.891666666666667,
      "grad_norm": 14.319807052612305,
      "learning_rate": 2.3425925925925928e-06,
      "loss": 0.9796,
      "step": 3788
    },
    {
      "epoch": 7.89375,
      "grad_norm": 6.3620476722717285,
      "learning_rate": 2.340277777777778e-06,
      "loss": 0.7035,
      "step": 3789
    },
    {
      "epoch": 7.895833333333333,
      "grad_norm": 16.56809425354004,
      "learning_rate": 2.3379629629629634e-06,
      "loss": 1.0849,
      "step": 3790
    },
    {
      "epoch": 7.897916666666667,
      "grad_norm": 13.899609565734863,
      "learning_rate": 2.3356481481481485e-06,
      "loss": 1.2447,
      "step": 3791
    },
    {
      "epoch": 7.9,
      "grad_norm": 74.83450317382812,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.9267,
      "step": 3792
    },
    {
      "epoch": 7.902083333333334,
      "grad_norm": 46.15559387207031,
      "learning_rate": 2.3310185185185187e-06,
      "loss": 1.2649,
      "step": 3793
    },
    {
      "epoch": 7.904166666666667,
      "grad_norm": 21.273290634155273,
      "learning_rate": 2.3287037037037038e-06,
      "loss": 1.119,
      "step": 3794
    },
    {
      "epoch": 7.90625,
      "grad_norm": 25.267608642578125,
      "learning_rate": 2.3263888888888893e-06,
      "loss": 1.3298,
      "step": 3795
    },
    {
      "epoch": 7.908333333333333,
      "grad_norm": 12.57016372680664,
      "learning_rate": 2.324074074074074e-06,
      "loss": 1.0763,
      "step": 3796
    },
    {
      "epoch": 7.910416666666666,
      "grad_norm": 12.649547576904297,
      "learning_rate": 2.3217592592592595e-06,
      "loss": 1.1878,
      "step": 3797
    },
    {
      "epoch": 7.9125,
      "grad_norm": 20.17094612121582,
      "learning_rate": 2.3194444444444446e-06,
      "loss": 1.0434,
      "step": 3798
    },
    {
      "epoch": 7.914583333333333,
      "grad_norm": 10.536286354064941,
      "learning_rate": 2.3171296296296297e-06,
      "loss": 0.6366,
      "step": 3799
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 30.282333374023438,
      "learning_rate": 2.314814814814815e-06,
      "loss": 0.7936,
      "step": 3800
    },
    {
      "epoch": 7.91875,
      "grad_norm": 12.507997512817383,
      "learning_rate": 2.3125000000000003e-06,
      "loss": 0.9106,
      "step": 3801
    },
    {
      "epoch": 7.920833333333333,
      "grad_norm": 28.721689224243164,
      "learning_rate": 2.3101851851851854e-06,
      "loss": 1.6268,
      "step": 3802
    },
    {
      "epoch": 7.922916666666667,
      "grad_norm": 61.9716682434082,
      "learning_rate": 2.3078703703703705e-06,
      "loss": 1.824,
      "step": 3803
    },
    {
      "epoch": 7.925,
      "grad_norm": 19.65113067626953,
      "learning_rate": 2.305555555555556e-06,
      "loss": 1.4691,
      "step": 3804
    },
    {
      "epoch": 7.927083333333333,
      "grad_norm": 55.48576736450195,
      "learning_rate": 2.3032407407407407e-06,
      "loss": 1.3606,
      "step": 3805
    },
    {
      "epoch": 7.929166666666667,
      "grad_norm": 14.240429878234863,
      "learning_rate": 2.3009259259259262e-06,
      "loss": 0.8991,
      "step": 3806
    },
    {
      "epoch": 7.93125,
      "grad_norm": 16.781404495239258,
      "learning_rate": 2.2986111111111113e-06,
      "loss": 0.9246,
      "step": 3807
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 92.6229476928711,
      "learning_rate": 2.2962962962962964e-06,
      "loss": 0.6976,
      "step": 3808
    },
    {
      "epoch": 7.935416666666667,
      "grad_norm": 16.70313262939453,
      "learning_rate": 2.293981481481482e-06,
      "loss": 1.2434,
      "step": 3809
    },
    {
      "epoch": 7.9375,
      "grad_norm": 40.9476318359375,
      "learning_rate": 2.2916666666666666e-06,
      "loss": 1.2427,
      "step": 3810
    },
    {
      "epoch": 7.939583333333333,
      "grad_norm": 47.2243537902832,
      "learning_rate": 2.289351851851852e-06,
      "loss": 1.0115,
      "step": 3811
    },
    {
      "epoch": 7.941666666666666,
      "grad_norm": 45.462371826171875,
      "learning_rate": 2.2870370370370372e-06,
      "loss": 1.617,
      "step": 3812
    },
    {
      "epoch": 7.94375,
      "grad_norm": 121.63703155517578,
      "learning_rate": 2.2847222222222223e-06,
      "loss": 1.3158,
      "step": 3813
    },
    {
      "epoch": 7.945833333333333,
      "grad_norm": 8.043100357055664,
      "learning_rate": 2.2824074074074074e-06,
      "loss": 0.5517,
      "step": 3814
    },
    {
      "epoch": 7.947916666666667,
      "grad_norm": 32.0506706237793,
      "learning_rate": 2.280092592592593e-06,
      "loss": 1.34,
      "step": 3815
    },
    {
      "epoch": 7.95,
      "grad_norm": 5.990603923797607,
      "learning_rate": 2.277777777777778e-06,
      "loss": 0.5874,
      "step": 3816
    },
    {
      "epoch": 7.952083333333333,
      "grad_norm": 5.561386585235596,
      "learning_rate": 2.275462962962963e-06,
      "loss": 0.2561,
      "step": 3817
    },
    {
      "epoch": 7.954166666666667,
      "grad_norm": 42.1416015625,
      "learning_rate": 2.2731481481481482e-06,
      "loss": 0.967,
      "step": 3818
    },
    {
      "epoch": 7.95625,
      "grad_norm": 8.39306640625,
      "learning_rate": 2.2708333333333333e-06,
      "loss": 0.6608,
      "step": 3819
    },
    {
      "epoch": 7.958333333333333,
      "grad_norm": 7.704436779022217,
      "learning_rate": 2.268518518518519e-06,
      "loss": 0.5883,
      "step": 3820
    },
    {
      "epoch": 7.960416666666667,
      "grad_norm": 49.413516998291016,
      "learning_rate": 2.266203703703704e-06,
      "loss": 1.0754,
      "step": 3821
    },
    {
      "epoch": 7.9625,
      "grad_norm": 8.724123001098633,
      "learning_rate": 2.263888888888889e-06,
      "loss": 0.6527,
      "step": 3822
    },
    {
      "epoch": 7.964583333333334,
      "grad_norm": 9.260714530944824,
      "learning_rate": 2.2615740740740746e-06,
      "loss": 0.6198,
      "step": 3823
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 35.276893615722656,
      "learning_rate": 2.2592592592592592e-06,
      "loss": 2.0577,
      "step": 3824
    },
    {
      "epoch": 7.96875,
      "grad_norm": 6.135025978088379,
      "learning_rate": 2.2569444444444448e-06,
      "loss": 0.2433,
      "step": 3825
    },
    {
      "epoch": 7.970833333333333,
      "grad_norm": 10.14393138885498,
      "learning_rate": 2.25462962962963e-06,
      "loss": 0.979,
      "step": 3826
    },
    {
      "epoch": 7.972916666666666,
      "grad_norm": 43.46544647216797,
      "learning_rate": 2.252314814814815e-06,
      "loss": 0.8757,
      "step": 3827
    },
    {
      "epoch": 7.975,
      "grad_norm": 29.148725509643555,
      "learning_rate": 2.25e-06,
      "loss": 0.7767,
      "step": 3828
    },
    {
      "epoch": 7.977083333333333,
      "grad_norm": 12.234481811523438,
      "learning_rate": 2.247685185185185e-06,
      "loss": 0.8319,
      "step": 3829
    },
    {
      "epoch": 7.979166666666667,
      "grad_norm": 23.450162887573242,
      "learning_rate": 2.2453703703703707e-06,
      "loss": 1.1145,
      "step": 3830
    },
    {
      "epoch": 7.98125,
      "grad_norm": 9.349023818969727,
      "learning_rate": 2.2430555555555558e-06,
      "loss": 0.6635,
      "step": 3831
    },
    {
      "epoch": 7.983333333333333,
      "grad_norm": 98.7757797241211,
      "learning_rate": 2.240740740740741e-06,
      "loss": 0.7128,
      "step": 3832
    },
    {
      "epoch": 7.985416666666667,
      "grad_norm": 25.710525512695312,
      "learning_rate": 2.238425925925926e-06,
      "loss": 2.031,
      "step": 3833
    },
    {
      "epoch": 7.9875,
      "grad_norm": 61.42301940917969,
      "learning_rate": 2.2361111111111115e-06,
      "loss": 1.2026,
      "step": 3834
    },
    {
      "epoch": 7.989583333333333,
      "grad_norm": 22.33003807067871,
      "learning_rate": 2.2337962962962966e-06,
      "loss": 0.6494,
      "step": 3835
    },
    {
      "epoch": 7.991666666666667,
      "grad_norm": 89.38728332519531,
      "learning_rate": 2.2314814814814817e-06,
      "loss": 0.6196,
      "step": 3836
    },
    {
      "epoch": 7.99375,
      "grad_norm": 10.749675750732422,
      "learning_rate": 2.2291666666666668e-06,
      "loss": 0.9492,
      "step": 3837
    },
    {
      "epoch": 7.995833333333334,
      "grad_norm": 56.961666107177734,
      "learning_rate": 2.226851851851852e-06,
      "loss": 1.6883,
      "step": 3838
    },
    {
      "epoch": 7.997916666666667,
      "grad_norm": 30.40623664855957,
      "learning_rate": 2.2245370370370374e-06,
      "loss": 1.284,
      "step": 3839
    },
    {
      "epoch": 8.0,
      "grad_norm": 24.575878143310547,
      "learning_rate": 2.222222222222222e-06,
      "loss": 1.6636,
      "step": 3840
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.6111111111111112,
      "eval_f1": 0.5304006059028257,
      "eval_loss": 1.2572879791259766,
      "eval_runtime": 34.4193,
      "eval_samples_per_second": 5.23,
      "eval_steps_per_second": 2.615,
      "step": 3840
    },
    {
      "epoch": 8.002083333333333,
      "grad_norm": 14.668434143066406,
      "learning_rate": 2.2199074074074076e-06,
      "loss": 0.7821,
      "step": 3841
    },
    {
      "epoch": 8.004166666666666,
      "grad_norm": 18.218854904174805,
      "learning_rate": 2.2175925925925927e-06,
      "loss": 1.1806,
      "step": 3842
    },
    {
      "epoch": 8.00625,
      "grad_norm": 8.069916725158691,
      "learning_rate": 2.215277777777778e-06,
      "loss": 0.6086,
      "step": 3843
    },
    {
      "epoch": 8.008333333333333,
      "grad_norm": 27.742584228515625,
      "learning_rate": 2.2129629629629633e-06,
      "loss": 1.3171,
      "step": 3844
    },
    {
      "epoch": 8.010416666666666,
      "grad_norm": 14.258305549621582,
      "learning_rate": 2.2106481481481484e-06,
      "loss": 0.9151,
      "step": 3845
    },
    {
      "epoch": 8.0125,
      "grad_norm": 13.946307182312012,
      "learning_rate": 2.2083333333333335e-06,
      "loss": 1.1045,
      "step": 3846
    },
    {
      "epoch": 8.014583333333333,
      "grad_norm": 19.216590881347656,
      "learning_rate": 2.2060185185185186e-06,
      "loss": 0.5618,
      "step": 3847
    },
    {
      "epoch": 8.016666666666667,
      "grad_norm": 8.950948715209961,
      "learning_rate": 2.203703703703704e-06,
      "loss": 0.281,
      "step": 3848
    },
    {
      "epoch": 8.01875,
      "grad_norm": 24.3887996673584,
      "learning_rate": 2.2013888888888892e-06,
      "loss": 1.3119,
      "step": 3849
    },
    {
      "epoch": 8.020833333333334,
      "grad_norm": 9.002065658569336,
      "learning_rate": 2.1990740740740743e-06,
      "loss": 1.035,
      "step": 3850
    },
    {
      "epoch": 8.022916666666667,
      "grad_norm": 49.10575866699219,
      "learning_rate": 2.1967592592592594e-06,
      "loss": 1.2965,
      "step": 3851
    },
    {
      "epoch": 8.025,
      "grad_norm": 23.915842056274414,
      "learning_rate": 2.1944444444444445e-06,
      "loss": 1.107,
      "step": 3852
    },
    {
      "epoch": 8.027083333333334,
      "grad_norm": 80.19317626953125,
      "learning_rate": 2.19212962962963e-06,
      "loss": 1.3602,
      "step": 3853
    },
    {
      "epoch": 8.029166666666667,
      "grad_norm": 45.19846725463867,
      "learning_rate": 2.1898148148148147e-06,
      "loss": 1.2238,
      "step": 3854
    },
    {
      "epoch": 8.03125,
      "grad_norm": 4.80684757232666,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 0.1501,
      "step": 3855
    },
    {
      "epoch": 8.033333333333333,
      "grad_norm": 25.795392990112305,
      "learning_rate": 2.1851851851851853e-06,
      "loss": 0.5867,
      "step": 3856
    },
    {
      "epoch": 8.035416666666666,
      "grad_norm": 35.414451599121094,
      "learning_rate": 2.1828703703703704e-06,
      "loss": 0.9249,
      "step": 3857
    },
    {
      "epoch": 8.0375,
      "grad_norm": 17.336885452270508,
      "learning_rate": 2.180555555555556e-06,
      "loss": 0.9387,
      "step": 3858
    },
    {
      "epoch": 8.039583333333333,
      "grad_norm": 15.921598434448242,
      "learning_rate": 2.178240740740741e-06,
      "loss": 1.1997,
      "step": 3859
    },
    {
      "epoch": 8.041666666666666,
      "grad_norm": 5.7076239585876465,
      "learning_rate": 2.175925925925926e-06,
      "loss": 0.1387,
      "step": 3860
    },
    {
      "epoch": 8.04375,
      "grad_norm": 8.809296607971191,
      "learning_rate": 2.1736111111111112e-06,
      "loss": 0.5884,
      "step": 3861
    },
    {
      "epoch": 8.045833333333333,
      "grad_norm": 82.56256866455078,
      "learning_rate": 2.1712962962962963e-06,
      "loss": 0.957,
      "step": 3862
    },
    {
      "epoch": 8.047916666666667,
      "grad_norm": 17.765878677368164,
      "learning_rate": 2.1689814814814814e-06,
      "loss": 1.1266,
      "step": 3863
    },
    {
      "epoch": 8.05,
      "grad_norm": 51.7377815246582,
      "learning_rate": 2.166666666666667e-06,
      "loss": 2.1131,
      "step": 3864
    },
    {
      "epoch": 8.052083333333334,
      "grad_norm": 19.297443389892578,
      "learning_rate": 2.164351851851852e-06,
      "loss": 0.7552,
      "step": 3865
    },
    {
      "epoch": 8.054166666666667,
      "grad_norm": 7.752924919128418,
      "learning_rate": 2.162037037037037e-06,
      "loss": 0.4984,
      "step": 3866
    },
    {
      "epoch": 8.05625,
      "grad_norm": 15.153432846069336,
      "learning_rate": 2.1597222222222227e-06,
      "loss": 0.4863,
      "step": 3867
    },
    {
      "epoch": 8.058333333333334,
      "grad_norm": 147.1260528564453,
      "learning_rate": 2.1574074074074073e-06,
      "loss": 0.8908,
      "step": 3868
    },
    {
      "epoch": 8.060416666666667,
      "grad_norm": 10.026886940002441,
      "learning_rate": 2.155092592592593e-06,
      "loss": 0.7525,
      "step": 3869
    },
    {
      "epoch": 8.0625,
      "grad_norm": 9.104884147644043,
      "learning_rate": 2.152777777777778e-06,
      "loss": 0.54,
      "step": 3870
    },
    {
      "epoch": 8.064583333333333,
      "grad_norm": 20.816377639770508,
      "learning_rate": 2.150462962962963e-06,
      "loss": 0.5866,
      "step": 3871
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 13.680819511413574,
      "learning_rate": 2.148148148148148e-06,
      "loss": 0.6019,
      "step": 3872
    },
    {
      "epoch": 8.06875,
      "grad_norm": 5.98802375793457,
      "learning_rate": 2.1458333333333333e-06,
      "loss": 0.2778,
      "step": 3873
    },
    {
      "epoch": 8.070833333333333,
      "grad_norm": 134.28173828125,
      "learning_rate": 2.1435185185185188e-06,
      "loss": 1.3282,
      "step": 3874
    },
    {
      "epoch": 8.072916666666666,
      "grad_norm": 6.0789594650268555,
      "learning_rate": 2.141203703703704e-06,
      "loss": 0.6457,
      "step": 3875
    },
    {
      "epoch": 8.075,
      "grad_norm": 48.51189041137695,
      "learning_rate": 2.138888888888889e-06,
      "loss": 1.089,
      "step": 3876
    },
    {
      "epoch": 8.077083333333333,
      "grad_norm": 47.42504119873047,
      "learning_rate": 2.136574074074074e-06,
      "loss": 1.9034,
      "step": 3877
    },
    {
      "epoch": 8.079166666666667,
      "grad_norm": 4.327808856964111,
      "learning_rate": 2.1342592592592596e-06,
      "loss": 0.1204,
      "step": 3878
    },
    {
      "epoch": 8.08125,
      "grad_norm": 5.48147439956665,
      "learning_rate": 2.1319444444444447e-06,
      "loss": 0.6314,
      "step": 3879
    },
    {
      "epoch": 8.083333333333334,
      "grad_norm": 4.984490394592285,
      "learning_rate": 2.1296296296296298e-06,
      "loss": 0.5817,
      "step": 3880
    },
    {
      "epoch": 8.085416666666667,
      "grad_norm": 10.079636573791504,
      "learning_rate": 2.1273148148148153e-06,
      "loss": 0.9975,
      "step": 3881
    },
    {
      "epoch": 8.0875,
      "grad_norm": 6.48576021194458,
      "learning_rate": 2.125e-06,
      "loss": 0.671,
      "step": 3882
    },
    {
      "epoch": 8.089583333333334,
      "grad_norm": 26.365272521972656,
      "learning_rate": 2.1226851851851855e-06,
      "loss": 1.2765,
      "step": 3883
    },
    {
      "epoch": 8.091666666666667,
      "grad_norm": 22.341842651367188,
      "learning_rate": 2.1203703703703706e-06,
      "loss": 1.5713,
      "step": 3884
    },
    {
      "epoch": 8.09375,
      "grad_norm": 4.796687602996826,
      "learning_rate": 2.1180555555555557e-06,
      "loss": 0.5562,
      "step": 3885
    },
    {
      "epoch": 8.095833333333333,
      "grad_norm": 8.746379852294922,
      "learning_rate": 2.115740740740741e-06,
      "loss": 0.5098,
      "step": 3886
    },
    {
      "epoch": 8.097916666666666,
      "grad_norm": 9.389175415039062,
      "learning_rate": 2.113425925925926e-06,
      "loss": 0.5771,
      "step": 3887
    },
    {
      "epoch": 8.1,
      "grad_norm": 13.835872650146484,
      "learning_rate": 2.1111111111111114e-06,
      "loss": 1.275,
      "step": 3888
    },
    {
      "epoch": 8.102083333333333,
      "grad_norm": 7.189682483673096,
      "learning_rate": 2.1087962962962965e-06,
      "loss": 0.5532,
      "step": 3889
    },
    {
      "epoch": 8.104166666666666,
      "grad_norm": 21.166946411132812,
      "learning_rate": 2.1064814814814816e-06,
      "loss": 1.0517,
      "step": 3890
    },
    {
      "epoch": 8.10625,
      "grad_norm": 20.998165130615234,
      "learning_rate": 2.1041666666666667e-06,
      "loss": 0.1165,
      "step": 3891
    },
    {
      "epoch": 8.108333333333333,
      "grad_norm": 6.1242804527282715,
      "learning_rate": 2.1018518518518522e-06,
      "loss": 0.6848,
      "step": 3892
    },
    {
      "epoch": 8.110416666666667,
      "grad_norm": 14.417948722839355,
      "learning_rate": 2.0995370370370373e-06,
      "loss": 1.3841,
      "step": 3893
    },
    {
      "epoch": 8.1125,
      "grad_norm": 63.0937385559082,
      "learning_rate": 2.0972222222222224e-06,
      "loss": 1.7112,
      "step": 3894
    },
    {
      "epoch": 8.114583333333334,
      "grad_norm": 9.720227241516113,
      "learning_rate": 2.0949074074074075e-06,
      "loss": 1.0987,
      "step": 3895
    },
    {
      "epoch": 8.116666666666667,
      "grad_norm": 11.470197677612305,
      "learning_rate": 2.0925925925925926e-06,
      "loss": 1.2187,
      "step": 3896
    },
    {
      "epoch": 8.11875,
      "grad_norm": 8.917155265808105,
      "learning_rate": 2.090277777777778e-06,
      "loss": 0.6118,
      "step": 3897
    },
    {
      "epoch": 8.120833333333334,
      "grad_norm": 56.51304244995117,
      "learning_rate": 2.087962962962963e-06,
      "loss": 1.4499,
      "step": 3898
    },
    {
      "epoch": 8.122916666666667,
      "grad_norm": 25.609600067138672,
      "learning_rate": 2.0856481481481483e-06,
      "loss": 0.9784,
      "step": 3899
    },
    {
      "epoch": 8.125,
      "grad_norm": 8.830765724182129,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.5764,
      "step": 3900
    },
    {
      "epoch": 8.127083333333333,
      "grad_norm": 19.094375610351562,
      "learning_rate": 2.0810185185185185e-06,
      "loss": 0.9903,
      "step": 3901
    },
    {
      "epoch": 8.129166666666666,
      "grad_norm": 16.44935417175293,
      "learning_rate": 2.078703703703704e-06,
      "loss": 1.2441,
      "step": 3902
    },
    {
      "epoch": 8.13125,
      "grad_norm": 47.967132568359375,
      "learning_rate": 2.076388888888889e-06,
      "loss": 0.9015,
      "step": 3903
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 19.589447021484375,
      "learning_rate": 2.0740740740740742e-06,
      "loss": 1.7192,
      "step": 3904
    },
    {
      "epoch": 8.135416666666666,
      "grad_norm": 5.796812057495117,
      "learning_rate": 2.0717592592592593e-06,
      "loss": 0.5871,
      "step": 3905
    },
    {
      "epoch": 8.1375,
      "grad_norm": 10.319558143615723,
      "learning_rate": 2.0694444444444444e-06,
      "loss": 1.3137,
      "step": 3906
    },
    {
      "epoch": 8.139583333333333,
      "grad_norm": 31.768482208251953,
      "learning_rate": 2.06712962962963e-06,
      "loss": 1.7548,
      "step": 3907
    },
    {
      "epoch": 8.141666666666667,
      "grad_norm": 8.420366287231445,
      "learning_rate": 2.064814814814815e-06,
      "loss": 0.7333,
      "step": 3908
    },
    {
      "epoch": 8.14375,
      "grad_norm": 6.5065083503723145,
      "learning_rate": 2.0625e-06,
      "loss": 0.6283,
      "step": 3909
    },
    {
      "epoch": 8.145833333333334,
      "grad_norm": 35.35881042480469,
      "learning_rate": 2.0601851851851853e-06,
      "loss": 1.1036,
      "step": 3910
    },
    {
      "epoch": 8.147916666666667,
      "grad_norm": 46.34573745727539,
      "learning_rate": 2.0578703703703708e-06,
      "loss": 1.0389,
      "step": 3911
    },
    {
      "epoch": 8.15,
      "grad_norm": 39.92768478393555,
      "learning_rate": 2.0555555555555555e-06,
      "loss": 1.2984,
      "step": 3912
    },
    {
      "epoch": 8.152083333333334,
      "grad_norm": 9.6328706741333,
      "learning_rate": 2.053240740740741e-06,
      "loss": 1.0147,
      "step": 3913
    },
    {
      "epoch": 8.154166666666667,
      "grad_norm": 9.96436595916748,
      "learning_rate": 2.050925925925926e-06,
      "loss": 0.6522,
      "step": 3914
    },
    {
      "epoch": 8.15625,
      "grad_norm": 7.0103983879089355,
      "learning_rate": 2.048611111111111e-06,
      "loss": 0.253,
      "step": 3915
    },
    {
      "epoch": 8.158333333333333,
      "grad_norm": 19.135894775390625,
      "learning_rate": 2.0462962962962967e-06,
      "loss": 1.3794,
      "step": 3916
    },
    {
      "epoch": 8.160416666666666,
      "grad_norm": 8.246526718139648,
      "learning_rate": 2.0439814814814814e-06,
      "loss": 1.034,
      "step": 3917
    },
    {
      "epoch": 8.1625,
      "grad_norm": 133.94134521484375,
      "learning_rate": 2.041666666666667e-06,
      "loss": 1.1462,
      "step": 3918
    },
    {
      "epoch": 8.164583333333333,
      "grad_norm": 9.796951293945312,
      "learning_rate": 2.039351851851852e-06,
      "loss": 0.4054,
      "step": 3919
    },
    {
      "epoch": 8.166666666666666,
      "grad_norm": 28.828208923339844,
      "learning_rate": 2.037037037037037e-06,
      "loss": 2.8813,
      "step": 3920
    },
    {
      "epoch": 8.16875,
      "grad_norm": 37.277400970458984,
      "learning_rate": 2.034722222222222e-06,
      "loss": 1.1021,
      "step": 3921
    },
    {
      "epoch": 8.170833333333333,
      "grad_norm": 9.593130111694336,
      "learning_rate": 2.0324074074074077e-06,
      "loss": 1.1795,
      "step": 3922
    },
    {
      "epoch": 8.172916666666667,
      "grad_norm": 63.959068298339844,
      "learning_rate": 2.030092592592593e-06,
      "loss": 1.3679,
      "step": 3923
    },
    {
      "epoch": 8.175,
      "grad_norm": 37.13188934326172,
      "learning_rate": 2.027777777777778e-06,
      "loss": 0.983,
      "step": 3924
    },
    {
      "epoch": 8.177083333333334,
      "grad_norm": 13.17685317993164,
      "learning_rate": 2.0254629629629634e-06,
      "loss": 0.6769,
      "step": 3925
    },
    {
      "epoch": 8.179166666666667,
      "grad_norm": 5.873676300048828,
      "learning_rate": 2.023148148148148e-06,
      "loss": 0.5068,
      "step": 3926
    },
    {
      "epoch": 8.18125,
      "grad_norm": 7.034409046173096,
      "learning_rate": 2.0208333333333336e-06,
      "loss": 0.7269,
      "step": 3927
    },
    {
      "epoch": 8.183333333333334,
      "grad_norm": 8.75090217590332,
      "learning_rate": 2.0185185185185187e-06,
      "loss": 0.5666,
      "step": 3928
    },
    {
      "epoch": 8.185416666666667,
      "grad_norm": 242.31072998046875,
      "learning_rate": 2.016203703703704e-06,
      "loss": 1.5986,
      "step": 3929
    },
    {
      "epoch": 8.1875,
      "grad_norm": 6.87977409362793,
      "learning_rate": 2.0138888888888893e-06,
      "loss": 0.3177,
      "step": 3930
    },
    {
      "epoch": 8.189583333333333,
      "grad_norm": 7.602223873138428,
      "learning_rate": 2.011574074074074e-06,
      "loss": 0.8583,
      "step": 3931
    },
    {
      "epoch": 8.191666666666666,
      "grad_norm": 58.01676940917969,
      "learning_rate": 2.0092592592592595e-06,
      "loss": 1.4288,
      "step": 3932
    },
    {
      "epoch": 8.19375,
      "grad_norm": 31.760395050048828,
      "learning_rate": 2.0069444444444446e-06,
      "loss": 1.7501,
      "step": 3933
    },
    {
      "epoch": 8.195833333333333,
      "grad_norm": 11.274988174438477,
      "learning_rate": 2.0046296296296297e-06,
      "loss": 1.0292,
      "step": 3934
    },
    {
      "epoch": 8.197916666666666,
      "grad_norm": 7.9096550941467285,
      "learning_rate": 2.002314814814815e-06,
      "loss": 1.602,
      "step": 3935
    },
    {
      "epoch": 8.2,
      "grad_norm": 12.37069034576416,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7822,
      "step": 3936
    },
    {
      "epoch": 8.202083333333333,
      "grad_norm": 14.042308807373047,
      "learning_rate": 1.9976851851851854e-06,
      "loss": 0.6281,
      "step": 3937
    },
    {
      "epoch": 8.204166666666667,
      "grad_norm": 7.786080360412598,
      "learning_rate": 1.9953703703703705e-06,
      "loss": 0.5171,
      "step": 3938
    },
    {
      "epoch": 8.20625,
      "grad_norm": 14.790201187133789,
      "learning_rate": 1.993055555555556e-06,
      "loss": 0.5629,
      "step": 3939
    },
    {
      "epoch": 8.208333333333334,
      "grad_norm": 20.138147354125977,
      "learning_rate": 1.9907407407407407e-06,
      "loss": 1.0592,
      "step": 3940
    },
    {
      "epoch": 8.210416666666667,
      "grad_norm": 44.78482437133789,
      "learning_rate": 1.9884259259259262e-06,
      "loss": 1.4419,
      "step": 3941
    },
    {
      "epoch": 8.2125,
      "grad_norm": 79.35842895507812,
      "learning_rate": 1.9861111111111113e-06,
      "loss": 1.3028,
      "step": 3942
    },
    {
      "epoch": 8.214583333333334,
      "grad_norm": 58.29545593261719,
      "learning_rate": 1.9837962962962964e-06,
      "loss": 0.6175,
      "step": 3943
    },
    {
      "epoch": 8.216666666666667,
      "grad_norm": 28.84710121154785,
      "learning_rate": 1.9814814814814815e-06,
      "loss": 2.0542,
      "step": 3944
    },
    {
      "epoch": 8.21875,
      "grad_norm": 7.318437576293945,
      "learning_rate": 1.9791666666666666e-06,
      "loss": 0.557,
      "step": 3945
    },
    {
      "epoch": 8.220833333333333,
      "grad_norm": 32.36354064941406,
      "learning_rate": 1.976851851851852e-06,
      "loss": 1.0102,
      "step": 3946
    },
    {
      "epoch": 8.222916666666666,
      "grad_norm": 4.67917013168335,
      "learning_rate": 1.9745370370370373e-06,
      "loss": 0.1158,
      "step": 3947
    },
    {
      "epoch": 8.225,
      "grad_norm": 65.4971923828125,
      "learning_rate": 1.9722222222222224e-06,
      "loss": 1.1618,
      "step": 3948
    },
    {
      "epoch": 8.227083333333333,
      "grad_norm": 12.423675537109375,
      "learning_rate": 1.9699074074074074e-06,
      "loss": 1.1427,
      "step": 3949
    },
    {
      "epoch": 8.229166666666666,
      "grad_norm": 7.483856678009033,
      "learning_rate": 1.967592592592593e-06,
      "loss": 0.8908,
      "step": 3950
    },
    {
      "epoch": 8.23125,
      "grad_norm": 5.505402088165283,
      "learning_rate": 1.965277777777778e-06,
      "loss": 0.5557,
      "step": 3951
    },
    {
      "epoch": 8.233333333333333,
      "grad_norm": 8.045970916748047,
      "learning_rate": 1.962962962962963e-06,
      "loss": 0.3075,
      "step": 3952
    },
    {
      "epoch": 8.235416666666667,
      "grad_norm": 6.279229640960693,
      "learning_rate": 1.9606481481481483e-06,
      "loss": 0.299,
      "step": 3953
    },
    {
      "epoch": 8.2375,
      "grad_norm": 15.94173812866211,
      "learning_rate": 1.9583333333333334e-06,
      "loss": 1.5347,
      "step": 3954
    },
    {
      "epoch": 8.239583333333334,
      "grad_norm": 4.497419357299805,
      "learning_rate": 1.956018518518519e-06,
      "loss": 0.5407,
      "step": 3955
    },
    {
      "epoch": 8.241666666666667,
      "grad_norm": 97.9576416015625,
      "learning_rate": 1.953703703703704e-06,
      "loss": 1.0064,
      "step": 3956
    },
    {
      "epoch": 8.24375,
      "grad_norm": 11.918502807617188,
      "learning_rate": 1.951388888888889e-06,
      "loss": 0.8243,
      "step": 3957
    },
    {
      "epoch": 8.245833333333334,
      "grad_norm": 22.19184112548828,
      "learning_rate": 1.949074074074074e-06,
      "loss": 2.0624,
      "step": 3958
    },
    {
      "epoch": 8.247916666666667,
      "grad_norm": 5.410747051239014,
      "learning_rate": 1.9467592592592593e-06,
      "loss": 0.1816,
      "step": 3959
    },
    {
      "epoch": 8.25,
      "grad_norm": 13.924705505371094,
      "learning_rate": 1.944444444444445e-06,
      "loss": 1.1348,
      "step": 3960
    },
    {
      "epoch": 8.252083333333333,
      "grad_norm": 13.080724716186523,
      "learning_rate": 1.94212962962963e-06,
      "loss": 0.3953,
      "step": 3961
    },
    {
      "epoch": 8.254166666666666,
      "grad_norm": 7.87922477722168,
      "learning_rate": 1.939814814814815e-06,
      "loss": 0.4708,
      "step": 3962
    },
    {
      "epoch": 8.25625,
      "grad_norm": 12.110111236572266,
      "learning_rate": 1.9375e-06,
      "loss": 0.985,
      "step": 3963
    },
    {
      "epoch": 8.258333333333333,
      "grad_norm": 45.563209533691406,
      "learning_rate": 1.935185185185185e-06,
      "loss": 1.4825,
      "step": 3964
    },
    {
      "epoch": 8.260416666666666,
      "grad_norm": 4.627253532409668,
      "learning_rate": 1.9328703703703707e-06,
      "loss": 0.234,
      "step": 3965
    },
    {
      "epoch": 8.2625,
      "grad_norm": 6.526143550872803,
      "learning_rate": 1.930555555555556e-06,
      "loss": 0.2798,
      "step": 3966
    },
    {
      "epoch": 8.264583333333333,
      "grad_norm": 58.77310562133789,
      "learning_rate": 1.928240740740741e-06,
      "loss": 1.1239,
      "step": 3967
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 9.39808464050293,
      "learning_rate": 1.925925925925926e-06,
      "loss": 0.5596,
      "step": 3968
    },
    {
      "epoch": 8.26875,
      "grad_norm": 18.949308395385742,
      "learning_rate": 1.9236111111111115e-06,
      "loss": 1.4535,
      "step": 3969
    },
    {
      "epoch": 8.270833333333334,
      "grad_norm": 26.517465591430664,
      "learning_rate": 1.921296296296296e-06,
      "loss": 1.3603,
      "step": 3970
    },
    {
      "epoch": 8.272916666666667,
      "grad_norm": 76.16089630126953,
      "learning_rate": 1.9189814814814817e-06,
      "loss": 1.385,
      "step": 3971
    },
    {
      "epoch": 8.275,
      "grad_norm": 212.7210235595703,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.354,
      "step": 3972
    },
    {
      "epoch": 8.277083333333334,
      "grad_norm": 6.123861312866211,
      "learning_rate": 1.914351851851852e-06,
      "loss": 0.6641,
      "step": 3973
    },
    {
      "epoch": 8.279166666666667,
      "grad_norm": 26.37754249572754,
      "learning_rate": 1.9120370370370374e-06,
      "loss": 1.8869,
      "step": 3974
    },
    {
      "epoch": 8.28125,
      "grad_norm": 38.777225494384766,
      "learning_rate": 1.909722222222222e-06,
      "loss": 1.8709,
      "step": 3975
    },
    {
      "epoch": 8.283333333333333,
      "grad_norm": 21.98801040649414,
      "learning_rate": 1.9074074074074076e-06,
      "loss": 1.7583,
      "step": 3976
    },
    {
      "epoch": 8.285416666666666,
      "grad_norm": 28.904926300048828,
      "learning_rate": 1.905092592592593e-06,
      "loss": 1.6118,
      "step": 3977
    },
    {
      "epoch": 8.2875,
      "grad_norm": 39.5888786315918,
      "learning_rate": 1.9027777777777778e-06,
      "loss": 1.042,
      "step": 3978
    },
    {
      "epoch": 8.289583333333333,
      "grad_norm": 29.595748901367188,
      "learning_rate": 1.9004629629629631e-06,
      "loss": 1.168,
      "step": 3979
    },
    {
      "epoch": 8.291666666666666,
      "grad_norm": 24.730506896972656,
      "learning_rate": 1.8981481481481484e-06,
      "loss": 1.0317,
      "step": 3980
    },
    {
      "epoch": 8.29375,
      "grad_norm": 162.27191162109375,
      "learning_rate": 1.8958333333333333e-06,
      "loss": 2.5861,
      "step": 3981
    },
    {
      "epoch": 8.295833333333333,
      "grad_norm": 16.97342300415039,
      "learning_rate": 1.8935185185185186e-06,
      "loss": 0.8911,
      "step": 3982
    },
    {
      "epoch": 8.297916666666667,
      "grad_norm": 44.448360443115234,
      "learning_rate": 1.891203703703704e-06,
      "loss": 1.3427,
      "step": 3983
    },
    {
      "epoch": 8.3,
      "grad_norm": 9.855881690979004,
      "learning_rate": 1.888888888888889e-06,
      "loss": 0.8349,
      "step": 3984
    },
    {
      "epoch": 8.302083333333334,
      "grad_norm": 8.647411346435547,
      "learning_rate": 1.8865740740740743e-06,
      "loss": 0.3896,
      "step": 3985
    },
    {
      "epoch": 8.304166666666667,
      "grad_norm": 13.196799278259277,
      "learning_rate": 1.8842592592592592e-06,
      "loss": 1.1267,
      "step": 3986
    },
    {
      "epoch": 8.30625,
      "grad_norm": 59.240055084228516,
      "learning_rate": 1.8819444444444445e-06,
      "loss": 2.0008,
      "step": 3987
    },
    {
      "epoch": 8.308333333333334,
      "grad_norm": 6.678117752075195,
      "learning_rate": 1.8796296296296299e-06,
      "loss": 0.7154,
      "step": 3988
    },
    {
      "epoch": 8.310416666666667,
      "grad_norm": 9.790763854980469,
      "learning_rate": 1.877314814814815e-06,
      "loss": 0.9859,
      "step": 3989
    },
    {
      "epoch": 8.3125,
      "grad_norm": 21.006563186645508,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 1.7229,
      "step": 3990
    },
    {
      "epoch": 8.314583333333333,
      "grad_norm": 153.8466796875,
      "learning_rate": 1.8726851851851854e-06,
      "loss": 1.2132,
      "step": 3991
    },
    {
      "epoch": 8.316666666666666,
      "grad_norm": 11.382450103759766,
      "learning_rate": 1.8703703703703705e-06,
      "loss": 1.2231,
      "step": 3992
    },
    {
      "epoch": 8.31875,
      "grad_norm": 9.27717113494873,
      "learning_rate": 1.8680555555555558e-06,
      "loss": 0.5864,
      "step": 3993
    },
    {
      "epoch": 8.320833333333333,
      "grad_norm": 11.061345100402832,
      "learning_rate": 1.865740740740741e-06,
      "loss": 1.7933,
      "step": 3994
    },
    {
      "epoch": 8.322916666666666,
      "grad_norm": 11.42005729675293,
      "learning_rate": 1.863425925925926e-06,
      "loss": 0.6169,
      "step": 3995
    },
    {
      "epoch": 8.325,
      "grad_norm": 14.449913024902344,
      "learning_rate": 1.8611111111111113e-06,
      "loss": 0.9839,
      "step": 3996
    },
    {
      "epoch": 8.327083333333333,
      "grad_norm": 3.7755613327026367,
      "learning_rate": 1.8587962962962964e-06,
      "loss": 0.1151,
      "step": 3997
    },
    {
      "epoch": 8.329166666666667,
      "grad_norm": 30.217456817626953,
      "learning_rate": 1.8564814814814817e-06,
      "loss": 1.0947,
      "step": 3998
    },
    {
      "epoch": 8.33125,
      "grad_norm": 16.73680877685547,
      "learning_rate": 1.854166666666667e-06,
      "loss": 1.185,
      "step": 3999
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 8.95942211151123,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 0.5863,
      "step": 4000
    },
    {
      "epoch": 8.335416666666667,
      "grad_norm": 9.688681602478027,
      "learning_rate": 1.8495370370370372e-06,
      "loss": 0.3471,
      "step": 4001
    },
    {
      "epoch": 8.3375,
      "grad_norm": 34.71933364868164,
      "learning_rate": 1.8472222222222225e-06,
      "loss": 1.3375,
      "step": 4002
    },
    {
      "epoch": 8.339583333333334,
      "grad_norm": 8.471638679504395,
      "learning_rate": 1.8449074074074074e-06,
      "loss": 0.4662,
      "step": 4003
    },
    {
      "epoch": 8.341666666666667,
      "grad_norm": 329.6246643066406,
      "learning_rate": 1.8425925925925927e-06,
      "loss": 1.6493,
      "step": 4004
    },
    {
      "epoch": 8.34375,
      "grad_norm": 6.741431713104248,
      "learning_rate": 1.840277777777778e-06,
      "loss": 0.9391,
      "step": 4005
    },
    {
      "epoch": 8.345833333333333,
      "grad_norm": 4.922357082366943,
      "learning_rate": 1.837962962962963e-06,
      "loss": 0.5326,
      "step": 4006
    },
    {
      "epoch": 8.347916666666666,
      "grad_norm": 155.6930389404297,
      "learning_rate": 1.8356481481481484e-06,
      "loss": 1.0833,
      "step": 4007
    },
    {
      "epoch": 8.35,
      "grad_norm": 27.995895385742188,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 1.918,
      "step": 4008
    },
    {
      "epoch": 8.352083333333333,
      "grad_norm": 20.529720306396484,
      "learning_rate": 1.8310185185185186e-06,
      "loss": 1.9265,
      "step": 4009
    },
    {
      "epoch": 8.354166666666666,
      "grad_norm": 16.2010440826416,
      "learning_rate": 1.828703703703704e-06,
      "loss": 1.6274,
      "step": 4010
    },
    {
      "epoch": 8.35625,
      "grad_norm": 9.744790077209473,
      "learning_rate": 1.826388888888889e-06,
      "loss": 0.9856,
      "step": 4011
    },
    {
      "epoch": 8.358333333333333,
      "grad_norm": 7.809116363525391,
      "learning_rate": 1.8240740740740743e-06,
      "loss": 0.5408,
      "step": 4012
    },
    {
      "epoch": 8.360416666666667,
      "grad_norm": 8.687561988830566,
      "learning_rate": 1.8217592592592594e-06,
      "loss": 0.6267,
      "step": 4013
    },
    {
      "epoch": 8.3625,
      "grad_norm": 18.995849609375,
      "learning_rate": 1.8194444444444445e-06,
      "loss": 1.3064,
      "step": 4014
    },
    {
      "epoch": 8.364583333333334,
      "grad_norm": 73.62969207763672,
      "learning_rate": 1.8171296296296298e-06,
      "loss": 0.7204,
      "step": 4015
    },
    {
      "epoch": 8.366666666666667,
      "grad_norm": 12.37569808959961,
      "learning_rate": 1.8148148148148151e-06,
      "loss": 1.1516,
      "step": 4016
    },
    {
      "epoch": 8.36875,
      "grad_norm": 13.2819185256958,
      "learning_rate": 1.8125e-06,
      "loss": 1.2301,
      "step": 4017
    },
    {
      "epoch": 8.370833333333334,
      "grad_norm": 21.68018913269043,
      "learning_rate": 1.8101851851851853e-06,
      "loss": 1.5513,
      "step": 4018
    },
    {
      "epoch": 8.372916666666667,
      "grad_norm": 16.588809967041016,
      "learning_rate": 1.8078703703703704e-06,
      "loss": 1.0192,
      "step": 4019
    },
    {
      "epoch": 8.375,
      "grad_norm": 8.993714332580566,
      "learning_rate": 1.8055555555555557e-06,
      "loss": 1.0674,
      "step": 4020
    },
    {
      "epoch": 8.377083333333333,
      "grad_norm": 34.54105758666992,
      "learning_rate": 1.803240740740741e-06,
      "loss": 0.8217,
      "step": 4021
    },
    {
      "epoch": 8.379166666666666,
      "grad_norm": 9.158987998962402,
      "learning_rate": 1.800925925925926e-06,
      "loss": 0.5501,
      "step": 4022
    },
    {
      "epoch": 8.38125,
      "grad_norm": 69.39957427978516,
      "learning_rate": 1.7986111111111112e-06,
      "loss": 1.5253,
      "step": 4023
    },
    {
      "epoch": 8.383333333333333,
      "grad_norm": 85.93534088134766,
      "learning_rate": 1.7962962962962965e-06,
      "loss": 0.8788,
      "step": 4024
    },
    {
      "epoch": 8.385416666666666,
      "grad_norm": 11.691224098205566,
      "learning_rate": 1.7939814814814816e-06,
      "loss": 0.5176,
      "step": 4025
    },
    {
      "epoch": 8.3875,
      "grad_norm": 9.67340087890625,
      "learning_rate": 1.7916666666666667e-06,
      "loss": 0.5354,
      "step": 4026
    },
    {
      "epoch": 8.389583333333333,
      "grad_norm": 33.67900848388672,
      "learning_rate": 1.789351851851852e-06,
      "loss": 1.1495,
      "step": 4027
    },
    {
      "epoch": 8.391666666666667,
      "grad_norm": 25.449356079101562,
      "learning_rate": 1.7870370370370371e-06,
      "loss": 1.2705,
      "step": 4028
    },
    {
      "epoch": 8.39375,
      "grad_norm": 21.738492965698242,
      "learning_rate": 1.7847222222222225e-06,
      "loss": 1.8236,
      "step": 4029
    },
    {
      "epoch": 8.395833333333334,
      "grad_norm": 17.27604866027832,
      "learning_rate": 1.7824074074074073e-06,
      "loss": 1.279,
      "step": 4030
    },
    {
      "epoch": 8.397916666666667,
      "grad_norm": 50.263118743896484,
      "learning_rate": 1.7800925925925926e-06,
      "loss": 1.1827,
      "step": 4031
    },
    {
      "epoch": 8.4,
      "grad_norm": 47.89837646484375,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.8879,
      "step": 4032
    },
    {
      "epoch": 8.402083333333334,
      "grad_norm": 20.313325881958008,
      "learning_rate": 1.775462962962963e-06,
      "loss": 1.1947,
      "step": 4033
    },
    {
      "epoch": 8.404166666666667,
      "grad_norm": 27.267210006713867,
      "learning_rate": 1.7731481481481484e-06,
      "loss": 0.5787,
      "step": 4034
    },
    {
      "epoch": 8.40625,
      "grad_norm": 4.399737358093262,
      "learning_rate": 1.7708333333333337e-06,
      "loss": 0.112,
      "step": 4035
    },
    {
      "epoch": 8.408333333333333,
      "grad_norm": 189.31271362304688,
      "learning_rate": 1.7685185185185186e-06,
      "loss": 1.3009,
      "step": 4036
    },
    {
      "epoch": 8.410416666666666,
      "grad_norm": 7.768319129943848,
      "learning_rate": 1.7662037037037039e-06,
      "loss": 0.49,
      "step": 4037
    },
    {
      "epoch": 8.4125,
      "grad_norm": 29.31614112854004,
      "learning_rate": 1.7638888888888892e-06,
      "loss": 1.9967,
      "step": 4038
    },
    {
      "epoch": 8.414583333333333,
      "grad_norm": 36.63279342651367,
      "learning_rate": 1.761574074074074e-06,
      "loss": 1.4217,
      "step": 4039
    },
    {
      "epoch": 8.416666666666666,
      "grad_norm": 10.182966232299805,
      "learning_rate": 1.7592592592592594e-06,
      "loss": 0.9272,
      "step": 4040
    },
    {
      "epoch": 8.41875,
      "grad_norm": 6.275667667388916,
      "learning_rate": 1.7569444444444445e-06,
      "loss": 0.5901,
      "step": 4041
    },
    {
      "epoch": 8.420833333333333,
      "grad_norm": 50.13089370727539,
      "learning_rate": 1.7546296296296298e-06,
      "loss": 1.1777,
      "step": 4042
    },
    {
      "epoch": 8.422916666666667,
      "grad_norm": 158.069091796875,
      "learning_rate": 1.752314814814815e-06,
      "loss": 1.4163,
      "step": 4043
    },
    {
      "epoch": 8.425,
      "grad_norm": 160.1696014404297,
      "learning_rate": 1.75e-06,
      "loss": 0.9742,
      "step": 4044
    },
    {
      "epoch": 8.427083333333334,
      "grad_norm": 32.33990478515625,
      "learning_rate": 1.7476851851851853e-06,
      "loss": 1.0267,
      "step": 4045
    },
    {
      "epoch": 8.429166666666667,
      "grad_norm": 31.577184677124023,
      "learning_rate": 1.7453703703703706e-06,
      "loss": 0.3257,
      "step": 4046
    },
    {
      "epoch": 8.43125,
      "grad_norm": 114.29993438720703,
      "learning_rate": 1.7430555555555557e-06,
      "loss": 0.9791,
      "step": 4047
    },
    {
      "epoch": 8.433333333333334,
      "grad_norm": 16.6116943359375,
      "learning_rate": 1.740740740740741e-06,
      "loss": 1.0341,
      "step": 4048
    },
    {
      "epoch": 8.435416666666667,
      "grad_norm": 28.891693115234375,
      "learning_rate": 1.738425925925926e-06,
      "loss": 1.135,
      "step": 4049
    },
    {
      "epoch": 8.4375,
      "grad_norm": 14.16479206085205,
      "learning_rate": 1.7361111111111112e-06,
      "loss": 1.3333,
      "step": 4050
    },
    {
      "epoch": 8.439583333333333,
      "grad_norm": 55.80172348022461,
      "learning_rate": 1.7337962962962965e-06,
      "loss": 1.1397,
      "step": 4051
    },
    {
      "epoch": 8.441666666666666,
      "grad_norm": 8.442035675048828,
      "learning_rate": 1.7314814814814814e-06,
      "loss": 0.5432,
      "step": 4052
    },
    {
      "epoch": 8.44375,
      "grad_norm": 35.34999084472656,
      "learning_rate": 1.7291666666666667e-06,
      "loss": 1.6881,
      "step": 4053
    },
    {
      "epoch": 8.445833333333333,
      "grad_norm": 49.71033477783203,
      "learning_rate": 1.726851851851852e-06,
      "loss": 0.8478,
      "step": 4054
    },
    {
      "epoch": 8.447916666666666,
      "grad_norm": 16.692550659179688,
      "learning_rate": 1.724537037037037e-06,
      "loss": 1.2541,
      "step": 4055
    },
    {
      "epoch": 8.45,
      "grad_norm": 97.58399200439453,
      "learning_rate": 1.7222222222222224e-06,
      "loss": 1.6562,
      "step": 4056
    },
    {
      "epoch": 8.452083333333333,
      "grad_norm": 9.039480209350586,
      "learning_rate": 1.7199074074074077e-06,
      "loss": 0.5946,
      "step": 4057
    },
    {
      "epoch": 8.454166666666667,
      "grad_norm": 6.340404510498047,
      "learning_rate": 1.7175925925925926e-06,
      "loss": 0.255,
      "step": 4058
    },
    {
      "epoch": 8.45625,
      "grad_norm": 35.94474411010742,
      "learning_rate": 1.715277777777778e-06,
      "loss": 0.7497,
      "step": 4059
    },
    {
      "epoch": 8.458333333333334,
      "grad_norm": 22.97216796875,
      "learning_rate": 1.7129629629629632e-06,
      "loss": 1.0661,
      "step": 4060
    },
    {
      "epoch": 8.460416666666667,
      "grad_norm": 14.112776756286621,
      "learning_rate": 1.7106481481481483e-06,
      "loss": 1.2042,
      "step": 4061
    },
    {
      "epoch": 8.4625,
      "grad_norm": 12.940988540649414,
      "learning_rate": 1.7083333333333334e-06,
      "loss": 1.2693,
      "step": 4062
    },
    {
      "epoch": 8.464583333333334,
      "grad_norm": 23.074050903320312,
      "learning_rate": 1.7060185185185187e-06,
      "loss": 1.1023,
      "step": 4063
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 8.199882507324219,
      "learning_rate": 1.7037037037037038e-06,
      "loss": 1.076,
      "step": 4064
    },
    {
      "epoch": 8.46875,
      "grad_norm": 15.19092082977295,
      "learning_rate": 1.7013888888888891e-06,
      "loss": 1.1772,
      "step": 4065
    },
    {
      "epoch": 8.470833333333333,
      "grad_norm": 5.659494876861572,
      "learning_rate": 1.699074074074074e-06,
      "loss": 0.9085,
      "step": 4066
    },
    {
      "epoch": 8.472916666666666,
      "grad_norm": 32.886959075927734,
      "learning_rate": 1.6967592592592593e-06,
      "loss": 0.7761,
      "step": 4067
    },
    {
      "epoch": 8.475,
      "grad_norm": 6.459892749786377,
      "learning_rate": 1.6944444444444446e-06,
      "loss": 0.5968,
      "step": 4068
    },
    {
      "epoch": 8.477083333333333,
      "grad_norm": 21.037256240844727,
      "learning_rate": 1.6921296296296297e-06,
      "loss": 0.6118,
      "step": 4069
    },
    {
      "epoch": 8.479166666666666,
      "grad_norm": 8.51901626586914,
      "learning_rate": 1.689814814814815e-06,
      "loss": 0.4729,
      "step": 4070
    },
    {
      "epoch": 8.48125,
      "grad_norm": 29.58734893798828,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 1.081,
      "step": 4071
    },
    {
      "epoch": 8.483333333333333,
      "grad_norm": 49.97817611694336,
      "learning_rate": 1.6851851851851852e-06,
      "loss": 1.7714,
      "step": 4072
    },
    {
      "epoch": 8.485416666666667,
      "grad_norm": 11.892419815063477,
      "learning_rate": 1.6828703703703706e-06,
      "loss": 0.9177,
      "step": 4073
    },
    {
      "epoch": 8.4875,
      "grad_norm": 45.47859191894531,
      "learning_rate": 1.6805555555555559e-06,
      "loss": 0.6428,
      "step": 4074
    },
    {
      "epoch": 8.489583333333334,
      "grad_norm": 12.039363861083984,
      "learning_rate": 1.6782407407407408e-06,
      "loss": 0.8763,
      "step": 4075
    },
    {
      "epoch": 8.491666666666667,
      "grad_norm": 5.026404857635498,
      "learning_rate": 1.675925925925926e-06,
      "loss": 0.2395,
      "step": 4076
    },
    {
      "epoch": 8.49375,
      "grad_norm": 10.103894233703613,
      "learning_rate": 1.6736111111111112e-06,
      "loss": 0.8365,
      "step": 4077
    },
    {
      "epoch": 8.495833333333334,
      "grad_norm": 27.66707992553711,
      "learning_rate": 1.6712962962962965e-06,
      "loss": 1.2155,
      "step": 4078
    },
    {
      "epoch": 8.497916666666667,
      "grad_norm": 50.85506820678711,
      "learning_rate": 1.6689814814814818e-06,
      "loss": 1.7979,
      "step": 4079
    },
    {
      "epoch": 8.5,
      "grad_norm": 8.210469245910645,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.7504,
      "step": 4080
    },
    {
      "epoch": 8.502083333333333,
      "grad_norm": 8.289114952087402,
      "learning_rate": 1.664351851851852e-06,
      "loss": 0.5833,
      "step": 4081
    },
    {
      "epoch": 8.504166666666666,
      "grad_norm": 10.526349067687988,
      "learning_rate": 1.6620370370370373e-06,
      "loss": 0.2152,
      "step": 4082
    },
    {
      "epoch": 8.50625,
      "grad_norm": 105.32066345214844,
      "learning_rate": 1.6597222222222224e-06,
      "loss": 1.1891,
      "step": 4083
    },
    {
      "epoch": 8.508333333333333,
      "grad_norm": 9.312978744506836,
      "learning_rate": 1.6574074074074075e-06,
      "loss": 0.9025,
      "step": 4084
    },
    {
      "epoch": 8.510416666666666,
      "grad_norm": 13.776140213012695,
      "learning_rate": 1.6550925925925928e-06,
      "loss": 0.7751,
      "step": 4085
    },
    {
      "epoch": 8.5125,
      "grad_norm": 7.322604656219482,
      "learning_rate": 1.6527777777777779e-06,
      "loss": 0.603,
      "step": 4086
    },
    {
      "epoch": 8.514583333333333,
      "grad_norm": 27.868745803833008,
      "learning_rate": 1.6504629629629632e-06,
      "loss": 1.1347,
      "step": 4087
    },
    {
      "epoch": 8.516666666666667,
      "grad_norm": 46.136962890625,
      "learning_rate": 1.648148148148148e-06,
      "loss": 2.0261,
      "step": 4088
    },
    {
      "epoch": 8.51875,
      "grad_norm": 9.756003379821777,
      "learning_rate": 1.6458333333333334e-06,
      "loss": 0.8843,
      "step": 4089
    },
    {
      "epoch": 8.520833333333334,
      "grad_norm": 11.822301864624023,
      "learning_rate": 1.6435185185185187e-06,
      "loss": 0.9645,
      "step": 4090
    },
    {
      "epoch": 8.522916666666667,
      "grad_norm": 16.642175674438477,
      "learning_rate": 1.6412037037037038e-06,
      "loss": 1.1417,
      "step": 4091
    },
    {
      "epoch": 8.525,
      "grad_norm": 22.84630012512207,
      "learning_rate": 1.638888888888889e-06,
      "loss": 0.8482,
      "step": 4092
    },
    {
      "epoch": 8.527083333333334,
      "grad_norm": 8.642504692077637,
      "learning_rate": 1.6365740740740744e-06,
      "loss": 0.5926,
      "step": 4093
    },
    {
      "epoch": 8.529166666666667,
      "grad_norm": 31.135900497436523,
      "learning_rate": 1.6342592592592593e-06,
      "loss": 1.1818,
      "step": 4094
    },
    {
      "epoch": 8.53125,
      "grad_norm": 30.929065704345703,
      "learning_rate": 1.6319444444444446e-06,
      "loss": 0.7954,
      "step": 4095
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 8.460081100463867,
      "learning_rate": 1.62962962962963e-06,
      "loss": 0.5915,
      "step": 4096
    },
    {
      "epoch": 8.535416666666666,
      "grad_norm": 16.44303321838379,
      "learning_rate": 1.6273148148148148e-06,
      "loss": 1.2924,
      "step": 4097
    },
    {
      "epoch": 8.5375,
      "grad_norm": 45.26395034790039,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 1.2288,
      "step": 4098
    },
    {
      "epoch": 8.539583333333333,
      "grad_norm": 9.783884048461914,
      "learning_rate": 1.6226851851851852e-06,
      "loss": 0.9133,
      "step": 4099
    },
    {
      "epoch": 8.541666666666666,
      "grad_norm": 51.89487838745117,
      "learning_rate": 1.6203703703703705e-06,
      "loss": 1.2983,
      "step": 4100
    },
    {
      "epoch": 8.54375,
      "grad_norm": 13.26855182647705,
      "learning_rate": 1.6180555555555558e-06,
      "loss": 1.7523,
      "step": 4101
    },
    {
      "epoch": 8.545833333333333,
      "grad_norm": 47.24570846557617,
      "learning_rate": 1.6157407407407407e-06,
      "loss": 1.9558,
      "step": 4102
    },
    {
      "epoch": 8.547916666666667,
      "grad_norm": 94.53837585449219,
      "learning_rate": 1.613425925925926e-06,
      "loss": 1.2515,
      "step": 4103
    },
    {
      "epoch": 8.55,
      "grad_norm": 4.3276262283325195,
      "learning_rate": 1.6111111111111113e-06,
      "loss": 0.1962,
      "step": 4104
    },
    {
      "epoch": 8.552083333333334,
      "grad_norm": 19.20165252685547,
      "learning_rate": 1.6087962962962964e-06,
      "loss": 1.0716,
      "step": 4105
    },
    {
      "epoch": 8.554166666666667,
      "grad_norm": 23.863670349121094,
      "learning_rate": 1.6064814814814817e-06,
      "loss": 0.5773,
      "step": 4106
    },
    {
      "epoch": 8.55625,
      "grad_norm": 23.586483001708984,
      "learning_rate": 1.6041666666666668e-06,
      "loss": 1.8077,
      "step": 4107
    },
    {
      "epoch": 8.558333333333334,
      "grad_norm": 58.65827560424805,
      "learning_rate": 1.601851851851852e-06,
      "loss": 1.1435,
      "step": 4108
    },
    {
      "epoch": 8.560416666666667,
      "grad_norm": 34.15890121459961,
      "learning_rate": 1.5995370370370372e-06,
      "loss": 1.4067,
      "step": 4109
    },
    {
      "epoch": 8.5625,
      "grad_norm": 4.846898555755615,
      "learning_rate": 1.5972222222222221e-06,
      "loss": 0.1157,
      "step": 4110
    },
    {
      "epoch": 8.564583333333333,
      "grad_norm": 9.724809646606445,
      "learning_rate": 1.5949074074074074e-06,
      "loss": 1.0001,
      "step": 4111
    },
    {
      "epoch": 8.566666666666666,
      "grad_norm": 50.89746856689453,
      "learning_rate": 1.5925925925925927e-06,
      "loss": 1.7482,
      "step": 4112
    },
    {
      "epoch": 8.56875,
      "grad_norm": 35.35070037841797,
      "learning_rate": 1.5902777777777778e-06,
      "loss": 1.3154,
      "step": 4113
    },
    {
      "epoch": 8.570833333333333,
      "grad_norm": 23.415632247924805,
      "learning_rate": 1.5879629629629632e-06,
      "loss": 1.1146,
      "step": 4114
    },
    {
      "epoch": 8.572916666666666,
      "grad_norm": 48.18274688720703,
      "learning_rate": 1.5856481481481485e-06,
      "loss": 1.5034,
      "step": 4115
    },
    {
      "epoch": 8.575,
      "grad_norm": 33.531654357910156,
      "learning_rate": 1.5833333333333333e-06,
      "loss": 1.4477,
      "step": 4116
    },
    {
      "epoch": 8.577083333333333,
      "grad_norm": 12.588930130004883,
      "learning_rate": 1.5810185185185187e-06,
      "loss": 0.5552,
      "step": 4117
    },
    {
      "epoch": 8.579166666666667,
      "grad_norm": 45.41214370727539,
      "learning_rate": 1.578703703703704e-06,
      "loss": 0.9881,
      "step": 4118
    },
    {
      "epoch": 8.58125,
      "grad_norm": 85.10969543457031,
      "learning_rate": 1.576388888888889e-06,
      "loss": 1.2172,
      "step": 4119
    },
    {
      "epoch": 8.583333333333334,
      "grad_norm": 11.476555824279785,
      "learning_rate": 1.5740740740740742e-06,
      "loss": 0.8442,
      "step": 4120
    },
    {
      "epoch": 8.585416666666667,
      "grad_norm": 8.60761547088623,
      "learning_rate": 1.5717592592592593e-06,
      "loss": 0.7102,
      "step": 4121
    },
    {
      "epoch": 8.5875,
      "grad_norm": 9.036279678344727,
      "learning_rate": 1.5694444444444446e-06,
      "loss": 0.6909,
      "step": 4122
    },
    {
      "epoch": 8.589583333333334,
      "grad_norm": 53.942108154296875,
      "learning_rate": 1.5671296296296299e-06,
      "loss": 1.6146,
      "step": 4123
    },
    {
      "epoch": 8.591666666666667,
      "grad_norm": 7.146313190460205,
      "learning_rate": 1.5648148148148148e-06,
      "loss": 0.5792,
      "step": 4124
    },
    {
      "epoch": 8.59375,
      "grad_norm": 17.594575881958008,
      "learning_rate": 1.5625e-06,
      "loss": 1.3415,
      "step": 4125
    },
    {
      "epoch": 8.595833333333333,
      "grad_norm": 6.9561028480529785,
      "learning_rate": 1.5601851851851854e-06,
      "loss": 0.4987,
      "step": 4126
    },
    {
      "epoch": 8.597916666666666,
      "grad_norm": 9.815581321716309,
      "learning_rate": 1.5578703703703705e-06,
      "loss": 1.0555,
      "step": 4127
    },
    {
      "epoch": 8.6,
      "grad_norm": 7.838327407836914,
      "learning_rate": 1.5555555555555558e-06,
      "loss": 0.3206,
      "step": 4128
    },
    {
      "epoch": 8.602083333333333,
      "grad_norm": 18.01093292236328,
      "learning_rate": 1.5532407407407409e-06,
      "loss": 1.3159,
      "step": 4129
    },
    {
      "epoch": 8.604166666666666,
      "grad_norm": 97.43742370605469,
      "learning_rate": 1.550925925925926e-06,
      "loss": 1.368,
      "step": 4130
    },
    {
      "epoch": 8.60625,
      "grad_norm": 15.15847110748291,
      "learning_rate": 1.5486111111111113e-06,
      "loss": 1.0063,
      "step": 4131
    },
    {
      "epoch": 8.608333333333333,
      "grad_norm": 17.38489532470703,
      "learning_rate": 1.5462962962962964e-06,
      "loss": 2.0139,
      "step": 4132
    },
    {
      "epoch": 8.610416666666667,
      "grad_norm": 27.620868682861328,
      "learning_rate": 1.5439814814814815e-06,
      "loss": 1.921,
      "step": 4133
    },
    {
      "epoch": 8.6125,
      "grad_norm": 27.3459415435791,
      "learning_rate": 1.5416666666666668e-06,
      "loss": 1.3039,
      "step": 4134
    },
    {
      "epoch": 8.614583333333334,
      "grad_norm": 18.389333724975586,
      "learning_rate": 1.539351851851852e-06,
      "loss": 1.9374,
      "step": 4135
    },
    {
      "epoch": 8.616666666666667,
      "grad_norm": 14.82397747039795,
      "learning_rate": 1.5370370370370372e-06,
      "loss": 1.2841,
      "step": 4136
    },
    {
      "epoch": 8.61875,
      "grad_norm": 33.24980163574219,
      "learning_rate": 1.5347222222222225e-06,
      "loss": 0.5706,
      "step": 4137
    },
    {
      "epoch": 8.620833333333334,
      "grad_norm": 125.35296630859375,
      "learning_rate": 1.5324074074074074e-06,
      "loss": 0.6483,
      "step": 4138
    },
    {
      "epoch": 8.622916666666667,
      "grad_norm": 42.405113220214844,
      "learning_rate": 1.5300925925925927e-06,
      "loss": 0.3375,
      "step": 4139
    },
    {
      "epoch": 8.625,
      "grad_norm": 6.037684440612793,
      "learning_rate": 1.527777777777778e-06,
      "loss": 0.9447,
      "step": 4140
    },
    {
      "epoch": 8.627083333333333,
      "grad_norm": 6.147265911102295,
      "learning_rate": 1.5254629629629631e-06,
      "loss": 0.9749,
      "step": 4141
    },
    {
      "epoch": 8.629166666666666,
      "grad_norm": 11.523483276367188,
      "learning_rate": 1.5231481481481482e-06,
      "loss": 0.5485,
      "step": 4142
    },
    {
      "epoch": 8.63125,
      "grad_norm": 26.17581558227539,
      "learning_rate": 1.5208333333333333e-06,
      "loss": 1.3246,
      "step": 4143
    },
    {
      "epoch": 8.633333333333333,
      "grad_norm": 8.155919075012207,
      "learning_rate": 1.5185185185185186e-06,
      "loss": 1.0261,
      "step": 4144
    },
    {
      "epoch": 8.635416666666666,
      "grad_norm": 32.82434844970703,
      "learning_rate": 1.516203703703704e-06,
      "loss": 1.3268,
      "step": 4145
    },
    {
      "epoch": 8.6375,
      "grad_norm": 93.8074722290039,
      "learning_rate": 1.5138888888888888e-06,
      "loss": 1.9368,
      "step": 4146
    },
    {
      "epoch": 8.639583333333333,
      "grad_norm": 52.48190689086914,
      "learning_rate": 1.5115740740740741e-06,
      "loss": 1.3267,
      "step": 4147
    },
    {
      "epoch": 8.641666666666667,
      "grad_norm": 8.196348190307617,
      "learning_rate": 1.5092592592592594e-06,
      "loss": 0.3013,
      "step": 4148
    },
    {
      "epoch": 8.64375,
      "grad_norm": 11.010284423828125,
      "learning_rate": 1.5069444444444445e-06,
      "loss": 1.0398,
      "step": 4149
    },
    {
      "epoch": 8.645833333333334,
      "grad_norm": 5.091958999633789,
      "learning_rate": 1.5046296296296298e-06,
      "loss": 0.5254,
      "step": 4150
    },
    {
      "epoch": 8.647916666666667,
      "grad_norm": 14.259431838989258,
      "learning_rate": 1.5023148148148152e-06,
      "loss": 0.8835,
      "step": 4151
    },
    {
      "epoch": 8.65,
      "grad_norm": 7.247906684875488,
      "learning_rate": 1.5e-06,
      "loss": 0.3007,
      "step": 4152
    },
    {
      "epoch": 8.652083333333334,
      "grad_norm": 14.569366455078125,
      "learning_rate": 1.4976851851851853e-06,
      "loss": 1.5714,
      "step": 4153
    },
    {
      "epoch": 8.654166666666667,
      "grad_norm": 12.818551063537598,
      "learning_rate": 1.4953703703703704e-06,
      "loss": 1.1496,
      "step": 4154
    },
    {
      "epoch": 8.65625,
      "grad_norm": 11.312235832214355,
      "learning_rate": 1.4930555555555555e-06,
      "loss": 0.9898,
      "step": 4155
    },
    {
      "epoch": 8.658333333333333,
      "grad_norm": 18.748430252075195,
      "learning_rate": 1.4907407407407409e-06,
      "loss": 1.5239,
      "step": 4156
    },
    {
      "epoch": 8.660416666666666,
      "grad_norm": 12.390924453735352,
      "learning_rate": 1.488425925925926e-06,
      "loss": 1.3603,
      "step": 4157
    },
    {
      "epoch": 8.6625,
      "grad_norm": 24.296709060668945,
      "learning_rate": 1.4861111111111113e-06,
      "loss": 0.7873,
      "step": 4158
    },
    {
      "epoch": 8.664583333333333,
      "grad_norm": 79.7103271484375,
      "learning_rate": 1.4837962962962966e-06,
      "loss": 1.335,
      "step": 4159
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 44.9908447265625,
      "learning_rate": 1.4814814814814815e-06,
      "loss": 1.264,
      "step": 4160
    },
    {
      "epoch": 8.66875,
      "grad_norm": 11.963067054748535,
      "learning_rate": 1.4791666666666668e-06,
      "loss": 1.0593,
      "step": 4161
    },
    {
      "epoch": 8.670833333333333,
      "grad_norm": 18.159717559814453,
      "learning_rate": 1.476851851851852e-06,
      "loss": 1.2098,
      "step": 4162
    },
    {
      "epoch": 8.672916666666667,
      "grad_norm": 29.494686126708984,
      "learning_rate": 1.4745370370370372e-06,
      "loss": 0.9387,
      "step": 4163
    },
    {
      "epoch": 8.675,
      "grad_norm": 3.967442750930786,
      "learning_rate": 1.4722222222222225e-06,
      "loss": 0.1952,
      "step": 4164
    },
    {
      "epoch": 8.677083333333334,
      "grad_norm": 11.9129638671875,
      "learning_rate": 1.4699074074074074e-06,
      "loss": 0.9132,
      "step": 4165
    },
    {
      "epoch": 8.679166666666667,
      "grad_norm": 17.716442108154297,
      "learning_rate": 1.4675925925925927e-06,
      "loss": 1.1426,
      "step": 4166
    },
    {
      "epoch": 8.68125,
      "grad_norm": 31.985336303710938,
      "learning_rate": 1.465277777777778e-06,
      "loss": 1.0928,
      "step": 4167
    },
    {
      "epoch": 8.683333333333334,
      "grad_norm": 6.37960147857666,
      "learning_rate": 1.4629629629629629e-06,
      "loss": 0.603,
      "step": 4168
    },
    {
      "epoch": 8.685416666666667,
      "grad_norm": 24.39519500732422,
      "learning_rate": 1.4606481481481482e-06,
      "loss": 0.9297,
      "step": 4169
    },
    {
      "epoch": 8.6875,
      "grad_norm": 24.690397262573242,
      "learning_rate": 1.4583333333333335e-06,
      "loss": 0.7775,
      "step": 4170
    },
    {
      "epoch": 8.689583333333333,
      "grad_norm": 48.84019088745117,
      "learning_rate": 1.4560185185185186e-06,
      "loss": 0.8242,
      "step": 4171
    },
    {
      "epoch": 8.691666666666666,
      "grad_norm": 40.48038101196289,
      "learning_rate": 1.453703703703704e-06,
      "loss": 2.3017,
      "step": 4172
    },
    {
      "epoch": 8.69375,
      "grad_norm": 13.549174308776855,
      "learning_rate": 1.4513888888888892e-06,
      "loss": 1.2468,
      "step": 4173
    },
    {
      "epoch": 8.695833333333333,
      "grad_norm": 18.304527282714844,
      "learning_rate": 1.449074074074074e-06,
      "loss": 1.5131,
      "step": 4174
    },
    {
      "epoch": 8.697916666666666,
      "grad_norm": 22.335092544555664,
      "learning_rate": 1.4467592592592594e-06,
      "loss": 1.1759,
      "step": 4175
    },
    {
      "epoch": 8.7,
      "grad_norm": 19.011598587036133,
      "learning_rate": 1.4444444444444445e-06,
      "loss": 0.6417,
      "step": 4176
    },
    {
      "epoch": 8.702083333333333,
      "grad_norm": 35.9295654296875,
      "learning_rate": 1.4421296296296298e-06,
      "loss": 0.8659,
      "step": 4177
    },
    {
      "epoch": 8.704166666666667,
      "grad_norm": 6.19563627243042,
      "learning_rate": 1.439814814814815e-06,
      "loss": 0.5365,
      "step": 4178
    },
    {
      "epoch": 8.70625,
      "grad_norm": 9.018149375915527,
      "learning_rate": 1.4375e-06,
      "loss": 0.5458,
      "step": 4179
    },
    {
      "epoch": 8.708333333333334,
      "grad_norm": 15.476114273071289,
      "learning_rate": 1.4351851851851853e-06,
      "loss": 1.2227,
      "step": 4180
    },
    {
      "epoch": 8.710416666666667,
      "grad_norm": 6.233034133911133,
      "learning_rate": 1.4328703703703706e-06,
      "loss": 1.027,
      "step": 4181
    },
    {
      "epoch": 8.7125,
      "grad_norm": 62.69807815551758,
      "learning_rate": 1.4305555555555555e-06,
      "loss": 0.9771,
      "step": 4182
    },
    {
      "epoch": 8.714583333333334,
      "grad_norm": 4.948574542999268,
      "learning_rate": 1.4282407407407408e-06,
      "loss": 0.1202,
      "step": 4183
    },
    {
      "epoch": 8.716666666666667,
      "grad_norm": 92.15906524658203,
      "learning_rate": 1.4259259259259261e-06,
      "loss": 0.6729,
      "step": 4184
    },
    {
      "epoch": 8.71875,
      "grad_norm": 49.776851654052734,
      "learning_rate": 1.4236111111111112e-06,
      "loss": 1.3017,
      "step": 4185
    },
    {
      "epoch": 8.720833333333333,
      "grad_norm": 112.66648864746094,
      "learning_rate": 1.4212962962962965e-06,
      "loss": 1.3961,
      "step": 4186
    },
    {
      "epoch": 8.722916666666666,
      "grad_norm": 44.79891586303711,
      "learning_rate": 1.4189814814814814e-06,
      "loss": 1.2897,
      "step": 4187
    },
    {
      "epoch": 8.725,
      "grad_norm": 129.34751892089844,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 1.2815,
      "step": 4188
    },
    {
      "epoch": 8.727083333333333,
      "grad_norm": 11.410163879394531,
      "learning_rate": 1.414351851851852e-06,
      "loss": 0.7341,
      "step": 4189
    },
    {
      "epoch": 8.729166666666666,
      "grad_norm": 31.673057556152344,
      "learning_rate": 1.4120370370370371e-06,
      "loss": 0.991,
      "step": 4190
    },
    {
      "epoch": 8.73125,
      "grad_norm": 14.847663879394531,
      "learning_rate": 1.4097222222222222e-06,
      "loss": 1.4667,
      "step": 4191
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 76.20582580566406,
      "learning_rate": 1.4074074074074075e-06,
      "loss": 0.8157,
      "step": 4192
    },
    {
      "epoch": 8.735416666666667,
      "grad_norm": 15.581509590148926,
      "learning_rate": 1.4050925925925926e-06,
      "loss": 1.6927,
      "step": 4193
    },
    {
      "epoch": 8.7375,
      "grad_norm": 11.28167724609375,
      "learning_rate": 1.402777777777778e-06,
      "loss": 0.7398,
      "step": 4194
    },
    {
      "epoch": 8.739583333333334,
      "grad_norm": 13.516016006469727,
      "learning_rate": 1.4004629629629633e-06,
      "loss": 1.0631,
      "step": 4195
    },
    {
      "epoch": 8.741666666666667,
      "grad_norm": 111.97561645507812,
      "learning_rate": 1.3981481481481481e-06,
      "loss": 1.7142,
      "step": 4196
    },
    {
      "epoch": 8.74375,
      "grad_norm": 32.39856719970703,
      "learning_rate": 1.3958333333333335e-06,
      "loss": 1.0989,
      "step": 4197
    },
    {
      "epoch": 8.745833333333334,
      "grad_norm": 30.42510986328125,
      "learning_rate": 1.3935185185185188e-06,
      "loss": 2.0924,
      "step": 4198
    },
    {
      "epoch": 8.747916666666667,
      "grad_norm": 15.9868803024292,
      "learning_rate": 1.3912037037037039e-06,
      "loss": 0.8916,
      "step": 4199
    },
    {
      "epoch": 8.75,
      "grad_norm": 25.087860107421875,
      "learning_rate": 1.3888888888888892e-06,
      "loss": 2.1483,
      "step": 4200
    },
    {
      "epoch": 8.752083333333333,
      "grad_norm": 16.090557098388672,
      "learning_rate": 1.386574074074074e-06,
      "loss": 1.6239,
      "step": 4201
    },
    {
      "epoch": 8.754166666666666,
      "grad_norm": 8.567198753356934,
      "learning_rate": 1.3842592592592594e-06,
      "loss": 0.8782,
      "step": 4202
    },
    {
      "epoch": 8.75625,
      "grad_norm": 6.828665733337402,
      "learning_rate": 1.3819444444444447e-06,
      "loss": 0.8639,
      "step": 4203
    },
    {
      "epoch": 8.758333333333333,
      "grad_norm": 6.758376121520996,
      "learning_rate": 1.3796296296296296e-06,
      "loss": 0.6788,
      "step": 4204
    },
    {
      "epoch": 8.760416666666666,
      "grad_norm": 36.116600036621094,
      "learning_rate": 1.3773148148148149e-06,
      "loss": 1.7084,
      "step": 4205
    },
    {
      "epoch": 8.7625,
      "grad_norm": 97.16835021972656,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 1.7498,
      "step": 4206
    },
    {
      "epoch": 8.764583333333333,
      "grad_norm": 44.996822357177734,
      "learning_rate": 1.3726851851851853e-06,
      "loss": 0.6295,
      "step": 4207
    },
    {
      "epoch": 8.766666666666667,
      "grad_norm": 12.735595703125,
      "learning_rate": 1.3703703703703706e-06,
      "loss": 1.2035,
      "step": 4208
    },
    {
      "epoch": 8.76875,
      "grad_norm": 28.948909759521484,
      "learning_rate": 1.3680555555555559e-06,
      "loss": 1.2654,
      "step": 4209
    },
    {
      "epoch": 8.770833333333334,
      "grad_norm": 5.756765842437744,
      "learning_rate": 1.3657407407407408e-06,
      "loss": 0.2934,
      "step": 4210
    },
    {
      "epoch": 8.772916666666667,
      "grad_norm": 73.72917175292969,
      "learning_rate": 1.363425925925926e-06,
      "loss": 0.9952,
      "step": 4211
    },
    {
      "epoch": 8.775,
      "grad_norm": 4.509458541870117,
      "learning_rate": 1.3611111111111112e-06,
      "loss": 0.5103,
      "step": 4212
    },
    {
      "epoch": 8.777083333333334,
      "grad_norm": 11.03087043762207,
      "learning_rate": 1.3587962962962965e-06,
      "loss": 0.9439,
      "step": 4213
    },
    {
      "epoch": 8.779166666666667,
      "grad_norm": 36.309471130371094,
      "learning_rate": 1.3564814814814816e-06,
      "loss": 1.6869,
      "step": 4214
    },
    {
      "epoch": 8.78125,
      "grad_norm": 8.581396102905273,
      "learning_rate": 1.3541666666666667e-06,
      "loss": 0.732,
      "step": 4215
    },
    {
      "epoch": 8.783333333333333,
      "grad_norm": 10.472458839416504,
      "learning_rate": 1.351851851851852e-06,
      "loss": 1.0397,
      "step": 4216
    },
    {
      "epoch": 8.785416666666666,
      "grad_norm": 11.734533309936523,
      "learning_rate": 1.3495370370370373e-06,
      "loss": 1.0591,
      "step": 4217
    },
    {
      "epoch": 8.7875,
      "grad_norm": 19.746028900146484,
      "learning_rate": 1.3472222222222222e-06,
      "loss": 1.0121,
      "step": 4218
    },
    {
      "epoch": 8.789583333333333,
      "grad_norm": 5.099910259246826,
      "learning_rate": 1.3449074074074075e-06,
      "loss": 0.2163,
      "step": 4219
    },
    {
      "epoch": 8.791666666666666,
      "grad_norm": 13.101693153381348,
      "learning_rate": 1.3425925925925928e-06,
      "loss": 0.7522,
      "step": 4220
    },
    {
      "epoch": 8.79375,
      "grad_norm": 20.656696319580078,
      "learning_rate": 1.340277777777778e-06,
      "loss": 0.9003,
      "step": 4221
    },
    {
      "epoch": 8.795833333333333,
      "grad_norm": 6.500315189361572,
      "learning_rate": 1.3379629629629632e-06,
      "loss": 0.1477,
      "step": 4222
    },
    {
      "epoch": 8.797916666666667,
      "grad_norm": 11.039098739624023,
      "learning_rate": 1.335648148148148e-06,
      "loss": 0.9795,
      "step": 4223
    },
    {
      "epoch": 8.8,
      "grad_norm": 55.9163818359375,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 1.5332,
      "step": 4224
    },
    {
      "epoch": 8.802083333333334,
      "grad_norm": 88.5132827758789,
      "learning_rate": 1.3310185185185187e-06,
      "loss": 2.2222,
      "step": 4225
    },
    {
      "epoch": 8.804166666666667,
      "grad_norm": 35.20698928833008,
      "learning_rate": 1.3287037037037038e-06,
      "loss": 1.3827,
      "step": 4226
    },
    {
      "epoch": 8.80625,
      "grad_norm": 33.90860366821289,
      "learning_rate": 1.326388888888889e-06,
      "loss": 1.0631,
      "step": 4227
    },
    {
      "epoch": 8.808333333333334,
      "grad_norm": 6.7541069984436035,
      "learning_rate": 1.3240740740740742e-06,
      "loss": 0.5569,
      "step": 4228
    },
    {
      "epoch": 8.810416666666667,
      "grad_norm": 38.35353469848633,
      "learning_rate": 1.3217592592592593e-06,
      "loss": 1.263,
      "step": 4229
    },
    {
      "epoch": 8.8125,
      "grad_norm": 18.872386932373047,
      "learning_rate": 1.3194444444444446e-06,
      "loss": 1.1126,
      "step": 4230
    },
    {
      "epoch": 8.814583333333333,
      "grad_norm": 11.60097885131836,
      "learning_rate": 1.31712962962963e-06,
      "loss": 1.0075,
      "step": 4231
    },
    {
      "epoch": 8.816666666666666,
      "grad_norm": 24.530881881713867,
      "learning_rate": 1.3148148148148148e-06,
      "loss": 1.9466,
      "step": 4232
    },
    {
      "epoch": 8.81875,
      "grad_norm": 18.692914962768555,
      "learning_rate": 1.3125000000000001e-06,
      "loss": 0.3891,
      "step": 4233
    },
    {
      "epoch": 8.820833333333333,
      "grad_norm": 10.024937629699707,
      "learning_rate": 1.3101851851851852e-06,
      "loss": 0.755,
      "step": 4234
    },
    {
      "epoch": 8.822916666666666,
      "grad_norm": 8.299214363098145,
      "learning_rate": 1.3078703703703705e-06,
      "loss": 0.572,
      "step": 4235
    },
    {
      "epoch": 8.825,
      "grad_norm": 8.486701965332031,
      "learning_rate": 1.3055555555555556e-06,
      "loss": 0.4795,
      "step": 4236
    },
    {
      "epoch": 8.827083333333333,
      "grad_norm": 11.63558292388916,
      "learning_rate": 1.3032407407407407e-06,
      "loss": 0.5746,
      "step": 4237
    },
    {
      "epoch": 8.829166666666667,
      "grad_norm": 11.925477027893066,
      "learning_rate": 1.300925925925926e-06,
      "loss": 0.7474,
      "step": 4238
    },
    {
      "epoch": 8.83125,
      "grad_norm": 18.782373428344727,
      "learning_rate": 1.2986111111111114e-06,
      "loss": 0.8972,
      "step": 4239
    },
    {
      "epoch": 8.833333333333334,
      "grad_norm": 38.68320846557617,
      "learning_rate": 1.2962962962962962e-06,
      "loss": 1.3161,
      "step": 4240
    },
    {
      "epoch": 8.835416666666667,
      "grad_norm": 54.41499328613281,
      "learning_rate": 1.2939814814814816e-06,
      "loss": 0.6721,
      "step": 4241
    },
    {
      "epoch": 8.8375,
      "grad_norm": 26.05728530883789,
      "learning_rate": 1.2916666666666669e-06,
      "loss": 1.0986,
      "step": 4242
    },
    {
      "epoch": 8.839583333333334,
      "grad_norm": 16.14219856262207,
      "learning_rate": 1.289351851851852e-06,
      "loss": 0.8736,
      "step": 4243
    },
    {
      "epoch": 8.841666666666667,
      "grad_norm": 5.580020904541016,
      "learning_rate": 1.2870370370370373e-06,
      "loss": 0.1833,
      "step": 4244
    },
    {
      "epoch": 8.84375,
      "grad_norm": 25.408836364746094,
      "learning_rate": 1.2847222222222222e-06,
      "loss": 1.5017,
      "step": 4245
    },
    {
      "epoch": 8.845833333333333,
      "grad_norm": 8.25832748413086,
      "learning_rate": 1.2824074074074075e-06,
      "loss": 0.4567,
      "step": 4246
    },
    {
      "epoch": 8.847916666666666,
      "grad_norm": 54.65373611450195,
      "learning_rate": 1.2800925925925928e-06,
      "loss": 1.5755,
      "step": 4247
    },
    {
      "epoch": 8.85,
      "grad_norm": 80.40288543701172,
      "learning_rate": 1.2777777777777779e-06,
      "loss": 1.4887,
      "step": 4248
    },
    {
      "epoch": 8.852083333333333,
      "grad_norm": 16.071901321411133,
      "learning_rate": 1.275462962962963e-06,
      "loss": 1.1281,
      "step": 4249
    },
    {
      "epoch": 8.854166666666666,
      "grad_norm": 43.86336135864258,
      "learning_rate": 1.2731481481481483e-06,
      "loss": 2.4722,
      "step": 4250
    },
    {
      "epoch": 8.85625,
      "grad_norm": 10.12681770324707,
      "learning_rate": 1.2708333333333334e-06,
      "loss": 0.8805,
      "step": 4251
    },
    {
      "epoch": 8.858333333333333,
      "grad_norm": 24.39705467224121,
      "learning_rate": 1.2685185185185187e-06,
      "loss": 0.8629,
      "step": 4252
    },
    {
      "epoch": 8.860416666666667,
      "grad_norm": 19.490522384643555,
      "learning_rate": 1.266203703703704e-06,
      "loss": 0.9212,
      "step": 4253
    },
    {
      "epoch": 8.8625,
      "grad_norm": 30.823139190673828,
      "learning_rate": 1.2638888888888889e-06,
      "loss": 1.4062,
      "step": 4254
    },
    {
      "epoch": 8.864583333333334,
      "grad_norm": 29.06355094909668,
      "learning_rate": 1.2615740740740742e-06,
      "loss": 1.0628,
      "step": 4255
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 18.818796157836914,
      "learning_rate": 1.2592592592592593e-06,
      "loss": 1.2981,
      "step": 4256
    },
    {
      "epoch": 8.86875,
      "grad_norm": 128.96229553222656,
      "learning_rate": 1.2569444444444446e-06,
      "loss": 1.7801,
      "step": 4257
    },
    {
      "epoch": 8.870833333333334,
      "grad_norm": 11.1619873046875,
      "learning_rate": 1.25462962962963e-06,
      "loss": 0.9397,
      "step": 4258
    },
    {
      "epoch": 8.872916666666667,
      "grad_norm": 22.214889526367188,
      "learning_rate": 1.2523148148148148e-06,
      "loss": 1.5834,
      "step": 4259
    },
    {
      "epoch": 8.875,
      "grad_norm": 30.177064895629883,
      "learning_rate": 1.25e-06,
      "loss": 1.1315,
      "step": 4260
    },
    {
      "epoch": 8.877083333333333,
      "grad_norm": 88.7984619140625,
      "learning_rate": 1.2476851851851852e-06,
      "loss": 1.5917,
      "step": 4261
    },
    {
      "epoch": 8.879166666666666,
      "grad_norm": 16.470088958740234,
      "learning_rate": 1.2453703703703705e-06,
      "loss": 0.7155,
      "step": 4262
    },
    {
      "epoch": 8.88125,
      "grad_norm": 26.143844604492188,
      "learning_rate": 1.2430555555555556e-06,
      "loss": 0.9857,
      "step": 4263
    },
    {
      "epoch": 8.883333333333333,
      "grad_norm": 11.295166969299316,
      "learning_rate": 1.240740740740741e-06,
      "loss": 0.8475,
      "step": 4264
    },
    {
      "epoch": 8.885416666666666,
      "grad_norm": 46.8838996887207,
      "learning_rate": 1.238425925925926e-06,
      "loss": 1.3517,
      "step": 4265
    },
    {
      "epoch": 8.8875,
      "grad_norm": 18.893827438354492,
      "learning_rate": 1.2361111111111113e-06,
      "loss": 1.5236,
      "step": 4266
    },
    {
      "epoch": 8.889583333333333,
      "grad_norm": 210.44691467285156,
      "learning_rate": 1.2337962962962964e-06,
      "loss": 1.5381,
      "step": 4267
    },
    {
      "epoch": 8.891666666666667,
      "grad_norm": 12.608502388000488,
      "learning_rate": 1.2314814814814815e-06,
      "loss": 0.4719,
      "step": 4268
    },
    {
      "epoch": 8.89375,
      "grad_norm": 70.5015640258789,
      "learning_rate": 1.2291666666666666e-06,
      "loss": 1.4103,
      "step": 4269
    },
    {
      "epoch": 8.895833333333334,
      "grad_norm": 31.16480827331543,
      "learning_rate": 1.226851851851852e-06,
      "loss": 1.4464,
      "step": 4270
    },
    {
      "epoch": 8.897916666666667,
      "grad_norm": 7.37916898727417,
      "learning_rate": 1.2245370370370372e-06,
      "loss": 0.4719,
      "step": 4271
    },
    {
      "epoch": 8.9,
      "grad_norm": 7.909671783447266,
      "learning_rate": 1.2222222222222223e-06,
      "loss": 0.4768,
      "step": 4272
    },
    {
      "epoch": 8.902083333333334,
      "grad_norm": 9.118995666503906,
      "learning_rate": 1.2199074074074076e-06,
      "loss": 0.3399,
      "step": 4273
    },
    {
      "epoch": 8.904166666666667,
      "grad_norm": 16.934663772583008,
      "learning_rate": 1.2175925925925927e-06,
      "loss": 0.6497,
      "step": 4274
    },
    {
      "epoch": 8.90625,
      "grad_norm": 109.53507232666016,
      "learning_rate": 1.2152777777777778e-06,
      "loss": 1.1209,
      "step": 4275
    },
    {
      "epoch": 8.908333333333333,
      "grad_norm": 70.21910858154297,
      "learning_rate": 1.212962962962963e-06,
      "loss": 0.7306,
      "step": 4276
    },
    {
      "epoch": 8.910416666666666,
      "grad_norm": 7.37446403503418,
      "learning_rate": 1.2106481481481482e-06,
      "loss": 0.4676,
      "step": 4277
    },
    {
      "epoch": 8.9125,
      "grad_norm": 6.460292816162109,
      "learning_rate": 1.2083333333333333e-06,
      "loss": 0.4525,
      "step": 4278
    },
    {
      "epoch": 8.914583333333333,
      "grad_norm": 6.083749294281006,
      "learning_rate": 1.2060185185185186e-06,
      "loss": 0.278,
      "step": 4279
    },
    {
      "epoch": 8.916666666666666,
      "grad_norm": 28.007341384887695,
      "learning_rate": 1.2037037037037037e-06,
      "loss": 1.4474,
      "step": 4280
    },
    {
      "epoch": 8.91875,
      "grad_norm": 34.792015075683594,
      "learning_rate": 1.201388888888889e-06,
      "loss": 1.4341,
      "step": 4281
    },
    {
      "epoch": 8.920833333333333,
      "grad_norm": 100.52362060546875,
      "learning_rate": 1.1990740740740742e-06,
      "loss": 1.1034,
      "step": 4282
    },
    {
      "epoch": 8.922916666666667,
      "grad_norm": 41.068817138671875,
      "learning_rate": 1.1967592592592593e-06,
      "loss": 2.9135,
      "step": 4283
    },
    {
      "epoch": 8.925,
      "grad_norm": 30.944297790527344,
      "learning_rate": 1.1944444444444446e-06,
      "loss": 1.2086,
      "step": 4284
    },
    {
      "epoch": 8.927083333333334,
      "grad_norm": 4.035264492034912,
      "learning_rate": 1.1921296296296297e-06,
      "loss": 0.1168,
      "step": 4285
    },
    {
      "epoch": 8.929166666666667,
      "grad_norm": 25.304052352905273,
      "learning_rate": 1.189814814814815e-06,
      "loss": 0.9759,
      "step": 4286
    },
    {
      "epoch": 8.93125,
      "grad_norm": 32.004920959472656,
      "learning_rate": 1.1875e-06,
      "loss": 0.8387,
      "step": 4287
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 4.887947082519531,
      "learning_rate": 1.1851851851851854e-06,
      "loss": 0.4998,
      "step": 4288
    },
    {
      "epoch": 8.935416666666667,
      "grad_norm": 10.165024757385254,
      "learning_rate": 1.1828703703703705e-06,
      "loss": 0.6218,
      "step": 4289
    },
    {
      "epoch": 8.9375,
      "grad_norm": 87.99909973144531,
      "learning_rate": 1.1805555555555556e-06,
      "loss": 1.2772,
      "step": 4290
    },
    {
      "epoch": 8.939583333333333,
      "grad_norm": 13.694548606872559,
      "learning_rate": 1.1782407407407407e-06,
      "loss": 0.639,
      "step": 4291
    },
    {
      "epoch": 8.941666666666666,
      "grad_norm": 32.32876968383789,
      "learning_rate": 1.175925925925926e-06,
      "loss": 1.0249,
      "step": 4292
    },
    {
      "epoch": 8.94375,
      "grad_norm": 4.444859504699707,
      "learning_rate": 1.1736111111111113e-06,
      "loss": 0.1247,
      "step": 4293
    },
    {
      "epoch": 8.945833333333333,
      "grad_norm": 10.00564956665039,
      "learning_rate": 1.1712962962962964e-06,
      "loss": 0.5153,
      "step": 4294
    },
    {
      "epoch": 8.947916666666666,
      "grad_norm": 23.628620147705078,
      "learning_rate": 1.1689814814814817e-06,
      "loss": 1.0507,
      "step": 4295
    },
    {
      "epoch": 8.95,
      "grad_norm": 62.762638092041016,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 1.5113,
      "step": 4296
    },
    {
      "epoch": 8.952083333333333,
      "grad_norm": 22.324066162109375,
      "learning_rate": 1.1643518518518519e-06,
      "loss": 1.2588,
      "step": 4297
    },
    {
      "epoch": 8.954166666666667,
      "grad_norm": 55.711143493652344,
      "learning_rate": 1.162037037037037e-06,
      "loss": 1.0982,
      "step": 4298
    },
    {
      "epoch": 8.95625,
      "grad_norm": 5.855955123901367,
      "learning_rate": 1.1597222222222223e-06,
      "loss": 0.9518,
      "step": 4299
    },
    {
      "epoch": 8.958333333333334,
      "grad_norm": 4.356768608093262,
      "learning_rate": 1.1574074074074076e-06,
      "loss": 0.0999,
      "step": 4300
    },
    {
      "epoch": 8.960416666666667,
      "grad_norm": 26.917434692382812,
      "learning_rate": 1.1550925925925927e-06,
      "loss": 1.0682,
      "step": 4301
    },
    {
      "epoch": 8.9625,
      "grad_norm": 13.69915771484375,
      "learning_rate": 1.152777777777778e-06,
      "loss": 1.1925,
      "step": 4302
    },
    {
      "epoch": 8.964583333333334,
      "grad_norm": 36.68516540527344,
      "learning_rate": 1.1504629629629631e-06,
      "loss": 1.1656,
      "step": 4303
    },
    {
      "epoch": 8.966666666666667,
      "grad_norm": 6.521118640899658,
      "learning_rate": 1.1481481481481482e-06,
      "loss": 0.1531,
      "step": 4304
    },
    {
      "epoch": 8.96875,
      "grad_norm": 18.95459747314453,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 1.0003,
      "step": 4305
    },
    {
      "epoch": 8.970833333333333,
      "grad_norm": 20.065805435180664,
      "learning_rate": 1.1435185185185186e-06,
      "loss": 1.0164,
      "step": 4306
    },
    {
      "epoch": 8.972916666666666,
      "grad_norm": 13.036999702453613,
      "learning_rate": 1.1412037037037037e-06,
      "loss": 0.7287,
      "step": 4307
    },
    {
      "epoch": 8.975,
      "grad_norm": 164.12799072265625,
      "learning_rate": 1.138888888888889e-06,
      "loss": 1.2844,
      "step": 4308
    },
    {
      "epoch": 8.977083333333333,
      "grad_norm": 11.168927192687988,
      "learning_rate": 1.1365740740740741e-06,
      "loss": 0.6867,
      "step": 4309
    },
    {
      "epoch": 8.979166666666666,
      "grad_norm": 77.4810562133789,
      "learning_rate": 1.1342592592592594e-06,
      "loss": 1.5325,
      "step": 4310
    },
    {
      "epoch": 8.98125,
      "grad_norm": 56.340789794921875,
      "learning_rate": 1.1319444444444445e-06,
      "loss": 1.4137,
      "step": 4311
    },
    {
      "epoch": 8.983333333333333,
      "grad_norm": 22.036781311035156,
      "learning_rate": 1.1296296296296296e-06,
      "loss": 1.4266,
      "step": 4312
    },
    {
      "epoch": 8.985416666666667,
      "grad_norm": 5.018531799316406,
      "learning_rate": 1.127314814814815e-06,
      "loss": 0.4718,
      "step": 4313
    },
    {
      "epoch": 8.9875,
      "grad_norm": 27.897632598876953,
      "learning_rate": 1.125e-06,
      "loss": 1.1541,
      "step": 4314
    },
    {
      "epoch": 8.989583333333334,
      "grad_norm": 84.2701416015625,
      "learning_rate": 1.1226851851851853e-06,
      "loss": 1.0544,
      "step": 4315
    },
    {
      "epoch": 8.991666666666667,
      "grad_norm": 128.2613983154297,
      "learning_rate": 1.1203703703703704e-06,
      "loss": 0.6147,
      "step": 4316
    },
    {
      "epoch": 8.99375,
      "grad_norm": 32.475502014160156,
      "learning_rate": 1.1180555555555557e-06,
      "loss": 1.9069,
      "step": 4317
    },
    {
      "epoch": 8.995833333333334,
      "grad_norm": 7.969343662261963,
      "learning_rate": 1.1157407407407408e-06,
      "loss": 0.5816,
      "step": 4318
    },
    {
      "epoch": 8.997916666666667,
      "grad_norm": 5.175130844116211,
      "learning_rate": 1.113425925925926e-06,
      "loss": 0.5155,
      "step": 4319
    },
    {
      "epoch": 9.0,
      "grad_norm": 57.36851119995117,
      "learning_rate": 1.111111111111111e-06,
      "loss": 1.5945,
      "step": 4320
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.6277777777777778,
      "eval_f1": 0.551476143219653,
      "eval_loss": 1.3044795989990234,
      "eval_runtime": 34.4003,
      "eval_samples_per_second": 5.233,
      "eval_steps_per_second": 2.616,
      "step": 4320
    }
  ],
  "logging_steps": 1,
  "max_steps": 4800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3532138786816e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
